[{"id": "1911.00093", "submitter": "Rise Ooi", "authors": "Rise Ooi, Takeshi Iwashita, Takeshi Fukaya, Akihiro Ida, Rio Yokota", "title": "Effect of Mixed Precision Computing on H-Matrix Vector Multiplication in\n  BEM Analysis", "comments": "Accepted manuscript to International Conference on High Performance\n  Computing in Asia-Pacific Region (HPCAsia2020), January 15--17, 2020,\n  Fukuoka, Japan", "journal-ref": null, "doi": "10.1145/3368474.3368479", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Matrix (H-matrix) is an approximation technique which splits a\ntarget dense matrix into multiple submatrices, and where a selected portion of\nsubmatrices are low-rank approximated. The technique substantially reduces both\ntime and space complexity of dense matrix vector multiplication, and hence has\nbeen applied to numerous practical problems.\n  In this paper, we aim to accelerate the H-matrix vector multiplication by\nintroducing mixed precision computing, where we employ both binary64 (FP64) and\nbinary32 (FP32) arithmetic operations. We propose three methods to introduce\nmixed precision computing to H-matrix vector multiplication, and then evaluate\nthem in a boundary element method (BEM) analysis. The numerical tests examine\nthe effects of mixed precision computing, particularly on the required\nsimulation time and rate of convergence of the iterative (BiCG-STAB) linear\nsolver. We confirm the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 04:07:52 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Ooi", "Rise", ""], ["Iwashita", "Takeshi", ""], ["Fukaya", "Takeshi", ""], ["Ida", "Akihiro", ""], ["Yokota", "Rio", ""]]}, {"id": "1911.00267", "submitter": "Johannes Bund", "authors": "Johannes Bund and Christoph Lenzen and Moti Medina", "title": "Optimal Metastability-Containing Sorting via Parallel Prefix Computation", "comments": "This article generalizes and extends work presented at DATE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Friedrichs et al. (TC 2018) showed that metastability can be contained when\nsorting inputs arising from time-to-digital converters, i.e., measurement\nvalues can be correctly sorted without resolving metastability using\nsynchronizers first. However, this work left open whether this can be done by\nsmall circuits. We show that this is indeed possible, by providing a circuit\nthat sorts Gray code inputs (possibly containing a metastable bit) and has\nasymptotically optimal depth and size. Our solution utilizes the parallel\nprefix computation (PPC) framework (JACM 1980). We improve this construction by\nbounding its fan-out by an arbitrary $f \\geq 3$, without affecting depth and\nincreasing circuit size by a small constant factor only. Thus, we obtain the\nfirst PPC circuits with asymptotically optimal size, constant fan-out, and\noptimal depth. To show that applying the PPC framework to the sorting task is\nfeasible, we prove that the latter can, despite potential metastability, be\ndecomposed such that the core operation is associative. We obtain\nasymptotically optimal metastability-containing sorting networks. We complement\nthese results with simulations, independently verifying the correctness as well\nas small size and delay of our circuits.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 09:01:18 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Bund", "Johannes", ""], ["Lenzen", "Christoph", ""], ["Medina", "Moti", ""]]}, {"id": "1911.00435", "submitter": "Carlos G. Oliver Mr", "authors": "Pericles Philippopoulos, Alessandro Ricottone, Carlos G. Oliver", "title": "Difficulty Scaling in Proof of Work for Decentralized Problem Solving", "comments": null, "journal-ref": null, "doi": "10.5195/ledger.2020.194", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose DIPS Difficulty-based Incentives for Problem Solving), a simple\nmodification of the Bitcoin proof-of-work algorithm that rewards blockchain\nminers for solving optimization problems of scientific interest. The result is\na blockchain which redirects some of the computational resources invested in\nhash-based mining towards scientific computation, effectively reducing the\namount of energy `wasted' on mining. DIPS builds the solving incentive directly\nin the proof-of-work by providing a reduction in block hashing difficulty when\noptimization improvements are found. A key advantage of this scheme is that\ndecentralization is preserved and no additional protocol layers are required on\ntop of the standard blockchain. We study two incentivization schemes and\nprovide simulation results showing that DIPS is able to reduce the amount of\nhash-power used in the network while generating solutions to optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 15:54:31 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Philippopoulos", "Pericles", ""], ["Ricottone", "Alessandro", ""], ["Oliver", "Carlos G.", ""]]}, {"id": "1911.00623", "submitter": "Sauptik Dhar", "authors": "Sauptik Dhar, Junyao Guo, Jiayi Liu, Samarth Tripathi, Unmesh Kurup,\n  Mohak Shah", "title": "On-Device Machine Learning: An Algorithms and Learning Theory\n  Perspective", "comments": "Edge Learning, TinyML, Resource Constrained Machine Learning, Deep\n  learning on device, Statistical Learning Theory, 45 pages survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant paradigm for using machine learning models on a device is to\ntrain a model in the cloud and perform inference using the trained model on the\ndevice. However, with increasing number of smart devices and improved hardware,\nthere is interest in performing model training on the device. Given this surge\nin interest, a comprehensive survey of the field from a device-agnostic\nperspective sets the stage for both understanding the state-of-the-art and for\nidentifying open challenges and future avenues of research. However, on-device\nlearning is an expansive field with connections to a large number of related\ntopics in AI and machine learning (including online learning, model adaptation,\none/few-shot learning, etc.). Hence, covering such a large number of topics in\na single survey is impractical. This survey finds a middle ground by\nreformulating the problem of on-device learning as resource constrained\nlearning where the resources are compute and memory. This reformulation allows\ntools, techniques, and algorithms from a wide variety of research areas to be\ncompared equitably. In addition to summarizing the state-of-the-art, the survey\nalso identifies a number of challenges and next steps for both the algorithmic\nand theoretical aspects of on-device learning.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 01:16:02 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 07:04:34 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Dhar", "Sauptik", ""], ["Guo", "Junyao", ""], ["Liu", "Jiayi", ""], ["Tripathi", "Samarth", ""], ["Kurup", "Unmesh", ""], ["Shah", "Mohak", ""]]}, {"id": "1911.00759", "submitter": "Mohsin Ur Rahman", "authors": "Mohsin Ur Rahman", "title": "Leader Election in the Internet of Things: Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The different functions of a leader node in Ad hoc networks, particularly in\nWireless Sensor Networks (WSNs) and Internet of Things (IoT) include generation\nof keys for encryption/decryption, finding a node with minimum energy or node\nlocated in an extreme side of the network. One essential application of IoT is\nto monitor any dangerous, sensitive or non-accessible site. Such type of\napplication requires the election of a leader located on the extreme left of\nthe network. Indeed, one can then use any leader election (LE) algorithm to\nfind this node, and then start the process of finding its boundary nodes.\nHowever, the chosen algorithm must be fault-tolerant and robust since the\ndetection of a node failure under such circumstances becomes impossible. This\npaper surveys the most significant work performed in this direction, highlights\nthe challenges and proposes possible extensions.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 18:02:40 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 00:43:13 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Rahman", "Mohsin Ur", ""]]}, {"id": "1911.00837", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Jelle Hellings, Mohammad Sadoghi", "title": "RCC: Resilient Concurrent Consensus for High-Throughput Secure\n  Transaction Processing", "comments": "To appear in the proceedings of 37th IEEE International Conference on\n  Data Engineering (ICDE) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we saw the emergence of consensus-based database systems that\npromise resilience against failures, strong data provenance, and federated data\nmanagement. Typically, these fully-replicated systems are operated on top of a\nprimary-backup consensus protocol, which limits the throughput of these systems\nto the capabilities of a single replica (the primary). To push throughput\nbeyond this single-replica limit, we propose concurrent consensus. In\nconcurrent consensus, replicas independently propose transactions, thereby\nreducing the influence of any single replica on performance. To put this idea\nin practice, we propose our RCC paradigm that can turn any primary-backup\nconsensus protocol into a concurrent consensus protocol by running many\nconsensus instances concurrently. RCC is designed with performance in mind and\nrequires minimal coordination between instances. Furthermore, RCC also promises\nincreased resilience against failures. We put the design of RCC to the test by\nimplementing it in ResilientDB, our high-performance resilient blockchain\nfabric, and comparing it with state-of-the-art primary-backup consensus\nprotocols. Our experiments show that RCC achieves up to 2.75x higher throughput\nthan other consensus protocols and can be scaled to 91 replicas.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 06:03:16 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 20:44:22 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Gupta", "Suyash", ""], ["Hellings", "Jelle", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1911.00838", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Jelle Hellings, Sajjad Rahnama, Mohammad Sadoghi", "title": "Proof-of-Execution: Reaching Consensus through Fault-Tolerant\n  Speculation", "comments": "To appear in the proceedings of 24th International Conference on\n  Extending Database Technology (EDBT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-party data management and blockchain systems require data sharing among\nparticipants. To provide resilient and consistent data sharing, transactions\nengines rely on Byzantine FaultTolerant consensus (BFT), which enables\noperations during failures and malicious behavior. Unfortunately, existing BFT\nprotocols are unsuitable for high-throughput applications due to their high\ncomputational costs, high communication costs, high client latencies, and/or\nreliance on twin-paths and non-faulty clients. In this paper, we present the\nProof-of-Execution consensus protocol (PoE) that alleviates these challenges.\nAt the core of PoE are out-of-order processing and speculative execution, which\nallow PoE to execute transactions before consensus is reached among the\nreplicas. With these techniques, PoE manages to reduce the costs of BFT in\nnormal cases, while guaranteeing reliable consensus for clients in all cases.\nWe envision the use of PoE in high-throughput multi-party data-management and\nblockchain systems. To validate this vision, we implement PoE in our efficient\nResilientDB fabric and extensively evaluate PoE against several\nstate-of-the-art BFT protocols. Our evaluation showcases that PoE achieves\nup-to-80% higher throughputs than existing BFT protocols in the presence of\nfailures.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 06:03:22 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 01:27:18 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 01:12:31 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gupta", "Suyash", ""], ["Hellings", "Jelle", ""], ["Rahnama", "Sajjad", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1911.00889", "submitter": "Dimitrios Stathis", "authors": "Dimitrios Stathis, Chirag Sudarshan, Yu Yang, Matthias Jung, Syed Asad\n  Mohamad Hasan Jafri, Christian Weis, Ahmed Hemani, Anders Lansner, Norbert\n  Wehn", "title": "eBrainII: A 3 kW Realtime Custom 3D DRAM integrated ASIC implementation\n  of a Biologically Plausible Model of a Human Scale Cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Artificial Neural Networks (ANNs) like CNN/DNN and LSTM are not\nbiologically plausible and in spite of their initial success, they cannot\nattain the cognitive capabilities enabled by the dynamic hierarchical\nassociative memory systems of biological brains. The biologically plausible\nspiking brain models, for e.g. cortex, basal ganglia and amygdala have a\ngreater potential to achieve biological brain like cognitive capabilities.\nBayesian Confidence Propagation Neural Network (BCPNN) is a biologically\nplausible spiking model of cortex. A human scale model of BCPNN in real time\nrequires 162 TFlops/s, 50 TBs of synaptic weight storage to be accessed with a\nbandwidth of 200 TBs. The spiking bandwidth is relatively modest at 250 GBs/s.\nA hand optimized implementation of rodent scale BCPNN has been implemented on\nTesla K80 GPUs require 3 kW, we extrapolate from that a human scale network\nwill require 3 MW. These power numbers rule out such implementations for field\ndeployment as advanced cognition engines in embedded systems. The key\ninnovation that this paper reports is that it is feasible and affordable to\nimplement real time BCPNN as a custom tiled ASIC in 28 nm technology with\ncustom 3D DRAM - eBrain II - that consumes 3 kWs for human scale and 12 W for\nrodent scale cortex model. Such implementations eminently fulfill the demands\nfor field deployment.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 14:02:58 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Stathis", "Dimitrios", ""], ["Sudarshan", "Chirag", ""], ["Yang", "Yu", ""], ["Jung", "Matthias", ""], ["Jafri", "Syed Asad Mohamad Hasan", ""], ["Weis", "Christian", ""], ["Hemani", "Ahmed", ""], ["Lansner", "Anders", ""], ["Wehn", "Norbert", ""]]}, {"id": "1911.00950", "submitter": "Sari Sultan", "authors": "Sari Sultan and Ayed Salman", "title": "Calcium Vulnerability Scanner (CVS): A Deeper Look", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional vulnerability scanning methods are time-consuming and indecisive,\nand they negatively affect network performance by generating high network\ntraffic. In this paper, we present a novel vulnerability scanner that is\ntime-efficient, simple, accurate, and safe. We call it a Calcium Vulnerability\nScanner (CVS). Our contribution to vulnerability scanning are the following:\n(i) minimize its required time and network traffic: compared to current\ntechnologies, we reduced the former by an average of 79% and the latter by\n99.9%, (ii) increase its accuracy: compared to current technologies, we\nimproved this by an average of 2600%, and (iii) enable the scanner to learn\nfrom previous scans in order to reduce future scanning time and enhance\naccuracy: compared to current technologies, CVS reduced scanning time by an\naverage of 97%. CVS enables a new frontier in vulnerability scanning and allow\nfor scalable and efficient deployment of such tools in large-scale networks,\ncontainers, edge computing, and cloud computing.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 19:18:13 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Sultan", "Sari", ""], ["Salman", "Ayed", ""]]}, {"id": "1911.01064", "submitter": "Dileban Karunamoorthy", "authors": "Ermyas Abebe, Dushyant Behl, Chander Govindarajan, Yining Hu, Dileban\n  Karunamoorthy, Petr Novotny, Vinayaka Pandit, Venkatraman Ramakrishna,\n  Christian Vecchiola", "title": "Enabling Enterprise Blockchain Interoperability with Trusted Data\n  Transfer (industry track)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of permissioned blockchain networks in enterprise settings has\nseen an increase in growth over the past few years. While encouraging, this is\nleading to the emergence of new data, asset and process silos limiting the\npotential value these networks bring to the broader ecosystem. Mechanisms for\nenabling network interoperability help preserve the benefits of independent\nsovereign networks, while allowing for the transfer or sharing of data, assets\nand processes across network boundaries. However, a naive approach to\ninteroperability based on traditional point-to-point integration is\ninsufficient for preserving the underlying trust decentralized networks\nprovide. In this paper, we lay the foundation for an approach to\ninteroperability based on a communication protocol that derives trust from the\nunderlying network consensus protocol. We present an architecture and a set of\nbuilding blocks that can be adapted for use in a range of network\nimplementations and demonstrate a proof-of-concept for trusted data-sharing\nbetween two independent trade finance and supply-chain networks, each running\non Hyperledger Fabric. We show how existing blockchain deployments can be\nadapted for interoperation and discuss the security and extensibility of our\narchitecture and mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 07:43:28 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Abebe", "Ermyas", ""], ["Behl", "Dushyant", ""], ["Govindarajan", "Chander", ""], ["Hu", "Yining", ""], ["Karunamoorthy", "Dileban", ""], ["Novotny", "Petr", ""], ["Pandit", "Vinayaka", ""], ["Ramakrishna", "Venkatraman", ""], ["Vecchiola", "Christian", ""]]}, {"id": "1911.01195", "submitter": "Pierre Ohlmann", "authors": "Thomas Colcombet, Nathana\\\"el Fijalkow, Pierre Ohlmann", "title": "Controlling a random population", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DC cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bertrand et al. introduced a model of parameterised systems, where each agent\nis represented by a finite state system, and studied the following control\nproblem: for any number of agents, does there exist a controller able to bring\nall agents to a target state? They showed that the problem is decidable and\nEXPTIME-complete in the adversarial setting, and posed as an open problem the\nstochastic setting, where the agent is represented by a Markov decision\nprocess. In this paper, we show that the stochastic control problem is\ndecidable. Our solution makes significant uses of well quasi orders, of the\nmax-flow min-cut theorem, and of the theory of regular cost functions. We\nintroduce an intermediate problem of independent interest called the sequential\nflow problem, and study the complexity of solving it.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 13:32:18 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 14:53:34 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 13:59:55 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Colcombet", "Thomas", ""], ["Fijalkow", "Nathana\u00ebl", ""], ["Ohlmann", "Pierre", ""]]}, {"id": "1911.01225", "submitter": "Fan (Fred) Lin", "authors": "Fred Lin, Keyur Muzumdar, Nikolay Pavlovich Laptev, Mihai-Valentin\n  Curelea, Seunghak Lee, Sriram Sankar", "title": "Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale\n  Service Environment", "comments": "13 pages", "journal-ref": "POMACS 2020", "doi": "10.1145/3392149", "report-no": null, "categories": "cs.DC cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Root cause analysis in a large-scale production environment is challenging\ndue to the complexity of services running across global data centers. Due to\nthe distributed nature of a large-scale system, the various hardware, software,\nand tooling logs are often maintained separately, making it difficult to review\nthe logs jointly for understanding production issues. Another challenge in\nreviewing the logs for identifying issues is the scale - there could easily be\nmillions of entities, each described by hundreds of features. In this paper we\npresent a fast dimensional analysis framework that automates the root cause\nanalysis on structured logs with improved scalability.\n  We first explore item-sets, i.e. combinations of feature values, that could\nidentify groups of samples with sufficient support for the target failures\nusing the Apriori algorithm and a subsequent improvement, FP-Growth. These\nalgorithms were designed for frequent item-set mining and association rule\nlearning over transactional databases. After applying them on structured logs,\nwe select the item-sets that are most unique to the target failures based on\nlift. We propose pre-processing steps with the use of a large-scale real-time\ndatabase and post-processing techniques and parallelism to further speed up the\nanalysis and improve interpretability, and demonstrate that such optimization\nis necessary for handling large-scale production datasets. We have successfully\nrolled out this approach for root cause investigation purposes in a large-scale\ninfrastructure. We also present the setup and results from multiple production\nuse cases in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 01:03:01 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 19:49:04 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Lin", "Fred", ""], ["Muzumdar", "Keyur", ""], ["Laptev", "Nikolay Pavlovich", ""], ["Curelea", "Mihai-Valentin", ""], ["Lee", "Seunghak", ""], ["Sankar", "Sriram", ""]]}, {"id": "1911.01231", "submitter": "Mohammad Mahdi Dehshibi Dr.", "authors": "Mohammad Fazlali and Sara Moazezi Eftekhar and Mohammad Mahdi Dehshibi\n  and Hadi Tabatabaee Malazi and Masoud Nosrati", "title": "Raft Consensus Algorithm: an Effective Substitute for Paxos in High\n  Throughput P2P-based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the significant problem in peer-to-peer databases is collision\nproblem. These databases do not rely on a central leader that is a reason to\nincrease scalability and fault tolerance. Utilizing these systems in high\nthroughput computing cause more flexibility in computing system and meanwhile\nsolve the problems in most of the computing systems which are depend on a\ncentral nodes. There are limited researches in this scope and they seem are not\nsuitable for using in a large scale. In this paper, we used Cassandra which is\na distributed database based on peer-to-peer network as a high throughput\ncomputing system. Cassandra uses Paxos to elect central leader by default that\ncauses collision problem. Among existent consensus algorithms Raft separates\nthe key elements of consensus, such as leader election, so enforces a stronger\ndegree of coherency to reduce the number of states that must be considered,\nsuch as collision.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:07:58 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Fazlali", "Mohammad", ""], ["Eftekhar", "Sara Moazezi", ""], ["Dehshibi", "Mohammad Mahdi", ""], ["Malazi", "Hadi Tabatabaee", ""], ["Nosrati", "Masoud", ""]]}, {"id": "1911.01626", "submitter": "Jason Li", "authors": "Jason Li", "title": "Faster Parallel Algorithm for Approximate Shortest Path", "comments": "53 pages, STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first $m\\,\\text{polylog}(n)$ work, $\\text{polylog}(n)$ time\nalgorithm in the PRAM model that computes $(1+\\epsilon)$-approximate\nsingle-source shortest paths on weighted, undirected graphs. This improves upon\nthe breakthrough result of Cohen~[JACM'00] that achieves $O(m^{1+\\epsilon_0})$\nwork and $\\text{polylog}(n)$ time. While most previous approaches, including\nCohen's, leveraged the power of hopsets, our algorithm builds upon the recent\ndevelopments in \\emph{continuous optimization}, studying the shortest path\nproblem from the lens of the closely-related \\emph{minimum transshipment}\nproblem. To obtain our algorithm, we demonstrate a series of near-linear work,\npolylogarithmic-time reductions between the problems of approximate shortest\npath, approximate transshipment, and $\\ell_1$-embeddings, and establish a\nrecursive algorithm that cycles through the three problems and reduces the\ngraph size on each cycle. As a consequence, we also obtain faster parallel\nalgorithms for approximate transshipment and $\\ell_1$-embeddings with\npolylogarithmic distortion. The minimum transshipment algorithm in particular\nimproves upon the previous best $m^{1+o(1)}$ work sequential algorithm of\nSherman~[SODA'17].\n  To improve readability, the paper is almost entirely self-contained, save for\nseveral staple theorems in algorithms and combinatorics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 05:28:08 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 06:39:59 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 07:48:00 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2020 07:47:12 GMT"}, {"version": "v5", "created": "Thu, 10 Jun 2021 05:47:12 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Li", "Jason", ""]]}, {"id": "1911.01676", "submitter": "Pedro Ramalhete", "authors": "Andreia Correia, Pedro Ramalhete, Pascal Felber", "title": "A Wait-Free Universal Construct for Large Objects", "comments": "Source code available on github https://github.com/pramalhe/CX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrency has been a subject of study for more than 50 years. Still, many\ndevelopers struggle to adapt their sequential code to be accessed concurrently.\nThis need has pushed for generic solutions and specific concurrent data\nstructures.\n  Wait-free universal constructs are attractive as they can turn a sequential\nimplementation of any object into an equivalent, yet concurrent and wait-free,\nimplementation. While highly relevant from a research perspective, these\ntechniques are of limited practical use when the underlying object or data\nstructure is sizable. The copy operation can consume much of the CPU's\nresources and significantly degrade performance.\n  To overcome this limitation, we have designed CX, a multi-instance-based\nwait-free universal construct that substantially reduces the amount of copy\noperations. The construct maintains a bounded number of instances of the object\nthat can potentially be brought up to date. We applied CX to several sequential\nimplementations of data structures, including STL implementations, and compared\nthem with existing wait-free constructs. Our evaluation shows that CX performs\nsignificantly better in most experiments, and can even rival with hand-written\nlock-free and wait-free data structures, simultaneously providing wait-free\nprogress, safe memory reclamation and high reader scalability.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 09:16:54 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Correia", "Andreia", ""], ["Ramalhete", "Pedro", ""], ["Felber", "Pascal", ""]]}, {"id": "1911.01708", "submitter": "Olasupo Ajayi", "authors": "Olasupo O. Ajayi, Antoine B. Bagula and Kun Ma", "title": "Fourth Industrial Revolution for Development: The Relevance of Cloud\n  Federation in Healthcare Support", "comments": "25 pages, 24 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inefficient healthcare is a major concern among many African nations and can\nbe mitigated by building world-class infrastructure connecting different\nmedical facilities for collaboration and resource sharing. Such infrastructure\nshould support collection and exchange of medical data for the purpose of\naccessing expertise not available locally. It should be equipped with modern\ntechnologies of the fourth industrial revolution, providing decision support to\ndoctors thereby enabling African nations leapfrog from poorly equipped to\nmedically prepared. Sadly, world-class healthcare infrastructure are a missing\npiece in the African public health ecosystem. Medical facilities are either\nnon-existent or prohibitively expensive when they exist. Federated cloud\ncomputing can provide a solution to this challenge. Being a model that allows\ncollaboration between multiple Cloud service providers through resources\npooling; it allows for the execution of tasks on computing resources flexibly\nand cost efficiently. This paper aims to connect unconnected medical facilities\nin Africa by proposing a Cloud federation for healthcare using cooperative and\ncompetitive collaboration models. Simulations were carried out to test the\nefficacy of these models using five different workload allocation schemes:\nFirst-Fit-Descending (FFD), Best-Fit-Descending (BFD), Binary-Search-Best-Fit\n(BSBF); Genetic Algorithm meta-heuristic and Stable Roommate Allocation\neconomic model for both light and heavy workloads. Results of simulations\nrevealed that the cooperative model resulted in lower delays but higher\nresource utilisation; while the competitive provided faster service delivery\nand better quality of service. BSBF and BFD resulted in the best resources\nutilisation and energy conservation. Finally, deployment considerations and\npotential business models for federated Cloud for African healthcare were\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 10:59:18 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Ajayi", "Olasupo O.", ""], ["Bagula", "Antoine B.", ""], ["Ma", "Kun", ""]]}, {"id": "1911.01715", "submitter": "Diego Ferigo", "authors": "Diego Ferigo, Silvio Traversaro, Giorgio Metta, Daniele Pucci", "title": "Gym-Ignition: Reproducible Robotic Simulations for Reinforcement\n  Learning", "comments": "Accepted in SII2020", "journal-ref": "2020 IEEE/SICE International Symposium on System Integration (SII)", "doi": "10.1109/SII46433.2020.9025951", "report-no": null, "categories": "cs.RO cs.DC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Gym-Ignition, a new framework to create reproducible\nrobotic environments for reinforcement learning research. It interfaces with\nthe new generation of Gazebo, part of the Ignition Robotics suite, which\nprovides three main improvements for reinforcement learning applications\ncompared to the alternatives: 1) the modular architecture enables using the\nsimulator as a C++ library, simplifying the interconnection with external\nsoftware; 2) multiple physics and rendering engines are supported as plugins,\nsimplifying their selection during the execution; 3) the new distributed\nsimulation capability allows simulating complex scenarios while sharing the\nload on multiple workers and machines. The core of Gym-Ignition is a component\nthat contains the Ignition Gazebo simulator and exposes a simple interface for\nits configuration and execution. We provide a Python package that allows\ndevelopers to create robotic environments simulated in Ignition Gazebo.\nEnvironments expose the common OpenAI Gym interface, making them compatible\nout-of-the-box with third-party frameworks containing reinforcement learning\nalgorithms. Simulations can be executed in both headless and GUI mode, the\nphysics engine can run in accelerated mode, and instances can be parallelized.\nFurthermore, the Gym-Ignition software architecture provides abstraction of the\nRobot and the Task, making environments agnostic on the specific runtime. This\nabstraction allows their execution also in a real-time setting on actual\nrobotic platforms, even if driven by different middlewares.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 11:19:58 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 08:23:41 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Ferigo", "Diego", ""], ["Traversaro", "Silvio", ""], ["Metta", "Giorgio", ""], ["Pucci", "Daniele", ""]]}, {"id": "1911.01858", "submitter": "Frederic Nataf", "authors": "Fr\\'ed\\'eric Nataf (Laboratory J.L. Lions, Sorbonne Universit\\'e, CNRS\n  UMR 7598, INRIA ALPINES) and Pierre-Henri Tournier (Laboratory J.L. Lions,\n  Sorbonne Universit\\'e, CNRS UMR 7598, INRIA ALPINES)", "title": "A GenEO Domain Decomposition method for Saddle Point problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an adaptive element-based domain decomposition (DD) method for\nsolving saddle point problems defined as a block two by two matrix. The\nalgorithm does not require any knowledge of the constrained space. We assume\nthat all sub matrices are sparse and that the diagonal blocks are spectrally\nequivalent to a sum of positive semi definite matrices. The latter assumption\nenables the design of adaptive coarse space for DD methods that extends the\nGenEO theory to saddle point problems. Numerical results on three dimensional\nelasticity problems for steel-rubber structures discretized by a finite element\nwith continuous pressure are shown for up to one billion degrees of freedom.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:17:03 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 13:24:10 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 13:44:14 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 07:01:44 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Nataf", "Fr\u00e9d\u00e9ric", "", "Laboratory J.L. Lions, Sorbonne Universit\u00e9, CNRS\n  UMR 7598, INRIA ALPINES"], ["Tournier", "Pierre-Henri", "", "Laboratory J.L. Lions,\n  Sorbonne Universit\u00e9, CNRS UMR 7598, INRIA ALPINES"]]}, {"id": "1911.01923", "submitter": "Ahmad Shawahna", "authors": "Hafiz ur Rahman, Farag Azzedin, Ahmad Shawahna, Faisal Sajjad, Alyahya\n  Saleh Abdulrahman", "title": "Performance Evaluation of VDI Environment", "comments": "7 pages, 15 figures, 2016 Sixth International Conference on\n  Innovative Computing Technology (INTECH)", "journal-ref": null, "doi": "10.1109/INTECH.2016.7845102", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization technology is widely used for sharing the abilities of\ncomputer systems by splitting the resources among various virtual PCs. In\ntraditional computer systems, the utilization of hardware is not maximized and\nthe resources are not fully consumed. By using the virtualization technology,\nthe maximum utilization of computer system hardware is possible. Today, many\norganizations especially educational institutes are adopting virtualization\ntechnology to minimize the costs and increase the flexibility, responsiveness,\nand efficiency. In this paper, we built a virtual educational lab in which we\nsuccessfully implemented Virtual Desktop Infrastructure (VDI) and created more\nthan 30 virtual desktops. The virtual desktops are created on the Citrix\nXenServer hypervisor using the Citrix XenDesktop. Finally, we used benchmarking\nsoftware tool called Login VSI in order to measure the performance of the\nvirtual desktop infrastructure and the XenServer hypervisor. The results have\nshown that the system performs well with 30 virtual desktops without reaching\nthe saturation point.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 23:38:10 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Rahman", "Hafiz ur", ""], ["Azzedin", "Farag", ""], ["Shawahna", "Ahmad", ""], ["Sajjad", "Faisal", ""], ["Abdulrahman", "Alyahya Saleh", ""]]}, {"id": "1911.01941", "submitter": "Sukhpal Singh Gill", "authors": "Sukhpal Singh Gill, Shreshth Tuli, Minxian Xu, Inderpreet Singh, Karan\n  Vijay Singh, Dominic Lindsay, Shikhar Tuli, Daria Smirnova, Manmeet Singh,\n  Udit Jain, Haris Pervaiz, Bhanu Sehgal, Sukhwinder Singh Kaila, Sanjay Misra,\n  Mohammad Sadegh Aslanpour, Harshit Mehta, Vlado Stankovski and Peter\n  Garraghan", "title": "Transformative effects of IoT, Blockchain and Artificial Intelligence on\n  cloud computing: Evolution, vision, trends and open challenges", "comments": "30 Pages, 4 Figures and Preprint version - Published in Elsevier's\n  Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cloud computing plays a critical role in modern society and enables a range\nof applications from infrastructure to social media. Such system must cope with\nvarying load and evolving usage reflecting societies interaction and dependency\non automated computing systems whilst satisfying Quality of Service (QoS)\nguarantees. Enabling these systems are a cohort of conceptual technologies,\nsynthesized to meet demand of evolving computing applications. In order to\nunderstand current and future challenges of such system, there is a need to\nidentify key technologies enabling future applications. In this study, we aim\nto explore how three emerging paradigms (Blockchain, IoT and Artificial\nIntelligence) will influence future cloud computing systems. Further, we\nidentify several technologies driving these paradigms and invite international\nexperts to discuss the current status and future directions of cloud computing.\nFinally, we proposed a conceptual model for cloud futurology to explore the\ninfluence of emerging paradigms and technologies on evolution of cloud\ncomputing.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 08:17:27 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Gill", "Sukhpal Singh", ""], ["Tuli", "Shreshth", ""], ["Xu", "Minxian", ""], ["Singh", "Inderpreet", ""], ["Singh", "Karan Vijay", ""], ["Lindsay", "Dominic", ""], ["Tuli", "Shikhar", ""], ["Smirnova", "Daria", ""], ["Singh", "Manmeet", ""], ["Jain", "Udit", ""], ["Pervaiz", "Haris", ""], ["Sehgal", "Bhanu", ""], ["Kaila", "Sukhwinder Singh", ""], ["Misra", "Sanjay", ""], ["Aslanpour", "Mohammad Sadegh", ""], ["Mehta", "Harshit", ""], ["Stankovski", "Vlado", ""], ["Garraghan", "Peter", ""]]}, {"id": "1911.01956", "submitter": "Peilin Zhong", "authors": "Alexandr Andoni, Clifford Stein, Peilin Zhong", "title": "Parallel Approximate Undirected Shortest Paths Via Low Hop Emulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a $(1+\\varepsilon)$-approximate parallel algorithm for computing\nshortest paths in undirected graphs, achieving $\\mathrm{poly}(\\log n)$ depth\nand $m\\mathrm{poly}(\\log n)$ work for $n$-nodes $m$-edges graphs. Although\nsequential algorithms with (nearly) optimal running time have been known for\nseveral decades, near-optimal parallel algorithms have turned out to be a much\ntougher challenge. For $(1+\\varepsilon)$-approximation, all prior algorithms\nwith $\\mathrm{poly}(\\log n)$ depth perform at least $\\Omega(mn^{c})$ work for\nsome constant $c>0$. Improving this long-standing upper bound obtained by Cohen\n(STOC'94) has been open for $25$ years.\n  We develop several new tools of independent interest. One of them is a new\nnotion beyond hopsets --- low hop emulator --- a $\\mathrm{poly}(\\log\nn)$-approximate emulator graph in which every shortest path has at most\n$O(\\log\\log n)$ hops (edges). Direct applications of the low hop emulators are\nparallel algorithms for $\\mathrm{poly}(\\log n)$-approximate single source\nshortest path (SSSP), Bourgain's embedding, metric tree embedding, and low\ndiameter decomposition, all with $\\mathrm{poly}(\\log n)$ depth and\n$m\\mathrm{poly}(\\log n)$ work.\n  To boost the approximation ratio to $(1+\\varepsilon)$, we introduce\ncompressible preconditioners and apply it inside Sherman's framework (SODA'17)\nto solve the more general problem of uncapacitated minimum cost flow (a.k.a.,\ntransshipment problem). Our algorithm computes a $(1+\\varepsilon)$-approximate\nuncapacitated minimum cost flow in $\\mathrm{poly}(\\log n)$ depth using\n$m\\mathrm{poly}(\\log n)$ work. As a consequence, it also improves the\nstate-of-the-art sequential running time from $m\\cdot 2^{O(\\sqrt{\\log n})}$ to\n$m\\mathrm{poly}(\\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 17:33:02 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Andoni", "Alexandr", ""], ["Stein", "Clifford", ""], ["Zhong", "Peilin", ""]]}, {"id": "1911.02013", "submitter": "Jun Zhao", "authors": "Wubing Chen, Zhiying Xu, Shuyu Shi, Yang Zhao, Jun Zhao", "title": "A Survey of Blockchain Applications in Different Domains", "comments": "Published in Proceedings of the 2018 International Conference on\n  Blockchain Technology and Application (ICBTA)", "journal-ref": null, "doi": "10.1145/3301403.3301407", "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains have received much attention recently since they provide\ndecentralized approaches to the creation and management of value. Many banks,\nInternet companies, car manufacturers, and even governments worldwide have\nincorporated or started considering blockchains to improve the security,\nscalability, and efficiency of their services. In this paper, we survey\nblockchain applications in different areas. These areas include cryptocurrency,\nhealthcare, advertising, insurance, copyright protection, energy, and societal\napplications. Our work provides a timely summary for individuals and\norganizations interested in blockchains. We envision our study to motivate more\nblockchain applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 03:27:27 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Chen", "Wubing", ""], ["Xu", "Zhiying", ""], ["Shi", "Shuyu", ""], ["Zhao", "Yang", ""], ["Zhao", "Jun", ""]]}, {"id": "1911.02046", "submitter": "Xiaoyu Cao", "authors": "Xiaoyu Cao, Jinyuan Jia and Neil Zhenqiang Gong", "title": "Data Poisoning Attacks to Local Differential Privacy Protocols", "comments": "To appear in the 30th Usenix Security Symposium (Usenix Security\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Differential Privacy (LDP) protocols enable an untrusted data collector\nto perform privacy-preserving data analytics. In particular, each user locally\nperturbs its data to preserve privacy before sending it to the data collector,\nwho aggregates the perturbed data to obtain statistics of interest. In the past\nseveral years, researchers from multiple communities -- such as security,\ndatabase, and theoretical computer science -- have proposed many LDP protocols.\nThese studies mainly focused on improving the utility of the LDP protocols.\nHowever, the security of LDP protocols is largely unexplored. In this work, we\naim to bridge this gap. We focus on LDP protocols for frequency estimation and\nheavy hitter identification, which are two basic data analytics tasks.\nSpecifically, we show that an attacker can inject fake users into an LDP\nprotocol and the fake users send carefully crafted data to the data collector\nsuch that the LDP protocol estimates high frequencies for arbitrary\nattacker-chosen items or identifies them as heavy hitters. We call our attacks\ndata poisoning attacks. We theoretically and/or empirically show the\neffectiveness of our attacks. We also explore three countermeasures against our\nattacks. Our experimental results show that they can effectively defend against\nour attacks in some scenarios but have limited effectiveness in others,\nhighlighting the needs for new defenses against our attacks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:20:32 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 14:44:18 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Cao", "Xiaoyu", ""], ["Jia", "Jinyuan", ""], ["Gong", "Neil Zhenqiang", ""]]}, {"id": "1911.02114", "submitter": "Li Tan", "authors": "Li Tan, Marc Charest, Nathan DeBardeleben, Qiang Guan, Ben Bergen", "title": "Soft Error Resilience and Failure Recovery for Continuum Dynamics\n  Applications", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The persistently growing resilience concerns of large-scale computing systems\ntoday require not only generic fault tolerance approaches, but also\napplication-level resilience, due to demanding efficiency and various\ndomain-specific requirements. Scientific applications within a particular\ndomain generally comply with domain conservation laws, which can be leveraged\nas an error detection criterion to study the resilience of this domain of\napplications sharing similar program characteristics. However, it is\nchallenging to achieve application resilience: (a) how to identify the\ninvariants of a given domain of applications, knowing the conservation laws,\nand (b) how to utilize the invariants to efficiently detect and recover from\nfailures in application runs.\n  In this work, we target several continuum dynamics software packages,\nFleCSALE [1] and CODY [2] (with intrinsic invariants during computation), study\ntheir resilience to soft errors online (injected using an open-source fault\ninjector), and investigate the opportunities for non-intrusive and lightweight\nfailure recovery (checksum-based invariant checking). We propose a\nchecksum-retry approach to achieve our goals, and experimental results on a\nvirtualized platform with extensive fault injection campaigns demonstrate the\neffectiveness and efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 22:37:42 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Tan", "Li", ""], ["Charest", "Marc", ""], ["DeBardeleben", "Nathan", ""], ["Guan", "Qiang", ""], ["Bergen", "Ben", ""]]}, {"id": "1911.02118", "submitter": "Li Tan", "authors": "Li Tan, Nathan DeBardeleben", "title": "Failure Analysis and Quantification for Contemporary and Future\n  Supercomputers", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale computing systems today are assembled by numerous computing units\nfor massive computational capability needed to solve problems at scale, which\nenables failures common events in supercomputing scenarios. Considering the\ndemanding resilience requirements of supercomputers today, we present a\nquantitative study on fine-grained failure modeling for contemporary and future\nlarge-scale computing systems. We integrate various types of failures from\ndifferent system hierarchical levels and system components, and summarize the\noverall system failure rates formally. Given that nowadays system-wise failure\nrate needs to be capped under a threshold value for reliability and\ncost-efficiency purposes, we quantitatively discuss different scenarios of\nsystem resilience, and analyze the impacts of resilience to different error\ntypes on the variation of system failure rates, and the correlation of\nhierarchical failure rates. Moreover, we formalize and showcase the resilience\nefficiency of failure-bounded supercomputers today.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 22:45:44 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Tan", "Li", ""], ["DeBardeleben", "Nathan", ""]]}, {"id": "1911.02122", "submitter": "Christina Delimitrou", "authors": "Yanqi Zhang and Yu Gan and Christina Delimitrou", "title": "uqSim: Scalable and Validated Simulation of Cloud Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current cloud services are moving away from monolithic designs and towards\ngraphs of many loosely-coupled, single-concerned microservices. Microservices\nhave several advantages, including speeding up development and deployment,\nallowing specialization of the software infrastructure, and helping with\ndebugging and error isolation. At the same time they introduce several hardware\nand software challenges. Given that most of the performance and efficiency\nimplications of microservices happen at scales larger than what is available\noutside production deployments, studying such effects requires designing the\nright simulation infrastructures.\n  We present uqSim, a scalable and validated queueing network simulator\ndesigned specifically for interactive microservices. uqSim provides detailed\nintra- and inter-microservice models that allow it to faithfully reproduce the\nbehavior of complex, many-tier applications. uqSim is also modular, allowing\nreuse of individual models across microservices and end-to-end applications. We\nhave validated uqSim both against simple and more complex microservices graphs,\nand have shown that it accurately captures performance in terms of throughput\nand tail latency. Finally, we use uqSim to model the tail at scale effects of\nrequest fanout, and the performance impact of power management in\nlatency-sensitive microservices.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 22:49:53 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Zhang", "Yanqi", ""], ["Gan", "Yu", ""], ["Delimitrou", "Christina", ""]]}, {"id": "1911.02134", "submitter": "Yujing Chen", "authors": "Yujing Chen, Yue Ning, Martin Slawski and Huzefa Rangwala", "title": "Asynchronous Online Federated Learning for Edge Devices with Non-IID\n  Data", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a machine learning paradigm where a shared central\nmodel is learned across distributed edge devices while the training data\nremains on these devices. Federated Averaging (FedAvg) is the leading\noptimization method for training non-convex models in this setting with a\nsynchronized protocol. However, the assumptions made by FedAvg are not\nrealistic given the heterogeneity of devices. In particular, the volume and\ndistribution of collected data vary in the training process due to different\nsampling rates of edge devices. The edge devices themselves also vary in their\navailable communication bandwidth and system configurations, such as memory,\nprocessor speed, and power requirements. This leads to vastly different\ntraining times as well as model/data transfer times. Furthermore, availability\nissues at edge devices can lead to a lack of contribution from specific edge\ndevices to the federated model. In this paper, we present an Asynchronous\nOnline Federated Learning (ASO-Fed) framework, where the edge devices perform\nonline learning with continuous streaming local data and a central server\naggregates model parameters from clients. Our framework updates the central\nmodel in an asynchronous manner to tackle the challenges associated with both\nvarying computational loads at heterogeneous edge devices and edge devices that\nlag behind or dropout. We perform extensive experiments on a simulated\nbenchmark image dataset and three real-world non-IID streaming datasets. The\nresults demonstrate the effectiveness of \\model~on converging fast and\nmaintaining good prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 23:24:29 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 19:30:10 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 22:38:31 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chen", "Yujing", ""], ["Ning", "Yue", ""], ["Slawski", "Martin", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "1911.02178", "submitter": "Emily Herbert", "authors": "Emily Herbert and Arjun Guha", "title": "A Language-based Serverless Function Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is an approach to cloud computing that allows\nprogrammers to run serverless functions in response to external events.\nServerless functions are priced at sub-second granularity, support transparent\nelasticity, and relieve programmers from managing the operating system. Thus\nserverless functions allow programmers to focus on writing application code,\nand the cloud provider to manage computing resources globally. Unfortunately,\ntoday's serverless platforms exhibit high latency, because it is difficult to\nmaximize resource utilization while minimizing operating costs.\n  This paper presents serverless function acceleration, which is an approach\nthat transparently lowers the latency and resource utilization of a large class\nof serverless functions. We accomplish this using language-based sandboxing,\nwhereas existing serverless platforms employ more expensive operating system\nsandboxing technologies, such as containers and virtual machines. OS-based\nsandboxing is compatible with more programs than language-based techniques.\nHowever, instead of ruling out any programs, we use language-based sandboxing\nwhen possible, and OS-based sandboxing if necessary. Moreover, we seamlessly\ntransition between language-based and OS-based sandboxing by leveraging the\nfact that serverless functions must tolerate re-execution for fault tolerance.\nTherefore, when a serverless function attempts to perform an unsupported\noperation in the language-based sandbox, we can safely re-execute it in a\ncontainer. We use a new approach to trace compilation to build source-level,\ninterprocedural, execution trace trees for serverless functions written in\nJavaScript. We compile trace trees to a safe subset of Rust, validate the\ncompiler output, and link it to a runtime system. We evaluate these techniques\nin our implementation, which we call Containerless.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 03:20:16 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 19:33:16 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 23:07:16 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 21:44:54 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Herbert", "Emily", ""], ["Guha", "Arjun", ""]]}, {"id": "1911.02254", "submitter": "Chaoyue Niu", "authors": "Chaoyue Niu, Fan Wu, Shaojie Tang, Lifeng Hua, Rongfei Jia, Chengfei\n  Lv, Zhihua Wu, and Guihai Chen", "title": "Secure Federated Submodel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning was proposed with an intriguing vision of achieving\ncollaborative machine learning among numerous clients without uploading their\nprivate data to a cloud server. However, the conventional framework requires\neach client to leverage the full model for learning, which can be prohibitively\ninefficient for resource-constrained clients and large-scale deep learning\ntasks. We thus propose a new framework, called federated submodel learning,\nwhere clients download only the needed parts of the full model, namely\nsubmodels, and then upload the submodel updates. Nevertheless, the \"position\"\nof a client's truly required submodel corresponds to her private data, and its\ndisclosure to the cloud server during interactions inevitably breaks the tenet\nof federated learning. To integrate efficiency and privacy, we have designed a\nsecure federated submodel learning scheme coupled with a private set union\nprotocol as a cornerstone. Our secure scheme features the properties of\nrandomized response, secure aggregation, and Bloom filter, and endows each\nclient with a customized plausible deniability, in terms of local differential\nprivacy, against the position of her desired submodel, thus protecting her\nprivate data. We further instantiated our scheme with the e-commerce\nrecommendation scenario in Alibaba, implemented a prototype system, and\nextensively evaluated its performance over 30-day Taobao user data. The\nanalysis and evaluation results demonstrate the feasibility and scalability of\nour scheme from model accuracy and convergency, practical communication,\ncomputation, and storage overheads, as well as manifest its remarkable\nadvantages over the conventional federated learning framework.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 08:49:23 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 14:07:41 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Niu", "Chaoyue", ""], ["Wu", "Fan", ""], ["Tang", "Shaojie", ""], ["Hua", "Lifeng", ""], ["Jia", "Rongfei", ""], ["Lv", "Chengfei", ""], ["Wu", "Zhihua", ""], ["Chen", "Guihai", ""]]}, {"id": "1911.02275", "submitter": "Yujing Wang", "authors": "Yujing Wang, Darrel Ma", "title": "Developing a Process in Architecting Microservice Infrastructure with\n  Docker, Kubernetes, and Istio", "comments": "To be submitted to IEEE (CLOUD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an application usage grows, its owner scales up vertically by replacing\nold machines with more powerful ones. This methodology is expensive and leads\nto resource waste. In response to the business needs, internet giants have\ndeveloped the microservice architecture, which lets developers divide up their\napplication into smaller units that can be hosted on multiple machines, thus\nenabling horizontal scale up. We propose a triphasic incremental process to\ntransform a traditional application into a microservice application that\nguarantees stability during the operation. Then we demonstrated such\nmethodology in a prototype microservice application based on an existing\nmonolithic application. First, the developer splits a monolithic application\ninto atomic services and aggregated services. Second, these services are\npackaged, containerized, and then deployed on Kubernetes. During this stage,\nIstio is deployed on the Kubernetes cluster to establish pod level\ncommunications, delegate traffic flows and filter requests, and enable the\nautoscaler. Other external add-ons, such as database connections, are defined\nin service entry. In the last stage, we developed an algorithm guideline to\nminimize inter-service calls by compiling all needed calls into a list and\nperform one finalized call. Although it increases memory usage, it avoided the\nwait time incurred during interservice calls. We then investigated managing\nconfigurations using config maps, recommended a pipeline being developed to\nperform automatic rollover.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:49:11 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Wang", "Yujing", ""], ["Ma", "Darrel", ""]]}, {"id": "1911.02373", "submitter": "Davood Mohajerani", "authors": "Alexander Brandt, Davood Mohajerani, Marc Moreno Maza, Jeeva Paudel,\n  Linxiao Wang", "title": "KLARAPTOR: A Tool for Dynamically Finding Optimal Kernel Launch\n  Parameters Targeting CUDA Programs", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1906.00142", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present KLARAPTOR (Kernel LAunch parameters RAtional Program\nestimaTOR), a new tool built on top of the LLVM Pass Framework and NVIDIA CUPTI\nAPI to dynamically determine the optimal values of kernel launch parameters of\na CUDA program P. To be precise, we describe a novel technique to statically\nbuild (at the compile time of P) a so-called rational program R. Using a\nperformance prediction model, and knowing particular data and hardware\nparameters of P at runtime, the program R can automatically and dynamically\ndetermine the values of launch parameters of P that will yield optimal\nperformance. Our technique can be applied to parallel programs in general, as\nwell as to generic performance prediction models which account for program and\nhardware parameters. We are particularly interested in programs targeting\nmanycore accelerators. We have implemented and successfully tested our\ntechnique in the context of GPU kernels written in CUDA using the MWP-CWP\nperformance prediction model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 00:24:56 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Brandt", "Alexander", ""], ["Mohajerani", "Davood", ""], ["Maza", "Marc Moreno", ""], ["Paudel", "Jeeva", ""], ["Wang", "Linxiao", ""]]}, {"id": "1911.02392", "submitter": "Zhifeng Jia", "authors": "Zhifeng Jia, Rui Chen and Jie Li", "title": "DeLottery: A Novel Decentralized Lottery System Based on Blockchain\n  Technology", "comments": "This paper contains 6 pages with 2 figures", "journal-ref": null, "doi": null, "report-no": "Report-no: C2007", "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design DeLottery, a decentralized lottery system based on\nblock chain technology and smart contracts. Lottery is a classical form of\nentertainment and charity for centuries. Facing the bottleneck of the\ncombination between lottery and information technology, we use smart contracts\nand blockchain in decentralized, intelligent, and secure systems for lottery\nindustries. Moreover, we are inspired by the algorithm of RANDAO, an\noutstanding way of random number generation in blockchain scenario. The\ncomponents and the functions of the novel system are described in details. We\nimplement DeLottery in a blockchain network and show functioning procedure and\nsecurity of the proposed lottery system.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 13:57:03 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Jia", "Zhifeng", ""], ["Chen", "Rui", ""], ["Li", "Jie", ""]]}, {"id": "1911.02516", "submitter": "Alessandro Rigazzi", "authors": "Alessandro Rigazzi", "title": "DC-S3GD: Delay-Compensated Stale-Synchronous SGD for Large-Scale\n  Decentralized Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism has become the de facto standard for training Deep Neural\nNetwork on multiple processing units. In this work we propose DC-S3GD, a\ndecentralized (without Parameter Server) stale-synchronous version of the\nDelay-Compensated Asynchronous Stochastic Gradient Descent (DC-ASGD) algorithm.\nIn our approach, we allow for the overlap of computation and communication, and\ncompensate the inherent error with a first-order correction of the gradients.\nWe prove the effectiveness of our approach by training Convolutional Neural\nNetwork with large batches and achieving state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 17:54:56 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Rigazzi", "Alessandro", ""]]}, {"id": "1911.02628", "submitter": "Nguyen Pham", "authors": "Soumyottam Chatterjee, Gopal Pandurangan, Nguyen Dinh Pham", "title": "Distributed MST: A Smoothed Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study smoothed analysis of distributed graph algorithms, focusing on the\nfundamental minimum spanning tree (MST) problem. With the goal of studying the\ntime complexity of distributed MST as a function of the \"perturbation\" of the\ninput graph, we posit a {\\em smoothing model} that is parameterized by a\nsmoothing parameter $0 \\leq \\epsilon(n) \\leq 1$ which controls the amount of\n{\\em random} edges that can be added to an input graph $G$ per round.\nInformally, $\\epsilon(n)$ is the probability (typically a small function of\n$n$, e.g., $n^{-\\frac{1}{4}}$) that a random edge can be added to a node per\nround. The added random edges, once they are added, can be used (only) for\ncommunication.\n  We show upper and lower bounds on the time complexity of distributed MST in\nthe above smoothing model. We present a distributed algorithm that, with high\nprobability,\\footnote{Throughout, with high probability (whp) means with\nprobability at least $1 - n^{-c}$, for some fixed, positive constant $c$.}\ncomputes an MST and runs in $\\tilde{O}(\\min\\{\\frac{1}{\\sqrt{\\epsilon(n)}}\n2^{O(\\sqrt{\\log n})}, D + \\sqrt{n}\\})$ rounds\\footnote{The notation $\\tilde{O}$\nhides a $\\polylog(n)$ factor and $\\tilde{\\Omega}$ hides a\n$\\frac{1}{\\polylog{(n)}}$ factor, where $n$ is the number of nodes of the\ngraph.} where $\\epsilon$ is the smoothing parameter, $D$ is the network\ndiameter and $n$ is the network size. To complement our upper bound, we also\nshow a lower bound of $\\tilde{\\Omega}(\\min\\{\\frac{1}{\\sqrt{\\epsilon(n)}},\nD+\\sqrt{n}\\})$. We note that the upper and lower bounds essentially match\nexcept for a multiplicative $2^{O(\\sqrt{\\log n})} \\polylog(n)$ factor.\n  Our work can be considered as a first step in understanding the smoothed\ncomplexity of distributed graph algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 21:05:52 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chatterjee", "Soumyottam", ""], ["Pandurangan", "Gopal", ""], ["Pham", "Nguyen Dinh", ""]]}, {"id": "1911.02794", "submitter": "Guoming Tang", "authors": "Fang Liu, Guoming Tang, Youhuizi Li, Zhiping Cai, Xingzhou Zhang,\n  Tongqing Zhou", "title": "A Survey on Edge Computing Systems and Tools", "comments": "24 pages, 21 figures, 4 tables, 87 references", "journal-ref": "Proceedings of the IEEE, 2019", "doi": "10.1109/JPROC.2019.2920341", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the visions of Internet of Things and 5G communications, the edge\ncomputing systems integrate computing, storage and network resources at the\nedge of the network to provide computing infrastructure, enabling developers to\nquickly develop and deploy edge applications. Nowadays the edge computing\nsystems have received widespread attention in both industry and academia. To\nexplore new research opportunities and assist users in selecting suitable edge\ncomputing systems for specific applications, this survey paper provides a\ncomprehensive overview of the existing edge computing systems and introduces\nrepresentative projects. A comparison of open source tools is presented\naccording to their applicability. Finally, we highlight energy efficiency and\ndeep learning optimization of edge computing systems. Open issues for analyzing\nand designing an edge computing system are also studied in this survey.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 08:16:40 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Liu", "Fang", ""], ["Tang", "Guoming", ""], ["Li", "Youhuizi", ""], ["Cai", "Zhiping", ""], ["Zhang", "Xingzhou", ""], ["Zhou", "Tongqing", ""]]}, {"id": "1911.02910", "submitter": "Valderi Leithardt", "authors": "Valderi R. Q. Leithardt, Douglas A. dos Santos, Luis A. Silva, Felipe\n  Viel, Cesar A. Zeferino and Jorge Sa Silva", "title": "A Solution for Controlling and Managing User Profiles based on Data\n  Privacy for IoT Applications", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT is an emerging area in which we expect to have billions of devices\nconnected to the Internet by 2020. IoT applications can offer many benefits to\nenvironments, society and the economy through the interconnection and\ncooperation of smart objects. However, there are many privacy challenges, such\nas authentication, authorization, and confidentiality of personal data. With\nthis in mind, we have developed a solution for managing user profiles based on\nprivacy and evolution. For that, we define the criteria and characteristics for\neach environment. The contribution of the work also applies to the evolution of\nthe UbiPri Middleware, with a module implemented and tested according to the\nrules of environments so that they can modify the user's profile over time\naccording to their frequency in the environment. The modified profile can be\nraised, reduced and blocked. This implementation has been validated using\nscripts that perform probabilistic simulation and user authentication. From the\nrules assigned to the environments, it was possible to perceive the high\nadaptability of the implementation, and it can be easily adjusted to any IoT\nenvironment that wants to treat the authentication and privacy of environments\nand users. The results obtained were satisfactory and could be useful for both\nthe evolution of the UbiPri Middleware and other related works.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 10:35:09 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Leithardt", "Valderi R. Q.", ""], ["Santos", "Douglas A. dos", ""], ["Silva", "Luis A.", ""], ["Viel", "Felipe", ""], ["Zeferino", "Cesar A.", ""], ["Silva", "Jorge Sa", ""]]}, {"id": "1911.03001", "submitter": "Wei Ding", "authors": "Haifeng Liu, Wei Ding, Yuan Chen, Weilong Guo, Shuoran Liu, Tianpeng\n  Li, Mofei Zhang, Jianxing Zhao, Hongyin Zhu, Zhengyi Zhu", "title": "CFS: A Distributed File System for Large Scale Container Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose CFS, a distributed file system for large scale container\nplatforms. CFS supports both sequential and random file accesses with optimized\nstorage for both large files and small files, and adopts different replication\nprotocols for different write scenarios to improve the replication performance.\nIt employs a metadata subsystem to store and distribute the file metadata\nacross different storage nodes based on the memory usage. This metadata\nplacement strategy avoids the need of data rebalancing during capacity\nexpansion. CFS also provides POSIX-compliant APIs with relaxed semantics and\nmetadata atomicity to improve the system performance.\n  We performed a comprehensive comparison with Ceph, a widely-used distributed\nfile system on container platforms. Our experimental results show that, in\ntesting 7 commonly used metadata operations, CFS gives around 3 times\nperformance boost on average. In addition, CFS exhibits better\nrandom-read/write performance in highly concurrent environments with multiple\nclients and processes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 02:43:07 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Liu", "Haifeng", ""], ["Ding", "Wei", ""], ["Chen", "Yuan", ""], ["Guo", "Weilong", ""], ["Liu", "Shuoran", ""], ["Li", "Tianpeng", ""], ["Zhang", "Mofei", ""], ["Zhao", "Jianxing", ""], ["Zhu", "Hongyin", ""], ["Zhu", "Zhengyi", ""]]}, {"id": "1911.03028", "submitter": "Robert Kelly", "authors": "Robert Kelly, Barak A. Pearlmutter, Phil Maguire", "title": "Lock-Free Hopscotch Hashing", "comments": "15 pages, to appear in APOCS20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a lock-free version of Hopscotch Hashing. Hopscotch\nHashing is an open addressing algorithm originally proposed by Herlihy, Shavit,\nand Tzafrir, which is known for fast performance and excellent cache locality.\nThe algorithm allows users of the table to skip or jump over irrelevant\nentries, allowing quick search, insertion, and removal of entries. Unlike\ntraditional linear probing, Hopscotch Hashing is capable of operating under a\nhigh load factor, as probe counts remain small. Our lock-free version improves\non both speed, cache locality, and progress guarantees of the original, being a\nchimera of two concurrent hash tables. We compare our data structure to various\nother lock-free and blocking hashing algorithms and show that its performance\nis in many cases superior to existing strategies. The proposed lock-free\nversion overcomes some of the drawbacks associated with the original blocking\nversion, leading to a substantial boost in scalability while maintaining\nattractive features like physical deletion or probe-chain compression.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:55:54 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kelly", "Robert", ""], ["Pearlmutter", "Barak A.", ""], ["Maguire", "Phil", ""]]}, {"id": "1911.03051", "submitter": "Olasupo Ajayi", "authors": "Kun Ma, Antoine Bagula and Olasupo Ajayi", "title": "Quality of Service (QoS) Modelling in Federated Cloud Computing", "comments": "21 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building around the idea of a large scale server infrastructure with a\npotentially large number of tailored resources, which are capable of\ninteracting to facilitate the deployment, adaptation, and support of services,\ncloud computing needs to frequently reschedule and manage various application\ntasks in order to accommodate the requests of a wide range and number of users.\nOne of the challenges of cloud computing is to support and manage\nQuality-of-Service (QoS) by designing efficient techniques for the allocation\nof tasks between users and the cloud virtual resources, as well as assigning\nvirtual resources to the cloud physical resources. The migration of virtual\nresources across physical resources is another challenge that requires\nconsiderable attention; especially in federated cloud computing environments\nwherein, providers might be willing to offer their unused resources as a\nservice to the federation (cooperative allocation) and pull back these\nresources for their own use when they are needed (competitive allocation). This\npaper revisits the issue of QoS in cloud computing by formulating and\npresenting i) a multi-QoS task allocation model for the assignment of tasks to\nvirtual machines and ii) a virtual machine migration model for a federated\ncloud computing environment by considering cases where resource providers are\noperating in cooperative or competitive mode. A new differential evolution (DE)\nbased binding policy for task allocation and a novel virtual machine model are\nproposed as solutions for the problem of QoS support in federated cloud\nenvironments. The experimental results show that the proposed solutions\nimproved the quality of service in the cloud computing environment and reveal\nthe relative advantages of operating a mixed cooperation and competition model\nin a federated cloud environment.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 05:07:14 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Ma", "Kun", ""], ["Bagula", "Antoine", ""], ["Ajayi", "Olasupo", ""]]}, {"id": "1911.03062", "submitter": "Christos Kotsalos", "authors": "Christos Kotsalos, Jonas Latt, Joel Beny and Bastien Chopard", "title": "Digital Blood in Massively Parallel CPU/GPU Systems for the Study of\n  Platelet Transport", "comments": null, "journal-ref": "Royal Society - Interface Focus (2020)", "doi": "10.1098/rsfs.2019.0116", "report-no": null, "categories": "physics.comp-ph cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a highly versatile computational framework for the simulation of\ncellular blood flow focusing on extreme performance without compromising\naccuracy or complexity. The tool couples the lattice Boltzmann solver Palabos\nfor the simulation of the blood plasma, a novel finite element method (FEM)\nsolver for the resolution of the deformable blood cells, and an immersed\nboundary method for the coupling of the two phases. The design of the tool\nsupports hybrid CPU-GPU executions (fluid, fluid-solid interaction on CPUs, the\nFEM solver on GPUs), and is non-intrusive, as each of the three components can\nbe replaced in a modular way. The FEM-based kernel for solid dynamics\noutperforms other FEM solvers and its performance is comparable to the\nstate-of-the-art mass-spring systems. We perform an exhaustive performance\nanalysis on Piz Daint at the Swiss National Supercomputing Centre and provide\ncase studies focused on platelet transport. The tests show that this versatile\nframework combines unprecedented accuracy with massive performance, rendering\nit suitable for the upcoming exascale architectures.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 05:52:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Kotsalos", "Christos", ""], ["Latt", "Jonas", ""], ["Beny", "Joel", ""], ["Chopard", "Bastien", ""]]}, {"id": "1911.03183", "submitter": "Erik-Jan van Kesteren", "authors": "Erik-Jan van Kesteren, Chang Sun, Daniel L. Oberski, Michel Dumontier,\n  Lianne Ippel", "title": "Privacy-Preserving Generalized Linear Models using Distributed Block\n  Coordinate Descent", "comments": "Fully reproducible code for all results and images can be found at\n  https://github.com/vankesteren/privacy-preserving-glm, and the software\n  package can be found at https://github.com/vankesteren/privreg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Combining data from varied sources has considerable potential for knowledge\ndiscovery: collaborating data parties can mine data in an expanded feature\nspace, allowing them to explore a larger range of scientific questions.\nHowever, data sharing among different parties is highly restricted by legal\nconditions, ethical concerns, and / or data volume. Fueled by these concerns,\nthe fields of cryptography and distributed learning have made great progress\ntowards privacy-preserving and distributed data mining. However, practical\nimplementations have been hampered by the limited scope or computational\ncomplexity of these methods. In this paper, we greatly extend the range of\nanalyses available for vertically partitioned data, i.e., data collected by\nseparate parties with different features on the same subjects. To this end, we\npresent a novel approach for privacy-preserving generalized linear models, a\nfundamental and powerful framework underlying many prediction and\nclassification procedures. We base our method on a distributed block coordinate\ndescent algorithm to obtain parameter estimates, and we develop an extension to\ncompute accurate standard errors without additional communication cost. We\ncritically evaluate the information transfer for semi-honest collaborators and\nshow that our protocol is secure against data reconstruction. Through both\nsimulated and real-world examples we illustrate the functionality of our\nproposed algorithm. Without leaking information, our method performs as well on\nvertically partitioned data as existing methods on combined data -- all within\nmere minutes of computation time. We conclude that our method is a viable\napproach for vertically partitioned data analysis with a wide range of\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 11:07:07 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["van Kesteren", "Erik-Jan", ""], ["Sun", "Chang", ""], ["Oberski", "Daniel L.", ""], ["Dumontier", "Michel", ""], ["Ippel", "Lianne", ""]]}, {"id": "1911.03291", "submitter": "Loick Bonniot", "authors": "Lo\\\"ick Bonniot (WIDE), Christoph Neumann, Fran\\c{c}ois Ta\\\"iani\n  (WIDE)", "title": "PnyxDB: a Lightweight Leaderless Democratic Byzantine Fault Tolerant\n  Replicated Datastore", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine-Fault-Tolerant (BFT) systems are rapidly emerging as a viable\ntechnology for production-grade systems, notably in closed consortia\ndeployments for nancial and supply-chain applications. Unfortunately, most\nalgorithms proposed so far to coordinate these systems suffer from substantial\nscalability issues, and lack important features to implement Internet-scale\ngovernance mechanisms. In this paper, we observe that many application\nworkloads offer little concurrency, and propose PnyxDB, an\neventually-consistent Byzantine Fault Tolerant replicated datastore that\nexhibits both high scalability and low latency. Our approach is based on\nconditional endorsements, that allow nodes to specify the set of transactions\nthat must not be committed for the endorsement to be valid. In addition to its\nhigh scalability, PnyxDB supports application-level voting, i.e. individual\nnodes are able to endorse or reject a transaction according to\napplication-defined policies without compromising consistency. We provide a\ncomparison against BFTSMaRt and Tendermint, two competitors with different\ndesign aims, and show that our implementation speeds up commit latencies by a\nfactor of 11, remaining below 5 seconds in a worldwide geodistributed\ndeployment of 180 nodes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:43:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Bonniot", "Lo\u00efck", "", "WIDE"], ["Neumann", "Christoph", "", "WIDE"], ["Ta\u00efani", "Fran\u00e7ois", "", "WIDE"]]}, {"id": "1911.03298", "submitter": "Muhammad Baqer Mollah Mr.", "authors": "Muhammad Baqer Mollah, Jun Zhao, Dusit Niyato, Kwok-Yan Lam, Xin\n  Zhang, Amer M.Y.M. Ghias, Leong Hai Koh, Lei Yang", "title": "Blockchain for Future Smart Grid: A Comprehensive Survey", "comments": "26 pages, 13 figures, 5 tables", "journal-ref": "IEEE Internet of Things Journal, 2020", "doi": "10.1109/JIOT.2020.2993601", "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.SI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of smart grid has been introduced as a new vision of the\nconventional power grid to figure out an efficient way of integrating green and\nrenewable energy technologies. In this way, Internet-connected smart grid, also\ncalled energy Internet, is also emerging as an innovative approach to ensure\nthe energy from anywhere at any time. The ultimate goal of these developments\nis to build a sustainable society. However, integrating and coordinating a\nlarge number of growing connections can be a challenging issue for the\ntraditional centralized grid system. Consequently, the smart grid is undergoing\na transformation to the decentralized topology from its centralized form. On\nthe other hand, blockchain has some excellent features which make it a\npromising application for smart grid paradigm. In this paper, we aim to provide\na comprehensive survey on application of blockchain in smart grid. As such, we\nidentify the significant security challenges of smart grid scenarios that can\nbe addressed by blockchain. Then, we present a number of blockchain-based\nrecent research works presented in different literatures addressing security\nissues in the area of smart grid. We also summarize several related practical\nprojects, trials, and products that have been emerged recently. Finally, we\ndiscuss essential research challenges and future directions of applying\nblockchain to smart grid security issues.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:51:39 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 07:42:41 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Mollah", "Muhammad Baqer", ""], ["Zhao", "Jun", ""], ["Niyato", "Dusit", ""], ["Lam", "Kwok-Yan", ""], ["Zhang", "Xin", ""], ["Ghias", "Amer M. Y. M.", ""], ["Koh", "Leong Hai", ""], ["Yang", "Lei", ""]]}, {"id": "1911.03444", "submitter": "Karl B\\\"ackstr\\\"om", "authors": "Karl B\\\"ackstr\\\"om, Marina Papatriantafilou, Philippas Tsigas", "title": "MindTheStep-AsyncPSGD: Adaptive Asynchronous Parallel Stochastic\n  Gradient Descent", "comments": "12 pages, 3 figures, accepted in IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is very useful in optimization problems\nwith high-dimensional non-convex target functions, and hence constitutes an\nimportant component of several Machine Learning and Data Analytics methods.\nRecently there have been significant works on understanding the parallelism\ninherent to SGD, and its convergence properties. Asynchronous, parallel SGD\n(AsyncPSGD) has received particular attention, due to observed performance\nbenefits. On the other hand, asynchrony implies inherent challenges in\nunderstanding the execution of the algorithm and its convergence, stemming from\nthe fact that the contribution of a thread might be based on an old (stale)\nview of the state. In this work we aim to deepen the understanding of AsyncPSGD\nin order to increase the statistical efficiency in the presence of stale\ngradients. We propose new models for capturing the nature of the staleness\ndistribution in a practical setting. Using the proposed models, we derive a\nstaleness-adaptive SGD framework, MindTheStep-AsyncPSGD, for adapting the step\nsize in an online-fashion, which provably reduces the negative impact of\nasynchrony. Moreover, we provide general convergence time bounds for a wide\nclass of staleness-adaptive step size strategies for convex target functions.\nWe also provide a detailed empirical study, showing how our approach implies\nfaster convergence for deep learning applications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 18:53:10 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["B\u00e4ckstr\u00f6m", "Karl", ""], ["Papatriantafilou", "Marina", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1911.03451", "submitter": "Xingyao Zhang", "authors": "Xingyao Zhang, Shuaiwen Leon Song, Chenhao Xie, Jing Wang, Weigong\n  Zhang and Xin Fu", "title": "Enabling Highly Efficient Capsule Networks Processing Through A\n  PIM-Based Architecture Design", "comments": "To appear in the 2020 26th International Symposium on\n  High-Performance Computer Architecture (HPCA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the CNNs have achieved great successes in the image\nprocessing tasks, e.g., image recognition and object detection. Unfortunately,\ntraditional CNN's classification is found to be easily misled by increasingly\ncomplex image features due to the usage of pooling operations, hence unable to\npreserve accurate position and pose information of the objects. To address this\nchallenge, a novel neural network structure called Capsule Network has been\nproposed, which introduces equivariance through capsules to significantly\nenhance the learning ability for image segmentation and object detection. Due\nto its requirement of performing a high volume of matrix operations, CapsNets\nhave been generally accelerated on modern GPU platforms that provide highly\noptimized software library for common deep learning tasks. However, based on\nour performance characterization on modern GPUs, CapsNets exhibit low\nefficiency due to the special program and execution features of their routing\nprocedure, including massive unshareable intermediate variables and intensive\nsynchronizations, which are very difficult to optimize at software level. To\naddress these challenges, we propose a hybrid computing architecture design\nnamed \\textit{PIM-CapsNet}. It preserves GPU's on-chip computing capability for\naccelerating CNN types of layers in CapsNet, while pipelining with an off-chip\nin-memory acceleration solution that effectively tackles routing procedure's\ninefficiency by leveraging the processing-in-memory capability of today's 3D\nstacked memory. Using routing procedure's inherent parallellization feature,\nour design enables hierarchical improvements on CapsNet inference efficiency\nthrough minimizing data movement and maximizing parallel processing in memory.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 06:03:46 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zhang", "Xingyao", ""], ["Song", "Shuaiwen Leon", ""], ["Xie", "Chenhao", ""], ["Wang", "Jing", ""], ["Zhang", "Weigong", ""], ["Fu", "Xin", ""]]}, {"id": "1911.03456", "submitter": "Gabriele D'Angelo", "authors": "Moreno Marzolla, Gabriele D'Angelo", "title": "Parallel Data Distribution Management on Shared-Memory Multiprocessors", "comments": "arXiv admin note: text overlap with arXiv:1703.06680", "journal-ref": "ACM Transactions on Modeling and Computer Simulation (TOMACS),\n  Vol. 30, No. 1, Article 5. ACM, February 2020. ISSN: 1049-3301", "doi": "10.1145/3369759", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying intersections between two sets of d-dimensional\naxis-parallel rectangles appears frequently in the context of agent-based\nsimulation studies. For this reason, the High Level Architecture (HLA)\nspecification -- a standard framework for interoperability among simulators --\nincludes a Data Distribution Management (DDM) service whose responsibility is\nto report all intersections between a set of subscription and update regions.\nThe algorithms at the core of the DDM service are CPU-intensive, and could\ngreatly benefit from the large computing power of modern multi-core processors.\nIn this paper we propose two parallel solutions to the DDM problem that can\noperate effectively on shared-memory multiprocessors. The first solution is\nbased on a data structure (the Interval Tree) that allows concurrent\ncomputation of intersections between subscription and update regions. The\nsecond solution is based on a novel parallel extension of the Sort Based\nMatching algorithm, whose sequential version is considered among the most\nefficient solutions to the DDM problem. Extensive experimental evaluation of\nthe proposed algorithms confirm their effectiveness on taking advantage of\nmultiple execution units in a shared-memory architecture.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 13:18:14 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 10:24:12 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Marzolla", "Moreno", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "1911.03458", "submitter": "Yu-Sheng Lin", "authors": "Yu-Sheng Lin, Wei-Chao Chen, Shao-Yi Chien", "title": "MERIT: Tensor Transform for Memory-Efficient Vision Processing on\n  Parallel Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally intensive deep neural networks (DNNs) are well-suited to run\non GPUs, but newly developed algorithms usually require the heavily optimized\nDNN routines to work efficiently, and this problem could be even more difficult\nfor specialized DNN architectures. In this paper, we propose a mathematical\nformulation which can be useful for transferring the algorithm optimization\nknowledge across computing platforms. We discover that data movement and\nstorage inside parallel processor architectures can be viewed as tensor\ntransforms across memory hierarchies, making it possible to describe many\nmemory optimization techniques mathematically. Such transform, which we call\nMemory Efficient Ranged Inner-Product Tensor (MERIT) transform, can be applied\nto not only DNN tasks but also many traditional machine learning and computer\nvision computations. Moreover, the tensor transforms can be readily mapped to\nexisting vector processor architectures. In this paper, we demonstrate that\nmany popular applications can be converted to a succinct MERIT notation on\nGPUs, speeding up GPU kernels up to 20 times while using only half as many code\ntokens. We also use the principle of the proposed transform to design a\nspecialized hardware unit called MERIT-z processor. This processor can be\napplied to a variety of DNN tasks as well as other computer vision tasks while\nproviding comparable area and power efficiency to dedicated DNN ASICs.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 15:03:43 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Lin", "Yu-Sheng", ""], ["Chen", "Wei-Chao", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "1911.03527", "submitter": "Yehia Elkhatib PhD", "authors": "Maria Salama, Yehia Elkhatib, Gordon S. Blair", "title": "IoTNetSim: A Modelling and Simulation Platform for End-to-End IoT\n  Services and Networking", "comments": null, "journal-ref": "UCC 2019", "doi": "10.1145/3344341.3368820", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-of-Things (IoT) systems are becoming increasingly complex,\nheterogeneous and pervasive, integrating a variety of physical devices and\nvirtual services that are spread across architecture layers (cloud, fog, edge)\nusing different connection types. As such, research and design of such systems\nhave proven to be challenging. Despite the influx in IoT research and the\nsignificant benefits of simulation-based approaches in supporting research,\nthere is a general lack of appropriate modelling and simulation platforms to\ncreate a detailed representation of end-to-end IoT services, i.e. from the\nunderlying IoT nodes to the application layer in the cloud along with the\nunderlying networking infrastructure. To aid researchers and practitioners in\novercoming these challenges, we propose IoTNetSim, a novel self-contained\nextendable platform for modelling and simulation of end-to-end IoT services.\nThe platform supports modelling heterogeneous IoT nodes (sensors, actuators,\ngateways, etc.) with their fine-grained details (mobility, energy profile,\netc.), as well as different models of application logic and network\nconnectivity. The proposed work is distinct from the current literature, being\nan all-in-one tool for end-to-end IoT services with a multi-layered\narchitecture that allows modelling IoT systems with different structures. We\nexperimentally validate and evaluate our IoTNetSim implementation using two\nvery large-scale real-world cases from the natural environment and disaster\nmonitoring IoT domains.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 21:03:44 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Salama", "Maria", ""], ["Elkhatib", "Yehia", ""], ["Blair", "Gordon S.", ""]]}, {"id": "1911.03563", "submitter": "Khaza Anuarul Hoque", "authors": "Samaikya Valluripally, Aniket Gulhane, Reshmi Mitra, Khaza Anuarul\n  Hoque, Prasad Calyam", "title": "Attack Trees for Security and Privacy in Social Virtual Reality Learning\n  Environments", "comments": "Accepted for publication in in the IEEE Consumer Communications &\n  Networking Conference (CCNC 2020)", "journal-ref": null, "doi": "10.1109/CCNC46108.2020.9045724", "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Virtual Reality Learning Environment (VRLE) is a novel edge computing\nplatform for collaboration amongst distributed users. Given that VRLEs are used\nfor critical applications (e.g., special education, public safety training), it\nis important to ensure security and privacy issues. In this paper, we present a\nnovel framework to obtain quantitative assessments of threats and\nvulnerabilities for VRLEs. Based on the use cases from an actual social VRLE\nviz., vSocial, we first model the security and privacy using the attack trees.\nSubsequently, these attack trees are converted into stochastic timed automata\nrepresentations that allow for rigorous statistical model checking. Such an\nanalysis helps us adopt pertinent design principles such as hardening,\ndiversity and principle of least privilege to enhance the resilience of social\nVRLEs. Through experiments in a vSocial case study, we demonstrate the\neffectiveness of our attack tree modeling with a reduction of 26% in\nprobability of loss of integrity (security) and 80% in privacy leakage\n(privacy) in before and after scenarios pertaining to the adoption of the\ndesign principles.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 22:25:21 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Valluripally", "Samaikya", ""], ["Gulhane", "Aniket", ""], ["Mitra", "Reshmi", ""], ["Hoque", "Khaza Anuarul", ""], ["Calyam", "Prasad", ""]]}, {"id": "1911.03600", "submitter": "Hailiang Zhao", "authors": "Hailiang Zhao, Shuiguang Deng, Zijie Liu, Jianwei Yin, Schahram\n  Dustdar", "title": "Distributed Redundant Placement for Microservice-based Applications at\n  the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-access Edge Computing (MEC) is booming as a promising paradigm to push\nthe computation and communication resources from cloud to the network edge to\nprovide services and to perform computations. With container technologies,\nmobile devices with small memory footprint can run composite microservice-based\napplications without time-consuming backbone. Service placement at the edge is\nof importance to put MEC from theory into practice. However, current\nstate-of-the-art research does not sufficiently take the composite property of\nservices into consideration. Besides, although Kubernetes has certain abilities\nto heal container failures, high availability cannot be ensured due to\nheterogeneity and variability of edge sites. To deal with these problems, we\npropose a distributed redundant placement framework SAA-RP and a GA-based\nServer Selection (GASS) algorithm for microservice-based applications with\nsequential combinatorial structure. We formulate a stochastic optimization\nproblem with the uncertainty of microservice request considered, and then\ndecide for each microservice, how it should be deployed and with how many\ninstances as well as on which edge sites to place them. Benchmark policies are\nimplemented in two scenarios, where redundancy is allowed and not,\nrespectively. Numerical results based on a real-world dataset verify that GASS\nsignificantly outperforms all the benchmark policies.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 03:06:48 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 11:51:47 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zhao", "Hailiang", ""], ["Deng", "Shuiguang", ""], ["Liu", "Zijie", ""], ["Yin", "Jianwei", ""], ["Dustdar", "Schahram", ""]]}, {"id": "1911.03654", "submitter": "Anis Elgabli", "authors": "Anis Elgabli, Jihong Park, Sabbir Ahmed, and Mehdi Bennis", "title": "L-FGADMM: Layer-Wise Federated Group ADMM for Communication Efficient\n  Decentralized Deep Learning", "comments": "6 pages; 4 figures; presented at IEEE WCNC'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a communication-efficient decentralized deep learning\nalgorithm, coined layer-wise federated group ADMM (L-FGADMM). To minimize an\nempirical risk, every worker in L-FGADMM periodically communicates with two\nneighbors, in which the periods are separately adjusted for different layers of\nits deep neural network. A constrained optimization problem for this setting is\nformulated and solved using the stochastic version of GADMM proposed in our\nprior work. Numerical evaluations show that by less frequently exchanging the\nlargest layer, L-FGADMM can significantly reduce the communication cost,\nwithout compromising the convergence speed. Surprisingly, despite less\nexchanged information and decentralized operations, intermittently skipping the\nlargest layer consensus in L-FGADMM creates a regularizing effect, thereby\nachieving the test accuracy as high as federated learning (FL), a baseline\nmethod with the entire layer consensus by the aid of a central entity.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 10:03:21 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 09:33:25 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Elgabli", "Anis", ""], ["Park", "Jihong", ""], ["Ahmed", "Sabbir", ""], ["Bennis", "Mehdi", ""]]}, {"id": "1911.03709", "submitter": "Saad Wazir", "authors": "Saad Wazir, Ataul Aziz Ikram, Hamza Ali Imran, Hanif Ullah, Ahmed\n  Jamal Ikram, Maryam Ehsan", "title": "Performance Comparison of MPICH and MPI4py on Raspberry Pi-3B Beowulf\n  Cluster", "comments": null, "journal-ref": "March 2019, Journal of Advanced Research in Dynamical and Control\n  Systems 11(03-Special Issue):1766-1774", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Moore's Law is running out. Instead of making powerful computer by increasing\nnumber of transistor now we are moving toward Parallelism. Beowulf cluster\nmeans cluster of any Commodity hardware. Our Cluster works exactly similar to\ncurrent day's supercomputers. The motivation is to create a small sized, cheap\ndevice on which students and researchers can get hands on experience. There is\na master node, which interacts with user and all other nodes are slave nodes.\nLoad is equally divided among all nodes and they send their results to master.\nMaster combines those results and show the final output to the user. For\ncommunication between nodes we have created a network over Ethernet. We are\nusing MPI4py, which a Python based implantation of Message Passing Interface\n(MPI) and MPICH which also an open source implementation of MPI and allows us\nto code in C, C++ and Fortran. MPI is a standard for writing codes for such\nclusters. We have written parallel programs of Monte Carlo's Simulation for\nfinding value of pi and prime number generator in Python and C++ making use of\nMPI4py and MPICH respectively. We have also written sequential programs\napplying same algorithms in Python. Then we compared the time it takes to run\nthem on cluster in parallel and in sequential on a computer having 6500 core i7\nIntel processor. It is found that making use of parallelism; we were able to\noutperform an expensive computer which costs much more than our cluster.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 15:07:12 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Wazir", "Saad", ""], ["Ikram", "Ataul Aziz", ""], ["Imran", "Hamza Ali", ""], ["Ullah", "Hanif", ""], ["Ikram", "Ahmed Jamal", ""], ["Ehsan", "Maryam", ""]]}, {"id": "1911.03991", "submitter": "Asma Balamane", "authors": "Asma Balamane and Zina Taklit", "title": "Using Deep Neural Networks for Estimating Loop Unrolling Factor", "comments": "Key words: Loop Unrolling, Deep Neural Network, Feature Extraction,\n  TIRAMISU compiler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing programs requires deep expertise. On one hand, it is a tedious\ntask, because it requires a lot of tests to find out the best combination of\noptimizations to apply with their best factors. On the other hand, this task is\ncritical, because it may degrade the performance of programs instead of\nimproving it. The automatization of this task can deal with this problem and\npermit to obtain good results. Optimizing loops that take the most significant\npart of the program execution time plays a crucial role to achieve best\nperformance. In this paper, we address Loop unrolling optimization, by\nproposing a deep Neural Network model to predict the optimal unrolling factor\nfor programs written for TIRAMISU. TIRAMISU is a polyhedral framework designed\nto generate high performance code for multiple platforms including multicores,\nGPUs, and distributed machines. TIRAMISU introduces a scheduling language with\nnovel commands to explicitly manage the complexities that arise when targeting\nthese systems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 20:19:33 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Balamane", "Asma", ""], ["Taklit", "Zina", ""]]}, {"id": "1911.04078", "submitter": "Kai Li", "authors": "Kai Li, Yuzhe Tang, Jiaqi Chen, Zhehu Yuan, Cheng Xu, Jianliang Xu", "title": "Cost-Effective Data Feeds to Blockchains via Workload-Adaptive Data\n  Replication", "comments": "Blockchain storage replication, Data feed, GRuB, 20 pages, Middleware\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feeding external data to a blockchain, a.k.a. data feed, is an essential task\nto enable blockchain interoperability and support emerging cross-domain\napplications, notably stablecoins. Given the data-intensive feeds in real life\n(e.g., high-frequency price updates) and the high cost in using blockchain,\nnamely Gas, it is imperative to reduce the Gas cost of data feeds. Motivated by\nthe constant-changing workloads in finance and other applications, this work\nfocuses on designing a dynamic, workload-aware approach for cost effectiveness\nin Gas. This design space is understudied in the existing blockchain research\nwhich has so far focused on static data placement.\n  This work presents GRuB, a cost-effective data feed that dynamically\nreplicates data between the blockchain and an off-chain cloud storage. GRuB's\ndata replication is workload-adaptive by monitoring the current workload and\nmaking online decisions w.r.t. data replication. A series of online algorithms\nare proposed that achieve the bounded worst-case cost in blockchain's Gas. GRuB\nruns the decision-making components on the untrusted cloud off-chain for lower\nGas costs, and employs a security protocol to authenticate the data transferred\nbetween the blockchain and cloud. The overall GRuB system can autonomously\nachieve low Gas costs with changing workloads.\n  We built a GRuB prototype functional with Ethereum and Google LevelDB, and\nsupported real applications in stablecoins. Under real workloads collected from\nthe Ethereum contract-call history and mixed workloads of YCSB, we\nsystematically evaluate GRuB's cost which shows a saving of Gas by 10% ~ 74%,\nwith comparison to the baselines of static data-placement.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 05:08:41 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 02:03:56 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 23:10:55 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Li", "Kai", ""], ["Tang", "Yuzhe", ""], ["Chen", "Jiaqi", ""], ["Yuan", "Zhehu", ""], ["Xu", "Cheng", ""], ["Xu", "Jianliang", ""]]}, {"id": "1911.04200", "submitter": "Maciej Besta", "authors": "Maciej Besta, Raghavendra Kanakagiri, Harun Mustafa, Mikhail\n  Karasikov, Gunnar R\\\"atsch, Torsten Hoefler, Edgar Solomonik", "title": "Communication-Efficient Jaccard Similarity for High-Performance\n  Distributed Genome Comparisons", "comments": null, "journal-ref": "Proceedings of the 34st IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS'20), 2020", "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.PF q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jaccard similarity index is an important measure of the overlap of two\nsets, widely used in machine learning, computational genomics, information\nretrieval, and many other areas. We design and implement SimilarityAtScale, the\nfirst communication-efficient distributed algorithm for computing the Jaccard\nsimilarity among pairs of large datasets. Our algorithm provides an efficient\nencoding of this problem into a multiplication of sparse matrices. Both the\nencoding and sparse matrix product are performed in a way that minimizes data\nmovement in terms of communication and synchronization costs. We apply our\nalgorithm to obtain similarity among all pairs of a set of large samples of\ngenomes. This task is a key part of modern metagenomics analysis and an\nevergrowing need due to the increasing availability of high-throughput DNA\nsequencing data. The resulting scheme is the first to enable accurate Jaccard\ndistance derivations for massive datasets, using largescale distributed-memory\nsystems. We package our routines in a tool, called GenomeAtScale, that combines\nthe proposed algorithm with tools for processing input sequences. Our\nevaluation on real data illustrates that one can use GenomeAtScale to\neffectively employ tens of thousands of processors to reach new frontiers in\nlarge-scale genomic and metagenomic analysis. While GenomeAtScale can be used\nto foster DNA research, the more general underlying SimilarityAtScale algorithm\nmay be used for high-performance distributed similarity computations in other\ndata analytics application domains.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 11:57:54 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 13:59:00 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 15:57:32 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Besta", "Maciej", ""], ["Kanakagiri", "Raghavendra", ""], ["Mustafa", "Harun", ""], ["Karasikov", "Mikhail", ""], ["R\u00e4tsch", "Gunnar", ""], ["Hoefler", "Torsten", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1911.04559", "submitter": "Anirban Das", "authors": "Anirban Das and Thomas Brunschwiler", "title": "Privacy is What We Care About: Experimental Investigation of Federated\n  Learning on Edge Devices", "comments": "Accepted in ACM AIChallengeIoT 2019, New York, USA", "journal-ref": null, "doi": "10.1145/3363347.3363365", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning enables training of a general model through edge devices\nwithout sending raw data to the cloud. Hence, this approach is attractive for\ndigital health applications, where data is sourced through edge devices and\nusers care about privacy. Here, we report on the feasibility to train deep\nneural networks on the Raspberry Pi4s as edge devices. A CNN, a LSTM and a MLP\nwere successfully trained on the MNIST data-set. Further, federated learning is\ndemonstrated experimentally on IID and non-IID samples in a parametric study,\nto benchmark the model convergence. The weight updates from the workers are\nshared with the cloud to train the general model through federated learning.\nWith the CNN and the non-IID samples a test-accuracy of up to 85% could be\nachieved within a training time of 2 minutes, while exchanging less than $10$\nMB data per device. In addition, we discuss federated learning from an use-case\nstandpoint, elaborating on privacy risks and labeling requirements for the\napplication of emotion detection from sound. Based on the experimental\nfindings, we discuss possible research directions to improve model and system\nperformance. Finally, we provide best practices for a practitioner, considering\nthe implementation of federated learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 20:44:03 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Das", "Anirban", ""], ["Brunschwiler", "Thomas", ""]]}, {"id": "1911.04587", "submitter": "Xintao Wu", "authors": "Depeng Xu, Shuhan Yuan, Xintao Wu", "title": "Achieving Differential Privacy in Vertically Partitioned Multiparty\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving differential privacy has been well studied under centralized\nsetting. However, it's very challenging to preserve differential privacy under\nmultiparty setting, especially for the vertically partitioned case. In this\nwork, we propose a new framework for differential privacy preserving multiparty\nlearning in the vertically partitioned setting. Our core idea is based on the\nfunctional mechanism that achieves differential privacy of the released model\nby adding noise to the objective function. We show the server can simply\ndissect the objective function into single-party and cross-party sub-functions,\nand allocate computation and perturbation of their polynomial coefficients to\nlocal parties. Our method needs only one round of noise addition and secure\naggregation. The released model in our framework achieves the same utility as\napplying the functional mechanism in the centralized setting. Evaluation on\nreal-world and synthetic datasets for linear and logistic regressions shows the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 22:28:07 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Xu", "Depeng", ""], ["Yuan", "Shuhan", ""], ["Wu", "Xintao", ""]]}, {"id": "1911.04629", "submitter": "Quan Nguyen Hoang", "authors": "Quan Nguyen, Andre Cronje, Michael Kong", "title": "Fast Stochastic Peer Selection in Proof-of-Stake Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of peer selection, which randomly selects a peer from a set, is\ncommonplace in Proof-of-Stake (PoS) protocols. In PoS, peers are chosen\nrandomly with probability proportional to the amount of stake that they\npossess. This paper presents an approach that relates PoS peer selection to\nRoulette-wheel selection, which is frequently used in genetic and evolutionary\nalgorithms or complex network modelling. In particular, we introduce the use of\nstochastic acceptance algorithm [6] for fast peer selection. The roulette-wheel\nselection algorithm [6] achieves O(1) complexity based on stochastic\nacceptance, whereas searching based algorithms may take O(N ) or O(logN )\ncomplexity in a network of N peers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 01:24:39 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Nguyen", "Quan", ""], ["Cronje", "Andre", ""], ["Kong", "Michael", ""]]}, {"id": "1911.04650", "submitter": "Wumo Yan", "authors": "Zhuojin Li, Wumo Yan, Marco Paolieri, Leana Golubchik", "title": "Throughput Prediction of Asynchronous SGD in TensorFlow", "comments": null, "journal-ref": null, "doi": "10.1145/3358960.3379141", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning frameworks can train neural networks using multiple\nnodes in parallel, each computing parameter updates with stochastic gradient\ndescent (SGD) and sharing them asynchronously through a central parameter\nserver. Due to communication overhead and bottlenecks, the total throughput of\nSGD updates in a cluster scales sublinearly, saturating as the number of nodes\nincreases. In this paper, we present a solution to predicting training\nthroughput from profiling traces collected from a single-node configuration.\nOur approach is able to model the interaction of multiple nodes and the\nscheduling of concurrent transmissions between the parameter server and each\nnode. By accounting for the dependencies between received parts and pending\ncomputations, we predict overlaps between computation and communication and\ngenerate synthetic execution traces for configurations with multiple nodes. We\nvalidate our approach on TensorFlow training jobs for popular image\nclassification neural networks, on AWS and on our in-house cluster, using nodes\nequipped with GPUs or only with CPUs. We also investigate the effects of data\ntransmission policies used in TensorFlow and the accuracy of our approach when\ncombined with optimizations of the transmission schedule.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 03:17:20 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 06:32:31 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Li", "Zhuojin", ""], ["Yan", "Wumo", ""], ["Paolieri", "Marco", ""], ["Golubchik", "Leana", ""]]}, {"id": "1911.04678", "submitter": "Hiroto Yasumi", "authors": "Hiroto Yasumi, Fukuhito Ooshita and Michiko Inoue", "title": "Uniform Partition in Population Protocol Model under Weak Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on a uniform partition problem in a population protocol model. The\nuniform partition problem aims to divide a population into k groups of the same\nsize, where k is a given positive integer. In the case of k=2 (called uniform\nbipartition), a previous work clarified space complexity under various\nassumptions: 1) an initialized base station (BS) or no BS, 2) weak or global\nfairness, 3) designated or arbitrary initial states of agents, and 4) symmetric\nor asymmetric protocols, except for the setting that agents execute a protocol\nfrom arbitrary initial states under weak fairness in the model with an\ninitialized base station. In this paper, we clarify the space complexity for\nthis remaining setting. In this setting, we prove that P states are necessary\nand sufficient to realize asymmetric protocols, and that P+1 states are\nnecessary and sufficient to realize symmetric protocols, where P is the known\nupper bound of the number of agents. From these results and the previous work,\nwe have clarified the solvability of the uniform bipartition for each\ncombination of assumptions. Additionally, we newly consider an assumption on a\nmodel of a non-initialized BS and clarify solvability and space complexity in\nthe assumption. Moreover, the results in this paper can be applied to the case\nthat k is an arbitrary integer (called uniform k-partition).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 05:24:27 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Yasumi", "Hiroto", ""], ["Ooshita", "Fukuhito", ""], ["Inoue", "Michiko", ""]]}, {"id": "1911.04698", "submitter": "Jieyi Long", "authors": "Jieyi Long and Ribao Wei", "title": "Scalable BFT Consensus Mechanism Through Aggregated Signature Gossip", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new BFT consensus mechanism which enables\nthousands of nodes to participate in the consensus process, and supports very\nhigh transaction throughput. This is achieved via an aggregated signature\ngossip protocol which can significantly reduce the messaging complexity and\nthus allows a large number of nodes to reach consensus quickly. The proposed\naggregated signature gossip protocol leverages a non-interactive leaderless BLS\nsignature aggregation scheme. We have proven the correctness of the protocol,\nand analyzed its efficiency. In our analysis, each node only needs to send and\nreceive O(logn) messages to reach agreement, where each message just contains a\ncouple kilobytes of data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 06:22:58 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Long", "Jieyi", ""], ["Wei", "Ribao", ""]]}, {"id": "1911.04743", "submitter": "Yukiko Yamauchi", "authors": "Shotaro Yoshimura and Yukiko Yamauchi", "title": "Network Creation Games with Local Information and Edge Swaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the swap game (SG) selfish players, each of which is associated to a\nvertex, form a graph by edge swaps, i.e., a player changes its strategy by\nsimultaneously removing an adjacent edge and forming a new edge (Alon et al.,\n2013). The cost of a player considers the average distance to all other players\nor the maximum distance to other players. Any SG by $n$ players starting from a\ntree converges to an equilibrium with a constant Price of Anarchy (PoA) within\n$O(n^3)$ edge swaps (Lenzner, 2011). We focus on SGs where each player knows\nthe subgraph induced by players within distance $k$. Therefore, each player\ncannot compute its cost nor a best response. We first consider pessimistic\nplayers who consider the worst-case global graph. We show that any SG starting\nfrom a tree (i) always converges to an equilibrium within $O(n^3)$ edge swaps\nirrespective of the value of $k$, (ii) the PoA is $\\Theta(n)$ for $k=1,2,3$,\nand (iii) the PoA is constant for $k \\geq 4$. We then introduce weakly\npessimistic players and optimistic players and show that these less pessimistic\nplayers achieve constant PoA for $k \\leq 3$ at the cost of best response\ncycles.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 09:05:49 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Yoshimura", "Shotaro", ""], ["Yamauchi", "Yukiko", ""]]}, {"id": "1911.04757", "submitter": "Sayaka Kamei", "authors": "Sayaka Kamei, Anissa Lamani, Fukuhito Ooshita, S\\'ebastien Tixeuil and\n  Koichi Wada", "title": "Gathering on Rings for Myopic Asynchronous Robots with Lights", "comments": "This is a full version of the conference paper in OPODIS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate gathering algorithms for asynchronous autonomous mobile robots\nmoving in uniform ring-shaped networks. Different from most work using the\nLook-Compute-Move (LCM) model, we assume that robots have limited visibility\nand lights. That is, robots can observe nodes only within a certain fixed\ndistance, and emit a color from a set of constant number of colors. We consider\ngathering algorithms depending on two parameters related to the initial\nconfiguration: $M_{init}$, which denotes the number of nodes between two border\nnodes, and $O_{init}$, which denotes the number of nodes hosting robots between\ntwo border nodes. In both cases, a border node is a node hosting one or more\nrobots that cannot see other robots on at least one side. Our main contribution\nis to prove that, if $M_{init}$ or $O_{init}$ is odd, gathering is always\nfeasible with three or four colors. The proposed algorithms do not require\nadditional assumptions, such as knowledge of the number of robots, multiplicity\ndetection capabilities, or the assumption of towerless initial configurations.\nThese results demonstrate the power of lights to achieve gathering of robots\nwith limited visibility.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 09:39:03 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Kamei", "Sayaka", ""], ["Lamani", "Anissa", ""], ["Ooshita", "Fukuhito", ""], ["Tixeuil", "S\u00e9bastien", ""], ["Wada", "Koichi", ""]]}, {"id": "1911.04946", "submitter": "Zheng Wang", "authors": "Vicent Sanz Marco, Ben Taylor, Zheng Wang, Yehia Elkhatib", "title": "Optimizing Deep Learning Inference on Embedded Systems Through Adaptive\n  Model Selection", "comments": "Accepted to be published at ACM TECS. arXiv admin note: substantial\n  text overlap with arXiv:1805.04252", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks ( DNNs ) are becoming a key enabling technology for many\napplication domains. However, on-device inference on battery-powered,\nresource-constrained embedding systems is often infeasible due to prohibitively\nlong inferencing time and resource requirements of many DNNs. Offloading\ncomputation into the cloud is often unacceptable due to privacy concerns, high\nlatency, or the lack of connectivity. While compression algorithms often\nsucceed in reducing inferencing times, they come at the cost of reduced\naccuracy. This paper presents a new, alternative approach to enable efficient\nexecution of DNNs on embedded devices. Our approach dynamically determines\nwhich DNN to use for a given input, by considering the desired accuracy and\ninference time. It employs machine learning to develop a low-cost predictive\nmodel to quickly select a pre-trained DNN to use for a given input and the\noptimization constraint. We achieve this by first off-line training a\npredictive model, and then using the learned model to select a DNN model to use\nfor new, unseen inputs. We apply our approach to two representative DNN\ndomains: image classification and machine translation. We evaluate our approach\non a Jetson TX2 embedded deep learning platform and consider a range of\ninfluential DNN models including convolutional and recurrent neural networks.\nFor image classification, we achieve a 1.8x reduction in inference time with a\n7.52% improvement in accuracy, over the most-capable single DNN model. For\nmachine translation, we achieve a 1.34x reduction in inference time over the\nmost-capable single model, with little impact on the quality of translation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 23:56:19 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Marco", "Vicent Sanz", ""], ["Taylor", "Ben", ""], ["Wang", "Zheng", ""], ["Elkhatib", "Yehia", ""]]}, {"id": "1911.05145", "submitter": "\\'Alvaro Garc\\'ia-P\\'erez", "authors": "\\'Alvaro Garc\\'ia-P\\'erez and Maria A. Schett", "title": "Deconstructing Stellar Consensus (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the recent blockchain proposals, such as Stellar and Ripple, allow\nfor open membership while using quorum-like structures typical for classical\nByzantine consensus with closed membership. This is achieved by constructing\nquorums in a decentralised way: each participant independently chooses whom to\ntrust, and quorums arise from these individual decisions. Unfortunately, the\nconsensus protocols underlying such blockchains are poorly understood, and\ntheir correctness has not been rigorously investigated. In this paper we\nrigorously prove correct the Stellar Consensus Protocol (SCP), with our proof\ngiving insights into the protocol structure and its use of lower-level\nabstractions. To this end, we first propose an abstract version of SCP that\nuses as a black box Stellar's federated voting primitive (analogous to reliable\nByzantine broadcast), previously investigated by Garc\\'ia-P\\'erez and Gotsman.\nThe abstract consensus protocol highlights a modular structure in Stellar and\ncan be proved correct by reusing the previous results on federated voting.\nHowever, it is unsuited for realistic implementations, since its processes\nmaintain infinite state. We thus establish a refinement between the abstract\nprotocol and the concrete SCP that uses only finite state, thereby carrying\nover the result about the correctness of former to the latter. Our results help\nestablish the theoretical foundations of decentralised blockchains like Stellar\nand gain confidence in their correctness.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 21:06:36 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 16:56:13 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 14:35:59 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Garc\u00eda-P\u00e9rez", "\u00c1lvaro", ""], ["Schett", "Maria A.", ""]]}, {"id": "1911.05146", "submitter": "Ammar Ahmad Awan", "authors": "Ammar Ahmad Awan, Arpan Jain, Quentin Anthony, Hari Subramoni, and\n  Dhabaleswar K. Panda", "title": "HyPar-Flow: Exploiting MPI and Keras for Scalable Hybrid-Parallel DNN\n  Training using TensorFlow", "comments": "18 pages, 10 figures, Accepted, to be presented at ISC '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce training time of large-scale DNNs, scientists have started to\nexplore parallelization strategies like data-parallelism, model-parallelism,\nand hybrid-parallelism. While data-parallelism has been extensively studied and\ndeveloped, several problems exist in realizing model-parallelism and\nhybrid-parallelism efficiently. Four major problems we focus on are: 1)\ndefining a notion of a distributed model across processes, 2) implementing\nforward/back-propagation across process boundaries that requires explicit\ncommunication, 3) obtaining parallel speedup on an inherently sequential task,\nand 4) achieving scalability without losing out on a model's accuracy. To\naddress these problems, we create HyPar-Flow --- a model-size/-type agnostic,\nscalable, practical, and user-transparent system for hybrid-parallel training\nby exploiting MPI, Keras, and TensorFlow. HyPar-Flow provides a single API that\ncan be used to perform data, model, and hybrid parallel training of any Keras\nmodel at scale. We create an internal distributed representation of the\nuser-provided Keras model, utilize TF's Eager execution features for\ndistributed forward/back-propagation across processes, exploit pipelining to\nimprove performance and leverage efficient MPI primitives for scalable\ncommunication. Between model partitions, we use send and recv to exchange\nlayer-data/partial-errors while allreduce is used to accumulate/average\ngradients across model replicas. Beyond the design and implementation of\nHyPar-Flow, we also provide comprehensive correctness and performance results\non three state-of-the-art HPC systems including TACC Frontera (#5 on\nTop500.org). For ResNet-1001, an ultra-deep model, HyPar-Flow provides: 1) Up\nto 1.6x speedup over Horovod-based data-parallel training, 2) 110x speedup over\nsingle-node on 128 Stampede2 nodes, and 3) 481x speedup over single-node on 512\nFrontera nodes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 21:07:42 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 15:16:53 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Awan", "Ammar Ahmad", ""], ["Jain", "Arpan", ""], ["Anthony", "Quentin", ""], ["Subramoni", "Hari", ""], ["Panda", "Dhabaleswar K.", ""]]}, {"id": "1911.05160", "submitter": "Vikram Jadhao", "authors": "JCS Kadupitiya and Vikram Jadhao and Prateek Sharma", "title": "Modeling The Temporally Constrained Preemptions of Transient Cloud VMs", "comments": "12 pages, 9 figures; to appear at HPDC 2020", "journal-ref": null, "doi": "10.1145/3369583.3392671", "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient cloud servers such as Amazon Spot instances, Google Preemptible\nVMs, and Azure Low-priority batch VMs, can reduce cloud computing costs by as\nmuch as $10\\times$, but can be unilaterally preempted by the cloud provider.\nUnderstanding preemption characteristics (such as frequency) is a key first\nstep in minimizing the effect of preemptions on application performance,\navailability, and cost. However, little is understood about temporally\nconstrained preemptions---wherein preemptions must occur in a given time\nwindow. We study temporally constrained preemptions by conducting a large scale\nempirical study of Google's Preemptible VMs (that have a maximum lifetime of 24\nhours), develop a new preemption probability model, new model-driven resource\nmanagement policies, and implement them in a batch computing service for\nscientific computing workloads. Our statistical and experimental analysis\nindicates that temporally constrained preemptions are not uniformly\ndistributed, but are time-dependent and have a bathtub shape. We find that\nexisting memoryless models and policies are not suitable for temporally\nconstrained preemptions. We develop a new probability model for bathtub\npreemptions, and analyze it through the lens of reliability theory. To\nhighlight the effectiveness of our model, we develop optimized policies for job\nscheduling and checkpointing. Compared to existing techniques, our model-based\npolicies can reduce the probability of job failure by more than $2\\times$. We\nalso implement our policies as part of a batch computing service for scientific\ncomputing applications, which reduces cost by $5\\times$ compared to\nconventional cloud deployments and keeps performance overheads under $3\\%$.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 21:58:15 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 19:34:57 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Kadupitiya", "JCS", ""], ["Jadhao", "Vikram", ""], ["Sharma", "Prateek", ""]]}, {"id": "1911.05181", "submitter": "Jonathan Baxter", "authors": "Douglas Aberdeen, Jonathan Baxter and Robert Edwards", "title": "92c/MFlops/s, Ultra-Large-Scale Neural-Network Training on a PIII\n  Cluster", "comments": "SC '00: Proceedings of the 2000 ACM/IEEE Conference on Supercomputing", "journal-ref": "ACM/IEEE SC 2000 Conference (SC00)", "doi": "10.1109/SC.2000.10031", "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks with millions of adjustable parameters and a\nsimilar number of training examples are a potential solution for difficult,\nlarge-scale pattern recognition problems in areas such as speech and face\nrecognition, classification of large volumes of web data, and finance. The\nbottleneck is that neural network training involves iterative gradient descent\nand is extremely computationally intensive. In this paper we present a\ntechnique for distributed training of Ultra Large Scale Neural Networks (ULSNN)\non Bunyip, a Linux-based cluster of 196 Pentium III processors. To illustrate\nULSNN training we describe an experiment in which a neural network with 1.73\nmillion adjustable parameters was trained to recognize machine-printed Japanese\ncharacters from a database containing 9 million training patterns. The training\nruns with a average performance of 163.3 GFlops/s (single precision). With a\nmachine cost of \\$150,913, this yields a price/performance ratio of\n92.4c/MFlops/s (single precision). For comparison purposes, training using\ndouble precision and the ATLAS DGEMM produces a sustained performance of 70\nMFlops/s or \\$2.16 / MFlop/s (double precision).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 22:57:09 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Aberdeen", "Douglas", ""], ["Baxter", "Jonathan", ""], ["Edwards", "Robert", ""]]}, {"id": "1911.05193", "submitter": "Bogdan Penkovsky", "authors": "Michael Dubrovsky, Marshall Ball, Bogdan Penkovsky", "title": "Optical Proof of Work", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most cryptocurrencies rely on Proof-of-Work (PoW) \"mining\" for resistance to\nSybil and double-spending attacks, as well as a mechanism for currency\nissuance. Hashcash PoW has successfully secured the Bitcoin network since its\ninception, however, as the network has expanded to take on additional value\nstorage and transaction volume, Bitcoin PoW's heavy reliance on electricity has\ncreated scalability issues, environmental concerns, and systemic risks. Mining\nefforts have concentrated in areas with low electricity costs, creating single\npoints of failure. Although PoW security properties rely on imposing a\ntrivially verifiable economic cost on miners, there is no fundamental reason\nfor it to consist primarily of electricity cost. The authors propose a novel\nPoW algorithm, Optical Proof of Work (oPoW), to eliminate energy as the primary\ncost of mining. Proposed algorithm imposes economic difficulty on the miners,\nhowever, the cost is concentrated in hardware (capital expense-CAPEX) rather\nthan electricity (operating expenses-OPEX). The oPoW scheme involves minimal\nmodifications to Hashcash-like PoW schemes, inheriting safety/security\nproperties from such schemes.\n  Rapid growth and improvement in silicon photonics over the last two decades\nhas led to the commercialization of silicon photonic co-processors (integrated\ncircuits that use photons instead of electrons to perform specialized computing\ntasks) for low-energy deep learning. oPoW is optimized for this technology such\nthat miners are incentivized to use specialized, energy-efficient photonics for\ncomputation. Beyond providing energy savings, oPoW has the potential to improve\nnetwork scalability, enable decentralized mining outside of low electricity\ncost areas, and democratize issuance. Due to the CAPEX dominance of mining\ncosts, oPoW hashrate will be significantly less sensitive to underlying coin\nprice declines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 23:11:45 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 19:13:42 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Dubrovsky", "Michael", ""], ["Ball", "Marshall", ""], ["Penkovsky", "Bogdan", ""]]}, {"id": "1911.05239", "submitter": "Shantanu Das", "authors": "Shantanu Das, Giuseppe A. Di Luna, Paola Flocchini, Nicola Santoro,\n  Giovanni Viglietta, Masafumi Yamashita", "title": "Oblivious Permutations on the Plane", "comments": "This is the full version of the article with the same title published\n  at OPODIS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed system of n identical mobile robots operating in\nthe two dimensional Euclidian plane. As in the previous studies, we consider\nthe robots to be anonymous, oblivious, dis-oriented, and without any\ncommunication capabilities, operating based on the Look-Compute-Move model\nwhere the next location of a robot depends only on its view of the current\nconfiguration. Even in this seemingly weak model, most formation problems which\nrequire constructing specific configurations, can be solved quite easily when\nthe robots are fully synchronized with each other. In this paper we introduce\nand study a new class of problems which, unlike the formation problems so far,\ncannot always be solved even in the fully synchronous model with atomic and\nrigid moves. This class of problems requires the robots to permute their\nlocations in the plane. In particular, we are interested in implementing two\nspecial types of permutations -- permutations without any fixed points and\npermutations of order $n$. The former (called MOVE-ALL) requires each robot to\nvisit at least two of the initial locations, while the latter (called\nVISIT-ALL) requires every robot to visit each of the initial locations in a\nperiodic manner. We provide a characterization of the solvability of these\nproblems, showing the main challenges in solving this class of problems for\nmobile robots. We also provide algorithms for the feasible cases, in particular\ndistinguishing between one-step algorithms (where each configuration must be a\npermutation of the original configuration) and multi-step algorithms (which\nallow intermediate configurations). These results open a new research direction\nin mobile distributed robotics which has not been investigated before.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 01:47:50 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Das", "Shantanu", ""], ["Di Luna", "Giuseppe A.", ""], ["Flocchini", "Paola", ""], ["Santoro", "Nicola", ""], ["Viglietta", "Giovanni", ""], ["Yamashita", "Masafumi", ""]]}, {"id": "1911.05328", "submitter": "Yuan Tang", "authors": "Yuan Tang", "title": "Improving the Space-Time Efficiency of Processor-Oblivious Matrix\n  Multiplication Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic cache-oblivious parallel matrix multiplication algorithms achieve\noptimality either in time or space, but not both, which promotes lots of\nresearch on the best possible balance or tradeoff of such algorithms. We study\nmodern processor-oblivious runtime systems and figure out several ways to\nimprove algorithm's time bound while still bounding space and cache\nrequirements to be asymptotically optimal. By our study, we give out sublinear\ntime, optimal work, space and cache algorithms for both general matrix\nmultiplication on a semiring and Strassen-like fast algorithm. Our experiments\nalso show such algorithms have empirical advantages over classic counterparts.\nOur study provides new insights and research angles on how to optimize\ncache-oblivious parallel algorithms from both theoretical and empirical\nperspectives.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 07:13:33 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Tang", "Yuan", ""]]}, {"id": "1911.05333", "submitter": "Yuan Tang", "authors": "Yuan Tang", "title": "Nested Dataflow Algorithms for Dynamic Programming Recurrences with more\n  than O(1) Dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming problems have wide applications in real world and have\nbeen studied extensively in both serial and parallel settings. In 1994, Galil\nand Park developed work-efficient and sublinear-time algorithms for several\nimportant dynamic programming problems based on the closure method and matrix\nproduct method. However, in the same paper, they raised an open question\nwhether such an algorithm exists for the general GAP problem. % In this paper,\nwe answer their question by developing the first work-efficient and\nsublinear-time GAP algorithm based on the closure method and Nested Dataflow\nmethod. % We also improve the time bounds of classic work-efficient,\ncache-oblivious and cache-efficient algorithms for the 1D problem and GAP\nproblem, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 07:24:05 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Tang", "Yuan", ""]]}, {"id": "1911.05642", "submitter": "Latif U. Khan", "authors": "Latif U. Khan, Shashi Raj Pandey, Nguyen H. Tran, Walid Saad, Zhu Han,\n  Minh N. H. Nguyen, Choong Seon Hong", "title": "Federated Learning for Edge Networks: Resource Optimization and\n  Incentive Mechanism", "comments": "The first two authors contributed equally. This article has been\n  accepted for publication in IEEE Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a rapid proliferation of smart Internet of Things\n(IoT) devices. IoT devices with intelligence require the use of effective\nmachine learning paradigms. Federated learning can be a promising solution for\nenabling IoT-based smart applications. In this paper, we present the primary\ndesign aspects for enabling federated learning at network edge. We model the\nincentive-based interaction between a global server and participating devices\nfor federated learning via a Stackelberg game to motivate the participation of\nthe devices in the federated learning process. We present several open research\nchallenges with their possible solutions. Finally, we provide an outlook on\nfuture research.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 03:29:04 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 08:24:25 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 07:53:35 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Khan", "Latif U.", ""], ["Pandey", "Shashi Raj", ""], ["Tran", "Nguyen H.", ""], ["Saad", "Walid", ""], ["Han", "Zhu", ""], ["Nguyen", "Minh N. H.", ""], ["Hong", "Choong Seon", ""]]}, {"id": "1911.05660", "submitter": "Nandita Vijaykumar", "authors": "Nandita Vijaykumar", "title": "Enhancing Programmability, Portability, and Performance with Rich\n  Cross-Layer Abstractions", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmability, performance portability, and resource efficiency have\nemerged as critical challenges in harnessing complex and diverse architectures\ntoday to obtain high performance and energy efficiency. While there is abundant\nresearch, and thus significant improvements, at different levels of the stack\nthat address these very challenges, in this thesis, we observe that we are\nfundamentally limited by the interfaces and abstractions between the\napplication and the underlying system/hardware--specifically, the\nhardware-software interface. The existing narrow interfaces pose two critical\nchallenges. First, significant effort and expertise are required to write\nhigh-performance code to harness the full potential of today's diverse and\nsophisticated hardware. Second, as a hardware/system designer, architecting\nfaster and more efficient systems is challenging as the vast majority of the\nprogram's semantic content gets lost in translation with today's\nhardware-software interface. Moving towards the future, these challenges in\nprogrammability and efficiency will be even more intractable as we architect\nincreasingly heterogeneous and sophisticated systems.\n  This thesis makes the case for rich low-overhead cross-layer abstractions as\na highly effective means to address the above challenges. These abstractions\nare designed to communicate higher-level program information from the\napplication to the underlying system and hardware in a highly efficient manner,\nrequiring only minor additions to the existing interfaces. In doing so, they\nenable a rich space of hardware-software cooperative mechanisms to optimize for\nperformance. We propose 4 different approaches to designing richer abstractions\nbetween the application, system software, and hardware architecture in\ndifferent contexts to significantly improve programmability, portability, and\nperformance in CPUs and GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 18:22:11 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Vijaykumar", "Nandita", ""]]}, {"id": "1911.05662", "submitter": "Xiaoming Chen", "authors": "Xiaoming Chen, Yinhe Han, Yu Wang", "title": "Communication Lower Bound in Convolution Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current convolutional neural network (CNN) accelerators, communication\n(i.e., memory access) dominates the energy consumption. This work provides\ncomprehensive analysis and methodologies to minimize the communication for CNN\naccelerators. For the off-chip communication, we derive the theoretical lower\nbound for any convolutional layer and propose a dataflow to reach the lower\nbound. This fundamental problem has never been solved by prior studies. The\non-chip communication is minimized based on an elaborate workload and storage\nmapping scheme. We in addition design a communication-optimal CNN accelerator\narchitecture. Evaluations based on the 65nm technology demonstrate that the\nproposed architecture nearly reaches the theoretical minimum communication in a\nthree-level memory hierarchy and it is computation dominant. The gap between\nthe energy efficiency of our accelerator and the theoretical best value is only\n37-87%.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 04:54:17 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 02:08:40 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 03:04:10 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Chen", "Xiaoming", ""], ["Han", "Yinhe", ""], ["Wang", "Yu", ""]]}, {"id": "1911.05692", "submitter": "Ahmad Shawahna", "authors": "Basem AL-Madani, Ahmad Shawahna, and Mohammad Qureshi", "title": "Anomaly Detection for Industrial Control Networks using Machine Learning\n  with the help from the Inter-Arrival Curves", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial Control Networks (ICN) such as Supervisory Control and Data\nAcquisition (SCADA) systems are widely used in industries for monitoring and\ncontrolling physical processes. These industries include power generation and\nsupply, gas and oil production and delivery, water and waste management,\ntelecommunication and transport facilities. The integration of internet exposes\nthese systems to cyber threats. The consequences of compromised ICN are\ndetermine for a country economic and functional sustainability. Therefore,\nenforcing security and ensuring correctness operation became one of the biggest\nconcerns for Industrial Control Systems (ICS), and need to be addressed. In\nthis paper, we propose an anomaly detection approach for ICN using the physical\nproperties of the system. We have developed operational baseline of electricity\ngeneration process and reduced the feature set using greedy and genetic feature\nselection algorithms. The classification is done based on Support Vector\nMachine (SVM), k-Nearest Neighbor (k-NN), and C4.5 decision tree with the help\nfrom the inter-arrival curves. The results show that the proposed approach\nsuccessfully detects anomalies with a high degree of accuracy. In addition,\nthey proved that SVM and C4.5 produces accurate results even for high\nsensitivity attacks when they used with the inter-arrival curves. As compared\nto this, k-NN is unable to produce good results for low and medium sensitivity\nattacks test cases.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 01:25:45 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["AL-Madani", "Basem", ""], ["Shawahna", "Ahmad", ""], ["Qureshi", "Mohammad", ""]]}, {"id": "1911.05839", "submitter": "Akshay Bhosale", "authors": "Akshay Bhosale and Rudolf Eigenmann", "title": "Compile-time Parallelization of Subscripted Subscript Patterns", "comments": "15 pages , 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of scientific applications are making use of irregular\ndata access patterns. An important class of such patterns involve\nsubscripted-subscripts, wherein an array value appears in the index expression\nof another array. Even though the information required to parallelize loops\nwith such patterns is often available in the program, present compiler\ntechniques fall short of analyzing that information. In this paper we present a\nstudy of subscripted-subscripts, the properties that define the subscript\narrays, and an algorithm based on symbolic range aggregation, that will help\nprove the presence of some of the properties of the subscript array in the\nprogram. We show that, in an important class of programs, the algorithm can\nboost the performance from essentially sequential execution to close to fully\nparallel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 22:19:33 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Bhosale", "Akshay", ""], ["Eigenmann", "Rudolf", ""]]}, {"id": "1911.05918", "submitter": "Vaneet Aggarwal", "authors": "Ajay Badita and Parimal Parag and Vaneet Aggarwal", "title": "Optimal Server Selection for Straggler Mitigation", "comments": null, "journal-ref": "IEEE/ACM Transactions on Networking, vol. 28, no. 2, pp. 709--721,\n  April 2020", "doi": "10.1109/TNET.2020.2973224", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of large-scale distributed compute systems is adversely\nimpacted by stragglers when the execution time of a job is uncertain. To manage\nstragglers, we consider a multi-fork approach for job scheduling, where\nadditional parallel servers are added at forking instants. In terms of the\nforking instants and the number of additional servers, we compute the job\ncompletion time and the cost of server utilization when the task processing\ntimes are assumed to have a shifted exponential distribution. We use this study\nto provide insights into the scheduling design of the forking instants and the\nassociated number of additional servers to be started. Numerical results\ndemonstrate orders of magnitude improvement in cost in the regime of low\ncompletion times as compared to the prior works.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 03:18:20 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Badita", "Ajay", ""], ["Parag", "Parimal", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1911.05953", "submitter": "Jae-Yun Kim", "authors": "Jae-Yun Kim, Jun-Mo Lee, Yeon-Jae Koo, Sang-Hyeon Park, Soo-Mook Moon", "title": "Ethanos: Lightweight Bootstrapping for Ethereum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As ethereum blockchain has become popular, the number of users and\ntransactions has skyrocketed, causing an explosive increase of its data size.\nAs a result, ordinary clients using PCs or smartphones cannot easily bootstrap\nas a full node, but rely on other full nodes such as the miners to run or\nverify transactions. This may affect the security of ethereum, so light\nbootstrapping techniques such as fast sync has been proposed to download only\nparts of full data, yet the space overhead is still too high. One of the\nbiggest space overhead that cannot easily be reduced is caused by saving the\nstate of all accounts in the block's state trie. Fortunately, we found that\nmore than 90% of accounts are inactive and old transactions are hard to be\nmanipulated. Based on these observations, this paper propose a novel\noptimization technique called ethanos that can reduce bootstrapping cost by\nsweeping inactive accounts periodically and by not downloading old\ntransactions. If an inactive account becomes active, ethanos restore its state\nby running a restoration transaction. Also, ethanos gives incentives for\narchive nodes to maintain the old transactions for possible re-verification. We\nimplemented ethanos by instrumenting the go-ethereum (geth) client and\nevaluated with the real 113 million transactions from 14 million accounts\nbetween 7M-th and 8M-th blocks in ethereum. Our experimental result shows that\nethanos can reduce the size of the account state by half, which, if combined\nwith removing old transactions, may reduce the storage size for bootstrapping\nto around 1GB. This would be reasonable enough for ordinary clients to\nbootstrap on their personal devices.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 05:55:17 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Kim", "Jae-Yun", ""], ["Lee", "Jun-Mo", ""], ["Koo", "Yeon-Jae", ""], ["Park", "Sang-Hyeon", ""], ["Moon", "Soo-Mook", ""]]}, {"id": "1911.05991", "submitter": "Taisuke Yasuda", "authors": "Manuel Fernandez, David P. Woodruff, Taisuke Yasuda", "title": "Graph Spanners in the Message-Passing Model", "comments": "ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph spanners are sparse subgraphs which approximately preserve all pairwise\nshortest-path distances in an input graph. The notion of approximation can be\nadditive, multiplicative, or both, and many variants of this problem have been\nextensively studied. We study the problem of computing a graph spanner when the\nedges of the input graph are distributed across two or more sites in an\narbitrary, possibly worst-case partition, and the goal is for the sites to\nminimize the communication used to output a spanner. We assume the\nmessage-passing model of communication, for which there is a point-to-point\nlink between all pairs of sites as well as a coordinator who is responsible for\nproducing the output. We stress that the subset of edges that each site has is\nnot related to the network topology, which is fixed to be point-to-point. While\nthis model has been extensively studied for related problems such as graph\nconnectivity, it has not been systematically studied for graph spanners. We\npresent the first tradeoffs for total communication versus the quality of the\nspanners computed, for two or more sites, as well as for additive and\nmultiplicative notions of distortion. We show separations in the communication\ncomplexity when edges are allowed to occur on multiple sites, versus when each\nedge occurs on at most one site. We obtain nearly tight bounds (up to polylog\nfactors) for the communication of additive $2$-spanners in both the with and\nwithout duplication models, multiplicative $(2k-1)$-spanners in the with\nduplication model, and multiplicative $3$ and $5$-spanners in the without\nduplication model. Our lower bound for multiplicative $3$-spanners employs\nbiregular bipartite graphs rather than the usual Erd\\H{o}s girth conjecture\ngraphs and may be of wider interest.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 08:36:12 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 14:00:36 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Fernandez", "Manuel", ""], ["Woodruff", "David P.", ""], ["Yasuda", "Taisuke", ""]]}, {"id": "1911.06459", "submitter": "Haidar Khan", "authors": "Michael P. Perrone, Haidar Khan, Changhoan Kim, Anastasios Kyrillidis,\n  Jerry Quinn, Valentina Salapura", "title": "Optimal Mini-Batch Size Selection for Fast Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a methodology for selecting the mini-batch size that\nminimizes Stochastic Gradient Descent (SGD) learning time for single and\nmultiple learner problems. By decoupling algorithmic analysis issues from\nhardware and software implementation details, we reveal a robust empirical\ninverse law between mini-batch size and the average number of SGD updates\nrequired to converge to a specified error threshold. Combining this empirical\ninverse law with measured system performance, we create an accurate,\nclosed-form model of average training time and show how this model can be used\nto identify quantifiable implications for both algorithmic and hardware aspects\nof machine learning. We demonstrate the inverse law empirically, on both image\nrecognition (MNIST, CIFAR10 and CIFAR100) and machine translation (Europarl)\ntasks, and provide a theoretic justification via proving a novel bound on\nmini-batch SGD training.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 03:07:27 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Perrone", "Michael P.", ""], ["Khan", "Haidar", ""], ["Kim", "Changhoan", ""], ["Kyrillidis", "Anastasios", ""], ["Quinn", "Jerry", ""], ["Salapura", "Valentina", ""]]}, {"id": "1911.06462", "submitter": "Diksha Gupta", "authors": "Diksha Gupta, Jared Saia and Maxwell Young", "title": "Resource-Competitive Sybil Defenses", "comments": "46 Pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof-of-work(PoW) is an algorithmic tool used to secure networks by imposing\na computational cost on participating devices. Unfortunately, traditional PoW\nschemes require that correct devices perform significant computational work in\nperpetuity, even when the system is not under attack. We address this issue by\ndesigning general PoW protocols that ensure two properties. First, the fraction\nof identities in the system that are controlled by an attacker is a minority.\nSecond, the computational cost of our protocol is comparable to the cost of an\nattacker. In particular, we present an efficient algorithm, GMCOM, which\nguarantees that the average computational cost to the good ID per unit time is\nO(J + sqrt(T(J+1))), where J is the average number of joins by the good IDs and\nT is the average computational spending of the adversary. Additionally, we\ndiscuss a precursor to this algorithm, CCOM, which guarantees an average\ncomputational cost to good IDs per unit time of O(J+T). We prove a lower bound\nshowing that GMCOM's spending rate is asymptotically optimal among a large\nfamily of algorithms. Finally, we provide empirical evidence that our\nalgorithms can be significantly more efficient than previous defenses under\nvarious attack scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 03:22:59 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Gupta", "Diksha", ""], ["Saia", "Jared", ""], ["Young", "Maxwell", ""]]}, {"id": "1911.06525", "submitter": "S\\\"oren Henning", "authors": "S\\\"oren Henning and Wilhelm Hasselbring", "title": "Scalable and Reliable Multi-Dimensional Aggregation of Sensor Data\n  Streams", "comments": "6 pages", "journal-ref": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9006452", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ever-increasing amounts of data and requirements to process them in real time\nlead to more and more analytics platforms and software systems being designed\naccording to the concept of stream processing. A common area of application is\nthe processing of continuous data streams from sensors, for example, IoT\ndevices or performance monitoring tools. In addition to analyzing pure sensor\ndata, analyses of data for groups of sensors often need to be performed as\nwell. Therefore, data streams of the individual sensors have to be continuously\naggregated to a data stream for a group. Motivated by a real-world application\nscenario, we propose that such a stream aggregation approach has to allow for\naggregating sensors in hierarchical groups, support multiple such hierarchies\nin parallel, provide reconfiguration at runtime, and preserve the scalability\nand reliability qualities induced by applying stream processing techniques. We\npropose a stream processing architecture fulfilling these requirements, which\ncan be integrated into existing big data architectures. We present a pilot\nimplementation of such an extended architecture and show how it is used in\nindustry. Furthermore, in experimental evaluations we show that our solution\nscales linearly with the amount of sensors and provides adequate reliability in\nthe case of faults.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 09:05:16 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Henning", "S\u00f6ren", ""], ["Hasselbring", "Wilhelm", ""]]}, {"id": "1911.06633", "submitter": "Shreshth Tuli", "authors": "Shreshth Tuli, Nipam Basumatary, Sukhpal Singh Gill, Mohsen Kahani,\n  Rajesh Chand Arya, Gurpreet Singh Wander, Rajkumar Buyya", "title": "HealthFog: An Ensemble Deep Learning based Smart Healthcare System for\n  Automatic Diagnosis of Heart Diseases in Integrated IoT and Fog Computing\n  Environments", "comments": null, "journal-ref": "Future Generation Computing Systems, 2020", "doi": "10.1016/j.future.2019.10.043", "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing provides resources over the Internet and allows a plethora of\napplications to be deployed to provide services for different industries. The\nmajor bottleneck being faced currently in these cloud frameworks is their\nlimited scalability and hence inability to cater to the requirements of\ncentralized Internet of Things (IoT) based compute environments. The main\nreason for this is that latency-sensitive applications like health monitoring\nand surveillance systems now require computation over large amounts of data\n(Big Data) transferred to centralized database and from database to cloud data\ncenters which leads to drop in performance of such systems. The new paradigms\nof fog and edge computing provide innovative solutions by bringing resources\ncloser to the user and provide low latency and energy-efficient solutions for\ndata processing compared to cloud domains. Still, the current fog models have\nmany limitations and focus from a limited perspective on either accuracy of\nresults or reduced response time but not both. We proposed a novel framework\ncalled HealthFog for integrating ensemble deep learning in Edge computing\ndevices and deployed it for a real-life application of automatic Heart Disease\nanalysis. HealthFog delivers healthcare as a fog service using IoT devices and\nefficiently manages the data of heart patients, which comes as user requests.\nFog-enabled cloud framework, FogBus is used to deploy and test the performance\nof the proposed model in terms of power consumption, network bandwidth,\nlatency, jitter, accuracy and execution time. HealthFog is configurable to\nvarious operation modes that provide the best Quality of Service or prediction\naccuracy, as required, in diverse fog computation scenarios and for different\nuser requirements.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 13:50:27 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Tuli", "Shreshth", ""], ["Basumatary", "Nipam", ""], ["Gill", "Sukhpal Singh", ""], ["Kahani", "Mohsen", ""], ["Arya", "Rajesh Chand", ""], ["Wander", "Gurpreet Singh", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1911.06714", "submitter": "Ali Mohammed", "authors": "Ali Mohammed, Aurelien Cavelan, Florina M. Ciorba, Ruben M. Cabezon,\n  Ioana Banicesu", "title": "Two-level Dynamic Load Balancing for High Performance Scientific\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications are often complex, irregular, and\ncomputationally-intensive. To accommodate the ever-increasing computational\ndemands of scientific applications, high-performance computing (HPC) systems\nhave become larger and more complex, offering parallelism at multiple levels\n(e.g., nodes, cores per node, threads per core). Scientific applications need\nto exploit all the available multilevel hardware parallelism to harness the\navailable computational power. The performance of applications executing on\nsuch HPC systems may adversely be affected by load imbalance at multiple\nlevels, caused by problem, algorithmic, and systemic characteristics.\nNevertheless, most existing load balancing methods do not simultaneously\naddress load imbalance at multiple levels. This work investigates the impact of\nload imbalance on the performance of three scientific applications at the\nthread and process levels. We jointly apply and evaluate selected dynamic loop\nself-scheduling (DLS) techniques to both levels. Specifically, we employ the\nextended LaPeSD OpenMP runtime library at the thread level and extend the\nDLS4LB MPI-based dynamic load balancing library at the process level. This\napproach is generic and applicable to any multiprocess-multithreaded\ncomputationally-intensive application (programmed using MPI and OpenMP). We\nconduct an exhaustive set of experiments to assess and compare six DLS\ntechniques at the thread level and eleven at the process level. The results\nshow that improved application performance, by up to 21%, can only be achieved\nby jointly addressing load imbalance at the two levels. We offer insights into\nthe performance of the selected DLS techniques and discuss the interplay of\nload balancing at the thread level and process level.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 15:56:54 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Mohammed", "Ali", ""], ["Cavelan", "Aurelien", ""], ["Ciorba", "Florina M.", ""], ["Cabezon", "Ruben M.", ""], ["Banicesu", "Ioana", ""]]}, {"id": "1911.06859", "submitter": "Minsoo Rhu", "authors": "Bongjoon Hyun, Youngeun Kwon, Yujeong Choi, John Kim, Minsoo Rhu", "title": "NeuMMU: Architectural Support for Efficient Address Translations in\n  Neural Processing Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To satisfy the compute and memory demands of deep neural networks, neural\nprocessing units (NPUs) are widely being utilized for accelerating deep\nlearning algorithms. Similar to how GPUs have evolved from a slave device into\na mainstream processor architecture, it is likely that NPUs will become first\nclass citizens in this fast-evolving heterogeneous architecture space. This\npaper makes a case for enabling address translation in NPUs to decouple the\nvirtual and physical memory address space. Through a careful data-driven\napplication characterization study, we root-cause several limitations of prior\nGPU-centric address translation schemes and propose a memory management unit\n(MMU) that is tailored for NPUs. Compared to an oracular MMU design point, our\nproposal incurs only an average 0.06% performance overhead.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 20:10:01 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hyun", "Bongjoon", ""], ["Kwon", "Youngeun", ""], ["Choi", "Yujeong", ""], ["Kim", "John", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1911.06918", "submitter": "Jung-Woo Chang", "authors": "Jung-Woo Chang, Saehyun Ahn, Keon-Woo Kang, Suk-Ju Kang", "title": "Towards Design Methodology of Efficient Fast Algorithms for Accelerating\n  Generative Adversarial Networks on FPGAs", "comments": "Proceedings of the 25th Asia and South Pacific Design Automation\n  Conference (ASP-DAC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown excellent performance in\nimage and speech applications. GANs create impressive data primarily through a\nnew type of operator called deconvolution (DeConv) or transposed convolution\n(Conv). To implement the DeConv layer in hardware, the state-of-the-art\naccelerator reduces the high computational complexity via the DeConv-to-Conv\nconversion and achieves the same results. However, there is a problem that the\nnumber of filters increases due to this conversion. Recently, Winograd minimal\nfiltering has been recognized as an effective solution to improve the\narithmetic complexity and resource efficiency of the Conv layer. In this paper,\nwe propose an efficient Winograd DeConv accelerator that combines these two\northogonal approaches on FPGAs. Firstly, we introduce a new class of fast\nalgorithm for DeConv layers using Winograd minimal filtering. Since there are\nregular sparse patterns in Winograd filters, we further amortize the\ncomputational complexity by skipping zero weights. Secondly, we propose a new\ndataflow to prevent resource underutilization by reorganizing the filter layout\nin the Winograd domain. Finally, we propose an efficient architecture for\nimplementing Winograd DeConv by designing the line buffer and exploring the\ndesign space. Experimental results on various GANs show that our accelerator\nachieves up to 1.78x~8.38x speedup over the state-of-the-art DeConv\naccelerators.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 23:49:38 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chang", "Jung-Woo", ""], ["Ahn", "Saehyun", ""], ["Kang", "Keon-Woo", ""], ["Kang", "Suk-Ju", ""]]}, {"id": "1911.06922", "submitter": "Abdul Dakkak", "authors": "Cheng Li, Abdul Dakkak, Jinjun Xiong, Wen-mei Hwu", "title": "Benanza: Automatic $\\mu$Benchmark Generation to Compute \"Lower-bound\"\n  Latency and Inform Optimizations of Deep Learning Models on GPUs", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS47924.2020.00053", "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Deep Learning (DL) models have been increasingly used in latency-sensitive\napplications, there has been a growing interest in improving their response\ntime. An important venue for such improvement is to profile the execution of\nthese models and characterize their performance to identify possible\noptimization opportunities. However, the current profiling tools lack the\nhighly desired abilities to characterize ideal performance, identify sources of\ninefficiency, and quantify the benefits of potential optimizations. Such\ndeficiencies have led to slow characterization/optimization cycles that cannot\nkeep up with the fast pace at which new DL models are introduced.\n  We propose Benanza, a sustainable and extensible benchmarking and analysis\ndesign that speeds up the characterization/optimization cycle of DL models on\nGPUs. Benanza consists of four major components: a model processor that parses\nmodels into an internal representation, a configurable benchmark generator that\nautomatically generates micro-benchmarks given a set of models, a database of\nbenchmark results, and an analyzer that computes the \"lower-bound\" latency of\nDL models using the benchmark data and informs optimizations of model\nexecution. The \"lower-bound\" latency metric estimates the ideal model execution\non a GPU system and serves as the basis for identifying optimization\nopportunities in frameworks or system libraries. We used Benanza to evaluate 30\nONNX models in MXNet, ONNX Runtime, and PyTorch on 7 GPUs ranging from Kepler\nto the latest Turing, and identified optimizations in parallel layer execution,\ncuDNN convolution algorithm selection, framework inefficiency, layer fusion,\nand using Tensor Cores.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 00:24:05 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 01:15:16 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 16:46:32 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Li", "Cheng", ""], ["Dakkak", "Abdul", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1911.06944", "submitter": "Donghui Yan", "authors": "Ke Alexander Wang, Xinran Bian, Pan Liu, Donghui Yan", "title": "$DC^2$: A Divide-and-conquer Algorithm for Large-scale Kernel Learning\n  with Application to Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divide-and-conquer is a general strategy to deal with large scale problems.\nIt is typically applied to generate ensemble instances, which potentially\nlimits the problem size it can handle. Additionally, the data are often divided\nby random sampling which may be suboptimal. To address these concerns, we\npropose the $DC^2$ algorithm. Instead of ensemble instances, we produce\nstructure-preserving signature pieces to be assembled and conquered. $DC^2$\nachieves the efficiency of sampling-based large scale kernel methods while\nenabling parallel multicore or clustered computation. The data partition and\nsubsequent compression are unified by recursive random projections. Empirically\ndividing the data by random projections induces smaller mean squared\napproximation errors than conventional random sampling. The power of $DC^2$ is\ndemonstrated by our clustering algorithm $rpfCluster^+$, which is as accurate\nas some fastest approximate spectral clustering algorithms while maintaining a\nrunning time close to that of K-means clustering. Analysis on $DC^2$ when\napplied to spectral clustering shows that the loss in clustering accuracy due\nto data division and reduction is upper bounded by the data approximation error\nwhich would vanish with recursive random projections. Due to its easy\nimplementation and flexibility, we expect $DC^2$ to be applicable to general\nlarge scale learning problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 03:10:36 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Wang", "Ke Alexander", ""], ["Bian", "Xinran", ""], ["Liu", "Pan", ""], ["Yan", "Donghui", ""]]}, {"id": "1911.06949", "submitter": "Hanpeng Hu", "authors": "Hanpeng Hu, Dan Wang, Chuan Wu", "title": "Distributed Machine Learning through Heterogeneous Edge Systems", "comments": "Copyright 2020, Association for the Advancement of Artificial\n  Intelligence (www.aaai.org). All rights reserved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many emerging AI applications request distributed machine learning (ML) among\nedge systems (e.g., IoT devices and PCs at the edge of the Internet), where\ndata cannot be uploaded to a central venue for model training, due to their\nlarge volumes and/or security/privacy concerns. Edge devices are intrinsically\nheterogeneous in computing capacity, posing significant challenges to parameter\nsynchronization for parallel training with the parameter server (PS)\narchitecture. This paper proposes ADSP, a parameter synchronization scheme for\ndistributed machine learning (ML) with heterogeneous edge systems. Eliminating\nthe significant waiting time occurring with existing parameter synchronization\nmodels, the core idea of ADSP is to let faster edge devices continue training,\nwhile committing their model updates at strategically decided intervals. We\ndesign algorithms that decide time points for each worker to commit its model\nupdate, and ensure not only global model convergence but also faster\nconvergence. Our testbed implementation and experiments show that ADSP\noutperforms existing parameter synchronization models significantly in terms of\nML model convergence time, scalability and adaptability to large heterogeneity.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 03:47:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hu", "Hanpeng", ""], ["Wang", "Dan", ""], ["Wu", "Chuan", ""]]}, {"id": "1911.06969", "submitter": "Xuhao Chen", "authors": "Xuhao Chen, Roshan Dathathri, Gurbinder Gill, Keshav Pingali", "title": "Pangolin: An Efficient and Flexible Graph Pattern Mining System on CPU\n  and GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in graph pattern mining (GPM) problems such as\nmotif counting. GPM systems have been developed to provide unified interfaces\nfor programming algorithms for these problems and for running them on parallel\nsystems. However, existing systems may take hours to mine even simple patterns\nin moderate-sized graphs, which significantly limits their real-world\nusability.\n  We present Pangolin, a high-performance and flexible in-memory GPM framework\ntargeting shared-memory CPUs and GPUs. Pangolin is the first GPM system that\nprovides high-level abstractions for GPU processing. It provides a simple\nprogramming interface based on the extend-reduce-filter model, which enables\nusers to specify application-specific knowledge for search space pruning and\nisomorphism test elimination. We describe novel optimizations that exploit\nlocality, reduce memory consumption, and mitigate the overheads of dynamic\nmemory allocation and synchronization.\n  Evaluation on a 28-core CPU demonstrates that Pangolin outperforms existing\nGPM frameworks Arabesque, RStream, and Fractal by 49x, 88x, and 80x on average,\nrespectively. Acceleration on a V100 GPU further improves performance of\nPangolin by 15x on average. Compared to state-of-the-art hand-optimized GPM\napplications, Pangolin provides competitive performance with less programming\neffort.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 05:59:22 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 21:08:54 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chen", "Xuhao", ""], ["Dathathri", "Roshan", ""], ["Gill", "Gurbinder", ""], ["Pingali", "Keshav", ""]]}, {"id": "1911.07154", "submitter": "Yasamin Nazari", "authors": "Yasamin Nazari", "title": "Sparse Hopsets in Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give the first Congested Clique algorithm that computes a sparse hopset\nwith polylogarithmic hopbound in polylogarithmic time. Given a graph $G=(V,E)$,\na $(\\beta,\\epsilon)$-hopset $H$ with \"hopbound\" $\\beta$, is a set of edges\nadded to $G$ such that for any pair of nodes $u$ and $v$ in $G$ there is a path\nwith at most $\\beta$ hops in $G \\cup H$ with length within $(1+\\epsilon)$ of\nthe shortest path between $u$ and $v$ in $G$.\n  Our hopsets are significantly sparser than the recent construction of\nCensor-Hillel et al. [6], that constructs a hopset of size\n$\\tilde{O}(n^{3/2})$, but with a smaller polylogarithmic hopbound. On the other\nhand, the previously known constructions of sparse hopsets with polylogarithmic\nhopbound in the Congested Clique model, proposed by Elkin and Neiman\n[10],[11],[12], all require polynomial rounds.\n  One tool that we use is an efficient algorithm that constructs an\n$\\ell$-limited neighborhood cover, that may be of independent interest.\n  Finally, as a side result, we also give a hopset construction in a variant of\nthe low-memory Massively Parallel Computation model, with improved running time\nover existing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 05:10:25 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 00:04:08 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Nazari", "Yasamin", ""]]}, {"id": "1911.07260", "submitter": "Yunming Zhang", "authors": "Yunming Zhang, Ajay Brahmakshatriya, Xinyi Chen, Laxman Dhulipala,\n  Shoaib Kamil, Saman Amarasinghe, Julian Shun", "title": "Optimizing Ordered Graph Algorithms with GraphIt", "comments": null, "journal-ref": "CGO 2020", "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph problems can be solved using ordered parallel graph algorithms\nthat achieve significant speedup over their unordered counterparts by reducing\nredundant work. This paper introduces a new priority-based extension to\nGraphIt, a domain-specific language for writing graph applications, to simplify\nwriting high-performance parallel ordered graph algorithms. The extension\nenables vertices to be processed in a dynamic order while hiding low-level\nimplementation details from the user. We extend the compiler with new program\nanalyses, transformations, and code generation to produce fast implementations\nof ordered parallel graph algorithms. We also introduce bucket fusion, a new\nperformance optimization that fuses together different rounds of ordered\nalgorithms to reduce synchronization overhead, resulting in\n$1.2\\times$--3$\\times$ speedup over the fastest existing ordered algorithm\nimplementations on road networks with large diameters. With the extension,\nGraphIt achieves up to 3$\\times$ speedup on six ordered graph algorithms over\nstate-of-the-art frameworks and hand-optimized implementations (Julienne,\nGalois, and GAPBS) that support ordered algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 15:51:02 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 23:37:14 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zhang", "Yunming", ""], ["Brahmakshatriya", "Ajay", ""], ["Chen", "Xinyi", ""], ["Dhulipala", "Laxman", ""], ["Kamil", "Shoaib", ""], ["Amarasinghe", "Saman", ""], ["Shun", "Julian", ""]]}, {"id": "1911.07298", "submitter": "Muhammad Samir Khan", "authors": "Muhammad Samir Khan, Lewis Tseng, Nitin H. Vaidya", "title": "Exact Byzantine Consensus on Arbitrary Directed Graphs under Local\n  Broadcast Model", "comments": "arXiv admin note: text overlap with arXiv:1903.11677", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Byzantine consensus in a synchronous system where nodes are\nconnected by a network modeled as a directed graph, i.e., communication links\nbetween neighboring nodes are not necessarily bi-directional. The directed\ngraph model is motivated by wireless networks wherein asymmetric communication\nlinks can occur. In the classical point-to-point communication model, a message\nsent on a communication link is private between the two nodes on the link. This\nallows a Byzantine faulty node to equivocate, i.e., send inconsistent\ninformation to its neighbors. This paper considers the local broadcast model of\ncommunication, wherein transmission by a node is received identically by all of\nits outgoing neighbors. This allows such neighbors to detect a faulty node's\nattempt to equivocate, effectively depriving the faulty nodes of the ability to\nsend conflicting information to different neighbors.\n  Prior work has obtained sufficient and necessary conditions on undirected\ngraphs to be able to achieve Byzantine consensus under the local broadcast\nmodel. In this paper, we obtain tight conditions on directed graphs to be able\nto achieve Byzantine consensus with binary inputs under the local broadcast\nmodel. The results obtained in the paper provide insights into the trade-off\nbetween directionality of communication and the ability to achieve consensus.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 06:59:51 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Khan", "Muhammad Samir", ""], ["Tseng", "Lewis", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1911.07351", "submitter": "Bishakh Chandra Ghosh", "authors": "Bishakh Chandra Ghosh, Sourav Kanti Addya, Nishant Baranwal Somy,\n  Shubha Brata Nath, Sandip Chakraborty, Soumya K Ghosh", "title": "Caching Techniques to Improve Latency in Serverless Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has gained a significant traction in recent times\nbecause of its simplicity of development, deployment and fine-grained billing.\nHowever, while implementing complex services comprising databases, file stores,\nor more than one serverless function, the performance in terms of latency of\nserving requests often degrades severely. In this work, we analyze different\nserverless architectures with AWS Lambda services and compare their performance\nin terms of latency with a traditional virtual machine (VM) based approach. We\nobserve that database access latency in serverless architecture is almost 14\ntimes than that in VM based setup. Further, we introduce some caching\nstrategies which can improve the response time significantly, and compare their\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 21:45:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ghosh", "Bishakh Chandra", ""], ["Addya", "Sourav Kanti", ""], ["Somy", "Nishant Baranwal", ""], ["Nath", "Shubha Brata", ""], ["Chakraborty", "Sandip", ""], ["Ghosh", "Soumya K", ""]]}, {"id": "1911.07384", "submitter": "Zehua Cheng", "authors": "Zehua Cheng, Weiyang Wang, Yan Pan, Thomas Lukasiewicz", "title": "Distributed Low Precision Training Without Mixed Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low precision training is one of the most popular strategies for deploying\nthe deep model on limited hardware resources. Fixed point implementation of\nDCNs has the potential to alleviate complexities and facilitate potential\ndeployment on embedded hardware. However, most low precision training solution\nis based on a mixed precision strategy. In this paper, we have presented an\nablation study on different low precision training strategy and propose a\nsolution for IEEE FP-16 format throughout the training process. We tested the\nResNet50 on 128 GPU cluster on ImageNet-full dataset. We have viewed that it is\nnot essential to use FP32 format to train the deep models. We have viewed that\ncommunication cost reduction, model compression, and large-scale distributed\ntraining are three coupled problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:56:52 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 05:37:54 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Cheng", "Zehua", ""], ["Wang", "Weiyang", ""], ["Pan", "Yan", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "1911.07444", "submitter": "Yujing Wang", "authors": "Yujing Wang, Qinyang Bao", "title": "A Code Injection Method for Rapid Docker Image Building", "comments": "3 pages to be sumbitted to IEEE mobisecserv 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Docker images are composed of multiple layers, each of which contains a set\nof instructions, and an archive of files. Layers allow Docker to separate a\nlarge build task into smaller ones, such that when a part of the program is\nchanged, only the corresponding layer needs to be changed. Yet the current\nimplementation has major inefficiencies that make the rebuilding of an image\nunnecessarily slow when changes in bottom layers are required: uneven content\ndistribution amongst layers, the need to rebuild an entire layer during update,\nand the rebuild fall-throughs in many cases. In this paper, we propose a code\ninjection method that overcomes these inefficiencies by targeting only the\nchanged layer and then bypassing the layer's content checksum. This process is\ndeveloped specifically for an interpreted language such as Python, where\nchanges can be detected explicitly via text diff tools and run as-is without\ncompilation. We then demonstrate that this method can accelerate the rebuild\ntime, effectively reducing the O(n) where n = size of layer rebuild time to\nO(1). Whereas for compiled languages, literal code injection cannot guarantee\nintegrity in compiled machine code. Expanding on the same code injection\nprinciple, multi-layer targeted code injection will be addressed in a future\ndiscussion.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:04:12 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 07:47:02 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 08:05:07 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wang", "Yujing", ""], ["Bao", "Qinyang", ""]]}, {"id": "1911.07449", "submitter": "Junfeng Li", "authors": "Junfeng Li, Sameer G. Kulkarni, K. K. Ramakrishnan, Dan Li", "title": "Understanding Open Source Serverless Platforms: Design Considerations\n  and Performance", "comments": null, "journal-ref": "Proceedings of the 5th International Workshop on Serverless\n  Computing, Pages 37-42, 2019", "doi": "10.1145/3366623.3368139", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is increasingly popular because of the promise of lower\ncost and the convenience it provides to users who do not need to focus on\nserver management. This has resulted in the availability of a number of\nproprietary and open-source serverless solutions. We seek to understand how the\nperformance of serverless computing depends on a number of design issues using\nseveral popular open-source serverless platforms. We identify the\nidiosyncrasies affecting performance (throughput and latency) for different\nopen-source serverless platforms. Further, we observe that just having either\nresource-based (CPU and memory) or workload-based (request per second (RPS) or\nconcurrent requests) auto-scaling is inadequate to address the needs of the\nserverless platforms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:42:15 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 20:17:34 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 18:53:02 GMT"}, {"version": "v4", "created": "Fri, 13 Dec 2019 00:46:38 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Li", "Junfeng", ""], ["Kulkarni", "Sameer G.", ""], ["Ramakrishnan", "K. K.", ""], ["Li", "Dan", ""]]}, {"id": "1911.07531", "submitter": "Ronald Kriemann", "authors": "Steffen B\\\"orm, Sven Christophersen, Ronald Kriemann", "title": "Semi-Automatic Task Graph Construction for $\\mathcal{H}$-Matrix\n  Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method to construct task graphs for \\mcH-matrix arithmetic is\nintroduced, which uses the information associated with all tasks of the\nstandard recursive \\mcH-matrix algorithms, e.g., the block index set of the\nmatrix blocks involved in the computation. Task refinement, i.e., the\nreplacement of tasks by sub-computations, is then used to proceed in the\n\\mcH-matrix hierarchy until the matrix blocks containing the actual matrix data\nare reached. This process is a natural extension of the classical, recursive\nway in which \\mcH-matrix arithmetic is defined and thereby simplifies the\nefficient usage of many-core systems. Examples for standard and accumulator\nbased \\mcH-arithmetic are shown for model problems with different block\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 10:36:43 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["B\u00f6rm", "Steffen", ""], ["Christophersen", "Sven", ""], ["Kriemann", "Ronald", ""]]}, {"id": "1911.07615", "submitter": "Jun Li", "authors": "Jun Li, Xiaoman Shen, Lei Chen and Jiajia Chen", "title": "Bandwidth Slicing to Boost Federated Learning in Edge Computing", "comments": "Conference,3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandwidth slicing is introduced to support federated learning in edge\ncomputing to assure low communication delay for training traffic. Results\nreveal that bandwidth slicing significantly improves training efficiency while\nachieving good learning accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 07:49:22 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Li", "Jun", ""], ["Shen", "Xiaoman", ""], ["Chen", "Lei", ""], ["Chen", "Jiajia", ""]]}, {"id": "1911.07652", "submitter": "Linara Adilova", "authors": "Linara Adilova, Julia Rosenzweig, Michael Kamp", "title": "Information-Theoretic Perspective of Federated Learning", "comments": "5 pages, 8 figures Workshop on Information Theory and Machine\n  Learning, 33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to distributed machine learning is to train models on local\ndatasets and aggregate these models into a single, stronger model. A popular\ninstance of this form of parallelization is federated learning, where the nodes\nperiodically send their local models to a coordinator that aggregates them and\nredistributes the aggregation back to continue training with it. The most\nfrequently used form of aggregation is averaging the model parameters, e.g.,\nthe weights of a neural network. However, due to the non-convexity of the loss\nsurface of neural networks, averaging can lead to detrimental effects and it\nremains an open question under which conditions averaging is beneficial. In\nthis paper, we study this problem from the perspective of information theory:\nWe measure the mutual information between representation and inputs as well as\nrepresentation and labels in local models and compare it to the respective\ninformation contained in the representation of the averaged model. Our\nempirical results confirm previous observations about the practical usefulness\nof averaging for neural networks, even if local dataset distributions vary\nstrongly. Furthermore, we obtain more insights about the impact of the\naggregation frequency on the information flow and thus on the success of\ndistributed learning. These insights will be helpful both in improving the\ncurrent synchronization process and in further understanding the effects of\nmodel aggregation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 13:51:27 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Adilova", "Linara", ""], ["Rosenzweig", "Julia", ""], ["Kamp", "Michael", ""]]}, {"id": "1911.07690", "submitter": "M. Hadi Amini", "authors": "Ahmed Imteaj, M. Hadi Amini, Javad Mohammadi", "title": "Leveraging Decentralized Artificial Intelligence to Enhance Resilience\n  of Energy Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reintroduces the notion of resilience in the context of recent\nissues originated from climate change triggered events including severe\nhurricanes and wildfires. A recent example is PG&E's forced power outage to\ncontain wildfire risk which led to widespread power disruption. This paper\nfocuses on answering two questions: who is responsible for resilience? and how\nto quantify the monetary value of resilience? To this end, we first provide\npreliminary definitions of resilience for power systems. We then investigate\nthe role of natural hazards, especially wildfire, on power system resilience.\nFinally, we will propose a decentralized strategy for a resilient management\nsystem using distributed storage and demand response resources. Our proposed\nhigh fidelity model provides utilities, operators, and policymakers with a\nclearer picture for strategic decision making and preventive decisions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:13:48 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Imteaj", "Ahmed", ""], ["Amini", "M. Hadi", ""], ["Mohammadi", "Javad", ""]]}, {"id": "1911.07722", "submitter": "Celestine Mendler-D\\\"unner", "authors": "Nikolas Ioannou, Celestine Mendler-D\\\"unner, Thomas Parnell", "title": "SySCD: A System-Aware Parallel Coordinate Descent Algorithm", "comments": "accepted as a spotlight at NeurIPS 2019, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel parallel stochastic coordinate descent (SCD)\nalgorithm with convergence guarantees that exhibits strong scalability. We\nstart by studying a state-of-the-art parallel implementation of SCD and\nidentify scalability as well as system-level performance bottlenecks of the\nrespective implementation. We then take a principled approach to develop a new\nSCD variant which is designed to avoid the identified system bottlenecks, such\nas limited scaling due to coherence traffic of model sharing across threads,\nand inefficient CPU cache accesses. Our proposed system-aware parallel\ncoordinate descent algorithm (SySCD) scales to many cores and across numa\nnodes, and offers a consistent bottom line speedup in training time of up to\nx12 compared to an optimized asynchronous parallel SCD algorithm and up to x42,\ncompared to state-of-the-art GLM solvers (scikit-learn, Vowpal Wabbit, and H2O)\non a range of datasets and multi-core CPU architectures.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:47:30 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ioannou", "Nikolas", ""], ["Mendler-D\u00fcnner", "Celestine", ""], ["Parnell", "Thomas", ""]]}, {"id": "1911.07763", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan, Xiufeng Liu, Mansaf Alam", "title": "A Spark ML driven preprocessing approach for deep learning based\n  scholarly data applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data has found applications in multiple domains. One of the largest\nsources of textual big data is scientific documents and papers. Big scholarly\ndata have been used in numerous ways to create innovative applications such as\ncollaborator discovery, expert finding and research management systems. With\nthe advent of advanced machine and deep learning techniques, the accuracy and\nnovelty of such applications have risen manifold. However, the biggest\nchallenge in the development of deep learning models for scholarly applications\nin cloud based environment is the underutilization of resources because of the\nexcessive time taken by textual preprocessing. This paper presents a\npreprocessing pipeline that makes use of Spark for data ingestion and Spark ML\nfor pipelining preprocessing tasks. The evaluation of the proposed work is done\nusing a case study, which uses LSTM based text summarization for generating\ntitle or summary from abstract of any research. The ingestion, preprocessing\nand cumulative time for the proposed approach (P3SAPP) is much lower than the\nconventional approach (CA), which manifests in reduction of costs as well.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 09:18:23 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Khan", "Samiya", ""], ["Liu", "Xiufeng", ""], ["Alam", "Mansaf", ""]]}, {"id": "1911.07787", "submitter": "Yu Feng", "authors": "Yu Feng, Yuhao Zhu", "title": "PES: Proactive Event Scheduling for Responsive and Energy-Efficient\n  Mobile Web Computing", "comments": "International Symposium on Computer Architecture (ISCA). 2019", "journal-ref": null, "doi": "10.1145/3307650.3322248", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web applications are gradually shifting toward resource-constrained mobile\ndevices. As a result, the Web runtime system must simultaneously address two\nchallenges: responsiveness and energy-efficiency. Conventional Web runtime\nsystems fall short due to their reactive nature: they react to a user event\nonly after it is triggered. The reactive strategy leads to local optimizations\nthat schedule event executions one at a time, missing global optimization\nopportunities. This paper proposes Proactive Event Scheduling (PES). The key\nidea of PES is to proactively anticipate future events and thereby globally\ncoordinate scheduling decisions across events. Specifically, PES predicts\nevents that are likely to happen in the near future using a combination of\nstatistical inference and application code analysis. PES then speculatively\nexecutes future events ahead of time in a way that satisfies the QoS\nconstraints of all the events while minimizing the global energy consumption.\nFundamentally, PES unlocks more optimization opportunities by enlarging the\nscheduling window, which enables coordination across both outstanding events\nand predicted events. Hardware measurements show that PES reduces the QoS\nviolation and energy consumption by 61.2% and 26.5%, respectively, over the\nAndroid default Interactive CPU governor. It also reduces the QoS violation and\nenergy consumption by 63.1% and 17.9%, respectively, compared to EBS, a\nstate-of-the-art reactive scheduler.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:34:01 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Feng", "Yu", ""], ["Zhu", "Yuhao", ""]]}, {"id": "1911.07966", "submitter": "Dragos-Adrian (Adi) Seredinschi PhD", "authors": "Rachid Guerraoui and Jad Hamza and Dragos-Adrian Seredinschi and Marko\n  Vukolic", "title": "Can 100 Machines Agree?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agreement protocols have been typically deployed at small scale, e.g., using\nthree to five machines. This is because these protocols seem to suffer from a\nsharp performance decay. More specifically, as the size of a deployment---i.e.,\ndegree of replication---increases, the protocol performance greatly decreases.\nThere is not much experimental evidence for this decay in practice, however,\nnotably for larger system sizes, e.g., beyond a handful of machines.\n  In this paper we execute agreement protocols on up to 100 machines and\nobserve on their performance decay. We consider well-known agreement protocols\npart of mature systems, such as Apache ZooKeeper, etcd, and BFT-Smart, as well\nas a chain and a novel ring-based agreement protocol which we implement\nourselves.\n  We provide empirical evidence that current agreement protocols execute\ngracefully on 100 machines. We observe that throughput decay is initially sharp\n(consistent with previous observations); but intriguingly---as each system\ngrows beyond a few tens of replicas---the decay dampens. For chain- and\nring-based replication, this decay is slower than for the other systems. The\npositive takeaway from our evaluation is that mature agreement protocol\nimplementations can sustain out-of-the-box 300 to 500 requests per second when\nexecuting on 100 replicas on a wide-area public cloud platform. Chain- and\nring-based replication can reach between 4K and 11K (up to 20x improvements)\ndepending on the fault assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:31:52 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Hamza", "Jad", ""], ["Seredinschi", "Dragos-Adrian", ""], ["Vukolic", "Marko", ""]]}, {"id": "1911.07971", "submitter": "Raj Kumar Maity", "authors": "Venkata Gandikota, Daniel Kane, Raj Kumar Maity, Arya Mazumdar", "title": "vqSGD: Vector Quantized Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a family of vector quantization schemes \\emph{vqSGD}\n(Vector-Quantized Stochastic Gradient Descent) that provide an asymptotic\nreduction in the communication cost with convergence guarantees in first-order\ndistributed optimization. In the process we derive the following fundamental\ninformation theoretic fact: $\\Theta(\\frac{d}{R^2})$ bits are necessary and\nsufficient to describe an unbiased estimator ${\\hat{g}}({g})$ for any ${g}$ in\nthe $d$-dimensional unit sphere, under the constraint that\n$\\|{\\hat{g}}({g})\\|_2\\le R$ almost surely. In particular, we consider a\nrandomized scheme based on the convex hull of a point set, that returns an\nunbiased estimator of a $d$-dimensional gradient vector with almost surely\nbounded norm. We provide multiple efficient instances of our scheme, that are\nnear optimal, and require only $o(d)$ bits of communication at the expense of\ntolerable increase in error. The instances of our quantization scheme are\nobtained using the properties of binary error-correcting codes and provide a\nsmooth tradeoff between the communication and the estimation error of\nquantization. Furthermore, we show that \\emph{vqSGD} also offers strong privacy\nguarantees.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:48:01 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 18:22:02 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 09:18:41 GMT"}, {"version": "v4", "created": "Thu, 24 Dec 2020 21:07:55 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Gandikota", "Venkata", ""], ["Kane", "Daniel", ""], ["Maity", "Raj Kumar", ""], ["Mazumdar", "Arya", ""]]}, {"id": "1911.08031", "submitter": "Abdul Dakkak", "authors": "Cheng Li, Abdul Dakkak, Jinjun Xiong, Wen-mei Hwu", "title": "The Design and Implementation of a Scalable DL Benchmarking Platform", "comments": null, "journal-ref": "2020 IEEE 13th International Conference on Cloud Computing\n  (CLOUD), 414-425", "doi": "10.1109/CLOUD49709.2020.00063", "report-no": null, "categories": "cs.DC cs.GL cs.LG cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current Deep Learning (DL) landscape is fast-paced and is rife with\nnon-uniform models, hardware/software (HW/SW) stacks, but lacks a DL\nbenchmarking platform to facilitate evaluation and comparison of DL\ninnovations, be it models, frameworks, libraries, or hardware. Due to the lack\nof a benchmarking platform, the current practice of evaluating the benefits of\nproposed DL innovations is both arduous and error-prone - stifling the adoption\nof the innovations.\n  In this work, we first identify $10$ design features which are desirable\nwithin a DL benchmarking platform. These features include: performing the\nevaluation in a consistent, reproducible, and scalable manner, being framework\nand hardware agnostic, supporting real-world benchmarking workloads, providing\nin-depth model execution inspection across the HW/SW stack levels, etc. We then\npropose MLModelScope, a DL benchmarking platform design that realizes the $10$\nobjectives. MLModelScope proposes a specification to define DL model\nevaluations and techniques to provision the evaluation workflow using the\nuser-specified HW/SW stack. MLModelScope defines abstractions for frameworks\nand supports board range of DL models and evaluation scenarios. We implement\nMLModelScope as an open-source project with support for all major frameworks\nand hardware architectures. Through MLModelScope's evaluation and automated\nanalysis workflows, we performed case-study analyses of $37$ models across $4$\nsystems and show how model, hardware, and framework selection affects model\naccuracy and performance under different benchmarking scenarios. We further\ndemonstrated how MLModelScope's tracing capability gives a holistic view of\nmodel execution and helps pinpoint bottlenecks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 01:16:08 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Cheng", ""], ["Dakkak", "Abdul", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1911.08182", "submitter": "Davide Grossi", "authors": "Andrea Bracciali, Davide Grossi, Ronald de Haan", "title": "Decentralization in Open Quorum Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralisation is one of the promises introduced by blockchain\ntechnologies: fair and secure interaction amongst peers with no dominant\npositions, single points of failure or censorship. Decentralisation, however,\nappears difficult to be formally defined, possibly a continuum property of\nsystems that can be more or less decentralised, or can tend to decentralisation\nin their lifetime. In this paper we focus on decentralisation in quorum-based\napproaches to open (permissionless) consensus as illustrated in influential\nprotocols such as the Ripple and Stellar protocols. Drawing from game theory\nand computational complexity, we establish limiting results concerning the\ndecentralisation vs. safety trade-off in Ripple and Stellar, and we propose a\nnovel methodology to formalise and quantitatively analyse decentralisation in\nthis type of blockchains.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 10:01:03 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Bracciali", "Andrea", ""], ["Grossi", "Davide", ""], ["de Haan", "Ronald", ""]]}, {"id": "1911.08250", "submitter": "Aritra Dutta", "authors": "Aritra Dutta, El Houcine Bergou, Ahmed M. Abdelmoniem, Chen-Yu Ho,\n  Atal Narayan Sahu, Marco Canini, Panos Kalnis", "title": "On the Discrepancy between the Theoretical Analysis and Practical\n  Implementations of Compressed Communication for Distributed Deep Learning", "comments": "To Appear In Proceedings of Thirty-Fourth AAAI Conference on\n  Artificial Intelligence, 2020", "journal-ref": "In Proceedings of Thirty-Fourth AAAI Conference on Artificial\n  Intelligence, 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed communication, in the form of sparsification or quantization of\nstochastic gradients, is employed to reduce communication costs in distributed\ndata-parallel training of deep neural networks. However, there exists a\ndiscrepancy between theory and practice: while theoretical analysis of most\nexisting compression methods assumes compression is applied to the gradients of\nthe entire model, many practical implementations operate individually on the\ngradients of each layer of the model. In this paper, we prove that layer-wise\ncompression is, in theory, better, because the convergence rate is upper\nbounded by that of entire-model compression for a wide range of biased and\nunbiased compression methods. However, despite the theoretical bound, our\nexperimental study of six well-known methods shows that convergence, in\npractice, may or may not be better, depending on the actual trained model and\ncompression ratio. Our findings suggest that it would be advantageous for deep\nlearning frameworks to include support for both layer-wise and entire-model\ncompression.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 13:22:22 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Dutta", "Aritra", ""], ["Bergou", "El Houcine", ""], ["Abdelmoniem", "Ahmed M.", ""], ["Ho", "Chen-Yu", ""], ["Sahu", "Atal Narayan", ""], ["Canini", "Marco", ""], ["Kalnis", "Panos", ""]]}, {"id": "1911.08356", "submitter": "Fabian Schuiki", "authors": "Fabian Schuiki, Florian Zaruba, Torsten Hoefler, Luca Benini", "title": "Stream Semantic Registers: A Lightweight RISC-V ISA Extension Achieving\n  Full Compute Utilization in Single-Issue Cores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-issue processor cores are very energy efficient but suffer from the\nvon Neumann bottleneck, in that they must explicitly fetch and issue the\nloads/storse necessary to feed their ALU/FPU. Each instruction spent on moving\ndata is a cycle not spent on computation, limiting ALU/FPU utilization to 33%\non reductions. We propose \"Stream Semantic Registers\" to boost utilization and\nincrease energy efficiency. SSR is a lightweight, non-invasive RISC-V ISA\nextension which implicitly encodes memory accesses as register reads/writes,\neliminating a large number of loads/stores. We implement the proposed extension\nin the RTL of an existing multi-core cluster and synthesize the design for a\nmodern 22nm technology. Our extension provides a significant, 2x to 5x,\narchitectural speedup across different kernels at a small 11% increase in core\narea. Sequential code runs 3x faster on a single core, and 3x fewer cores are\nneeded in a cluster to achieve the same performance. The utilization increase\nto almost 100% in leads to a 2x energy efficiency improvement in a multi-core\ncluster. The extension reduces instruction fetches by up to 3.5x and\ninstruction cache power consumption by up to 5.6x. Compilers can automatically\nmap loop nests to SSRs, making the changes transparent to the programmer.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:37:55 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 13:19:00 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 05:11:19 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Schuiki", "Fabian", ""], ["Zaruba", "Florian", ""], ["Hoefler", "Torsten", ""], ["Benini", "Luca", ""]]}, {"id": "1911.08394", "submitter": "Klaus Reuter", "authors": "Victor Artigues, Katharina Kormann, Markus Rampp, Klaus Reuter", "title": "Evaluation of performance portability frameworks for the implementation\n  of a particle-in-cell code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on an in-depth evaluation of the performance portability\nframeworks Kokkos and RAJA with respect to their suitability for the\nimplementation of complex particle-in-cell (PIC) simulation codes, extending\nprevious studies based on codes from other domains. At the example of a\nparticle-in-cell model, we implemented the hotspot of the code in C++ and\nparallelized it using OpenMP, OpenACC, CUDA, Kokkos, and RAJA, targeting\nmulti-core (CPU) and graphics (GPU) processors. Both, Kokkos and RAJA appear\nmature, are usable for complex codes, and keep their promise to provide\nperformance portability across different architectures. Comparing the\nobtainable performance on state-of-the art hardware, but also considering\naspects such as code complexity, feature availability, and overall\nproductivity, we finally draw the conclusion that the Kokkos framework would be\nsuited best to tackle the massively parallel implementation of the full PIC\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:54:19 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Artigues", "Victor", ""], ["Kormann", "Katharina", ""], ["Rampp", "Markus", ""], ["Reuter", "Klaus", ""]]}, {"id": "1911.08413", "submitter": "Shreshth Tuli", "authors": "Riccardo Mancini, Shreshth Tuli, Tommaso Cucinotta and Rajkumar Buyya", "title": "iGateLink: A Gateway Library for Linking IoT, Edge, Fog and Cloud\n  Computing Environments", "comments": null, "journal-ref": "International Conference on Intelligent and Cloud Computing, 2019", "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the Internet of Things (IoT) has been growing in popularity,\nalong with the increasingly important role played by IoT gateways, mediating\nthe interactions among a plethora of heterogeneous IoT devices and cloud\nservices. In this paper, we present iGateLink, an open-source Android library\neasing the development of Android applications acting as a gateway between IoT\ndevices and Edge/Fog/Cloud Computing environments. Thanks to its pluggable\ndesign, modules providing connectivity with a number of devices acting as data\nsources or Fog/Cloud frameworks can be easily reused for different\napplications. Using iGateLink in two case-studies replicating previous works in\nthe healthcare and image processing domains, the library proved to be effective\nin adapting to different scenarios and speeding up the development of gateway\napplications, as compared to the use of conventional methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 18:34:51 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Mancini", "Riccardo", ""], ["Tuli", "Shreshth", ""], ["Cucinotta", "Tommaso", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1911.08517", "submitter": "Mo Yu", "authors": "Xiang Ni, Jing Li, Mo Yu, Wang Zhou, Kun-Lung Wu", "title": "Generalizable Resource Allocation in Stream Processing via Deep\n  Reinforcement Learning", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of resource allocation in stream processing,\nwhere continuous data flows must be processed in real time in a large\ndistributed system. To maximize system throughput, the resource allocation\nstrategy that partitions the computation tasks of a stream processing graph\nonto computing devices must simultaneously balance workload distribution and\nminimize communication. Since this problem of graph partitioning is known to be\nNP-complete yet crucial to practical streaming systems, many heuristic-based\nalgorithms have been developed to find reasonably good solutions. In this\npaper, we present a graph-aware encoder-decoder framework to learn a\ngeneralizable resource allocation strategy that can properly distribute\ncomputation tasks of stream processing graphs unobserved from training data.\nWe, for the first time, propose to leverage graph embedding to learn the\nstructural information of the stream processing graphs. Jointly trained with\nthe graph-aware decoder using deep reinforcement learning, our approach can\neffectively find optimized solutions for unseen graphs. Our experiments show\nthat the proposed model outperforms both METIS, a state-of-the-art graph\npartitioning algorithm, and an LSTM-based encoder-decoder model, in about 70%\nof the test cases.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:25:42 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ni", "Xiang", ""], ["Li", "Jing", ""], ["Yu", "Mo", ""], ["Zhou", "Wang", ""], ["Wu", "Kun-Lung", ""]]}, {"id": "1911.08727", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Zhenheng Tang, Qiang Wang, Kaiyong Zhao, Xiaowen Chu", "title": "Layer-wise Adaptive Gradient Sparsification for Distributed Deep\n  Learning with Convergence Guarantees", "comments": "8 pages. To appear at ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the long training time of large deep neural network (DNN) models,\ndistributed synchronous stochastic gradient descent (S-SGD) is commonly used on\na cluster of workers. However, the speedup brought by multiple workers is\nlimited by the communication overhead. Two approaches, namely pipelining and\ngradient sparsification, have been separately proposed to alleviate the impact\nof communication overheads. Yet, the gradient sparsification methods can only\ninitiate the communication after the backpropagation, and hence miss the\npipelining opportunity. In this paper, we propose a new distributed\noptimization method named LAGS-SGD, which combines S-SGD with a novel\nlayer-wise adaptive gradient sparsification (LAGS) scheme. In LAGS-SGD, every\nworker selects a small set of \"significant\" gradients from each layer\nindependently whose size can be adaptive to the communication-to-computation\nratio of that layer. The layer-wise nature of LAGS-SGD opens the opportunity of\noverlapping communications with computations, while the adaptive nature of\nLAGS-SGD makes it flexible to control the communication time. We prove that\nLAGS-SGD has convergence guarantees and it has the same order of convergence\nrate as vanilla S-SGD under a weak analytical assumption. Extensive experiments\nare conducted to verify the analytical assumption and the convergence\nperformance of LAGS-SGD. Experimental results on a 16-GPU cluster show that\nLAGS-SGD outperforms the original S-SGD and existing sparsified S-SGD without\nlosing obvious model accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 06:24:50 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 06:46:41 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 14:22:12 GMT"}, {"version": "v4", "created": "Mon, 2 Mar 2020 01:54:25 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Shi", "Shaohuai", ""], ["Tang", "Zhenheng", ""], ["Wang", "Qiang", ""], ["Zhao", "Kaiyong", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1911.08772", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Xiaowen Chu, Ka Chun Cheung, Simon See", "title": "Understanding Top-k Sparsification in Distributed Deep Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed stochastic gradient descent (SGD) algorithms are widely deployed\nin training large-scale deep learning models, while the communication overhead\namong workers becomes the new system bottleneck. Recently proposed gradient\nsparsification techniques, especially Top-$k$ sparsification with error\ncompensation (TopK-SGD), can significantly reduce the communication traffic\nwithout an obvious impact on the model accuracy. Some theoretical studies have\nbeen carried out to analyze the convergence property of TopK-SGD. However,\nexisting studies do not dive into the details of Top-$k$ operator in gradient\nsparsification and use relaxed bounds (e.g., exact bound of Random-$k$) for\nanalysis; hence the derived results cannot well describe the real convergence\nperformance of TopK-SGD. To this end, we first study the gradient distributions\nof TopK-SGD during the training process through extensive experiments. We then\ntheoretically derive a tighter bound for the Top-$k$ operator. Finally, we\nexploit the property of gradient distribution to propose an approximate top-$k$\nselection algorithm, which is computing-efficient for GPUs, to improve the\nscaling efficiency of TopK-SGD by significantly reducing the computing\noverhead. Codes are available at:\n\\url{https://github.com/hclhkbu/GaussianK-SGD}.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 08:50:59 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Shi", "Shaohuai", ""], ["Chu", "Xiaowen", ""], ["Cheung", "Ka Chun", ""], ["See", "Simon", ""]]}, {"id": "1911.08779", "submitter": "Zheng Wang", "authors": "Donglin Chen, Jianbin Fang, Chuanfu Xu, Shizhao Chen, Zheng Wang", "title": "Characterizing Scalability of Sparse Matrix-Vector Multiplications on\n  Phytium FT-2000+ Many-cores", "comments": "Accepted to be published at IJPP", "journal-ref": null, "doi": "10.1007/s10766-019-00646-x", "report-no": null, "categories": "cs.DC cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the scalability of parallel programs is crucial for software\noptimization and hardware architecture design. As HPC hardware is moving\ntowards many-core design, it becomes increasingly difficult for a parallel\nprogram to make effective use of all available processor cores. This makes\nscalability analysis increasingly important. This paper presents a quantitative\nstudy for characterizing the scalability of sparse matrix-vector\nmultiplications (SpMV) on Phytium FT-2000+, an ARM-based many-core architecture\nfor HPC computing. We choose to study SpMV as it is a common operation in\nscientific and HPC applications. Due to the newness of ARM-based many-core\narchitectures, there is little work on understanding the SpMV scalability on\nsuch hardware design. To close the gap, we carry out a large-scale empirical\nevaluation involved over 1,000 representative SpMV datasets. We show that,\nwhile many computation-intensive SpMV applications contain extensive\nparallelism, achieving a linear speedup is non-trivial on Phytium FT-2000+. To\nbetter understand what software and hardware parameters are most important for\ndetermining the scalability of a given SpMV kernel, we develop a performance\nanalytical model based on the regression tree. We show that our model is highly\neffective in characterizing SpMV scalability, offering useful insights to help\napplication developers for better optimizing SpMV on an emerging HPC\narchitecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 09:12:58 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Chen", "Donglin", ""], ["Fang", "Jianbin", ""], ["Xu", "Chuanfu", ""], ["Chen", "Shizhao", ""], ["Wang", "Zheng", ""]]}, {"id": "1911.08787", "submitter": "Sebastian Mueller", "authors": "Angelo Capossele, Sebastian Mueller, Andreas Penzkofer", "title": "Robustness and efficiency of leaderless probabilistic consensus\n  protocols within Byzantine infrastructures", "comments": "21 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates leaderless binary majority consensus protocols with\nlow computational complexity in noisy Byzantine infrastructures. Using computer\nsimulations, we show that explicit randomization of the consensus protocol can\nsignificantly increase the robustness towards faulty and malicious nodes. We\nidentify the optimal amount of randomness for various Byzantine attack\nstrategies on different kinds of network topologies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 09:32:31 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Capossele", "Angelo", ""], ["Mueller", "Sebastian", ""], ["Penzkofer", "Andreas", ""]]}, {"id": "1911.08803", "submitter": "Oguzhan Ersoy", "authors": "Oguzhan Ersoy, Stefanie Roos and Zekeriya Erkin", "title": "How to profit from payments channels", "comments": "Financial Cryptography and Data Security (FC) 2020", "journal-ref": null, "doi": null, "report-no": "02: A typo in one of the authors name is corrected", "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment channel networks like Bitcoin's Lightning network are an auspicious\napproach for realizing high transaction throughput and almost-instant\nconfirmations in blockchain networks. However, the ability to successfully make\npayments in such networks relies on the willingness of participants to lock\ncollateral in the network. In Lightning, the key financial incentive is to lock\ncollateral are small fees for routing payments for other participants. While\nusers can choose these fees, currently, they mainly stick to the default fees.\nBy providing insights on beneficial choices for fees, we aim to incentivize\nusers to lock more collateral and improve the effectiveness of the network.\n  In this paper, we consider a node $\\mathbf{A}$ that given the network\ntopology and the channel details selects where to establish channels and how\nmuch fee to charge such that its financial gain is maximized. We formalize the\noptimization problem and show that it is NP-hard. We design a greedy algorithm\nto approximate the optimal solution. In each step, our greedy algorithm selects\na node which maximizes the total reward concerning the number of shortest paths\npassing through $\\mathbf{A}$ and channel fees. Our simulation study leverages\nreal-world data set to quantify the impact of our gain optimization and\nindicates that our strategy is at least a factor two better than other\nstrategies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 10:30:27 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 19:23:21 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Ersoy", "Oguzhan", ""], ["Roos", "Stefanie", ""], ["Erkin", "Zekeriya", ""]]}, {"id": "1911.08905", "submitter": "Ke He", "authors": "Ke He, Bo Liu, Yu Zhang, Andrew Ling and Dian Gu", "title": "FeCaffe: FPGA-enabled Caffe with OpenCL for Deep Learning Training and\n  Inference on Intel Stratix 10", "comments": "11 pages, 7 figures and 4 tables", "journal-ref": "FPGA 2020 The 2020 ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays", "doi": "10.1145/3373087.3375389", "report-no": "314", "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning and Convolutional Neural Network (CNN) have becoming\nincreasingly more popular and important in both academic and industrial areas\nin recent years cause they are able to provide better accuracy and result in\nclassification, detection and recognition areas, compared to traditional\napproaches. Currently, there are many popular frameworks in the market for deep\nlearning development, such as Caffe, TensorFlow, Pytorch, and most of\nframeworks natively support CPU and consider GPU as the mainline accelerator by\ndefault. FPGA device, viewed as a potential heterogeneous platform, still\ncannot provide a comprehensive support for CNN development in popular\nframeworks, in particular to the training phase. In this paper, we firstly\npropose the FeCaffe, i.e. FPGA-enabled Caffe, a hierarchical software and\nhardware design methodology based on the Caffe to enable FPGA to support\nmainline deep learning development features, e.g. training and inference with\nCaffe. Furthermore, we provide some benchmarks with FeCaffe by taking some\nclassical CNN networks as examples, and further analysis of kernel execution\ntime in details accordingly. Finally, some optimization directions including\nFPGA kernel design, system pipeline, network architecture, user case\napplication and heterogeneous platform levels, have been proposed gradually to\nimprove FeCaffe performance and efficiency. The result demonstrates the\nproposed FeCaffe is capable of supporting almost full features during CNN\nnetwork training and inference respectively with high degree of design\nflexibility, expansibility and reusability for deep learning development.\nCompared to prior studies, our architecture can support more network and\ntraining settings, and current configuration can achieve 6.4x and 8.4x average\nexecution time improvement for forward and backward respectively for LeNet.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:52:26 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["He", "Ke", ""], ["Liu", "Bo", ""], ["Zhang", "Yu", ""], ["Ling", "Andrew", ""], ["Gu", "Dian", ""]]}, {"id": "1911.08907", "submitter": "Ruobing Han", "authors": "Ruobing Han, James Demmel, Yang You", "title": "Auto-Precision Scaling for Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been reported that the communication cost for synchronizing gradients\ncan be a bottleneck, which limits the scalability of distributed deep learning.\nUsing low-precision gradients is a promising technique for reducing the\nbandwidth requirement. In this work, we propose Auto Precision Scaling (APS),\nan algorithm that can improve the accuracy when we communicate gradients by\nlow-precision floating-point values. APS can improve the accuracy for all\nprecisions with a trivial communication cost. Our experimental results show\nthat for many applications, APS can train state-of-the-art models by 8-bit\ngradients with no or only a tiny accuracy loss (<0.05%). Furthermore, we can\navoid any accuracy loss by designing a hybrid-precision technique. Finally, we\npropose a performance model to evaluate the proposed method. Our experimental\nresults show that APS can get a significant speedup over state-of-the-art\nmethods. To make it available to researchers and developers, we design and\nimplement CPD (Customized-Precision Deep Learning) system, which can simulate\nthe training process using an arbitrary low-precision customized floating-point\nformat. We integrate CPD into PyTorch and make it open-source.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 13:41:45 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 04:39:01 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 11:08:55 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Han", "Ruobing", ""], ["Demmel", "James", ""], ["You", "Yang", ""]]}, {"id": "1911.08963", "submitter": "Francisco Igual", "authors": "Gregorio Quintana-Ort\\'i and Fernando Hernando and Francisco D. Igual", "title": "Parallel Implementations for Computing the Minimum Distance of a Random\n  Linear Code on Multicomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum distance of a linear code is a key concept in information theory.\nTherefore, the time required by its computation is very important to many\nproblems in this area. In this paper, we introduce a family of implementations\nof the Brouwer-Zimmermann algorithm for distributed-memory architectures for\ncomputing the minimum distance of a random linear code over F2. Both current\ncommercial and public-domain software only work on either unicore architectures\nor shared-memory architectures, which are limited in the number of\ncores/processors employed in the computation. Our implementations focus on\ndistributed-memory architectures, thus being able to employ hundreds or even\nthousands of cores in the computation of the minimum distance. Our experimental\nresults show that our implementations are much faster, even up to several\norders of magnitude, than current implementations widely used nowadays.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:24:23 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Quintana-Ort\u00ed", "Gregorio", ""], ["Hernando", "Fernando", ""], ["Igual", "Francisco D.", ""]]}, {"id": "1911.09030", "submitter": "Cong Xie", "authors": "Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, Haibin Lin", "title": "Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with\n  Adaptive Learning Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When scaling distributed training, the communication overhead is often the\nbottleneck. In this paper, we propose a novel SGD variant with reduced\ncommunication and adaptive learning rates. We prove the convergence of the\nproposed algorithm for smooth but non-convex problems. Empirical results show\nthat the proposed algorithm significantly reduces the communication overhead,\nwhich, in turn, reduces the training time by up to 30% for the 1B word dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 16:58:40 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 00:26:57 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Oluwasanmi", ""], ["Gupta", "Indranil", ""], ["Lin", "Haibin", ""]]}, {"id": "1911.09135", "submitter": "Vishwesh Jatala", "authors": "Vishwesh Jatala, Loc Hoang, Roshan Dathathri, Gurbinder Gill, V\n  Krishna Nandivada, Keshav Pingali", "title": "An Adaptive Load Balancer For Graph Analytical Applications on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load-balancing among the threads of a GPU for graph analytics workloads is\ndifficult because of the irregular nature of graph applications and the high\nvariability in vertex degrees, particularly in power-law graphs. We describe a\nnovel load balancing scheme to address this problem. Our scheme is implemented\nin the IrGL compiler to allow users to generate efficient load balanced code\nfor a GPU from high-level sequential programs. We evaluated several graph\nanalytics applications on up to 16 distributed GPUs using IrGL to compile the\ncode and the Gluon substrate for inter-GPU communication. Our experiments show\nthat this scheme can achieve an average speed-up of 2.2x on inputs that suffer\nfrom severe load imbalance problems when previous state-of-the-art\nload-balancing schemes are used.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 19:14:45 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 18:09:32 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Jatala", "Vishwesh", ""], ["Hoang", "Loc", ""], ["Dathathri", "Roshan", ""], ["Gill", "Gurbinder", ""], ["Nandivada", "V Krishna", ""], ["Pingali", "Keshav", ""]]}, {"id": "1911.09208", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Sajjad Rahnama, Mohammad Sadoghi", "title": "Permissioned Blockchain Through the Looking Glass: Architectural and\n  Implementation Lessons Learned", "comments": "To appear in the proceedings of 40th IEEE International Conference on\n  Distributed Computing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the inception of Bitcoin, the distributed systems community has shown\ninterest in the design of efficient blockchain systems. However, initial\nblockchain applications (like Bitcoin) attain very low throughput, which has\npromoted the design of permissioned blockchain systems. These permissioned\nblockchain systems employ classical Byzantine-Fault Tolerant (BFT) protocols to\nreach consensus. However, existing permissioned blockchain systems still attain\nlow throughputs (of the order 10K txns/s). As a result, existing works blame\nthis low throughput on the associated BFT protocol and expend resources in\ndeveloping optimized protocols. We believe such blames only depict a one-sided\nstory. In specific, we raise a simple question: can a well-crafted system based\non a classical BFT protocol outperform a modern protocol? We show that\ndesigning such a well-crafted system is possible and illustrate that even if\nsuch a system employs a three-phase protocol, it can outperform another system\nutilizing a single-phase protocol. This endeavor requires us to dissect a\npermissioned blockchain system and highlight different factors that affect its\nperformance. Based on our insights, we present the design of our\nenterprise-grade, high-throughput yielding permissioned blockchain system,\nResilientDB, that employs multi-threaded deep pipelines, to balance tasks at a\nreplica, and provides guidelines for future designs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 23:07:53 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 04:42:18 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Gupta", "Suyash", ""], ["Rahnama", "Sajjad", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1911.09447", "submitter": "Gregor Ulm", "authors": "Gregor Ulm, Simon Smith, Adrian Nilsson, Emil Gustavsson, Mats\n  Jirstrand", "title": "S-RASTER: Contraction Clustering for Evolving Data Streams", "comments": "24 pages, 5 figures, 2 tables", "journal-ref": "Journal of Big Data (Springer) Vol. 7, Article number: 62 (2020)", "doi": "10.1186/s40537-020-00336-3", "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contraction Clustering (RASTER) is a single-pass algorithm for density-based\nclustering of 2D data. It can process arbitrary amounts of data in linear time\nand in constant memory, quickly identifying approximate clusters. It also\nexhibits good scalability in the presence of multiple CPU cores. RASTER\nexhibits very competitive performance compared to standard clustering\nalgorithms, but at the cost of decreased precision. Yet, RASTER is limited to\nbatch processing and unable to identify clusters that only exist temporarily.\nIn contrast, S-RASTER is an adaptation of RASTER to the stream processing\nparadigm that is able to identify clusters in evolving data streams. This\nalgorithm retains the main benefits of its parent algorithm, i.e. single-pass\nlinear time cost and constant memory requirements for each discrete time step\nwithin a sliding window. The sliding window is efficiently pruned, and\nclustering is still performed in linear time. Like RASTER, S-RASTER trades off\nan often negligible amount of precision for speed. Our evaluation shows that\ncompeting algorithms are at least 50% slower. Furthermore, S-RASTER shows good\nqualitative results, based on standard metrics. It is very well suited to\nreal-world scenarios where clustering does not happen continually but only\nperiodically.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 13:01:43 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 12:57:20 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 22:48:06 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 19:38:21 GMT"}, {"version": "v5", "created": "Wed, 16 Sep 2020 12:43:59 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ulm", "Gregor", ""], ["Smith", "Simon", ""], ["Nilsson", "Adrian", ""], ["Gustavsson", "Emil", ""], ["Jirstrand", "Mats", ""]]}, {"id": "1911.09561", "submitter": "Oliviero Riganelli", "authors": "Leonardo Mariani, Mauro Pezz\\`e, Oliviero Riganelli, Rui Xin", "title": "Predicting Failures in Multi-Tier Distributed Systems", "comments": "Accepted for publication in Journal of Systems and Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications are implemented as multi-tier software systems, and are\nexecuted on distributed infrastructures, like cloud infrastructures, to benefit\nfrom the cost reduction that derives from dynamically allocating resources\non-demand. In these systems, failures are becoming the norm rather than the\nexception, and predicting their occurrence, as well as locating the responsible\nfaults, are essential enablers of preventive and corrective actions that can\nmitigate the impact of failures, and significantly improve the dependability of\nthe systems. Current failure prediction approaches suffer either from false\npositives or limited accuracy, and do not produce enough information to\neffectively locate the responsible faults. In this paper, we present PreMiSE, a\nlightweight and precise approach to predict failures and locate the\ncorresponding faults in multi-tier distributed systems. PreMiSE blends\nanomaly-based and signature-based techniques to identify multi-tier failures\nthat impact on performance indicators, with high precision and low false\npositive rate. The experimental results that we obtained on a Cloud-based IP\nMultimedia Subsystem indicate that PreMiSE can indeed predict and locate\npossible failure occurrences with high precision and low overhead.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 15:55:09 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Mariani", "Leonardo", ""], ["Pezz\u00e8", "Mauro", ""], ["Riganelli", "Oliviero", ""], ["Xin", "Rui", ""]]}, {"id": "1911.09671", "submitter": "Yuanhao Wei", "authors": "Guy E. Blelloch, Yuanhao Wei", "title": "LL/SC and Atomic Copy: Constant Time, Space Efficient Implementations\n  using only pointer-width CAS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing concurrent algorithms, Load-Link/Store-Conditional (LL/SC) is\noften the ideal primitive to have because unlike Compare and Swap (CAS), LL/SC\nis immune to the ABA problem. However, the full semantics of LL/SC are not\nsupported by any modern machine, so there has been a significant amount of work\non simulations of LL/SC using Compare and Swap (CAS), a synchronization\nprimitive that enjoys widespread hardware support. All of the algorithms so far\nthat are constant time either use unbounded sequence numbers (and thus base\nobjects of unbounded size), or require $\\Omega(MP)$ space for $M$ LL/SC object\n(where $P$ is the number of processes). We present a constant time\nimplementation of $M$ LL/SC objects using $\\Theta(M+kP^2)$ space, where $k$ is\nthe maximum number of overlapping LL/SC operations per process (usually a\nconstant), and requiring only pointer-sized CAS objects. Our implementation can\nalso be used to implement $L$-word $LL/SC$ objects in $\\Theta(L)$ time (for\nboth $LL$ and $SC$) and $\\Theta((M+kP^2)L)$ space. To achieve these bounds, we\nbegin by implementing a new primitive called Single-Writer Copy which takes a\npointer to a word sized memory location and atomically copies its contents into\nanother object. The restriction is that only one process is allowed to\nwrite/copy into the destination object at a time. We believe this primitive\nwill be very useful in designing other concurrent algorithms as well.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:57:25 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 19:45:42 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 14:18:50 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Wei", "Yuanhao", ""]]}, {"id": "1911.09721", "submitter": "Raj Kumar Maity", "authors": "Avishek Ghosh, Raj Kumar Maity, Swanand Kadhe, Arya Mazumdar and\n  Kannan Ramchandran", "title": "Communication-Efficient and Byzantine-Robust Distributed Learning with\n  Error Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a communication-efficient distributed learning algorithm that is\nrobust against Byzantine worker machines. We propose and analyze a distributed\ngradient-descent algorithm that performs a simple thresholding based on\ngradient norms to mitigate Byzantine failures. We show the (statistical)\nerror-rate of our algorithm matches that of Yin et al.~\\cite{dong}, which uses\nmore complicated schemes (coordinate-wise median, trimmed mean). Furthermore,\nfor communication efficiency, we consider a generic class of\n$\\delta$-approximate compressors from Karimireddi et al.~\\cite{errorfeed} that\nencompasses sign-based compressors and top-$k$ sparsification. Our algorithm\nuses compressed gradients and gradient norms for aggregation and Byzantine\nremoval respectively. We establish the statistical error rate for non-convex\nsmooth loss functions. We show that, in certain range of the compression factor\n$\\delta$, the (order-wise) rate of convergence is not affected by the\ncompression operation. Moreover, we analyze the compressed gradient descent\nalgorithm with error feedback (proposed in \\cite{errorfeed}) in a distributed\nsetting and in the presence of Byzantine worker machines. We show that\nexploiting error feedback improves the statistical error rate. Finally, we\nexperimentally validate our results and show good performance in convergence\nfor convex (least-square regression) and non-convex (neural network training)\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 19:39:53 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 20:04:58 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 20:27:47 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 20:21:26 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ghosh", "Avishek", ""], ["Maity", "Raj Kumar", ""], ["Kadhe", "Swanand", ""], ["Mazumdar", "Arya", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1911.09747", "submitter": "Yu Ye", "authors": "Yu Ye, Hao Chen, Zheng Ma, Ming Xiao", "title": "Decentralized Consensus Optimization Based on Parallel Random Walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) has recently been\nrecognized as a promising approach for large-scale machine learning models.\nHowever, very few results study ADMM from the aspect of communication costs,\nespecially jointly with running time. In this letter, we investigate the\ncommunication efficiency and running time of ADMM in solving the consensus\noptimization problem over decentralized networks. We first review the effort of\nrandom walk ADMM (W-ADMM), which reduces communication costs at the expense of\nrunning time. To accelerate the convergence speed of W-ADMM, we propose the\nparallel random walk ADMM (PW-ADMM) algorithm, where multiple random walks are\nactive at the same time. Moreover, to further reduce the running time of\nPW-ADMM, the intelligent parallel random walk ADMM (IPW-ADMM) algorithm is\nproposed through integrating the \\textit{Random Walk with Choice} with PW-ADMM.\nBy numerical results from simulations, we demonstrate that the proposed\nalgorithms can be both communication efficient and fast in running speed\ncompared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 21:08:08 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Ye", "Yu", ""], ["Chen", "Hao", ""], ["Ma", "Zheng", ""], ["Xiao", "Ming", ""]]}, {"id": "1911.09824", "submitter": "Ren Bing", "authors": "Shengwen Yang, Bing Ren, Xuhui Zhou, Liping Liu", "title": "Parallel Distributed Logistic Regression for Vertical Federated Learning\n  without Third-Party Coordinator", "comments": "IJCAI-19 Workshop on Federated Machine Learning for User Privacy and\n  Data Confidentiality (IJCAI (FML)) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is a new distributed learning mechanism which allows model\ntraining on a large corpus of decentralized data owned by different data\nproviders, without sharing or leakage of raw data. According to the\ncharacteristics of data dis-tribution, it could be usually classified into\nthree categories: horizontal federated learning, vertical federated learning,\nand federated transfer learning. In this paper we present a solution for\nparallel dis-tributed logistic regression for vertical federated learning. As\ncompared with existing works, the role of third-party coordinator is removed in\nour proposed solution. The system is built on the pa-rameter server\narchitecture and aims to speed up the model training via utilizing a cluster of\nservers in case of large volume of training data. We also evaluate the\nperformance of the parallel distributed model training and the experimental\nresults show the great scalability of the system.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 02:57:01 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Yang", "Shengwen", ""], ["Ren", "Bing", ""], ["Zhou", "Xuhui", ""], ["Liu", "Liping", ""]]}, {"id": "1911.09829", "submitter": "Hasan Al Maruf", "authors": "Hasan Al Maruf, Mosharaf Chowdhury", "title": "Effectively Prefetching Remote Memory with Leap", "comments": null, "journal-ref": "2020 USENIX Annual Technical Conference, USENIX ATC 2020, July\n  15-17, pages: 843--857", "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory disaggregation over RDMA can improve the performance of\nmemory-constrained applications by replacing disk swapping with remote memory\naccesses. However, state-of-the-art memory disaggregation solutions still use\ndata path components designed for slow disks. As a result, applications\nexperience remote memory access latency significantly higher than that of the\nunderlying low-latency network, which itself is too high for many applications.\n  In this paper, we propose Leap, a prefetching solution for remote memory\naccesses due to memory disaggregation. At its core, Leap employs an online,\nmajority-based prefetching algorithm, which increases the page cache hit rate.\nWe complement it with a lightweight and efficient data path in the kernel that\nisolates each application's data path to the disaggregated memory and mitigates\nlatency bottlenecks arising from legacy throughput-optimizing operations.\nIntegration of Leap in the Linux kernel improves the median and tail remote\npage access latencies of memory-bound applications by up to 104.04x and 22.62x,\nrespectively, over the default data path. This leads to up to 10.16x\nperformance improvements for applications using disaggregated memory in\ncomparison to the state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 03:29:52 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Maruf", "Hasan Al", ""], ["Chowdhury", "Mosharaf", ""]]}, {"id": "1911.09849", "submitter": "Arjun Singhvi", "authors": "Arjun Singhvi, Kevin Houck, Arjun Balasubramanian, Mohammed Danish\n  Shaikh, Shivaram Venkataraman and Aditya Akella", "title": "Archipelago: A Scalable Low-Latency Serverless Platform", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased use of micro-services to build web applications has spurred the\nrapid growth of Function-as-a-Service (FaaS) or serverless computing platforms.\nWhile FaaS simplifies provisioning and scaling for application developers, it\nintroduces new challenges in resource management that need to be handled by the\ncloud provider. Our analysis of popular serverless workloads indicates that\nschedulers need to handle functions that are very short-lived, have\nunpredictable arrival patterns, and require expensive setup of sandboxes. The\nchallenge of running a large number of such functions in a multi-tenant cluster\nmakes existing scheduling frameworks unsuitable.\n  We present Archipelago, a platform that enables low latency request execution\nin a multi-tenant serverless setting. Archipelago views each application as a\nDAG of functions, and every DAG in associated with a latency deadline.\nArchipelago achieves its per-DAG request latency goals by: (1) partitioning a\ngiven cluster into a number of smaller worker pools, and associating each pool\nwith a semi-global scheduler (SGS), (2) using a latency-aware scheduler within\neach SGS along with proactive sandbox allocation to reduce overheads, and (3)\nusing a load balancing layer to route requests for different DAGs to the\nappropriate SGS, and automatically scale the number of SGSs per DAG. Our\ntestbed results show that Archipelago meets the latency deadline for more than\n99% of realistic application request workloads, and reduces tail latencies by\nup to 36X compared to state-of-the-art serverless platforms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 04:28:03 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Singhvi", "Arjun", ""], ["Houck", "Kevin", ""], ["Balasubramanian", "Arjun", ""], ["Shaikh", "Mohammed Danish", ""], ["Venkataraman", "Shivaram", ""], ["Akella", "Aditya", ""]]}, {"id": "1911.09925", "submitter": "Hasan Genc", "authors": "Hasan Genc, Seah Kim, Alon Amid, Ameer Haj-Ali, Vighnesh Iyer, Pranav\n  Prakash, Jerry Zhao, Daniel Grubb, Harrison Liew, Howard Mao, Albert Ou,\n  Colin Schmidt, Samuel Steffl, John Wright, Ion Stoica, Jonathan Ragan-Kelley,\n  Krste Asanovic, Borivoje Nikolic, Yakun Sophia Shao", "title": "Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via\n  Full-Stack Integration", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN accelerators are often developed and evaluated in isolation without\nconsidering the cross-stack, system-level effects in real-world environments.\nThis makes it difficult to appreciate the impact of System-on-Chip (SoC)\nresource contention, OS overheads, and programming-stack inefficiencies on\noverall performance/energy-efficiency. To address this challenge, we present\nGemmini, an open-source*, full-stack DNN accelerator generator. Gemmini\ngenerates a wide design-space of efficient ASIC accelerators from a flexible\narchitectural template, together with flexible programming stacks and full SoCs\nwith shared resources that capture system-level effects. Gemmini-generated\naccelerators have also been fabricated, delivering up to three\norders-of-magnitude speedups over high-performance CPUs on various DNN\nbenchmarks.\n  * https://github.com/ucb-bar/gemmini\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 08:51:28 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 10:33:50 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 06:53:12 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Genc", "Hasan", ""], ["Kim", "Seah", ""], ["Amid", "Alon", ""], ["Haj-Ali", "Ameer", ""], ["Iyer", "Vighnesh", ""], ["Prakash", "Pranav", ""], ["Zhao", "Jerry", ""], ["Grubb", "Daniel", ""], ["Liew", "Harrison", ""], ["Mao", "Howard", ""], ["Ou", "Albert", ""], ["Schmidt", "Colin", ""], ["Steffl", "Samuel", ""], ["Wright", "John", ""], ["Stoica", "Ion", ""], ["Ragan-Kelley", "Jonathan", ""], ["Asanovic", "Krste", ""], ["Nikolic", "Borivoje", ""], ["Shao", "Yakun Sophia", ""]]}, {"id": "1911.10071", "submitter": "Aleksei Triastcyn", "authors": "Aleksei Triastcyn, Boi Faltings", "title": "Federated Learning with Bayesian Differential Privacy", "comments": "Accepted at 2019 IEEE International Conference on Big Data (IEEE Big\n  Data 2019). 10 pages, 2 figures, 4 tables. arXiv admin note: text overlap\n  with arXiv:1901.09697", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9005465", "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reinforcing federated learning with formal privacy\nguarantees. We propose to employ Bayesian differential privacy, a relaxation of\ndifferential privacy for similarly distributed data, to provide sharper privacy\nloss bounds. We adapt the Bayesian privacy accounting method to the federated\nsetting and suggest multiple improvements for more efficient privacy budgeting\nat different levels. Our experiments show significant advantage over the\nstate-of-the-art differential privacy bounds for federated learning on image\nclassification tasks, including a medical application, bringing the privacy\nbudget below 1 at the client level, and below 0.1 at the instance level. Lower\namounts of noise also benefit the model accuracy and reduce the number of\ncommunication rounds.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 14:54:04 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Triastcyn", "Aleksei", ""], ["Faltings", "Boi", ""]]}, {"id": "1911.10175", "submitter": "Houxiang Ji", "authors": "Zhangxiaowen Gong, Houxiang Ji, Christopher Fletcher, Christopher\n  Hughes, Josep Torrellas", "title": "SparseTrain:Leveraging Dynamic Sparsity in Training DNNs on\n  General-Purpose SIMD Processors", "comments": null, "journal-ref": null, "doi": "10.1145/3410463.3414655", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our community has greatly improved the efficiency of deep learning\napplications, including by exploiting sparsity in inputs. Most of that work,\nthough, is for inference, where weight sparsity is known statically, and/or for\nspecialized hardware. We propose a scheme to leverage dynamic sparsity during\ntraining. In particular, we exploit zeros introduced by the ReLU activation\nfunction to both feature maps and their gradients. This is challenging because\nthe sparsity degree is moderate and the locations of zeros change over time. We\nalso rely purely on software. We identify zeros in a dense data representation\nwithout transforming the data and performs conventional vectorized computation.\nVariations of the scheme are applicable to all major components of training:\nforward propagation, backward propagation by inputs, and backward propagation\nby weights. Our method significantly outperforms a highly-optimized dense\ndirect convolution on several popular deep neural networks. At realistic\nsparsity, we speed up the training of the non-initial convolutional layers in\nVGG16, ResNet-34, ResNet-50, and Fixup ResNet-50 by 2.19x, 1.37x, 1.31x, and\n1.51x respectively on an Intel Skylake-X CPU.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 18:19:32 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Gong", "Zhangxiaowen", ""], ["Ji", "Houxiang", ""], ["Fletcher", "Christopher", ""], ["Hughes", "Christopher", ""], ["Torrellas", "Josep", ""]]}, {"id": "1911.10340", "submitter": "Deepak Rajendraprasad", "authors": "K. S. Ajish Kumar, Deepak Rajendraprasad, K. S. Sudeep", "title": "Oriented Diameter of Star Graphs", "comments": "Full version of the paper to be presented in CALDAM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An {\\em orientation} of an undirected graph $G$ is an assignment of exactly\none direction to each edge of $G$. Converting two-way traffic networks to\none-way traffic networks and bidirectional communication networks to\nunidirectional communication networks are practical instances of graph\norientations. In these contexts minimising the diameter of the resulting\noriented graph is of prime interest.\n  The $n$-star network topology was proposed as an alternative to the hypercube\nnetwork topology for multiprocessor systems by Akers and Krishnamurthy [IEEE\nTrans. on Computers (1989)]. The $n$-star graph $S_n$ consists of $n!$\nvertices, each labelled with a distinct permutation of $[n]$. Two vertices are\nadjacent if their labels differ exactly in the first and one other position.\n$S_n$ is an $(n-1)$-regular, vertex-transitive graph with diameter $\\lfloor\n3(n-1)/2 \\rfloor$. Orientations of $S_n$, called unidirectional star graphs and\ndistributed routing protocols over them were studied by Day and Tripathi\n[Information Processing Letters (1993)] and Fujita [The First International\nSymposium on Computing and Networking (CANDAR 2013)]. Fujita showed that the\n(directed) diameter of this unidirectional star graph $\\overrightarrow{S_n}$ is\nat most $\\lceil{5n/2}\\rceil + 2$.\n  In this paper, we propose a new distributed routing algorithm for the same\n$\\overrightarrow{S_n}$ analysed by Fujita, which routes a packet from any node\n$s$ to any node $t$ at an undirected distance $d$ from $s$ using at most\n$\\min\\{4d+4, 2n+4\\}$ hops. This shows that the (directed) diameter of\n$\\overrightarrow{S_n}$ is at most $2n+4$. We also show that the diameter of\n$\\overrightarrow{S_n}$ is at least $2n$ when $n \\geq 7$, thereby showing that\nour upper bound is tight up to an additive factor.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 09:46:53 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Kumar", "K. S. Ajish", ""], ["Rajendraprasad", "Deepak", ""], ["Sudeep", "K. S.", ""]]}, {"id": "1911.10344", "submitter": "Christoph Dibak", "authors": "Christoph Dibak and Wolfgang Nowak and Frank D\\\"urr and Kurt Rothermel", "title": "Using Surrogate Models and Data Assimilation for Efficient Mobile\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical simulations on mobile devices are an important tool for engineers\nand decision makers in the field. However, providing simulation results on\nmobile devices is challenging due to the complexity of the simulation,\nrequiring remote server resources and distributed mobile computation. The\nadditional large size of multi-dimensional simulation results leads to the\ninsufficient performance of existing approaches, especially when the bandwidth\nof wireless communication is scarce. In this article, we present an optimized\nnovel approach utilizing surrogate models and data assimilation techniques to\nreduce the communication overhead. Evaluations show that our approach is up to\n$6.5$ times faster than streaming results from the server while still meeting\nrequired quality constraints.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 10:36:02 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Dibak", "Christoph", ""], ["Nowak", "Wolfgang", ""], ["D\u00fcrr", "Frank", ""], ["Rothermel", "Kurt", ""]]}, {"id": "1911.10361", "submitter": "Tung-Wei Kuo", "authors": "Tung-Wei Kuo, Kung Chen", "title": "No Need for Recovery: A Simple Two-Step Byzantine Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a deterministic two-step Byzantine consensus protocol\nthat achieves safety and liveness. A two-step Byzantine consensus protocol only\nneeds two communication steps to commit in the absence of faults. Most two-step\nByzantine consensus protocols exploit optimism and require a recovery protocol\nin the presence of faults. In this paper, we give a simple two-step Byzantine\nconsensus protocol that does not need a recovery protocol.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 12:54:44 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Kuo", "Tung-Wei", ""], ["Chen", "Kung", ""]]}, {"id": "1911.10486", "submitter": "Alexander Spiegelman", "authors": "Alexander Spiegelman and Arik Rinberg", "title": "ACE: Abstract Consensus Encapsulation for Liveness Boosting of State\n  Machine Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of cross-organization attack-prone byzantine\nfault-tolerant (BFT) systems, so-called Blockchains, providing asynchronous\nstate machine replication (SMR) solutions is no longer a theoretical concern.\nThis paper introduces ACE: a general framework for the software design of\nfault-tolerant SMR systems. We first propose a new leader-based-view (LBV)\nabstraction that encapsulates the core properties provided by each view in a\npartially synchronous consensus algorithm, designed according to the\nleader-based view-by-view paradigm (e.g., PBFT and Paxos). Then, we compose\nseveral LBV instances in a non-trivial way in order to boost asynchronous\nliveness of existing SMR solutions.\n  ACE is model agnostic - it abstracts away any model assumptions that\nconsensus protocols may have, e.g., the ratio and types of faulty parties. For\nexample, when the LBV abstraction is instantiated with a partially synchronous\nconsensus algorithm designed to tolerate crash failures, e.g., Paxos or Raft,\nACE yields an asynchronous SMR for $n = 2f+1$ parties. However, if the LBV\nabstraction is instantiated with a byzantine protocol like PBFT or HotStuff,\nthen ACE yields an asynchronous byzantine SMR for $n = 3f+1$ parties.\n  To demonstrate the power of ACE, we implement it in C++, instantiate the LBV\nabstraction with a view implementation of HotStuff -- a state of the art\npartially synchronous byzantine agreement protocol -- and compare it with the\nbase HotStuff implementation under different adversarial scenarios. Our\nevaluation shows that while ACE is outperformed by HotStuff in the optimistic,\nsynchronous, failure-free case, ACE has absolute superiority during network\nasynchrony and attacks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 09:47:29 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Spiegelman", "Alexander", ""], ["Rinberg", "Arik", ""]]}, {"id": "1911.10804", "submitter": "Jes\\'us Rodr\\'iguez S\\'anchez", "authors": "Jesus Rodriguez Sanchez, Fredrik Rusek, Ove Edfors, Liang Liu", "title": "An Iterative Interference Cancellation Algorithm for Large Intelligent\n  Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Large Intelligent Surface (LIS) concept is a promising technology aiming\nto revolutionize wireless communication by exploiting spatial multiplexing at\nits fullest. Despite of its potential, due to the size of the LIS and the large\nnumber of antenna elements involved there is a need of decentralized\narchitectures together with distributed algorithms which can reduce the\ninter-connection data-rate and computational requirement in the Central\nProcessing Unit (CPU). In this article we address the uplink detection problem\nin the LIS system and propose a decentralize architecture based on panels,\nwhich perform local linear processing. We also provide the sum-rate capacity\nfor such architecture and derive an algorithm to obtain the equalizer, which\naims to maximize the sum-rate capacity. A performance analysis is also\npresented, including a comparison to a naive approach based on a reduced form\nof the matched filter (MF) method. The results shows the superiority of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 10:16:24 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Sanchez", "Jesus Rodriguez", ""], ["Rusek", "Fredrik", ""], ["Edfors", "Ove", ""], ["Liu", "Liang", ""]]}, {"id": "1911.11286", "submitter": "Mohammad Roohitavaf", "authors": "Mohammad Roohitavaf, Kun Ren, Gene Zhang, Sami Ben-romdhane", "title": "LogPlayer: Fault-tolerant Exactly-once Delivery using gRPC Asynchronous\n  Streaming", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design of our LogPlayer that is a component\nresponsible for fault-tolerant delivery of transactional mutations recorded on\na WAL to the backend storage shards. LogPlayer relies on gRPC for asynchronous\nstreaming. However, the design provided in this paper can be used with other\nasynchronous streaming platforms. We model check the correctness of LogPlayer\nby TLA+. In particular, our TLA+ specification shows that LogPlayer guarantees\nin-order exactly-once delivery of WAL entries to the storage shards, even in\nthe presence of shards or LogPlayer failures. Our experiments show LogPlayer is\ncapable of efficient delivery with sub-millisecond latency, and it is\nsignificantly more efficient than Apache Kafka for designing a WAL system with\nexactly-once guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 00:00:35 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Roohitavaf", "Mohammad", ""], ["Ren", "Kun", ""], ["Zhang", "Gene", ""], ["Ben-romdhane", "Sami", ""]]}, {"id": "1911.11313", "submitter": "Yifan Sun", "authors": "Yifan Sun, Nicolas Bohm Agostini, Shi Dong, David Kaeli", "title": "Summarizing CPU and GPU Design Trends with Product Data", "comments": "Fix flops/watt error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moore's Law and Dennard Scaling have guided the semiconductor industry for\nthe past few decades. Recently, both laws have faced validity challenges as\ntransistor sizes approach the practical limits of physics. We are interested in\ntesting the validity of these laws and reflect on the reasons responsible. In\nthis work, we collect data of more than 4000 publicly-available CPU and GPU\nproducts. We find that transistor scaling remains critical in keeping the laws\nvalid. However, architectural solutions have become increasingly important and\nwill play a larger role in the future. We observe that GPUs consistently\ndeliver higher performance than CPUs. GPU performance continues to rise because\nof increases in GPU frequency, improvements in the thermal design power (TDP),\nand growth in die size. But we also see the ratio of GPU to CPU performance\nmoving closer to parity, thanks to new SIMD extensions on CPUs and increased\nCPU core counts.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 03:06:26 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 04:23:41 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Sun", "Yifan", ""], ["Agostini", "Nicolas Bohm", ""], ["Dong", "Shi", ""], ["Kaeli", "David", ""]]}, {"id": "1911.11329", "submitter": "Guodong Zhao", "authors": "Gang Wu1, Guodong Zhao, Yidong Song", "title": "Index-Based Scheduling for Parallel State Machine Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State Machine Replication (SMR) is a fundamental approach to designing\nservice with fault tolerance. However, its requirement for the deterministic\nexecution of transactions often results in single-threaded replicas, which\ncannot fully exploit the multicore capabilities of today's processors.\nTherefore, parallel SMR has become a hot topic of recent research. The basic\nidea behind it is that independent transactions can be executed in parallel,\nwhile dependent transactions must be executed in their relative order to ensure\nconsistency among replicas. The dependency detection of existing parallel SMR\nmethods is mainly based on pairwise transaction comparison or batch comparison.\nThese methods cannot simultaneously guarantee both effective detection and\nconcurrent execution. Moreover, the scheduling process cannot execute\nconcurrently, which introduces extra scheduling overhead as well. In order to\nfurther reduce scheduling overhead and ensure the parallel execution of\ntransactions, we propose an efficient scheduler based on a specific index\nstructure. The index is composed of a Bloom Filter and the associated\ntransaction queues, which provides an efficient dependency detection and\npreserve necessary dependency information respectively. Based on the index\nstructure, we further devise an elaborated concurrent scheduling process. The\nexperimental results show that the proposed scheduler is more efficient,\nscalable and robust than the comparison methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 04:06:21 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Wu1", "Gang", ""], ["Zhao", "Guodong", ""], ["Song", "Yidong", ""]]}, {"id": "1911.11516", "submitter": "Alexander Murray", "authors": "Alexander Murray, Michael Kyesswa, Philipp Schmurr, H\\\"useyin K\n  \\c{C}akmak, Veit Hagenmeyer", "title": "A Comparison of Partitioning Strategies in AC Optimal Power Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highly non-convex AC optimal power flow problem is known to scale very\npoorly with respect to the number of lines and buses. To achieve improved\ncomputational speed and scalability, we apply a distributed optimization\nalgorithm, the so-called Augmented Lagrangian based Alternating Direction\nInexact Newton (ALADIN) method. However, the question of how to optimally\npartition a power grid for use in distributed optimization remains open in the\nliterature. In the present paper, we compare four partitioning strategies using\nthe standard IEEE 9, 14, 30, 39, 57, 118, 300 bus models as the performance\nbenchmark. To this end, we apply the graph partitioners KaFFPa and METIS as\nwell as two spectral clustering methods to the aforementioned bus models. For\nlarger grids, KaFFPa yields the best results on average.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 13:24:00 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Murray", "Alexander", ""], ["Kyesswa", "Michael", ""], ["Schmurr", "Philipp", ""], ["\u00c7akmak", "H\u00fcseyin K", ""], ["Hagenmeyer", "Veit", ""]]}, {"id": "1911.11576", "submitter": "Guoping Long", "authors": "Guoping Long, Jun Yang, Wei Lin", "title": "FusionStitching: Boosting Execution Efficiency of Memory Intensive\n  Computations for DL Workloads", "comments": "11+ pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance optimization is the art of continuous seeking a harmonious\nmapping between the application domain and hardware. Recent years have\nwitnessed a surge of deep learning (DL) applications in industry. Conventional\nwisdom for optimizing such workloads mainly focus on compute intensive ops\n(GEMM, Convolution, etc). Yet we show in this work, that the performance of\nmemory intensive computations is vital to E2E performance in practical DL\nmodels. We propose \\emph{FusionStitching}, a optimization framework capable of\nfusing memory intensive \\emph{elementwise}, \\emph{reduction} and fine grained\n\\emph{GEMM/Batched-GEMM} ops, with or without data dependences, into large\ncomputation units, then mapping and transforming them into efficient GPU\nkernels. We formulate the fusion plan optimization as an integer linear\nprogramming (ILP) problem, and propose a set of empirical heuristics to reduce\nthe combinatorial search space. In order to map optimized fusion plans to\nhardware, we propose a technique to effectively compose various groups of\ncomputations into a single GPU kernel, by fully leveraging on chip resources\nlike scratchpads or registers. Experimental results on six benchmarks and four\nindustry scale practical models are encouraging. Overall,\n\\emph{FusionStitching} can reach up to 5.7x speedup compared to Tensorflow\nbaseline, and achieves 1.25x to 1.85x performance speedups compared to current\nstate of the art, with 1.4x on average (geometric mean).\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 22:49:11 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Long", "Guoping", ""], ["Yang", "Jun", ""], ["Lin", "Wei", ""]]}, {"id": "1911.11624", "submitter": "Miguel Coimbra", "authors": "Miguel E. Coimbra, Alexandre P. Francisco and Lu\\'is Veiga", "title": "An analysis of the graph processing landscape", "comments": "42 pages, 5 figures, 2 tables", "journal-ref": "Journal of Big Data, 8 (2021), 1-41", "doi": "10.1186/s40537-021-00443-9", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The value of graph-based big data can be unlocked by exploring the topology\nand metrics of the networks they represent, and the computational approaches to\nthis exploration take on many forms. The use-case of performing global\ncomputations over a graph, it is first ingested into a graph processing system\nfrom one of many digital representations. Extracting information from graphs\ninvolves processing all their elements globally, and can be done with\nsingle-machine systems (with varying approaches to hardware usage), distributed\nsystems (either homogeneous or heterogeneous groups of machines) and systems\ndedicated to high-performance computing (HPC).\n  We provide an overview of different aspects of the graph processing landscape\nand describe classes of systems based on a set of dimensions we detail. The\ndimensions we detail encompass paradigms to express graph processing, different\ntypes of systems to use, coordination and communication models in distributed\ngraph processing, partitioning techniques and different definitions related to\nthe potential for a graph to be updated. This survey is aimed at both the\nexperienced software engineer or researcher as well as the newcomer looking for\nan understanding of the landscape of solutions (and their limitations) for\ngraph processing.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 15:14:43 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 16:11:29 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 14:54:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Coimbra", "Miguel E.", ""], ["Francisco", "Alexandre P.", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1911.11725", "submitter": "Nino Arsov", "authors": "Nino Arsov, Goran Velinov, Aleksandar S. Dimovski, Bojana Koteska,\n  Dragan Sahpaski, Margina Kon-Popovska", "title": "Prediction of Horizontal Data Partitioning Through Query Execution Cost\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The excessively increased volume of data in modern data management systems\ndemands an improved system performance, frequently provided by data\ndistribution, system scalability and performance optimization techniques.\nOptimized horizontal data partitioning has a significant influence of\ndistributed data management systems. An optimally partitioned schema found in\nthe early phase of logical database design without loading of real data in the\nsystem and its adaptation to changes of business environment are very important\nfor a successful implementation, system scalability and performance\nimprovement. In this paper we present a novel approach for finding an optimal\nhorizontally partitioned schema that manifests a minimal total execution cost\nof a given database workload. Our approach is based on a formal model that\nenables abstraction of the predicates in the workload queries, and are\nsubsequently used to define all relational fragments. This approach has\npredictive features acquired by simulation of horizontal partitioning, without\nloading any data into the partitions, but instead, altering the statistics in\nthe database catalogs. We define an optimization problem and employ a genetic\nalgorithm (GA) to find an approximately optimal horizontally partitioned\nschema. The solutions to the optimization problem are evaluated using\nPostgreSQL's query optimizer. The initial experimental evaluation of our\napproach confirms its efficiency and correctness, and the numbers imply that\nthe approach is effective in reducing the workload execution cost.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:02:58 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Arsov", "Nino", ""], ["Velinov", "Goran", ""], ["Dimovski", "Aleksandar S.", ""], ["Koteska", "Bojana", ""], ["Sahpaski", "Dragan", ""], ["Kon-Popovska", "Margina", ""]]}, {"id": "1911.11815", "submitter": "Xiaoyu Cao", "authors": "Minghong Fang, Xiaoyu Cao, Jinyuan Jia and Neil Zhenqiang Gong", "title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning", "comments": "The paper was submitted to Usenix Security Symposium in February 2019\n  and will appear in Usenix Security Symposium 2020; fixing an error in Theorem\n  1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated learning, multiple client devices jointly learn a machine\nlearning model: each client device maintains a local model for its local\ntraining dataset, while a master device maintains a global model via\naggregating the local models from the client devices. The machine learning\ncommunity recently proposed several federated learning methods that were\nclaimed to be robust against Byzantine failures (e.g., system failures,\nadversarial manipulations) of certain client devices. In this work, we perform\nthe first systematic study on local model poisoning attacks to federated\nlearning. We assume an attacker has compromised some client devices, and the\nattacker manipulates the local model parameters on the compromised client\ndevices during the learning process such that the global model has a large\ntesting error rate. We formulate our attacks as optimization problems and apply\nour attacks to four recent Byzantine-robust federated learning methods. Our\nempirical results on four real-world datasets show that our attacks can\nsubstantially increase the error rates of the models learnt by the federated\nlearning methods that were claimed to be robust against Byzantine failures of\nsome client devices. We generalize two defenses for data poisoning attacks to\ndefend against our local model poisoning attacks. Our evaluation results show\nthat one defense can effectively defend against our attacks in some cases, but\nthe defenses are not effective enough in other cases, highlighting the need for\nnew defenses against our local model poisoning attacks to federated learning.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 20:10:04 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 20:59:34 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Fang", "Minghong", ""], ["Cao", "Xiaoyu", ""], ["Jia", "Jinyuan", ""], ["Gong", "Neil Zhenqiang", ""]]}, {"id": "1911.11906", "submitter": "Max Carlson", "authors": "Max Carlson, Robert M. Kirby, Hari Sundar", "title": "A Scalable Framework for Solving Fractional Diffusion Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of fractional order differential operators is receiving renewed\nattention in many scientific fields. In order to accommodate researchers doing\nwork in these areas, there is a need for highly scalable numerical methods for\nsolving partial differential equations that involve fractional order operators\non complex geometries. These operators have desirable special properties that\nalso change the computational considerations in such a way that undermines\ntraditional methods and makes certain other approaches more appealing. We have\ndeveloped a scalable framework for solving fractional diffusion equations using\none such method, specifically the method of eigenfunction expansion. In this\npaper, we will discuss the specific parallelization strategies used to\nefficiently compute the full set of eigenvalues and eigenvectors for a\ndiscretized Laplace eigenvalue problem and apply them to construct approximate\nsolutions to our fractional order model problems. Additionally, we demonstrate\nthe performance of the method on the Frontera computing cluster and the\naccuracy of the method on simple geometries using known exact solutions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 01:33:57 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Carlson", "Max", ""], ["Kirby", "Robert M.", ""], ["Sundar", "Hari", ""]]}, {"id": "1911.11915", "submitter": "Sachini Jayasekara", "authors": "Sachini Jayasekara, Aaron Harwood, Shanika Karunasekera", "title": "A Utilization Model for Optimization of Checkpoint Intervals in\n  Distributed Stream Processing Systems", "comments": null, "journal-ref": "Future Generation Computer Systems 110 (2020) 68-79", "doi": "10.1016/j.future.2020.04.019", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art distributed stream processing systems such as Apache Flink\nand Storm have recently included checkpointing to provide fault-tolerance for\nstateful applications. This is a necessary eventuality as these systems head\ninto the Exascale regime, and is evidently more efficient than replication as\nstate size grows. However current systems use a nominal value for the\ncheckpoint interval, indicative of assuming roughly 1 failure every 19 days,\nthat does not take into account the salient aspects of the checkpoint process,\nnor the system scale, which can readily lead to inefficient system operation.\nTo address this shortcoming, we provide a rigorous derivation of utilization --\nthe fraction of total time available for the system to do useful work -- that\nincorporates checkpoint interval, failure rate, checkpoint cost, failure\ndetection and restart cost, depth of the system topology and message delay. Our\nmodel yields an elegant expression for utilization and provides an optimal\ncheckpoint interval given these parameters, interestingly showing it to be\ndependent only on checkpoint cost and failure rate. We confirm the accuracy and\nefficacy of our model through experiments with Apache Flink, where we obtain\nimprovements in system utilization for every case, especially as the system\nsize increases. Our model provides a solid theoretical basis for the analysis\nand optimization of more elaborate checkpointing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 02:16:03 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Jayasekara", "Sachini", ""], ["Harwood", "Aaron", ""], ["Karunasekera", "Shanika", ""]]}, {"id": "1911.11973", "submitter": "Abhinav Aggarwal", "authors": "Abhinav Aggarwal, William F. Vining, Diksha Gupta, Jared Saia, Melanie\n  E. Moses", "title": "A Most Irrational Foraging Algorithm", "comments": "Presented at BDA 2019 (co-located with PODC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a foraging algorithm, GoldenFA, in which search direction is\nchosen based on the Golden Ratio. We show both theoretically and empirically\nthat GoldenFA is more efficient for a single searcher than a comparable\nalgorithm where search direction is chosen uniformly at random. Moreover, we\ngive a variant of our algorithm that parallelizes linearly with the number of\nsearchers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 06:14:06 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Aggarwal", "Abhinav", ""], ["Vining", "William F.", ""], ["Gupta", "Diksha", ""], ["Saia", "Jared", ""], ["Moses", "Melanie E.", ""]]}, {"id": "1911.11974", "submitter": "Abhinav Aggarwal", "authors": "John Erickson, Abhinav Aggarwal, Melanie E. Moses", "title": "On the Minimal Set of Inputs Required for Efficient Neuro-Evolved\n  Foraging", "comments": "Presented at BDA 2019 (Colocated with PODC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform an ablation study of \\neatfa, a neuro-evolved\nforaging algorithm that has recently been shown to forage efficiently under\ndifferent resource distributions. Through selective disabling of input signals,\nwe identify a \\emph{sufficiently} minimal set of input features that contribute\nthe most towards determining search trajectories which favor high resource\ncollection rates. Our experiments reveal that, independent of how the resources\nare distributed in the arena, the signals involved in imparting the controller\nthe ability to switch from searching of resources to transporting them back to\nthe nest are the most critical. Additionally, we find that pheromones play a\nkey role in boosting performance of the controller by providing signals for\ninformed locomotion in search for unforaged resources.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 06:18:22 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Erickson", "John", ""], ["Aggarwal", "Abhinav", ""], ["Moses", "Melanie E.", ""]]}, {"id": "1911.12162", "submitter": "Fran\\c{c}ois Tessier", "authors": "Fran\\c{c}ois Tessier, Maxime Martinasso, Matteo Chesi, Mark Klein,\n  Miguel Gila", "title": "Dynamically Provisioning Cray DataWarp Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Complex applications and workflows needs are often exclusively expressed in\nterms of computational resources on HPC systems. In many cases, other resources\nlike storage or network are not allocatable and are shared across the entire\nHPC system. By looking at the storage resource in particular, any workflow or\napplication should be able to select both its preferred data manager and its\nrequired storage capability or capacity. To achieve such a goal, new mechanisms\nshould be introduced. In this work, we introduce such a mechanism for\ndynamically provision a data management system on top of storage devices. We\nparticularly focus our effort on deploying a BeeGFS instance across multiple\nDataWarp nodes on a Cray XC50 system. However, we also demonstrate that the\nsame mechanism can be used to deploy BeeGFS on non-Cray system.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:08:36 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Tessier", "Fran\u00e7ois", ""], ["Martinasso", "Maxime", ""], ["Chesi", "Matteo", ""], ["Klein", "Mark", ""], ["Gila", "Miguel", ""]]}, {"id": "1911.12447", "submitter": "Philipp Witte", "authors": "Philipp A. Witte and Mathias Louboutin and Charles Jones and Felix J.\n  Herrmann", "title": "Serverless seismic imaging in the cloud", "comments": "Submitted to the 2020 Rice Oil & Gas HPC Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This abstract presents a serverless approach to seismic imaging in the cloud\nbased on high-throughput containerized batch processing, event-driven\ncomputations and a domain-specific language compiler for solving the underlying\nwave equations. A 3D case study on Azure demonstrates that this approach allows\nreducing the operating cost of up to a factor of 6, making the cloud a viable\nalternative to on-premise HPC clusters for seismic imaging.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 22:21:39 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Witte", "Philipp A.", ""], ["Louboutin", "Mathias", ""], ["Jones", "Charles", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "1911.12711", "submitter": "Hagar Meir", "authors": "Hagar Meir, Artem Barger, Yacov Manevich, Yoav Tock", "title": "Lockless Transaction Isolation in Hyperledger Fabric", "comments": "Published in the 2019 IEEE International Conference on Blockchain\n  (Blockchain-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperledger Fabric is a distributed operating system for permissioned\nblockchains hosted by the Linux Foundation. It is the first truly extensible\nblockchain system for running distributed applications at enterprise grade\nscale. To achieve this, Hyperledger Fabric introduces a novel\nexecute-order-validate blockchain architecture, allowing parallelization of\ntransaction execution and validation. However, this raises the need for\ntransaction isolation. Today transaction isolation is attained by locking the\nentire state database during simulation of transactions and database updates.\nThis lock is one of the major performance bottlenecks as observed by previous\nwork.\n  This work presents a new lock-free approach for providing transaction\nisolation. It harnesses the already existing versioning of key-value pairs in\nthe database, used primarily for a read-write conflict detection during the\nvalidation phase, to create a version-based snapshot isolation. We further\nimplement and evaluate our new approach. We show that our solution outperforms\nthe current implementation by 8.1x and that it is comparable to the optimal\nsolution where no isolation mechanism is applied.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 13:44:58 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Meir", "Hagar", ""], ["Barger", "Artem", ""], ["Manevich", "Yacov", ""], ["Tock", "Yoav", ""]]}, {"id": "1911.12896", "submitter": "Michael Kamp", "authors": "Michael Kamp, Mario Boley, Michael Mock, Daniel Keren, Assaf Schuster,\n  Izchak Sharfman", "title": "Adaptive Communication Bounds for Distributed Online Learning", "comments": null, "journal-ref": "Proceedings of the 7th NIPS Workshop on Optimization for Machine\n  Learning, 2014", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider distributed online learning protocols that control the exchange\nof information between local learners in a round-based learning scenario. The\nlearning performance of such a protocol is intuitively optimal if approximately\nthe same loss is incurred as in a hypothetical serial setting. If a protocol\naccomplishes this, it is inherently impossible to achieve a strong\ncommunication bound at the same time. In the worst case, every input is\nessential for the learning performance, even for the serial setting, and thus\nneeds to be exchanged between the local learners. However, it is reasonable to\ndemand a bound that scales well with the hardness of the serialized prediction\nproblem, as measured by the loss received by a serial online learning\nalgorithm. We provide formal criteria based on this intuition and show that\nthey hold for a simplified version of a previously published protocol.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 23:12:34 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Kamp", "Michael", ""], ["Boley", "Mario", ""], ["Mock", "Michael", ""], ["Keren", "Daniel", ""], ["Schuster", "Assaf", ""], ["Sharfman", "Izchak", ""]]}, {"id": "1911.12899", "submitter": "Michael Kamp", "authors": "Michael Kamp, Sebastian Bothe, Mario Boley, Michael Mock", "title": "Communication-Efficient Distributed Online Learning with Kernels", "comments": null, "journal-ref": "Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n  2016", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an efficient distributed online learning protocol for low-latency\nreal-time services. It extends a previously presented protocol to kernelized\nonline learners that represent their models by a support vector expansion.\nWhile such learners often achieve higher predictive performance than their\nlinear counterparts, communicating the support vector expansions becomes\ninefficient for large numbers of support vectors. The proposed extension allows\nfor a larger class of online learning algorithms---including those alleviating\nthe problem above through model compression. In addition, we characterize the\nquality of the proposed protocol by introducing a novel criterion that requires\nthe communication to be bounded by the loss suffered.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 23:22:30 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Kamp", "Michael", ""], ["Bothe", "Sebastian", ""], ["Boley", "Mario", ""], ["Mock", "Michael", ""]]}, {"id": "1911.12930", "submitter": "Dinusha Vatsalan", "authors": "Dinusha Vatsalan, Peter Christen, and Erhard Rahm", "title": "Incremental Clustering Techniques for Multi-Party Privacy-Preserving\n  Record Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-Preserving Record Linkage (PPRL) supports the integration of\nsensitive information from multiple datasets, in particular the\nprivacy-preserving matching of records referring to the same entity. PPRL has\ngained much attention in many application areas, with the most prominent ones\nin the healthcare domain. PPRL techniques tackle this problem by conducting\nlinkage on masked (encoded) values. Employing PPRL on records from multiple\n(more than two) parties/sources (multi-party PPRL, MP-PPRL) is an increasingly\nimportant but challenging problem that so far has not been sufficiently solved.\nExisting MP-PPRL approaches are limited to finding only those entities that are\npresent in all parties thereby missing entities that match only in a subset of\nparties. Furthermore, previous MP-PPRL approaches face substantial scalability\nlimitations due to the need of a large number of comparisons between masked\nrecords. We thus propose and evaluate new MP-PPRL approaches that find matches\nin any subset of parties and still scale to many parties. Our approaches\nmaintain all matches within clusters, where these clusters are incrementally\nextended or refined by considering records from one party after the other. An\nempirical evaluation using multiple real datasets ranging from 3 to 26 parties\neach containing up to $5$ million records validates that our protocols are\nefficient, and significantly outperform existing MP-PPRL approaches in terms of\nlinkage quality and scalability.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 02:53:35 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Vatsalan", "Dinusha", ""], ["Christen", "Peter", ""], ["Rahm", "Erhard", ""]]}, {"id": "1911.13074", "submitter": "Danijel Zlaus", "authors": "Danijel \\v{Z}laus and Domen Mongus", "title": "Efficient method for parallel computation of geodesic transformation on\n  CPU", "comments": "\\c{opyright} 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": "10.1109/TPDS.2019.2953057", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fast Central Processing Unit (CPU) implementation of\ngeodesic morphological operations using stream processing. In contrast to the\ncurrent state-of-the-art, that focuses on achieving insensitivity to the filter\nsizes with efficient data structures, the proposed approach achieves efficient\ncomputation of long chains of elementary $3 \\times 3$ filters using multicore\nand Single Instruction Multiple Data (SIMD) processing. In comparison to the\nrelated methods, up to $100$ times faster computation of common geodesic\noperators is achieved in this way, allowing for real-time processing (with over\n$30$ FPS) of up to $1500$ filters long chains, applied on $1024\\times 1024$\nimages. In addition, the proposed approach outperformed GPGPU, and proved to be\nmore efficient than the comparable streaming method for the computation of\nmorphological erosions and dilations with window sizes up to $183\\times 183$ in\nthe case of using char and $27\\times27$ when using double data types.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 12:09:39 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["\u017dlaus", "Danijel", ""], ["Mongus", "Domen", ""]]}, {"id": "1911.13160", "submitter": "Maxime Martinasso", "authors": "Felipe A. Cruz, Maxime Martinasso", "title": "FirecREST: RESTful API on Cray XC systems", "comments": "Presented at the Cray User Group 2019 (CUG'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As science gateways are becoming an increasingly popular digital interface\nfor scientific communities, it is also becoming increasingly important for\nHigh-Performance Computing centers to provide a modern Web-enabled APIs. With\nsuch interface, science gateways can easily integrate access to HPC center\nresources. This work presents the FirecREST API, a RESTful Web API\ninfrastructure that allows scientific communities to access the various\nintegrated resources and services available from the Cray XC systems at the\nSwiss National Supercomputing Centre. FirecREST requirements are derived from\nuse cases described in this work.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:47:33 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Cruz", "Felipe A.", ""], ["Martinasso", "Maxime", ""]]}, {"id": "1911.13208", "submitter": "Lei Liu", "authors": "Lei Liu", "title": "QoS-Aware Machine Learning-based Multiple Resources Scheduling for\n  Microservices in Cloud Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservices have been dominating in the modern cloud environment. To\nimprove cost efficiency, multiple microservices are normally co-located on a\nserver. Thus, the run-time resource scheduling becomes the pivot for QoS\ncontrol. However, the scheduling exploration space enlarges rapidly with the\nincreasing server resources - cores, cache, bandwidth, etc. - and the diversity\nof microservices. Consequently, the existing schedulers might not meet the\nrapid changes in service demands. Besides, we observe that there exist resource\ncliffs in the scheduling space. It not only impacts the exploration efficiency,\nmaking it difficult to converge to the optimal scheduling solution, but also\nresults in severe QoS fluctuation. To overcome these problems, we propose a\nnovel machine learning-based scheduling mechanism called OSML. It uses\nresources and runtime states as the input and employs two MLP models and a\nreinforcement learning model to perform scheduling space exploration. Thus,\nOSML can reach an optimal solution much faster than traditional approaches.\nMore importantly, it can automatically detect the resource cliff and avoid them\nduring exploration. To verify the effectiveness of OSML and obtain a\nwell-generalized model, we collect a dataset containing over 2-billion samples\nfrom 11 typical microservices running on real servers over 9 months. Under the\nsame QoS constraint, experimental results show that OSML outperforms the\nstate-of-the-art work, and achieves around 5 times scheduling speed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 21:05:00 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 08:29:41 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Liu", "Lei", ""]]}, {"id": "1911.13214", "submitter": "Lionel Eyraud-Dubois", "authors": "Julien Herrmann (UB, LaBRI, TADAAM), Olivier Beaumont (HiePACS, UB,\n  LaBRI), Lionel Eyraud-Dubois (HiePACS, UB, LaBRI), Julien Hermann, Alexis\n  Joly (ZENITH, LIRMM, UM), Alena Shilova (HiePACS, UB, LaBRI)", "title": "Optimal checkpointing for heterogeneous chains: how to train deep neural\n  networks with limited memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new activation checkpointing method which allows to\nsignificantly decrease memory usage when training Deep Neural Networks with the\nback-propagation algorithm. Similarly to checkpoint-ing techniques coming from\nthe literature on Automatic Differentiation, it consists in dynamically\nselecting the forward activations that are saved during the training phase, and\nthen automatically recomputing missing activations from those previously\nrecorded. We propose an original computation model that combines two types of\nactivation savings: either only storing the layer inputs, or recording the\ncomplete history of operations that produced the outputs (this uses more\nmemory, but requires fewer recomputations in the backward phase), and we\nprovide an algorithm to compute the optimal computation sequence for this\nmodel. This paper also describes a PyTorch implementation that processes the\nentire chain, dealing with any sequential DNN whose internal layers may be\narbitrarily complex and automatically executing it according to the optimal\ncheckpointing strategy computed given a memory limit. Through extensive\nexperiments, we show that our implementation consistently outperforms existing\ncheckpoint-ing approaches for a large class of networks, image sizes and batch\nsizes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 13:05:11 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Herrmann", "Julien", "", "UB, LaBRI, TADAAM"], ["Beaumont", "Olivier", "", "HiePACS, UB,\n  LaBRI"], ["Eyraud-Dubois", "Lionel", "", "HiePACS, UB, LaBRI"], ["Hermann", "Julien", "", "ZENITH, LIRMM, UM"], ["Joly", "Alexis", "", "ZENITH, LIRMM, UM"], ["Shilova", "Alena", "", "HiePACS, UB, LaBRI"]]}, {"id": "1911.13237", "submitter": "Tianyuan Zhang", "authors": "Tianyuan Zhang, Bichen Wu, Xin Wang, Joseph Gonzalez, Kurt Keutzer", "title": "Domain-Aware Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with more parameters and FLOPs have higher capacity and\ngeneralize better to diverse domains. But to be deployed on edge devices, the\nmodel's complexity has to be constrained due to limited compute resource. In\nthis work, we propose a method to improve the model capacity without increasing\ninference-time complexity. Our method is based on an assumption of data\nlocality: for an edge device, within a short period of time, the input data to\nthe device are sampled from a single domain with relatively low diversity.\nTherefore, it is possible to utilize a specialized, low-complexity model to\nachieve good performance in that input domain. To leverage this, we propose\nDomain-aware Dynamic Network (DDN), which is a high-capacity dynamic network in\nwhich each layer contains multiple weights. During inference, based on the\ninput domain, DDN dynamically combines those weights into one single weight\nthat specializes in the given domain. This way, DDN can keep the inference-time\ncomplexity low but still maintain a high capacity. Experiments show that\nwithout increasing the parameters, FLOPs, and actual latency, DDN achieves up\nto 2.6\\% higher AP50 than a static network on the BDD100K object-detection\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 12:00:58 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zhang", "Tianyuan", ""], ["Wu", "Bichen", ""], ["Wang", "Xin", ""], ["Gonzalez", "Joseph", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1911.13252", "submitter": "Julia El Zini", "authors": "Julia El Zini, Yara Rizk and Mariette Awad", "title": "An Optimized and Energy-Efficient Parallel Implementation of\n  Non-Iteratively Trained Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNN) have been successfully applied to various\nsequential decision-making tasks, natural language processing applications, and\ntime-series predictions. Such networks are usually trained through\nback-propagation through time (BPTT) which is prohibitively expensive,\nespecially when the length of the time dependencies and the number of hidden\nneurons increase. To reduce the training time, extreme learning machines (ELMs)\nhave been recently applied to RNN training, reaching a 99\\% speedup on some\napplications. Due to its non-iterative nature, ELM training, when parallelized,\nhas the potential to reach higher speedups than BPTT.\n  In this work, we present \\opt, an optimized parallel RNN training algorithm\nbased on ELM that takes advantage of the GPU shared memory and of parallel QR\nfactorization algorithms to efficiently reach optimal solutions. The\ntheoretical analysis of the proposed algorithm is presented on six RNN\narchitectures, including LSTM and GRU, and its performance is empirically\ntested on ten time-series prediction applications. \\opt~is shown to reach up to\n845 times speedup over its sequential counterpart and to require up to 20x less\ntime to train than parallel BPTT.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:30:53 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zini", "Julia El", ""], ["Rizk", "Yara", ""], ["Awad", "Mariette", ""]]}, {"id": "1911.13294", "submitter": "Jukka Suomela", "authors": "Alkida Balliu, Sebastian Brandt, Yuval Efron, Juho Hirvonen, Yannic\n  Maus, Dennis Olivetti, Jukka Suomela", "title": "Classification of distributed binary labeling problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete classification of the deterministic distributed time\ncomplexity for a family of graph problems: binary labeling problems in trees.\nThese are locally checkable problems that can be encoded with an alphabet of\nsize two in the edge labeling formalism. Examples of binary labeling problems\ninclude sinkless orientation, sinkless and sourceless orientation, 2-vertex\ncoloring, perfect matching, and the task of coloring edges red and blue such\nthat all nodes are incident to at least one red and at least one blue edge.\nMore generally, we can encode e.g. any cardinality constraints on indegrees and\noutdegrees.\n  We study the deterministic time complexity of solving a given binary labeling\nproblem in trees, in the usual LOCAL model of distributed computing. We show\nthat the complexity of any such problem is in one of the following classes:\n$O(1)$, $\\Theta(\\log n)$, $\\Theta(n)$, or unsolvable. In particular, a problem\nthat can be represented in the binary labeling formalism cannot have time\ncomplexity $\\Theta(\\log^* n)$, and hence we know that e.g. any encoding of\nmaximal matchings has to use at least three labels (which is tight).\n  Furthermore, given the description of any binary labeling problem, we can\neasily determine in which of the four classes it is and what is an\nasymptotically optimal algorithm for solving it. Hence the distributed time\ncomplexity of binary labeling problems is decidable, not only in principle, but\nalso in practice: there is a simple and efficient algorithm that takes the\ndescription of a binary labeling problem and outputs its distributed time\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:51:35 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 18:11:17 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 12:32:21 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Efron", "Yuval", ""], ["Hirvonen", "Juho", ""], ["Maus", "Yannic", ""], ["Olivetti", "Dennis", ""], ["Suomela", "Jukka", ""]]}]