[{"id": "1305.0087", "submitter": "Jiyan Yang", "authors": "Jiyan Yang and Xiangrui Meng and Michael W. Mahoney", "title": "Quantile Regression for Large-scale Applications", "comments": "35 pages; long version of a paper appearing in the 2013 ICML. Version\n  to appear in the SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a method to estimate the quantiles of the conditional\ndistribution of a response variable, and as such it permits a much more\naccurate portrayal of the relationship between the response variable and\nobserved covariates than methods such as Least-squares or Least Absolute\nDeviations regression. It can be expressed as a linear program, and, with\nappropriate preprocessing, interior-point methods can be used to find a\nsolution for moderately large problems. Dealing with very large problems,\n\\emph(e.g.), involving data up to and beyond the terabyte regime, remains a\nchallenge. Here, we present a randomized algorithm that runs in nearly linear\ntime in the size of the input and that, with constant probability, computes a\n$(1+\\epsilon)$ approximate solution to an arbitrary quantile regression\nproblem. As a key step, our algorithm computes a low-distortion\nsubspace-preserving embedding with respect to the loss function of quantile\nregression. Our empirical evaluation illustrates that our algorithm is\ncompetitive with the best previous work on small to medium-sized problems, and\nthat in addition it can be implemented in MapReduce-like environments and\napplied to terabyte-sized problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 05:21:03 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 18:18:50 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2014 00:33:30 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Yang", "Jiyan", ""], ["Meng", "Xiangrui", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1305.0483", "submitter": "Mayank Gupta MR", "authors": "A V Choudhari, N A Pande and M R Gupta", "title": "Feasibility Analysis of Low Cost Graphical Processing Units for\n  Electromagnetic Field Simulations by Finite Difference Time Domain Method", "comments": null, "journal-ref": null, "doi": "10.5120/11739-7396", "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among several techniques available for solving Computational Electromagnetics\n(CEM) problems, the Finite Difference Time Domain (FDTD) method is one of the\nbest suited approaches when a parallelized hardware platform is used. In this\npaper we investigate the feasibility of implementing the FDTD method using the\nNVIDIA GT 520, a low cost Graphical Processing Unit (GPU), for solving the\ndifferential form of Maxwell's equation in time domain. Initially a generalized\nbenchmarking problem of bandwidth test and another benchmarking problem of\n'matrix left division is discussed for understanding the correlation between\nthe problem size and the performance on the CPU and the GPU respectively. This\nis further followed by the discussion of the FDTD method, again implemented on\nboth, the CPU and the GT520 GPU. For both of the above comparisons, the CPU\nused is Intel E5300, a low cost dual core CPU.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 15:37:36 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Choudhari", "A V", ""], ["Pande", "N A", ""], ["Gupta", "M R", ""]]}, {"id": "1305.0606", "submitter": "Alireza Mahdian", "authors": "Alireza Mahdian, Richard Han, Qin Lv and Shivakant Mishra", "title": "Results from a Practical Deployment of the MyZone Decentralized P2P\n  Social Network", "comments": "24 pages, 13 Figures, 1 Table. arXiv admin note: text overlap with\n  arXiv:1110.5371", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents MyZone, a private online social network for relatively\nsmall, closely-knit communities. MyZone has three important distinguishing\nfeatures. First, users keep the ownership of their data and have complete\ncontrol over maintaining their privacy. Second, MyZone is free from any\npossibility of content censorship and is highly resilient to any single point\nof disconnection. Finally, MyZone minimizes deployment cost by minimizing its\ncomputation, storage and network bandwidth requirements. It incorporates both a\nP2P architecture and a centralized architecture in its design ensuring high\navailability, security and privacy. A prototype of MyZone was deployed over a\nperiod of 40 days with a membership of more than one hundred users. The paper\nprovides a detailed evaluation of the results obtained from this deployment.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 01:34:18 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Mahdian", "Alireza", ""], ["Han", "Richard", ""], ["Lv", "Qin", ""], ["Mishra", "Shivakant", ""]]}, {"id": "1305.0711", "submitter": "Nina Evseenko", "authors": "Nina Evseenko", "title": "New hybrid distributed voting algorithm", "comments": "3 pages, 2 figures, ICCAT 2013 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Increasing data volumes requires additional rating techniques. Reputation\nsystems are the subject of much research. There are various techniques to rate\ncontent that facilitate the search of quality content. Page rank, citation\nindex and votes from users are some rating examples. In the article I focus on\ndecentralized vote systems. The article reviews several distributed vote\ndesigns. I list the distributed vote requirements. A new hybrid algorithm is\nproposed which operates in the structured overlay P2P DHT Kademlia network.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 14:07:58 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Evseenko", "Nina", ""]]}, {"id": "1305.1044", "submitter": "Andrea Mercurio Dr", "authors": "Andrea Mercurio, Alessandro Di Giorgio, Fabio Purificato", "title": "Optimal Fully Electric Vehicle load balancing with an ADMM algorithm in\n  Smartgrids", "comments": "This paper has been accepted for the 21st Mediterranean Conference on\n  Control and Automation, therefore it is subjected to IEEE Copyrights. See\n  IEEE copyright notice at http://www.ieee.org/documents/ieeecopyrightform.pdf", "journal-ref": null, "doi": "10.1109/MED.2013.6608708", "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a system architecture and a suitable control\nmethodology for the load balancing of Fully Electric Vehicles at Charging\nStation (CS). Within the proposed architecture, control methodologies allow to\nadapt Distributed Energy Resources (DER) generation profiles and active loads\nto ensure economic benefits to each actor. The key aspect is the organization\nin two levels of control: at local level a Load Area Controller (LAC) optimally\ncalculates the FEVs charging sessions, while at higher level a Macro Load Area\nAggregator (MLAA) provides DER with energy production profiles, and LACs with\nenergy withdrawal profiles. Proposed control methodologies involve the solution\nof a Walrasian market equilibrium and the design of a distributed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 19:16:54 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Mercurio", "Andrea", ""], ["Di Giorgio", "Alessandro", ""], ["Purificato", "Fabio", ""]]}, {"id": "1305.1121", "submitter": "Peter Robinson", "authors": "John Augustine, Anisur Rahaman Molla, Ehab Morsy, Gopal Pandurangan,\n  Peter Robinson, Eli Upfal", "title": "Storage and Search in Dynamic Peer-to-Peer Networks", "comments": "to appear at SPAA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study robust and efficient distributed algorithms for searching, storing,\nand maintaining data in dynamic Peer-to-Peer (P2P) networks. P2P networks are\nhighly dynamic networks that experience heavy node churn (i.e., nodes join and\nleave the network continuously over time). Our goal is to guarantee, despite\nhigh node churn rate, that a large number of nodes in the network can store,\nretrieve, and maintain a large number of data items. Our main contributions are\nfast randomized distributed algorithms that guarantee the above with high\nprobability (whp) even under high adversarial churn:\n  1. A randomized distributed search algorithm that (whp) guarantees that\nsearches from as many as $n - o(n)$ nodes ($n$ is the stable network size)\nsucceed in ${O}(\\log n)$-rounds despite ${O}(n/\\log^{1+\\delta} n)$ churn, for\nany small constant $\\delta > 0$, per round. We assume that the churn is\ncontrolled by an oblivious adversary (that has complete knowledge and control\nof what nodes join and leave and at what time, but is oblivious to the random\nchoices made by the algorithm).\n  2. A storage and maintenance algorithm that guarantees (whp) data items can\nbe efficiently stored (with only $\\Theta(\\log{n})$ copies of each data item)\nand maintained in a dynamic P2P network with churn rate up to\n${O}(n/\\log^{1+\\delta} n)$ per round. Our search algorithm together with our\nstorage and maintenance algorithm guarantees that as many as $n - o(n)$ nodes\ncan efficiently store, maintain, and search even under ${O}(n/\\log^{1+\\delta}\nn)$ churn per round. Our algorithms require only polylogarithmic in $n$ bits to\nbe processed and sent (per round) by each node.\n  To the best of our knowledge, our algorithms are the first-known,\nfully-distributed storage and search algorithms that provably work under highly\ndynamic settings (i.e., high churn rates per step).\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 09:04:39 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Augustine", "John", ""], ["Molla", "Anisur Rahaman", ""], ["Morsy", "Ehab", ""], ["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""], ["Upfal", "Eli", ""]]}, {"id": "1305.1157", "submitter": "Timo Bingmann", "authors": "Timo Bingmann and Peter Sanders", "title": "Parallel String Sample Sort", "comments": "34 pages, 7 figures and 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss how string sorting algorithms can be parallelized on modern\nmulti-core shared memory machines. As a synthesis of the best sequential string\nsorting algorithms and successful parallel sorting algorithms for atomic\nobjects, we propose string sample sort. The algorithm makes effective use of\nthe memory hierarchy, uses additional word level parallelism, and largely\navoids branch mispredictions. Additionally, we parallelize variants of multikey\nquicksort and radix sort that are also useful in certain situations.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 12:03:56 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Bingmann", "Timo", ""], ["Sanders", "Peter", ""]]}, {"id": "1305.1183", "submitter": "Ji\\v{r}\\'i Filipovi\\v{c}", "authors": "J. Filipovi\\v{c}, M. Madzin, J. Fousek, L. Matyska", "title": "Optimizing CUDA Code By Kernel Fusion---Application on BLAS", "comments": "Manuscript submitted to SIAM Journal on Scientific Computing", "journal-ref": "Supercomput (2015) 71: 3934", "doi": "10.1007/s11227-015-1483-z", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern GPUs are able to perform significantly more arithmetic operations than\ntransfers of a single word to or from global memory. Hence, many GPU kernels\nare limited by memory bandwidth and cannot exploit the arithmetic power of\nGPUs. However, the memory locality can be often improved by kernel fusion when\na sequence of kernels is executed and some kernels in this sequence share data.\n  In this paper, we show how kernels performing map, reduce or their nested\ncombinations can be fused automatically by our source-to-source compiler. To\ndemonstrate the usability of the compiler, we have implemented several BLAS-1\nand BLAS-2 routines and show how the performance of their sequences can be\nimproved by fusions.\n  Compared to similar sequences using CUBLAS, our compiler is able to generate\ncode that is up to 2.61x faster for the examples tested.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 13:32:22 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2013 09:37:08 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Filipovi\u010d", "J.", ""], ["Madzin", "M.", ""], ["Fousek", "J.", ""], ["Matyska", "L.", ""]]}, {"id": "1305.1422", "submitter": "Peter Wittek", "authors": "Peter Wittek, Shi Chao Gao, Ik Soo Lim, Li Zhao", "title": "Somoclu: An Efficient Parallel Library for Self-Organizing Maps", "comments": "26 pages, 9 figures. The code is available at\n  https://peterwittek.github.io/somoclu/", "journal-ref": "Journal of Statistical Software, 78(9), 1-21 (2017)", "doi": "10.18637/jss.v078.i09", "report-no": null, "categories": "cs.DC cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Somoclu is a massively parallel tool for training self-organizing maps on\nlarge data sets written in C++. It builds on OpenMP for multicore execution,\nand on MPI for distributing the workload across the nodes in a cluster. It is\nalso able to boost training by using CUDA if graphics processing units are\navailable. A sparse kernel is included, which is useful for high-dimensional\nbut sparse data, such as the vector spaces common in text mining workflows.\nPython, R and MATLAB interfaces facilitate interactive use. Apart from fast\nexecution, memory use is highly optimized, enabling training large emergent\nmaps even on a single computer.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 06:43:26 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 12:40:08 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 10:48:52 GMT"}, {"version": "v4", "created": "Fri, 9 Jun 2017 14:03:01 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Wittek", "Peter", ""], ["Gao", "Shi Chao", ""], ["Lim", "Ik Soo", ""], ["Zhao", "Li", ""]]}, {"id": "1305.1459", "submitter": "Pier Stanislao Paolucci", "authors": "Pier Stanislao Paolucci, Iuliana Bacivarov, Gert Goossens, Rainer\n  Leupers, Fr\\'ed\\'eric Rousseau, Christoph Schumacher, Lothar Thiele, Piero\n  Vicini", "title": "EURETILE 2010-2012 summary: first three years of activity of the\n  European Reference Tiled Experiment", "comments": "56 pages", "journal-ref": null, "doi": "10.12837/2013T01", "report-no": null, "categories": "cs.DC cs.AR cs.NE cs.OS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the summary of first three years of activity of the EURETILE FP7\nproject 247846. EURETILE investigates and implements brain-inspired and\nfault-tolerant foundational innovations to the system architecture of massively\nparallel tiled computer architectures and the corresponding programming\nparadigm. The execution targets are a many-tile HW platform, and a many-tile\nsimulator. A set of SW process - HW tile mapping candidates is generated by the\nholistic SW tool-chain using a combination of analytic and bio-inspired\nmethods. The Hardware dependent Software is then generated, providing OS\nservices with maximum efficiency/minimal overhead. The many-tile simulator\ncollects profiling data, closing the loop of the SW tool chain. Fine-grain\nparallelism inside processes is exploited by optimized intra-tile compilation\ntechniques, but the project focus is above the level of the elementary tile.\nThe elementary HW tile is a multi-processor, which includes a fault tolerant\nDistributed Network Processor (for inter-tile communication) and ASIP\naccelerators. Furthermore, EURETILE investigates and implements the innovations\nfor equipping the elementary HW tile with high-bandwidth, low-latency\nbrain-like inter-tile communication emulating 3 levels of connection hierarchy,\nnamely neural columns, cortical areas and cortex, and develops a dedicated\ncortical simulation benchmark: DPSNN-STDP (Distributed Polychronous Spiking\nNeural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverages\non the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPES\nIntegrated Project (2006-2009).\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 10:22:31 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Paolucci", "Pier Stanislao", ""], ["Bacivarov", "Iuliana", ""], ["Goossens", "Gert", ""], ["Leupers", "Rainer", ""], ["Rousseau", "Fr\u00e9d\u00e9ric", ""], ["Schumacher", "Christoph", ""], ["Thiele", "Lothar", ""], ["Vicini", "Piero", ""]]}, {"id": "1305.1840", "submitter": "Ward Jaradat", "authors": "Ward Jaradat, Alan Dearle and Adam Barker", "title": "A Dataflow Language for Decentralised Orchestration of Web Service\n  Workflows", "comments": "To appear in Proceedings of the IEEE 2013 7th International Workshop\n  on Scientific Workflows, in conjunction with IEEE SERVICES 2013", "journal-ref": null, "doi": "10.1109/SERVICES.2013.30", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orchestrating centralised service-oriented workflows presents significant\nscalability challenges that include: the consumption of network bandwidth,\ndegradation of performance, and single points of failure. This paper presents a\nhigh-level dataflow specification language that attempts to address these\nscalability challenges. This language provides simple abstractions for\norchestrating large-scale web service workflows, and separates between the\nworkflow logic and its execution. It is based on a data-driven model that\npermits parallelism to improve the workflow performance. We provide a\ndecentralised architecture that allows the computation logic to be moved\n\"closer\" to services involved in the workflow. This is achieved through\npartitioning the workflow specification into smaller fragments that may be sent\nto remote orchestration services for execution. The orchestration services rely\non proxies that exploit connectivity to services in the workflow. These proxies\nperform service invocations and compositions on behalf of the orchestration\nservices, and carry out data collection, retrieval, and mediation tasks. The\nevaluation of our architecture implementation concludes that our decentralised\napproach reduces the execution time of workflows, and scales accordingly with\nthe increasing size of data sets.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 14:59:20 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Jaradat", "Ward", ""], ["Dearle", "Alan", ""], ["Barker", "Adam", ""]]}, {"id": "1305.1842", "submitter": "Ward Jaradat", "authors": "Ward Jaradat, Alan Dearle and Adam Barker", "title": "An Architecture for Decentralised Orchestration of Web Service Workflows", "comments": "To appear in Proceedings of the IEEE 20th International Conference on\n  Web Services (ICWS 2013)", "journal-ref": null, "doi": "10.1109/ICWS.2013.84", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service-oriented workflows are typically executed using a centralised\norchestration approach that presents significant scalability challenges. These\nchallenges include the consumption of network bandwidth, degradation of\nperformance, and single-points of failure. We provide a decentralised\norchestration architecture that attempts to address these challenges. Our\narchitecture adopts a design model that permits the computation to be moved\n\"closer\" to services in a workflow. This is achieved by partitioning workflows\nspecified using our simple dataflow language into smaller fragments, which may\nbe sent to remote locations for execution.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 15:07:08 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Jaradat", "Ward", ""], ["Dearle", "Alan", ""], ["Barker", "Adam", ""]]}, {"id": "1305.2319", "submitter": "Yehia El-khatib PhD", "authors": "Yehia Elkhatib, Gordon S. Blair and Bholanathsingh Surajbali", "title": "Experiences of Using a Hybrid Cloud to Construct an Environmental\n  Virtual Observatory", "comments": null, "journal-ref": "In Proceedings of the Third Workshop on Cloud Data and Platforms\n  (a EuroSys 2013 workshop), Prague, Czech Republic, April 14 2013", "doi": "10.1145/2460756.2460759", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental science is often fragmented: data is collected using mismatched\nformats and conventions, and models are misaligned and run in isolation. Cloud\ncomputing offers a lot of potential in the way of resolving such issues by\nsupporting data from different sources and at various scales, by facilitating\nthe integration of models to create more sophisticated software services, and\nby providing a sustainable source of suitable computational and storage\nresources. In this paper, we highlight some of our experiences in building the\nEnvironmental Virtual Observatory pilot (EVOp), a tailored cloud-based\ninfrastructure and associated web-based tools designed to enable users from\ndifferent backgrounds to access data concerning different environmental issues.\nWe review our architecture design, the current deployment and prototypes. We\nalso reflect on lessons learned. We believe that such experiences are of\nbenefit to other scientific communities looking to assemble virtual\nobservatories or similar virtual research environments.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 12:08:43 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Elkhatib", "Yehia", ""], ["Blair", "Gordon S.", ""], ["Surajbali", "Bholanathsingh", ""]]}, {"id": "1305.2353", "submitter": "Jonathan Hogg", "authors": "Jonathan Hogg and Jennifer Scott", "title": "Compressed threshold pivoting for sparse symmetric indefinite systems", "comments": "Submitted to SIMAX", "journal-ref": null, "doi": null, "report-no": "STFC Preprint Series RAL-P-2013-007", "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key technique for controlling numerical stability in sparse direct solvers\nis threshold partial pivoting. When selecting a pivot, the entire candidate\npivot column below the diagonal must be up-to-date and must be scanned. If the\nfactorization is parallelized across a large number of cores, communication\nlatencies can be the dominant computational cost.\n  In this paper, we propose two alternative pivoting strategies for sparse\nsymmetric indefinite matrices that significantly reduce communication by\ncompressing the necessary data into a small matrix that can be used to select\npivots. Once pivots have been chosen, they can be applied in a\ncommunication-efficient fashion.\n  For an n x p submatrix on P processors, we show our methods perform a\nfactorization using O(log P) messages instead of the O(p log P) for threshold\npartial pivoting. The additional costs in terms of operations and communication\nbandwidth are relatively small.\n  A stability proof is given and numerical results using a range of symmetric\nindefinite matrices arising from practical problems are used to demonstrate the\npractical robustness. Timing results on large random examples illustrate the\npotential speedup on current multicore machines.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 14:39:52 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Hogg", "Jonathan", ""], ["Scott", "Jennifer", ""]]}, {"id": "1305.2684", "submitter": "Ashraf Karray", "authors": "Achraf Karray, Rym Teyeb and Maher Ben Jemaa", "title": "A Heuristic Approach for Web-Service Discovery and Selection", "comments": null, "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 5, No 2, April 2013", "doi": "10.5121/ijcsit.2013.5210", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In today's businesses, service-oriented architectures represent the main\nparadigm for IT infrastructures. Indeed, the emergence of Internet made it\npossible to set up an exploitable environment to distribute applications on a\nlarge scale, and this, by adapting the notion of \"service\". With the\nintegration of this paradigm in Business to Business Domain (B2B), the number\nof web services becomes very significant. Due to this increase, the discovery\nand selection of web services meeting customer requirement become a very\ndifficult operation. Further, QoS properties must be taking into account in the\nweb service selection. Moreover, with the significant number of web service,\nnecessary time for the discovery of a service will be rather long. In this\npaper, we propose an approach based on a new heuristic method called \"Bees\nAlgorithm\" inspired from honey bees behavior. We use this technique of\noptimization in order to discover appropriate web services, meeting customer\nrequirements, in least time and taking into account the QoS properties.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 07:02:57 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Karray", "Achraf", ""], ["Teyeb", "Rym", ""], ["Jemaa", "Maher Ben", ""]]}, {"id": "1305.2865", "submitter": "Sultan Ullah", "authors": "Sultan Ullah and Zheng Xuefeng and Zhou Feng", "title": "TCloud: A Dynamic Framework and Policies for Access Control across\n  Multiple Domains in Cloud Computing", "comments": null, "journal-ref": "International Journal of Computer Applications, Volume 62, No.2,\n  January 2013, 01-07", "doi": "10.5120/10049-4636", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a cloud computing environment, access control policy is an effective means\nof fortification cloud users and cloud resources services against security\ninfringements. Based on analysis of current cloud computing security\ncharacteristics, the preamble of the concept of trust, role-based access\ncontrol policy, combined with the characteristics of the cloud computing\nenvironment, there are multiple security management domains, so a new cross\ndomain framework is for access control is proposed which is based on trust. It\nwill establish and calculate the degree of trust in the single as well as\nmultiple domains. Role Based Access Control is used for the implementation of\nthe access control policies in a single domain environment with the\nintroduction of the trust concept. In multiple domains the access control will\nbe based on the conversion of roles. On the basis of trust, and role based\naccess control model, a new novel framework of flexible cross domain access\ncontrol framework is presented. The role assignment and conversion will take\nplace dynamically.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 09:16:17 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Ullah", "Sultan", ""], ["Xuefeng", "Zheng", ""], ["Feng", "Zhou", ""]]}, {"id": "1305.3031", "submitter": "Ashkan Paya Mr.", "authors": "Ashkan Paya and Dan C. Marinescu", "title": "Clustering Algorithms for Scale-free Networks and Applications to Cloud\n  Resource Management", "comments": "14 pages, 8 Figurs, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce algorithms for the construction of scale-free\nnetworks and for clustering around the nerve centers, nodes with a high\nconnectivity in a scale-free networks. We argue that such overlay networks\ncould support self-organization in a complex system like a cloud computing\ninfrastructure and allow the implementation of optimal resource management\npolicies.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 06:19:43 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Paya", "Ashkan", ""], ["Marinescu", "Dan C.", ""]]}, {"id": "1305.3105", "submitter": "Daqiang Zhang", "authors": "Daqiang Zhang, Qin Zou, Zhiren Sun", "title": "SECA: Snapshot-based Event Detection for Checking Asynchronous Context\n  Consistency in Ubiquitous Computing", "comments": "This paper is not presented in WCNC'2012 as I missed the time owing\n  to the traffic jam. So the paper is not included in IEEE Explorer, although\n  it is in the Proceedings of the WCNC'2012. in Proceedings of the 2012 IEEE\n  Wireless Communications and Networking Conference (WCNC 2012), pp. 3339--3344", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-consistency checking is challenging in the dynamic and uncertain\nubiquitous computing environments. This is because contexts are often noisy\nowing to unreliable sensing data streams, inaccurate data measurement, fragile\nconnectivity and resource constraints. One of the state-of-the-art efforts is\nCEDA, which concurrently detects context consistency by exploring the\n\\emph{happened-before} relation among events. However, CEDA is seriously\nlimited by several side effects --- centralized detection manner that easily\ngets down the checker process, heavy computing complexity and false negative.\n  In this paper, we propose SECA: Snapshot-based Event Detection for Checking\nAsynchronous Context Consistency in ubiquitous computing. SECA introduces\nsnapshot-based timestamp to check event relations, which can detect scenarios\nwhere CEDA fails. Moreover, it simplifies the logical clock instead of adopting\nthe vector clock, and thus significantly reduces both time and space\ncomplexity. Empirical studies show that SECA outperforms CEDA in terms of\ndetection accuracy, scalability, and computing complexity.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 10:48:54 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Zhang", "Daqiang", ""], ["Zou", "Qin", ""], ["Sun", "Zhiren", ""]]}, {"id": "1305.3123", "submitter": "Muhammad Hilman", "authors": "Muhammad Hilman, Heru Suhartanto, Arry Yanuar", "title": "Performance Analysis of Embarassingly Parallel Application on Cluster\n  Computer Environment: A Case Study of Virtual Screening with Autodock Vina\n  1.1 on Hastinapura Cluster", "comments": "2010 International Conference on Advanced Computer Science and\n  Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IT based scientific research requires high computational resources. The\nlimitation on funding and infrastructure led the high performance computing era\nfrom supercomputer to cluster and grid computing technology. Parallel\napplication running well on cluster computer as well as supercomputer, one of\nthe type is embarrassingly parallel application. Many scientist loves EP\nbecause it doesn't need any sophisticated technique but gives amazing\nperformance. This paper discuss the bioinformatics research that used\nembarrassingly application and show its performance on cluster computer.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 11:57:20 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Hilman", "Muhammad", ""], ["Suhartanto", "Heru", ""], ["Yanuar", "Arry", ""]]}, {"id": "1305.3338", "submitter": "Daqiang Zhang", "authors": "Ching-Hsien Hsu, Daqiang Zhang, Chao-Tung Yang and Hai-Cheng Chu", "title": "An Efficient Method for Optimizing RFID Reader Deployment and Energy\n  Saving", "comments": "To appear in Sensor Letters, 2013", "journal-ref": "Sensor Letters ISSN: 1546-198X (Print): EISSN: 1546-1971 (Online)\n  , http://www.aspbs.com/sensorlett/", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid proliferation of Radio Frequency IDentification (RFID) systems\nrealizes integration of physical world with the cyber ones. One of the most\npromising is the Internet of Things (IoT), a vision in which the Internet\nextends into our daily activities through wireless networks of uniquely\nidentifiable objects. Given that modern RFID systems are being deployed in\nlarge-scale for different applications, without optimizing reader's\ndistribution, many of the readers will be redundant, resulting waste of energy.\nAdditionally, eliminating redundant eaders can also decrease probability of\nreader collisions, as a result, enhancing system performance and efficiency. In\nthis paper, an overlap aware (OA) technique is proposed for eliminating\nredundant readers. The OA is a distributed approach, which does not need to\ncollect global information for centralizing control, aims to detect maximum\namount of redundant readers could be safely removed or turned off with\npreserving original RFID network coverage. A significant improvement of the OA\nscheme is that the amount of \"write-to-tag\" operations could be largely reduced\nduring the redundant reader identification phase. In order to accurately\nevaluate the performance of the proposed method, it was performed in a variety\nof scenarios. The experiment results show that the proposed method can provide\nreliable performance with detecting higher redundancy and has lower algorithm\noverheads as compared with several well known methods, such as the RRE, LEO,\nthe hybrid algorithm (LEO+RRE) and the DRRE.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 02:14:48 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Hsu", "Ching-Hsien", ""], ["Zhang", "Daqiang", ""], ["Yang", "Chao-Tung", ""], ["Chu", "Hai-Cheng", ""]]}, {"id": "1305.3354", "submitter": "Sandip Chakraborty", "authors": "Sandip Chakraborty, Soumyadip Majumder, Diganta Goswami", "title": "Approximate Congestion Games for Load Balancing in Distributed\n  Environment", "comments": "A version of this work has been presented at International Workshop\n  on Distributed System (IWDS) 2010, IIT Kanpur, India, as a \"work-in-progress\"\n  report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of game theoretic models has been quite successful in describing\nvarious cooperative and non-cooperative optimization problems in networks and\nother domains of computer systems. In this paper, we study an application of\ngame theoretic models in the domain of distributed system, where nodes play a\ngame to balance the total processing loads among themselves. We have used\ncongestion gaming model, a model of game theory where many agents compete for\nallocating resources, and studied the existence of Nash Equilibrium for such\ntypes of games. As the classical congestion game is known to be PLS-Complete,\nwe use an approximation, called the \\epsilon-Congestion game, which converges\nto \\epsilon-Nash equilibrium within finite number of steps under selected\nconditions. Our focus is to define the load balancing problem using the model\nof \\epsilon-congestion games, and finally provide a greedy algorithm for load\nbalancing in distributed systems. We have simulated our proposed system to show\nthe effect of \\epsilon-congestion game, and the distribution of load at\nequilibrium state.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 05:06:02 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Chakraborty", "Sandip", ""], ["Majumder", "Soumyadip", ""], ["Goswami", "Diganta", ""]]}, {"id": "1305.3945", "submitter": "Yanpei Liu", "authors": "Gauri Joshi and Yanpei Liu and Emina Soljanin", "title": "On the Delay-Storage Trade-off in Content Download from Coded\n  Distributed Storage Systems", "comments": "Paper accepted by JSAC-DS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how coding in distributed storage reduces expected\ndownload time, in addition to providing reliability against disk failures. The\nexpected download time is reduced because when a content file is encoded to add\nredundancy and distributed across multiple disks, reading only a subset of the\ndisks is sufficient to reconstruct the content. For the same total storage\nused, coding exploits the diversity in storage better than simple replication,\nand hence gives faster download. We use a novel fork-join queuing framework to\nmodel multiple users requesting the content simultaneously, and derive bounds\non the expected download time. Our system model and results are a novel\ngeneralization of the fork-join system that is studied in queueing theory\nliterature. Our results demonstrate the fundamental trade-off between the\nexpected download time and the amount of storage space. This trade-off can be\nused for design of the amount of redundancy required to meet the delay\nconstraints on content delivery.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 22:03:57 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 21:13:45 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Joshi", "Gauri", ""], ["Liu", "Yanpei", ""], ["Soljanin", "Emina", ""]]}, {"id": "1305.3976", "submitter": "John Stockie", "authors": "Jeffrey K. Wiens and John M. Stockie", "title": "An efficient parallel immersed boundary algorithm using a\n  pseudo-compressible fluid solver", "comments": null, "journal-ref": "Journal of Computational Physics, 281:917-941, 2015", "doi": "10.1016/j.jcp.2014.10.058", "report-no": null, "categories": "cs.DC math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithm for the immersed boundary method on\ndistributed-memory architectures, with the computational complexity of a\ncompletely explicit method and excellent parallel scaling. The algorithm\nutilizes the pseudo-compressibility method recently proposed by Guermond and\nMinev [Comptes Rendus Mathematique, 348:581-585, 2010] that uses a directional\nsplitting strategy to discretize the incompressible Navier-Stokes equations,\nthereby reducing the linear systems to a series of one-dimensional tridiagonal\nsystems. We perform numerical simulations of several fluid-structure\ninteraction problems in two and three dimensions and study the accuracy and\nconvergence rates of the proposed algorithm. For these problems, we compare the\nproposed algorithm against other second-order projection-based fluid solvers.\nLastly, the strong and weak scaling properties of the proposed algorithm are\ninvestigated.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 04:07:45 GMT"}, {"version": "v2", "created": "Mon, 19 May 2014 05:43:27 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Wiens", "Jeffrey K.", ""], ["Stockie", "John M.", ""]]}, {"id": "1305.4196", "submitter": "Ralf Meyer", "authors": "R. Meyer", "title": "Efficient Parallelization of Short-Range Molecular Dynamics Simulations\n  on Many-Core Systems", "comments": "12 pages, 8 figures", "journal-ref": "Phys. Rev. E 88, 053309 (2013)", "doi": "10.1103/PhysRevE.88.053309", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a highly parallel algorithm for molecular dynamics\nsimulations with short-range forces on single node multi- and many-core\nsystems. The algorithm is designed to achieve high parallel speedups for\nstrongly inhomogeneous systems like nanodevices or nanostructured materials. In\nthe proposed scheme the calculation of the forces and the generation of\nneighbor lists is divided into small tasks. The tasks are then executed by a\nthread pool according to a dependent task schedule. This schedule is\nconstructed in such a way that a particle is never accessed by two threads at\nthe same time.Benchmark simulations on a typical 12 core machine show that the\ndescribed algorithm achieves excellent parallel efficiencies above 80 % for\ndifferent kinds of systems and all numbers of cores. For inhomogeneous systems\nthe speedups are strongly superior to those obtained with spatial\ndecomposition. Further benchmarks were performed on an Intel Xeon Phi\ncoprocessor. These simulations demonstrate that the algorithm scales well to\nlarge numbers of cores.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 21:41:22 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2013 18:21:46 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2013 21:41:52 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Meyer", "R.", ""]]}, {"id": "1305.4228", "submitter": "Wei Yu", "authors": "Wei Yu, Junpeng Chen", "title": "The state-of-the-art in web-scale semantic information processing for\n  cloud computing", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on integrated infrastructure of resource sharing and computing in\ndistributed environment, cloud computing involves the provision of dynamically\nscalable and provides virtualized resources as services over the Internet.\nThese applications also bring a large scale heterogeneous and distributed\ninformation which pose a great challenge in terms of the semantic ambiguity. It\nis critical for application services in cloud computing environment to provide\nusers intelligent service and precise information. Semantic information\nprocessing can help users deal with semantic ambiguity and information overload\nefficiently through appropriate semantic models and semantic information\nprocessing technology. The semantic information processing have been\nsuccessfully employed in many fields such as the knowledge representation,\nnatural language understanding, intelligent web search, etc. The purpose of\nthis report is to give an overview of existing technologies for semantic\ninformation processing in cloud computing environment, to propose a research\ndirection for addressing distributed semantic reasoning and parallel semantic\ncomputing by exploiting semantic information newly available in cloud computing\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 05:36:16 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Yu", "Wei", ""], ["Chen", "Junpeng", ""]]}, {"id": "1305.4263", "submitter": "Peva Blanchard", "authors": "Peva Blanchard, Shlomi Dolev, Joffroy Beauquier, Sylvie Dela\\\"et", "title": "Self-Stabilizing Paxos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first self-stabilizing consensus and replicated state machine\nfor asynchronous message passing systems. The scheme does not require that all\nparticipants make a certain number of steps prior to reaching a practically\ninfinite execution where the replicated state machine exhibits the desired\nbehavior. In other words, the system reaches a configuration from which it\noperates according to the specified requirements of the replicated\nstate-machine, for a long enough execution regarding all practical\nconsiderations.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 13:20:11 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Blanchard", "Peva", ""], ["Dolev", "Shlomi", ""], ["Beauquier", "Joffroy", ""], ["Dela\u00ebt", "Sylvie", ""]]}, {"id": "1305.4365", "submitter": "Anjan Krishnamurthy", "authors": "Anjan K. Koundinya, Harish G., Srinath N. K., Raghavendra G. E.,\n  Pramod Y. V., Sandeep R. and Punith Kumar G", "title": "Performance Analysis of Parallel Pollard's Rho Algorithm", "comments": "8 pages, 4 figures, Journal", "journal-ref": null, "doi": "10.5121/ijcsit.2013.5214", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer factorization is one of the vital algorithms discussed as a part of\nanalysis of any black-box cipher suites where the cipher algorithm is based on\nnumber theory. The origin of the problem is from Discrete Logarithmic Problem\nwhich appears under the analysis of the crypto-graphic algorithms as seen by a\ncrypt-analyst. The integer factorization algorithm poses a potential in\ncomputational science too, obtaining the factors of a very large number is\nchallenging with a limited computing infrastructure. This paper analyses the\nPollards Rho heuristic with a varying input size to evaluate the performance\nunder a multi-core environment and also to estimate the threshold for each\ncomputing infrastructure.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 14:44:57 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Koundinya", "Anjan K.", ""], ["G.", "Harish", ""], ["K.", "Srinath N.", ""], ["E.", "Raghavendra G.", ""], ["V.", "Pramod Y.", ""], ["R.", "Sandeep", ""], ["G", "Punith Kumar", ""]]}, {"id": "1305.4367", "submitter": "Raphael Jolly", "authors": "Rapha\\\"el Jolly", "title": "Parallelizing Stream with Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream is re-interpreted in terms of a Lazy monad. Future is substituted for\nLazy in the obtained construct, resulting in possible parallelization of any\nalgorithm expressible as a Stream computation. The principle is tested against\ntwo example algorithms. Performance is evaluated, and a way to improve it\nbriefly discussed.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 15:00:14 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Jolly", "Rapha\u00ebl", ""]]}, {"id": "1305.4376", "submitter": "Lukasz Swierczewski", "authors": "Lukasz Swierczewski", "title": "3DES ECB Optimized for Massively Parallel CUDA GPU Architecture", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Modern computers have graphics cards with much higher theoretical efficiency\nthan conventional CPU. The paper presents application possibilities GPU CUDA\nacceleration for encryption of data using the new architecture tailored to the\n3DES algorithm, characterized by increased security compared to the normal DES.\nThe algorithm used in ECB mode (Electronic Codebook), in which 64-bit data\nblocks are encrypted independently by stream processors (CUDA cores).\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 16:02:42 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Swierczewski", "Lukasz", ""]]}, {"id": "1305.4675", "submitter": "Amitabh Trehan", "authors": "Amitabh Trehan", "title": "Algorithms for Self-Healing Networks", "comments": "Ph.D. Dissertation. University of New Mexico Computer Science, May\n  2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern networks are \\emph{reconfigurable}, in the sense that the\ntopology of the network can be changed by the nodes in the network. For\nexample, peer-to-peer, wireless and ad-hoc networks are reconfigurable. More\ngenerally, many social networks, such as a company's organizational chart;\ninfrastructure networks, such as an airline's transportation network; and\nbiological networks, such as the human brain, are also reconfigurable. Modern\nreconfigurable networks have a complexity unprecedented in the history of\nengineering, resembling more a dynamic and evolving living animal rather than a\nstructure of steel designed from a blueprint. Unfortunately, our mathematical\nand algorithmic tools have not yet developed enough to handle this complexity\nand fully exploit the flexibility of these networks.\n  We believe that it is no longer possible to build networks that are scalable\nand never have node failures. Instead, these networks should be able to admit\nsmall, and maybe, periodic failures and still recover like skin heals from a\ncut. This process, where the network can recover itself by maintaining key\ninvariants in response to attack by a powerful adversary is what we call\n\\emph{self-healing}.\n  Here, we present several fast and provably good distributed algorithms for\nself-healing in reconfigurable dynamic networks. Each of these algorithms have\ndifferent properties, a different set of gaurantees and limitations. We also\ndiscuss future directions and theoretical questions we would like to answer.\n%in the final dissertation that this document is proposed to lead to.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 23:05:46 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Trehan", "Amitabh", ""]]}, {"id": "1305.4781", "submitter": "Martin Horsch", "authors": "Martin Horsch and Christoph Niethammer and Jadran Vrabec and Hans\n  Hasse", "title": "Computational molecular engineering as an emerging technology in process\n  engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present level of development of molecular force field methods is assessed\nfrom the point of view of simulation-based engineering, outlining the immediate\nperspective for further development and highlighting the newly emerging\ndiscipline of Computational Molecular Engineering (CME) which makes basic\nresearch in soft matter physics fruitful for industrial applications. Within\nthe coming decade, major breakthroughs can be reached if a research focus is\nplaced on processes at interfaces, combining aspects where an increase in the\naccessible length and time scales due to massively parallel high-performance\ncomputing will lead to particularly significant improvements.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 10:54:38 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Horsch", "Martin", ""], ["Niethammer", "Christoph", ""], ["Vrabec", "Jadran", ""], ["Hasse", "Hans", ""]]}, {"id": "1305.4868", "submitter": "Dan Dobre Dan Dobre", "authors": "Christian Cachin, Dan Dobre, Marko Vukolic", "title": "Asynchronous BFT Storage with 2t+1 Data Replicas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of Byzantine Fault Tolerant (BFT) storage is the main concern\npreventing its adoption in practice. This cost stems from the need to maintain\nat least 3t+1 replicas in different storage servers in the asynchronous model,\nso that t Byzantine replica faults can be tolerated. In this paper, we present\nMDStore, the first fully asynchronous read/write BFT storage protocol that\nreduces the number of data replicas to as few as 2t+1, maintaining 3t+1\nreplicas of metadata at (possibly) different servers. At the heart of MDStore\nstore is its metadata service that is built upon a new abstraction we call\ntimestamped storage. Timestamped storage both allows for conditional writes\n(facilitating the implementation of a metadata service) and has consensus\nnumber one (making it implementable wait-free in an asynchronous system despite\nfaults). In addition to its low data replication factor, MDStore offers very\nstrong guarantees implementing multi-writer multi-reader atomic wait-free\nsemantics and tolerating any number of Byzantine readers and crash-faulty\nwriters. We further show that MDStore data replication overhead is optimal;\nnamely, we prove a lower bound of 2t+1 on the number of data replicas that\napplies even to crash-tolerant storage with a fault-free metadata service\noracle. Finally, we prove that separating data from metadata for reducing the\ncost of BFT storage is not possible without cryptographic assumptions. However,\nour MDStore protocol uses only lightweight cryptographic hash functions.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 16:06:23 GMT"}, {"version": "v2", "created": "Fri, 24 May 2013 10:13:27 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2014 12:05:02 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Cachin", "Christian", ""], ["Dobre", "Dan", ""], ["Vukolic", "Marko", ""]]}, {"id": "1305.5120", "submitter": "Edoardo Di Napoli", "authors": "Mario Berljafa (1) and Edoardo Di Napoli (2 and 3) ((1) Department of\n  Mathematics, University of Zagreb, (2) Juelich Supercomputing Centre,\n  Forschungszentrum Juelich, (3) AICES, RWTH Aachen)", "title": "A Parallel and Scalable Iterative Solver for Sequences of Dense\n  Eigenproblems Arising in FLAPW", "comments": "Submitted to 10th International Conference on Parallel Processing and\n  Applied Mathematics(PPAM 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In one of the most important methods in Density Functional Theory - the\nFull-Potential Linearized Augmented Plane Wave (FLAPW) method - dense\ngeneralized eigenproblems are organized in long sequences. Moreover each\neigenproblem is strongly correlated to the next one in the sequence. We propose\na novel approach which exploits such correlation through the use of an\neigensolver based on subspace iteration and accelerated with Chebyshev\npolynomials. The resulting solver, parallelized using the Elemental library\nframework, achieves excellent scalability and is competitive with current dense\nparallel eigensolvers.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 10:08:01 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Berljafa", "Mario", "", "2 and 3"], ["Di Napoli", "Edoardo", "", "2 and 3"]]}, {"id": "1305.5179", "submitter": "Salvatore Cuomo", "authors": "Salvatore Cuomo and Ardelio Gallettiy and Giulio Giuntay and Alfredo\n  Staracey", "title": "Surface Reconstruction from Scattered Point via RBF Interpolation on GPU", "comments": "arXiv admin note: text overlap with arXiv:0909.5413 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a parallel implicit method based on radial basis\nfunctions (RBF) for surface reconstruction. The applicability of RBF methods is\nhindered by its computational demand, that requires the solution of linear\nsystems of size equal to the number of data points. Our reconstruction\nimplementation relies on parallel scientific libraries and is supported for\nmassively multi-core architectures, namely Graphic Processor Units (GPUs). The\nperformance of the proposed method in terms of accuracy of the reconstruction\nand computing time shows that the RBF interpolant can be very effective for\nsuch problem.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 16:08:33 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Cuomo", "Salvatore", ""], ["Gallettiy", "Ardelio", ""], ["Giuntay", "Giulio", ""], ["Staracey", "Alfredo", ""]]}, {"id": "1305.5520", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari, Fabian Kuhn", "title": "Distributed Minimum Cut Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing approximate minimum edge cuts by\ndistributed algorithms. We use a standard synchronous message passing model\nwhere in each round, $O(\\log n)$ bits can be transmitted over each edge (a.k.a.\nthe CONGEST model). We present a distributed algorithm that, for any weighted\ngraph and any $\\epsilon \\in (0, 1)$, with high probability finds a cut of size\nat most $O(\\epsilon^{-1}\\lambda)$ in $O(D) + \\tilde{O}(n^{1/2 + \\epsilon})$\nrounds, where $\\lambda$ is the size of the minimum cut. This algorithm is based\non a simple approach for analyzing random edge sampling, which we call the\nrandom layering technique. In addition, we also present another distributed\nalgorithm, which is based on a centralized algorithm due to Matula [SODA '93],\nthat with high probability computes a cut of size at most $(2+\\epsilon)\\lambda$\nin $\\tilde{O}((D+\\sqrt{n})/\\epsilon^5)$ rounds for any $\\epsilon>0$.\n  The time complexities of both of these algorithms almost match the\n$\\tilde{\\Omega}(D + \\sqrt{n})$ lower bound of Das Sarma et al. [STOC '11], thus\nleading to an answer to an open question raised by Elkin [SIGACT-News '04] and\nDas Sarma et al. [STOC '11].\n  Furthermore, we also strengthen the lower bound of Das Sarma et al. by\nextending it to unweighted graphs. We show that the same lower bound also holds\nfor unweighted multigraphs (or equivalently for weighted graphs in which\n$O(w\\log n)$ bits can be transmitted in each round over an edge of weight $w$),\neven if the diameter is $D=O(\\log n)$. For unweighted simple graphs, we show\nthat even for networks of diameter $\\tilde{O}(\\frac{1}{\\lambda}\\cdot\n\\sqrt{\\frac{n}{\\alpha\\lambda}})$, finding an $\\alpha$-approximate minimum cut\nin networks of edge connectivity $\\lambda$ or computing an\n$\\alpha$-approximation of the edge connectivity requires $\\tilde{\\Omega}(D +\n\\sqrt{\\frac{n}{\\alpha\\lambda}})$ rounds.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 19:13:15 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 19:16:07 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1305.5608", "submitter": "Wei Wang", "authors": "Wei Wang, Baochun Li, Ben Liang", "title": "To Reserve or Not to Reserve: Optimal Online Multi-Instance Acquisition\n  in IaaS Clouds", "comments": "Part of this paper has appeared in USENIX ICAC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrastructure-as-a-Service (IaaS) clouds offer diverse instance purchasing\noptions. A user can either run instances on demand and pay only for what it\nuses, or it can prepay to reserve instances for a long period, during which a\nusage discount is entitled. An important problem facing a user is how these two\ninstance options can be dynamically combined to serve time-varying demands at\nminimum cost. Existing strategies in the literature, however, require either\nexact knowledge or the distribution of demands in the long-term future, which\nsignificantly limits their use in practice. Unlike existing works, we propose\ntwo practical online algorithms, one deterministic and another randomized, that\ndynamically combine the two instance options online without any knowledge of\nthe future. We show that the proposed deterministic (resp., randomized)\nalgorithm incurs no more than 2-alpha (resp., e/(e-1+alpha)) times the minimum\ncost obtained by an optimal offline algorithm that knows the exact future a\npriori, where alpha is the entitled discount after reservation. Our online\nalgorithms achieve the best possible competitive ratios in both the\ndeterministic and randomized cases, and can be easily extended to cases when\nshort-term predictions are reliable. Simulations driven by a large volume of\nreal-world traces show that significant cost savings can be achieved with\nprevalent IaaS prices.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 02:58:50 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Wang", "Wei", ""], ["Li", "Baochun", ""], ["Liang", "Ben", ""]]}, {"id": "1305.5639", "submitter": "Ruonan Wang", "authors": "Ruonan Wang and Christopher Harris", "title": "Scaling Radio Astronomy Signal Correlation on Heterogeneous\n  Supercomputers Using Various Data Distribution Methodologies", "comments": null, "journal-ref": null, "doi": "10.1007/s10686-013-9340-7", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation radio telescopes will require orders of magnitude more\ncomputing power to provide a view of the universe with greater sensitivity. In\nthe initial stages of the signal processing flow of a radio telescope, signal\ncorrelation is one of the largest challenges in terms of handling huge data\nthroughput and intensive computations. We implemented a GPU cluster based\nsoftware correlator with various data distribution models and give a systematic\ncomparison based on testing results obtained using the Fornax supercomputer. By\nanalyzing the scalability and throughput of each model, optimal approaches are\nidentified across a wide range of problem sizes, covering the scale of next\ngeneration telescopes.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 07:20:54 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Wang", "Ruonan", ""], ["Harris", "Christopher", ""]]}, {"id": "1305.5800", "submitter": "Ilya Mirsky", "authors": "Dave Dice, Danny Hendler and Ilya Mirsky", "title": "Lightweight Contention Management for Efficient Compare-and-Swap\n  Operations", "comments": "25 pages, to be published in Euro-Par 2013 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many concurrent data-structure implementations use the well-known\ncompare-and-swap (CAS) operation, supported in hardware by most modern\nmultiprocessor architectures for inter-thread synchronization. A key weakness\nof the CAS operation is the degradation in its performance in the presence of\nmemory contention.\n  In this work we study the following question: can software-based contention\nmanagement improve the efficiency of hardware-provided CAS operations? Our\nperformance evaluation establishes that lightweight contention management\nsupport can greatly improve performance under medium and high contention levels\nwhile typically incurring only small overhead when contention is low.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 17:05:51 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Dice", "Dave", ""], ["Hendler", "Danny", ""], ["Mirsky", "Ilya", ""]]}, {"id": "1305.5826", "submitter": "Kian Hsiang Low", "authors": "Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan\n  Tan, Patrick Jaillet", "title": "Parallel Gaussian Process Regression with Low-Rank Covariance Matrix\n  Approximations", "comments": "29th Conference on Uncertainty in Artificial Intelligence (UAI 2013),\n  Extended version with proofs, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are Bayesian non-parametric models that are widely\nused for probabilistic regression. Unfortunately, it cannot scale well with\nlarge data nor perform real-time predictions due to its cubic time cost in the\ndata size. This paper presents two parallel GP regression methods that exploit\nlow-rank covariance matrix approximations for distributing the computational\nload among parallel machines to achieve time efficiency and scalability. We\ntheoretically guarantee the predictive performances of our proposed parallel\nGPs to be equivalent to that of some centralized approximate GP regression\nmethods: The computation of their centralized counterparts can be distributed\namong parallel machines, hence achieving greater time efficiency and\nscalability. We analytically compare the properties of our parallel GPs such as\ntime, space, and communication complexity. Empirical evaluation on two\nreal-world datasets in a cluster of 20 computing nodes shows that our parallel\nGPs are significantly more time-efficient and scalable than their centralized\ncounterparts and exact/full GP while achieving predictive performances\ncomparable to full GP.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 19:00:28 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Chen", "Jie", ""], ["Cao", "Nannan", ""], ["Low", "Kian Hsiang", ""], ["Ouyang", "Ruofei", ""], ["Tan", "Colin Keng-Yan", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1305.6113", "submitter": "EPTCS", "authors": "Frank Zeyda (University of York), Ana Cavalcanti (University of York)", "title": "Refining SCJ Mission Specifications into Parallel Handler Designs", "comments": "In Proceedings Refine 2013, arXiv:1305.5634", "journal-ref": "EPTCS 115, 2013, pp. 52-67", "doi": "10.4204/EPTCS.115.4", "report-no": null, "categories": "cs.LO cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety-Critical Java (SCJ) is a recent technology that restricts the\nexecution and memory model of Java in such a way that applications can be\nstatically analysed and certified for their real-time properties and safe use\nof memory. Our interest is in the development of comprehensive and sound\ntechniques for the formal specification, refinement, design, and implementation\nof SCJ programs, using a correct-by-construction approach. As part of this\nwork, we present here an account of laws and patterns that are of general use\nfor the refinement of SCJ mission specifications into designs of parallel\nhandlers used in the SCJ programming paradigm. Our notation is a combination of\nlanguages from the Circus family, supporting state-rich reactive models with\nthe addition of class objects and real-time properties. Our work is a first\nstep to elicit laws of programming for SCJ and fits into a refinement strategy\nthat we have developed previously to derive SCJ programs.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 05:40:09 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Zeyda", "Frank", "", "University of York"], ["Cavalcanti", "Ana", "", "University of York"]]}, {"id": "1305.6123", "submitter": "Murali Kalyan mr", "authors": "S.M.M.M Kalyan kumar and Dr S C Pradhan", "title": "Building Internal Cloud at NIC : A Preview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most of computing environments in the IT support organization like NIC\nare designed to run in centralized datacentre. The centralized infrastructure\nof various development projects are used to deploy their services on it and\nconnecting remotely to that datacentre from all the stations of organization.\nCurrently these servers are mostly underutilized due to the static and\nconventional approaches used for accessing and utilizing of these resources.\nThe cloud patterns is much needful for optimizing resource utilization and\nreducing the investments on unnecessary costs. So, we build up and prototyped a\nprivate cloud system called nIC(NIC Internal Cloud) to leverage the benefits of\ncloud environment. For this system we adopted the combination of various\ntechniques from open source software community. The user-base of nIC consists\ndevelopers, web and database admins, service providers and desktop users from\nvarious projects in NIC. We can optimize the resource usage by customizing the\nuser based template services on these virtualized infrastructure. It will also\nincreases the flexibility of the managing and maintenance of the operations\nlike archiving, disaster recovery and scaling of resources. The open-source\napproach is further decreases the enterprise costs. In this paper, we describe\nthe design and analysis of implementing issues on internal cloud environments\nin NIC and similar organizations.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 06:35:01 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["kumar", "S. M. M. M Kalyan", ""], ["Pradhan", "Dr S C", ""]]}, {"id": "1305.6203", "submitter": "Grigore Stamatescu", "authors": "Iulia Dumitru, Grigore Stamatescu, Ioana Fagarasan, Sergiu Stelian\n  Iliescu", "title": "Dynamic Management Techniques for Increasing Energy Efficiency within a\n  Data Center", "comments": "5 pages, 3 figures", "journal-ref": "Proc. of the 1st UNITE Doctoral Symposium, pp. 129-133, Bucharest,\n  Romania, 27-28 June, 2011, PRINTECH, ISSN 2247-6040", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ours days data centers provide the global community an indispensable\nservice: nearly unlimited access to almost any kind of information we can\nimagine by supporting most Internet services such as: Web hosting and\nE-commerce services. Because of their capacity and their work, data centers\nhave various impacts on the environment, but those related with the electricity\nuse are by far the most important. In this paper, we present several power and\nenergy management techniques for data centers and we will focus our attention\non techniques that are explicitly tailored to servers and their workloads.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 13:06:36 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Dumitru", "Iulia", ""], ["Stamatescu", "Grigore", ""], ["Fagarasan", "Ioana", ""], ["Iliescu", "Sergiu Stelian", ""]]}, {"id": "1305.6325", "submitter": "Crist\\'obal A. Navarro", "authors": "Cristobal A. Navarro, Fabrizio Canfora, Nancy Hitschfeld Kahler", "title": "Multi-core computation of transfer matrices for strip lattices in the\n  Potts model", "comments": null, "journal-ref": null, "doi": "10.1109/HPCC.and.EUC.2013.27", "report-no": null, "categories": "physics.comp-ph cond-mat.stat-mech cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transfer-matrix technique is a convenient way for studying strip lattices\nin the Potts model since the compu- tational costs depend just on the periodic\npart of the lattice and not on the whole. However, even when the cost is\nreduced, the transfer-matrix technique is still an NP-hard problem since the\ntime T(|V|, |E|) needed to compute the matrix grows ex- ponentially as a\nfunction of the graph width. In this work, we present a parallel\ntransfer-matrix implementation that scales performance under multi-core\narchitectures. The construction of the matrix is based on several repetitions\nof the deletion- contraction technique, allowing parallelism suitable to\nmulti-core machines. Our experimental results show that the multi-core\nimplementation achieves speedups of 3.7X with p = 4 processors and 5.7X with p\n= 8. The efficiency of the implementation lies between 60% and 95%, achieving\nthe best balance of speedup and efficiency at p = 4 processors for actual\nmulti-core architectures. The algorithm also takes advantage of the lattice\nsymmetry, making the transfer matrix computation to run up to 2X faster than\nits non-symmetric counterpart and use up to a quarter of the original space.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 20:57:53 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2013 02:44:55 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Navarro", "Cristobal A.", ""], ["Canfora", "Fabrizio", ""], ["Kahler", "Nancy Hitschfeld", ""]]}, {"id": "1305.6474", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Martin Wimmer, Daniel Cederman, Jesper Larsson Tr\\\"aff, and Philippas\n  Tsigas", "title": "Configurable Strategies for Work-stealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work-stealing systems are typically oblivious to the nature of the tasks they\nare scheduling. For instance, they do not know or take into account how long a\ntask will take to execute or how many subtasks it will spawn. Moreover, the\nactual task execution order is typically determined by the underlying task\nstorage data structure, and cannot be changed. There are thus possibilities for\noptimizing task parallel executions by providing information on specific tasks\nand their preferred execution order to the scheduling system.\n  We introduce scheduling strategies to enable applications to dynamically\nprovide hints to the task-scheduling system on the nature of specific tasks.\nScheduling strategies can be used to independently control both local task\nexecution order as well as steal order. In contrast to conventional scheduling\npolicies that are normally global in scope, strategies allow the scheduler to\napply optimizations on individual tasks. This flexibility greatly improves\ncomposability as it allows the scheduler to apply different, specific\nscheduling choices for different parts of applications simultaneously. We\npresent a number of benchmarks that highlight diverse, beneficial effects that\ncan be achieved with scheduling strategies. Some benchmarks (branch-and-bound,\nsingle-source shortest path) show that prioritization of tasks can reduce the\ntotal amount of work compared to standard work-stealing execution order. For\nother benchmarks (triangle strip generation) qualitatively better results can\nbe achieved in shorter time. Other optimizations, such as dynamic merging of\ntasks or stealing of half the work, instead of half the tasks, are also shown\nto improve performance. Composability is demonstrated by examples that combine\ndifferent strategies, both within the same kernel (prefix sum) as well as when\nscheduling multiple kernels (prefix sum and unbalanced tree search).\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 12:59:25 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Wimmer", "Martin", ""], ["Cederman", "Daniel", ""], ["Tr\u00e4ff", "Jesper Larsson", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1305.6624", "submitter": "Sathya Peri", "authors": "Priyanka Kumar and Sathya Peri", "title": "A TimeStamp based Multi-version STM Protocol that satisfies Opacity and\n  Multi-Version Permissiveness", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Transactional Memory Systems (STM) are a promising alternative to\nlock based systems for concurrency control in shared memory systems. In\nmultiversion STM systems, each write on a transaction object produces a new\nversion of that object. The advantage obtained by storing multiple versions is\nthat one can ensure that read operations do not fail. Opacity is a commonly\nused correctness criterion for STM systems. Multi-Version permissive STM system\nnever aborts a read-only transaction. Although many multi-version STM systems\nhave been proposed, to the best of our knowledge none of them have been\nformally proved to satisfy opacity. In this paper we present a time-stamp based\nmultiversion STM system that satisfies opacity and mv-permissiveness. We\nformally prove the correctness of the proposed STM system. We also present\ngarbage collection procedure which deletes unwanted versions of the transaction\nobjects and formally prove it correctness.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 20:41:06 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Kumar", "Priyanka", ""], ["Peri", "Sathya", ""]]}, {"id": "1305.6738", "submitter": "Efstratios Rappos", "authors": "Efstratios Rappos and Stephan Robert", "title": "Using GPU Simulation to Accurately Fit to the Power-Law Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC physics.comp-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a methodology for fitting experimental data to the\ndiscrete power-law distribution and provides the results of a detailed\nsimulation exercise used to calculate accurate cutoff values used to assess the\nfit to a power-law distribution when using the maximum likelihood estimation\nfor the exponent of the distribution. Using massively parallel programming\ncomputing, we were able to accelerate by a factor of 60 the computational time\nrequired for these calculations across a range of parameters and construct a\nseries of detailed tables containing the test values to be used in a\nKolmogorov-Smirnov goodness-of-fit test, allowing for an accurate assessment of\nthe power-law fit from empirical data.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2013 09:26:41 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Rappos", "Efstratios", ""], ["Robert", "Stephan", ""]]}, {"id": "1305.6861", "submitter": "J\\\"urgen K\\\"ursch", "authors": "J\\\"urgen K\\\"ursch", "title": "Design and Realization of a Scalable Simulator of Magnetic Resonance\n  Tomography", "comments": "PhD thesis, RWTH Aachen, Germany, 2003, in German, available at\n  http://darwin.bth.rwth-aachen.de/opus3/volltexte/2003/709/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In research activities regarding Magnetic Resonance Imaging in medicine,\nsimulation tools with a universal approach are rare. Usually, simulators are\ndeveloped and used which tend to be restricted to a particular, small range of\napplications. This led to the design and implementation of a new simulator\nPARSPIN, the subject of this thesis. In medical applications, the Bloch\nequation is a well-suited mathematical model of the underlying physics with a\nwide scope. In this thesis, it is shown how analytical solutions of the Bloch\nequation can be found, which promise substantial execution time advantages over\nnumerical solution methods. From these analytical solutions of the Bloch\nequation, a new formalism for the description and the analysis of complex\nimaging experiments is derived, the K-t formalism. It is shown that modern\nimaging methods can be better explained by the K-t formalism than by observing\nand analysing the magnetization of each spin of a spin ensemble. Various\napproaches for a numerical simulation of Magnetic Resonance imaging are\ndiscussed. It is shown that a simulation tool based on the K-t formalism\npromises a substantial gain in execution time. Proper spatial discretization\naccording to the sampling theorem, a topic rarely discussed in literature, is\nuniversally derived from the K-t formalism in this thesis. A spin-based\nsimulator is an application with high demands to computing facilities even on\nmodern hardware. In this thesis, two approaches for a parallelized software\narchitecture are designed, analysed and evaluated with regard to a reduction of\nexecution time. A number of possible applications in research and education are\ndemonstrated. For a choice of imaging experiments, results produced both\nexperimentally and by simulation are compared.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2013 16:37:13 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["K\u00fcrsch", "J\u00fcrgen", ""]]}, {"id": "1305.7167", "submitter": "Pavel Zaichenkov", "authors": "Pavel Zaichenkov (1 and 4), Bert Gijsbers (2 and 3), Clemens Grelck\n  (3), Olga Tveretina (1), Alex Shafarenko (1) ((1) University of\n  Hertfordshire, (2) Ghent University, (3) University of Amsterdam, (4) Moscow\n  Institute of Physics and Technology)", "title": "A Case Study in Coordination Programming: Performance Evaluation of\n  S-Net vs Intel's Concurrent Collections", "comments": "9 pages, 8 figures, 1 table, accepted for PLC 2014 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a programming methodology and runtime performance case study\ncomparing the declarative data flow coordination language S-Net with Intel's\nConcurrent Collections (CnC). As a coordination language S-Net achieves a\nnear-complete separation of concerns between sequential software components\nimplemented in a separate algorithmic language and their parallel orchestration\nin an asynchronous data flow streaming network. We investigate the merits of\nS-Net and CnC with the help of a relevant and non-trivial linear algebra\nproblem: tiled Cholesky decomposition. We describe two alternative S-Net\nimplementations of tiled Cholesky factorization and compare them with two CnC\nimplementations, one with explicit performance tuning and one without, that\nhave previously been used to illustrate Intel CnC. Our experiments on a 48-core\nmachine demonstrate that S-Net manages to outperform CnC on this problem.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 17:21:26 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2013 11:01:14 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2013 00:40:07 GMT"}, {"version": "v4", "created": "Thu, 3 Apr 2014 12:24:47 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Zaichenkov", "Pavel", "", "1 and 4"], ["Gijsbers", "Bert", "", "2 and 3"], ["Grelck", "Clemens", ""], ["Tveretina", "Olga", ""], ["Shafarenko", "Alex", ""]]}, {"id": "1305.7383", "submitter": "Antonio Villani", "authors": "Roberto Di Pietro, Flavio Lombardi, Antonio Villani", "title": "CUDA Leaks: Information Leakage in GPU Architectures", "comments": null, "journal-ref": null, "doi": "10.1145/2801153", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Processing Units (GPUs) are deployed on most present server,\ndesktop, and even mobile platforms. Nowadays, a growing number of applications\nleverage the high parallelism offered by this architecture to speed-up general\npurpose computation. This phenomenon is called GPGPU computing (General Purpose\nGPU computing). The aim of this work is to discover and highlight security\nissues related to CUDA, the most widespread platform for GPGPU computing. In\nparticular, we provide details and proofs-of-concept about a novel set of\nvulnerabilities CUDA architectures are subject to, that could be exploited to\ncause severe information leak. Following (detailed) intuitions rooted on sound\nengineering security, we performed several experiments targeting the last two\ngenerations of CUDA devices: Fermi and Kepler. We discovered that these two\nfamilies do suffer from information leakage vulnerabilities. In particular,\nsome vulnerabilities are shared between the two architectures, while others are\nidiosyncratic of the Kepler architecture. As a case study, we report the impact\nof one of these vulnerabilities on a GPU implementation of the AES encryption\nalgorithm. We also suggest software patches and alternative approaches to\ntackle the presented vulnerabilities. To the best of our knowledge this is the\nfirst work showing that information leakage in CUDA is possible using just\nstandard CUDA instructions. We expect our work to pave the way for further\nresearch in the field.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 12:57:56 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 14:55:46 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Di Pietro", "Roberto", ""], ["Lombardi", "Flavio", ""], ["Villani", "Antonio", ""]]}, {"id": "1305.7403", "submitter": "Adam Barker", "authors": "Jonathan Stuart Ward and Adam Barker", "title": "Monitoring Large-Scale Cloud Systems with Layered Gossip Protocols", "comments": "Extended Abstract for the ACM International Symposium on\n  High-Performance Parallel and Distributed Computing (HPDC 2013) Poster Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring is an essential aspect of maintaining and developing computer\nsystems that increases in difficulty proportional to the size of the system.\nThe need for robust monitoring tools has become more evident with the advent of\ncloud computing. Infrastructure as a Service (IaaS) clouds allow end users to\ndeploy vast numbers of virtual machines as part of dynamic and transient\narchitectures. Current monitoring solutions, including many of those in the\nopen-source domain rely on outdated concepts including manual deployment and\nconfiguration, centralised data collection and adapt poorly to membership\nchurn.\n  In this paper we propose the development of a cloud monitoring suite to\nprovide scalable and robust lookup, data collection and analysis services for\nlarge-scale cloud systems. In lieu of centrally managed monitoring we propose a\nmulti-tier architecture using a layered gossip protocol to aggregate monitoring\ninformation and facilitate lookup, information collection and the\nidentification of redundant capacity. This allows for a resource aware data\ncollection and storage architecture that operates over the system being\nmonitored. This in turn enables monitoring to be done in-situ without the need\nfor significant additional infrastructure to facilitate monitoring services. We\nevaluate this approach against alternative monitoring paradigms and demonstrate\nhow our solution is well adapted to usage in a cloud-computing context.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 14:11:15 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Ward", "Jonathan Stuart", ""], ["Barker", "Adam", ""]]}, {"id": "1305.7429", "submitter": "Petr  Kuznetsov", "authors": "Marco Canini, Petr Kuznetsov, Dan Levin, Stefan Schmid", "title": "A Distributed SDN Control Plane for Consistent Policy Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-defined networking (SDN) is a novel paradigm that out-sources the\ncontrol of packet-forwarding switches to a set of software controllers. The\nmost fundamental task of these controllers is the correct implementation of the\n\\emph{network policy}, i.e., the intended network behavior. In essence, such a\npolicy specifies the rules by which packets must be forwarded across the\nnetwork.\n  This paper studies a distributed SDN control plane that enables\n\\emph{concurrent} and \\emph{robust} policy implementation. We introduce a\nformal model describing the interaction between the data plane and a\ndistributed control plane (consisting of a collection of fault-prone\ncontrollers). Then we formulate the problem of \\emph{consistent} composition of\nconcurrent network policy updates (short: the \\emph{CPC Problem}). To\nanticipate scenarios in which some conflicting policy updates must be rejected,\nwe enable the composition via a natural \\emph{transactional} interface with\nall-or-nothing semantics.\n  We show that the ability of an $f$-resilient distributed control plane to\nprocess concurrent policy updates depends on the tag complexity, i. e., the\nnumber of policy labels (a.k.a. \\emph{tags}) available to the controllers, and\ndescribe a CPC protocol with optimal tag complexity $f+2$.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 14:45:31 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 09:31:47 GMT"}, {"version": "v3", "created": "Tue, 20 May 2014 16:40:20 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Canini", "Marco", ""], ["Kuznetsov", "Petr", ""], ["Levin", "Dan", ""], ["Schmid", "Stefan", ""]]}]