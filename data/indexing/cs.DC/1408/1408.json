[{"id": "1408.0384", "submitter": "Chhaya Trehan", "authors": "Shay Kutten and Chhaya Trehan", "title": "Fast and Compact Distributed Verification and Self-Stabilization of a\n  DFS Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for distributed verification and silent-stabilization\nof a DFS(Depth First Search) spanning tree of a connected network. Computing\nand maintaining such a DFS tree is an important task, e.g., for constructing\nefficient routing schemes. Our algorithm improves upon previous work in various\nways. Comparable previous work has space and time complexities of $O(n\\log\n\\Delta)$ bits per node and $O(nD)$ respectively, where $\\Delta$ is the highest\ndegree of a node, $n$ is the number of nodes and $D$ is the diameter of the\nnetwork. In contrast, our algorithm has a space complexity of $O(\\log n)$ bits\nper node, which is optimal for silent-stabilizing spanning trees and runs in\n$O(n)$ time. In addition, our solution is modular since it utilizes the\ndistributed verification algorithm as an independent subtask of the overall\nsolution. It is possible to use the verification algorithm as a stand alone\ntask or as a subtask in another algorithm. To demonstrate the simplicity of\nconstructing efficient DFS algorithms using the modular approach, We also\npresent a (non-sielnt) self-stabilizing DFS token circulation algorithm for\ngeneral networks based on our silent-stabilizing DFS tree. The complexities of\nthis token circulation algorithm are comparable to the known ones.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 15:19:46 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 22:18:10 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Kutten", "Shay", ""], ["Trehan", "Chhaya", ""]]}, {"id": "1408.0395", "submitter": "Matthias Feldotto", "authors": "Matthias Feldotto and Christian Scheideler and Kalman Graffi", "title": "HSkip+: A Self-Stabilizing Overlay Network for Nodes with Heterogeneous\n  Bandwidths", "comments": "This is a long version of a paper published by IEEE in the\n  Proceedings of the 14-th IEEE International Conference on Peer-to-Peer\n  Computing", "journal-ref": null, "doi": "10.1109/P2P.2014.6934300", "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present and analyze HSkip+, a self-stabilizing overlay\nnetwork for nodes with arbitrary heterogeneous bandwidths. HSkip+ has the same\ntopology as the Skip+ graph proposed by Jacob et al. [PODC 2009] but its\nself-stabilization mechanism significantly outperforms the self-stabilization\nmechanism proposed for Skip+. Also, the nodes are now ordered according to\ntheir bandwidths and not according to their identifiers. Various other\nsolutions have already been proposed for overlay networks with heterogeneous\nbandwidths, but they are not self-stabilizing. In addition to HSkip+ being\nself-stabilizing, its performance is on par with the best previous bounds on\nthe time and work for joining or leaving a network of peers of logarithmic\ndiameter and degree and arbitrary bandwidths. Also, the dilation and congestion\nfor routing messages is on par with the best previous bounds for such networks,\nso that HSkip+ combines the advantages of both worlds. Our theoretical\ninvestigations are backed by simulations demonstrating that HSkip+ is indeed\nperforming much better than Skip+ and working correctly under high churn rates.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 16:51:56 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 14:08:54 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Feldotto", "Matthias", ""], ["Scheideler", "Christian", ""], ["Graffi", "Kalman", ""]]}, {"id": "1408.0500", "submitter": "Da Zheng", "authors": "Da Zheng, Disa Mhembere, Randal Burns, Joshua Vogelstein, Carey E.\n  Priebe, Alexander S. Szalay", "title": "FlashGraph: Processing Billion-Node Graphs on an Array of Commodity SSDs", "comments": "published in FAST'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph analysis performs many random reads and writes, thus, these workloads\nare typically performed in memory. Traditionally, analyzing large graphs\nrequires a cluster of machines so the aggregate memory exceeds the graph size.\nWe demonstrate that a multicore server can process graphs with billions of\nvertices and hundreds of billions of edges, utilizing commodity SSDs with\nminimal performance loss. We do so by implementing a graph-processing engine on\ntop of a user-space SSD file system designed for high IOPS and extreme\nparallelism. Our semi-external memory graph engine called FlashGraph stores\nvertex state in memory and edge lists on SSDs. It hides latency by overlapping\ncomputation with I/O. To save I/O bandwidth, FlashGraph only accesses edge\nlists requested by applications from SSDs; to increase I/O throughput and\nreduce CPU overhead for I/O, it conservatively merges I/O requests. These\ndesigns maximize performance for applications with different I/O\ncharacteristics. FlashGraph exposes a general and flexible vertex-centric\nprogramming interface that can express a wide variety of graph algorithms and\ntheir optimizations. We demonstrate that FlashGraph in semi-external memory\nperforms many algorithms with performance up to 80% of its in-memory\nimplementation and significantly outperforms PowerGraph, a popular distributed\nin-memory graph engine.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 13:44:09 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 06:49:18 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2015 01:41:54 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Zheng", "Da", ""], ["Mhembere", "Disa", ""], ["Burns", "Randal", ""], ["Vogelstein", "Joshua", ""], ["Priebe", "Carey E.", ""], ["Szalay", "Alexander S.", ""]]}, {"id": "1408.0501", "submitter": "Alejandro Frery", "authors": "Andr\\'e L. L. Aquino, Orlando S. Junior, Alejandro C. Frery, \\'Edler\n  Lins de Albuquerque, Raquel A. F. Mini", "title": "MuSA: Multivariate Sampling Algorithm for Wireless Sensor Networks", "comments": null, "journal-ref": "IEEE Transactions on Computers, pages 968--978, volume 53, number\n  4, April 2014", "doi": "10.1109/TC.2012.229", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wireless sensor network can be used to collect and process environmental\ndata, which is often of multivariate nature. This work proposes a multivariate\nsampling algorithm based on component analysis techniques in wireless sensor\nnetworks. To improve the sampling, the algorithm uses component analysis\ntechniques to rank the data. Once ranked, the most representative data is\nretained. Simulation results show that our technique reduces the data keeping\nits representativeness. In addition, the energy consumption and delay to\ndeliver the data on the network are reduced.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 13:50:17 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Aquino", "Andr\u00e9 L. L.", ""], ["Junior", "Orlando S.", ""], ["Frery", "Alejandro C.", ""], ["de Albuquerque", "\u00c9dler Lins", ""], ["Mini", "Raquel A. F.", ""]]}, {"id": "1408.0510", "submitter": "Maurizio Naldi", "authors": "Maurizio Naldi", "title": "A note on \"The Need for End-to-End Evaluation of Cloud Availability\"", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud availability is a major performance parameter for cloud platforms, but\nthere are very few measurements on commercial platforms, and most of them rely\non outage reports as appeared on specialized sites, providers' dashboards, or\nthe general press. A paper recently presented at the PAM 2014 conference by Hu\net alii reports the results of a measurement campaign. In this note, the\nresults of that paper are summarized, highlighting sources of inaccuracy and\nsome possible improvements. In particular, the use of a low probing frequency\ncould lead to non detection of short outages, as well as to an inaccurate\nestimation of the outage duration statistics. Overcoming this lack of accuracy\nis relevant to properly assess SLA violations and establish the basis for\ninsurance claims.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 16:55:44 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Naldi", "Maurizio", ""]]}, {"id": "1408.0517", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally and Jeremy Kepner", "title": "Big Data Dimensional Analysis", "comments": "From IEEE HPEC 2014", "journal-ref": null, "doi": "10.1109/HPEC.2014.7040944", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to collect and analyze large amounts of data is a growing problem\nwithin the scientific community. The growing gap between data and users calls\nfor innovative tools that address the challenges faced by big data volume,\nvelocity and variety. One of the main challenges associated with big data\nvariety is automatically understanding the underlying structures and patterns\nof the data. Such an understanding is required as a pre-requisite to the\napplication of advanced analytics to the data. Further, big data sets often\ncontain anomalies and errors that are difficult to know a priori. Current\napproaches to understanding data structure are drawn from the traditional\ndatabase ontology design. These approaches are effective, but often require too\nmuch human involvement to be effective for the volume, velocity and variety of\ndata encountered by big data systems. Dimensional Data Analysis (DDA) is a\nproposed technique that allows big data analysts to quickly understand the\noverall structure of a big dataset, determine anomalies. DDA exploits\nstructures that exist in a wide class of data to quickly determine the nature\nof the data and its statical anomalies. DDA leverages existing schemas that are\nemployed in big data databases today. This paper presents DDA, applies it to a\nnumber of data sets, and measures its performance. The overhead of DDA is low\nand can be applied to existing big data systems without greatly impacting their\ncomputing requirements.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 17:22:01 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1408.0557", "submitter": "Hsin-Hao Su", "authors": "Danupon Nanongkai, Hsin-Hao Su", "title": "Almost-Tight Distributed Minimum Cut Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing the minimum cut in a weighted distributed\nmessage-passing networks (the CONGEST model). Let $\\lambda$ be the minimum cut,\n$n$ be the number of nodes in the network, and $D$ be the network diameter. Our\nalgorithm can compute $\\lambda$ exactly in $O((\\sqrt{n} \\log^{*} n+D)\\lambda^4\n\\log^2 n)$ time. To the best of our knowledge, this is the first paper that\nexplicitly studies computing the exact minimum cut in the distributed setting.\nPreviously, non-trivial sublinear time algorithms for this problem are known\nonly for unweighted graphs when $\\lambda\\leq 3$ due to Pritchard and\nThurimella's $O(D)$-time and $O(D+n^{1/2}\\log^* n)$-time algorithms for\ncomputing $2$-edge-connected and $3$-edge-connected components.\n  By using the edge sampling technique of Karger's, we can convert this\nalgorithm into a $(1+\\epsilon)$-approximation $O((\\sqrt{n}\\log^{*}\nn+D)\\epsilon^{-5}\\log^3 n)$-time algorithm for any $\\epsilon>0$. This improves\nover the previous $(2+\\epsilon)$-approximation $O((\\sqrt{n}\\log^{*}\nn+D)\\epsilon^{-5}\\log^2 n\\log\\log n)$-time algorithm and\n$O(\\epsilon^{-1})$-approximation $O(D+n^{\\frac{1}{2}+\\epsilon}\n\\mathrm{poly}\\log n)$-time algorithm of Ghaffari and Kuhn. Due to the lower\nbound of $\\Omega(D+n^{1/2}/\\log n)$ by Das Sarma et al. which holds for any\napproximation algorithm, this running time is tight up to a $ \\mathrm{poly}\\log\nn$ factor.\n  To get the stated running time, we developed an approximation algorithm which\ncombines the ideas of Thorup's algorithm and Matula's contraction algorithm. It\nsaves an $\\epsilon^{-9}\\log^{7} n$ factor as compared to applying Thorup's tree\npacking theorem directly. Then, we combine Kutten and Peleg's tree partitioning\nalgorithm and Karger's dynamic programming to achieve an efficient distributed\nalgorithm that finds the minimum cut when we are given a spanning tree that\ncrosses the minimum cut exactly once.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 00:23:28 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Nanongkai", "Danupon", ""], ["Su", "Hsin-Hao", ""]]}, {"id": "1408.0574", "submitter": "Adam Sealfon", "authors": "Adam Sealfon and Aikaterini Sotiraki", "title": "Agreement in Partitioned Dynamic Networks", "comments": "A summary of these results will appear as a brief announcement in\n  DISC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the dynamic network model, the communication graph is assumed to be\nconnected in every round but is otherwise arbitrary. We consider the related\nsetting of $p$-partitioned dynamic networks, in which the communication graph\nin each round consists of at most $p$ connected components. We explore the\nproblem of $k$-agreement in this model for $k\\geq p$. We show that if the\nnumber of processes is unknown then it is impossible to achieve $k$-agreement\nfor any $k$ and any $p\\geq 2$. Given an upper bound $n$ on the number of\nprocesses, we provide algorithms achieving $k$-agreement in $p(n-p)$ rounds for\n$k=p$ and in $O(n/\\epsilon)$ rounds for $k=\\lceil (1+\\epsilon)p \\rceil$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 03:25:20 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Sealfon", "Adam", ""], ["Sotiraki", "Aikaterini", ""]]}, {"id": "1408.0620", "submitter": "Matthias F\\\"ugger", "authors": "Bernadette Charron-Bost, Matthias F\\\"ugger, Thomas Nowak", "title": "Approximate Consensus in Highly Dynamic Networks: The Role of Averaging\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the approximate consensus problem in highly\ndynamic networks in which topology may change continually and unpredictably. We\nprove that in both synchronous and partially synchronous systems, approximate\nconsensus is solvable if and only if the communication graph in each round has\na rooted spanning tree, i.e., there is a coordinator at each time. The striking\npoint in this result is that the coordinator is not required to be unique and\ncan change arbitrarily from round to round. Interestingly, the class of\naveraging algorithms, which are memoryless and require no process identifiers,\nentirely captures the solvability issue of approximate consensus in that the\nproblem is solvable if and only if it can be solved using any averaging\nalgorithm. Concerning the time complexity of averaging algorithms, we show that\napproximate consensus can be achieved with precision of $\\varepsilon$ in a\ncoordinated network model in $O(n^{n+1} \\log\\frac{1}{\\varepsilon})$ synchronous\nrounds, and in $O(\\Delta n^{n\\Delta+1} \\log\\frac{1}{\\varepsilon})$ rounds when\nthe maximum round delay for a message to be delivered is $\\Delta$. While in\ngeneral, an upper bound on the time complexity of averaging algorithms has to\nbe exponential, we investigate various network models in which this exponential\nbound in the number of nodes reduces to a polynomial bound. We apply our\nresults to networked systems with a fixed topology and classical benign fault\nmodels, and deduce both known and new results for approximate consensus in\nthese systems. In particular, we show that for solving approximate consensus, a\ncomplete network can tolerate up to 2n-3 arbitrarily located link faults at\nevery round, in contrast with the impossibility result established by Santoro\nand Widmayer (STACS '89) showing that exact consensus is not solvable with n-1\nlink faults per round originating from the same node.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 09:27:17 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 10:19:10 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Charron-Bost", "Bernadette", ""], ["F\u00fcgger", "Matthias", ""], ["Nowak", "Thomas", ""]]}, {"id": "1408.0812", "submitter": "Calvin Newport", "authors": "Calvin Newport", "title": "Lower Bounds for Structuring Unreliable Radio Networks", "comments": "An extended abstract of this work appears in the 2014 proceedings of\n  the International Symposium on Distributed Computing (DISC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study lower bounds for randomized solutions to the maximal\nindependent set (MIS) and connected dominating set (CDS) problems in the dual\ngraph model of radio networks---a generalization of the standard graph-based\nmodel that now includes unreliable links controlled by an adversary. We begin\nby proving that a natural geographic constraint on the network topology is\nrequired to solve these problems efficiently (i.e., in time polylogarthmic in\nthe network size). We then prove the importance of the assumption that nodes\nare provided advance knowledge of their reliable neighbors (i.e, neighbors\nconnected by reliable links). Combined, these results answer an open question\nby proving that the efficient MIS and CDS algorithms from [Censor-Hillel, PODC\n2011] are optimal with respect to their dual graph model assumptions. They also\nprovide insight into what properties of an unreliable network enable efficient\nlocal computation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 20:39:55 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Newport", "Calvin", ""]]}, {"id": "1408.0818", "submitter": "Himanshu Chauhan", "authors": "Weil-Lun Hung and Himanshu Chauhan and Vijay K. Garg", "title": "ActiveMonitor: Non-blocking Monitor Executions for Increased Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of novel ideas on design and implementation of monitor\nobjects for multi-threaded programs. Our approach has two main goals: (a)\nincrease parallelism in monitor objects and thus provide performance gains\n(shorter runtimes) for multi-threaded programs, and (b) introduce constructs\nthat allow programmers to easily write monitor-based multi-threaded programs\nthat can achieve these performance gains. We describe the concepts of our\nframework, called ActiveMonitor, and its prototype implementation using\nfutures. We evaluate its performance in terms of runtimes of multi-threaded\nprograms on linked-list, bounded-buffer, and other fundamental problems\nimplemented in Java. We compare the runtimes of our implementation against\nimplementations using Java's reentrant locks, recently proposed automatic\nsignaling framework AutoSynch, and some other techniques from the literature.\nThe results of of the evaluation indicate that monitors based on our framework\nprovide significant gains in runtime performance in comparison to traditional\nmonitors implemented using Java's reentrant locks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 20:58:16 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Hung", "Weil-Lun", ""], ["Chauhan", "Himanshu", ""], ["Garg", "Vijay K.", ""]]}, {"id": "1408.0876", "submitter": "Congmin Fan", "authors": "Congmin Fan, Ying Jun Zhang, Xiaojun Yuan", "title": "Dynamic Nested Clustering for Parallel PHY-Layer Processing in\n  Cloud-RANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Featured by centralized processing and cloud based infrastructure, Cloud\nRadio Access Network (C-RAN) is a promising solution to achieve an\nunprecedented system capacity in future wireless cellular networks. The huge\ncapacity gain mainly comes from the centralized and coordinated signal\nprocessing at the cloud server. However, full-scale coordination in a\nlarge-scale C-RAN requires the processing of very large channel matrices,\nleading to high computational complexity and channel estimation overhead. To\nresolve this challenge, we exploit the near-sparsity of large C-RAN channel\nmatrices, and derive a unified theoretical framework for clustering and\nparallel processing. Based on the framework, we propose a dynamic nested\nclustering (DNC) algorithm that not only greatly improves the system\nscalability in terms of baseband-processing and channel-estimation complexity,\nbut also is amenable to various parallel processing strategies for different\ndata center architectures. With the proposed algorithm, we show that the\ncomputation time for the optimal linear detector is greatly reduced from\n$O(N^3)$ to no higher than $O(N^{\\frac{42}{23}})$, where $N$ is the number of\nRRHs in C-RAN.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 06:32:44 GMT"}, {"version": "v2", "created": "Sat, 27 Dec 2014 11:59:23 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Fan", "Congmin", ""], ["Zhang", "Ying Jun", ""], ["Yuan", "Xiaojun", ""]]}, {"id": "1408.0979", "submitter": "Madhavan Mukund", "authors": "Sumit Kumar Jha, Madhavan Mukund, Ratul Saha, P S Thiagarajan", "title": "Distributed Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The formal verification of large probabilistic models is important and\nchallenging. Exploiting the concurrency that is often present is one way to\naddress this problem. Here we study a restricted class of asynchronous\ndistributed probabilistic systems in which the synchronizations determine the\nprobability distribution for the next moves of the participating agents. The\nkey restriction we impose is that the synchronizations are deterministic, in\nthe sense that any two simultaneously enabled synchronizations must involve\ndisjoint sets of agents. As a result, this network of agents can be viewed as a\nsuccinct and distributed presentation of a large global Markov chain. A rich\nclass of Markov chains can be represented this way.\n  We define an interleaved semantics for our model in terms of the local\nsynchronization actions. The network structure induces an independence relation\non these actions, which, in turn, induces an equivalence relation over the\ninterleaved runs in the usual way. We construct a natural probability measure\nover these equivalence classes of runs by exploiting Mazurkiewicz trace theory\nand the probability measure space of the associated global Markov chain.\n  It turns out that verification of our model, called DMCs (distributed Markov\nchains), can often be efficiently carried out by exploiting the partial order\nnature of the interleaved semantics. To demonstrate this, we develop a\nstatistical model checking (SMC) procedure and use it to verify two large\ndistributed probabilistic networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 14:13:31 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Jha", "Sumit Kumar", ""], ["Mukund", "Madhavan", ""], ["Saha", "Ratul", ""], ["Thiagarajan", "P S", ""]]}, {"id": "1408.1021", "submitter": "Hammurabi Mendes", "authors": "Irina Calciu, Hammurabi Mendes, Maurice Herlihy", "title": "The Adaptive Priority Queue with Elimination and Combining", "comments": "Accepted at DISC'14 - this is the full version with appendices,\n  including more algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priority queues are fundamental abstract data structures, often used to\nmanage limited resources in parallel programming. Several proposed parallel\npriority queue implementations are based on skiplists, harnessing the potential\nfor parallelism of the add() operations. In addition, methods such as Flat\nCombining have been proposed to reduce contention by batching together multiple\noperations to be executed by a single thread. While this technique can decrease\nlock-switching overhead and the number of pointer changes required by the\nremoveMin() operations in the priority queue, it can also create a sequential\nbottleneck and limit parallelism, especially for non-conflicting add()\noperations.\n  In this paper, we describe a novel priority queue design, harnessing the\nscalability of parallel insertions in conjunction with the efficiency of\nbatched removals. Moreover, we present a new elimination algorithm suitable for\na priority queue, which further increases concurrency on balanced workloads\nwith similar numbers of add() and removeMin() operations. We implement and\nevaluate our design using a variety of techniques including locking, atomic\noperations, hardware transactional memory, as well as employing adaptive\nheuristics given the workload.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 16:16:56 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Calciu", "Irina", ""], ["Mendes", "Hammurabi", ""], ["Herlihy", "Maurice", ""]]}, {"id": "1408.1127", "submitter": "Konstantin Solnushkin S", "authors": "Konstantin S. Solnushkin", "title": "SADDLE: A Modular Design Automation Framework for Cluster Supercomputers\n  and Data Centres", "comments": "13 pages, 2 figures, 1 table. The work was presented at the\n  International Supercomputing Conference (ISC'14) in Leipzig, Germany", "journal-ref": "Springer, Lecture Notes in Computer Science (LNCS), Volume 8488,\n  2014, pp 232-244", "doi": "10.1007/978-3-319-07518-1_15", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present SADDLE, a modular framework for automated design of\ncluster supercomputers and data centres. In contrast with commonly used\napproaches that operate on logic gate level (Verilog, VHDL) or board level\n(such as EDA tools), SADDLE works at a much higher level of abstraction: its\nbuilding blocks are ready-made servers, network switches, power supply systems\nand so on. Modular approach provides the potential to include low-level tools\nas elements of SADDLE's design workflow, moving towards the goal of electronic\nsystem level (ESL) design automation. Designs produced by SADDLE include\nproject documentation items such as bills of materials and wiring diagrams,\nproviding a formal specification of a computer system and streamlining assembly\noperations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 21:59:26 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Solnushkin", "Konstantin S.", ""]]}, {"id": "1408.1467", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler", "title": "Interactive Channel Capacity Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first capacity approaching coding schemes that robustly\nsimulate any interactive protocol over an adversarial channel that corrupts any\n$\\epsilon$ fraction of the transmitted symbols. Our coding schemes achieve a\ncommunication rate of $1 - O(\\sqrt{\\epsilon \\log \\log 1/\\epsilon})$ over any\nadversarial channel. This can be improved to $1 - O(\\sqrt{\\epsilon})$ for\nrandom, oblivious, and computationally bounded channels, or if parties have\nshared randomness unknown to the channel.\n  Surprisingly, these rates exceed the $1 - \\Omega(\\sqrt{H(\\epsilon)}) = 1 -\n\\Omega(\\sqrt{\\epsilon \\log 1/\\epsilon})$ interactive channel capacity bound\nwhich [Kol and Raz; STOC'13] recently proved for random errors. We conjecture\n$1 - \\Theta(\\sqrt{\\epsilon \\log \\log 1/\\epsilon})$ and $1 -\n\\Theta(\\sqrt{\\epsilon})$ to be the optimal rates for their respective settings\nand therefore to capture the interactive channel capacity for random and\nadversarial errors.\n  In addition to being very communication efficient, our randomized coding\nschemes have multiple other advantages. They are computationally efficient,\nextremely natural, and significantly simpler than prior (non-capacity\napproaching) schemes. In particular, our protocols do not employ any coding but\nallow the original protocol to be performed as-is, interspersed only by short\nexchanges of hash values. When hash values do not match, the parties backtrack.\nOur approach is, as we feel, by far the simplest and most natural explanation\nfor why and how robust interactive communication in a noisy environment is\npossible.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 02:32:15 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 10:59:59 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Haeupler", "Bernhard", ""]]}, {"id": "1408.1540", "submitter": "Ramij Rahaman", "authors": "Ramij Rahaman, Marcin Wie\\'sniak and Marek \\.Zukowski", "title": "Quantum Byzantine Agreement via Hardy correlations and entanglement\n  swapping", "comments": "The protocol presented here is a solution of the original Byzantine\n  agreement problem and not its sub-problem like detectable Byzantine\n  agreement. Comments are welcome", "journal-ref": "Phys. Rev. A 92, 042302 (2015)", "doi": "10.1103/PhysRevA.92.042302", "report-no": null, "categories": "quant-ph cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a device-independent quantum scheme for the {\\em Byzantine\nGenerals} problem. The protocol is for three parties. Party $C$ is to send two\nidentical one bit messages to parties $A$ and $B$. The receivers $A$ and $B$\nmay exchange two one bit messages informing the other party on the message\nreceived from $C$. A bit flipping error in one of the transmissions, does not\nallow the receiving parties to establish what was the message of $C$. Our\nquantum scheme has the feature that if the messages of the Byzantine protocol\nare readable (that is give an unambiguous bit value for any of the receivers),\nthen any error by $C$ (cheating by one of the commanding general) is\nimpossible. $A$ and $B$ do not have to exchange protocol messages to be sure of\nthis.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 11:04:44 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Rahaman", "Ramij", ""], ["Wie\u015bniak", "Marcin", ""], ["\u017bukowski", "Marek", ""]]}, {"id": "1408.1600", "submitter": "Animesh Chaturvedi Mr.", "authors": "Animesh Chaturvedi", "title": "Change Impact Analysis Based Regression Testing of Web Services", "comments": "Master of Technology Thesis, PDPM Indian Institute of Information\n  Technology, Design and Manufacturing Jabalpur (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the effort required to make changes in web services is one of the\nprimary goals in web service projects maintenance and evolution. Normally,\nfunctional and non-functional testing of a web service is performed by testing\nthe operations specified in its WSDL. The regression testing is performed by\nidentifying the changes made thereafter to the web service code and the WSDL.\nIn this thesis, we present a tool-supported approach to perform efficient\nregression testing of web services. By representing a web service as a directed\ngraph of WSDL elements, we identify and gathers the changed portions of the\ngraph and use this information to reduce regression testing efforts.\nSpecifically, we identify, categorize, and capture the web service testing\nneeds in two different ways, namely, Operationalized Regression Testing of Web\nService (ORTWS) and Parameterized Regression Testing of Web Service (PRTWS).\nBoth of the approach can be combined to reduce the regression testing efforts\nin the web service project. The proposed approach is prototyped as a tool,\nnamed as Automatic Web Service Change Management (AWSCM), which helps in\nselecting the relevant test cases to construct reduced test suite from the old\ntest suite. We present few case studies on different web service projects to\ndemonstrate the applicability of the proposed tool. The reduction in the effort\nfor regression testing of web service is also estimated.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 14:14:14 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Chaturvedi", "Animesh", ""]]}, {"id": "1408.1605", "submitter": "Enrico Mastrostefano", "authors": "Mauro Bisson, Massimo Bernaschi, Enrico Mastrostefano", "title": "Parallel Distributed Breadth First Search on the Kepler Architecture", "comments": "In this revision we adopt a technique to reduce the size of exchanged\n  messages that relies on the use of a bitmap. This change halves, by itself,\n  the total execution time. Now the code reaches 800 GTEPS on 4096 Kepler GPUs.\n  We also made some modifications to the Introduction and to the performance\n  section. Added new references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results obtained by using an evolution of our CUDA-based\nsolution for the exploration, via a Breadth First Search, of large graphs. This\nlatest version exploits at its best the features of the Kepler architecture and\nrelies on a combination of techniques to reduce both the number of\ncommunications among the GPUs and the amount of exchanged data. The final\nresult is a code that can visit more than 800 billion edges in a second by\nusing a cluster equipped with 4096 Tesla K20X GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 14:34:15 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 15:17:54 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Bisson", "Mauro", ""], ["Bernaschi", "Massimo", ""], ["Mastrostefano", "Enrico", ""]]}, {"id": "1408.1664", "submitter": "Yetian Chen", "authors": "Yetian Chen, Jin Tian, Olga Nikolova and Srinivas Aluru", "title": "A Parallel Algorithm for Exact Bayesian Structure Discovery in Bayesian\n  Networks", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Bayesian structure discovery in Bayesian networks requires exponential\ntime and space. Using dynamic programming (DP), the fastest known sequential\nalgorithm computes the exact posterior probabilities of structural features in\n$O(2(d+1)n2^n)$ time and space, if the number of nodes (variables) in the\nBayesian network is $n$ and the in-degree (the number of parents) per node is\nbounded by a constant $d$. Here we present a parallel algorithm capable of\ncomputing the exact posterior probabilities for all $n(n-1)$ edges with optimal\nparallel space efficiency and nearly optimal parallel time efficiency. That is,\nif $p=2^k$ processors are used, the run-time reduces to\n$O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per\nprocessor. Our algorithm is based the observation that the subproblems in the\nsequential DP algorithm constitute a $n$-$D$ hypercube. We take a delicate way\nto coordinate the computation of correlated DP procedures such that large\namount of data exchange is suppressed. Further, we develop parallel techniques\nfor two variants of the well-known \\emph{zeta transform}, which have\napplications outside the context of Bayesian networks. We demonstrate the\ncapability of our algorithm on datasets with up to 33 variables and its\nscalability on up to 2048 processors. We apply our algorithm to a biological\ndata set for discovering the yeast pheromone response pathways.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 17:40:36 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2014 04:12:09 GMT"}, {"version": "v3", "created": "Sat, 13 Aug 2016 04:25:55 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Chen", "Yetian", ""], ["Tian", "Jin", ""], ["Nikolova", "Olga", ""], ["Aluru", "Srinivas", ""]]}, {"id": "1408.1727", "submitter": "Andrey Vladimirov", "authors": "Andrey Vladimirov and Cliff Addison", "title": "Cluster-level tuning of a shallow water equation solver on the Intel MIC\n  architecture", "comments": "Colfax Research publication. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE cs.DC physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper demonstrates the optimization of the execution environment of a\nhybrid OpenMP+MPI computational fluid dynamics code (shallow water equation\nsolver) on a cluster enabled with Intel Xeon Phi coprocessors. The discussion\nincludes: (1) Controlling the number and affinity of OpenMP threads to optimize\naccess to memory bandwidth; (2) Tuning the inter-operation of OpenMP and MPI to\npartition the problem for better data locality; (3) Ordering the MPI ranks in a\nway that directs some of the traffic into faster communication channels; (4)\nUsing efficient peer-to-peer communication between Xeon Phi coprocessors based\non the InfiniBand fabric.\n  With tuning, the application has 90% percent efficiency of parallel scaling\nup to 8 Intel Xeon Phi coprocessors in 2 compute nodes. For larger problems,\nscalability is even better, because of the greater computation to communication\nratio. However, problems of that size do not fit in the memory of one\ncoprocessor. The performance of the solver on one Intel Xeon Phi coprocessor\n7120P exceeds the performance on a dual-socket Intel Xeon E5-2697 v2 CPU by a\nfactor of 1.6x. In a 2-node cluster with 4 coprocessors per compute node, the\nMIC architecture yields 5.8x more performance than the CPUs. Only one line of\nlegacy Fortran code had to be changed in order to achieve the reported\nperformance on the MIC architecture (not counting changes to the command-line\ninterface). The methodology discussed in this paper is directly applicable to\nother bandwidth-bound stencil algorithms utilizing a hybrid OpenMP+MPI\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 22:53:51 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Vladimirov", "Andrey", ""], ["Addison", "Cliff", ""]]}, {"id": "1408.1935", "submitter": "Niloufar Shafiei", "authors": "Niloufar Shafiei", "title": "Non-Blocking Doubly-Linked Lists with Good Amortized Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new non-blocking doubly-linked list implementation for an\nasynchronous shared-memory system. It is the first such implementation for\nwhich an upper bound on amortized time complexity has been proved. In our\nimplementation, operations access the list via cursors. Each cursor is\nassociated with an item in the list and is local to a process. The\nimplementation supports two update operations, insertBefore and delete, and two\nmove operations, moveRight and moveLeft. An insertBefore(c, x) operation\ninserts an item x into the list immediately before the cursor c's location. A\ndelete(c) operation removes the item at the cursor c's location and sets the\ncursor to the next item in the list. The move operations move the cursor one\nposition to the right or left. The update operations use single-word\nCompare&Swap instructions. The move operations only read shared memory and\nnever change the state of the data structure. If all update operations modify\ndifferent parts of the list, they run completely concurrently. Let cp(op) be\nthe maximum number of active cursors at any one time during the operation op.\nThe amortized complexity of each update operation op is O(cp(op)) and each move\noperation is O(1). We have written a detailed correctness proof and amortized\nanalysis of our implementation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 18:42:50 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Shafiei", "Niloufar", ""]]}, {"id": "1408.2041", "submitter": "Yucheng Low", "authors": "Yucheng Low, Joseph E. Gonzalez, Aapo Kyrola, Danny Bickson, Carlos E.\n  Guestrin, Joseph Hellerstein", "title": "GraphLab: A New Framework For Parallel Machine Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-340-349", "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and implementing efficient, provably correct parallel machine\nlearning (ML) algorithms is challenging. Existing high-level parallel\nabstractions like MapReduce are insufficiently expressive while low-level tools\nlike MPI and Pthreads leave ML experts repeatedly solving the same design\nchallenges. By targeting common patterns in ML, we developed GraphLab, which\nimproves upon abstractions like MapReduce by compactly expressing asynchronous\niterative algorithms with sparse computational dependencies while ensuring data\nconsistency and achieving a high degree of parallel performance. We demonstrate\nthe expressiveness of the GraphLab framework by designing and implementing\nparallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and\nCompressed Sensing. We show that using GraphLab we can achieve excellent\nparallel performance on large scale real-world problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:38:37 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Low", "Yucheng", ""], ["Gonzalez", "Joseph E.", ""], ["Kyrola", "Aapo", ""], ["Bickson", "Danny", ""], ["Guestrin", "Carlos E.", ""], ["Hellerstein", "Joseph", ""]]}, {"id": "1408.2060", "submitter": "Jie Chen", "authors": "Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan\n  Tan, Patrick Jaillet", "title": "Parallel Gaussian Process Regression with Low-Rank Covariance Matrix\n  Approximations", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-152-161", "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are Bayesian non-parametric models that are widely\nused for probabilistic regression. Unfortunately, it cannot scale well with\nlarge data nor perform real-time predictions due to its cubic time cost in the\ndata size. This paper presents two parallel GP regression methods that exploit\nlow-rank covariance matrix approximations for distributing the computational\nload among parallel machines to achieve time efficiency and scalability. We\ntheoretically guarantee the predictive performances of our proposed parallel\nGPs to be equivalent to that of some centralized approximate GP regression\nmethods: The computation of their centralized counterparts can be distributed\namong parallel machines, hence achieving greater time efficiency and\nscalability. We analytically compare the properties of our parallel GPs such as\ntime, space, and communication complexity. Empirical evaluation on two\nreal-world datasets in a cluster of 20 computing nodes shows that our parallel\nGPs are significantly more time-efficient and scalable than their centralized\ncounterparts and exact/full GP while achieving predictive performances\ncomparable to full GP.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:58:33 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Chen", "Jie", ""], ["Cao", "Nannan", ""], ["Low", "Kian Hsiang", ""], ["Ouyang", "Ruofei", ""], ["Tan", "Colin Keng-Yan", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1408.2071", "submitter": "Sriram Pemmaraju", "authors": "James W. Hegeman and Sriram V. Pemmaraju and Vivek B. Sardeshmukh", "title": "Near-Constant-Time Distributed Algorithms on a Congested Clique", "comments": "Full version of DISC 2014 paper. Updated Sep 2018 to reflect the fact\n  that using the Ghaffari et al. congested clique MIS algorithm from PODC 2018,\n  it is possible to compute a 2-ruling set in the congested clique in\n  O(logloglog n) rounds with high probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents constant-time and near-constant-time distributed\nalgorithms for a variety of problems in the congested clique model. We show how\nto compute a 3-ruling set in expected $O(\\log \\log \\log n)$ rounds and using\nthis, we obtain a constant-approximation to metric facility location, also in\nexpected $O(\\log \\log \\log n)$ rounds. In addition, assuming an input metric\nspace of constant doubling dimension, we obtain constant-round algorithms to\ncompute constant-factor approximations to the minimum spanning tree and the\nmetric facility location problems. These results significantly improve on the\nrunning time of the fastest known algorithms for these problems in the\ncongested clique setting.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 07:41:12 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 20:05:12 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Hegeman", "James W.", ""], ["Pemmaraju", "Sriram V.", ""], ["Sardeshmukh", "Vivek B.", ""]]}, {"id": "1408.2072", "submitter": "Sruti   S", "authors": "S. Bhagat, S. Gan Chaudhuri, K. Mukhopadhyaya", "title": "Formation of General Position by Asynchronous Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional distributed model of autonomous, homogeneous, mobile point\nrobots usually assumes that the robots do not create any visual obstruction for\nthe other robots, i.e., the robots are see through. In this paper, we consider\na slightly more realistic model, by incorporating the notion of obstructed\nvisibility (i.e., robots are not see through) for other robots. Under the new\nmodel of visibility, a robot may not have the full view of its surroundings.\nMany of the existing algorithms demand that each robot should have the complete\nknowledge of the positions of other robots. Since, vision is the only mean of\ntheir communication, it is required that the robots are in general position\n(i.e., no three robots are collinear). We consider asynchronous robots. They\nalso do not have common chirality (or any agreement on a global coordinate\nsystem). In this paper, we present a distributed algorithm for obtaining a\ngeneral position for the robots in finite time from any arbitrary\nconfiguration. The algorithm also assures collision free motion for each robot.\nThis algorithm may also be used as a preprocessing module for many other\nsubsequent tasks performed by the robots.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 07:43:54 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Bhagat", "S.", ""], ["Chaudhuri", "S. Gan", ""], ["Mukhopadhyaya", "K.", ""]]}, {"id": "1408.2116", "submitter": "Stephane Rovedakis", "authors": "Fran\\c{c}ois Delbot (LIP6), Christian Laforest (LIMOS), Stephane\n  Rovedakis (CEDRIC)", "title": "Self-stabilizing algorithms for Connected Vertex Cover and Clique\n  decomposition problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many wireless networks, there is no fixed physical backbone nor\ncentralized network management. The nodes of such a network have to\nself-organize in order to maintain a virtual backbone used to route messages.\nMoreover, any node of the network can be a priori at the origin of a malicious\nattack. Thus, in one hand the backbone must be fault-tolerant and in other hand\nit can be useful to monitor all network communications to identify an attack as\nsoon as possible. We are interested in the minimum \\emph{Connected Vertex\nCover} problem, a generalization of the classical minimum Vertex Cover problem,\nwhich allows to obtain a connected backbone. Recently, Delbot et\nal.~\\cite{DelbotLP13} proposed a new centralized algorithm with a constant\napproximation ratio of $2$ for this problem. In this paper, we propose a\ndistributed and self-stabilizing version of their algorithm with the same\napproximation guarantee. To the best knowledge of the authors, it is the first\ndistributed and fault-tolerant algorithm for this problem. The approach\nfollowed to solve the considered problem is based on the construction of a\nconnected minimal clique partition. Therefore, we also design the first\ndistributed self-stabilizing algorithm for this problem, which is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 14:43:04 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Delbot", "Fran\u00e7ois", "", "LIP6"], ["Laforest", "Christian", "", "LIMOS"], ["Rovedakis", "Stephane", "", "CEDRIC"]]}, {"id": "1408.2284", "submitter": "Da Zheng", "authors": "Da Zheng, Alexander Szalay, Andreas Terzis", "title": "Hadoop in Low-Power Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous work we introduced a so-called Amdahl blade microserver that\ncombines a low-power Atom processor, with a GPU and an SSD to provide a\nbalanced and energy-efficient system. Our preliminary results suggested that\nthe sequential I/O of Amdahl blades can be ten times higher than that a cluster\nof conventional servers with comparable power consumption. In this paper we\ninvestigate the performance and energy efficiency of Amdahl blades running\nHadoop. Our results show that Amdahl blades are 7.7 times and 3.4 times as\nenergy-efficient as the Open Cloud Consortium cluster for a data-intensive and\na compute-intensive application, respectively. The Hadoop Distributed\nFilesystem has relatively poor performance on Amdahl blades because both disk\nand network I/O are CPU-heavy operations on Atom processors. We demonstrate\nthree effective techniques to reduce CPU consumption and improve performance.\nHowever, even with these improvements, the Atom processor is still the system's\nbottleneck. We revisit Amdahl's law, and estimate that Amdahl blades need four\nAtom cores to be well balanced for Hadoop tasks.\n", "versions": [{"version": "v1", "created": "Sun, 10 Aug 2014 23:55:23 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Zheng", "Da", ""], ["Szalay", "Alexander", ""], ["Terzis", "Andreas", ""]]}, {"id": "1408.2657", "submitter": "Ben Cumming Dr", "authors": "Gilles Fourestey, Ben Cumming, Ladina Gilly, Thomas C. Schulthess", "title": "First Experiences With Validating and Using the Cray Power Management\n  Database Tool", "comments": "This paper was presented at the 2014 Cray User Group (CUG) user\n  meeting in Lugano, Switzerland,First Experiences With Validating and Using\n  the Cray Power Management Database Tool, Gilles Fourestey and Ben Cumming and\n  Ladina Gilly, Proceedings of the CUG meeting, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In October 2013 CSCS installed the first hybrid Cray XC-30 system, dubbed Piz\nDaint. This system features the power management database (PMDB), that was\nrecently introduced by Cray to collect detailed power consumption information\nin a non-intrusive manner. Power measurements are taken on each node, with\nadditional measurements for the Aries network and blowers, and recorded in a\ndatabase. This enables fine-grained reporting of power consumption that is not\npossible with external power meters, and is useful to both application\ndevelopers and facility operators. This paper will show how benchmarks of\nrepresentative applications at CSCS were used to validate the PMDB on Piz\nDaint. Furthermore we will elaborate, with the well-known HPL benchmark serving\nas prototypical application, on how the PMDB streamlines the tuning for optimal\npower efficiency in production, which lead to Piz Daint being recognised as the\nmost energy efficient petascale supercomputer presently in operation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 08:33:03 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Fourestey", "Gilles", ""], ["Cumming", "Ben", ""], ["Gilly", "Ladina", ""], ["Schulthess", "Thomas C.", ""]]}, {"id": "1408.2751", "submitter": "Harald Lampesberger", "authors": "Harald Lampesberger", "title": "Technologies for Web and cloud service interaction: a survey", "comments": "Accepted Version 2015-02-20, 41 pages, 19 figures, 3 tables, Service\n  Oriented Computing and Applications (2015)", "journal-ref": null, "doi": "10.1007/s11761-015-0174-1", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of Web and service technologies has led to a wide landscape of\nstandards and protocols for interaction between loosely coupled software\ncomponents. Examples range from Web applications, mashups, apps, and mobile\ndevices to enterprise-grade services. Cloud computing is the industrialization\nof service provision and delivery, where Web and enterprise services are\nconverging on a technological level. The article discusses this technological\nlandscape and, in particular, current trends with respect to cloud computing.\nThe survey focuses on the communication aspect of interaction by reviewing\nlanguages, protocols, and architectures that drive today's standards and\nsoftware implementations applicable in clouds. Technological advances will\naffect both client side and service side. There is a trend toward multiplexing,\nmultihoming, and encryption in upcoming transport mechanisms, especially for\narchitectures, where a client simultaneously sends a large number of requests\nto some service. Furthermore, there are emerging client-to-client communication\ncapabilities in Web clients that could establish a foundation for upcoming\nWeb-based messaging architectures.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 15:40:51 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 15:39:40 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2015 12:19:32 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Lampesberger", "Harald", ""]]}, {"id": "1408.2782", "submitter": "Will Rosenbaum", "authors": "Rafail Ostrovsky, Will Rosenbaum", "title": "Fast distributed almost stable marriages", "comments": "Various improvements in version 2: algorithms for general (not just\n  \"almost regular\") preferences; deterministic variant of the algorithm;\n  streamlined proof of approximation guarantee", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their seminal work on the Stable Marriage Problem, Gale and Shapley\ndescribe an algorithm which finds a stable matching in $O(n^2)$ communication\nrounds. Their algorithm has a natural interpretation as a distributed algorithm\nwhere each player is represented by a single processor. In this distributed\nmodel, Floreen, Kaski, Polishchuk, and Suomela recently showed that for bounded\npreference lists, terminating the Gale-Shapley algorithm after a constant\nnumber of rounds results in an almost stable matching. In this paper, we\ndescribe a new deterministic distributed algorithm which finds an almost stable\nmatching in $O(\\log^5 n)$ communication rounds for arbitrary preferences. We\nalso present a faster randomized variant which requires $O(\\log^2 n)$ rounds.\nThis run-time can be improved to $O(1)$ rounds for \"almost regular\" (and in\nparticular complete) preferences. To our knowledge, these are the first\nsub-polynomial round distributed algorithms for any variant of the stable\nmarriage problem with unbounded preferences.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 17:26:00 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 05:33:36 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Ostrovsky", "Rafail", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1408.2858", "submitter": "Francesco Silvestri", "authors": "Matteo Ceccarello and Francesco Silvestri", "title": "Experimental Evaluation of Multi-Round Matrix Multiplication on\n  MapReduce", "comments": "Proc. of 17th Meeting on Algorithm Engineering and Experiments\n  (ALENEX), 2015. The code is publicly available at http://www.dei.unipd.it/m3", "journal-ref": null, "doi": "10.1137/1.9781611973754.11", "report-no": null, "categories": "cs.DC cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach in the design of MapReduce algorithms is to minimize the\nnumber of rounds. Indeed, there are many examples in the literature of\nmonolithic MapReduce algorithms, which are algorithms requiring just one or two\nrounds. However, we claim that the design of monolithic algorithms may not be\nthe best approach in cloud systems. Indeed, multi-round algorithms may exploit\nsome features of cloud platforms by suitably setting the round number according\nto the execution context. In this paper we carry out an experimental study of\nmulti-round MapReduce algorithms aiming at investigating the performance of the\nmulti-round approach. We use matrix multiplication as a case study. We first\npropose a scalable Hadoop library, named M$_3$, for matrix multiplication in\nthe dense and sparse cases which allows to tradeoff round number with the\namount of data shuffled in each round and the amount of memory required by\nreduce functions. Then, we present an extensive study of this library on an\nin-house cluster and on Amazon Web Services aiming at showing its performance\nand at comparing monolithic and multi-round approaches. The experiments show\nthat, even without a low level optimization, it is possible to design\nmulti-round algorithms with a small running time overhead.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 21:23:11 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 22:25:08 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1408.2981", "submitter": "Eike Hermann M\\\"uller", "authors": "Andreas Dedner, Eike Hermann M\\\"uller, Robert Scheichl", "title": "Efficient Multigrid Preconditioners for Atmospheric Flow Simulations at\n  High Aspect Ratio", "comments": "22 pages, 6 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in fluid modelling require the efficient solution of highly\nanisotropic elliptic partial differential equations (PDEs) in \"flat\" domains.\nFor example, in numerical weather- and climate-prediction an elliptic PDE for\nthe pressure correction has to be solved at every time step in a thin spherical\nshell representing the global atmosphere. This elliptic solve can be one of the\ncomputationally most demanding components in semi-implicit semi-Lagrangian time\nstepping methods which are very popular as they allow for larger model time\nsteps and better overall performance. With increasing model resolution,\nalgorithmically efficient and scalable algorithms are essential to run the code\nunder tight operational time constraints. We discuss the theory and practical\napplication of bespoke geometric multigrid preconditioners for equations of\nthis type. The algorithms deal with the strong anisotropy in the vertical\ndirection by using the tensor-product approach originally analysed by B\\\"{o}rm\nand Hiptmair [Numer. Algorithms, 26/3 (2001), pp. 219-234]. We extend the\nanalysis to three dimensions under slightly weakened assumptions, and\nnumerically demonstrate its efficiency for the solution of the elliptic PDE for\nthe global pressure correction in atmospheric forecast models. For this we\ncompare the performance of different multigrid preconditioners on a\ntensor-product grid with a semi-structured and quasi-uniform horizontal mesh\nand a one dimensional vertical grid. The code is implemented in the Distributed\nand Unified Numerics Environment (DUNE), which provides an easy-to-use and\nscalable environment for algorithms operating on tensor-product grids. Parallel\nscalability of our solvers on up to 20,480 cores is demonstrated on the HECToR\nsupercomputer.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 11:41:02 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 16:00:44 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Dedner", "Andreas", ""], ["M\u00fcller", "Eike Hermann", ""], ["Scheichl", "Robert", ""]]}, {"id": "1408.3030", "submitter": "Fabian Reiter", "authors": "Fabian Reiter", "title": "Distributed Graph Automata and Verification of Distributed Algorithms", "comments": "26 pages, 6 figures, includes a condensed version of the author's\n  Master's thesis arXiv:1404.6503. (This version of the article (v2) is\n  identical to the previous one (v1), except for minor changes in phrasing.)", "journal-ref": null, "doi": "10.1109/LICS.2015.27", "report-no": null, "categories": "cs.FL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining ideas from distributed algorithms and alternating automata, we\nintroduce a new class of finite graph automata that recognize precisely the\nlanguages of finite graphs definable in monadic second-order logic. By\nrestricting transitions to be nondeterministic or deterministic, we also obtain\ntwo strictly weaker variants of our automata for which the emptiness problem is\ndecidable. As an application, we suggest how suitable graph automata might be\nuseful in formal verification of distributed algorithms, using Floyd-Hoare\nlogic.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 15:26:35 GMT"}, {"version": "v2", "created": "Sun, 28 Sep 2014 14:02:50 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Reiter", "Fabian", ""]]}, {"id": "1408.3033", "submitter": "Cristina Mu\\~noz", "authors": "Cristina Mu\\~noz and Pierre Leone", "title": "A Network Architecture for Distributed Event Based Systems in an\n  Ubiquitous Sensing Scenario", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Ubiquitous sensing devices frequently disseminate their data between them.\nThe use of a distributed event-based system that decouples publishers of\nsubscribers arises as an ideal candidate to implement the dissemination\nprocess. In this paper, we present a network architecture which merges the\nnetwork and overlay layers of typical structured event-based systems.\nDirectional Random Walks (DRWs) are used for the construction of this merged\nlayer. Our first results show that DRWs are suitable to balance the load using\na few nodes in the network to construct the dissemination path. As future work,\nwe propose to study the properties of this new layer and to work on the design\nof Bloom filters to manage broker nodes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 15:44:48 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Mu\u00f1oz", "Cristina", ""], ["Leone", "Pierre", ""]]}, {"id": "1408.3048", "submitter": "Mikolaj Szydlarski", "authors": "Mikolaj Szydlarski and Laura Grigori and Radek Stompor", "title": "Accelerating Cosmic Microwave Background map-making procedure through\n  preconditioning", "comments": "19 pages // Final version submitted to A&A", "journal-ref": "A&A 572, A39 (2014)", "doi": "10.1051/0004-6361/201323210", "report-no": null, "categories": "astro-ph.CO cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the sky signal from sequences of time ordered data is one of\nthe key steps in Cosmic Microwave Background (CMB) data analysis, commonly\nreferred to as the map-making problem. Some of the most popular and general\nmethods proposed for this problem involve solving generalised least squares\n(GLS) equations with non-diagonal noise weights given by a block-diagonal\nmatrix with Toeplitz blocks. In this work we study new map-making solvers\npotentially suitable for applications to the largest anticipated data sets.\nThey are based on iterative conjugate gradient (CG) approaches enhanced with\nnovel, parallel, two-level preconditioners. We apply the proposed solvers to\nexamples of simulated non-polarised and polarised CMB observations, and a set\nof idealised scanning strategies with sky coverage ranging from nearly a full\nsky down to small sky patches. We discuss in detail their implementation for\nmassively parallel computational platforms and their performance for a broad\nrange of parameters characterising the simulated data sets. We find that our\nbest new solver can outperform carefully-optimised standard solvers used today\nby a factor of as much as 5 in terms of the convergence rate and a factor of up\nto $4$ in terms of the time to solution, and to do so without significantly\nincreasing the memory consumption and the volume of inter-processor\ncommunication. The performance of the new algorithms is also found to be more\nstable and robust, and less dependent on specific characteristics of the\nanalysed data set. We therefore conclude that the proposed approaches are well\nsuited to address successfully challenges posed by new and forthcoming CMB data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 16:27:40 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 10:18:13 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Szydlarski", "Mikolaj", ""], ["Grigori", "Laura", ""], ["Stompor", "Radek", ""]]}, {"id": "1408.3079", "submitter": "Stefanie Roos", "authors": "Hani Salah, Stefanie Roos, Thorsten Strufe", "title": "A Lightweight Approach for Improving the Lookup Performance in\n  Kademlia-type Systems", "comments": "13 pages, 8 figures, conference version 'Diversity Entails\n  Improvement: A new Neighbour Selection Scheme for Kademlia-type Systems' at\n  IEEE P2P 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovery of nodes and content in large-scale distributed systems is\ngenerally based on Kademlia, today. Understanding Kademlia-type systems to\nimprove their performance is essential for maintaining a high service quality\nfor an increased number of participants, particularly when those systems are\nadopted by latency-sensitive applications.\n  This paper contributes to the understanding of Kademlia by studying the\nimpact of \\emph{diversifying} neighbours' identifiers within each routing table\nbucket on the lookup performance. We propose a new, yet backward-compatible,\nneighbour selection scheme that attempts to maximize the aforementioned\ndiversity. The scheme does not cause additional overhead except negligible\ncomputations for comparing the diversity of identifiers. We present a\ntheoretical model for the actual impact of the new scheme on the lookup's hop\ncount and validate it against simulations of three exemplary Kademlia-type\nsystems. We also measure the performance gain enabled by a partial deployment\nfor the scheme in the real KAD system. The results confirm the superiority of\nthe systems that incorporate our scheme.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 06:45:04 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Salah", "Hani", ""], ["Roos", "Stefanie", ""], ["Strufe", "Thorsten", ""]]}, {"id": "1408.3354", "submitter": "Jorge Plata-Chaves", "authors": "Jorge Plata-Chaves, Nikola Bogdanovic, Kostas Berberidis", "title": "Distributed Diffusion-Based LMS for Node-Specific Adaptive Parameter\n  Estimation", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed adaptive algorithm is proposed to solve a node-specific\nparameter estimation problem where nodes are interested in estimating\nparameters of local interest, parameters of common interest to a subset of\nnodes and parameters of global interest to the whole network. To address the\ndifferent node-specific parameter estimation problems, this novel algorithm\nrelies on a diffusion-based implementation of different Least Mean Squares\n(LMS) algorithms, each associated with the estimation of a specific set of\nlocal, common or global parameters. Coupled with the estimation of the\ndifferent sets of parameters, the implementation of each LMS algorithm is only\nundertaken by the nodes of the network interested in a specific set of local,\ncommon or global parameters. The study of convergence in the mean sense reveals\nthat the proposed algorithm is asymptotically unbiased. Moreover, a\nspatial-temporal energy conservation relation is provided to evaluate the\nsteady-state performance at each node in the mean-square sense. Finally, the\ntheoretical results and the effectiveness of the proposed technique are\nvalidated through computer simulations in the context of cooperative spectrum\nsensing in Cognitive Radio networks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 02:56:00 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Plata-Chaves", "Jorge", ""], ["Bogdanovic", "Nikola", ""], ["Berberidis", "Kostas", ""]]}, {"id": "1408.3432", "submitter": "Eli Gafni professor", "authors": "Eli Gafni", "title": "Snapshot for Time: The One-Shot Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for one-shot problems - problems where a processor executes a\nsingle operation-execution - timing constraints can be captured by conditions\non the relation between original outputs and supplementary snapshots. In\naddition to the dictionary definition of the word snapshot, in distributed\ncomputing snapshots also stand for a task that imposes relation among sets\nwhich are output of processors. Hence, constrains relating the timing between\noperation-executions of processors can be captured by the sets relation\nrepresenting a task.\n  This allows to bring to bear techniques developed for tasks, to one-shot\nobjects. In particular, for the one-shot case the question of linearizability\nis moot. Nevertheless, current proof techniques of object implementation\nrequire the prover to provide linearization-points even in the one shot case.\nTransforming the object into a task relieves the prover of an implementation\nfrom the burden of finding the \"linearization-points,\" since if the task is\nsolvable, linearization points are guaranteed to exist. We exhibit this\nadvantage with a new algorithm to implement MWMR register in a SWMR system.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 21:14:08 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Gafni", "Eli", ""]]}, {"id": "1408.3693", "submitter": "Zaid Towfic", "authors": "Zaid J. Towfic and Ali H. Sayed", "title": "Stability and Performance Limits of Adaptive Primal-Dual Networks", "comments": "16 pages, 9 figures", "journal-ref": "IEEE Transactions on Signal Processing, vol. 63, no. 11, pp.\n  2888-2903, Jun. 2015", "doi": "10.1109/TSP.2015.2415759", "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies distributed primal-dual strategies for adaptation and\nlearning over networks from streaming data. Two first-order methods are\nconsidered based on the Arrow-Hurwicz (AH) and augmented Lagrangian (AL)\ntechniques. Several revealing results are discovered in relation to the\nperformance and stability of these strategies when employed over adaptive\nnetworks. The conclusions establish that the advantages that these methods have\nfor deterministic optimization problems do not necessarily carry over to\nstochastic optimization problems. It is found that they have narrower stability\nranges and worse steady-state mean-square-error performance than primal methods\nof the consensus and diffusion type. It is also found that the AH technique can\nbecome unstable under a partial observation model, while the other techniques\nare able to recover the unknown under this scenario. A method to enhance the\nperformance of AL strategies is proposed by tying the selection of the\nstep-size to their regularization parameter. It is shown that this method\nallows the AL algorithm to approach the performance of consensus and diffusion\nstrategies but that it remains less stable than these other strategies.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 01:52:42 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2015 20:32:43 GMT"}, {"version": "v3", "created": "Wed, 13 May 2015 12:04:30 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Towfic", "Zaid J.", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1408.3704", "submitter": "Sivaraman Dasarathan", "authors": "Sivaraman Dasarathan, Cihan Tepedelenlioglu, Mahesh Banavar, and\n  Andreas Spanias", "title": "Robust Consensus in the Presence of Impulsive Channel Noise", "comments": "24 pages, 7 figures, Submitted to Transactions on Signal Processing,\n  Apr 2014 (Submitted, currently in review)", "journal-ref": null, "doi": "10.1109/TSP.2015.2408564", "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed average consensus algorithm robust to a wide range of impulsive\nchannel noise distributions is proposed. This work is the first of its kind in\nthe literature to propose a consensus algorithm which relaxes the requirement\nof finite moments on the communication noise. It is shown that the nodes reach\nconsensus asymptotically to a finite random variable whose expectation is the\ndesired sample average of the initial observations with a variance that depends\non the step size of the algorithm and the receiver nonlinear function. The\nasymptotic performance is characterized by deriving the asymptotic covariance\nmatrix using results from stochastic approximation theory. Simulations\ncorroborate our analytical findings and highlight the robustness of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 05:34:58 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Dasarathan", "Sivaraman", ""], ["Tepedelenlioglu", "Cihan", ""], ["Banavar", "Mahesh", ""], ["Spanias", "Andreas", ""]]}, {"id": "1408.3764", "submitter": "Loren Schwiebert", "authors": "Loren Schwiebert, Eyad Hailat, Kamel Rushaidat, Jason Mick, and\n  Jeffrey Potoff", "title": "An Efficient Cell List Implementation for Monte Carlo Simulation on GPUs", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing the performance potential of the modern day GPU architecture\nrequires judicious utilization of available parallel resources. Although\ndramatic reductions can often be obtained through straightforward mappings,\nfurther performance improvements often require algorithmic redesigns to more\nclosely exploit the target architecture. In this paper, we focus on efficient\nmolecular simulations for the GPU and propose a novel cell list algorithm that\nbetter utilizes its parallel resources. Our goal is an efficient GPU\nimplementation of large-scale Monte Carlo simulations for the grand canonical\nensemble. This is a particularly challenging application because there is\ninherently less computation and parallelism than in similar applications with\nmolecular dynamics. Consistent with the results of prior researchers, our\nsimulation results show traditional cell list implementations for Monte Carlo\nsimulations of molecular systems offer effectively no performance improvement\nfor small systems [5, 14], even when porting to the GPU. However for larger\nsystems, the cell list implementation offers significant gains in performance.\nFurthermore, our novel cell list approach results in better performance for all\nproblem sizes when compared with other GPU implementations with or without cell\nlists.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 19:30:37 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Schwiebert", "Loren", ""], ["Hailat", "Eyad", ""], ["Rushaidat", "Kamel", ""], ["Mick", "Jason", ""], ["Potoff", "Jeffrey", ""]]}, {"id": "1408.4423", "submitter": "Frank Hannig", "authors": "Frank Hannig, Dirk Koch, Daniel Ziener", "title": "Proceedings of the First International Workshop on FPGAs for Software\n  Programmers (FSP 2014)", "comments": "Website of the workshop: https://www12.cs.fau.de/ws/fsp2014/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers accepted at the First International Workshop\non FPGAs for Software Programmers (FSP 2014), held in Munich, Germany,\nSeptember 1st, 2014. FSP 2014 was co-located with the International Conference\non Field Programmable Logic and Applications (FPL).\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 18:43:54 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 05:40:38 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Hannig", "Frank", ""], ["Koch", "Dirk", ""], ["Ziener", "Daniel", ""]]}, {"id": "1408.4487", "submitter": "Mahdi Zamani", "authors": "Mahnush Movahedi and Mahdi Zamani", "title": "On Optimal Decision-Making in Ant Colonies", "comments": "Workshop on Biological Distributed Algorithms (BDA 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colonies of ants can collectively choose the best of several nests, even when\nmany of the active ants who organize the move visit only one site.\nUnderstanding such a behavior can help us design efficient distributed decision\nmaking algorithms. Marshall et al. propose a model for house-hunting in\ncolonies of ant Temnothorax albipennis. Unfortunately, their model does not\nachieve optimal decision-making while laboratory experiments show that, in\nfact, colonies usually achieve optimality during the house-hunting process. In\nthis paper, we argue that the model of Marshall et al. can achieve optimality\nby including nest size information in their mathematical model. We use lab\nresults of Pratt et al. to re-define the differential equations of Marshall et\nal. Finally, we sketch our strategy for testing the optimality of the new\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 21:46:04 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Movahedi", "Mahnush", ""], ["Zamani", "Mahdi", ""]]}, {"id": "1408.4587", "submitter": "Pier Stanislao Paolucci", "authors": "Pier Stanislao Paolucci, Iuliana Bacivarov, Devendra Rai, Lars Schor,\n  Lothar Thiele, Hoeseok Yang, Elena Pastorelli, Roberto Ammendola, Andrea\n  Biagioni, Ottorino Frezza, Francesca Lo Cicero, Alessandro Lonardo, Francesco\n  Simula, Laura Tosoratto, Piero Vicini", "title": "EURETILE D7.3 - Dynamic DAL benchmark coding, measurements on MPI\n  version of DPSNN-STDP (distributed plastic spiking neural net) and\n  improvements to other DAL codes", "comments": "34 pages. arXiv admin note: substantial text overlap with\n  arXiv:1310.8478", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.MS cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EURETILE project required the selection and coding of a set of dedicated\nbenchmarks. The project is about the software and hardware architecture of\nfuture many-tile distributed fault-tolerant systems. We focus on dynamic\nworkloads characterised by heavy numerical processing requirements. The\nambition is to identify common techniques that could be applied to both the\nEmbedded Systems and HPC domains. This document is the first public deliverable\nof Work Package 7: Challenging Tiled Applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 10:00:15 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Paolucci", "Pier Stanislao", ""], ["Bacivarov", "Iuliana", ""], ["Rai", "Devendra", ""], ["Schor", "Lars", ""], ["Thiele", "Lothar", ""], ["Yang", "Hoeseok", ""], ["Pastorelli", "Elena", ""], ["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Frezza", "Ottorino", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Simula", "Francesco", ""], ["Tosoratto", "Laura", ""], ["Vicini", "Piero", ""]]}, {"id": "1408.4715", "submitter": "Hugo A. Andrade", "authors": "Hugo A. Andrade, Simon Hogg, Stephan Ahrends", "title": "Making FPGAs Accessible to Scientists and Engineers as Domain Expert\n  Software Programmers with LabVIEW", "comments": "Presented at First International Workshop on FPGAs for Software\n  Programmers (FSP 2014) (arXiv:1408.4423)", "journal-ref": null, "doi": null, "report-no": "FSP/2014/02", "categories": "cs.SE cs.DC cs.OS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a graphical programming framework, LabVIEW, and\nassociated language and libraries, as well as programming techniques and\npatterns that we have found useful in making FPGAs accessible to scientists and\nengineers as domain expert software programmers.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 16:39:15 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Andrade", "Hugo A.", ""], ["Hogg", "Simon", ""], ["Ahrends", "Stephan", ""]]}, {"id": "1408.4721", "submitter": "Frank Hannig", "authors": "Moritz Schmid, Oliver Reiche, Christian Schmitt, Frank Hannig,\n  J\\\"urgen Teich", "title": "Code Generation for High-Level Synthesis of Multiresolution Applications\n  on FPGAs", "comments": "Presented at First International Workshop on FPGAs for Software\n  Programmers (FSP 2014) (arXiv:1408.4423)", "journal-ref": null, "doi": null, "report-no": "FSP/2014/04", "categories": "cs.CV cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution Analysis (MRA) is a mathematical method that is based on\nworking on a problem at different scales. One of its applications is medical\nimaging where processing at multiple scales, based on the concept of Gaussian\nand Laplacian image pyramids, is a well-known technique. It is often applied to\nreduce noise while preserving image detail on different levels of granularity\nwithout modifying the filter kernel. In scientific computing, multigrid methods\nare a popular choice, as they are asymptotically optimal solvers for elliptic\nPartial Differential Equations (PDEs). As such algorithms have a very high\ncomputational complexity that would overwhelm CPUs in the presence of real-time\nconstraints, application-specific processors come into consideration for\nimplementation. Despite of huge advancements in leveraging productivity in the\nrespective fields, designers are still required to have detailed knowledge\nabout coding techniques and the targeted architecture to achieve efficient\nsolutions. Recently, the HIPAcc framework was proposed as a means for automatic\ncode generation of image processing algorithms, based on a Domain-Specific\nLanguage (DSL). From the same code base, it is possible to generate code for\nefficient implementations on several accelerator technologies including\ndifferent types of Graphics Processing Units (GPUs) as well as reconfigurable\nlogic (FPGAs). In this work, we demonstrate the ability of HIPAcc to generate\ncode for the implementation of multiresolution applications on FPGAs and\nembedded GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 16:56:42 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Schmid", "Moritz", ""], ["Reiche", "Oliver", ""], ["Schmitt", "Christian", ""], ["Hannig", "Frank", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "1408.4725", "submitter": "Sam Skalicky", "authors": "Sam Skalicky, Andrew G. Schmidt, Matthew French", "title": "High Level Hardware/Software Embedded System Design with Redsharc", "comments": "Presented at First International Workshop on FPGAs for Software\n  Programmers (FSP 2014) (arXiv:1408.4423)", "journal-ref": null, "doi": null, "report-no": "FSP/2014/05", "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As tools for designing multiple processor systems-on-chips (MPSoCs) continue\nto evolve to meet the demands of developers, there exist systematic gaps that\nmust be bridged to provide a more cohesive hardware/software development\nenvironment. We present Redsharc to address these problems and enable: system\ngeneration, software/hardware compilation and synthesis, run-time control and\nexecution of MPSoCs. The efforts presented in this paper extend our previous\nwork to provide a rich API, build infrastructure, and runtime enabling\ndevelopers to design a system of simultaneously executing kernels in software\nor hardware, that communicate seamlessly. In this work we take Redsharc further\nto support a broader class of applications across a larger number of devices\nrequiring a more unified system development environment and build\ninfrastructure. To accomplish this we leverage existing tools and extend\nRedsharc with build and control infrastructure to relieve the burden of system\ndevelopment allowing software programmers to focus their efforts on application\nand kernel development.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 17:03:05 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Skalicky", "Sam", ""], ["Schmidt", "Andrew G.", ""], ["French", "Matthew", ""]]}, {"id": "1408.4939", "submitter": "Omer Arap", "authors": "Omer Arap, Martin Swany", "title": "Offloading MPI Parallel Prefix Scan (MPI_Scan) with the NetFPGA", "comments": "Presented at First International Workshop on FPGAs for Software\n  Programmers (FSP 2014) (arXiv:1408.4423)", "journal-ref": null, "doi": null, "report-no": "FSP/2014/06", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel programs written using the standard Message Passing Interface (MPI)\nfrequently depend upon the ability to efficiently execute collective\noperations. MPI_Scan is a collective operation defined in MPI that implements\nparallel prefix scan which is very useful primitive operation in several\nparallel applications. This operation can be very time consuming. In this\npaper, we explore the use of hardware programmable network interface cards\nutilizing standard media access protocols for offloading the MPI_Scan operation\nto the underlying network. Our work is based upon the NetFPGA - a programmable\nnetwork interface with an on-board Virtex FPGA and four Ethernet interfaces. We\nhave implemented a network-level MPI_Scan operation using the NetFPGA for use\nin MPI environments. This paper compares the performance of this implementation\nwith MPI over Ethernet for a small configuration.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 10:24:50 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Arap", "Omer", ""], ["Swany", "Martin", ""]]}, {"id": "1408.4959", "submitter": "Ruediger Willenberg", "authors": "Ruediger Willenberg, Paul Chow", "title": "A Software Parallel Programming Approach to FPGA-Accelerated Computing", "comments": "Presented at First International Workshop on FPGAs for Software\n  Programmers (FSP 2014) (arXiv:1408.4423)", "journal-ref": null, "doi": null, "report-no": "FSP/2014/08", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an effort to incorporate reconfigurable logic (FPGA)\ncomponents into a software programming model. For this purpose, we have\nimplemented a hardware engine for remote memory communication between hardware\ncomputation nodes and CPUs. The hardware engine is compatible with the API of\nGASNet, a popular communication library used for parallel computing\napplications. We have further implemented our own x86 and ARMv7 software\nversions of the GASNet Core API, enabling us to write distributed applications\nwith software and hardware GASNet components transparently communicating with\neach other.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 11:08:17 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Willenberg", "Ruediger", ""], ["Chow", "Paul", ""]]}, {"id": "1408.4965", "submitter": "Gordon Inggs", "authors": "Gordon Inggs, David Thomas, Wayne Luk", "title": "A Domain Specific Approach to Heterogeneous Computing: From Availability\n  to Accessibility", "comments": "Presented at First International Workshop on FPGAs for Software\n  Programmers (FSP 2014) (arXiv:1408.4423)", "journal-ref": null, "doi": null, "report-no": "FSP/2014/11", "categories": "cs.CE cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate a domain specific software development methodology for\nheterogeneous computing platforms such as Multicore CPUs, GPUs and FPGAs. We\nargue that three specific benefits are realised from adopting such an approach:\nportable, efficient implementations across heterogeneous platforms; domain\nspecific metrics of quality that characterise platforms in a form software\ndevelopers will understand; automatic, optimal partitioning across the\navailable computing resources. These three benefits allow a development\nmethodology for software developers where they describe their computational\nproblems in a single, easy to understand form, and after a modeling procedure\non the available resources, select how they would like to trade between various\ndomain specific metrics. Our work on the Forward Financial Framework ($F^3$)\ndemonstrates this methodology in practise. We are able to execute a range of\ncomputational finance option pricing tasks efficiently upon a wide range of\nCPU, GPU and FPGA computing platforms. We can also create accurate financial\ndomain metric models of walltime latency and statistical confidence.\nFurthermore, we believe that we can support automatic, optimal partitioning\nusing this execution and modelling capability.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 11:32:53 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Inggs", "Gordon", ""], ["Thomas", "David", ""], ["Luk", "Wayne", ""]]}, {"id": "1408.4969", "submitter": "Takaaki Miyajima", "authors": "Takaaki Miyajima, David Thomas, Hideharu Amano", "title": "An Automatic Mixed Software Hardware Pipeline Builder for CPU-FPGA\n  Platforms", "comments": "Presented at First International Workshop on FPGAs for Software\n  Programmers (FSP 2014) (arXiv:1408.4423)", "journal-ref": null, "doi": null, "report-no": "FSP/2014/13", "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our toolchain for accelerating application called Courier-FPGA, is designed\nfor utilize the processing power of CPU-FPGA platforms for software programmers\nand non-expert users. It automatically gathers runtime information of library\nfunctions from a running target binary, and constructs the function call graph\nincluding input-output data. Then, it uses corresponding predefined hardware\nmodules if these are ready for FPGA and prepares software functions on CPU by\nusing Pipeline Generator. The Pipeline Generator builds a pipeline control\nprogram by using Intel Threading Building Block to run both hardware modules\nand software functions in parallel. Finally, Courier-FPGA dynamically replaces\nthe original functions in the binary and accelerates it by using the built\npipeline. Courier-FPGA performs these acceleration processes without user\nintervention, source code tweaks or re-compilations of the binary. We describe\nthe technical details of this mixed software hardware pipeline on CPU-FPGA\nplatforms in this paper. In our case study, Courier-FPGA was used to accelerate\na corner detection using the Harris-Stephens method application binary on the\nZynq platform. A series of functions were off-loaded, and speed up 15.36 times\nwas achieved by using the built pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 11:50:40 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Miyajima", "Takaaki", ""], ["Thomas", "David", ""], ["Amano", "Hideharu", ""]]}, {"id": "1408.5845", "submitter": "Reza Arablouei", "authors": "Reza Arablouei, Stefan Werner, Kutluy{\\i}l Do\\u{g}an\\c{c}ay, and\n  Yih-Fang Huang", "title": "Analysis of a Reduced-Communication Diffusion LMS Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In diffusion-based algorithms for adaptive distributed estimation, each node\nof an adaptive network estimates a target parameter vector by creating an\nintermediate estimate and then combining the intermediate estimates available\nwithin its closed neighborhood. We analyze the performance of a\nreduced-communication diffusion least mean-square (RC-DLMS) algorithm, which\nallows each node to receive the intermediate estimates of only a subset of its\nneighbors at each iteration. This algorithm eases the usage of network\ncommunication resources and delivers a trade-off between estimation performance\nand communication cost. We show analytically that the RC-DLMS algorithm is\nstable and convergent in both mean and mean-square senses. We also calculate\nits theoretical steady-state mean-square deviation. Simulation results\ndemonstrate a good match between theory and experiment.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 17:42:41 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 00:40:57 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Arablouei", "Reza", ""], ["Werner", "Stefan", ""], ["Do\u011fan\u00e7ay", "Kutluy\u0131l", ""], ["Huang", "Yih-Fang", ""]]}, {"id": "1408.5962", "submitter": "EPTCS", "authors": "Giorgio Delzanno (DIBRIS, University of Genova), Michele Tatarek\n  (DIBRIS, University of Genova), Riccardo Traverso (FBK, Trento)", "title": "Model Checking Paxos in Spin", "comments": "In Proceedings GandALF 2014, arXiv:1408.5560", "journal-ref": "EPTCS 161, 2014, pp. 131-146", "doi": "10.4204/EPTCS.161.13", "report-no": null, "categories": "cs.LO cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal model of a distributed consensus algorithm in the\nexecutable specification language Promela extended with a new type of guards,\ncalled counting guards, needed to implement transitions that depend on majority\nvoting. Our formalization exploits abstractions that follow from reduction\ntheorems applied to the specific case-study. We apply the model checker Spin to\nautomatically validate finite instances of the model and to extract\npreconditions on the size of quorums used in the election phases of the\nprotocol.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 01:15:41 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Delzanno", "Giorgio", "", "DIBRIS, University of Genova"], ["Tatarek", "Michele", "", "DIBRIS, University of Genova"], ["Traverso", "Riccardo", "", "FBK, Trento"]]}, {"id": "1408.5963", "submitter": "EPTCS", "authors": "Antti Kuusisto", "title": "Infinite Networks, Halting and Local Algorithms", "comments": "In Proceedings GandALF 2014, arXiv:1408.5560", "journal-ref": "EPTCS 161, 2014, pp. 147-160", "doi": "10.4204/EPTCS.161.14", "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The immediate past has witnessed an increased amount of interest in local\nalgorithms, i.e., constant time distributed algorithms. In a recent survey of\nthe topic (Suomela, ACM Computing Surveys, 2013), it is argued that local\nalgorithms provide a natural framework that could be used in order to\ntheoretically control infinite networks in finite time. We study a\ncomprehensive collection of distributed computing models and prove that if\ninfinite networks are included in the class of structures investigated, then\nevery universally halting distributed algorithm is in fact a local algorithm.\nTo contrast this result, we show that if only finite networks are allowed, then\neven very weak distributed computing models can define nonlocal algorithms that\nhalt everywhere. The investigations in this article continue the studies in the\nintersection of logic and distributed computing initiated in (Hella et al.,\nPODC 2012) and (Kuusisto, CSL 2013).\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 01:15:54 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Kuusisto", "Antti", ""]]}, {"id": "1408.5979", "submitter": "EPTCS", "authors": "Rumyana Neykova (Imperial College London), Laura Bocchi (Imperial\n  College London), Nobuko Yoshida (Imperial College London)", "title": "Timed Runtime Monitoring for Multiparty Conversations", "comments": "In Proceedings BEAT 2014, arXiv:1408.5564", "journal-ref": "EPTCS 162, 2014, pp. 19-26", "doi": "10.4204/EPTCS.162.3", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic verification framework for protocols in real-time\ndistributed systems. The framework is based on Scribble, a tool-chain for\ndesign and verification of choreographies based on multiparty session types,\ndeveloped with our industrial partners. Drawing from recent work on multiparty\nsession types for real-time interactions, we extend Scribble with clocks,\nresets, and clock predicates constraining the times in which interactions\nshould occur. We present a timed API for Python to program distributed\nimplementations of Scribble specifications. A dynamic verification framework\nensures the safe execution of applications written with our timed API: we have\nimplemented dedicated runtime monitors that check that each interaction occurs\nat a correct timing with respect to the corresponding Scribble specification.\nThe performance of our implementation and its practicability are analysed via\nbenchmarking.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 02:15:40 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Neykova", "Rumyana", "", "Imperial College London"], ["Bocchi", "Laura", "", "Imperial\n  College London"], ["Yoshida", "Nobuko", "", "Imperial College London"]]}, {"id": "1408.6328", "submitter": "Marcos Assuncao", "authors": "Francois Rossigneux and Jean-Patrick Gelas and Laurent Lefevre and\n  Marcos Dias de Assuncao", "title": "A Generic and Extensible Framework for Monitoring Energy Consumption of\n  OpenStack Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cloud computing has been transformational to the IT industry, it is\nbuilt on large data centres that often consume massive amounts of electrical\npower. Efforts have been made to reduce the energy clouds consume, with certain\ndata centres now approaching a Power Usage Effectiveness (PUE) factor of 1.08.\nWhile this is an incredible mark, it also means that the IT infrastructure\naccounts for a large part of the power consumed by a data centre. Hence, means\nto monitor and analyse how energy is spent have never been so crucial. Such\nmonitoring is required not only for understanding how power is consumed, but\nalso for assessing the impact of energy management policies. In this article,\nwe draw lessons from experience on monitoring large-scale systems and introduce\nan energy monitoring software framework called KiloWatt API (KWAPI), able to\nhandle OpenStack clouds. The framework --- whose architecture is scalable,\nextensible, and completely integrated into OpenStack --- supports several\nwattmeter devices, multiple measurement formats, and minimises communication\noverhead.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 06:44:31 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Rossigneux", "Francois", ""], ["Gelas", "Jean-Patrick", ""], ["Lefevre", "Laurent", ""], ["de Assuncao", "Marcos Dias", ""]]}, {"id": "1408.6347", "submitter": "Aleem Akhtar Asif", "authors": "Aleem Akhtar, Aamir Shafi, Mohsan Jameel", "title": "Design and Implementation of Parallel Debugger and Profiler for MPJ\n  Express", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MPJ Express is a messaging system that allows computational scientists to\nwrite and execute parallel Java applications on High Performance Computing\n(HPC) hardware. Despite its successful adoption in the Java HPC community, the\nMPJ Express software currently does not provide any support for debugging and\nprofiling parallel applications and hence forces its users to rely on manual\nand tedious debugging/profiling methods. Support for such tools is essential to\nhelp application developers increase their overall productivity. To address\nthis we have developed debugging and profiling tools for MPJ Express, which are\nthe main topic of this paper. Key design goals for these tools include: 1)\nmaintain compatibility with existing logging, debugging, and visualizing tools,\n2) build these tools by extending existing debugging/profiling tools instead of\nreinventing the wheel. The first tool, named MPJDebug, builds on the\nopen-source Eclipse Integrated Development Environment (IDE). It provides an\nEclipse-based plugin developed using the Eclipse Plugin Development Environment\n(PDE). The default Eclipse debugger currently does not support debugging\nparallel applications running on a compute cluster. The second tool, named\nMPJProf, is a utility based on Tuning and Analysis Utility (TAU)-an open-source\nperformance evaluation tool. Our goal here is to exploit TAU to profile Java\napplications parallelized using MPJ Express by generating profiles and traces,\nwhich can later be visualized using existing tools like paraprof and Jumpshot.\nTowards the end of the paper, we quantify the overhead of using MPJProf, which\nwe found to be negligible in the profiling stage of parallel application\ndevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 08:44:25 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Akhtar", "Aleem", ""], ["Shafi", "Aamir", ""], ["Jameel", "Mohsan", ""]]}, {"id": "1408.6729", "submitter": "Alexandros Gerbessiotis", "authors": "Alexandros V. Gerbessiotis and Constantinos J. Siniolakis", "title": "BSP Sorting: An experimental Study", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bulk-Synchronous Parallel model of computation has been used for the\narchitecture independent design and analysis of parallel algorithms whose\nperformance is expressed not only in terms of problem size n but also in terms\nof parallel machine properties. In this paper the performance of\nimplementations of deterministic and randomized BSP sorting algorithms is\nexamined. The deterministic algorithm uses deterministic regular oversampling\nand parallel sample sorting and is augmented to handle duplicate keys\ntransparently with optimal asymptotic efficiency. The randomized algorithm is\nsample-sort based and uses oversampling and the ideas introduced with the\ndeterministic algorithm. The resulting randomized design, however, works\ndifferently from traditional parallel sample-sort based algorithms and is also\naugmented to transparently handle duplicate keys with optimal asymptotic\nefficiency thus eliminating the need to tag all input keys and to double\ncommunication/computation time. Both algorithms are shown to balance the\nwork-load evenly among the processors and the use and precise tuning of\noversampling that the BSP analysis allows combined with the transparent\nduplicate-key handling insures regular and balanced communication.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 14:04:52 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Gerbessiotis", "Alexandros V.", ""], ["Siniolakis", "Constantinos J.", ""]]}, {"id": "1408.6891", "submitter": "Rodrigo Calheiros", "authors": "Rajkumar Buyya, Rodrigo N. Calheiros, Jungmin Son, Amir Vahid\n  Dastjerdi, Young Yoon", "title": "Software-Defined Cloud Computing: Architectural Elements and Open\n  Challenges", "comments": "Keynote Paper, 3rd International Conference on Advances in Computing,\n  Communications and Informatics (ICACCI 2014), September 24-27, 2014, Delhi,\n  India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variety of existing cloud services creates a challenge for service\nproviders to enforce reasonable Software Level Agreements (SLA) stating the\nQuality of Service (QoS) and penalties in case QoS is not achieved. To avoid\nsuch penalties at the same time that the infrastructure operates with minimum\nenergy and resource wastage, constant monitoring and adaptation of the\ninfrastructure is needed. We refer to Software-Defined Cloud Computing, or\nsimply Software-Defined Clouds (SDC), as an approach for automating the process\nof optimal cloud configuration by extending virtualization concept to all\nresources in a data center. An SDC enables easy reconfiguration and adaptation\nof physical resources in a cloud infrastructure, to better accommodate the\ndemand on QoS through a software that can describe and manage various aspects\ncomprising the cloud environment. In this paper, we present an architecture for\nSDCs on data centers with emphasis on mobile cloud applications. We present an\nevaluation, showcasing the potential of SDC in two use cases-QoS-aware\nbandwidth allocation and bandwidth-aware, energy-efficient VM placement-and\ndiscuss the research challenges and opportunities in this emerging area.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 00:13:41 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 00:21:43 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Calheiros", "Rodrigo N.", ""], ["Son", "Jungmin", ""], ["Dastjerdi", "Amir Vahid", ""], ["Yoon", "Young", ""]]}, {"id": "1408.6923", "submitter": "Bogdan Oancea", "authors": "Bogdan Oancea, Tudorel Andrei, Raluca Mariana Dragoescu", "title": "GPGPU Computing", "comments": null, "journal-ref": "Proceedings of the CKS International Conference, 2012", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Since the first idea of using GPU to general purpose computing, things have\nevolved over the years and now there are several approaches to GPU programming.\nGPU computing practically began with the introduction of CUDA (Compute Unified\nDevice Architecture) by NVIDIA and Stream by AMD. These are APIs designed by\nthe GPU vendors to be used together with the hardware that they provide. A new\nemerging standard, OpenCL (Open Computing Language) tries to unify different\nGPU general computing API implementations and provides a framework for writing\nprograms executed across heterogeneous platforms consisting of both CPUs and\nGPUs. OpenCL provides parallel computing using task-based and data-based\nparallelism. In this paper we will focus on the CUDA parallel computing\narchitecture and programming model introduced by NVIDIA. We will present the\nbenefits of the CUDA programming model. We will also compare the two main\napproaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries\nto unify the GPGPU computing models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 05:24:20 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Oancea", "Bogdan", ""], ["Andrei", "Tudorel", ""], ["Dragoescu", "Raluca Mariana", ""]]}, {"id": "1408.7035", "submitter": "Elad Michael Schiller (PhD)", "authors": "Oscar Morales-Ponce and Elad M. Schiller and Paolo Falcone", "title": "Cooperation with Disagreement Correction in the Presence of\n  Communication Failures", "comments": "Extended version of the paper with the same name that appears in 17th\n  International IEEE Conference on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle-to-vehicle communication is a fundamental requirement in cooperative\nvehicular systems to achieve high performance while keeping high safety\nstandards. Vehicles periodically exchange critical information with nearby\nvehicles to determine their maneuvers according to the information quality and\nestablished strategies. However, wireless communication is prone to failures.\nThus, participants can be unaware that other participants have not received the\ninformation on time resulting in conflicting trajectories that may not be safe.\nWe present a deterministic solution that allows all participants to use a\ndefault strategy when other participants have not received on time the complete\ninformation. We base our solution on a timed distributed protocol that adapts\nits output according to the effect of message omission failures so that the\ndisagreement period occurs for no longer than a constant time (of the order of\nmilliseconds) that only depends on the message delay. We formally show the\ncorrectness and perform experiments to corroborate its efficiency. We explain\nhow the proposed solution can be used on vehicular platooning to attain high\nperformance and still guarantee high safety standards despite communication\nfailures. We believe that this work can facilitate the implementation of\ncooperative driving systems that have to deal with inherent (communication)\nuncertainties.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 14:34:00 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 23:00:57 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Morales-Ponce", "Oscar", ""], ["Schiller", "Elad M.", ""], ["Falcone", "Paolo", ""]]}, {"id": "1408.7070", "submitter": "Luiz Monnerat PhD", "authors": "Luiz Monnerat and Claudio L. Amorim", "title": "An effective single-hop distributed hash table with high lookup\n  performance and low traffic overhead", "comments": "This is the pre-peer reviewed version of the following article: Luiz\n  Monnerat and Claudio L. Amorim, An effective single-hop distributed hash\n  table with high lookup performance and low traffic overhead, Concurrency and\n  Computation: Practice and Experience (CCPE), 2014, which has been published\n  in final form at http://onlinelibrary.wiley.com/doi/10.1002/cpe.3342/abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Hash Tables (DHTs) have been used in several applications, but\nmost DHTs have opted to solve lookups with multiple hops, to minimize bandwidth\ncosts while sacrificing lookup latency. This paper presents D1HT, an original\nDHT which has a peer-to-peer and self-organizing architecture and maximizes\nlookup performance with reasonable maintenance traffic, and a Quarantine\nmechanism to reduce overheads caused by volatile peers. We implemented both\nD1HT and a prominent single-hop DHT, and we performed an extensive and highly\nrepresentative DHT experimental comparison, followed by complementary\nanalytical studies. In comparison with current single-hop DHTs, our results\nshowed that D1HT consistently had the lowest bandwidth requirements, with\ntypical reductions of up to one order of magnitude, and that D1HT could be used\neven in popular Internet applications with millions of users. In addition, we\nran the first latency experiments comparing DHTs to directory servers, which\nrevealed that D1HT can achieve latencies equivalent to or better than a\ndirectory server, and confirmed its greater scalability properties. Overall,\nour extensive set of results allowed us to conclude that D1HT can provide a\nvery effective solution for a broad range of environments, from large-scale\ncorporate datacenters to widely deployed Internet applications.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 16:59:25 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Monnerat", "Luiz", ""], ["Amorim", "Claudio L.", ""]]}]