[{"id": "2106.00003", "submitter": "Firas Hamze", "authors": "Firas Hamze", "title": "Parallelized Computation and Backpropagation Under Angle-Parametrized\n  Orthogonal Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodology for parallel acceleration of learning in the\npresence of matrix orthogonality and unitarity constraints of interest in\nseveral branches of machine learning. We show how an apparently sequential\nelementary rotation parametrization can be restructured into blocks of\ncommutative operations using a well-known tool for coloring the edges of\ncomplete graphs, in turn widely applied to schedule round-robin\n(all-against-all) sports tournaments. The resulting decomposition admits an\nalgorithm to compute a fully-parametrized orthogonal matrix from its rotation\nparameters in $O(n)$ sequential steps and one to compute the gradient of a\ntraining loss with respect to its parameters in $O(n\\log n)$ steps. We discuss\nparametric restrictions of interest to generative modeling and present\npromising performance results with a prototype GPU implementation.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:47:03 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Hamze", "Firas", ""]]}, {"id": "2106.00066", "submitter": "Sudeep Pasricha", "authors": "Ninad Hogade, Sudeep Pasricha, Howard Jay Siegel", "title": "Energy and Network Aware Workload Management for Geographically\n  Distributed Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cloud service providers are distributing data centers geographically to\nminimize energy costs through intelligent workload distribution. With\nincreasing data volumes in emerging cloud workloads, it is critical to factor\nin the network costs for transferring workloads across data centers. For\ngeo-distributed data centers, many researchers have been exploring strategies\nfor energy cost minimization and intelligent inter-data-center workload\ndistribution separately. However, prior work does not comprehensively and\nsimultaneously consider data center energy costs, data transfer costs, and data\ncenter queueing delay. In this paper, we propose a novel game theory-based\nworkload management framework that takes a holistic approach to the cloud\noperating cost minimization problem by making intelligent scheduling decisions\naware of data transfer costs and the data center queueing delay. Our framework\nperforms intelligent workload management that considers heterogeneity in data\ncenter compute capability, cooling power, interference effects from task\nco-location in servers, time-of-use electricity pricing, renewable energy, net\nmetering, peak demand pricing distribution, and network pricing. Our\nsimulations show that the proposed game-theoretic technique can minimize the\ncloud operating cost more effectively than existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 19:15:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Hogade", "Ninad", ""], ["Pasricha", "Sudeep", ""], ["Siegel", "Howard Jay", ""]]}, {"id": "2106.00083", "submitter": "Daniel Engel", "authors": "Daniel Engel, Maurice Herlihy", "title": "Composing Networks of Automated Market Makers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated market makers (AMMs) are automata that trade electronic assets at\nrates set by mathematical formulas. AMMs are usually implemented by smart\ncontracts on blockchains. In practice, AMMs are often composed: and outputs\nfrom AMMs can be directed into other compatible AMMs. This paper proposes a\nmathematical model for AMM composition. We define sequential and parallel\ncomposition operators for AMMs in a way that ensures that AMMs are closed under\ncomposition, in a way that works for \"higher-dimensional\" AMMs that manage more\nthan two asset classes, and so the composition of AMMs in \"stable\" states\nremains stable.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 20:09:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 17:53:07 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Engel", "Daniel", ""], ["Herlihy", "Maurice", ""]]}, {"id": "2106.00275", "submitter": "He Yang", "authors": "He Yang", "title": "H-FL: A Hierarchical Communication-Efficient and Privacy-Protected\n  Architecture for Federated Learning", "comments": "Accepted by IJCAI 2021, 7pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longstanding goals of federated learning (FL) require rigorous privacy\nguarantees and low communication overhead while holding a relatively high model\naccuracy. However, simultaneously achieving all the goals is extremely\nchallenging. In this paper, we propose a novel framework called hierarchical\nfederated learning (H-FL) to tackle this challenge. Considering the degradation\nof the model performance due to the statistic heterogeneity of the training\ndata, we devise a runtime distribution reconstruction strategy, which\nreallocates the clients appropriately and utilizes mediators to rearrange the\nlocal training of the clients. In addition, we design a compression-correction\nmechanism incorporated into H-FL to reduce the communication overhead while not\nsacrificing the model performance. To further provide privacy guarantees, we\nintroduce differential privacy while performing local training, which injects\nmoderate amount of noise into only part of the complete model. Experimental\nresults show that our H-FL framework achieves the state-of-art performance on\ndifferent datasets for the real-world image recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 07:15:31 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yang", "He", ""]]}, {"id": "2106.00344", "submitter": "Manuel Bravo", "authors": "Manuel Bravo, Alexey Gotsman, Borja de R\\'egil, Hengfeng Wei", "title": "UniStore: A fault-tolerant marriage of causal and strong consistency\n  (extended version)", "comments": "Extended version of a paper from USENIX ATC'21: Annual Technical\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern online services rely on data stores that replicate their data across\ngeographically distributed data centers. Providing strong consistency in such\ndata stores results in high latencies and makes the system vulnerable to\nnetwork partitions. The alternative of relaxing consistency violates crucial\ncorrectness properties. A compromise is to allow multiple consistency levels to\ncoexist in the data store. In this paper we present UniStore, the first\nfault-tolerant and scalable data store that combines causal and strong\nconsistency. The key challenge we address in UniStore is to maintain liveness\ndespite data center failures: this could be compromised if a strong transaction\ntakes a dependency on a causal transaction that is later lost because of a\nfailure. UniStore ensures that such situations do not arise while paying the\ncost of durability for causal transactions only when necessary. We evaluate\nUniStore on Amazon EC2 using both microbenchmarks and a sample application. Our\nresults show that UniStore effectively and scalably combines causal and strong\nconsistency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 09:30:23 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:26:59 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Bravo", "Manuel", ""], ["Gotsman", "Alexey", ""], ["de R\u00e9gil", "Borja", ""], ["Wei", "Hengfeng", ""]]}, {"id": "2106.00374", "submitter": "Michal Dory", "authors": "Michal Dory, Merav Parter", "title": "Fault-Tolerant Labeling and Compact Routing Schemes", "comments": "PODC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents fault-tolerant (FT) labeling schemes for general graphs,\nas well as, improved FT routing schemes. For a given $n$-vertex graph $G$ and a\nbound $f$ on the number of faults, an $f$-FT connectivity labeling scheme is a\ndistributed data structure that assigns each of the graph edges and vertices a\nshort label, such that given the labels of the vertices $s$ and $t$, and at\nmost $f$ failing edges $F$, one can determine if $s$ and $t$ are connected in\n$G \\setminus F$. The primary complexity measure is the length of the individual\nlabels. Since their introduction by [Courcelle, Twigg, STACS '07], compact FT\nlabeling schemes have been devised only for a limited collection of graph\nfamilies. In this work, we fill in this gap by proposing two (independent) FT\nconnectivity labeling schemes for general graphs, with a nearly optimal label\nlength. This serves the basis for providing also FT approximate distance\nlabeling schemes, and ultimately also routing schemes. Our main results for an\n$n$-vertex graph and a fault bound $f$ are:\n  -- There is a randomized FT connectivity labeling scheme with a label length\nof $O(f+\\log n)$ bits, hence optimal for $f=O(\\log n)$. This scheme is based on\nthe notion of cycle space sampling [Pritchard, Thurimella, TALG '11].\n  -- There is a randomized FT connectivity labeling scheme with a label length\nof $O(\\log^3 n)$ bits (independent of the number of faults $f$). This scheme is\nbased on the notion of linear sketches of [Ahn et al., SODA '12].\n  -- For $k\\geq 1$, there is a randomized routing scheme that routes a message\nfrom $s$ to $t$ in the presence of a set $F$ of faulty edges, with stretch\n$O(|F|^2 k)$ and routing tables of size $\\tilde{O}(f^3 n^{1/k})$.\n  This significantly improves over the state-of-the-art bounds by [Chechik,\nICALP '11], providing the first scheme with sub-linear FT labeling and routing\nschemes for general graphs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:36:30 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Dory", "Michal", ""], ["Parter", "Merav", ""]]}, {"id": "2106.00565", "submitter": "Kris Nikov", "authors": "Kris Nikov, Marcos Martinez, Simon Wegener, Jose Nunez-Yanez, Zbigniew\n  Chamski, Kyriakos Georgiou and Kerstin Eder", "title": "Robust and accurate fine-grain power models for embedded systems with no\n  on-chip PMU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel approach to event-based power modelling for\nembedded platforms that do not have a Performance Monitoring Unit (PMU). The\nmethod involves complementing the target hardware platform, where the physical\npower data is measured, with another platform on which the CPU performance\ndata, that is needed for model generation, can be collected. The methodology is\nused to generate accurate fine-grain power models for the the Gaisler GR712RC\ndual-core LEON3 fault-tolerant SPARC processor with on-board power sensors and\nno PMU. A Kintex UltraScale FPGA is used as the support platform to obtain the\nrequired CPU performance data, by running a soft-core representation of the\ndual-core LEON3 as on the GR712RC but with a PMU implementation. Both platforms\nexecute the same benchmark set and data collection is synchronised using\nper-sample timestamps so that the power sensor data from the GR712RC board can\nbe matched to the PMU data from the FPGA. The synchronised samples are then\nprocessed by the Robust Energy and Power Predictor Selection (REPPS) software\nin order to generate power models. The models achieve less than 2% power\nestimation error when validated on an industrial use-case and can successfully\nfollow program phases, which makes them suitable for runtime power profiling.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 10:04:27 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:57:38 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 13:40:09 GMT"}, {"version": "v4", "created": "Tue, 27 Jul 2021 08:44:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Nikov", "Kris", ""], ["Martinez", "Marcos", ""], ["Wegener", "Simon", ""], ["Nunez-Yanez", "Jose", ""], ["Chamski", "Zbigniew", ""], ["Georgiou", "Kyriakos", ""], ["Eder", "Kerstin", ""]]}, {"id": "2106.00583", "submitter": "Aitor Arjona", "authors": "Aitor Arjona, Pedro Garc\\'ia-L\\'opez, Josep Samp\\'e, Aleksander\n  Slominski and Lionel Villard", "title": "Triggerflow: Trigger-based Orchestration of Serverless Workflows", "comments": "17 pages, 17 figures, preprint submitted to Future Generation\n  Computer Systems. arXiv admin note: substantial text overlap with\n  arXiv:2006.08654", "journal-ref": "Future Generation Computer Systems, Volume 124, November 2021,\n  Pages 215-229", "doi": "10.1016/j.future.2021.06.004", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As more applications are being moved to the Cloud thanks to serverless\ncomputing, it is increasingly necessary to support the native life cycle\nexecution of those applications in the data center. But existing cloud\norchestration systems either focus on short-running workflows (like IBM\nComposer or Amazon Step Functions Express Workflows) or impose considerable\noverheads for synchronizing massively parallel jobs (Azure Durable Functions,\nAmazon Step Functions). None of them are open systems enabling extensible\ninterception and optimization of custom workflows. We present Triggerflow: an\nextensible Trigger-based Orchestration architecture for serverless workflows.\nWe demonstrate that Triggerflow is a novel serverless building block capable of\nconstructing different reactive orchestrators (State Machines, Directed Acyclic\nGraphs, Workflow as code, Federated Learning orchestrator). We also validate\nthat it can support high-volume event processing workloads, auto-scale on\ndemand with scale down to zero when not used, and transparently guarantee fault\ntolerance and efficient resource usage when orchestrating long running\nscientific workflows.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:50:22 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 08:30:08 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Arjona", "Aitor", ""], ["Garc\u00eda-L\u00f3pez", "Pedro", ""], ["Samp\u00e9", "Josep", ""], ["Slominski", "Aleksander", ""], ["Villard", "Lionel", ""]]}, {"id": "2106.00881", "submitter": "Denis Kleyko", "authors": "Antonello Rosato, Massimo Panella, Denis Kleyko", "title": "Hyperdimensional Computing for Efficient Distributed Classification with\n  Randomized Neural Networks", "comments": "1 table, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the supervised learning domain, considering the recent prevalence of\nalgorithms with high computational cost, the attention is steering towards\nsimpler, lighter, and less computationally extensive training and inference\napproaches. In particular, randomized algorithms are currently having a\nresurgence, given their generalized elementary approach. By using randomized\nneural networks, we study distributed classification, which can be employed in\nsituations were data cannot be stored at a central location nor shared. We\npropose a more efficient solution for distributed classification by making use\nof a lossy compression approach applied when sharing the local classifiers with\nother agents. This approach originates from the framework of hyperdimensional\ncomputing, and is adapted herein. The results of experiments on a collection of\ndatasets demonstrate that the proposed approach has usually higher accuracy\nthan local classifiers and getting close to the benchmark - the centralized\nclassifier. This work can be considered as the first step towards analyzing the\nvariegated horizon of distributed randomized neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:33:56 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Rosato", "Antonello", ""], ["Panella", "Massimo", ""], ["Kleyko", "Denis", ""]]}, {"id": "2106.00999", "submitter": "Mounssif Krouka", "authors": "Mounssif Krouka, Anis Elgabli, Chaouki ben Issaid, and Mehdi Bennis", "title": "Communication-Efficient Split Learning Based on Analog Communication and\n  Over the Air Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Split-learning (SL) has recently gained popularity due to its inherent\nprivacy-preserving capabilities and ability to enable collaborative inference\nfor devices with limited computational power. Standard SL algorithms assume an\nideal underlying digital communication system and ignore the problem of scarce\ncommunication bandwidth. However, for a large number of agents, limited\nbandwidth resources, and time-varying communication channels, the communication\nbandwidth can become the bottleneck. To address this challenge, in this work,\nwe propose a novel SL framework to solve the remote inference problem that\nintroduces an additional layer at the agent side and constrains the choices of\nthe weights and the biases to ensure over the air aggregation. Hence, the\nproposed approach maintains constant communication cost with respect to the\nnumber of agents enabling remote inference under limited bandwidth. Numerical\nresults show that our proposed algorithm significantly outperforms the digital\nimplementation in terms of communication-efficiency, especially as the number\nof agents grows large.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 07:49:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Krouka", "Mounssif", ""], ["Elgabli", "Anis", ""], ["Issaid", "Chaouki ben", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2106.01036", "submitter": "Shaked Matar", "authors": "Michael Elkin, Shaked Matar", "title": "Ultra-Sparse Near-Additive Emulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-additive (aka $(1+\\epsilon,\\beta)$-) emulators and spanners are a\nfundamental graph-algorithmic construct, with numerous applications for\ncomputing approximate shortest paths and related problems in distributed,\nstreaming and dynamic settings.\n  Known constructions of near-additive emulators enable one to trade between\ntheir sparsity (i.e., number of edges) and the additive stretch $\\beta$.\nSpecifically, for any pair of parameters $\\epsilon >0$, $ \\kappa=1,2,\\dots$,\none can have a $(1+\\epsilon,\\beta)$-emulator with $O(n^{1+1/\\kappa})$ edges,\nwith $\\beta = \\left(\\frac{\\log \\kappa}{\\epsilon}\\right)^{\\log \\kappa}$. At\ntheir sparsest, these emulators employ $c\\cdot n$ edges, for some constant\n$c\\geq 2$.\n  We tighten this bound, and show that in fact precisely $n^{1+1/\\kappa}$ edges\nsuffice.\n  In particular, our emulators can be \\emph{ultra-sparse}, i.e., we can have an\nemulator with $n+o(n)$ edges and $\\beta = \\left(\\frac{\\log {\\log n}}{\\epsilon\n}\\right)^{{\\log {\\log n}}(1+o(1))}$.\n  We also devise a distributed deterministic algorithm in the CONGEST model\nthat builds these emulators in low polynomial time (i.e., in $O(n^\\rho)$ time,\nfor an arbitrarily small constant parameter $\\rho >0$).\n  Finally, we also improve the state-of-the-art distributed deterministic\n\\congest-model construction of\n  $(1+\\epsilon,\\beta)$-spanners devised in the PODC'19 paper\n  [ElkinM19]. Specifically, the spanners of [ElkinM19] have $O(\\beta\\cdot\nn^{1+1/\\kappa})$ edges, i.e., at their sparsest they employ\n  $ O\\left(\\frac{\\log {\\log n}}{\\epsilon }\\right)^{{\\log {\\log n}}}\\cdot n$\nedges. In this paper, we devise an efficient distributed deterministic\nCONGEST-model algorithm that builds such spanners with $O(n^{1+1/\\kappa})$\nedges for $\\kappa = O\\left(\\frac{\\log n}{\\log ^{(3)}n}\\right)$. At their\nsparsest, these spanners employ only $O(n\\cdot {\\log {\\log n}})$ edges.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:10:08 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Elkin", "Michael", ""], ["Matar", "Shaked", ""]]}, {"id": "2106.01108", "submitter": "Fabien Dufoulon", "authors": "Fabien Dufoulon (Technion - Israel Institute of Technology), Shay\n  Kutten (Technion - Israel Institute of Technology) and William K. Moses Jr.\n  (University of Houston)", "title": "Efficient Deterministic Leader Election for Programmable Matter", "comments": "PODC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It was suggested that a programmable matter system (composed of multiple\ncomputationally weak mobile particles) should remain connected at all times\nsince otherwise, reconnection is difficult and may be impossible. At the same\ntime, it was not clear that allowing the system to disconnect carried a\nsignificant advantage in terms of time complexity. We demonstrate for a\nfundamental task, that of leader election, an algorithm where the system\ndisconnects and then reconnects automatically in a non-trivial way (particles\ncan move far away from their former neighbors and later reconnect to others).\nMoreover, the runtime of the temporarily disconnecting deterministic leader\nelection algorithm is linear in the diameter. Hence, the disconnecting --\nreconnecting algorithm is as fast as previous randomized algorithms. When\ncomparing to previous deterministic algorithms, we note that some of the\nprevious work assumed weaker schedulers. Still, the runtime of all the previous\ndeterministic algorithms that did not assume special shapes of the particle\nsystem (shapes with no holes) was at least quadratic in $n$, where $n$ is the\nnumber of particles in the system. (Moreover, the new algorithm is even faster\nin some parameters than the deterministic algorithms that did assume special\ninitial shapes.)\n  Since leader election is an important module in algorithms for various other\ntasks, the presented algorithm can be useful for speeding up other algorithms\nunder the assumption of a strong scheduler. This leaves open the question: \"can\na deterministic algorithm be as fast as the randomized ones also under weaker\nschedulers?\"\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:18:57 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Dufoulon", "Fabien", "", "Technion - Israel Institute of Technology"], ["Kutten", "Shay", "", "Technion - Israel Institute of Technology"], ["Moses", "William K.", "Jr.", "University of Houston"]]}, {"id": "2106.01273", "submitter": "Wenlong Tian", "authors": "Xuming Ye, Xiaoye Xue, Wenlong Tian, Zhiyong Xu, Weijun Xiao, Ruixuan\n  Li", "title": "Chunk Content is not Enough: Chunk-Context Aware Resemblance Detection\n  for Deduplication Delta Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing popularity of cloud storage, removing duplicated data across\nusers is getting more critical for service providers to reduce costs. Recently,\nData resemblance detection is a novel technology to detect redundancy among\nsimilarity. It extracts feature from each chunk content and treat chunks with\nhigh similarity as candidates for removing redundancy. However, popular\nresemblance methods such as \"N-transform\" and \"Finesse\" use only the chunk data\nfor feature extraction. A minor modification on the data chunk could seriously\ndeteriorate its capability for resemblance detection. In this paper, we\nproposes a novel chunk-context aware resemblance detection algorithm, called\nCARD, to mitigate this issue. CARD introduces a BP-Neural network-based\nchunk-context aware model, and uses N-sub-chunk shingles-based initial feature\nextraction strategy. It effectively integrates each data chunk content's\ninternal structure with the context information for feature extraction, the\nimpact of small changes in data chunks is significantly reduced. To evaluate\nits performance, we implement a CARD prototype and conduct extensive\nexperiments using real-world data sets. The results show that CARD can detect\nup to 75.03% more redundant data and accelerate the resemblance detection\noperations by 5.6 to 17.8 times faster compared with the state-of-the-art\nresemblance detection approaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:34:07 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ye", "Xuming", ""], ["Xue", "Xiaoye", ""], ["Tian", "Wenlong", ""], ["Xu", "Zhiyong", ""], ["Xiao", "Weijun", ""], ["Li", "Ruixuan", ""]]}, {"id": "2106.01329", "submitter": "Giacomo Indiveri", "authors": "Giacomo Indiveri", "title": "Introducing \"Neuromorphic Computing and Engineering\"", "comments": "NCE Editorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CE cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard nature of computing is currently being challenged by a range of\nproblems that start to hinder technological progress. One of the strategies\nbeing proposed to address some of these problems is to develop novel\nbrain-inspired processing methods and technologies, and apply them to a wide\nrange of application scenarios. This is an extremely challenging endeavor that\nrequires researchers in multiple disciplines to combine their efforts and\nco-design at the same time the processing methods, the supporting computing\narchitectures, and their underlying technologies. The journal ``Neuromorphic\nComputing and Engineering'' (NCE) has been launched to support this new\ncommunity in this effort and provide a forum and repository for presenting and\ndiscussing its latest advances. Through close collaboration with our colleagues\non the editorial team, the scope and characteristics of NCE have been designed\nto ensure it serves a growing transdisciplinary and dynamic community across\nacademia and industry.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 20:12:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Indiveri", "Giacomo", ""]]}, {"id": "2106.01331", "submitter": "Tao Chen", "authors": "Tao Chen and Miqing Li", "title": "Multi-Objectivizing Software Configuration Tuning (for a single\n  performance concern)", "comments": "13 pages, 7 figures, 4 tables. In Proceedings of the 29th ACM Joint\n  European Software Engineering Conference and Symposium on the Foundations of\n  Software Engineering (ESEC/FSE'21), 2021", "journal-ref": null, "doi": "10.1145/3468264.3468555", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically tuning software configuration for optimizing a single\nperformance attribute (e.g., minimizing latency) is not trivial, due to the\nnature of the configuration systems (e.g., complex landscape and expensive\nmeasurement). To deal with the problem, existing work has been focusing on\ndeveloping various effective optimizers. However, a prominent issue that all\nthese optimizers need to take care of is how to avoid the search being trapped\nin local optima -- a hard nut to crack for software configuration tuning due to\nits rugged and sparse landscape, and neighboring configurations tending to\nbehave very differently. Overcoming such in an expensive measurement setting is\neven more challenging. In this paper, we take a different perspective to tackle\nthis issue. Instead of focusing on improving the optimizer, we work on the\nlevel of optimization model. We do this by proposing a meta\nmulti-objectivization model (MMO) that considers an auxiliary performance\nobjective (e.g., throughput in addition to latency). What makes this model\nunique is that we do not optimize the auxiliary performance objective, but\nrather use it to make similarly-performing while different configurations less\ncomparable (i.e. Pareto nondominated to each other), thus preventing the search\nfrom being trapped in local optima.\n  Experiments on eight real-world software systems/environments with diverse\nperformance attributes reveal that our MMO model is statistically more\neffective than state-of-the-art single-objective counterparts in overcoming\nlocal optima (up to 42% gain), while using as low as 24% of their measurements\nto achieve the same (or better) performance result.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 03:03:53 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Chen", "Tao", ""], ["Li", "Miqing", ""]]}, {"id": "2106.01340", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Transaction Fee Mechanism Design", "comments": "Appears in the 22nd ACM Conference on Economics and Computation (EC\n  '21). This conference paper is derived from Sections 2, 4, 5, 6, and 8 of the\n  longer general-audience report published as arXiv:2012.00854", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS cs.GT econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand for blockchains such as Bitcoin and Ethereum is far larger than\nsupply, necessitating a mechanism that selects a subset of transactions to\ninclude \"on-chain\" from the pool of all pending transactions. EIP-1559 is a\nproposal to make several tightly coupled changes to the Ethereum blockchain's\ntransaction fee mechanism, including the introduction of variable-size blocks\nand a burned base fee that rises and falls with demand. These changes are\nslated for deployment in Ethereum's \"London fork,\" scheduled for late\nsummer~2021, at which point it will be the biggest economic change made to a\nmajor blockchain to date.\n  The first goal of this paper is to formalize the problem of designing a\ntransaction fee mechanism, taking into account the many idiosyncrasies of the\nblockchain setting (ranging from off-chain collusion between miners and users\nto the ease of money-burning). The second goal is to situate the specific\nmechanism proposed in EIP-1559 in this framework and rigorously interrogate its\ngame-theoretic properties. The third goal is to suggest competing designs that\noffer alternative sets of trade-offs. The final goal is to highlight research\nopportunities for the EC community that could help shape the future of\nblockchain transaction fee mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:48:32 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "2106.01674", "submitter": "Hao Liu", "authors": "Hao Liu, Qian Gao, Jiang Li, Xiaochao Liao, Hao Xiong, Guangxing Chen,\n  Wenlin Wang, Guobao Yang, Zhiwei Zha, Daxiang Dong, Dejing Dou, Haoyi Xiong", "title": "JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale\n  Online Inference at Baidu", "comments": "Accepted to SIGKDD 2021 applied data science track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In modern internet industries, deep learning based recommender systems have\nbecame an indispensable building block for a wide spectrum of applications,\nsuch as search engine, news feed, and short video clips. However, it remains\nchallenging to carry the well-trained deep models for online real-time\ninference serving, with respect to the time-varying web-scale traffics from\nbillions of users, in a cost-effective manner. In this work, we present JIZHI -\na Model-as-a-Service system - that per second handles hundreds of millions of\nonline inference requests to huge deep models with more than trillions of\nsparse parameters, for over twenty real-time recommendation services at Baidu,\nInc. In JIZHI, the inference workflow of every recommendation request is\ntransformed to a Staged Event-Driven Pipeline (SEDP), where each node in the\npipeline refers to a staged computation or I/O intensive task processor. With\ntraffics of real-time inference requests arrived, each modularized processor\ncan be run in a fully asynchronized way and managed separately. Besides, JIZHI\nintroduces heterogeneous and hierarchical storage to further accelerate the\nonline inference process by reducing unnecessary computations and potential\ndata access latency induced by ultra-sparse model parameters. Moreover, an\nintelligent resource manager has been deployed to maximize the throughput of\nJIZHI over the shared infrastructure by searching the optimal resource\nallocation plan from historical logs and fine-tuning the load shedding policies\nover intermediate system feedback. Extensive experiments have been done to\ndemonstrate the advantages of JIZHI from the perspectives of end-to-end service\nlatency, system-wide throughput, and resource consumption. JIZHI has helped\nBaidu saved more than ten million US dollars in hardware and utility costs\nwhile handling 200% more traffics without sacrificing inference efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:23:24 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Hao", ""], ["Gao", "Qian", ""], ["Li", "Jiang", ""], ["Liao", "Xiaochao", ""], ["Xiong", "Hao", ""], ["Chen", "Guangxing", ""], ["Wang", "Wenlin", ""], ["Yang", "Guobao", ""], ["Zha", "Zhiwei", ""], ["Dong", "Daxiang", ""], ["Dou", "Dejing", ""], ["Xiong", "Haoyi", ""]]}, {"id": "2106.01710", "submitter": "Milind Chabbi", "authors": "Zhizhou Zhang, Milind Chabbi, Adam Welc, Timothy Sherwood", "title": "Optimistic Concurrency Control for Real-world Go Programs (Extended\n  Version with Appendix)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a source-to-source transformation framework, GOCC, that consumes\nlock-based pessimistic concurrency programs in the Go language and transforms\nthem into optimistic concurrency programs that use Hardware Transactional\nMemory (HTM). The choice of the Go language is motivated by the fact that\nconcurrency is a first-class citizen in Go, and it is widely used in Go\nprograms. GOCC performs rich inter-procedural program analysis to detect and\nfilter lock-protected regions and performs AST-level code transformation of the\nsurrounding locks when profitable. Profitability is driven by both static\nanalyses of critical sections and dynamic analysis via execution profiles. A\ncustom HTM library, using perceptron, learns concurrency behavior and\ndynamically decides whether to use HTM in the rewritten lock/unlock points.\nGiven the rich history of transactional memory research but its lack of\nadoption in any industrial setting, we believe this workflow, which ultimately\nproduces source-code patches, is more apt for industry-scale adoption. Results\non widely adopted Go libraries and applications demonstrate significant (up to\n10x) and scalable performance gains resulting from our automated transformation\nwhile avoiding major performance regressions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:27:37 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Zhizhou", ""], ["Chabbi", "Milind", ""], ["Welc", "Adam", ""], ["Sherwood", "Timothy", ""]]}, {"id": "2106.01726", "submitter": "Ra\\'ul Nozal", "authors": "Ra\\'ul Nozal and Jose Luis Bosque", "title": "Exploiting co-execution with oneAPI: heterogeneity from a modern\n  perspective", "comments": "Accepted in Euro-Par 2021 (27th International Conference on Parallel\n  and Distributed Computing). 16 pages, 9 figures, 1 listing. Conference paper\n  - extended with API", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming efficiently heterogeneous systems is a major challenge, due to\nthe complexity of their architectures. Intel oneAPI, a new and powerful\nstandards-based unified programming model, built on top of SYCL, addresses\nthese issues. In this paper, oneAPI is provided with co-execution strategies to\nrun the same kernel between different devices, enabling the exploitation of\nstatic and dynamic policies. On top of that, static and dynamic load-balancing\nalgorithms are integrated and analyzed.\n  This work evaluates the performance and energy efficiency for a well-known\nset of regular and irregular HPC benchmarks, using an integrated GPU and CPU.\nExperimental results show that co-execution is worthwhile when using dynamic\nalgorithms, improving efficiency even more when using unified shared memory.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:56:01 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 20:24:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nozal", "Ra\u00fal", ""], ["Bosque", "Jose Luis", ""]]}, {"id": "2106.01880", "submitter": "Peter Davies", "authors": "Artur Czumaj, Peter Davies, Merav Parter", "title": "Component Stability in Low-Space Massively Parallel Computation", "comments": "45 pages, to appear at PODC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the power and limitations of component-stable algorithms in the\nlow-space model of Massively Parallel Computation (MPC). Recently Ghaffari,\nKuhn and Uitto (FOCS 2019) introduced the class of component-stable low-space\nMPC algorithms, which are, informally, defined as algorithms for which the\noutputs reported by the nodes in different connected components are required to\nbe independent. This very natural notion was introduced to capture most (if not\nall) of the known efficient MPC algorithms to date, and it was the first\ngeneral class of MPC algorithms for which one can show non-trivial conditional\nlower bounds. In this paper we enhance the framework of component-stable\nalgorithms and investigate its effect on the complexity of randomized and\ndeterministic low-space MPC. Our key contributions include:\n  1) We revise and formalize the lifting approach of Ghaffari, Kuhn and Uitto.\nThis requires a very delicate amendment of the notion of component stability,\nwhich allows us to fill in gaps in the earlier arguments.\n  2) We also extend the framework to obtain conditional lower bounds for\ndeterministic algorithms and fine-grained lower bounds that depend on the\nmaximum degree $\\Delta$.\n  3) We demonstrate a collection of natural graph problems for which\nnon-component-stable algorithms break the conditional lower bound obtained for\ncomponent-stable algorithms. This implies that, for both deterministic and\nrandomized algorithms, component-stable algorithms are conditionally weaker\nthan the non-component-stable ones.\n  Altogether our results imply that component-stability might limit the\ncomputational power of the low-space MPC model, at least in certain contexts,\npaving the way for improved upper bounds that escape the conditional lower\nbound setting of Ghaffari, Kuhn, and Uitto.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:25:46 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""], ["Parter", "Merav", ""]]}, {"id": "2106.01894", "submitter": "Merav Parter", "authors": "Shimon Kogan and Merav Parter", "title": "Low-Congestion Shortcuts in Constant Diameter Graphs", "comments": "To appear in PODC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low congestion shortcuts, introduced by Ghaffari and Haeupler (SODA 2016),\nprovide a unified framework for global optimization problems in the congest\nmodel of distributed computing. Roughly speaking, for a given graph $G$ and a\ncollection of vertex-disjoint connected subsets $S_1,\\ldots, S_\\ell \\subseteq\nV(G)$, $(c,d)$ low-congestion shortcuts augment each subgraph $G[S_i]$ with a\nsubgraph $H_i \\subseteq G$ such that: (i) each edge appears on at most $c$\nsubgraphs (congestion bound), and (ii) the diameter of each subgraph $G[S_i]\n\\cup H_i$ is bounded by $d$ (dilation bound). It is desirable to compute\nshortcuts of small congestion and dilation as these quantities capture the\nround complexity of many global optimization problems in the congest model. For\n$n$-vertex graphs with constant diameter $D=O(1)$, Elkin (STOC 2004) presented\nan (implicit) shortcuts lower bound with\n$c+d=\\widetilde{\\Omega}(n^{(D-2)/(2D-2)})$. A nearly matching upper bound,\nhowever, was only recently obtained for $D \\in \\{3,4\\}$ by Kitamura et al.\n(DISC 2019).\n  In this work, we resolve the long-standing complexity gap of shortcuts in\nconstant diameter graphs, originally posed by Lotker et al. (PODC 2001). We\npresent new shortcut constructions which match, up to poly-logarithmic terms,\nthe lower bounds of Das-Sarma et al. As a result, we provide improved and\nexistentially optimal algorithms for several network optimization tasks in\nconstant diameter graphs, including MST, $(1+\\epsilon)$-approximate minimum\ncuts and more.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:38:18 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 13:02:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kogan", "Shimon", ""], ["Parter", "Merav", ""]]}, {"id": "2106.02045", "submitter": "Marcel Leutenegger", "authors": "Marcel Leutenegger and Michael Weber", "title": "Least-squares fitting of Gaussian spots on graphics processing units", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA physics.ins-det", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The investigation of samples with a spatial resolution in the nanometer range\nrelies on the precise and stable positioning of the sample. Due to inherent\nmechanical instabilities of typical sample stages in optical microscopes, it is\nusually required to control and/or monitor the sample position during the\nacquisition. The tracking of sparsely distributed fiducial markers at high\nspeed allows stabilizing the sample position at millisecond time scales. For\nthis purpose, we present a scalable fitting algorithm with significantly\nimproved performance for two-dimensional Gaussian fits as compared to Gpufit.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:11:28 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Leutenegger", "Marcel", ""], ["Weber", "Michael", ""]]}, {"id": "2106.02066", "submitter": "Vaclav Rozhon", "authors": "Sebastian Brandt, Yi-Jun Chang, Jan Greb\\'ik, Christoph Grunau,\n  V\\'aclav Rozho\\v{n}, Zolt\\'an Vidny\\'anszky", "title": "Local Problems on Trees from the Perspectives of Distributed Algorithms,\n  Finitary Factors, and Descriptive Combinatorics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DC cs.DS math.LO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study connections between distributed local algorithms, finitary factors\nof iid processes, and descriptive combinatorics in the context of regular\ntrees.\n  We extend the Borel determinacy technique of Marks coming from descriptive\ncombinatorics and adapt it to the area of distributed computing. Using this\ntechnique, we prove deterministic distributed $\\Omega(\\log n)$-round lower\nbounds for problems from a natural class of homomorphism problems.\nInterestingly, these lower bounds seem beyond the current reach of the powerful\nround elimination technique responsible for all substantial locality lower\nbounds of the last years. Our key technical ingredient is a novel ID graph\ntechnique that we expect to be of independent interest.\n  We prove that a local problem admits a Baire measurable coloring if and only\nif it admits a local algorithm with local complexity $O(\\log n)$, extending the\nclassification of Baire measurable colorings of Bernshteyn. A key ingredient of\nthe proof is a new and simple characterization of local problems that can be\nsolved in $O(\\log n)$ rounds. We complement this result by showing separations\nbetween complexity classes from distributed computing, finitary factors, and\ndescriptive combinatorics. Most notably, the class of problems that allow a\ndistributed algorithm with sublogarithmic randomized local complexity is\nincomparable with the class of problems with a Borel solution.\n  We hope that our treatment will help to view all three perspectives as part\nof a common theory of locality, in which we follow the insightful paper of\n[Bernshteyn -- arXiv 2004.04905].\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 18:15:07 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Brandt", "Sebastian", ""], ["Chang", "Yi-Jun", ""], ["Greb\u00edk", "Jan", ""], ["Grunau", "Christoph", ""], ["Rozho\u0148", "V\u00e1clav", ""], ["Vidny\u00e1nszky", "Zolt\u00e1n", ""]]}, {"id": "2106.02305", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Zheng Xu, Zachary Garrett, Zachary Charles, Luyang Liu,\n  Gauri Joshi", "title": "Local Adaptivity in Federated Learning: Convergence and Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The federated learning (FL) framework trains a machine learning model using\ndecentralized data stored at edge client devices by periodically aggregating\nlocally trained models. Popular optimization algorithms of FL use vanilla\n(stochastic) gradient descent for both local updates at clients and global\nupdates at the aggregating server. Recently, adaptive optimization methods such\nas AdaGrad have been studied for server updates. However, the effect of using\nadaptive optimization methods for local updates at clients is not yet\nunderstood. We show in both theory and practice that while local adaptive\nmethods can accelerate convergence, they can cause a non-vanishing solution\nbias, where the final converged solution may be different from the stationary\npoint of the global objective function. We propose correction techniques to\novercome this inconsistency and complement the local adaptive methods for FL.\nExtensive experiments on realistic federated training tasks show that the\nproposed algorithms can achieve faster convergence and higher test accuracy\nthan the baselines without local adaptivity.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:36:59 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Jianyu", ""], ["Xu", "Zheng", ""], ["Garrett", "Zachary", ""], ["Charles", "Zachary", ""], ["Liu", "Luyang", ""], ["Joshi", "Gauri", ""]]}, {"id": "2106.02310", "submitter": "Sung Kuk Shyn", "authors": "Sung Kuk Shyn, Donghee Kim, and Kwangsu Kim", "title": "FedCCEA : A Practical Approach of Client Contribution Evaluation for\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Client contribution evaluation, also known as data valuation, is a crucial\napproach in federated learning(FL) for client selection and incentive\nallocation. However, due to restrictions of accessibility of raw data, only\nlimited information such as local weights and local data size of each client is\nopen for quantifying the client contribution. Using data size from available\ninformation, we introduce an empirical evaluation method called Federated\nClient Contribution Evaluation through Accuracy Approximation(FedCCEA). This\nmethod builds the Accuracy Approximation Model(AAM), which estimates a\nsimulated test accuracy using inputs of sampled data size and extracts the\nclients' data quality and data size to measure client contribution. FedCCEA\nstrengthens some advantages: (1) enablement of data size selection to the\nclients, (2) feasible evaluation time regardless of the number of clients, and\n(3) precise estimation in non-IID settings. We demonstrate the superiority of\nFedCCEA compared to previous methods through several experiments: client\ncontribution distribution, client removal, and robustness test to partial\nparticipation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:42:56 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Shyn", "Sung Kuk", ""], ["Kim", "Donghee", ""], ["Kim", "Kwangsu", ""]]}, {"id": "2106.02420", "submitter": "Emna Baccour", "authors": "Emna Baccour, Fatima Haouari, Aiman Erbad, Amr Mohamed, Kashif Bilal,\n  Mohsen Guizani, Mounir Hamdi", "title": "An Intelligent Resource Reservation for Crowdsourced Live Video\n  Streaming Applications in Geo-Distributed Cloud Environment", "comments": "Published in IEEE systems journal", "journal-ref": null, "doi": "10.1109/JSYST.2021.3077707", "report-no": null, "categories": "cs.NI cs.DC cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowdsourced live video streaming (livecast) services such as Facebook Live,\nYouNow, Douyu and Twitch are gaining more momentum recently. Allocating the\nlimited resources in a cost-effective manner while maximizing the Quality of\nService (QoS) through real-time delivery and the provision of the appropriate\nrepresentations for all viewers is a challenging problem. In our paper, we\nintroduce a machine-learning based predictive resource allocation framework for\ngeo-distributed cloud sites, considering the delay and quality constraints to\nguarantee the maximum QoS for viewers and the minimum cost for content\nproviders. First, we present an offline optimization that decides the required\ntranscoding resources in distributed regions near the viewers with a trade-off\nbetween the QoS and the overall cost. Second, we use machine learning to build\nforecasting models that proactively predict the approximate transcoding\nresources to be reserved at each cloud site ahead of time. Finally, we develop\na Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource\nallocation of real-time broadcasted videos on the rented resources. Extensive\nsimulations have shown that GNCA outperforms the state-of-the art resource\nallocation approaches for crowdsourced live streaming by achieving more than\n20% gain in terms of system cost while serving the viewers with relatively\nlower latency.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 11:45:09 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Baccour", "Emna", ""], ["Haouari", "Fatima", ""], ["Erbad", "Aiman", ""], ["Mohamed", "Amr", ""], ["Bilal", "Kashif", ""], ["Guizani", "Mohsen", ""], ["Hamdi", "Mounir", ""]]}, {"id": "2106.02440", "submitter": "Dennis Olivetti", "authors": "Alkida Balliu, Sebastian Brandt, Fabian Kuhn, Dennis Olivetti", "title": "Improved Distributed Lower Bounds for MIS and Bounded (Out-)Degree\n  Dominating Sets in Trees", "comments": "Accepted at PODC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Balliu, Brandt, and Olivetti [FOCS '20] showed the first\n$\\omega(\\log^* n)$ lower bound for the maximal independent set (MIS) problem in\ntrees. In this work we prove lower bounds for a much more relaxed family of\ndistributed symmetry breaking problems. As a by-product, we obtain improved\nlower bounds for the distributed MIS problem in trees.\n  For a parameter $k$ and an orientation of the edges of a graph $G$, we say\nthat a subset $S$ of the nodes of $G$ is a $k$-outdegree dominating set if $S$\nis a dominating set of $G$ and if in the induced subgraph $G[S]$, every node in\n$S$ has outdegree at most $k$. Note that for $k=0$, this definition coincides\nwith the definition of an MIS. For a given $k$, we consider the problem of\ncomputing a $k$-outdegree dominating set. We show that, even in regular trees\nof degree at most $\\Delta$, in the standard \\LOCAL model, there exists a\nconstant $\\epsilon>0$ such that for $k\\leq \\Delta^\\epsilon$, for the problem of\ncomputing a $k$-outdegree dominating set, any randomized algorithm requires at\nleast $\\Omega(\\min\\{\\log\\Delta,\\sqrt{\\log\\log n}\\})$ rounds and any\ndeterministic algorithm requires at least $\\Omega(\\min\\{\\log\\Delta,\\sqrt{\\log\nn}\\})$ rounds.\n  The proof of our lower bounds is based on the recently highly successful\nround elimination technique. We provide a novel way to do simplifications for\nround elimination, which we expect to be of independent interest. Our new proof\nis considerably simpler than the lower bound proof in [FOCS '20]. In\nparticular, our round elimination proof uses a family of problems that can be\ndescribed by only a constant number of labels. The existence of such a proof\nfor the MIS problem was believed impossible by the authors of [FOCS '20].\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 12:39:37 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Kuhn", "Fabian", ""], ["Olivetti", "Dennis", ""]]}, {"id": "2106.02529", "submitter": "Hao Wang", "authors": "Qing Yang, Hao Wang, Xiaoxiao Wu, Taotao Wang, Shengli Zhang", "title": "Blockchain for Transactive Energy Management of Distributed Energy\n  Resources in Smart Grid", "comments": "ACM International Conference on Future Energy Systems (ACM e-Energy)\n  2021. arXiv admin note: text overlap with arXiv:2105.00174", "journal-ref": null, "doi": "10.1145/3447555.3464848", "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents the design and implementation of a blockchain system that\nenables the trustable transactive energy management for distributed energy\nresources (DERs). We model the interactions among DERs, including energy\ntrading and flexible appliance scheduling, as a cost minimization problem.\nConsidering the dispersed nature and diverse ownership of DERs, we develop a\ndistributed algorithm to solve the optimization problem using the alternating\ndirection method of multipliers (ADMM) method. Furthermore, we develop a\nblockchain system, on which we implement the proposed algorithm with the smart\ncontract, to guarantee the transparency and correctness of the energy\nmanagement. We prototype the blockchain in a small-scale test network and\nevaluate it through experiments using real-world data. The experimental results\nvalidate the feasibility and effectiveness of our design.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:55:20 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yang", "Qing", ""], ["Wang", "Hao", ""], ["Wu", "Xiaoxiao", ""], ["Wang", "Taotao", ""], ["Zhang", "Shengli", ""]]}, {"id": "2106.02612", "submitter": "Tommaso Diotalevi", "authors": "Tommaso Diotalevi, Antonio Falabella, Barbara Martelli, Diego\n  Michelotto, Lucia Morganti, Daniele Bonacorsi, Luca Giommi, Simone Rossi\n  Tisbeni", "title": "Collection and harmonization of system logs and prototypal Analytics\n  services with the Elastic (ELK) suite at the INFN-CNAF computing centre", "comments": "Submitted to proceedings of International Symposium on Grids & Clouds\n  2019 (ISGC2019)", "journal-ref": "PoS(ISGC2019)027", "doi": "10.22323/1.351.0027", "report-no": null, "categories": "cs.DC cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The distributed Grid infrastructure for High Energy Physics experiments at\nthe Large Hadron Collider (LHC) in Geneva comprises a set of computing centres,\nspread all over the world, as part of the Worldwide LHC Computing Grid (WLCG).\nIn Italy, the Tier-1 functionalities are served by the INFN-CNAF data center,\nwhich provides also computing and storage resources to more than twenty non-LHC\nexperiments. For this reason, a high amount of logs are collected each day from\nvarious sources, which are highly heterogeneous and difficult to harmonize. In\nthis contribution, a working implementation of a system that collects, parses\nand displays the log information from CNAF data sources and the investigation\nof a Machine Learning based predictive maintenance system, is presented.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 10:21:55 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Diotalevi", "Tommaso", ""], ["Falabella", "Antonio", ""], ["Martelli", "Barbara", ""], ["Michelotto", "Diego", ""], ["Morganti", "Lucia", ""], ["Bonacorsi", "Daniele", ""], ["Giommi", "Luca", ""], ["Tisbeni", "Simone Rossi", ""]]}, {"id": "2106.02679", "submitter": "Joel Lamy-Poirier", "authors": "Joel Lamy-Poirier", "title": "Layered gradient accumulation and modular pipeline parallelism: fast and\n  efficient training of large language models", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The advent of the transformer has sparked a quick growth in the size of\nlanguage models, far outpacing hardware improvements. (Dense) transformers are\nexpected to reach the trillion-parameter scale in the near future, for which\ntraining requires thousands or even tens of thousands of GPUs. We investigate\nthe challenges of training at this scale and beyond on commercially available\nhardware. In particular, we analyse the shortest possible training time for\ndifferent configurations of distributed training, leveraging empirical scaling\nlaws for language models to estimate the optimal (critical) batch size.\nContrary to popular belief, we find no evidence for a memory wall, and instead\nargue that the real limitation -- other than the cost -- lies in the training\nduration.\n  In addition to this analysis, we introduce two new methods, \\textit{layered\ngradient accumulation} and \\textit{modular pipeline parallelism}, which\ntogether cut the shortest training time by half. The methods also reduce data\nmovement, lowering the network requirement to a point where a fast InfiniBand\nconnection is not necessary. This increased network efficiency also improve on\nthe methods introduced with the ZeRO optimizer, reducing the memory usage to a\ntiny fraction of the available GPU memory.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 19:21:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lamy-Poirier", "Joel", ""]]}, {"id": "2106.02942", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad", "title": "Time-Optimal Sublinear Algorithms for Matching and Vertex Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a near-tight analysis of the average \"query complexity\" -- \\`a la\nNguyen and Onak [FOCS'08] -- of the randomized greedy maximal matching\nalgorithm, improving over the bound of Yoshida, Yamamoto and Ito [STOC'09]. For\nany $n$-vertex graph of average degree $\\bar{d}$, this leads to the following\nsublinear-time algorithms for estimating the size of maximum matching and\nminimum vertex cover, all of which are provably time-optimal up to logarithmic\nfactors:\n  $\\bullet$ A multiplicative $(2+\\epsilon)$-approximation in\n$\\widetilde{O}(n/\\epsilon^2)$ time using adjacency list queries. This (nearly)\nmatches an $\\Omega(n)$ time lower bound for any multiplicative approximation\nand is, notably, the first $O(1)$-approximation that runs in $o(n^{1.5})$ time.\n  $\\bullet$ A $(2, \\epsilon n)$-approximation in $\\widetilde{O}((\\bar{d} +\n1)/\\epsilon^2)$ time using adjacency list queries. This (nearly) matches an\n$\\Omega(\\bar{d}+1)$ lower bound of Parnas and Ron [TCS'07] which holds for any\n$(O(1), \\epsilon n)$-approximation, and improves over the bounds of [Yoshida et\nal. STOC'09; Onak et al. SODA'12] and [Kapralov et al. SODA'20]: The former two\ntake at least quadratic time in the degree which can be as large as\n$\\Omega(n^2)$ and the latter obtains a much larger approximation.\n  $\\bullet$ A $(2, \\epsilon n)$-approximation in $\\widetilde{O}(n/\\epsilon^3)$\ntime using adjacency matrix queries. This (nearly) matches an $\\Omega(n)$ time\nlower bound in this model and improves over the $\\widetilde{O}(n\\sqrt{n})$-time\n$(2, \\epsilon n)$-approximate algorithm of [Chen, Kannan, and Khanna ICALP'20].\nIt also turns out that any non-trivial multiplicative approximation in the\nadjacency matrix model requires $\\Omega(n^2)$ time, so the additive $\\epsilon\nn$ error is necessary too.\n  As immediate corollaries, we get improved sublinear time estimators for\n(variants of) TSP and an improved AMPC algorithm for maximal matching.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 18:41:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Behnezhad", "Soheil", ""]]}, {"id": "2106.02956", "submitter": "Vipin Rathi", "authors": "Parth Yadav, Vipin Kumar Rathi", "title": "KupenStack: Kubernetes based Cloud Native OpenStack", "comments": "11 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  OpenStack is an open-source private cloud used to run VMs and its related\ncloud services. OpenStack deployment, management, and upgradation require lots\nof efforts and manual troubleshooting. Also, workloads and services offered by\nOpenStack cannot self-heal itself on failures. We present KupenStack, a\nCloud-Native OpenStack as Code model built on top of Kubernetes stack as Custom\nResources. KupenStack is a controller that interacts between Kubernetes and\nOpenStack and automates complex operations like scaling, LCM, zero-downtime,\nself-healing, version upgrades, configuration management, and offers OpenStack\nas a service through code. KupenStack builds cloud-native values like immutable\ninfrastructure, declarative APIs for OpenStack without changing any OpenStack\ncode. If a VM workload goes down for some reason, then KupenStack handles it\nand automatically spins up a new instance. KupenStack uses OpenStack on\nKubernetes deployment for lifecycle management of OpenStack.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 20:02:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yadav", "Parth", ""], ["Rathi", "Vipin Kumar", ""]]}, {"id": "2106.02969", "submitter": "Peter Richt\\'arik", "authors": "Mher Safaryan and Rustem Islamov and Xun Qian and Peter Richt\\'arik", "title": "FedNL: Making Newton-Type Methods Applicable to Federated Learning", "comments": "63 pages, 7 algorithms, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work of Islamov et al (2021), we propose a family of\nFederated Newton Learn (FedNL) methods, which we believe is a marked step in\nthe direction of making second-order methods applicable to FL. In contrast to\nthe aforementioned work, FedNL employs a different Hessian learning technique\nwhich i) enhances privacy as it does not rely on the training data to be\nrevealed to the coordinating server, ii) makes it applicable beyond generalized\nlinear models, and iii) provably works with general contractive compression\noperators for compressing the local Hessians, such as Top-$K$ or Rank-$R$,\nwhich are vastly superior in practice. Notably, we do not need to rely on error\nfeedback for our methods to work with contractive compressors. Moreover, we\ndevelop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that\nsupport partial participation, and globalization via cubic regularization and\nline search, respectively, and FedNL-BC, which is a variant that can further\nbenefit from bidirectional compression of gradients and models, i.e., smart\nuplink gradient and smart downlink model compression. We prove local\nconvergence rates that are independent of the condition number, the number of\ntraining data points, and compression variance. Our communication efficient\nHessian learning technique provably learns the Hessian at the optimum. Finally,\nwe perform a variety of numerical experiments that show that our FedNL methods\nhave state-of-the-art communication complexity when compared to key baselines.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 21:30:11 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Safaryan", "Mher", ""], ["Islamov", "Rustem", ""], ["Qian", "Xun", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2106.03122", "submitter": "Yizheng Huang", "authors": "Yizheng Huang, Huaizheng Zhang, Yonggang Wen, Peng Sun, Nguyen Binh\n  Duong TA", "title": "ModelCI-e: Enabling Continual Learning in Deep Learning Serving Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLOps is about taking experimental ML models to production, i.e., serving the\nmodels to actual users. Unfortunately, existing ML serving systems do not\nadequately handle the dynamic environments in which online data diverges from\noffline training data, resulting in tedious model updating and deployment\nworks. This paper implements a lightweight MLOps plugin, termed ModelCI-e\n(continuous integration and evolution), to address the issue. Specifically, it\nembraces continual learning (CL) and ML deployment techniques, providing\nend-to-end supports for model updating and validation without serving engine\ncustomization. ModelCI-e includes 1) a model factory that allows CL researchers\nto prototype and benchmark CL models with ease, 2) a CL backend to automate and\norchestrate the model updating efficiently, and 3) a web interface for an ML\nteam to manage CL service collaboratively. Our preliminary results demonstrate\nthe usability of ModelCI-e, and indicate that eliminating the interference\nbetween model updating and inference workloads is crucial for higher system\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 13:28:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Huang", "Yizheng", ""], ["Zhang", "Huaizheng", ""], ["Wen", "Yonggang", ""], ["Sun", "Peng", ""], ["TA", "Nguyen Binh Duong", ""]]}, {"id": "2106.03185", "submitter": "David Chan", "authors": "David Yu Cheng Chan and Philipp Woelfel", "title": "Tight Lower Bounds for the RMR Complexity of Recoverable Mutual\n  Exclusion", "comments": "36 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a tight RMR complexity lower bound for the recoverable mutual\nexclusion (RME) problem, defined by Golab and Ramaraju \\cite{GR2019a}. In\nparticular, we show that any $n$-process RME algorithm using only atomic read,\nwrite, fetch-and-store, fetch-and-increment, and compare-and-swap operations,\nhas an RMR complexity of $\\Omega(\\log n/\\log\\log n)$ on the CC and DSM model.\nThis lower bound covers all realistic synchronization primitives that have been\nused in RME algorithms and matches the best upper bounds of algorithms\nemploying swap objects (e.g., [5,6,10]).\n  Algorithms with better RMR complexity than that have only been obtained by\neither (i) assuming that all failures are system-wide [7], (ii) employing\nfetch-and-add objects of size $(\\log n)^{\\omega(1)}$ [12], or (iii) using\nartificially defined synchronization primitives that are not available in\nactual systems [6,9].\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 17:24:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chan", "David Yu Cheng", ""], ["Woelfel", "Philipp", ""]]}, {"id": "2106.03211", "submitter": "Nhuong Nguyen", "authors": "Nhuong V. Nguyen and Sybille Legitime", "title": "Distributed Learning and its Application for Time-Series Prediction", "comments": "8 pages, 10 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extreme events are occurrences whose magnitude and potential cause extensive\ndamage on people, infrastructure, and the environment. Motivated by the extreme\nnature of the current global health landscape, which is plagued by the\ncoronavirus pandemic, we seek to better understand and model extreme events.\nModeling extreme events is common in practice and plays an important role in\ntime-series prediction applications. Our goal is to (i) compare and investigate\nthe effect of some common extreme events modeling methods to explore which\nmethod can be practical in reality and (ii) accelerate the deep learning\ntraining process, which commonly uses deep recurrent neural network (RNN), by\nimplementing the asynchronous local Stochastic Gradient Descent (SGD) framework\namong multiple compute nodes. In order to verify our distributed extreme events\nmodeling, we evaluate our proposed framework on a stock data set S\\&P500, with\na standard recurrent neural network. Our intuition is to explore the (best)\nextreme events modeling method which could work well under the distributed deep\nlearning setting. Moreover, by using asynchronous distributed learning, we aim\nto significantly reduce the communication cost among the compute nodes and\ncentral server, which is the main bottleneck of almost all distributed learning\nframeworks.\n  We implement our proposed work and evaluate its performance on representative\ndata sets, such as S&P500 stock in $5$-year period. The experimental results\nvalidate the correctness of the design principle and show a significant\ntraining duration reduction upto $8$x, compared to the baseline single compute\nnode. Our results also show that our proposed work can achieve the same level\nof test accuracy, compared to the baseline setting.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 18:57:30 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 22:04:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Nguyen", "Nhuong V.", ""], ["Legitime", "Sybille", ""]]}, {"id": "2106.03219", "submitter": "Shilei Tian", "authors": "Shilei Tian and Jon Chesterfield and Johannes Doerfert and Barbara\n  Chapman", "title": "Experience Report: Writing A Portable GPU Runtime with OpenMP 5.1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  GPU runtimes are historically implemented in CUDA or other vendor specific\nlanguages dedicated to GPU programming. In this work we show that OpenMP 5.1,\nwith minor compiler extensions, is capable of replacing existing solutions\nwithout a performance penalty. The result is a performant and portable GPU\nruntime that can be compiled with LLVM/Clang to Nvidia and AMD GPUs without the\nneed for CUDA or HIP during its development and compilation.\n  While we tried to be OpenMP compliant, we identified the need for compiler\nextensions to achieve the CUDA performance with our OpenMP runtime. We hope\nthat future versions of OpenMP adopt our extensions to make device programming\nin OpenMP also portable across compilers, not only across execution platforms.\n  The library we ported to OpenMP is the OpenMP device runtime that provides\nOpenMP functionality on the GPU. This work opens the door for shipping OpenMP\noffloading with a Linux distribution's LLVM package as the package manager\nwould not need a vendor SDK to build the compiler and runtimes. Furthermore,\nour OpenMP device runtime can support a new GPU target through the use of a few\ncompiler intrinsics rather than requiring a reimplementation of the entire\nruntime.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 19:40:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tian", "Shilei", ""], ["Chesterfield", "Jon", ""], ["Doerfert", "Johannes", ""], ["Chapman", "Barbara", ""]]}, {"id": "2106.03328", "submitter": "Ramy E. Ali", "authors": "Jinhyun So, Ramy E. Ali, Basak Guler, Jiantao Jiao, Salman Avestimehr", "title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure aggregation is a critical component in federated learning, which\nenables the server to learn the aggregate model of the users without observing\ntheir local models. Conventionally, secure aggregation algorithms focus only on\nensuring the privacy of individual users in a single training round. We contend\nthat such designs can lead to significant privacy leakages over multiple\ntraining rounds, due to partial user selection/participation at each round of\nfederated learning. In fact, we empirically show that the conventional random\nuser selection strategies for federated learning lead to leaking users'\nindividual models within number of rounds linear in the number of users. To\naddress this challenge, we introduce a secure aggregation framework with\nmulti-round privacy guarantees. In particular, we introduce a new metric to\nquantify the privacy guarantees of federated learning over multiple training\nrounds, and develop a structured user selection strategy that guarantees the\nlong-term privacy of each user (over any number of training rounds). Our\nframework also carefully accounts for the fairness and the average number of\nparticipating users at each round. We perform several experiments on MNIST and\nCIFAR-10 datasets in the IID and the non-IID settings to demonstrate the\nperformance improvement over the baseline algorithms, both in terms of privacy\nprotection and test accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 04:14:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["So", "Jinhyun", ""], ["Ali", "Ramy E.", ""], ["Guler", "Basak", ""], ["Jiao", "Jiantao", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2106.03539", "submitter": "Heinz Schmidt", "authors": "Heinz W. Schmidt", "title": "How to Bake Quantum into Your Pet Petri Nets and Have Your Net Theory\n  Too", "comments": "24 pages incl. supplementary material in appendix. Accepted for the\n  15th Symposium and Summer School On Service-Oriented Computing (Submitted 2\n  April 2021, https://www.summersoc.eu). Final revised and authenticated\n  version to appear in Springer CCIS (https://www.springer.com/series/7899)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Petri nets have found widespread use among many application domains, not\nleast due to their human-friendly graphical syntax for the composition of\ninteracting distributed and asynchronous processes and services, based in\npartial-order dependencies and concurrent executions. Petri nets also come with\nabstract semantics, and mathematical methods for compositional synthesis,\nstructural checks and behavioural analysis. These have led to the use of\nvarious kinds of nets for real-time, distributed and parallel programming\nlanguages, software and services systems, with a view to their interfaces and\ninteraction protocols. These affordances make Petri nets invaluable for\ndistributed software architecture approaches focused on components, their\nmutual dependencies and environment-facing interactions. Quantum computing --\nand in particular quantum software engineering -- is in its infancy and could\nbenefit from the accumulated insights of software architecture research and of\nnet theory, its methods, and its applications. In this paper, we establish a\nconnection between Petri nets and quantum systems, such that net theory and the\ncomponent architecture of nets may help in the synthesis and analysis of\nabstract software models and their interface protocols in hybrid\nclassical-and-quantum programming languages and services systems. We leverage\nsome insights from net formalisms for software specification for a versatile\nrecipe to bake quantum into extant Petri net flavours, and prove universality\nand compositionality of Petri nets for quantum programming.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:06:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Schmidt", "Heinz W.", ""]]}, {"id": "2106.03601", "submitter": "Junfeng Li", "authors": "Junfeng Li, Sameer G. Kulkarni, K. K. Ramakrishnan, Dan Li", "title": "Analyzing Open-Source Serverless Platforms: Characteristics and\n  Performance", "comments": null, "journal-ref": null, "doi": "10.18293/SEKE2021-129", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is increasingly popular because of its lower cost and\neasier deployment. Several cloud service providers (CSPs) offer serverless\ncomputing on their public clouds, but it may bring the vendor lock-in risk. To\navoid this limitation, many open-source serverless platforms come out to allow\ndevelopers to freely deploy and manage functions on self-hosted clouds.\nHowever, building effective functions requires much expertise and thorough\ncomprehension of platform frameworks and features that affect performance. It\nis a challenge for a service developer to differentiate and select the\nappropriate serverless platform for different demands and scenarios. Thus, we\nelaborate the frameworks and event processing models of four popular\nopen-source serverless platforms and identify their salient idiosyncrasies. We\nanalyze the root causes of performance differences between different service\nexporting and auto-scaling modes on those platforms. Further, we provide\nseveral insights for future work, such as auto-scaling and metric collection.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:16:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Junfeng", ""], ["Kulkarni", "Sameer G.", ""], ["Ramakrishnan", "K. K.", ""], ["Li", "Dan", ""]]}, {"id": "2106.03617", "submitter": "Ricardo Macedo", "authors": "Ricardo Macedo, Yusuke Tanimura, Jason Haga, Vijay Chidambaram, Jos\\'e\n  Pereira, Jo\\~ao Paulo", "title": "PAIO: A Software-Defined Storage Data Plane Framework", "comments": "15 pages, 8 figures. Submitted to IEEE Transactions on Parallel and\n  Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PAIO, the first general-purpose framework that enables system\ndesigners to build custom-made Software-Defined Storage (SDS) data plane\nstages. It provides the means to implement storage optimizations adaptable to\ndifferent workflows and user-defined policies, and allows straightforward\nintegration with existing applications and I/O layers. PAIO allows stages to be\nintegrated with modern SDS control planes to ensure holistic control and\nsystem-wide optimal performance. We demonstrate the performance and\napplicability of PAIO with two use cases. The first improves 99th percentile\nlatency by 4x in industry-standard LSM-based key-value stores. The second\nensures dynamic per-application bandwidth guarantees under shared storage\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:43:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:00:27 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Macedo", "Ricardo", ""], ["Tanimura", "Yusuke", ""], ["Haga", "Jason", ""], ["Chidambaram", "Vijay", ""], ["Pereira", "Jos\u00e9", ""], ["Paulo", "Jo\u00e3o", ""]]}, {"id": "2106.03727", "submitter": "Royson Lee", "authors": "Royson Lee, Stylianos I. Venieris, Nicholas D. Lane", "title": "Deep Neural Network-based Enhancement for Image and Video Streaming\n  Systems: A Survey and Future Directions", "comments": "Accepted for publication at the ACM Computing Surveys (CSUR) journal,\n  2021. arXiv admin note: text overlap with arXiv:2010.05838", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-enabled smartphones and ultra-wide displays are transforming a\nvariety of visual apps spanning from on-demand movies and 360{\\deg} videos to\nvideo-conferencing and live streaming. However, robustly delivering visual\ncontent under fluctuating networking conditions on devices of diverse\ncapabilities remains an open problem. In recent years, advances in the field of\ndeep learning on tasks such as super-resolution and image enhancement have led\nto unprecedented performance in generating high-quality images from low-quality\nones, a process we refer to as neural enhancement. In this paper, we survey\nstate-of-the-art content delivery systems that employ neural enhancement as a\nkey component in achieving both fast response time and high visual quality. We\nfirst present the components and architecture of existing content delivery\nsystems, highlighting their challenges and motivating the use of neural\nenhancement models as a countermeasure. We then cover the deployment challenges\nof these models and analyze existing systems and their design decisions in\nefficiently overcoming these technical challenges. Additionally, we underline\nthe key trends and common approaches across systems that target diverse\nuse-cases. Finally, we present promising future directions based on the latest\ninsights from deep learning research to further boost the quality of experience\nof content delivery systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:42:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lee", "Royson", ""], ["Venieris", "Stylianos I.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2106.03753", "submitter": "Ny Aina Andriambolamalala", "authors": "Ny Aina Andriambolamalala and Vlady Ravelomanana", "title": "Energy-Efficient Naming in Beeping Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A single-hop beeping network is a distributed communication model in which\nall stations can communicate with one another by transmitting only one-bit\nmessages, called beeps. This paper focuses on resolving the distributed\ncomputing area's two fundamental problems: naming and counting problems. We are\nparticularly interested in optimizing the energy complexity and the running\ntime of algorithms to resolve these problems. Our contribution is to design\nrandomized algorithms with an optimal running time of O(n log n) and an energy\ncomplexity of O(log n) for both the naming and counting problems on single-hop\nbeeping networks of n stations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:18:19 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:42:49 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Andriambolamalala", "Ny Aina", ""], ["Ravelomanana", "Vlady", ""]]}, {"id": "2106.03824", "submitter": "Quanquan C. Liu", "authors": "Quanquan C. Liu, Jessica Shi, Shangdi Yu, Laxman Dhulipala, Julian\n  Shun", "title": "Parallel Batch-Dynamic $k$-Core Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining a $k$-core decomposition quickly in a dynamic graph is an\nimportant problem in many applications, including social network analytics,\ngraph visualization, centrality measure computations, and community detection\nalgorithms. The main challenge for designing efficient $k$-core decomposition\nalgorithms is that a single change to the graph can cause the decomposition to\nchange significantly.\n  We present the first parallel batch-dynamic algorithm for maintaining an\napproximate $k$-core decomposition that is efficient in both theory and\npractice. Given an initial graph with $m$ edges, and a batch of $B$ updates,\nour algorithm maintains a $(2 + \\delta)$-approximation of the coreness values\nfor all vertices (for any constant $\\delta > 0$) in $O(B\\log^2 m)$ amortized\nwork and $O(\\log^2 m \\log\\log m)$ depth (parallel time) with high probability.\nOur algorithm also maintains a low out-degree orientation of the graph in the\nsame bounds. We implemented and experimentally evaluated our algorithm on a\n30-core machine with two-way hyper-threading on $11$ graphs of varying\ndensities and sizes. Compared to the state-of-the-art algorithms, our algorithm\nachieves up to a 114.52x speedup against the best multicore implementation and\nup to a 497.63x speedup against the best sequential algorithm, obtaining\nresults for graphs that are orders-of-magnitude larger than those used in\nprevious studies.\n  In addition, we present the first approximate static $k$-core algorithm with\nlinear work and polylogarithmic depth. We show that on a 30-core machine with\ntwo-way hyper-threading, our implementation achieves up to a 3.9x speedup in\nthe static case over the previous state-of-the-art parallel algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:45:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Quanquan C.", ""], ["Shi", "Jessica", ""], ["Yu", "Shangdi", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2106.03943", "submitter": "Ajay Kshemkalyani", "authors": "Ajay D. Kshemkalyani and Gokarna Sharma", "title": "Near-Optimal Dispersion on Arbitrary Anonymous Graphs", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected, anonymous, port-labeled graph of $n$ memory-less nodes,\n$m$ edges, and degree $\\Delta$, we consider the problem of dispersing $k\\leq n$\nrobots (or tokens) positioned initially arbitrarily on one or more nodes of the\ngraph to exactly $k$ different nodes of the graph, one on each node. The\nobjective is to simultaneously minimize time to achieve dispersion and memory\nrequirement at each robot. If all $k$ robots are positioned initially on a\nsingle node, depth first search (DFS) traversal solves this problem in\n$O(\\min\\{m,k\\Delta\\})$ time with $\\Theta(\\log(k+\\Delta))$ bits at each robot.\nHowever, if robots are positioned initially on multiple nodes, the best\npreviously known algorithm solves this problem in $O(\\min\\{m,k\\Delta\\}\\cdot\n\\log \\ell)$ time storing $\\Theta(\\log(k+\\Delta))$ bits at each robot, where\n$\\ell\\leq k/2$ is the number of multiplicity nodes in the initial\nconfiguration. In this paper, we present a novel multi-source DFS traversal\nalgorithm solving this problem in $O(\\min\\{m,k\\Delta\\})$ time with\n$\\Theta(\\log(k+\\Delta))$ bits at each robot, improving the time bound of the\nbest previously known algorithm by $O(\\log \\ell)$ and matching asymptotically\nthe single-source DFS traversal bounds. This is the first algorithm for\ndispersion that is optimal in both time and memory in arbitrary anonymous\ngraphs of constant degree, $\\Delta=O(1)$. Furthermore, the result holds in both\nsynchronous and asynchronous settings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:08:48 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kshemkalyani", "Ajay D.", ""], ["Sharma", "Gokarna", ""]]}, {"id": "2106.03998", "submitter": "Shuo Liu", "authors": "Shuo Liu, Nirupam Gupta, Nitin H. Vaidya", "title": "Asynchronous Distributed Optimization with Redundancy in Cost Functions", "comments": "37 pages, 4 figures. Related to our prior work on Byzantine\n  fault-tolerance distributed optimization in redundancy in cost functions\n  (doi:10.1145/3382734.3405748 and arXiv:2101.09337)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of asynchronous distributed multi-agent\noptimization on server-based system architecture. In this problem, each agent\nhas a local cost, and the goal for the agents is to collectively find a minimum\nof their aggregate cost. A standard algorithm to solve this problem is the\niterative distributed gradient-descent (DGD) method being implemented\ncollaboratively by the server and the agents. In the synchronous setting, the\nalgorithm proceeds from one iteration to the next only after all the agents\ncomplete their expected communication with the server. However, such synchrony\ncan be expensive and even infeasible in real-world applications. We show that\nwaiting for all the agents is unnecessary in many applications of distributed\noptimization, including distributed machine learning, due to redundancy in the\ncost functions (or {\\em data}). Specifically, we consider a generic notion of\nredundancy named $(r,\\epsilon)$-redundancy implying solvability of the original\nmulti-agent optimization problem with $\\epsilon$ accuracy, despite the removal\nof up to $r$ (out of total $n$) agents from the system. We present an\nasynchronous DGD algorithm where in each iteration the server only waits for\n(any) $n-r$ agents, instead of all the $n$ agents. Assuming\n$(r,\\epsilon)$-redundancy, we show that our asynchronous algorithm converges to\nan approximate solution with error that is linear in $\\epsilon$ and $r$.\nMoreover, we also present a generalization of our algorithm to tolerate some\nByzantine faulty agents in the system. Finally, we demonstrate the improved\ncommunication efficiency of our algorithm through experiments on MNIST and\nFashion-MNIST using the benchmark neural network LeNet.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 22:40:18 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Shuo", ""], ["Gupta", "Nirupam", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2106.04122", "submitter": "Minghui Xu", "authors": "Minghui Xu, Shuo Liu, Dongxiao Yu, Xiuzhen Cheng, Shaoyong Guo, Jiguo\n  Yu", "title": "CloudChain: A Cloud Blockchain Using Shared Memory Consensus and RDMA", "comments": "12 pages, 8 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technologies can enable secure computing environments among\nmistrusting parties. Permissioned blockchains are particularly enlightened by\ncompanies, enterprises, and government agencies due to their efficiency,\ncustomizability, and governance-friendly features. Obviously, seamlessly fusing\nblockchain and cloud computing can significantly benefit permissioned\nblockchains; nevertheless, most blockchains implemented on clouds are\noriginally designed for loosely-coupled networks where nodes communicate\nasynchronously, failing to take advantages of the closely-coupled nature of\ncloud servers. In this paper, we propose an innovative cloud-oriented\nblockchain -- CloudChain, which is a modularized three-layer system composed of\nthe network layer, consensus layer, and blockchain layer. CloudChain is based\non a shared-memory model where nodes communicate synchronously by direct memory\naccesses. We realize the shared-memory model with the Remote Direct Memory\nAccess technology, based on which we propose a shared-memory consensus\nalgorithm to ensure presistence and liveness, the two crucial blockchain\nsecurity properties countering Byzantine nodes. We also implement a CloudChain\nprototype based on a RoCEv2-based testbed to experimentally validate our\ndesign, and the results verify the feasibility and efficiency of CloudChain.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:15:35 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xu", "Minghui", ""], ["Liu", "Shuo", ""], ["Yu", "Dongxiao", ""], ["Cheng", "Xiuzhen", ""], ["Guo", "Shaoyong", ""], ["Yu", "Jiguo", ""]]}, {"id": "2106.04502", "submitter": "Mikhail Khodak", "authors": "Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina Balcan,\n  Virginia Smith, Ameet Talwalkar", "title": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections\n  to Weight-Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning hyperparameters is a crucial but arduous part of the machine learning\npipeline. Hyperparameter optimization is even more challenging in federated\nlearning, where models are learned over a distributed network of heterogeneous\ndevices; here, the need to keep data on device and perform local training makes\nit difficult to efficiently train and evaluate configurations. In this work, we\ninvestigate the problem of federated hyperparameter tuning. We first identify\nkey challenges and show how standard approaches may be adapted to form\nbaselines for the federated setting. Then, by making a novel connection to the\nneural architecture search technique of weight-sharing, we introduce a new\nmethod, FedEx, to accelerate federated hyperparameter tuning that is applicable\nto widely-used federated optimization methods such as FedAvg and recent\nvariants. Theoretically, we show that a FedEx variant correctly tunes the\non-device learning rate in the setting of online convex optimization across\ndevices. Empirically, we show that FedEx can outperform natural baselines for\nfederated hyperparameter tuning by several percentage points on the\nShakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using\nthe same training budget.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:42:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Khodak", "Mikhail", ""], ["Tu", "Renbo", ""], ["Li", "Tian", ""], ["Li", "Liam", ""], ["Balcan", "Maria-Florina", ""], ["Smith", "Virginia", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "2106.04514", "submitter": "Pei Zhang", "authors": "Kaiwen Long, Chong Xing, Yuebin Qi, Pei Zhang, Changsong Wu, Wenxiao\n  Fang, Jing Tan, Jie Chen, Shiming Zhang, Zuosheng Wang, Zuanmin Liu, Cao\n  Liang, Jiaxiang Xu", "title": "GearV: A Two-Gear Hypervisor for Mixed-Criticality IoT Systems", "comments": "12 pages, 8 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents GearV, a two-gear lightweight hypervisor architecture to\naddress the some known challenges. By dividing hypervisor into some partitions,\nand dividing scheduling policies into Gear1 and Gear2 respectively, GearV\ncreates a consolidated platform to run best-effort system and safety-critical\nsystem simultaneously with managed engineering effort. The two-gears\narchitecture also simplifies retrofitting the virtualization systems. We\nbelieve that GearV can serves as a reasonable hypervisor architecture for the\nmix-critical IoT systems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 14:06:20 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Long", "Kaiwen", ""], ["Xing", "Chong", ""], ["Qi", "Yuebin", ""], ["Zhang", "Pei", ""], ["Wu", "Changsong", ""], ["Fang", "Wenxiao", ""], ["Tan", "Jing", ""], ["Chen", "Jie", ""], ["Zhang", "Shiming", ""], ["Wang", "Zuosheng", ""], ["Liu", "Zuanmin", ""], ["Liang", "Cao", ""], ["Xu", "Jiaxiang", ""]]}, {"id": "2106.04516", "submitter": "Matthew W. Hoffman", "authors": "Fan Yang, Gabriel Barth-Maron, Piotr Sta\\'nczyk, Matthew Hoffman, Siqi\n  Liu, Manuel Kroiss, Aedan Pope, Alban Rrustemi", "title": "Launchpad: A Programming Model for Distributed Machine Learning Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major driver behind the success of modern machine learning algorithms has\nbeen their ability to process ever-larger amounts of data. As a result, the use\nof distributed systems in both research and production has become increasingly\nprevalent as a means to scale to this growing data. At the same time, however,\ndistributing the learning process can drastically complicate the implementation\nof even simple algorithms. This is especially problematic as many machine\nlearning practitioners are not well-versed in the design of distributed\nsystems, let alone those that have complicated communication topologies. In\nthis work we introduce Launchpad, a programming model that simplifies the\nprocess of defining and launching distributed systems that is specifically\ntailored towards a machine learning audience. We describe our framework, its\ndesign philosophy and implementation, and give a number of examples of common\nlearning algorithms whose designs are greatly simplified by this approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:02:10 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Yang", "Fan", ""], ["Barth-Maron", "Gabriel", ""], ["Sta\u0144czyk", "Piotr", ""], ["Hoffman", "Matthew", ""], ["Liu", "Siqi", ""], ["Kroiss", "Manuel", ""], ["Pope", "Aedan", ""], ["Rrustemi", "Alban", ""]]}, {"id": "2106.04517", "submitter": "Michael Gundall", "authors": "Michael Gundall, Hans Dieter Schotten", "title": "Assessing Open Interfaces and Protocols of PLCs for Computation\n  Offloading at Field Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable logic controllers (PLCs) are the core element of industrial\nplants in todays deployments. They read sensor values, execute control\nalgorithms, and write output values. Furthermore, industrial plants have\nlifetimes of one or more decades. Thus, in a realistic Industry 4.0 scenario,\nthese devices have to be integrated in novel systems. In order to apply\nadvanced concepts and technologies, such as computation offloading, which\nrequires data exchange between PLCs and edge cloud, we investigate open\ncommunication interfaces of two typical PLCs of Siemens S7 series. Hence, each\nof the interfaces is analyzed based on plug & play capability, if metadata is\nprovided, protocol efficiency, and performance. For the latter, the smallest\npossible update time for each of the interfaces will be measured.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:09:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gundall", "Michael", ""], ["Schotten", "Hans Dieter", ""]]}, {"id": "2106.04636", "submitter": "Andrew Chia", "authors": "Andrew Chia", "title": "Automatically Differentiable Random Coefficient Logistic Demand\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the random coefficient logistic demand (BLP) model can be phrased\nas an automatically differentiable moment function, including the incorporation\nof numerical safeguards proposed in the literature. This allows gradient-based\nfrequentist and quasi-Bayesian estimation using the Continuously Updating\nEstimator (CUE). Drawing from the machine learning literature, we outline\nhitherto under-utilized best practices in both frequentist and Bayesian\nestimation techniques. Our Monte Carlo experiments compare the performance of\nCUE, 2S-GMM, and LTE estimation. Preliminary findings indicate that the CUE\nestimated using LTE and frequentist optimization has a lower bias but higher\nMAE compared to the traditional 2-Stage GMM (2S-GMM) approach. We also find\nthat using credible intervals from MCMC sampling for the non-linear parameters\ntogether with frequentist analytical standard errors for the concentrated out\nlinear parameters provides empirical coverage closest to the nominal level. The\naccompanying admest Python package provides a platform for replication and\nextensibility.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:50:11 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chia", "Andrew", ""]]}, {"id": "2106.04727", "submitter": "Shangdi Yu", "authors": "Shangdi Yu, Yiqiu Wang, Yan Gu, Laxman Dhulipala, Julian Shun", "title": "ParChain: A Framework for Parallel Hierarchical Agglomerative Clustering\n  using Nearest-Neighbor Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the hierarchical clustering problem, where the goal is to\nproduce a dendrogram that represents clusters at varying scales of a data set.\nWe propose the ParChain framework for designing parallel hierarchical\nagglomerative clustering (HAC) algorithms, and using the framework we obtain\nnovel parallel algorithms for the complete linkage, average linkage, and Ward's\nlinkage criteria. Compared to most previous parallel HAC algorithms, which\nrequire quadratic memory, our new algorithms require only linear memory, and\nare scalable to large data sets. ParChain is based on our parallelization of\nthe nearest-neighbor chain algorithm, and enables multiple clusters to be\nmerged on every round. We introduce two key optimizations that are critical for\nefficiency: a range query optimization that reduces the number of distance\ncomputations required when finding nearest neighbors of clusters, and a caching\noptimization that stores a subset of previously computed distances, which are\nlikely to be reused.\n  Experimentally, we show that our highly-optimized implementations using 48\ncores with two-way hyper-threading achieve 5.8--110.1x speedup over\nstate-of-the-art parallel HAC algorithms and achieve 13.75--54.23x\nself-relative speedup. Compared to state-of-the-art algorithms, our algorithms\nrequire up to 237.3x less space. Our algorithms are able to scale to data set\nsizes with tens of millions of points, which existing algorithms are not able\nto handle.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 23:13:27 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Yu", "Shangdi", ""], ["Wang", "Yiqiu", ""], ["Gu", "Yan", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2106.04759", "submitter": "Artin Spiridonoff", "authors": "Artin Spiridonoff, Alex Olshevsky and Ioannis Ch. Paschalidis", "title": "Communication-efficient SGD: From Local SGD to One-Shot Averaging", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.02582", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider speeding up stochastic gradient descent (SGD) by parallelizing it\nacross multiple workers. We assume the same data set is shared among $N$\nworkers, who can take SGD steps and coordinate with a central server. While it\nis possible to obtain a linear reduction in the variance by averaging all the\nstochastic gradients at every step, this requires a lot of communication\nbetween the workers and the server, which can dramatically reduce the gains\nfrom parallelism. The Local SGD method, proposed and analyzed in the earlier\nliterature, suggests machines should make many local steps between such\ncommunications. While the initial analysis of Local SGD showed it needs $\\Omega\n( \\sqrt{T} )$ communications for $T$ local gradient steps in order for the\nerror to scale proportionately to $1/(NT)$, this has been successively improved\nin a string of papers, with the state-of-the-art requiring $\\Omega \\left( N\n\\left( \\mbox{ polynomial in log } (T) \\right) \\right)$ communications. In this\npaper, we suggest a Local SGD scheme that communicates less overall by\ncommunicating less frequently as the number of iterations grows. Our analysis\nshows that this can achieve an error that scales as $1/(NT)$ with a number of\ncommunications that is completely independent of $T$. In particular, we show\nthat $\\Omega(N)$ communications are sufficient. Empirical evidence suggests\nthis bound is close to tight as we further show that $\\sqrt{N}$ or $N^{3/4}$\ncommunications fail to achieve linear speed-up in simulations. Moreover, we\nshow that under mild assumptions, the main of which is twice differentiability\non any neighborhood of the optimal solution, one-shot averaging which only uses\na single round of communication can also achieve the optimal convergence rate\nasymptotically.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 01:10:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Spiridonoff", "Artin", ""], ["Olshevsky", "Alex", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "2106.04808", "submitter": "Shantanu Pal", "authors": "Shantanu Pal, Ali Dorri, Raja Jurdak", "title": "Blockchain for IoT Access Control: Recent Trends and Future Research\n  Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of wireless sensor networks, smart devices, and\ntraditional information and communication technologies, there is tremendous\ngrowth in the use of Internet of Things (IoT) applications and services in our\neveryday life. IoT systems deal with high volumes of data. This data can be\nparticularly sensitive, as it may include health, financial, location, and\nother highly personal information. Fine-grained security management in IoT\ndemands effective access control. Several proposals discuss access control for\nthe IoT, however, a limited focus is given to the emerging blockchain-based\nsolutions for IoT access control. In this paper, we review the recent trends\nand critical needs for blockchain-based solutions for IoT access control. We\nidentify several important aspects of blockchain, including decentralised\ncontrol, secure storage and sharing information in a trustless manner, for IoT\naccess control including their benefits and limitations. Finally, we note some\nfuture research directions on how to converge blockchain in IoT access control\nefficiently and effectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 04:53:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pal", "Shantanu", ""], ["Dorri", "Ali", ""], ["Jurdak", "Raja", ""]]}, {"id": "2106.04811", "submitter": "Jem Guhit", "authors": "Jem Guhit, Edward Colone, Shawn McKee, Kris Steinhoff, and Katarina\n  Thomas", "title": "Benchmarking NetBASILISK: a Network Security Project for Science", "comments": "12 pages, 4 figures, presented at vCHEP '21 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infrastructures supporting distributed scientific collaborations must address\ncompeting goals in both providing high-performance access to resources while\nsimultaneously securing the infrastructure against security threats. The\nNetBASILISK project is attempting to improve the security of such\ninfrastructures while not adversely impacting their performance. This paper\nwill present our work to create a benchmark and monitoring infrastructure that\nallows us to test for any degradation in transferring data into a NetBASILISK\nprotected site.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 05:08:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Guhit", "Jem", ""], ["Colone", "Edward", ""], ["McKee", "Shawn", ""], ["Steinhoff", "Kris", ""], ["Thomas", "Katarina", ""]]}, {"id": "2106.04885", "submitter": "Shantanu Pal", "authors": "Shantanu Pal, Ambrose Hill, Tahiry Rabehaja, Michael Hitchens", "title": "A Blockchain-Based Trust Management Framework with Verifiable\n  Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been tremendous interest in the development of formal trust models\nand metrics through the use of analytics (e.g., Belief Theory and Bayesian\nmodels), logics (e.g., Epistemic and Subjective Logic) and other mathematical\nmodels. The choice of trust metric will depend on context, circumstance and\nuser requirements and there is no single best metric for use in all\ncircumstances. Where different users require different trust metrics to be\nemployed the trust score calculations should still be based on all available\ntrust evidence. Trust is normally computed using past experiences but, in\npractice (especially in centralised systems), the validity and accuracy of\nthese experiences are taken for granted. In this paper, we provide a formal\nframework and practical blockchain-based implementation that allows independent\ntrust providers to implement different trust metrics in a distributed manner\nwhile still allowing all trust providers to base their calculations on a common\nset of trust evidence. Further, our design allows experiences to be provably\nlinked to interactions without the need for a central authority. This leads to\nthe notion of evidence-based trust with provable interactions. Leveraging\nblockchain allows the trust providers to offer their services in a competitive\nmanner, charging fees while users are provided with payments for recording\nexperiences. Performance details of the blockchain implementation are provided.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:10:03 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pal", "Shantanu", ""], ["Hill", "Ambrose", ""], ["Rabehaja", "Tahiry", ""], ["Hitchens", "Michael", ""]]}, {"id": "2106.04979", "submitter": "Artur Podobas PhD", "authors": "Martin Svedin, Steven W. D. Chien, Gibson Chikafa, Niclas Jansson,\n  Artur Podobas", "title": "Benchmarking the Nvidia GPU Lineage: From Early K80 to Modern A100 with\n  Asynchronous Memory Transfers", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many, Graphics Processing Units (GPUs) provides a source of reliable\ncomputing power. Recently, Nvidia introduced its 9th generation HPC-grade GPUs,\nthe Ampere 100, claiming significant performance improvements over previous\ngenerations, particularly for AI-workloads, as well as introducing new\narchitectural features such as asynchronous data movement. But how well does\nthe A100 perform on non-AI benchmarks, and can we expect the A100 to deliver\nthe application improvements we have grown used to with previous GPU\ngenerations? In this paper, we benchmark the A100 GPU and compare it to four\nprevious generations of GPUs, with particular focus on empirically quantifying\nour derived performance expectations, and -- should those expectations be\nundelivered -- investigate whether the introduced data-movement features can\noffset any eventual loss in performance? We find that the A100 delivers less\nperformance increase than previous generations for the well-known Rodinia\nbenchmark suite; we show that some of these performance anomalies can be\nremedied through clever use of the new data-movement features, which we\nmicrobenchmark and demonstrate where (and more importantly, how) they should be\nused.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:56:31 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 13:25:13 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Svedin", "Martin", ""], ["Chien", "Steven W. D.", ""], ["Chikafa", "Gibson", ""], ["Jansson", "Niclas", ""], ["Podobas", "Artur", ""]]}, {"id": "2106.05001", "submitter": "Mi Luo", "authors": "Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, Jiashi Feng", "title": "No Fear of Heterogeneity: Classifier Calibration for Federated Learning\n  with Non-IID Data", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in training classification models in the real-world\nfederated system is learning with non-IID data. To cope with this, most of the\nexisting works involve enforcing regularization in local optimization or\nimproving the model aggregation scheme at the server. Other works also share\npublic datasets or synthesized samples to supplement the training of\nunder-represented classes or introduce a certain level of personalization.\nThough effective, they lack a deep understanding of how the data heterogeneity\naffects each layer of a deep classification model. In this paper, we bridge\nthis gap by performing an experimental analysis of the representations learned\nby different layers. Our observations are surprising: (1) there exists a\ngreater bias in the classifier than other layers, and (2) the classification\nperformance can be significantly improved by post-calibrating the classifier\nafter federated training. Motivated by the above findings, we propose a novel\nand simple algorithm called Classifier Calibration with Virtual Representations\n(CCVR), which adjusts the classifier using virtual representations sampled from\nan approximated gaussian mixture model. Experimental results demonstrate that\nCCVR achieves state-of-the-art performance on popular federated learning\nbenchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple\nyet effective method can shed some light on the future research of federated\nlearning with non-IID data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:02:29 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Luo", "Mi", ""], ["Chen", "Fei", ""], ["Hu", "Dapeng", ""], ["Zhang", "Yifan", ""], ["Liang", "Jian", ""], ["Feng", "Jiashi", ""]]}, {"id": "2106.05050", "submitter": "Jawad Haj-Yahya", "authors": "Jawad Haj-Yahya, Jeremie S. Kim, A. Giray Yaglikci, Ivan Puddu, Lois\n  Orosa, Juan G\\'omez Luna, Mohammed Alser, Onur Mutlu", "title": "IChannels: Exploiting Current Management Mechanisms to Create Covert\n  Channels in Modern Processors", "comments": "To appear in ISCA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To operate efficiently across a wide range of workloads with varying power\nrequirements, a modern processor applies different current management\nmechanisms, which briefly throttle instruction execution while they adjust\nvoltage and frequency to accommodate for power-hungry instructions (PHIs) in\nthe instruction stream. Doing so 1) reduces the power consumption of non-PHI\ninstructions in typical workloads and 2) optimizes system voltage regulators'\ncost and area for the common use case while limiting current consumption when\nexecuting PHIs.\n  However, these mechanisms may compromise a system's confidentiality\nguarantees. In particular, we observe that multilevel side-effects of\nthrottling mechanisms, due to PHI-related current management mechanisms, can be\ndetected by two different software contexts (i.e., sender and receiver) running\non 1) the same hardware thread, 2) co-located Simultaneous Multi-Threading\n(SMT) threads, and 3) different physical cores.\n  Based on these new observations on current management mechanisms, we develop\na new set of covert channels, IChannels, and demonstrate them in real modern\nIntel processors (which span more than 70% of the entire client and server\nprocessor market). Our analysis shows that IChannels provides more than 24x the\nchannel capacity of state-of-the-art power management covert channels. We\npropose practical and effective mitigations to each covert channel in IChannels\nby leveraging the insights we gain through a rigorous characterization of real\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 13:03:08 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:24:57 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Haj-Yahya", "Jawad", ""], ["Kim", "Jeremie S.", ""], ["Yaglikci", "A. Giray", ""], ["Puddu", "Ivan", ""], ["Orosa", "Lois", ""], ["Luna", "Juan G\u00f3mez", ""], ["Alser", "Mohammed", ""], ["Mutlu", "Onur", ""]]}, {"id": "2106.05108", "submitter": "Jonas H. M\\\"uller Kornd\\\"orfer", "authors": "Jonas H. M\\\"uller Kornd\\\"orfer and Ahmed Eleliemy and Ali Mohammed and\n  Florina M. Ciorba", "title": "LB4OMP: A Dynamic Load Balancing Library for Multithreaded Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exascale computing systems will exhibit high degrees of hierarchical\nparallelism, with thousands of computing nodes and hundreds of cores per node.\n  Efficiently exploiting hierarchical parallelism is challenging due to load\nimbalance that arises at multiple levels.\n  OpenMP is the most widely-used standard for expressing and exploiting the\never-increasing node-level parallelism.\n  The scheduling options in OpenMP are insufficient to address the load\nimbalance that arises during the execution of multithreaded applications.\n  The limited scheduling options in OpenMP hinder research on novel scheduling\ntechniques which require comparison with others from the literature.\n  This work introduces LB4OMP, an open-source dynamic load balancing library\nthat implements successful scheduling algorithms from the literature.\n  LB4OMP is a research infrastructure designed to spur and support present and\nfuture scheduling research, for the benefit of multithreaded applications\nperformance.\n  Through an extensive performance analysis campaign, we assess the\neffectiveness and demystify the performance of all loop scheduling techniques\nin the library.\n  We show that, for numerous applications-systems pairs, the scheduling\ntechniques in LB4OMP outperform the scheduling options in OpenMP.\n  Node-level load balancing using LB4OMP leads to reduced cross-node load\nimbalance and to improved MPI+OpenMP applications performance, which is\ncritical for Exascale computing.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:36:11 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kornd\u00f6rfer", "Jonas H. M\u00fcller", ""], ["Eleliemy", "Ahmed", ""], ["Mohammed", "Ali", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "2106.05177", "submitter": "Rafael Ferreira da Silva", "authors": "Rafael Ferreira da Silva, Henri Casanova, Kyle Chard, Tain\\~a Coleman,\n  Dan Laney, Dong Ahn, Shantenu Jha, Dorran Howell, Stian Soiland-Reys, Ilkay\n  Altintas, Douglas Thain, Rosa Filgueira, Yadu Babuji, Rosa M. Badia, Bartosz\n  Balis, Silvina Caino-Lores, Scott Callaghan, Frederik Coppens, Michael R.\n  Crusoe, Kaushik De, Frank Di Natale, Tu M. A. Do, Bjoern Enders, Thomas\n  Fahringer, Anne Fouilloux, Grigori Fursin, Alban Gaignard, Alex Ganose,\n  Daniel Garijo, Sandra Gesing, Carole Goble, Adil Hasan, Sebastiaan Huber,\n  Daniel S. Katz, Ulf Leser, Douglas Lowe, Bertram Ludaescher, Ketan\n  Maheshwari, Maciej Malawski, Rajiv Mayani, Kshitij Mehta, Andre Merzky, Todd\n  Munson, Jonathan Ozik, Lo\\\"ic Pottier, Sashko Ristov, Mehdi Roozmeh, Renan\n  Souza, Fr\\'ed\\'eric Suter, Benjamin Tovar, Matteo Turilli, Karan Vahi, Alvaro\n  Vidal-Torreira, Wendy Whitcup, Michael Wilde, Alan Williams, Matthew Wolf,\n  Justin Wozniak", "title": "Workflows Community Summit: Advancing the State-of-the-art of Scientific\n  Workflows Management Systems Research and Development", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.4915801", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific workflows are a cornerstone of modern scientific computing, and\nthey have underpinned some of the most significant discoveries of the last\ndecade. Many of these workflows have high computational, storage, and/or\ncommunication demands, and thus must execute on a wide range of large-scale\nplatforms, from large clouds to upcoming exascale HPC platforms. Workflows will\nplay a crucial role in the data-oriented and post-Moore's computing landscape\nas they democratize the application of cutting-edge research techniques,\ncomputationally intensive methods, and use of new computing platforms. As\nworkflows continue to be adopted by scientific projects and user communities,\nthey are becoming more complex. Workflows are increasingly composed of tasks\nthat perform computations such as short machine learning inference, multi-node\nsimulations, long-running machine learning model training, amongst others, and\nthus increasingly rely on heterogeneous architectures that include CPUs but\nalso GPUs and accelerators. The workflow management system (WMS) technology\nlandscape is currently segmented and presents significant barriers to entry due\nto the hundreds of seemingly comparable, yet incompatible, systems that exist.\nAnother fundamental problem is that there are conflicting theoretical bases and\nabstractions for a WMS. Systems that use the same underlying abstractions can\nlikely be translated between, which is not the case for systems that use\ndifferent abstractions. More information:\nhttps://workflowsri.org/summits/technical\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:18:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["da Silva", "Rafael Ferreira", ""], ["Casanova", "Henri", ""], ["Chard", "Kyle", ""], ["Coleman", "Tain\u00e3", ""], ["Laney", "Dan", ""], ["Ahn", "Dong", ""], ["Jha", "Shantenu", ""], ["Howell", "Dorran", ""], ["Soiland-Reys", "Stian", ""], ["Altintas", "Ilkay", ""], ["Thain", "Douglas", ""], ["Filgueira", "Rosa", ""], ["Babuji", "Yadu", ""], ["Badia", "Rosa M.", ""], ["Balis", "Bartosz", ""], ["Caino-Lores", "Silvina", ""], ["Callaghan", "Scott", ""], ["Coppens", "Frederik", ""], ["Crusoe", "Michael R.", ""], ["De", "Kaushik", ""], ["Di Natale", "Frank", ""], ["Do", "Tu M. A.", ""], ["Enders", "Bjoern", ""], ["Fahringer", "Thomas", ""], ["Fouilloux", "Anne", ""], ["Fursin", "Grigori", ""], ["Gaignard", "Alban", ""], ["Ganose", "Alex", ""], ["Garijo", "Daniel", ""], ["Gesing", "Sandra", ""], ["Goble", "Carole", ""], ["Hasan", "Adil", ""], ["Huber", "Sebastiaan", ""], ["Katz", "Daniel S.", ""], ["Leser", "Ulf", ""], ["Lowe", "Douglas", ""], ["Ludaescher", "Bertram", ""], ["Maheshwari", "Ketan", ""], ["Malawski", "Maciej", ""], ["Mayani", "Rajiv", ""], ["Mehta", "Kshitij", ""], ["Merzky", "Andre", ""], ["Munson", "Todd", ""], ["Ozik", "Jonathan", ""], ["Pottier", "Lo\u00efc", ""], ["Ristov", "Sashko", ""], ["Roozmeh", "Mehdi", ""], ["Souza", "Renan", ""], ["Suter", "Fr\u00e9d\u00e9ric", ""], ["Tovar", "Benjamin", ""], ["Turilli", "Matteo", ""], ["Vahi", "Karan", ""], ["Vidal-Torreira", "Alvaro", ""], ["Whitcup", "Wendy", ""], ["Wilde", "Michael", ""], ["Williams", "Alan", ""], ["Wolf", "Matthew", ""], ["Wozniak", "Justin", ""]]}, {"id": "2106.05245", "submitter": "Peter Macgregor", "authors": "Peter Macgregor and He Sun", "title": "Local Algorithms for Finding Densely Connected Clusters", "comments": "This work is accepted at ICML'21 for a long presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Local graph clustering is an important algorithmic technique for analysing\nmassive graphs, and has been widely applied in many research fields of data\nscience. While the objective of most (local) graph clustering algorithms is to\nfind a vertex set of low conductance, there has been a sequence of recent\nstudies that highlight the importance of the inter-connection between clusters\nwhen analysing real-world datasets. Following this line of research, in this\nwork we study local algorithms for finding a pair of vertex sets defined with\nrespect to their inter-connection and their relationship with the rest of the\ngraph. The key to our analysis is a new reduction technique that relates the\nstructure of multiple sets to a single vertex set in the reduced graph. Among\nmany potential applications, we show that our algorithms successfully recover\ndensely connected clusters in the Interstate Disputes Dataset and the US\nMigration Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:40:45 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Macgregor", "Peter", ""], ["Sun", "He", ""]]}, {"id": "2106.05345", "submitter": "Jashwant Raj Gunasekaran", "authors": "Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran,\n  Mahmut Taylan Kandemir, Chita R. Das", "title": "Cocktail: Leveraging Ensemble Learning for Optimized Model Serving in\n  Public Cloud", "comments": "Accepeted at NSDI' 2022", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a growing demand for adopting ML models for a varietyof application\nservices, it is vital that the frameworks servingthese models are capable of\ndelivering highly accurate predic-tions with minimal latency along with reduced\ndeploymentcosts in a public cloud environment. Despite high latency,prior works\nin this domain are crucially limited by the accu-racy offered by individual\nmodels. Intuitively, model ensem-bling can address the accuracy gap by\nintelligently combiningdifferent models in parallel. However, selecting the\nappro-priate models dynamically at runtime to meet the desiredaccuracy with low\nlatency at minimal deployment cost is anontrivial problem. Towards this, we\nproposeCocktail, a costeffective ensembling-based model serving\nframework.Cock-tailcomprises of two key components: (i) a dynamic\nmodelselection framework, which reduces the number of modelsin the ensemble,\nwhile satisfying the accuracy and latencyrequirements; (ii) an adaptive\nresource management (RM)framework that employs a distributed proactive\nautoscalingpolicy combined with importance sampling, to efficiently allo-cate\nresources for the models. The RM framework leveragestransient virtual machine\n(VM) instances to reduce the de-ployment cost in a public cloud. A prototype\nimplementationofCocktailon the AWS EC2 platform and exhaustive evalua-tions\nusing a variety of workloads demonstrate thatCocktailcan reduce deployment cost\nby 1.45x, while providing 2xreduction in latency and satisfying the target\naccuracy for upto 96% of the requests, when compared to\nstate-of-the-artmodel-serving frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 19:23:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Gunasekaran", "Jashwant Raj", ""], ["Mishra", "Cyan Subhra", ""], ["Thinakaran", "Prashanth", ""], ["Kandemir", "Mahmut Taylan", ""], ["Das", "Chita R.", ""]]}, {"id": "2106.05373", "submitter": "Steven W. D. Chien", "authors": "Artur Podobas, Martin Svedin, Steven W. D. Chien, Ivy B. Peng, Naresh\n  Balaji Ravichandran, Pawel Herman, Anders Lansner, Stefano Markidis", "title": "StreamBrain: An HPC Framework for Brain-like Neural Networks on CPUs,\n  GPUs and FPGAs", "comments": "Accepted for publication at the International Symposium on Highly\n  Efficient Accelerators and Reconfigurable Technologies (HEART 2021)", "journal-ref": null, "doi": "10.1145/3468044.3468052", "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern deep learning method based on backpropagation has surged in\npopularity and has been used in multiple domains and application areas. At the\nsame time, there are other -- less-known -- machine learning algorithms with a\nmature and solid theoretical foundation whose performance remains unexplored.\nOne such example is the brain-like Bayesian Confidence Propagation Neural\nNetwork (BCPNN). In this paper, we introduce StreamBrain -- a framework that\nallows neural networks based on BCPNN to be practically deployed in\nHigh-Performance Computing systems. StreamBrain is a domain-specific language\n(DSL), similar in concept to existing machine learning (ML) frameworks, and\nsupports backends for CPUs, GPUs, and even FPGAs. We empirically demonstrate\nthat StreamBrain can train the well-known ML benchmark dataset MNIST within\nseconds, and we are the first to demonstrate BCPNN on STL-10 size networks. We\nalso show how StreamBrain can be used to train with custom floating-point\nformats and illustrate the impact of using different bfloat variations on BCPNN\nusing FPGAs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 20:28:18 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Podobas", "Artur", ""], ["Svedin", "Martin", ""], ["Chien", "Steven W. D.", ""], ["Peng", "Ivy B.", ""], ["Ravichandran", "Naresh Balaji", ""], ["Herman", "Pawel", ""], ["Lansner", "Anders", ""], ["Markidis", "Stefano", ""]]}, {"id": "2106.05463", "submitter": "Hong Su Dr.", "authors": "Hong Su", "title": "Cross-chain Interaction Model In a Fully Verified Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are different kinds of blockchains, which have been applied in various\nareas. Blockchains are relatively independent systems that are apt to form\nisolated data islands. Then cross-chain interaction is proposed to connect\ndifferent blockchains. However, the current cross-chain methods do not maintain\nthe security of the original blockchain. They either depend on a less secure\nthird-party system or a less secure method. This makes the cross-chain\ninteraction less secure than the original blockchains (the security downgrade\nissues), or the cross-chain interaction can be done even if the paired\nblockchain does not exist (the blockchain invisible issue). In this paper, we\nfirst propose a system interaction model and use it to analyze the possible\nsecurity issues. Based on conclusions got from the proposed model, we propose\nthe cross-chain method that verifies the data of the paired blockchain by the\nconsensus algorithm of the paired blockchain (the CIFuV method). With this\nmethod, the cross-chain interaction can be as the same security as in the\npaired blockchain. At last, we evaluate the security issues during the system\ninteraction process, and the possibility to have the CIFuV model on the public\nblockchains.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 02:47:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Su", "Hong", ""]]}, {"id": "2106.05475", "submitter": "Cong Nguyen", "authors": "Cong T. Nguyen, Diep N. Nguyen, Dinh Thai Hoang, Hoang-Anh Pham, Eryk\n  Dutkiewicz", "title": "Jointly Optimize Coding and Node Selection for Distributed Computing\n  over Wireless Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work aims to jointly optimize the coding and node selection to minimize\nthe processing time for distributed computing tasks over wireless edge\nnetworks. Since the joint optimization problem formulation is NP-hard and\nnonlinear, we leverage the discrete characteristic of its decision variables to\ntransform the problem into an equivalent linear formulation. This linearization\ncan guarantee to find the optimal solutions and significantly reduce the\nproblem's complexity. Simulations based on real-world datasets show that the\nproposed approach can reduce the total processing time up to 2.3 times compared\nwith that of state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 03:29:10 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Nguyen", "Cong T.", ""], ["Nguyen", "Diep N.", ""], ["Hoang", "Dinh Thai", ""], ["Pham", "Hoang-Anh", ""], ["Dutkiewicz", "Eryk", ""]]}, {"id": "2106.05485", "submitter": "Leonid Sokolinsky", "authors": "Leonid B. Sokolinsky, Irina M. Sokolinskaya", "title": "VaLiPro: Linear Programming Validator for Cluster Computing Systems", "comments": "Submitted to \"Supercomputing Frontiers and Innovations\" journal", "journal-ref": "Parallel Computational Technologies. PCT 2021. Communications in\n  Computer and Information Science, vol. 1437", "doi": "10.1007/978-3-030-81691-9_12", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents and evaluates a scalable algorithm for validating\nsolutions of linear programming problems on cluster computing systems. The main\nidea of the method is to generate a regular set of points (validation set) on a\nsmall-radius hypersphere centered at the point of the solution under\nvalidation. The objective function is calculated for each point of the\nvalidation set that belongs to the feasible region. If all these values are\nless than or equal to the value of the objective function at the point under\nvalidation, then this point is the correct solution. The parallel\nimplementation of the VaLiPro algorithm is performed in C++ through the\nparallel BSF-skeleton, which encapsulates all aspects related to the MPI-based\nparallelization of the program. We provide the results of large-scale\ncomputational experiments on a cluster computing system to study the\nscalability of the VaLiPro algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 03:56:49 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 03:13:12 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Sokolinsky", "Leonid B.", ""], ["Sokolinskaya", "Irina M.", ""]]}, {"id": "2106.05584", "submitter": "Minxian Xu", "authors": "Minxian Xu, Qiheng Zhou, Huaming Wu, Weiwei Lin, Kejiang Ye,\n  Chengzhong Xu", "title": "PDMA: Probabilistic Service Migration Approach for Delay-aware and\n  Mobility-aware Mobile Edge Computing", "comments": "22 pages, 10 figures, accepted by Software: Practice and Experience,\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As a key technology in the 5G era, Mobile Edge Computing (MEC) has developed\nrapidly in recent years. MEC aims to reduce the service delay of mobile users,\nwhile alleviating the processing pressure on the core network. MEC can be\nregarded as an extension of cloud computing on the user side, which can deploy\nedge servers and bring computing resources closer to mobile users, and provide\nmore efficient interactions. However, due to the user's dynamic mobility, the\ndistance between the user and the edge server will change dynamically, which\nmay cause fluctuations in Quality of Service (QoS). Therefore, when a mobile\nuser moves in the MEC environment, certain approaches are needed to schedule\nservices deployed on the edge server to ensure the user experience. In this\npaper, we model service scheduling in MEC scenarios and propose a delay-aware\nand mobility-aware service management approach based on concise probabilistic\nmethods. This approach has low computational complexity and can effectively\nreduce service delay and migration costs. Furthermore, we conduct experiments\nby utilizing multiple realistic datasets and use iFogSim to evaluate the\nperformance of the algorithm. The results show that our proposed approach can\noptimize the performance on service delay, with 8% to 20% improvement and\nreduce the migration cost by more than 75% compared with baselines during the\nrush hours.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:23:48 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Xu", "Minxian", ""], ["Zhou", "Qiheng", ""], ["Wu", "Huaming", ""], ["Lin", "Weiwei", ""], ["Ye", "Kejiang", ""], ["Xu", "Chengzhong", ""]]}, {"id": "2106.06022", "submitter": "Martin Bauer", "authors": "Martin Bauer", "title": "IoT Virtualization with ML-based Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For IoT to reach its full potential, the sharing and reuse of information in\ndifferent applications and across verticals is of paramount importance.\nHowever, there are a plethora of IoT platforms using different representations,\nprotocols and interaction patterns. To address this issue, the Fed4IoT project\nhas developed an IoT virtualization platform that, on the one hand, integrates\ninformation from many different source platforms and, on the other hand, makes\nthe information required by the respective users available in the target\nplatform of choice. To enable this, information is translated into a common,\nneutral exchange format. The format of choice is NGSI-LD, which is being\nstandardized by the ETSI Industry Specification Group on Context Information\nManagement (ETSI ISG CIM). Thing Visors are the components that translate the\nsource information to NGSI-LD, which is then delivered to the target platform\nand translated into the target format. ThingVisors can be implemented by hand,\nbut this requires significant human effort, especially considering the\nheterogeneity of low level information produced by a multitude of sensors.\nThus, supporting the human developer and, ideally, fully automating the process\nof extracting and enriching data and translating it to NGSI-LD is a crucial\nstep. Machine learning is a promising approach for this, but it typically\nrequires large amounts of hand-labelled data for training, an effort that makes\nit unrealistic in many IoT scenarios. A programmatic labelling approach called\nknowledge infusion that encodes expert knowledge is used for matching a schema\nor ontology extracted from the data with a target schema or ontology, providing\nthe basis for annotating the data and facilitating the translation to NGSI-LD.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 19:56:48 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bauer", "Martin", ""]]}, {"id": "2106.06150", "submitter": "Da Zheng", "authors": "Jialin Dong, Da Zheng, Lin F. Yang, Geroge Karypis", "title": "Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs", "comments": "The paper is published in KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are powerful tools for learning from graph data\nand are widely used in various applications such as social network\nrecommendation, fraud detection, and graph search. The graphs in these\napplications are typically large, usually containing hundreds of millions of\nnodes. Training GNN models on such large graphs efficiently remains a big\nchallenge. Despite a number of sampling-based methods have been proposed to\nenable mini-batch training on large graphs, these methods have not been proved\nto work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU\ntraining. The state-of-the-art sampling-based methods are usually not optimized\nfor these real-world hardware setups, in which data movement between CPUs and\nGPUs is a bottleneck. To address this issue, we propose Global Neighborhood\nSampling that aims at training GNNs on giant graphs specifically for\nmixed-CPU-GPU training. The algorithm samples a global cache of nodes\nperiodically for all mini-batches and stores them in GPUs. This global cache\nallows in-GPU importance sampling of mini-batches, which drastically reduces\nthe number of nodes in a mini-batch, especially in the input layer, to reduce\ndata copy between CPU and GPU and mini-batch computation without compromising\nthe training convergence rate or model accuracy. We provide a highly efficient\nimplementation of this method and show that our implementation outperforms an\nefficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant\ngraphs. It outperforms an efficient implementation of LADIES with small layers\nby a factor of 2X-14X while achieving much higher accuracy than LADIES.We also\ntheoretically analyze the proposed algorithm and show that with cached node\ndata of a proper size, it enjoys a comparable convergence rate as the\nunderlying node-wise sampling method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 03:30:25 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Dong", "Jialin", ""], ["Zheng", "Da", ""], ["Yang", "Lin F.", ""], ["Karypis", "Geroge", ""]]}, {"id": "2106.06161", "submitter": "Rory Mitchell", "authors": "Rory Mitchell, Daniel Stokes, Eibe Frank, Geoffrey Holmes", "title": "Bandwidth-Optimal Random Shuffling for GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear-time algorithms that are traditionally used to shuffle data on CPUs,\nsuch as the method of Fisher-Yates, are not well suited to implementation on\nGPUs due to inherent sequential dependencies. Moreover, existing parallel\nshuffling algorithms show unsatisfactory performance on GPU architectures\nbecause they incur a large number of read/write operations to high latency\nglobal memory. To address this, we provide a method of generating pseudo-random\npermutations in parallel by fusing suitable pseudo-random bijective functions\nwith stream compaction operations. Our algorithm, termed `bijective shuffle'\ntrades increased per-thread arithmetic operations for reduced global memory\ntransactions. It is work-efficient, deterministic, and only requires a single\nglobal memory read and write per shuffle input, thus maximising use of global\nmemory bandwidth. To empirically demonstrate the correctness of the algorithm,\nwe develop a consistent, linear time, statistical test for the quality of\npseudo-random permutations based on kernel space embeddings. Empirical results\nshow that the bijective shuffle algorithm outperforms competing algorithms on\nmulticore CPUs and GPUs, showing improvements of between one and two orders of\nmagnitude and approaching peak device bandwidth.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 04:10:13 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Mitchell", "Rory", ""], ["Stokes", "Daniel", ""], ["Frank", "Eibe", ""], ["Holmes", "Geoffrey", ""]]}, {"id": "2106.06242", "submitter": "Dragi Kimovski", "authors": "Roland Math\\'a, Dragi Kimovski, Anatoliy Zabrovskiy, Christian\n  Timmerer and Radu Prodan", "title": "Where to Encode: A Performance Analysis of x86 and Arm-based Amazon EC2\n  Instances", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video streaming became an undivided part of the Internet. To efficiently\nutilize the limited network bandwidth it is essential to encode the video\ncontent. However, encoding is a computationally intensive task, involving\nhigh-performance resources provided by private infrastructures or public\nclouds. Public clouds, such as Amazon EC2, provide a large portfolio of\nservices and instances optimized for specific purposes and budgets. The\nmajority of Amazon instances use x86 processors, such as Intel Xeon or AMD\nEPYC. However, following the recent trends in computer architecture, Amazon\nintroduced Arm-based instances that promise up to 40% better cost-performance\nratio than comparable x86 instances for specific workloads. We evaluate in this\npaper the video encoding performance of x86 and Arm instances of four instance\nfamilies using the latest FFmpeg version and two video codecs. We examine the\nimpact of the encoding parameters, such as different presets and bitrates, on\nthe time and cost for encoding. Our experiments reveal that Arm instances show\nhigh time and cost-saving potential of up to 33.63% for specific bitrates and\npresets, especially for the x264 codec. However, the x86 instances are more\ngeneral and achieve low encoding times, regardless of the codec.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:50:28 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 13:21:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Math\u00e1", "Roland", ""], ["Kimovski", "Dragi", ""], ["Zabrovskiy", "Anatoliy", ""], ["Timmerer", "Christian", ""], ["Prodan", "Radu", ""]]}, {"id": "2106.06293", "submitter": "Dionysios Diamantopoulos", "authors": "Dionysios Diamantopoulos, Raphael Polig, Burkhard Ringlein, Mitra\n  Purandare, Beat Weiss, Christoph Hagleitner, Mark Lantz, Francois Abel", "title": "Acceleration-as-a-{\\mu}Service: A Cloud-native Monte-Carlo Option\n  Pricing Engine on CPUs, GPUs and Disaggregated FPGAs", "comments": "3 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The evolution of cloud applications into loosely-coupled microservices opens\nnew opportunities for hardware accelerators to improve workload performance.\nExisting accelerator techniques for cloud sacrifice the consolidation benefits\nof microservices. This paper presents CloudiFi, a framework to deploy and\ncompare accelerators as a cloud service. We evaluate our framework in the\ncontext of a financial workload and present early results indicating up to 485x\ngains in microservice response time.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:27:11 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Diamantopoulos", "Dionysios", ""], ["Polig", "Raphael", ""], ["Ringlein", "Burkhard", ""], ["Purandare", "Mitra", ""], ["Weiss", "Beat", ""], ["Hagleitner", "Christoph", ""], ["Lantz", "Mark", ""], ["Abel", "Francois", ""]]}, {"id": "2106.06401", "submitter": "Edouard Oyallon", "authors": "Eugene Belilovsky (MILA), Louis Leconte (MLIA, CMAP), Lucas Caccia\n  (MILA), Michael Eickenberg, Edouard Oyallon (MLIA)", "title": "Decoupled Greedy Learning of CNNs for Synchronous and Asynchronous\n  Distributed Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.08164", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly cited inefficiency of neural network training using\nback-propagation is the update locking problem: each layer must wait for the\nsignal to propagate through the full network before updating. Several\nalternatives that can alleviate this issue have been proposed. In this context,\nwe consider a simple alternative based on minimal feedback, which we call\nDecoupled Greedy Learning (DGL). It is based on a classic greedy relaxation of\nthe joint training objective, recently shown to be effective in the context of\nConvolutional Neural Networks (CNNs) on large-scale image classification. We\nconsider an optimization of this objective that permits us to decouple the\nlayer training, allowing for layers or modules in networks to be trained with a\npotentially linear parallelization. With the use of a replay buffer we show\nthat this approach can be extended to asynchronous settings, where modules can\noperate and continue to update with possibly large communication delays. To\naddress bandwidth and memory issues we propose an approach based on online\nvector quantization. This allows to drastically reduce the communication\nbandwidth between modules and required memory for replay buffers. We show\ntheoretically and empirically that this approach converges and compare it to\nthe sequential solvers. We demonstrate the effectiveness of DGL against\nalternative approaches on the CIFAR-10 dataset and on the large-scale ImageNet\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:55:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Belilovsky", "Eugene", "", "MILA"], ["Leconte", "Louis", "", "MLIA, CMAP"], ["Caccia", "Lucas", "", "MILA"], ["Eickenberg", "Michael", "", "MLIA"], ["Oyallon", "Edouard", "", "MLIA"]]}, {"id": "2106.06433", "submitter": "Gagandeep Singh", "authors": "Gagandeep Singh, Mohammed Alser, Damla Senol Cali, Dionysios\n  Diamantopoulos, Juan G\\'omez-Luna, Henk Corporaal, Onur Mutlu", "title": "FPGA-Based Near-Memory Acceleration of Modern Data-Intensive\n  Applications", "comments": "This is an extended and updated version of a paper published in IEEE\n  Micro, vol. 41, no. 4, pp. 39-48, 1 July-Aug. 2021", "journal-ref": null, "doi": "10.1109/MM.2021.3088396", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data-intensive applications demand high computation capabilities with\nstrict power constraints. Unfortunately, such applications suffer from a\nsignificant waste of both execution cycles and energy in current computing\nsystems due to the costly data movement between the computation units and the\nmemory units. Genome analysis and weather prediction are two examples of such\napplications. Recent FPGAs couple a reconfigurable fabric with high-bandwidth\nmemory (HBM) to enable more efficient data movement and improve overall\nperformance and energy efficiency. This trend is an example of a paradigm shift\nto near-memory computing. We leverage such an FPGA with high-bandwidth memory\n(HBM) for improving the pre-alignment filtering step of genome analysis and\nrepresentative kernels from a weather prediction model. Our evaluation\ndemonstrates large speedups and energy savings over a high-end IBM POWER9\nsystem and a conventional FPGA board with DDR4 memory. We conclude that\nFPGA-based near-memory computing has the potential to alleviate the data\nmovement bottleneck for modern data-intensive applications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:43:04 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 08:06:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Singh", "Gagandeep", ""], ["Alser", "Mohammed", ""], ["Cali", "Damla Senol", ""], ["Diamantopoulos", "Dionysios", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Corporaal", "Henk", ""], ["Mutlu", "Onur", ""]]}, {"id": "2106.06445", "submitter": "Tuan Dinh", "authors": "Tuan Dinh, Kangwook Lee", "title": "Coded-InvNet for Resilient Prediction Serving Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by a new coded computation algorithm for invertible functions, we\npropose Coded-InvNet a new approach to design resilient prediction serving\nsystems that can gracefully handle stragglers or node failures. Coded-InvNet\nleverages recent findings in the deep learning literature such as invertible\nneural networks, Manifold Mixup, and domain translation algorithms, identifying\ninteresting research directions that span across machine learning and systems.\nOur experimental results show that Coded-InvNet can outperform existing\napproaches, especially when the compute resource overhead is as low as 10%. For\ninstance, without knowing which of the ten workers is going to fail, our\nalgorithm can design a backup task so that it can correctly recover the missing\nprediction result with an accuracy of 85.9%, significantly outperforming the\nprevious SOTA by 32.5%.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 15:02:11 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 22:21:26 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dinh", "Tuan", ""], ["Lee", "Kangwook", ""]]}, {"id": "2106.06465", "submitter": "Paolo Tasca", "authors": "Claudio J. Tessone, Paolo Tasca, Flavio Iannelli", "title": "Stochastic modelling of blockchain consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cond-mat.dis-nn physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain and general purpose distributed ledgers are foundational\ntechnologies which bring significant innovation in the infrastructures and\nother underpinnings of our socio-economic systems. These P2P technologies are\nable to securely diffuse information within and across networks, without need\nfor trustees or central authorities to enforce consensus. In this contribution,\nwe propose a minimalistic stochastic model to understand the dynamics of\nblockchain-based consensus. By leveraging on random-walk theory, we model block\npropagation delay on different network topologies and provide a classification\nof blockchain systems in terms of two emergent properties. Firstly, we identify\ntwo performing regimes: a functional regime corresponding to an optimal system\nfunction; and a non-functional regime characterised by a congested or branched\nstate of sub-optimal blockchains. Secondly, we discover a phase transition\nduring the emergence of consensus and numerically investigate the corresponding\ncritical point. Our results provide important insights into the consensus\nmechanism and sub-optimal states in decentralised systems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 15:37:32 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tessone", "Claudio J.", ""], ["Tasca", "Paolo", ""], ["Iannelli", "Flavio", ""]]}, {"id": "2106.06579", "submitter": "Yeshwanth Venkatesha", "authors": "Yeshwanth Venkatesha, Youngeun Kim, Leandros Tassiulas, Priyadarshini\n  Panda", "title": "Federated Learning with Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural networks get widespread adoption in resource-constrained embedded\ndevices, there is a growing need for low-power neural systems. Spiking Neural\nNetworks (SNNs)are emerging to be an energy-efficient alternative to the\ntraditional Artificial Neural Networks (ANNs) which are known to be\ncomputationally intensive. From an application perspective, as federated\nlearning involves multiple energy-constrained devices, there is a huge scope to\nleverage energy efficiency provided by SNNs. Despite its importance, there has\nbeen little attention on training SNNs on a large-scale distributed system like\nfederated learning. In this paper, we bring SNNs to a more realistic federated\nlearning scenario. Specifically, we propose a federated learning framework for\ndecentralized and privacy-preserving training of SNNs. To validate the proposed\nfederated learning framework, we experimentally evaluate the advantages of SNNs\non various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.\nWe observe that SNNs outperform ANNs in terms of overall accuracy by over 15%\nwhen the data is distributed across a large number of clients in the federation\nwhile providing up to5.3x energy efficiency. In addition to efficiency, we also\nanalyze the sensitivity of the proposed federated SNN framework to data\ndistribution among the clients, stragglers, and gradient noise and perform a\ncomprehensive comparison with ANNs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:00:58 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Venkatesha", "Yeshwanth", ""], ["Kim", "Youngeun", ""], ["Tassiulas", "Leandros", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2106.06601", "submitter": "Marko Kabi\\'c", "authors": "Marko Kabi\\'c, Simon Pintarelli, Anton Kozhevnikov and Joost\n  VandeVondele", "title": "COSTA: Communication-Optimal Shuffle and Transpose Algorithm with\n  Process Relabeling", "comments": "To be published in the proceedings of the 36th International\n  Conference on High Performance Computing, ISC High Performance 2021. The\n  implementation of the algorithm is available at:\n  https://github.com/eth-cscs/COSTA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication-avoiding algorithms for Linear Algebra have become increasingly\npopular, in particular for distributed memory architectures. In practice, these\nalgorithms assume that the data is already distributed in a specific way, thus\nmaking data reshuffling a key to use them. For performance reasons, a\nstraightforward all-to-all exchange must be avoided.\n  Here, we show that process relabeling (i.e. permuting processes in the final\nlayout) can be used to obtain communication optimality for data reshuffling,\nand that it can be efficiently found by solving a Linear Assignment Problem\n(Maximum Weight Bipartite Perfect Matching). Based on this, we have developed a\nCommunication-Optimal Shuffle and Transpose Algorithm (COSTA): this\nhighly-optimised algorithm implements $A=\\alpha\\cdot \\operatorname{op}(B) +\n\\beta \\cdot A,\\ \\operatorname{op} \\in \\{\\operatorname{transpose},\n\\operatorname{conjugate-transpose}, \\operatorname{identity}\\}$ on distributed\nsystems, where $A, B$ are matrices with potentially different (distributed)\nlayouts and $\\alpha, \\beta$ are scalars. COSTA can take advantage of the\ncommunication-optimal process relabeling even for heterogeneous network\ntopologies, where latency and bandwidth differ among nodes. The implementation\nnot only outperforms the best available ScaLAPACK redistribute and transpose\nroutines multiple times, but is also able to deal with more general matrix\nlayouts, in particular it is not limited to block-cyclic layouts. Finally, we\nuse COSTA to integrate a communication-optimal matrix multiplication algorithm\ninto the CP2K quantum chemistry simulation package. This way, we show that\nCOSTA can be used to unlock the full potential of recent Linear Algebra\nalgorithms in applications by facilitating interoperability between algorithms\nwith a wide range of data layouts, in addition to bringing significant\nredistribution speedups.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:31:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kabi\u0107", "Marko", ""], ["Pintarelli", "Simon", ""], ["Kozhevnikov", "Anton", ""], ["VandeVondele", "Joost", ""]]}, {"id": "2106.06627", "submitter": "Zichang Liu", "authors": "Li Chou, Zichang Liu, Zhuang Wang, Anshumali Shrivastava", "title": "Efficient and Less Centralized Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the rapid growth in mobile computing, massive amounts of data and\ncomputing resources are now located at the edge. To this end, Federated\nlearning (FL) is becoming a widely adopted distributed machine learning (ML)\nparadigm, which aims to harness this expanding skewed data locally in order to\ndevelop rich and informative models. In centralized FL, a collection of devices\ncollaboratively solve a ML task under the coordination of a central server.\nHowever, existing FL frameworks make an over-simplistic assumption about\nnetwork connectivity and ignore the communication bandwidth of the different\nlinks in the network. In this paper, we present and study a novel FL algorithm,\nin which devices mostly collaborate with other devices in a pairwise manner.\nOur nonparametric approach is able to exploit network topology to reduce\ncommunication bottlenecks. We evaluate our approach on various FL benchmarks\nand demonstrate that our method achieves 10X better communication efficiency\nand around 8% increase in accuracy compared to the centralized approach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 22:23:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chou", "Li", ""], ["Liu", "Zichang", ""], ["Wang", "Zhuang", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2106.06841", "submitter": "Stephen DiAdamo", "authors": "Rhea Parekh, Andrea Ricciardi, Ahmed Darwish, Stephen DiAdamo", "title": "Quantum Algorithms and Simulation for Parallel and Distributed Quantum\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A viable approach for building large-scale quantum computers is to interlink\nsmall-scale quantum computers with a quantum network to create a larger\ndistributed quantum computer. When designing quantum algorithms for such a\ndistributed quantum computer, one can make use of the added parallelization and\ndistribution abilities inherent in such a system. An added difficulty to\nconsider for distributed quantum computing is that a complex control system to\norchestrate the various components is required. In this work, we present\ndistributed and parallel versions of quantum algorithms and discuss potential\nbenefits and we propose a general scheme for controlling the system. Further,\nwe present the Interlin-q simulation platform which aims to simplify designing\nand simulating parallel and distributed quantum algorithms. Interlin-q's main\nfeatures are generating and executing control instructions across a simulated\nquantum network of simulated quantum computers. We demonstrate a simulation of\na proposed parallelized algorithm using Interlin-q and discuss steps for\ndeveloping Interlin-q into a control system for distributed quantum computers.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 19:41:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Parekh", "Rhea", ""], ["Ricciardi", "Andrea", ""], ["Darwish", "Ahmed", ""], ["DiAdamo", "Stephen", ""]]}, {"id": "2106.06843", "submitter": "Hangyu Zhu", "authors": "Hangyu Zhu, Jinjin Xu, Shiqing Liu and Yaochu Jin", "title": "Federated Learning on Non-IID Data: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is an emerging distributed machine learning framework for\nprivacy preservation. However, models trained in federated learning usually\nhave worse performance than those trained in the standard centralized learning\nmode, especially when the training data are not independent and identically\ndistributed (Non-IID) on the local devices. In this survey, we pro-vide a\ndetailed analysis of the influence of Non-IID data on both parametric and\nnon-parametric machine learning models in both horizontal and vertical\nfederated learning. In addition, cur-rent research work on handling challenges\nof Non-IID data in federated learning are reviewed, and both advantages and\ndisadvantages of these approaches are discussed. Finally, we suggest several\nfuture research directions before concluding the paper.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 19:45:35 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhu", "Hangyu", ""], ["Xu", "Jinjin", ""], ["Liu", "Shiqing", ""], ["Jin", "Yaochu", ""]]}, {"id": "2106.06934", "submitter": "Shunfeng Chu", "authors": "Shunfeng Chu, Jun Li, Jianxin Wang, Zhe Wang, Ming Ding, Yijin Zang,\n  Yuwen Qian, Wen Chen", "title": "Federated Learning Over Wireless Channels: Dynamic Resource Allocation\n  and Task Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of federated learning (FL), mobile devices (MDs) are\nable to train their local models with private data and sends them to a central\nserver for aggregation, thereby preventing sensitive raw data leakage. In this\npaper, we aim to improve the training performance of FL systems in the context\nof wireless channels and stochastic energy arrivals of MDs. To this purpose, we\ndynamically optimize MDs' transmission power and training task scheduling. We\nfirst model this dynamic programming problem as a constrained Markov decision\nprocess (CMDP). Due to high dimensions rooted from our CMDP problem, we propose\nonline stochastic learning methods to simplify the CMDP and design online\nalgorithms to obtain an efficient policy for all MDs. Since there are long-term\nconstraints in our CMDP, we utilize Lagrange multipliers approach to tackle\nthis issue. Furthermore, we prove the convergence of the proposed online\nstochastic learning algorithm. Numerical results indicate that the proposed\nalgorithms can achieve better performance than the benchmark algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 06:47:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chu", "Shunfeng", ""], ["Li", "Jun", ""], ["Wang", "Jianxin", ""], ["Wang", "Zhe", ""], ["Ding", "Ming", ""], ["Zang", "Yijin", ""], ["Qian", "Yuwen", ""], ["Chen", "Wen", ""]]}, {"id": "2106.07059", "submitter": "Hongyang Sun", "authors": "Lucas Perotin, Hongyang Sun, Padma Raghavan", "title": "Multi-Resource List Scheduling of Moldable Parallel Jobs under\n  Precedence Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The scheduling literature has traditionally focused on a single type of\nresource (e.g., computing nodes). However, scientific applications in modern\nHigh-Performance Computing (HPC) systems process large amounts of data, hence\nhave diverse requirements on different types of resources (e.g., cores, cache,\nmemory, I/O). All of these resources could potentially be exploited by the\nruntime scheduler to improve the application performance. In this paper, we\nstudy multi-resource scheduling to minimize the makespan of computational\nworkflows comprised of parallel jobs subject to precedence constraints. The\njobs are assumed to be moldable, allowing the scheduler to flexibly select a\nvariable set of resources before execution. We propose a multi-resource,\nlist-based scheduling algorithm, and prove that, on a system with $d$ types of\nschedulable resources, our algorithm achieves an approximation ratio of\n$1.619d+2.545\\sqrt{d}+1$ for any $d$, and a ratio of $d+O(\\sqrt[3]{d^2})$ for\nlarge $d$. We also present improved results for independent jobs and for jobs\nwith special precedence constraints (e.g., series-parallel graphs and trees).\nFinally, we prove a lower bound of $d$ on the approximation ratio of any list\nscheduling scheme with local priority considerations. To the best of our\nknowledge, these are the first approximation results for moldable workflows\nwith multiple resource requirements.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:56:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Perotin", "Lucas", ""], ["Sun", "Hongyang", ""], ["Raghavan", "Padma", ""]]}, {"id": "2106.07079", "submitter": "Sarper Ayd{\\i}n", "authors": "Sarper Ayd{\\i}n and Ceyhun Eksin", "title": "Decentralized Inertial Best-Response with Voluntary and Limited\n  Communication in Random Communication Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple autonomous agents interact over a random communication network to\nmaximize their individual utility functions which depend on the actions of\nother agents. We consider decentralized best-response with inertia type\nalgorithms in which agents form beliefs about the future actions of other\nplayers based on local information, and take an action that maximizes their\nexpected utility computed with respect to these beliefs or continue to take\ntheir previous action. We show convergence of these types of algorithms to a\nNash equilibrium in weakly acyclic games under the condition that the belief\nupdate and information exchange protocols successfully learn the actions of\nother players with positive probability in finite time given a static\nenvironment, i.e., when other agents' actions do not change. We design a\ndecentralized fictitious play algorithm with voluntary and limited\ncommunication (DFP-VL) protocols that satisfy this condition. In the voluntary\ncommunication protocol, each agent decides whom to exchange information with by\nassessing the novelty of its information and the potential effect of its\ninformation on others' assessments of their utility functions. The limited\ncommunication protocol entails agents sending only their most frequent action\nto agents that they decide to communicate with. Numerical experiments on a\ntarget assignment game demonstrate that the voluntary and limited communication\nprotocol can more than halve the number of communication attempts while\nretaining the same convergence rate as DFP in which agents constantly attempt\nto communicate.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 20:04:57 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ayd\u0131n", "Sarper", ""], ["Eksin", "Ceyhun", ""]]}, {"id": "2106.07094", "submitter": "Abolfazl Hashemi", "authors": "Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S. Dhillon", "title": "DP-NormFedAvg: Normalizing Client Updates for Privacy-Preserving\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on facilitating differentially private quantized\ncommunication between the clients and server in federated learning (FL).\nTowards this end, we propose to have the clients send a \\textit{private\nquantized} version of only the \\textit{unit vector} along the change in their\nlocal parameters to the server, \\textit{completely throwing away the magnitude\ninformation}. We call this algorithm \\texttt{DP-NormFedAvg} and show that it\nhas the same order-wise convergence rate as \\texttt{FedAvg} on smooth\nquasar-convex functions (an important class of non-convex functions for\nmodeling optimization of deep neural networks), thereby establishing that\ndiscarding the magnitude information is not detrimental from an optimization\npoint of view. We also introduce QTDL, a new differentially private\nquantization mechanism for unit-norm vectors, which we use in\n\\texttt{DP-NormFedAvg}. QTDL employs \\textit{discrete} noise having a\nLaplacian-like distribution on a \\textit{finite support} to provide privacy. We\nshow that under a growth-condition assumption on the per-sample client losses,\nthe extra per-coordinate communication cost in each round incurred due to\nprivacy by our method is $\\mathcal{O}(1)$ with respect to the model dimension,\nwhich is an improvement over prior work. Finally, we show the efficacy of our\nproposed method with experiments on fully-connected neural networks trained on\nCIFAR-10 and Fashion-MNIST.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 21:23:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Das", "Rudrajit", ""], ["Hashemi", "Abolfazl", ""], ["Sanghavi", "Sujay", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "2106.07155", "submitter": "Haibo Yang Mr", "authors": "Haibo Yang, Jia Liu, Elizabeth S. Bentley", "title": "CFedAvg: Achieving Efficient Communication and Fast Convergence in\n  Non-IID Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a prevailing distributed learning paradigm, where\na large number of workers jointly learn a model without sharing their training\ndata. However, high communication costs could arise in FL due to large-scale\n(deep) learning models and bandwidth-constrained connections. In this paper, we\nintroduce a communication-efficient algorithmic framework called CFedAvg for FL\nwith non-i.i.d. datasets, which works with general (biased or unbiased)\nSNR-constrained compressors. We analyze the convergence rate of CFedAvg for\nnon-convex functions with constant and decaying learning rates. The CFedAvg\nalgorithm can achieve an $\\mathcal{O}(1 / \\sqrt{mKT} + 1 / T)$ convergence rate\nwith a constant learning rate, implying a linear speedup for convergence as the\nnumber of workers increases, where $K$ is the number of local steps, $T$ is the\nnumber of total communication rounds, and $m$ is the total worker number. This\nmatches the convergence rate of distributed/federated learning without\ncompression, thus achieving high communication efficiency while not sacrificing\nlearning accuracy in FL. Furthermore, we extend CFedAvg to cases with\nheterogeneous local steps, which allows different workers to perform a\ndifferent number of local steps to better adapt to their own circumstances. The\ninteresting observation in general is that the noise/variance introduced by\ncompressors does not affect the overall convergence rate order for non-i.i.d.\nFL. We verify the effectiveness of our CFedAvg algorithm on three datasets with\ntwo gradient compression schemes of different compression ratios.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 04:27:19 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yang", "Haibo", ""], ["Liu", "Jia", ""], ["Bentley", "Elizabeth S.", ""]]}, {"id": "2106.07243", "submitter": "Zhuoqing Song", "authors": "Zhuoqing Song, Lei Shi, Shi Pu, Ming Yan", "title": "Compressed Gradient Tracking for Decentralized Optimization Over General\n  Directed Networks", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two communication-efficient algorithms for\ndecentralized optimization over a multi-agent network with general directed\nnetwork topology. In the first part, we consider a novel\ncommunication-efficient gradient tracking based method, termed Compressed\nPush-Pull (CPP), which combines the Push-Pull method with communication\ncompression. We show that CPP is applicable to a general class of unbiased\ncompression operators and achieves linear convergence for strongly convex and\nsmooth objective functions. In the second part, we propose a broadcast-like\nversion of CPP (B-CPP), which also achieves linear convergence rate under the\nsame conditions for the objective functions. B-CPP can be applied in an\nasynchronous broadcast setting and further reduce communication costs compared\nto CPP. Numerical experiments complement the theoretical analysis and confirm\nthe effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:53:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Song", "Zhuoqing", ""], ["Shi", "Lei", ""], ["Pu", "Shi", ""], ["Yan", "Ming", ""]]}, {"id": "2106.07289", "submitter": "Aleksandr Beznosikov", "authors": "Aleksandr Beznosikov and Vadim Sushko and Abdurakhmon Sadiev and\n  Alexander Gasnikov", "title": "Decentralized Personalized Federated Min-Max Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized Federated Learning has recently seen tremendous progress,\nallowing the design of novel machine learning applications preserving privacy\nof the data used for training. Existing theoretical results in this field\nmainly focus on distributed optimization under minimization problems. This\npaper is the first to study PFL for saddle point problems, which cover a\nbroader class of optimization tasks and are thus of more relevance for\napplications than the minimization. In this work, we consider a recently\nproposed PFL setting with the mixing objective function, an approach combining\nthe learning of a global model together with local distributed learners. Unlike\nmost of the previous papers, which considered only the centralized setting, we\nwork in a more general and decentralized setup. This allows to design and to\nanalyze more practical and federated ways to connect devices to the network. We\npresent two new algorithms for our problem. A theoretical analysis of the\nmethods is presented for smooth (strongly-)convex-(strongly-)concave saddle\npoint problems. We also demonstrate the effectiveness of our problem\nformulation and the proposed algorithms on experiments with neural networks\nwith adversarial noise.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 10:36:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Beznosikov", "Aleksandr", ""], ["Sushko", "Vadim", ""], ["Sadiev", "Abdurakhmon", ""], ["Gasnikov", "Alexander", ""]]}, {"id": "2106.07423", "submitter": "Saba Ahmadian", "authors": "Saba Ahmadian, Reza Salkhordeh, Onur Mutlu, Hossein Asadi", "title": "ETICA: Efficient Two-Level I/O Caching Architecture for Virtualized\n  Platforms", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems (Volume: 32,\n  Issue: 10, Oct. 1 2021)", "doi": "10.1109/TPDS.2021.3066308", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an Efficient Two-Level I/O Caching Architecture\n(ETICA) for virtualized platforms that can significantly improve I/O latency,\nendurance, and cost (in terms of cache size) while preserving the reliability\nof write-pending data blocks. As opposed to previous one-level I/O caching\nschemes in virtualized platforms, our proposed architecture 1) provides two\nlevels of cache by employing both Dynamic Random-Access Memory (DRAM) and SSD\nin the I/O caching layer of virtualized platforms and 2) effectively partitions\nthe cache space between running VMs to achieve maximum performance and minimum\ncache size. To manage the two-level cache, unlike the previous reuse distance\ncalculation schemes such as Useful Reuse Distance (URD), which only consider\nthe request type and neglect the impact of cache write policy, we propose a new\nmetric, Policy Optimized reuse Distance (POD). The key idea of POD is to\neffectively calculate the reuse distance and estimate the amount of two-level\nDRAM+SSD cache space to allocate by considering both 1) the request type and 2)\nthe cache write policy. Doing so results in enhanced performance and reduced\ncache size due to the allocation of cache blocks only for the requests that\nwould be served by the I/O cache. ETICA maintains the reliability of\nwrite-pending data blocks and improves performance by 1) assigning an effective\nand fixed write policy at each level of the I/O cache hierarchy and 2)\nemploying effective promotion and eviction methods between cache levels. Our\nextensive experiments conducted with a real implementation of the proposed\ntwo-level storage caching architecture show that ETICA provides 45% higher\nperformance, compared to the state-of-the-art caching schemes in virtualized\nplatforms, while improving both cache size and SSD endurance by 51.7% and\n33.8%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:39:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ahmadian", "Saba", ""], ["Salkhordeh", "Reza", ""], ["Mutlu", "Onur", ""], ["Asadi", "Hossein", ""]]}, {"id": "2106.07575", "submitter": "Xiaodong Yu", "authors": "Xiaodong Yu, Viktor Nikitin, Daniel J. Ching, Selin Aslan, Doga\n  Gursoy, Tekin Bicer", "title": "Scalable and accurate multi-GPU based image reconstruction of\n  large-scale ptychography data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.IV physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the advances in synchrotron light sources, together with the\ndevelopment of focusing optics and detectors, allow nanoscale ptychographic\nimaging of materials and biological specimens, the corresponding experiments\ncan yield terabyte-scale large volumes of data that can impose a heavy burden\non the computing platform. While Graphical Processing Units (GPUs) provide high\nperformance for such large-scale ptychography datasets, a single GPU is\ntypically insufficient for analysis and reconstruction. Several existing works\nhave considered leveraging multiple GPUs to accelerate the ptychographic\nreconstruction. However, they utilize only Message Passing Interface (MPI) to\nhandle the communications between GPUs. It poses inefficiency for the\nconfiguration that has multiple GPUs in a single node, especially while\nprocessing a single large projection, since it provides no optimizations to\nhandle the heterogeneous GPU interconnections containing both low-speed links,\ne.g., PCIe, and high-speed links, e.g., NVLink. In this paper, we provide a\nmulti-GPU implementation that can effectively solve large-scale ptychographic\nreconstruction problem with optimized performance on intra-node multi-GPU. We\nfocus on the conventional maximum-likelihood reconstruction problem using\nconjugate-gradient (CG) for the solution and propose a novel hybrid\nparallelization model to address the performance bottlenecks in CG solver.\nAccordingly, we develop a tool called PtyGer (Ptychographic GPU(multiple)-based\nreconstruction), implementing our hybrid parallelization model design. The\ncomprehensive evaluation verifies that PtyGer can fully preserve the original\nalgorithm's accuracy while achieving outstanding intra-node GPU scalability.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:30:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yu", "Xiaodong", ""], ["Nikitin", "Viktor", ""], ["Ching", "Daniel J.", ""], ["Aslan", "Selin", ""], ["Gursoy", "Doga", ""], ["Bicer", "Tekin", ""]]}, {"id": "2106.07578", "submitter": "Dimitrios Dimitriadis", "authors": "Dimitrios Dimitriadis, Kenichi Kumatani, Robert Gmyr, Yashesh Gaur and\n  Sefik Emre Eskimez", "title": "Dynamic Gradient Aggregation for Federated Domain Adaptation", "comments": "arXiv admin note: substantial text overlap with arXiv:2008.02452", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a new learning algorithm for Federated Learning (FL) is\nintroduced. The proposed scheme is based on a weighted gradient aggregation\nusing two-step optimization to offer a flexible training pipeline. Herein, two\ndifferent flavors of the aggregation method are presented, leading to an order\nof magnitude improvement in convergence speed compared to other distributed or\nFL training algorithms like BMUF and FedAvg. Further, the aggregation algorithm\nacts as a regularizer of the gradient quality. We investigate the effect of our\nFL algorithm in supervised and unsupervised Speech Recognition (SR) scenarios.\nThe experimental validation is performed based on three tasks: first, the\nLibriSpeech task showing a speed-up of 7x and 6% word error rate reduction\n(WERR) compared to the baseline results. The second task is based on session\nadaptation providing 20% WERR over a powerful LAS model. Finally, our\nunsupervised pipeline is applied to the conversational SR task. The proposed FL\nsystem outperforms the baseline systems in both convergence speed and overall\nmodel performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:34:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Dimitriadis", "Dimitrios", ""], ["Kumatani", "Kenichi", ""], ["Gmyr", "Robert", ""], ["Gaur", "Yashesh", ""], ["Eskimez", "Sefik Emre", ""]]}, {"id": "2106.07731", "submitter": "Burak Hasircioglu", "authors": "Burak Hasircioglu, Jesus Gomez-Vilardebo, Deniz Gunduz", "title": "Bivariate Polynomial Codes for Secure Distributed Matrix Multiplication", "comments": "arXiv admin note: text overlap with arXiv:2102.08304", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of secure distributed matrix multiplication. Coded\ncomputation has been shown to be an effective solution in distributed matrix\nmultiplication, both providing privacy against workers and boosting the\ncomputation speed by efficiently mitigating stragglers. In this work, we\npresent a non-direct secure extension of the recently introduced bivariate\npolynomial codes. Bivariate polynomial codes have been shown to be able to\nfurther speed up distributed matrix multiplication by exploiting the partial\nwork done by the stragglers rather than completely ignoring them while reducing\nthe upload communication cost and/or the workers' storage's capacity needs. We\nshow that, especially for upload communication or storage constrained settings,\nthe proposed approach reduces the average computation time of secure\ndistributed matrix multiplication compared to its competitors in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:57:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hasircioglu", "Burak", ""], ["Gomez-Vilardebo", "Jesus", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2106.07820", "submitter": "Zachary Charles", "authors": "Zachary Charles, Zachary Garrett, Zhouyuan Huo, Sergei Shmulyian,\n  Virginia Smith", "title": "On Large-Cohort Training for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning methods typically learn a model by iteratively sampling\nupdates from a population of clients. In this work, we explore how the number\nof clients sampled at each round (the cohort size) impacts the quality of the\nlearned model and the training dynamics of federated learning algorithms. Our\nwork poses three fundamental questions. First, what challenges arise when\ntrying to scale federated learning to larger cohorts? Second, what parallels\nexist between cohort sizes in federated learning and batch sizes in centralized\nlearning? Last, how can we design federated learning methods that effectively\nutilize larger cohort sizes? We give partial answers to these questions based\non extensive empirical evaluation. Our work highlights a number of challenges\nstemming from the use of larger cohorts. While some of these (such as\ngeneralization issues and diminishing returns) are analogs of large-batch\ntraining challenges, others (including training failures and fairness concerns)\nare unique to federated learning.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 00:40:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Charles", "Zachary", ""], ["Garrett", "Zachary", ""], ["Huo", "Zhouyuan", ""], ["Shmulyian", "Sergei", ""], ["Smith", "Virginia", ""]]}, {"id": "2106.07831", "submitter": "Yuan Lu", "authors": "Yingzi Gao, Yuan Lu, Zhenliang Lu, Qiang Tang, Jing Xu, Zhenfeng Zhang", "title": "Efficient Asynchronous Byzantine Agreement without Private Setups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For asynchronous binary agreement (ABA) with optimal resilience, prior\nprivate-setup free protocols (Cachin et al., CCS' 2002; Kokoris-Kogias et al.,\nCCS' 2020) incur $O({\\lambda}n^4)$ bits and $O(n^3)$ messages; for asynchronous\nmulti-valued agreement with external validity (VBA), Abraham et al. [2] very\nrecently gave the first elegant construction with $O(n^3)$ messages, relying on\npublic key infrastructure (PKI), but still costs $O({\\lambda} n^3 \\log n)$\nbits. We for the first time close the remaining efficiency gap, i.e., reducing\ntheir communication to $O({\\lambda} n^3)$ bits on average. At the core of our\ndesign, we give a systematic treatment of reasonably fair common randomness:\n  - We construct a reasonably fair common coin (Canetti and Rabin, STOC' 1993)\nin the asynchronous setting with PKI instead of private setup, using only\n$O({\\lambda} n^3)$ bit and constant asynchronous rounds. The common coin\nprotocol ensures that with at least 1/3 probability, all honest parties can\noutput a common bit that is as if uniformly sampled, rendering a more efficient\nprivate-setup free ABA with expected $O({\\lambda} n^3)$ bit communication and\nconstant running time.\n  - More interestingly, we lift our reasonably fair common coin protocol to\nattain perfect agreement without incurring any extra factor in the asymptotic\ncomplexities, resulting in an efficient reasonably fair leader election\nprimitive pluggable in all existing VBA protocols, thus reducing the\ncommunication of private-setup free VBA to expected $O({\\lambda} n^3)$ bits\nwhile preserving expected constant running time.\n  - Along the way, we improve an important building block, asynchronous\nverifiable secret sharing by presenting a private-setup free implementation\ncosting only $O({\\lambda} n^2)$ bits in the PKI setting. By contrast, prior art\nhaving the same complexity (Backes et al., CT-RSA' 2013) has to rely on a\nprivate setup.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 01:34:11 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gao", "Yingzi", ""], ["Lu", "Yuan", ""], ["Lu", "Zhenliang", ""], ["Tang", "Qiang", ""], ["Xu", "Jing", ""], ["Zhang", "Zhenfeng", ""]]}, {"id": "2106.07894", "submitter": "Jianlei Yang", "authors": "Jianlei Yang, Wenzhi Fu, Xingzhou Cheng, Xucheng Ye, Pengcheng Dai,\n  and Weisheng Zhao", "title": "S2Engine: A Novel Systolic Architecture for Sparse Convolutional Neural\n  Networks", "comments": "13 pages, 17 figures", "journal-ref": "IEEE Transactions on Computers, 2021", "doi": "10.1109/TC.2021.3087946", "report-no": null, "categories": "cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved great success in\nperforming cognitive tasks. However, execution of CNNs requires a large amount\nof computing resources and generates heavy memory traffic, which imposes a\nsevere challenge on computing system design. Through optimizing parallel\nexecutions and data reuse in convolution, systolic architecture demonstrates\ngreat advantages in accelerating CNN computations. However, regular internal\ndata transmission path in traditional systolic architecture prevents the\nsystolic architecture from completely leveraging the benefits introduced by\nneural network sparsity. Deployment of fine-grained sparsity on the existing\nsystolic architectures is greatly hindered by the incurred computational\noverheads. In this work, we propose S2Engine $-$ a novel systolic architecture\nthat can fully exploit the sparsity in CNNs with maximized data reuse. S2Engine\ntransmits compressed data internally and allows each processing element to\ndynamically select an aligned data from the compressed dataflow in convolution.\nCompared to the naive systolic array, S2Engine achieves about $3.2\\times$ and\nabout $3.0\\times$ improvements on speed and energy efficiency, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:08:37 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Yang", "Jianlei", ""], ["Fu", "Wenzhi", ""], ["Cheng", "Xingzhou", ""], ["Ye", "Xucheng", ""], ["Dai", "Pengcheng", ""], ["Zhao", "Weisheng", ""]]}, {"id": "2106.08026", "submitter": "Daniel Goodman Dr", "authors": "Daniel Goodman, Roni Haecki, Tim Harris", "title": "Modeling memory bandwidth patterns on NUMA machines with performance\n  counters", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computers used for data analytics are often NUMA systems with multiple\nsockets per machine, multiple cores per socket, and multiple thread contexts\nper core. To get the peak performance out of these machines requires the\ncorrect number of threads to be placed in the correct positions on the machine.\nOne particularly interesting element of the placement of memory and threads is\nthe way it effects the movement of data around the machine, and the increased\nlatency this can introduce to reads and writes. In this paper we describe work\non modeling the bandwidth requirements of an application on a NUMA compute node\nbased on the placement of threads. The model is parameterized by sampling\nperformance counters during 2 application runs with carefully chosen thread\nplacements. Evaluating the model with thousands of measurements shows a median\ndifference from predictions of 2.34% of the bandwidth. The results of this\nmodeling can be used in a number of ways varying from: Performance debugging\nduring development where the programmer can be alerted to potentially\nproblematic memory access patterns; To systems such as Pandia which take an\napplication and predict the performance and system load of a proposed thread\ncount and placement; To libraries of data structures such as Parallel\nCollections and Smart Arrays that can abstract from the user memory placement\nand thread placement issues when parallelizing code.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:20:00 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Goodman", "Daniel", ""], ["Haecki", "Roni", ""], ["Harris", "Tim", ""]]}, {"id": "2106.08114", "submitter": "Kexin Hu", "authors": "Kexin Hu, Kaiwen Guo, Qiang Tang, Zhenfeng Zhang, Hao Cheng, Zhiyang\n  Zhao", "title": "Leopard: Scaling BFT without Sacrificing Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of large-scale decentralized applications, a scalable and\nefficient Byzantine Fault Tolerant (BFT) protocol of hundreds of replicas is\ndesirable. Although the throughput of existing leader-based BFT protocols has\nreached a high level of $10^5$ requests per second for a small scale of\nreplicas, it drops significantly when the number of replicas increases, which\nleads to a lack of practicality. This paper focuses on the scalability of BFT\nprotocols and identifies a major bottleneck to leader-based BFT protocols due\nto the excessive workload of the leader at large scales. A new metric of\nscaling factor is defined to capture whether a BFT protocol will get stuck when\nit scales out, which can be used to measure the performance of efficiency and\nscalability of BFT protocols. We propose \"Leopard\", the first leader-based BFT\nprotocol that scales to multiple hundreds of replicas, and more importantly,\npreserves a high efficiency. We remove the bottleneck by introducing a\ntechnique of achieving a constant scaling factor, which takes full advantage of\nthe idle resource and adaptively balances the workload of the leader among all\nreplicas. We implement Leopard and evaluate its performance compared to\nHotStuff, the state-of-the-art BFT protocol. We run extensive experiments on\nthe two systems with up to 600 replicas. The results show that Leopard achieves\nsignificant performance improvements both on throughput and scalability. In\nparticular, the throughput of Leopard remains at a high level of $10^5$ when\nthe system scales out to 600 replicas. It achieves a $5\\times$ throughput over\nHotStuff when the scale is 300 (which is already the largest scale we can see\nthe progress of the latter in our experiments), and the gap becomes wider as\nthe number of replicas further increases.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:16:48 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 03:46:43 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 07:42:55 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Hu", "Kexin", ""], ["Guo", "Kaiwen", ""], ["Tang", "Qiang", ""], ["Zhang", "Zhenfeng", ""], ["Cheng", "Hao", ""], ["Zhao", "Zhiyang", ""]]}, {"id": "2106.08150", "submitter": "Robin Kobus", "authors": "Robin Kobus (1), Andr\\'e M\\\"uller (1), Daniel J\\\"unger (1), Christian\n  Hundt (2) and Bertil Schmidt (1) ((1) Johannes Gutenberg University Mainz,\n  Germany, (2) NVIDIA AI Technology Center Luxembourg)", "title": "MetaCache-GPU: Ultra-Fast Metagenomic Classification", "comments": "11 pages. To be published in ICPP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of DNA sequencing has dropped exponentially over the past decade,\nmaking genomic data accessible to a growing number of scientists. In\nbioinformatics, localization of short DNA sequences (reads) within large\ngenomic sequences is commonly facilitated by constructing index data structures\nwhich allow for efficient querying of substrings. Recent metagenomic\nclassification pipelines annotate reads with taxonomic labels by analyzing\ntheir $k$-mer histograms with respect to a reference genome database. CPU-based\nindex construction is often performed in a preprocessing phase due to the\nrelatively high cost of building irregular data structures such as hash maps.\nHowever, the rapidly growing amount of available reference genomes establishes\nthe need for index construction and querying at interactive speeds. In this\npaper, we introduce MetaCache-GPU -- an ultra-fast metagenomic short read\nclassifier specifically tailored to fit the characteristics of CUDA-enabled\naccelerators. Our approach employs a novel hash table variant featuring\nefficient minhash fingerprinting of reads for locality-sensitive hashing and\ntheir rapid insertion using warp-aggregated operations. Our performance\nevaluation shows that MetaCache-GPU is able to build large reference databases\nin a matter of seconds, enabling instantaneous operability, while popular\nCPU-based tools such as Kraken2 require over an hour for index construction on\nthe same data. In the context of an ever-growing number of reference genomes,\nMetaCache-GPU is the first metagenomic classifier that makes analysis pipelines\nwith on-demand composition of large-scale reference genome sets practical. The\nsource code is publicly available at https://github.com/muellan/metacache .\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:31:07 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kobus", "Robin", ""], ["M\u00fcller", "Andr\u00e9", ""], ["J\u00fcnger", "Daniel", ""], ["Hundt", "Christian", ""], ["Schmidt", "Bertil", ""]]}, {"id": "2106.08167", "submitter": "Duy Thanh Nguyen", "authors": "Duy Thanh Nguyen, Hyeonseung Je, Tuan Nghia Nguyen, Soojung Ryu,\n  Kyujung Lee, and Hyuk-Jae Lee", "title": "ShortcutFusion: From Tensorflow to FPGA-based accelerator with\n  reuse-aware memory allocation for shortcut data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Residual block is a very common component in recent state-of-the art CNNs\nsuch as EfficientNet or EfficientDet. Shortcut data accounts for nearly 40% of\nfeature-maps access in ResNet152 [8]. Most of the previous DNN compilers,\naccelerators ignore the shortcut data optimization. This paper presents\nShortcutFusion, an optimization tool for FPGA-based accelerator with a\nreuse-aware static memory allocation for shortcut data, to maximize on-chip\ndata reuse given resource constraints. From TensorFlow DNN models, the proposed\ndesign generates instruction sets for a group of nodes which uses an optimized\ndata reuse for each residual block. The accelerator design implemented on the\nXilinx KCU1500 FPGA card significantly outperforms NVIDIA RTX 2080 Ti, Titan\nXp, and GTX 1080 Ti for the EfficientNet inference. Compared to RTX 2080 Ti,\nthe proposed design is 1.35-2.33x faster and 6.7-7.9x more power efficient.\nCompared to the result from baseline, in which the weights, inputs, and outputs\nare accessed from the off-chip memory exactly once per each layer,\nShortcutFusion reduces the DRAM access by 47.8-84.8% for RetinaNet, Yolov3,\nResNet152, and EfficientNet. Given a similar buffer size to ShortcutMining [8],\nwhich also mine the shortcut data in hardware, the proposed work reduces\noff-chip access for feature-maps 5.27x while accessing weight from off-chip\nmemory exactly once.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:10:10 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 10:59:04 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nguyen", "Duy Thanh", ""], ["Je", "Hyeonseung", ""], ["Nguyen", "Tuan Nghia", ""], ["Ryu", "Soojung", ""], ["Lee", "Kyujung", ""], ["Lee", "Hyuk-Jae", ""]]}, {"id": "2106.08290", "submitter": "Elahe Vedadi", "authors": "Elahe Vedadi, Yasaman Keshtkarjahromi, Hulya Seferoglu", "title": "Coded Privacy-Preserving Computation at Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-party computation (MPC) is promising for privacy-preserving machine\nlearning algorithms at edge networks, like federated learning. Despite their\npotential, existing MPC algorithms fail short of adapting to the limited\nresources of edge devices. A promising solution, and the focus of this work, is\ncoded computation, which advocates the use of error-correcting codes to improve\nthe performance of distributed computing through \"smart\" data redundancy. In\nthis paper, we focus on coded privacy-preserving computation using Shamir's\nsecret sharing. In particular, we design novel coded privacy-preserving\ncomputation mechanisms; MatDot coded MPC (MatDot-CMPC) and PolyDot coded MPC\n(PolyDot-CMPC) by employing recently proposed coded computation algorithms;\nMatDot and PolyDot. We take advantage of the \"garbage terms\" that naturally\narise when polynomials are constructed in the design of MatDot-CMPC and\nPolyDot-CMPC to reduce the number of workers needed for privacy-preserving\ncomputation. Also, we analyze MatDot-CMPC and PolyDot-CMPC in terms of their\ncomputation, storage, communication overhead as well as recovery threshold, so\nthey can easily adapt to the limited resources of edge devices.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:04:03 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 19:01:28 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 21:52:19 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Vedadi", "Elahe", ""], ["Keshtkarjahromi", "Yasaman", ""], ["Seferoglu", "Hulya", ""]]}, {"id": "2106.08315", "submitter": "Aleksandr Beznosikov", "authors": "Aleksandr Beznosikov, Pavel Dvurechensky, Anastasia Koloskova,\n  Valentin Samokhin, Sebastian U Stich, Alexander Gasnikov", "title": "Decentralized Local Stochastic Extra-Gradient for Variational\n  Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider decentralized stochastic variational inequalities where the\nproblem data is distributed across many participating devices (heterogeneous,\nor non-IID data setting). We propose a novel method - based on stochastic\nextra-gradient - where participating devices can communicate over arbitrary,\npossibly time-varying network topologies. This covers both the fully\ndecentralized optimization setting and the centralized topologies commonly used\nin Federated Learning. Our method further supports multiple local updates on\nthe workers for reducing the communication frequency between workers. We\ntheoretically analyze the proposed scheme in the strongly monotone, monotone\nand non-monotone setting. As a special case, our method and analysis apply in\nparticular to decentralized stochastic min-max problems which are being studied\nwith increased interest in Deep Learning. For example, the training objective\nof Generative Adversarial Networks (GANs) are typically saddle point problems\nand the decentralized training of GANs has been reported to be extremely\nchallenging. While SOTA techniques rely on either repeated gossip rounds or\nproximal updates, we alleviate both of these requirements. Experimental results\nfor decentralized GAN demonstrate the effectiveness of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:45:51 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Beznosikov", "Aleksandr", ""], ["Dvurechensky", "Pavel", ""], ["Koloskova", "Anastasia", ""], ["Samokhin", "Valentin", ""], ["Stich", "Sebastian U", ""], ["Gasnikov", "Alexander", ""]]}, {"id": "2106.08318", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Dimitrios Vytiniotis and Grzegorz Swirszcz and\n  Viorica Patraucean and Joao Carreira", "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling", "comments": "Accepted to CVPR 2021. arXiv admin note: text overlap with\n  arXiv:2001.06232", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:50:22 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:52:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Vytiniotis", "Dimitrios", ""], ["Swirszcz", "Grzegorz", ""], ["Patraucean", "Viorica", ""], ["Carreira", "Joao", ""]]}, {"id": "2106.08448", "submitter": "Jakub Tarnawski", "authors": "Vincent Cohen-Addad, Silvio Lattanzi, Slobodan Mitrovi\\'c, Ashkan\n  Norouzi-Fard, Nikos Parotsidis, Jakub Tarnawski", "title": "Correlation Clustering in Constant Many Parallel Rounds", "comments": "ICML 2021 (long talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Correlation clustering is a central topic in unsupervised learning, with many\napplications in ML and data mining. In correlation clustering, one receives as\ninput a signed graph and the goal is to partition it to minimize the number of\ndisagreements. In this work we propose a massively parallel computation (MPC)\nalgorithm for this problem that is considerably faster than prior work. In\nparticular, our algorithm uses machines with memory sublinear in the number of\nnodes in the graph and returns a constant approximation while running only for\na constant number of rounds. To the best of our knowledge, our algorithm is the\nfirst that can provably approximate a clustering problem on graphs using only a\nconstant number of MPC rounds in the sublinear memory regime. We complement our\nanalysis with an experimental analysis of our techniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 21:45:45 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Lattanzi", "Silvio", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Norouzi-Fard", "Ashkan", ""], ["Parotsidis", "Nikos", ""], ["Tarnawski", "Jakub", ""]]}, {"id": "2106.08545", "submitter": "Shuo Liu", "authors": "Shuo Liu", "title": "A Survey on Fault-tolerance in Distributed Optimization and Machine\n  Learning", "comments": "35 pages, 1 figure, and 2 tables; Fixed a missing citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness of distributed optimization is an emerging field of study,\nmotivated by various applications of distributed optimization including\ndistributed machine learning, distributed sensing, and swarm robotics. With the\nrapid expansion of the scale of distributed systems, resilient distributed\nalgorithms for optimization are needed, in order to mitigate system failures,\ncommunication issues, or even malicious attacks. This survey investigates the\ncurrent state of fault-tolerance research in distributed optimization, and aims\nto provide an overview of the existing studies on both fault-tolerant\ndistributed optimization theories and applicable algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 04:12:01 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 03:40:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Shuo", ""]]}, {"id": "2106.08676", "submitter": "Athanasios Xygkis", "authors": "Rachid Guerraoui and Antoine Murat and Athanasios Xygkis", "title": "Velos: One-sided Paxos for RDMA applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers are becoming increasingly equipped with RDMA-capable\nNICs. These devices enable distributed systems to rely on algorithms designed\nfor shared memory. RDMA allows consensus to terminate within a few microsecond\nin failure-free scenarios, yet, RDMA-optimized algorithms still use expensive\ntwo-sided operations in case of failure. In this work, we present a new\nleader-based consensus algorithm that relies solely on one-sided RDMA verbs.\nOur algorithm is based on Paxos, it decides in a single one-sided RDMA\noperation in the common case, and changes leader also in a single one-sided\nRDMA operation in case of failure. We implement our algorithm in the form of an\nSMR system named Velos, and we evaluated our system against the\nstate-of-the-art competitor Mu. Compared to Mu, our solution adds a small\noverhead of approximately 0.6 microseconds in failure-free executions and\nshines during failover periods during which it is 13 times faster in changing\nleader.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:28:31 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Murat", "Antoine", ""], ["Xygkis", "Athanasios", ""]]}, {"id": "2106.08800", "submitter": "Ebrahim Farahmand", "authors": "Ebrahim Farahmand, Ali Mahani, Muhammad Abdullah Hanif, Muhammad\n  Shafique", "title": "High Performance and Optimal Configuration of Accurate Heterogeneous\n  Block-Based Approximate Adder", "comments": "Submitted to the IEEE-TCAD journal, 16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Approximate computing is an emerging paradigm to improve power and\nperformance efficiency for error-resilient application. Recent approximate\nadders have significantly extended the design space of accuracy-power\nconfigurable approximate adders, and find optimal designs by exploring the\ndesign space. In this paper, a new energy-efficient heterogeneous block-based\napproximate adder (HBBA) is proposed; which is a generic/configurable model\nthat can be transformed to a particular adder by defining some configurations.\nAn HBBA, in general, is composed of heterogeneous sub-adders, where each\nsub-adder can have a different configuration. A set of configurations of all\nthe sub-adders in an HBBA defines its configuration. The block-based adders are\napproximated through inexact logic configuration and truncated carry chains.\nHBBA increases design space providing additional design points that fall on the\nPareto-front and offer better power-accuracy trade-off compared to other\nconfigurations. Furthermore, to avoid Mont-Carlo simulations, we propose an\nanalytical modelling technique to evaluate the probability of error and\nProbability Mass Function (PMF) of error value. Moreover, the estimation method\nestimates delay, area and power of heterogeneous block-based approximate\nadders. Thus, based on the analytical model and estimation method, the optimal\nconfiguration under a given error constraint can be selected from the whole\ndesign space of the proposed adder model by exhaustive search. The simulation\nresults show that our HBBA provides improved accuracy in terms of error metrics\ncompared to some state-of-the-art approximate adders. HBBA with 32 bits length\nserves about 15% reduction in area and up to 17% reduction in energy compared\nto state-of-the-art approximate adders.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:03:49 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Farahmand", "Ebrahim", ""], ["Mahani", "Ali", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2106.08872", "submitter": "Noman Bashir", "authors": "Noman Bashir and Tian Guo and Mohammad Hajiesmaili and David Irwin and\n  Prashant Shenoy and Ramesh Sitaraman and Abel Souza and Adam Wierman", "title": "Enabling Sustainable Clouds: The Case for Virtualizing the Energy System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud platforms' growing energy demand and carbon emissions are raising\nconcern about their environmental sustainability. The current approach to\nenabling sustainable clouds focuses on improving energy-efficiency and\npurchasing carbon offsets. These approaches have limits: many cloud data\ncenters already operate near peak efficiency, and carbon offsets cannot scale\nto near zero carbon where there is little carbon left to offset. Instead,\nenabling sustainable clouds will require applications to adapt to when and\nwhere unreliable low-carbon energy is available. Applications cannot do this\ntoday because their energy use and carbon emissions are not visible to them, as\nthe energy system provides the rigid abstraction of a continuous, reliable\nenergy supply. This vision paper instead advocates for a ``carbon first''\napproach to cloud design that elevates carbon-efficiency to a first-class\nmetric. To do so, we argue that cloud platforms should virtualize the energy\nsystem by exposing visibility into, and software-defined control of, it to\napplications, enabling them to define their own abstractions for managing\nenergy and carbon emissions based on their own requirements.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:45:13 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Bashir", "Noman", ""], ["Guo", "Tian", ""], ["Hajiesmaili", "Mohammad", ""], ["Irwin", "David", ""], ["Shenoy", "Prashant", ""], ["Sitaraman", "Ramesh", ""], ["Souza", "Abel", ""], ["Wierman", "Adam", ""]]}, {"id": "2106.08882", "submitter": "Abolfazl Hashemi", "authors": "Anish Acharya, Abolfazl Hashemi, Prateek Jain, Sujay Sanghavi,\n  Inderjit S. Dhillon, Ufuk Topcu", "title": "Robust Training in High Dimensions via Block Coordinate Geometric Median\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geometric median (\\textsc{Gm}) is a classical method in statistics for\nachieving a robust estimation of the uncorrupted data; under gross corruption,\nit achieves the optimal breakdown point of 0.5. However, its computational\ncomplexity makes it infeasible for robustifying stochastic gradient descent\n(SGD) for high-dimensional optimization problems. In this paper, we show that\nby applying \\textsc{Gm} to only a judiciously chosen block of coordinates at a\ntime and using a memory mechanism, one can retain the breakdown point of 0.5\nfor smooth non-convex problems, with non-asymptotic convergence rates\ncomparable to the SGD with \\textsc{Gm}.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:55:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Acharya", "Anish", ""], ["Hashemi", "Abolfazl", ""], ["Jain", "Prateek", ""], ["Sanghavi", "Sujay", ""], ["Dhillon", "Inderjit S.", ""], ["Topcu", "Ufuk", ""]]}, {"id": "2106.08938", "submitter": "Anshul Jindal", "authors": "Anshul Jindal, Paul Staab, Pooja Kulkarni, Jorge Cardoso, Michael\n  Gerndt and Vladimir Podolskiy", "title": "Memory Leak Detection Algorithms in the Cloud-based Infrastructure", "comments": "10. pages. arXiv admin note: substantial text overlap with\n  arXiv:2101.09799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A memory leak in an application deployed on the cloud can affect the\navailability and reliability of the application. Therefore, identifying and\nultimately resolve it quickly is highly important. However, in the production\nenvironment running on the cloud, memory leak detection is a challenge without\nthe knowledge of the application or its internal object allocation details.\n  This paper addresses this challenge of detection of memory leaks in\ncloud-based infrastructure without having any internal knowledge by introducing\ntwo novel machine learning-based algorithms: Linear Backward Regression (LBR)\nand Precog and, their two variants: Linear Backward Regression with Change\nPoints Detection (LBRCPD) and Precog with Maximum Filteration (PrecogMF). These\nalgorithms only use one metric i.e the system's memory utilization on which the\napplication is deployed for detection of a memory leak. The developed\nalgorithm's accuracy was tested on 60 virtual machines manually labeled memory\nutilization data and it was found that the proposed PrecogMF algorithm achieves\nthe highest accuracy score of 85%. The same algorithm also achieves this by\ndecreasing the overall compute time by 80% when compared to LBR's compute time.\n  The paper also presents the different memory leak patterns found in the\nvarious memory leak applications and are further classified into different\nclasses based on their visual representation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:48:45 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Jindal", "Anshul", ""], ["Staab", "Paul", ""], ["Kulkarni", "Pooja", ""], ["Cardoso", "Jorge", ""], ["Gerndt", "Michael", ""], ["Podolskiy", "Vladimir", ""]]}, {"id": "2106.08987", "submitter": "Eva Siegmann", "authors": "Andrew Burford, Alan C. Calder, David Carlson, Barbara Chapman, Firat\n  Co\\c{S}Kun, Tony Curtis, Catherine Feldman, Robert J. Harrison, Yan Kang,\n  Benjamin Michalow-Icz, Eric Raut, Eva Siegmann, Daniel G. Wood, Robert L.\n  Deleon, Mathew Jones, Nikolay A. Simakov, Joseph P. White, Dossay Oryspayev", "title": "Ookami: Deployment and Initial Experiences", "comments": "14 pages, 7 figures, PEARC '21: Practice and Experience in Advanced\n  Research Computing, July 18--22, 2021, Boston, MA, USA", "journal-ref": null, "doi": "10.1145/3437359.3465578", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ookami is a computer technology testbed supported by the United States\nNational Science Foundation. It provides researchers with access to the A64FX\nprocessor developed by Fujitsu in collaboration with RIK{\\Xi}N for the Japanese\npath to exascale computing, as deployed in Fugaku, the fastest computer in the\nworld. By focusing on crucial architectural details, the ARM-based, multi-core,\n512-bit SIMD-vector processor with ultrahigh-bandwidth memory promises to\nretain familiar and successful programming models while achieving very high\nperformance for a wide range of applications. We review relevant technology and\nsystem details, and the main body of the paper focuses on initial experiences\nwith the hardware and software ecosystem for micro-benchmarks, mini-apps, and\nfull applications, and starts to answer questions about where such technologies\nfit into the NSF ecosystem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:37:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Burford", "Andrew", ""], ["Calder", "Alan C.", ""], ["Carlson", "David", ""], ["Chapman", "Barbara", ""], ["Co\u015eKun", "Firat", ""], ["Curtis", "Tony", ""], ["Feldman", "Catherine", ""], ["Harrison", "Robert J.", ""], ["Kang", "Yan", ""], ["Michalow-Icz", "Benjamin", ""], ["Raut", "Eric", ""], ["Siegmann", "Eva", ""], ["Wood", "Daniel G.", ""], ["Deleon", "Robert L.", ""], ["Jones", "Mathew", ""], ["Simakov", "Nikolay A.", ""], ["White", "Joseph P.", ""], ["Oryspayev", "Dossay", ""]]}, {"id": "2106.09109", "submitter": "Qun Li", "authors": "Qi Xia, Qun Li", "title": "QuantumFed: A Federated Learning Framework for Collaborative Quantum\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast development of quantum computing and deep learning, quantum\nneural networks have attracted great attention recently. By leveraging the\npower of quantum computing, deep neural networks can potentially overcome\ncomputational power limitations in classic machine learning. However, when\nmultiple quantum machines wish to train a global model using the local data on\neach machine, it may be very difficult to copy the data into one machine and\ntrain the model. Therefore, a collaborative quantum neural network framework is\nnecessary. In this article, we borrow the core idea of federated learning to\npropose QuantumFed, a quantum federated learning framework to have multiple\nquantum nodes with local quantum data train a mode together. Our experiments\nshow the feasibility and robustness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:28:11 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 00:53:37 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Xia", "Qi", ""], ["Li", "Qun", ""]]}, {"id": "2106.09349", "submitter": "Amirmohammad Pasdar", "authors": "Amirmohammad Pasdar, Zhongli Dong and Young Choon Lee", "title": "Blockchain Oracle Design Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain is a form of distributed ledger technology (DLT) where data is\nshared among users connected over the internet. Transactions are data state\nchanges on the blockchain that are permanently recorded in a secure and\ntransparent way without the need of a third party. Besides, the introduction of\nsmart contracts to the blockchain has added programmability to the blockchain\nand revolutionized the software ecosystem leading toward decentralized\napplications (DApps) attracting businesses and organizations to employ this\ntechnology. Although promising, blockchains and smart contracts have no access\nto the external systems (i.e., off-chain) where real-world data and events\nresides; consequently, the usability of smart contracts in terms of performance\nand programmability would be limited to the on-chain data. Hence,\n\\emph{blockchain oracles} are introduced to mitigate the issue and are defined\nas trusted third-party services that send and verify the external information\n(i.e., feedback) and submit it to smart contracts for triggering state changes\nin the blockchain. In this paper, we will study and analyze blockchain oracles\nwith regard to how they provide feedback to the blockchain and smart contracts.\nWe classify the blockchain oracle techniques into two major groups such as\nvoting-based strategies and reputation-based ones. The former mainly relies on\nparticipants' stakes for outcome finalization while the latter considers\nreputation in conjunction with authenticity proof mechanisms for data\ncorrectness and integrity. We then provide a structured description of patterns\nin detail for each classification and discuss research directions in the end.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:08:49 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Pasdar", "Amirmohammad", ""], ["Dong", "Zhongli", ""], ["Lee", "Young Choon", ""]]}, {"id": "2106.09485", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u, Matthieu Bloch, and Rafael F. Schaefer", "title": "Secure Multi-Function Computation with Private Remote Sources", "comments": "Shorter version to appear in the IEEE International Symposium on\n  Information Theory 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC cs.LG eess.SP math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a distributed function computation problem in which parties\nobserving noisy versions of a remote source facilitate the computation of a\nfunction of their observations at a fusion center through public communication.\nThe distributed function computation is subject to constraints, including not\nonly reliability and storage but also privacy and secrecy. Specifically, 1) the\nremote source should remain private from an eavesdropper and the fusion center,\nmeasured in terms of the information leaked about the remote source; 2) the\nfunction computed should remain secret from the eavesdropper, measured in terms\nof the information leaked about the arguments of the function, to ensure\nsecrecy regardless of the exact function used. We derive the exact rate regions\nfor lossless and lossy single-function computation and illustrate the lossy\nsingle-function computation rate region for an information bottleneck example,\nin which the optimal auxiliary random variables are characterized for\nbinary-input symmetric-output channels. We extend the approach to lossless and\nlossy asynchronous multiple-function computations with joint secrecy and\nprivacy constraints, in which case inner and outer bounds for the rate regions\ndiffering only in the Markov chain conditions imposed are characterized.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:34:40 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Bloch", "Matthieu", ""], ["Schaefer", "Rafael F.", ""]]}, {"id": "2106.09580", "submitter": "Kate Donahue", "authors": "Kate Donahue and Jon Kleinberg", "title": "Optimality and Stability in Federated Learning: A Game-theoretic\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed learning paradigm where multiple agents,\neach only with access to local data, jointly learn a global model. There has\nrecently been an explosion of research aiming not only to improve the accuracy\nrates of federated learning, but also provide certain guarantees around social\ngood properties such as total error. One branch of this research has taken a\ngame-theoretic approach, and in particular, prior work has viewed federated\nlearning as a hedonic game, where error-minimizing players arrange themselves\ninto federating coalitions. This past work proves the existence of stable\ncoalition partitions, but leaves open a wide range of questions, including how\nfar from optimal these stable solutions are. In this work, we motivate and\ndefine a notion of optimality given by the average error rates among federating\nagents (players). First, we provide and prove the correctness of an efficient\nalgorithm to calculate an optimal (error minimizing) arrangement of players.\nNext, we analyze the relationship between the stability and optimality of an\narrangement. First, we show that for some regions of parameter space, all\nstable arrangements are optimal (Price of Anarchy equal to 1). However, we show\nthis is not true for all settings: there exist examples of stable arrangements\nwith higher cost than optimal (Price of Anarchy greater than 1). Finally, we\ngive the first constant-factor bound on the performance gap between stability\nand optimality, proving that the total error of the worst stable solution can\nbe no higher than 9 times the total error of an optimal solution (Price of\nAnarchy bound of 9).\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:03:51 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Donahue", "Kate", ""], ["Kleinberg", "Jon", ""]]}, {"id": "2106.09922", "submitter": "Tomohiro Harada", "authors": "Tomohiro Harada and Enrique Alba and Gabriel Luque", "title": "A Fresh Approach to Evaluate Performance in Distributed Parallel Genetic\n  Algorithms", "comments": "22 pages, submitted to Applied Soft Computing and under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work proposes a novel approach to evaluate and analyze the behavior of\nmulti-population parallel genetic algorithms (PGAs) when running on a cluster\nof multi-core processors. In particular, we deeply study their numerical and\ncomputational behavior by proposing a mathematical model representing the\nobserved performance curves. In them, we discuss the emerging mathematical\ndescriptions of PGA performance instead of, e.g., individual isolated results\nsubject to visual inspection, for a better understanding of the effects of the\nnumber of cores used (scalability), their migration policy (the migration gap,\nin this paper), and the features of the solved problem (type of encoding and\nproblem size). The conclusions based on the real figures and the numerical\nmodels fitting them represent a fresh way of understanding their speed-up,\nrunning time, and numerical effort, allowing a comparison based on a few\nmeaningful numeric parameters. This represents a set of conclusions beyond the\nusual textual lessons found in past works on PGAs. It can be used as an\nestimation tool for the future performance of the algorithms and a way of\nfinding out their limitations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 05:07:14 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Harada", "Tomohiro", ""], ["Alba", "Enrique", ""], ["Luque", "Gabriel", ""]]}, {"id": "2106.10022", "submitter": "Li Shen", "authors": "Luofeng Liao, Li Shen, Jia Duan, Mladen Kolar, Dacheng Tao", "title": "Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax\n  Problems", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large scale convex-concave minimax problems arise in numerous applications,\nincluding game theory, robust training, and training of generative adversarial\nnetworks. Despite their wide applicability, solving such problems efficiently\nand effectively is challenging in the presence of large amounts of data using\nexisting stochastic minimax methods. We study a class of stochastic minimax\nmethods and develop a communication-efficient distributed stochastic\nextragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable\nfor solving convex-concave minimax problem in the Parameter-Server model.\nLocalAdaSEG has three main features: (i) periodic communication strategy\nreduces the communication cost between workers and the server; (ii) an adaptive\nlearning rate that is computed locally and allows for tuning-free\nimplementation; and (iii) theoretically, a nearly linear speed-up with respect\nto the dominant variance term, arising from estimation of the stochastic\ngradient, is proven in both the smooth and nonsmooth convex-concave settings.\nLocalAdaSEG is used to solve a stochastic bilinear game, and train generative\nadversarial network. We compare LocalAdaSEG against several existing optimizers\nfor minimax problems and demonstrate its efficacy through several experiments\nin both the homogeneous and heterogeneous settings.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 09:42:05 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Liao", "Luofeng", ""], ["Shen", "Li", ""], ["Duan", "Jia", ""], ["Kolar", "Mladen", ""], ["Tao", "Dacheng", ""]]}, {"id": "2106.10051", "submitter": "HyungSeon Oh", "authors": "HyungSeon Oh", "title": "Distributed optimal power flow", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0251948", "report-no": null, "categories": "math.OC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstract Objective: The objectives of this paper are to 1) construct a new\nnetwork model compatible with distributed computation, 2) construct the full\noptimal power flow (OPF) in a distributed fashion so that an effective,\nnon-inferior solution can be found, and 3) develop a scalable algorithm that\nguarantees the convergence to a local minimum.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:55:41 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Oh", "HyungSeon", ""]]}, {"id": "2106.10196", "submitter": "Junyuan Hong", "authors": "Junyuan Hong, Haotao Wang, Zhangyang Wang, Jiayu Zhou", "title": "Federated Robustness Propagation: Sharing Adversarial Robustness in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) emerges as a popular distributed learning schema that\nlearns a model from a set of participating users without requiring raw data to\nbe shared. One major challenge of FL comes from heterogeneity in users, which\nmay have distributionally different (or non-iid) data and varying computation\nresources. Just like in centralized learning, FL users also desire model\nrobustness against malicious attackers at test time. Whereas adversarial\ntraining (AT) provides a sound solution for centralized learning, extending its\nusage for FL users has imposed significant challenges, as many users may have\nvery limited training data as well as tight computational budgets, to afford\nthe data-hungry and costly AT. In this paper, we study a novel learning setting\nthat propagates adversarial robustness from high-resource users that can afford\nAT, to those low-resource users that cannot afford it, during the FL process.\nWe show that existing FL techniques cannot effectively propagate adversarial\nrobustness among non-iid users, and propose a simple yet effective propagation\napproach that transfers robustness through carefully designed\nbatch-normalization statistics. We demonstrate the rationality and\neffectiveness of our method through extensive experiments. Especially, the\nproposed method is shown to grant FL remarkable robustness even when only a\nsmall portion of users afford AT during learning. Codes will be published upon\nacceptance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:52:33 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Hong", "Junyuan", ""], ["Wang", "Haotao", ""], ["Wang", "Zhangyang", ""], ["Zhou", "Jiayu", ""]]}, {"id": "2106.10201", "submitter": "David Doty", "authors": "David Doty, Mahsa Eftekhari, Leszek G\\k{a}sieniec, Eric Severson,\n  Grzegorz Stachowiak, Przemys{\\l}aw Uzna\\'nski", "title": "A time and space optimal stable population protocol solving exact\n  majority", "comments": "combined paper that replaces both arXiv:2012.15800 and\n  arXiv:2011.07392", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study population protocols, a model of distributed computing appropriate\nfor modeling well-mixed chemical reaction networks and other physical systems\nwhere agents exchange information in pairwise interactions, but have no control\nover their schedule of interaction partners. The well-studied *majority*\nproblem is that of determining in an initial population of $n$ agents, each\nwith one of two opinions $A$ or $B$, whether there are more $A$, more $B$, or a\ntie. A *stable* protocol solves this problem with probability 1 by eventually\nentering a configuration in which all agents agree on a correct consensus\ndecision of $\\mathsf{A}$, $\\mathsf{B}$, or $\\mathsf{T}$, from which the\nconsensus cannot change. We describe a protocol that solves this problem using\n$O(\\log n)$ states ($\\log \\log n + O(1)$ bits of memory) and optimal expected\ntime $O(\\log n)$. The number of states $O(\\log n)$ is known to be optimal for\nthe class of polylogarithmic time stable protocols that are \"output dominant\"\nand \"monotone\". These are two natural constraints satisfied by our protocol,\nmaking it simultaneously time- and state-optimal for that class. We introduce a\nkey technique called a \"fixed resolution clock\" to achieve partial\nsynchronization.\n  Our protocol is *nonuniform*: the transition function has the value $\\left\n\\lceil {\\log n} \\right \\rceil$ encoded in it. We show that the protocol can be\nmodified to be uniform, while increasing the state complexity to $\\Theta(\\log n\n\\log \\log n)$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:21:16 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Doty", "David", ""], ["Eftekhari", "Mahsa", ""], ["G\u0105sieniec", "Leszek", ""], ["Severson", "Eric", ""], ["Stachowiak", "Grzegorz", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2106.10207", "submitter": "Max Ryabinin", "authors": "Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier,\n  Quentin Lhoest, Anton Sinitsin, Dmitry Popov, Dmitry Pyrkin, Maxim Kashirin,\n  Alexander Borzunov, Albert Villanova del Moral, Denis Mazur, Ilia Kobelev,\n  Yacine Jernite, Thomas Wolf, Gennady Pekhimenko", "title": "Distributed Deep Learning in Open Collaborations", "comments": "30 pages, 9 figures. Code: https://github.com/yandex-research/DeDLOC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning applications require increasingly more compute to train\nstate-of-the-art models. To address this demand, large corporations and\ninstitutions use dedicated High-Performance Computing clusters, whose\nconstruction and maintenance are both environmentally costly and well beyond\nthe budget of most organizations. As a result, some research directions become\nthe exclusive domain of a few large industrial and even fewer academic actors.\nTo alleviate this disparity, smaller groups may pool their computational\nresources and run collaborative experiments that benefit all participants. This\nparadigm, known as grid- or volunteer computing, has seen successful\napplications in numerous scientific areas. However, using this approach for\nmachine learning is difficult due to high latency, asymmetric bandwidth, and\nseveral challenges unique to volunteer computing. In this work, we carefully\nanalyze these constraints and propose a novel algorithmic framework designed\nspecifically for collaborative training. We demonstrate the effectiveness of\nour approach for SwAV and ALBERT pretraining in realistic conditions and\nachieve performance comparable to traditional setups at a fraction of the cost.\nFinally, we provide a detailed report of successful collaborative language\nmodel pretraining with 40 participants.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 16:23:13 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Diskin", "Michael", ""], ["Bukhtiyarov", "Alexey", ""], ["Ryabinin", "Max", ""], ["Saulnier", "Lucile", ""], ["Lhoest", "Quentin", ""], ["Sinitsin", "Anton", ""], ["Popov", "Dmitry", ""], ["Pyrkin", "Dmitry", ""], ["Kashirin", "Maxim", ""], ["Borzunov", "Alexander", ""], ["del Moral", "Albert Villanova", ""], ["Mazur", "Denis", ""], ["Kobelev", "Ilia", ""], ["Jernite", "Yacine", ""], ["Wolf", "Thomas", ""], ["Pekhimenko", "Gennady", ""]]}, {"id": "2106.10334", "submitter": "Michael Chang", "authors": "Michael Alan Chang, Aurojit Panda, Hantao Wang, Yuancheng Tsai, Rahul\n  Balakrishnan, Scott Shenker", "title": "AutoTune: Improving End-to-end Performance and Resource Efficiency for\n  Microservice Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most large web-scale applications are now built by composing collections\n(from a few up to 100s or 1000s) of microservices. Operators need to decide how\nmany resources are allocated to each microservice, and these allocations can\nhave a large impact on application performance. Manually determining\nallocations that are both cost-efficient and meet performance requirements is\nchallenging, even for experienced operators. In this paper we present AutoTune,\nan end-to-end tool that automatically minimizes resource utilization while\nmaintaining good application performance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:49:46 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 00:40:28 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chang", "Michael Alan", ""], ["Panda", "Aurojit", ""], ["Wang", "Hantao", ""], ["Tsai", "Yuancheng", ""], ["Balakrishnan", "Rahul", ""], ["Shenker", "Scott", ""]]}, {"id": "2106.10362", "submitter": "Zhuolun Xiang", "authors": "Rati Gelashvili, Lefteris Kokoris-Kogias, Alberto Sonnino, Alexander\n  Spiegelman, Zhuolun Xiang", "title": "Jolteon and Ditto: Network-Adaptive Efficient Consensus with\n  Asynchronous Fallback", "comments": "arXiv admin note: text overlap with arXiv:2103.03181", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing committee-based Byzantine state machine replication (SMR) protocols,\ntypically deployed in production blockchains, face a clear trade-off: (1) they\neither achieve linear communication cost in the happy path, but sacrifice\nliveness during periods of asynchrony, or (2) they are robust (progress with\nprobability one) but pay quadratic communication cost. We believe this\ntrade-off is unwarranted since existing linear protocols still have asymptotic\nquadratic cost in the worst case. We design Ditto, a Byzantine SMR protocol\nthat enjoys the best of both worlds: optimal communication on and off the happy\npath (linear and quadratic, respectively) and progress guarantee under\nasynchrony and DDoS attacks. We achieve this by replacing the\nview-synchronization of partially synchronous protocols with an asynchronous\nfallback mechanism at no extra asymptotic cost. Specifically, we start from\nHotStuff, a state-of-the-art linear protocol, and gradually build Ditto. As a\nseparate contribution and an intermediate step, we design a 2-chain version of\nHotStuff, Jolteon, which leverages a quadratic view-change mechanism to reduce\nthe latency of the standard 3-chain HotStuff. We implement and experimentally\nevaluate all our systems. Notably, Jolteon's commit latency outperforms\nHotStuff by 200-300ms with varying system size. Additionally, Ditto adapts to\nthe network and provides better performance than Jolteon under faulty\nconditions and better performance than VABA (a state-of-the-art asynchronous\nprotocol) under faultless conditions. This proves our case that breaking the\nrobustness-efficiency trade-off is in the realm of practicality.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 21:34:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gelashvili", "Rati", ""], ["Kokoris-Kogias", "Lefteris", ""], ["Sonnino", "Alberto", ""], ["Spiegelman", "Alexander", ""], ["Xiang", "Zhuolun", ""]]}, {"id": "2106.10499", "submitter": "Gordon E. Moon", "authors": "Gordon E. Moon, Hyoukjun Kwon, Geonhwa Jeong, Prasanth Chatarasi,\n  Sivasankaran Rajamanickam, Tushar Krishna", "title": "Evaluating Spatial Accelerator Architectures with Tiled Matrix-Matrix\n  Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing interest in custom spatial accelerators for machine\nlearning applications. These accelerators employ a spatial array of processing\nelements (PEs) interacting via custom buffer hierarchies and networks-on-chip.\nThe efficiency of these accelerators comes from employing optimized dataflow\n(i.e., spatial/temporal partitioning of data across the PEs and fine-grained\nscheduling) strategies to optimize data reuse. The focus of this work is to\nevaluate these accelerator architectures using a tiled general matrix-matrix\nmultiplication (GEMM) kernel. To do so, we develop a framework that finds\noptimized mappings (dataflow and tile sizes) for a tiled GEMM for a given\nspatial accelerator and workload combination, leveraging an analytical cost\nmodel for runtime and energy. Our evaluations over five spatial accelerators\ndemonstrate that the tiled GEMM mappings systematically generated by our\nframework achieve high performance on various GEMM workloads and accelerators.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 13:53:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Moon", "Gordon E.", ""], ["Kwon", "Hyoukjun", ""], ["Jeong", "Geonhwa", ""], ["Chatarasi", "Prasanth", ""], ["Rajamanickam", "Sivasankaran", ""], ["Krishna", "Tushar", ""]]}, {"id": "2106.10515", "submitter": "Qiang Huang", "authors": "Pingyi Luo, Qiang Huang, Anthony K. H. Tung", "title": "A Generic Distributed Clustering Framework for Massive Data", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a novel Generic distributEd clustEring frameworK\n(GEEK) beyond $k$-means clustering to process massive amounts of data. To deal\nwith different data types, GEEK first converts data in the original feature\nspace into a unified format of buckets; then, we design a new Seeding method\nbased on simILar bucKets (SILK) to determine initial seeds. Compared with\nstate-of-the-art seeding methods such as $k$-means++ and its variants, SILK can\nautomatically identify the number of initial seeds based on the closeness of\nshared data objects in similar buckets instead of pre-specifying $k$. Thus, its\ntime complexity is independent of $k$. With these well-selected initial seeds,\nGEEK only needs a one-pass data assignment to get the final clusters. We\nimplement GEEK on a distributed CPU-GPU platform for large-scale clustering. We\nevaluate the performance of GEEK over five large-scale real-life datasets and\nshow that GEEK can deal with massive data of different types and is comparable\nto (or even better than) many state-of-the-art customized GPU-based methods,\nespecially in large $k$ values.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 15:20:21 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Luo", "Pingyi", ""], ["Huang", "Qiang", ""], ["Tung", "Anthony K. H.", ""]]}, {"id": "2106.10665", "submitter": "Mario Figueiredo", "authors": "Francisco L. Andrade, M\\'ario A. T. Figueiredo, Jo\\~ao Xavier", "title": "Distributed Picard Iteration: Application to Distributed EM and\n  Distributed PCA", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work, we proposed a distributed Picard iteration (DPI) that allows\na set of agents, linked by a communication network, to find a fixed point of a\nlocally contractive (LC) map that is the average of individual maps held by\nsaid agents. In this work, we build upon the DPI and its local linear\nconvergence (LLC) guarantees to make several contributions. We show that\nSanger's algorithm for principal component analysis (PCA) corresponds to the\niteration of an LC map that can be written as the average of local maps, each\nmap known to each agent holding a subset of the data. Similarly, we show that a\nvariant of the expectation-maximization (EM) algorithm for parameter estimation\nfrom noisy and faulty measurements in a sensor network can be written as the\niteration of an LC map that is the average of local maps, each available at\njust one node. Consequently, via the DPI, we derive two distributed algorithms\n- distributed EM and distributed PCA - whose LLC guarantees follow from those\nthat we proved for the DPI. The verification of the LC condition for EM is\nchallenging, as the underlying operator depends on random samples, thus the LC\ncondition is of probabilistic nature.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 09:46:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Andrade", "Francisco L.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""], ["Xavier", "Jo\u00e3o", ""]]}, {"id": "2106.10685", "submitter": "Toni Mancini", "authors": "Quian Matteo Chen, Alberto Finzi, Toni Mancini, Igor Melatti, Enrico\n  Tronci", "title": "MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant\n  placements of relay nodes in mission critical wireless networks", "comments": "33 pages, 11 figures", "journal-ref": "Fundamenta Informaticae, 174(3-4):229-258, 2020", "doi": "10.3233/FI-2020-1941", "report-no": null, "categories": "cs.AI cs.DC cs.LO cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In critical infrastructures like airports, much care has to be devoted in\nprotecting radio communication networks from external electromagnetic\ninterference. Protection of such mission-critical radio communication networks\nis usually tackled by exploiting radiogoniometers: at least three suitably\ndeployed radiogoniometers, and a gateway gathering information from them,\npermit to monitor and localise sources of electromagnetic emissions that are\nnot supposed to be present in the monitored area. Typically, radiogoniometers\nare connected to the gateway through relay nodes. As a result, some degree of\nfault-tolerance for the network of relay nodes is essential in order to offer a\nreliable monitoring. On the other hand, deployment of relay nodes is typically\nquite expensive. As a result, we have two conflicting requirements: minimise\ncosts while guaranteeing a given fault-tolerance. In this paper, we address the\nproblem of computing a deployment for relay nodes that minimises the relay node\nnetwork cost while at the same time guaranteeing proper working of the network\neven when some of the relay nodes (up to a given maximum number) become faulty\n(fault-tolerance). We show that, by means of a computation-intensive\npre-processing on a HPC infrastructure, the above optimisation problem can be\nencoded as a 0/1 Linear Program, becoming suitable to be approached with\nstandard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT\nsolvers. Our problem formulation enables us to present experimental results\ncomparing the performance of these three solving technologies on a real case\nstudy of a relay node network deployment in areas of the Leonardo da Vinci\nAirport in Rome, Italy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 12:14:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Quian Matteo", ""], ["Finzi", "Alberto", ""], ["Mancini", "Toni", ""], ["Melatti", "Igor", ""], ["Tronci", "Enrico", ""]]}, {"id": "2106.10692", "submitter": "Toni Mancini", "authors": "T. Mancini, F. Mari, I. Melatti, I. Salvo, E. Tronci, J.K. Gruber, B.\n  Hayes, M. Prodanovic, L. Elmegaard", "title": "Parallel Statistical Model Checking for Safety Verification in Smart\n  Grids", "comments": "6 pages, 1 figure. In SmartGridComm 2018. IEEE, 2018", "journal-ref": null, "doi": "10.1109/SmartGridComm.2018.8587416", "report-no": null, "categories": "eess.SY cs.DC cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  By using small computing devices deployed at user premises, Autonomous Demand\nResponse (ADR) adapts users electricity consumption to given time-dependent\nelectricity tariffs. This allows end-users to save on their electricity bill\nand Distribution System Operators to optimise (through suitable time-dependent\ntariffs) management of the electric grid by avoiding demand peaks.\nUnfortunately, even with ADR, users power consumption may deviate from the\nexpected (minimum cost) one, e.g., because ADR devices fail to correctly\nforecast energy needs at user premises. As a result, the aggregated power\ndemand may present undesirable peaks. In this paper we address such a problem\nby presenting methods and a software tool (APD-Analyser) implementing them,\nenabling Distribution System Operators to effectively verify that a given\ntime-dependent electricity tariff achieves the desired goals even when\nend-users deviate from their expected behaviour. We show feasibility of the\nproposed approach through a realistic scenario from a medium voltage Danish\ndistribution network.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 13:11:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mancini", "T.", ""], ["Mari", "F.", ""], ["Melatti", "I.", ""], ["Salvo", "I.", ""], ["Tronci", "E.", ""], ["Gruber", "J. K.", ""], ["Hayes", "B.", ""], ["Prodanovic", "M.", ""], ["Elmegaard", "L.", ""]]}, {"id": "2106.10707", "submitter": "Lei Fan", "authors": "Wenlu Xuan, Zhongqi Zhao, Lei Fan and Zhu Han", "title": "Minimizing Delay in Network Function Visualization with Quantum\n  Computing", "comments": "Invited Paper by IEEE MASS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network function virtualization (NFV) is a crucial technology for the 5G\nnetwork development because it can improve the flexibility of employing\nhardware and reduce the construction of base stations. There are vast service\nchains in NFV to meet users' requests, which are composed of a sequence of\nnetwork functions. These virtual network functions (VNFs) are implemented in\nvirtual machines by software and virtual environment. How to deploy VMs to\nprocess VNFs of the service chains as soon as possible when users' requests are\nreceived is very challenging to solve by traditional algorithms on a large\nscale. Compared with traditional algorithms, quantum computing has better\ncomputational performance because of quantum parallelism. We build an integer\nlinear programming model of the VNF scheduling problem with the objective of\nminimizing delays and transfer it into the quadratic unconstrained binary\noptimization (QUBO) model. Our proposed heuristic algorithm employs a quantum\nannealer to solve the model. Finally, we evaluate the computational results and\nexplore the feasibility of leveraging quantum computing to solve the VNFs\nscheduling problem.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 14:52:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xuan", "Wenlu", ""], ["Zhao", "Zhongqi", ""], ["Fan", "Lei", ""], ["Han", "Zhu", ""]]}, {"id": "2106.10796", "submitter": "Enda Yu", "authors": "Enda Yu, Dezun Dong, Yemao Xu, Shuo Ouyang, Xiangke Liao", "title": "CD-SGD: Distributed Stochastic Gradient Descent with Compression and\n  Delay Compensation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communication overhead is the key challenge for distributed training.\nGradient compression is a widely used approach to reduce communication traffic.\nWhen combining with parallel communication mechanism method like pipeline,\ngradient compression technique can greatly alleviate the impact of\ncommunication overhead. However, there exists two problems of gradient\ncompression technique to be solved. Firstly, gradient compression brings in\nextra computation cost, which will delay the next training iteration. Secondly,\ngradient compression usually leads to the decrease of convergence accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 01:15:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yu", "Enda", ""], ["Dong", "Dezun", ""], ["Xu", "Yemao", ""], ["Ouyang", "Shuo", ""], ["Liao", "Xiangke", ""]]}, {"id": "2106.11257", "submitter": "Aleksandr Borzunov", "authors": "Eduard Gorbunov, Alexander Borzunov, Michael Diskin, Max Ryabinin", "title": "Secure Distributed Training at Scale", "comments": "55 pages, 6 figures. Code: https://github.com/yandex-research/btard", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the hardest problems in deep learning can be solved with the combined\neffort of many independent parties, as is the case for volunteer computing and\nfederated learning. These setups rely on high numbers of peers to provide\ncomputational resources or train on decentralized datasets. Unfortunately,\nparticipants in such systems are not always reliable. Any single participant\ncan jeopardize the entire training run by sending incorrect updates, whether\ndeliberately or by mistake. Training in presence of such peers requires\nspecialized distributed training algorithms with Byzantine tolerance. These\nalgorithms often sacrifice efficiency by introducing redundant communication or\npassing all updates through a trusted server. As a result, it can be infeasible\nto apply such algorithms to large-scale distributed deep learning, where models\ncan have billions of parameters. In this work, we propose a novel protocol for\nsecure (Byzantine-tolerant) decentralized training that emphasizes\ncommunication efficiency. We rigorously analyze this protocol: in particular,\nwe provide theoretical bounds for its resistance against Byzantine and Sybil\nattacks and show that it has a marginal communication overhead. To demonstrate\nits practical effectiveness, we conduct large-scale experiments on image\nclassification and language modeling in presence of Byzantine attackers.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:00:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gorbunov", "Eduard", ""], ["Borzunov", "Alexander", ""], ["Diskin", "Michael", ""], ["Ryabinin", "Max", ""]]}, {"id": "2106.11264", "submitter": "Feihu Huang", "authors": "Feihu Huang, Junyi Li, Heng Huang", "title": "Compositional Federated Learning: Applications in Distributionally\n  Robust Averaging and Meta Learning", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the paper, we propose an effective and efficient Compositional Federated\nLearning (ComFedL) algorithm for solving a new compositional Federated Learning\n(FL) framework, which frequently appears in many machine learning problems with\na hierarchical structure such as distributionally robust federated learning and\nmodel-agnostic meta learning (MAML). Moreover, we study the convergence\nanalysis of our ComFedL algorithm under some mild conditions, and prove that it\nachieves a fast convergence rate of $O(\\frac{1}{\\sqrt{T}})$, where $T$ denotes\nthe number of iteration. To the best of our knowledge, our algorithm is the\nfirst work to bridge federated learning with composition stochastic\noptimization. In particular, we first transform the distributionally robust FL\n(i.e., a minimax optimization problem) into a simple composition optimization\nproblem by using KL divergence regularization. At the same time, we also first\ntransform the distribution-agnostic MAML problem (i.e., a minimax optimization\nproblem) into a simple composition optimization problem. Finally, we apply two\npopular machine learning tasks, i.e., distributionally robust FL and MAML to\ndemonstrate the effectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:08:09 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Huang", "Feihu", ""], ["Li", "Junyi", ""], ["Huang", "Heng", ""]]}, {"id": "2106.11408", "submitter": "Firooz Shahriari-Mehr", "authors": "Firooz Shahriari-Mehr, David Bosch and Ashkan Panahi", "title": "Decentralized Constrained Optimization: Double Averaging and Gradient\n  Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the convex, finite-sum minimization problem with\nexplicit convex constraints over strongly connected directed graphs. The\nconstraint is an intersection of several convex sets each being known to only\none node. To solve this problem, we propose a novel decentralized projected\ngradient scheme based on local averaging and prove its convergence using only\nlocal functions' smoothness.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:55:06 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Shahriari-Mehr", "Firooz", ""], ["Bosch", "David", ""], ["Panahi", "Ashkan", ""]]}, {"id": "2106.11469", "submitter": "Johannes Blaschke P", "authors": "Johannes P. Blaschke (1), Aaron S. Brewster (2), Daniel W. Paley (2),\n  Derek Mendez (2), Nicholas K. Sauter (2), Wilko Kr\\\"oger (3), Murali Shankar\n  (3), Bjoern Enders (1), Deborah Bard (1) ((1) National Energy Research\n  Scientific Computing Center, Lawrence Berkeley National Laboratory, USA, (2)\n  Molecular Biophysics and Integrated Bioimaging Division, Lawrence Berkeley\n  National Laboratory, USA, (3) SLAC National Accelerator Laboratory, USA)", "title": "Real-Time XFEL Data Analysis at SLAC and NERSC: a Trial Run of Nascent\n  Exascale Experimental Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray scattering experiments using Free Electron Lasers (XFELs) are a\npowerful tool to determine the molecular structure and function of unknown\nsamples (such as COVID-19 viral proteins). XFEL experiments are a challenge to\ncomputing in two ways: i) due to the high cost of running XFELs, a fast\nturnaround time from data acquisition to data analysis is essential to make\ninformed decisions on experimental protocols; ii) data collection rates are\ngrowing exponentially, requiring new scalable algorithms. Here we report our\nexperiences analyzing data from two experiments at the Linac Coherent Light\nSource (LCLS) during September 2020. Raw data were analyzed on NERSC's Cori\nXC40 system, using the Superfacility paradigm: our workflow automatically moves\nraw data between LCLS and NERSC, where it is analyzed using the software\npackage CCTBX. We achieved real time data analysis with a turnaround time from\ndata acquisition to full molecular reconstruction in as little as 10 min --\nsufficient time for the experiment's operators to make informed decisions. By\nhosting the data analysis on Cori, and by automating LCLS-NERSC\ninteroperability, we achieved a data analysis rate which matches the data\nacquisition rate. Completing data analysis with 10 mins is a first for XFEL\nexperiments and an important milestone if we are to keep up with data\ncollection trends.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 01:09:12 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Blaschke", "Johannes P.", ""], ["Brewster", "Aaron S.", ""], ["Paley", "Daniel W.", ""], ["Mendez", "Derek", ""], ["Sauter", "Nicholas K.", ""], ["Kr\u00f6ger", "Wilko", ""], ["Shankar", "Murali", ""], ["Enders", "Bjoern", ""], ["Bard", "Deborah", ""]]}, {"id": "2106.11499", "submitter": "EPTCS", "authors": "Krisztina Fruzsa (TU Wien), Roman Kuznets (TU Wien), Ulrich Schmid (TU\n  Wien)", "title": "Fire!", "comments": "In Proceedings TARK 2021, arXiv:2106.10886", "journal-ref": "EPTCS 335, 2021, pp. 139-153", "doi": "10.4204/EPTCS.335.13", "report-no": null, "categories": "cs.DC cs.LO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we provide an epistemic analysis of a simple variant of the\nfundamental consistent broadcasting primitive for byzantine fault-tolerant\nasynchronous distributed systems. Our Firing Rebels with Relay (FRR) primitive\nenables agents with a local preference for acting/not acting to trigger an\naction (FIRE) at all correct agents, in an all-or-nothing fashion. By using the\nepistemic reasoning framework for byzantine multi-agent systems introduced in\nour TARK'19 paper, we develop the necessary and sufficient state of knowledge\nthat needs to be acquired by the agents in order to FIRE. It involves eventual\ncommon hope (a modality related to belief), which we show to be attained\nalready by achieving eventual mutual hope in the case of FRR. We also identify\nsubtle variations of the necessary and sufficient state of knowledge for FRR\nfor different assumptions on the local preferences.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:45:51 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Fruzsa", "Krisztina", "", "TU Wien"], ["Kuznets", "Roman", "", "TU Wien"], ["Schmid", "Ulrich", "", "TU\n  Wien"]]}, {"id": "2106.11505", "submitter": "EPTCS", "authors": "R. Ramanujam (Institute of Mathematical Sciences, Chennai)", "title": "Reasoning about Emergence of Collective Memory", "comments": "In Proceedings TARK 2021, arXiv:2106.10886", "journal-ref": "EPTCS 335, 2021, pp. 269-280", "doi": "10.4204/EPTCS.335.26", "report-no": null, "categories": "cs.FL cs.DC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We offer a very simple model of how collective memory may form. Agents keep\nsignalling within neighbourhoods, and depending on how many support each\nsignal, some signals \"win\" in that neighbourhood. By agents interacting between\ndifferent neighbourhoods, 'influence' spreads and sometimes, a collective\nsignal emerges. We propose a logic in which we can reason about such emergence\nof memory and present preliminary technical results on the logic.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:47:31 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ramanujam", "R.", "", "Institute of Mathematical Sciences, Chennai"]]}, {"id": "2106.11570", "submitter": "Sin Kit Lo", "authors": "Sin Kit Lo, Qinghua Lu, Hye-Young Paik, and Liming Zhu", "title": "FLRA: A Reference Architecture for Federated Learning Systems", "comments": "Accepted by ECSA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is an emerging machine learning paradigm that enables\nmultiple devices to train models locally and formulate a global model, without\nsharing the clients' local data. A federated learning system can be viewed as a\nlarge-scale distributed system, involving different components and stakeholders\nwith diverse requirements and constraints. Hence, developing a federated\nlearning system requires both software system design thinking and machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning perspectives, our previous systematic literature\nreview on the area shows that there is a distinct lack of considerations for\nsoftware architecture design for federated learning. In this paper, we propose\nFLRA, a reference architecture for federated learning systems, which provides a\ntemplate design for federated learning-based solutions. The proposed FLRA\nreference architecture is based on an extensive review of existing patterns of\nfederated learning systems found in the literature and existing industrial\nimplementation. The FLRA reference architecture consists of a pool of\narchitectural patterns that could address the frequently recurring design\nproblems in federated learning architectures. The FLRA reference architecture\ncan serve as a design guideline to assist architects and developers with\npractical solutions for their problems, which can be further customised.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:59:19 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Lo", "Sin Kit", ""], ["Lu", "Qinghua", ""], ["Paik", "Hye-Young", ""], ["Zhu", "Liming", ""]]}, {"id": "2106.11726", "submitter": "Gleb Radchenko", "authors": "Alexandra A. Kirsanova, Gleb I. Radchenko and Andrei N. Tchernykh", "title": "Fog computing state of the art: concept and classification of platforms\n  to support distributed computing systems", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the Internet of Things (IoT) becomes a part of our daily life, there is a\nrapid growth in connected devices. A well-established approach based on cloud\ncomputing technologies cannot provide the necessary quality of service in such\nan environment, particularly in terms of reducing data latency. Today, fog\ncomputing technology is seen as a novel approach for processing large amounts\nof critical and time-sensitive data. This article reviews cloud computing\ntechnology and analyzes the prerequisites for the evolution of this approach\nand the emergence of the concept of fog computing. As part of an overview of\nthe critical features of fog computing, we analyze the frequent confusion of\nthe concepts of fog and edge computing. We provide an overview of fog computing\ntechnologies: virtualization, containerization, orchestration, scalability,\nparallel computing environments, as well as a systematic analysis of the most\npopular platforms that support fog computing. As a result of the analysis, we\noffer two approaches to classification of the fog computing platforms: by the\nprinciple of openness/closure of components and a three-level classification\nbased on the provided platform functionality (Deploy-, Platform- and Ecosystem\nas a Service).\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 12:56:02 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kirsanova", "Alexandra A.", ""], ["Radchenko", "Gleb I.", ""], ["Tchernykh", "Andrei N.", ""]]}, {"id": "2106.11728", "submitter": "Gleb Radchenko", "authors": "Ivan Volkov, Gleb Radchenko, Andrey Tchernykh", "title": "Digital Twins, Internet of Things and Mobile Medicine: a Review of\n  Current Platforms to Support Smart Healthcare", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the population grows, the need for a quality level of medical services\ngrows correspondingly, so does the demand for information technology in\nmedicine. The concept of \"Smart Healthcare\" offers many approaches aimed at\nsolving the acute problems faced by modern healthcare. In this paper, we review\nthe main problems of modern healthcare, analyze existing approaches and\ntechnologies in the areas of digital twins, the Internet of Things and mobile\nmedicine, determine their effectiveness in solving the set problems, consider\nthe technologies that are used to monitor and treat patients and propose the\nconcept of the Smart Healthcare platform.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 13:01:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Volkov", "Ivan", ""], ["Radchenko", "Gleb", ""], ["Tchernykh", "Andrey", ""]]}, {"id": "2106.11750", "submitter": "Ian Schneider", "authors": "Ana Radovanovic, Ross Koningstein, Ian Schneider, Bokan Chen,\n  Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick\n  Care, Saurav Talukdar, Eric Mullen, Kendal Smith, MariEllen Cottman and\n  Walfredo Cirne", "title": "Carbon-Aware Computing for Datacenters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of CO$_2$ emitted per kilowatt-hour on an electricity grid varies\nby time of day and substantially varies by location due to the types of\ngeneration. Networked collections of warehouse scale computers, sometimes\ncalled Hyperscale Computing, emit more carbon than needed if operated without\nregard to these variations in carbon intensity. This paper introduces Google's\nsystem for Carbon-Intelligent Compute Management, which actively minimizes\nelectricity-based carbon footprint and power infrastructure costs by delaying\ntemporally flexible workloads. The core component of the system is a suite of\nanalytical pipelines used to gather the next day's carbon intensity forecasts,\ntrain day-ahead demand prediction models, and use risk-aware optimization to\ngenerate the next day's carbon-aware Virtual Capacity Curves (VCCs) for all\ndatacenter clusters across Google's fleet. VCCs impose hourly limits on\nresources available to temporally flexible workloads while preserving overall\ndaily capacity, enabling all such workloads to complete within a day. Data from\noperation shows that VCCs effectively limit hourly capacity when the grid's\nenergy supply mix is carbon intensive and delay the execution of temporally\nflexible workloads to \"greener\" times.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 04:17:06 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Radovanovic", "Ana", ""], ["Koningstein", "Ross", ""], ["Schneider", "Ian", ""], ["Chen", "Bokan", ""], ["Duarte", "Alexandre", ""], ["Roy", "Binz", ""], ["Xiao", "Diyue", ""], ["Haridasan", "Maya", ""], ["Hung", "Patrick", ""], ["Care", "Nick", ""], ["Talukdar", "Saurav", ""], ["Mullen", "Eric", ""], ["Smith", "Kendal", ""], ["Cottman", "MariEllen", ""], ["Cirne", "Walfredo", ""]]}, {"id": "2106.11757", "submitter": "Mohammad Mehdi Sharifi", "authors": "Mohammad Mehdi Sharifi, Lillian Pentecost, Ramin Rajaei, Arman Kazemi,\n  Qiuwen Lou, Gu-Yeon Wei, David Brooks, Kai Ni, X. Sharon Hu, Michael Niemier,\n  Marco Donato", "title": "Application-driven Design Exploration for Dense Ferroelectric Embedded\n  Non-volatile Memories", "comments": "Accepted at ISLPED 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The memory wall bottleneck is a key challenge across many data-intensive\napplications. Multi-level FeFET-based embedded non-volatile memories are a\npromising solution for denser and more energy-efficient on-chip memory.\nHowever, reliable multi-level cell storage requires careful optimizations to\nminimize the design overhead costs. In this work, we investigate the interplay\nbetween FeFET device characteristics, programming schemes, and memory array\narchitecture, and explore different design choices to optimize performance,\nenergy, area, and accuracy metrics for critical data-intensive workloads. From\nour cross-stack design exploration, we find that we can store DNN weights and\nsocial network graphs at a density of over 8MB/mm^2 and sub-2ns read access\nlatency without loss in application accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 01:39:16 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sharifi", "Mohammad Mehdi", ""], ["Pentecost", "Lillian", ""], ["Rajaei", "Ramin", ""], ["Kazemi", "Arman", ""], ["Lou", "Qiuwen", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""], ["Ni", "Kai", ""], ["Hu", "X. Sharon", ""], ["Niemier", "Michael", ""], ["Donato", "Marco", ""]]}, {"id": "2106.11773", "submitter": "Jacob John", "authors": "Jacob John and Shashank Gupta", "title": "A Survey on Serverless Computing", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet is responsible for accelerating growth in several fields such as\ndigital media, healthcare, the military. Furthermore, the Internet was founded\non the principle of allowing clients to communicating with servers. However,\nserverless computing is one such field that tries to break free from this\nparadigm. Event-driven compute services allow users to build more agile\napplications using capacity provisioning and a pay-for-value billing model.\nThis paper provides a formal account of the research contributions in the field\nof Serverless computing.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 13:34:44 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 10:51:57 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["John", "Jacob", ""], ["Gupta", "Shashank", ""]]}, {"id": "2106.11819", "submitter": "Claude Tadonki Dr. HDR", "authors": "Claude Tadonki", "title": "High Performance Optimization at the Door of the Exascale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  quest for processing speed potential. In fact, we always get a fraction of\nthe technically available computing power (so-called {\\em theoretical peak}),\nand the gap is likely to go hand-to-hand with the hardware complexity of the\ntarget system. Among the key aspects of this complexity, we have: the {\\em\nheterogeneity} of the computing units, the {\\em memory hierarchy and\npartitioning} including the non-uniform memory access (NUMA) configuration, and\nthe {\\em interconnect} for data exchanges among the computing nodes. Scientific\ninvestigations and cutting-edge technical activities should ideally scale-up\nwith respect to sustained performance. The special case of quantitative\napproaches for solving (large-scale) problems deserves a special focus. Indeed,\nmost of common real-life problems, even when considering the artificial\nintelligence paradigm, rely on optimization techniques for the main kernels of\nalgorithmic solutions. Mathematical programming and pure combinatorial methods\nare not easy to implement efficiently on large-scale supercomputers because of\n{\\em irregular control flow}, {\\em complex memory access patterns}, {\\em\nheterogeneous kernels}, {\\em numerical issues}, to name a few. We describe and\nexamine our thoughts from the standpoint of large-scale supercomputers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:35:05 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Tadonki", "Claude", ""]]}, {"id": "2106.12007", "submitter": "Marco D'Amico", "authors": "Marco D'Amico, Julita Corbalan", "title": "Energy hardware and workload aware job scheduling towards interconnected\n  HPC environments", "comments": null, "journal-ref": "Transactions on Parallel and Distributed Systems 2021", "doi": "10.1109/TPDS.2021.3090334", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  New HPC machines are getting close to the exascale. Power consumption for\nthose machines has been increasing, and researchers are studying ways to reduce\nit. A second trend is HPC machines' growing complexity, with increasing\nheterogeneous hardware components and different clusters architectures\ncooperating in the same machine. We refer to these environments with the term\nheterogeneous multi-cluster environments. With the aim of optimizing\nperformance and energy consumption in these environments, this paper proposes\nan Energy-Aware-Multi-Cluster (EAMC) job scheduling policy. EAMC-policy is able\nto optimize the scheduling and placement of jobs by predicting performance and\nenergy consumption of arriving jobs for different hardware architectures and\nprocessor frequencies, reducing workload's energy consumption, makespan, and\nresponse time. The policy assigns a different priority to each job-resource\ncombination so that the most efficient ones are favored, while less efficient\nones are still considered on a variable degree, reducing response time and\nincreasing cluster utilization. We implemented EAMC-policy in Slurm, and we\nevaluated a scenario in which two CPU clusters collaborate in the same machine.\nSimulations of workloads running applications modeled from real-world show a\nreduction of response time and makespan by up to 25% and 6% while saving up to\n20% of total energy consumed when compared to policies minimizing runtime, and\nby 49%, 26%, and 6% compared to policies minimizing energy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:26:01 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["D'Amico", "Marco", ""], ["Corbalan", "Julita", ""]]}, {"id": "2106.12012", "submitter": "Celestine Mendler-D\\\"unner", "authors": "Celestine Mendler-D\\\"unner, Wenshuo Guo, Stephen Bates, Michael I.\n  Jordan", "title": "Test-time Collective Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An increasingly common setting in machine learning involves multiple parties,\neach with their own data, who want to jointly make predictions on future test\npoints. Agents wish to benefit from the collective expertise of the full set of\nagents to make better predictions than they would individually, but may not be\nwilling to release their data or model parameters. In this work, we explore a\ndecentralized mechanism to make collective predictions at test time, leveraging\neach agent's pre-trained model without relying on external validation, model\nretraining, or data pooling. Our approach takes inspiration from the literature\nin social science on human consensus-making. We analyze our mechanism\ntheoretically, showing that it converges to inverse meansquared-error (MSE)\nweighting in the large-sample limit. To compute error bars on the collective\npredictions we propose a decentralized Jackknife procedure that evaluates the\nsensitivity of our mechanism to a single agent's prediction. Empirically, we\ndemonstrate that our scheme effectively combines models with differing quality\nacross the input space. The proposed consensus prediction achieves significant\ngains over classical model averaging, and even outperforms weighted averaging\nschemes that have access to additional validation data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:29:58 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mendler-D\u00fcnner", "Celestine", ""], ["Guo", "Wenshuo", ""], ["Bates", "Stephen", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2106.12091", "submitter": "Zhengchun Liu", "authors": "Zhengchun Liu, Rajkumar Kettimuthu, Michael E. Papka, Ian Foster", "title": "BFTrainer: Low-Cost Training of Neural Networks on Unfillable\n  Supercomputer Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supercomputer FCFS-based scheduling policies result in many transient idle\nnodes, a phenomenon that is only partially alleviated by backfill scheduling\nmethods that promote small jobs to run before large jobs. Here we describe how\nto realize a novel use for these otherwise wasted resources, namely, deep\nneural network (DNN) training. This important workload is easily organized as\nmany small fragments that can be configured dynamically to fit essentially any\nnode*time hole in a supercomputer's schedule. We describe how the task of\nrescaling suitable DNN training tasks to fit dynamically changing holes can be\nformulated as a deterministic mixed integer linear programming (MILP)-based\nresource allocation algorithm, and show that this MILP problem can be solved\nefficiently at run time. We show further how this MILP problem can be adapted\nto optimize for administrator- or user-defined metrics. We validate our method\nwith supercomputer scheduler logs and different DNN training scenarios, and\ndemonstrate efficiencies of up to 93% compared with running the same training\ntasks on dedicated nodes. Our method thus enables substantial supercomputer\nresources to be allocated to DNN training with no impact on other applications.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 22:53:19 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liu", "Zhengchun", ""], ["Kettimuthu", "Rajkumar", ""], ["Papka", "Michael E.", ""], ["Foster", "Ian", ""]]}, {"id": "2106.12169", "submitter": "Boyuan Feng", "authors": "Boyuan Feng, Yuke Wang, Tong Geng, Ang Li, Yufei Ding", "title": "APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU\n  Tensor Cores", "comments": "Accepted by SC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, accelerating neural networks with quantization has been\nwidely studied. Unfortunately, prior efforts with diverse precisions (e.g.,\n1-bit weights and 2-bit activations) are usually restricted by limited\nprecision support on GPUs (e.g., int1 and int4). To break such restrictions, we\nintroduce the first Arbitrary Precision Neural Network framework (APNN-TC) to\nfully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically,\nAPNN-TC first incorporates a novel emulation algorithm to support arbitrary\nshort bit-width computation with int1 compute primitives and XOR/AND Boolean\noperations. Second, APNN-TC integrates arbitrary precision layer designs to\nefficiently map our emulation algorithm to Tensor Cores with novel batching\nstrategies and specialized memory organization. Third, APNN-TC embodies a novel\narbitrary precision NN design to minimize memory access across layers and\nfurther improve performance. Extensive evaluations show that APNN-TC can\nachieve significant speedup over CUTLASS kernels and various NN models, such as\nResNet and VGG.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 05:39:34 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Feng", "Boyuan", ""], ["Wang", "Yuke", ""], ["Geng", "Tong", ""], ["Li", "Ang", ""], ["Ding", "Yufei", ""]]}, {"id": "2106.12178", "submitter": "Hossein Sadr", "authors": "Zeinab Khodaverdian, Hossein Sadr, Seyed Ahmad Edalatpanah and Mojdeh\n  Nazari Solimandarabi", "title": "Combination of Convolutional Neural Network and Gated Recurrent Unit for\n  Energy Aware Resource Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing service models have experienced rapid growth and inefficient\nresource usage is known as one of the greatest causes of high energy\nconsumption in cloud data centers. Resource allocation in cloud data centers\naiming to reduce energy consumption has been conducted using live migration of\nVirtual Machines (VMs) and their consolidation into the small number of\nPhysical Machines (PMs). However, the selection of the appropriate VM for\nmigration is an important challenge. To solve this issue, VMs can be classified\naccording to the pattern of user requests into sensitive or insensitive classes\nto latency, and thereafter suitable VMs can be selected for migration. In this\npaper, the combination of Convolution Neural Network (CNN) and Gated Recurrent\nUnit (GRU) is utilized for the classification of VMs in the Microsoft Azure\ndataset. Due to the fact the majority of VMs in this dataset are labeled as\ninsensitive to latency, migration of more VMs in this group not only reduces\nenergy consumption but also decreases the violation of Service Level Agreements\n(SLA). Based on the empirical results, the proposed model obtained an accuracy\nof 95.18which clearly demonstrates the superiority of our proposed model\ncompared to other existing models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 05:57:51 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Khodaverdian", "Zeinab", ""], ["Sadr", "Hossein", ""], ["Edalatpanah", "Seyed Ahmad", ""], ["Solimandarabi", "Mojdeh Nazari", ""]]}, {"id": "2106.12224", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Eyal de Lara and Aaron Ding and Cheol-Ho Hong and\n  Flavio Bonomi and Schahram Dustdar and Paul Harvey and Peter Hewkin and\n  Weisong Shi and Mark Thiele and Peter Willis", "title": "Revisiting the Arguments for Edge Computing Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article argues that low latency, high bandwidth, device proliferation,\nsustainable digital infrastructure, and data privacy and sovereignty continue\nto motivate the need for edge computing research even though its initial\nconcepts were formulated more than a decade ago.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 08:14:07 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Varghese", "Blesson", ""], ["de Lara", "Eyal", ""], ["Ding", "Aaron", ""], ["Hong", "Cheol-Ho", ""], ["Bonomi", "Flavio", ""], ["Dustdar", "Schahram", ""], ["Harvey", "Paul", ""], ["Hewkin", "Peter", ""], ["Shi", "Weisong", ""], ["Thiele", "Mark", ""], ["Willis", "Peter", ""]]}, {"id": "2106.12300", "submitter": "Fanhua Shang", "authors": "Hua Huang, Fanhua Shang, Yuanyuan Liu, Hongying Liu", "title": "Behavior Mimics Distribution: Combining Individual and Group Behaviors\n  for Federated Learning", "comments": "This paper has been accepted by International Joint Conference on\n  Artificial Intelligence (IJCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) has become an active and promising distributed\nmachine learning paradigm. As a result of statistical heterogeneity, recent\nstudies clearly show that the performance of popular FL methods (e.g., FedAvg)\ndeteriorates dramatically due to the client drift caused by local updates. This\npaper proposes a novel Federated Learning algorithm (called IGFL), which\nleverages both Individual and Group behaviors to mimic distribution, thereby\nimproving the ability to deal with heterogeneity. Unlike existing FL methods,\nour IGFL can be applied to both client and server optimization. As a\nby-product, we propose a new attention-based federated learning in the server\noptimization of IGFL. To the best of our knowledge, this is the first time to\nincorporate attention mechanisms into federated optimization. We conduct\nextensive experiments and show that IGFL can significantly improve the\nperformance of existing federated learning methods. Especially when the\ndistributions of data among individuals are diverse, IGFL can improve the\nclassification accuracy by about 13% compared with prior baselines.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:42:37 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Huang", "Hua", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Liu", "Hongying", ""]]}, {"id": "2106.12322", "submitter": "Louis Esperet", "authors": "Louis Esperet, S\\'ebastien Julliot, Arnaud de Mesmay", "title": "Distributed coloring and the local structure of unit-disk graphs", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coloring unit-disk graphs efficiently is an important problem in the global\nand distributed setting, with applications in radio channel assignment problems\nwhen the communication relies on omni-directional antennas of the same power.\nIn this context it is important to bound not only the complexity of the\ncoloring algorithms, but also the number of colors used. In this paper, we\nconsider two natural distributed settings. In the location-aware setting (when\nnodes know their coordinates in the plane), we give a constant time distributed\nalgorithm coloring any unit-disk graph $G$ with at most\n$(3+\\epsilon)\\omega(G)+6$ colors, for any constant $\\epsilon>0$, where\n$\\omega(G)$ is the clique number of $G$. This improves upon a classical\n3-approximation algorithm for this problem, for all unit-disk graphs whose\nchromatic number significantly exceeds their clique number. When nodes do not\nknow their coordinates in the plane, we give a distributed algorithm in the\nLOCAL model that colors every unit-disk graph $G$ with at most $5.68\\omega(G)$\ncolors in $O(2^{\\sqrt{\\log \\log n}})$ rounds. Moreover, when $\\omega(G)=O(1)$,\nthe algorithm runs in $O(\\log^* n)$ rounds. This algorithm is based on a study\nof the local structure of unit-disk graphs, which is of independent interest.\nWe conjecture that every unit-disk graph $G$ has average degree at most\n$4\\omega(G)$, which would imply the existence of a $O(\\log n)$ round algorithm\ncoloring any unit-disk graph $G$ with (approximatively) $4\\omega(G)$ colors.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:36:41 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Esperet", "Louis", ""], ["Julliot", "S\u00e9bastien", ""], ["de Mesmay", "Arnaud", ""]]}, {"id": "2106.12332", "submitter": "Stefanos Leonardos Mr.", "authors": "Yun Kuen Cheung, Stefanos Leonardos, Georgios Piliouras, Shyam Sridhar", "title": "From Griefing to Stability in Blockchain Mining Economies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.MA econ.TH math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a game-theoretic model of blockchain mining economies and show that\ngriefing, a practice according to which participants harm other participants at\nsome lesser cost to themselves, is a prevalent threat at its Nash equilibria.\nThe proof relies on a generalization of evolutionary stability to\nnon-homogeneous populations via griefing factors (ratios that measure network\nlosses relative to deviator's own losses) which leads to a formal theoretical\nargument for the dissipation of resources, consolidation of power and high\nentry barriers that are currently observed in practice.\n  A critical assumption in this type of analysis is that miners' decisions have\nsignificant influence in aggregate network outcomes (such as network hashrate).\nHowever, as networks grow larger, the miner's interaction more closely\nresembles a distributed production economy or Fisher market and its stability\nproperties change. In this case, we derive a proportional response (PR) update\nprotocol which converges to market equilibria at which griefing is irrelevant.\nConvergence holds for a wide range of miners risk profiles and various degrees\nof resource mobility between blockchains with different mining technologies.\nOur empirical findings in a case study with four mineable cryptocurrencies\nsuggest that risk diversification, restricted mobility of resources (as\nenforced by different mining technologies) and network growth, all are\ncontributing factors to the stability of the inherently volatile blockchain\necosystem.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:54:26 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Cheung", "Yun Kuen", ""], ["Leonardos", "Stefanos", ""], ["Piliouras", "Georgios", ""], ["Sridhar", "Shyam", ""]]}, {"id": "2106.12406", "submitter": "Meisam Ansari", "authors": "Meisam Ansari, Mostafa Ansari", "title": "Mitigating the Impact of Distributed Generations on Relay Coordination\n  Using Fault Current Limiters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of distributed generation resources, in addition to considerable\nbenefits, causes some problems in the power system. One of the most critical\nproblems in the case of disruption is increasing short-circuit current level in\ngrids, which leads to change the protection devices settings in the downstream\nand upstream grid. By using fault current limiters (FCL), short-circuit\ncurrents in grids with distributed generation can be reduced to acceptable\nlevels, so there is no needed to change the protection relays settings of the\ndownstream grid (including distributed generations). However, by locating the\nFCL in the tie-feeder, the downstream grid is not more effective than the\nupstream grid and thus its reliability indices also will be changed. Therefore,\nthis paper shows that by locating the unidirectional fault current limiter\n(UFCL) in the tie-feeder, the necessity of changing in the relay protection\nsettings of upstream grids is prevented. In this paper, the proposed method is\nimplemented, and its efficiency is reported in six scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:50:05 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ansari", "Meisam", ""], ["Ansari", "Mostafa", ""]]}, {"id": "2106.12429", "submitter": "Manisha Luthra", "authors": "Manisha Luthra, Boris Koldehofe, Niels Danger, Pascal Weisenburger,\n  Guido Salvaneschi, Ioannis Stavrakakis", "title": "TCEP: Transitions in Operator Placement to Adapt to Dynamic Network\n  Environments", "comments": "Accepted for publication in Journal of Computer and System Sciences,\n  Special Issue on Algorithmic Theory of Dynamic Networks and its Application", "journal-ref": "Journal of Computer and System Sciences, May 2021", "doi": "10.1016/j.jcss.2021.05.003", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Distributed Complex Event Processing (DCEP) is a commonly used paradigm to\ndetect and act on situational changes of many applications, including the\nInternet of Things (IoT). DCEP achieves this using a simple specification of\nanalytical tasks on data streams called operators and their distributed\nexecution on a set of infrastructure. The adaptivity of DCEP to the dynamics of\nIoT applications is essential and very challenging in the face of changing\ndemands concerning Quality of Service. In our previous work, we addressed this\nissue by enabling transitions, which allow for the adaptive use of multiple\noperator placement mechanisms. In this article, we extend the transition\nmethodology by optimizing the costs of transition and analyzing the behaviour\nusing multiple operator placement mechanisms. Furthermore, we provide an\nextensive evaluation on the costs of transition imposed by operator migrations\nand learning, as it can inflict overhead on the performance if operated\nuncoordinatedly.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:31:39 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Luthra", "Manisha", ""], ["Koldehofe", "Boris", ""], ["Danger", "Niels", ""], ["Weisenburger", "Pascal", ""], ["Salvaneschi", "Guido", ""], ["Stavrakakis", "Ioannis", ""]]}, {"id": "2106.12485", "submitter": "Nicolas Guidotti", "authors": "Nicolas Guidotti, Pedro Ceyrat, Jo\\~ao Barreto, Jos\\'e Monteiro,\n  Rodrigo Rodrigues, Ricardo Fonseca, Xavier Martorell, and Antonio J. Pe\\~na", "title": "Particle-In-Cell Simulation using Asynchronous Tasking", "comments": "To be published on the 27th European Conference on Parallel and\n  Distributed Computing (Euro-Par 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, task-based programming models have emerged as a prominent\nalternative among shared-memory parallel programming paradigms. Inherently\nasynchronous, these models provide native support for dynamic load balancing\nand incorporate data flow concepts to selectively synchronize the tasks.\nHowever, tasking models are yet to be widely adopted by the HPC community and\ntheir effective advantages when applied to non-trivial, real-world HPC\napplications are still not well comprehended. In this paper, we study the\nparallelization of a production electromagnetic particle-in-cell (EM-PIC) code\nfor kinetic plasma simulations exploring different strategies using\nasynchronous task-based models. Our fully asynchronous implementation not only\nsignificantly outperforms a conventional, synchronous approach but also\nachieves near perfect scaling for 48 cores.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:57:34 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 14:28:31 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Guidotti", "Nicolas", ""], ["Ceyrat", "Pedro", ""], ["Barreto", "Jo\u00e3o", ""], ["Monteiro", "Jos\u00e9", ""], ["Rodrigues", "Rodrigo", ""], ["Fonseca", "Ricardo", ""], ["Martorell", "Xavier", ""], ["Pe\u00f1a", "Antonio J.", ""]]}, {"id": "2106.12549", "submitter": "Behnam Zeinali Mr", "authors": "Behnam Zeinali, Di Zhuang, J. Morris Chang", "title": "ESAI: Efficient Split Artificial Intelligence via Early Exiting Using\n  Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks have been outperforming conventional machine\nlearning algorithms in many computer vision-related tasks. However, it is not\ncomputationally acceptable to implement these models on mobile and IoT devices\nand the majority of devices are harnessing the cloud computing methodology in\nwhich outstanding deep learning models are responsible for analyzing the data\non the server. This can bring the communication cost for the devices and make\nthe whole system useless in those times where the communication is not\navailable. In this paper, a new framework for deploying on IoT devices has been\nproposed which can take advantage of both the cloud and the on-device models by\nextracting the meta-information from each sample's classification result and\nevaluating the classification's performance for the necessity of sending the\nsample to the server. Experimental results show that only 40 percent of the\ntest data should be sent to the server using this technique and the overall\naccuracy of the framework is 92 percent which improves the accuracy of both\nclient and server models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 04:47:53 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zeinali", "Behnam", ""], ["Zhuang", "Di", ""], ["Chang", "J. Morris", ""]]}, {"id": "2106.12626", "submitter": "Pedro Bizarro", "authors": "Ana Sofia Gomes, Jo\\~ao Oliveirinha, Pedro Cardoso, Pedro Bizarro", "title": "Railgun: managing large streaming windows under MAD requirements", "comments": "arXiv admin note: text overlap with arXiv:2009.00361", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some mission critical systems, e.g., fraud detection, require accurate,\nreal-time metrics over long time sliding windows on applications that demand\nhigh throughput and low latencies. As these applications need to run 'forever'\nand cope with large, spiky data loads, they further require to be run in a\ndistributed setting. We are unaware of any streaming system that provides all\nthose properties. Instead, existing systems take large simplifications, such as\nimplementing sliding windows as a fixed set of overlapping windows,\njeopardizing metric accuracy (violating regulatory rules) or latency (breaching\nservice agreements). In this paper, we propose Railgun, a fault-tolerant,\nelastic, and distributed streaming system supporting real-time sliding windows\nfor scenarios requiring high loads and millisecond-level latencies. We\nbenchmarked an initial prototype of Railgun using real data, showing\nsignificant lower latency than Flink and low memory usage independent of window\nsize. Further, we show that Railgun scales nearly linearly, respecting our\nmsec-level latencies at high percentiles (<250ms @ 99.9%) even under a load of\n1 million events per second.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:51:43 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Gomes", "Ana Sofia", ""], ["Oliveirinha", "Jo\u00e3o", ""], ["Cardoso", "Pedro", ""], ["Bizarro", "Pedro", ""]]}, {"id": "2106.12739", "submitter": "Zhiheng Zhong", "authors": "Zhiheng Zhong, Minxian Xu, Maria Alejandra Rodriguez, Chengzhong Xu,\n  and Rajkumar Buyya", "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and\n  Future Directions", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.GL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Containerization is a lightweight application virtualization technology,\nproviding high environmental consistency, operating system distribution\nportability, and resource isolation. Existing mainstream cloud service\nproviders have prevalently adopted container technologies in their distributed\nsystem infrastructures for automated application management. To handle the\nautomation of deployment, maintenance, autoscaling, and networking of\ncontainerized applications, container orchestration is proposed as an essential\nresearch problem. However, the highly dynamic and diverse feature of cloud\nworkloads and environments considerably raises the complexity of orchestration\nmechanisms. Machine learning algorithms are accordingly employed by container\norchestration systems for behavior modelling and prediction of\nmulti-dimensional performance metrics. Such insights could further improve the\nquality of resource provisioning decisions in response to the changing\nworkloads under complex environments. In this paper, we present a comprehensive\nliterature review of existing machine learning-based container orchestration\napproaches. Detailed taxonomies are proposed to classify the current researches\nby their common features. Moreover, the evolution of machine learning-based\ncontainer orchestration technologies from the year 2016 to 2021 has been\ndesigned based on objectives and metrics. A comparative analysis of the\nreviewed techniques is conducted according to the proposed taxonomies, with\nemphasis on their key characteristics. Finally, various open research\nchallenges and potential future directions are highlighted.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 02:55:35 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhong", "Zhiheng", ""], ["Xu", "Minxian", ""], ["Rodriguez", "Maria Alejandra", ""], ["Xu", "Chengzhong", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2106.12863", "submitter": "Ruo Ando", "authors": "Ruo Ando, Youki Kadobayashi and Hiroki Takakura", "title": "Choice of Parallelism: Multi-GPU Driven Pipeline for Huge Academic\n  Backbone Network", "comments": "This is an Accepted Manuscript of an article published by Taylor &\n  Francis Group in the International Journal of Parallel, Emergent &\n  Distributed Systems on 24/06/2021 av lable online:\n  http://www.tandfonline.com/ DOI: 10.1080/17445760.2021.1941009", "journal-ref": null, "doi": "10.1080/17445760.2021.1941009", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science Information Network (SINET) is a Japanese academic backbone network\nfor more than 800 research institutions and universities. In this paper, we\npresent a multi-GPU-driven pipeline for handling huge session data of SINET.\nOur pipeline consists of ELK stack, multi-GPU server, and Splunk. A multi-GPU\nserver is responsible for two procedures: discrimination and histogramming.\nDiscrimination is dividing session data into ingoing/outgoing with subnet mask\ncalculation and network address matching. Histogramming is grouping\ningoing/outgoing session data into bins with map-reduce. In our architecture,\nwe use GPU for the acceleration of ingress/egress discrimination of session\ndata. Also, we use a tiling design pattern for building a two-stage map-reduce\nof CPU and GPU. Our multi-GPU-driven pipeline has succeeded in processing huge\nworkloads of about 1.2 to 1.6 billion session streams (500GB-650GB) within 24\nhours.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:59:29 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ando", "Ruo", ""], ["Kadobayashi", "Youki", ""], ["Takakura", "Hiroki", ""]]}, {"id": "2106.12942", "submitter": "Mahmoud Hossam", "authors": "Mahmoud Hossam", "title": "High Performance Hyperspectral Image Classification using Graphics\n  Processing Units", "comments": "Master Thesis, Ain Shams University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time remote sensing applications like search and rescue missions,\nmilitary target detection, environmental monitoring, hazard prevention and\nother time-critical applications require onboard real time processing\ncapabilities or autonomous decision making. Some unmanned remote systems like\nsatellites are physically remote from their operators, and all control of the\nspacecraft and data returned by the spacecraft must be transmitted over a\nwireless radio link. This link may not be available for extended periods when\nthe satellite is out of line of sight of its ground station. Therefore,\nlightweight, small size and low power consumption hardware is essential for\nonboard real time processing systems. With increasing dimensionality, size and\nresolution of recent hyperspectral imaging sensors, additional challenges are\nposed upon remote sensing processing systems and more capable computing\narchitectures are needed. Graphical Processing Units (GPUs) emerged as\npromising architecture for light weight high performance computing that can\naddress these computational requirements for onboard systems. The goal of this\nstudy is to build high performance methods for onboard hyperspectral analysis.\nWe propose accelerated methods for the well-known recursive hierarchical\nsegmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a\nGPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the\nNational Aeronautics and Space Administration (NASA), which is designed to\nprovide rich classification information with several output levels. The\nachieved speedups by parallel solutions compared to CPU sequential\nimplementations are 21x for parallel single GPU and 240x for hybrid multi-node\ncomputer clusters with 16 computing nodes. The energy consumption is reduced to\n74% using a single GPU compared to the equivalent parallel CPU cluster.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:26:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hossam", "Mahmoud", ""]]}, {"id": "2106.13002", "submitter": "Christopher Hahn", "authors": "Petra Berenbrink, Felix Biermeier, Christopher Hahn, Dominik Kaaser", "title": "Self-Stabilizing Phase Clocks and the Adaptive Majority Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-stabilising phase clock for population protocols. In the\npopulation model we are given a system of $n$ identical agents which interact\nin a sequence of randomly chosen pairs. Our phase clock is leaderless and it\nrequires $O(\\log n)$ states. It runs forever and is, at any point of time, in a\nsynchronous state w.h.p. When started in an arbitrary configuration, it\nrecovers rapidly and enters a synchronous configuration within $O(\\log n)$\nparallel time w.h.p. Once the clock is synchronized, it stays in a synchronous\nconfiguration for at least poly $n$ parallel time w.h.p.\n  We use our clock to design a loosely self-stabilizing protocol that solves\nthe comparison problem introduced by Alistarh et al., 2021. In this problem, a\nsubset of agents has at any time either $A$ or $B$ as input. The goal is to\nkeep track which of the two opinions is (momentarily) the majority. We show\nthat if the initial majority has a support of at least $\\Omega(\\log n)$ agents\nand a sufficiently large bias is present, then the protocol converges to a\ncorrect output within $O(\\log n)$ time and stays in a correct configuration for\npoly $n$ time, w.h.p.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:31:25 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Berenbrink", "Petra", ""], ["Biermeier", "Felix", ""], ["Hahn", "Christopher", ""], ["Kaaser", "Dominik", ""]]}, {"id": "2106.13019", "submitter": "Laura Lawniczak", "authors": "Laura Lawniczak and Tobias Distler", "title": "Stream-based State-Machine Replication", "comments": "16 pages, extended version of EDCC 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing state-machine replication protocols for practical use is a complex\nand labor-intensive process because of the myriad of essential tasks (e.g.,\ndeployment, communication, recovery) that need to be taken into account in an\nimplementation. In this paper, we show how this problem can be addressed with\nstream-based replication, a novel approach that implements a replication\nprotocol as application on top of a data-stream processing framework. With such\nframework already handling most essential tasks and furthermore providing means\nfor debugging and monitoring, this technique has the key benefit of\nsignificantly minimizing overhead for both programmers as well as system\noperators. Our first stream-based protocol Tara tolerates crashes and comprises\nfull-fledged mechanisms for request handling, checkpointing, and view changes.\nStill, Tara's prototype implementation, which is based on Twitter's Heron\nframework, consists of fewer than 1,500 lines of application-level code.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:51:05 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lawniczak", "Laura", ""], ["Distler", "Tobias", ""]]}, {"id": "2106.13020", "submitter": "Sebastiaan Alvarez Rodriguez", "authors": "Sebastiaan Alvarez Rodriguez (1), Jayjeet Chakraborty (3), Aaron Chu\n  (2), Ivo Jimenez (2), Jeff LeFevre (2), Carlos Maltzahn (2), Alexandru Uta\n  (1) ((1) Leiden University, (2) UCSC Santa Cruz)", "title": "Zero-Cost, Arrow-Enabled Data Interface for Apache Spark", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data processing ecosystems are widespread and their components\nare highly specialized, such that efficient interoperability is urgent.\nRecently, Apache Arrow was chosen by the community to serve as a format\nmediator, providing efficient in-memory data representation. Arrow enables\nefficient data movement between data processing and storage engines,\nsignificantly improving interoperability and overall performance. In this work,\nwe design a new zero-cost data interoperability layer between Apache Spark and\nArrow-based data sources through the Arrow Dataset API. Our novel data\ninterface helps separate the computation (Spark) and data (Arrow) layers. This\nenables practitioners to seamlessly use Spark to access data from all Arrow\nDataset API-enabled data sources and frameworks. To benefit our community, we\nopen-source our work and show that consuming data through Apache Arrow is\nzero-cost: our novel data interface is either on-par or more performant than\nnative Spark.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:52:08 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Rodriguez", "Sebastiaan Alvarez", "", "Leiden University"], ["Chakraborty", "Jayjeet", "", "UCSC Santa Cruz"], ["Chu", "Aaron", "", "UCSC Santa Cruz"], ["Jimenez", "Ivo", "", "UCSC Santa Cruz"], ["LeFevre", "Jeff", "", "UCSC Santa Cruz"], ["Maltzahn", "Carlos", "", "UCSC Santa Cruz"], ["Uta", "Alexandru", "", "Leiden University"]]}, {"id": "2106.13039", "submitter": "Kang Wei", "authors": "Kang Wei, Jun Li, Chuan Ma, Ming Ding, Cailian Chen, Shi Jin, Zhu Han\n  and H. Vincent Poor", "title": "Low-Latency Federated Learning over Wireless Channels with Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated learning (FL), model training is distributed over clients and\nlocal models are aggregated by a central server. The performance of uploaded\nmodels in such situations can vary widely due to imbalanced data distributions,\npotential demands on privacy protections, and quality of transmissions. In this\npaper, we aim to minimize FL training delay over wireless channels, constrained\nby overall training performance as well as each client's differential privacy\n(DP) requirement. We solve this problem in the framework of multi-agent\nmulti-armed bandit (MAMAB) to deal with the situation where there are multiple\nclients confornting different unknown transmission environments, e.g., channel\nfading and interferences. Specifically, we first transform the long-term\nconstraints on both training performance and each client's DP into a virtual\nqueue based on the Lyapunov drift technique. Then, we convert the MAMAB to a\nmax-min bipartite matching problem at each communication round, by estimating\nrewards with the upper confidence bound (UCB) approach. More importantly, we\npropose two efficient solutions to this matching problem, i.e., modified\nHungarian algorithm and greedy matching with a better alternative (GMBA), in\nwhich the first one can achieve the optimal solution with a high complexity\nwhile the second one approaches a better trade-off by enabling a verified\nlow-complexity with little performance loss. In addition, we develop an upper\nbound on the expected regret of this MAMAB based FL framework, which shows a\nlinear growth over the logarithm of communication rounds, justifying its\ntheoretical feasibility. Extensive experimental results are conducted to\nvalidate the effectiveness of our proposed algorithms, and the impacts of\nvarious parameters on the FL performance over wireless edge networks are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 13:51:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Wei", "Kang", ""], ["Li", "Jun", ""], ["Ma", "Chuan", ""], ["Ding", "Ming", ""], ["Chen", "Cailian", ""], ["Jin", "Shi", ""], ["Han", "Zhu", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2106.13044", "submitter": "Jingcai Guo", "authors": "Xueyang Tang, Song Guo, Jingcai Guo", "title": "Personalized Federated Learning with Clustered Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the recent emerging personalized federated learning (PFL) that aims\nat dealing with the challenging problem of Non-I.I.D. data in the federated\nlearning (FL) setting. The key difference between PFL and conventional FL lies\nin the training target, of which the personalized models in PFL usually pursue\na trade-off between personalization (i.e., usually from local models) and\ngeneralization (i.e., usually from the global model) on trained models.\nConventional FL methods can hardly meet this target because of their both\nwell-developed global and local models. The prevalent PFL approaches usually\nmaintain a global model to guide the training process of local models and\ntransfer a proper degree of generalization to them. However, the sole global\nmodel can only provide one direction of generalization and may even transfer\nnegative effects to some local models when rich statistical diversity exists\nacross multiple local datasets. Based on our observation, most real or\nsynthetic data distributions usually tend to be clustered to some degree, of\nwhich we argue different directions of generalization can facilitate the PFL.\nIn this paper, we propose a novel concept called clustered generalization to\nhandle the challenge of statistical heterogeneity in FL. Specifically, we\nmaintain multiple global (generalized) models in the server to associate with\nthe corresponding amount of local model clusters in clients, and further\nformulate the PFL as a bi-level optimization problem that can be solved\nefficiently and robustly. We also conduct detailed theoretical analysis and\nprovide the convergence guarantee for the smooth non-convex objectives.\nExperimental results on both synthetic and real datasets show that our approach\nsurpasses the state-of-the-art by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:17:00 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Tang", "Xueyang", ""], ["Guo", "Song", ""], ["Guo", "Jingcai", ""]]}, {"id": "2106.13239", "submitter": "Huazhu Fu", "authors": "Li Li, Huazhu Fu, Bo Han, Cheng-Zhong Xu, Ling Shao", "title": "Federated Noisy Client Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) collaboratively aggregates a shared global model\ndepending on multiple local clients, while keeping the training data\ndecentralized in order to preserve data privacy. However, standard FL methods\nignore the noisy client issue, which may harm the overall performance of the\naggregated model. In this paper, we first analyze the noisy client statement,\nand then model noisy clients with different noise distributions (e.g.,\nBernoulli and truncated Gaussian distributions). To learn with noisy clients,\nwe propose a simple yet effective FL framework, named Federated Noisy Client\nLearning (Fed-NCL), which is a plug-and-play algorithm and contains two main\ncomponents: a data quality measurement (DQM) to dynamically quantify the data\nquality of each participating client, and a noise robust aggregation (NRA) to\nadaptively aggregate the local models of each client by jointly considering the\namount of local training data and the data quality of each client. Our Fed-NCL\ncan be easily applied in any standard FL workflow to handle the noisy client\nissue. Experimental results on various datasets demonstrate that our algorithm\nboosts the performances of different state-of-the-art systems with noisy\nclients.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 11:09:17 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Li", "Li", ""], ["Fu", "Huazhu", ""], ["Han", "Bo", ""], ["Xu", "Cheng-Zhong", ""], ["Shao", "Ling", ""]]}, {"id": "2106.13306", "submitter": "Dingwen Tao", "authors": "Chengming Zhang, Sian Jin, Tong Geng, Jiannan Tian, Ang Li, Dingwen\n  Tao", "title": "CEAZ: Accelerating Parallel I/O via Hardware-Algorithm Co-Design of\n  Efficient and Adaptive Lossy Compression", "comments": "14 pages, 17 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As supercomputers continue to grow to exascale, the amount of data that needs\nto be saved or transmitted is exploding. To this end, many previous works have\nstudied using error-bounded lossy compressors to reduce the data size and\nimprove the I/O performance. However, little work has been done for effectively\noffloading lossy compression onto FPGA-based SmartNICs to reduce the\ncompression overhead. In this paper, we propose a hardware-algorithm co-design\nof efficient and adaptive lossy compressor for scientific data on FPGAs (called\nCEAZ) to accelerate parallel I/O. Our contribution is fourfold: (1) We propose\nan efficient Huffman coding approach that can adaptively update Huffman\ncodewords online based on codewords generated offline (from a variety of\nrepresentative scientific datasets). (2) We derive a theoretical analysis to\nsupport a precise control of compression ratio under an error-bounded\ncompression mode, enabling accurate offline Huffman codewords generation. This\nalso helps us create a fixed-ratio compression mode for consistent throughput.\n(3) We develop an efficient compression pipeline by adopting cuSZ's\ndual-quantization algorithm to our hardware use case. (4) We evaluate CEAZ on\nfive real-world datasets with both a single FPGA board and 128 nodes from\nBridges-2 supercomputer. Experiments show that CEAZ outperforms the second-best\nFPGA-based lossy compressor by 2X of throughput and 9.6X of compression ratio.\nIt also improves MPI_File_write and MPI_Gather throughputs by up to 25.8X and\n24.8X, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 20:26:52 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhang", "Chengming", ""], ["Jin", "Sian", ""], ["Geng", "Tong", ""], ["Tian", "Jiannan", ""], ["Li", "Ang", ""], ["Tao", "Dingwen", ""]]}, {"id": "2106.13308", "submitter": "Tianchen Zhao Mr.", "authors": "Tianchen Zhao, Saibal De, Brian Chen, James Stokes, Shravan\n  Veerapaneni", "title": "Overcoming barriers to scalability in variational quantum Monte Carlo", "comments": "ACM/IEEE Proceedings of the International Conference for High\n  Performance Computing, Networking, Storage and Analysis (SC21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cond-mat.dis-nn quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational quantum Monte Carlo (VQMC) method received significant\nattention in the recent past because of its ability to overcome the curse of\ndimensionality inherent in many-body quantum systems. Close parallels exist\nbetween VQMC and the emerging hybrid quantum-classical computational paradigm\nof variational quantum algorithms. VQMC overcomes the curse of dimensionality\nby performing alternating steps of Monte Carlo sampling from a parametrized\nquantum state followed by gradient-based optimization. While VQMC has been\napplied to solve high-dimensional problems, it is known to be difficult to\nparallelize, primarily owing to the Markov Chain Monte Carlo (MCMC) sampling\nstep. In this work, we explore the scalability of VQMC when autoregressive\nmodels, with exact sampling, are used in place of MCMC. This approach can\nexploit distributed-memory, shared-memory and/or GPU parallelism in the\nsampling task without any bottlenecks. In particular, we demonstrate the\nGPU-scalability of VQMC for solving up to ten-thousand dimensional\ncombinatorial optimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 20:36:50 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 02:21:53 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhao", "Tianchen", ""], ["De", "Saibal", ""], ["Chen", "Brian", ""], ["Stokes", "James", ""], ["Veerapaneni", "Shravan", ""]]}, {"id": "2106.13422", "submitter": "Rachit Agarwal", "authors": "Rachit Agarwal, Tanmay Thapliyal, Sandeep Kumar Shukla", "title": "Vulnerability and Transaction behavior based detection of Malicious\n  Smart Contracts", "comments": "Submitted to a conf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart Contracts (SCs) in Ethereum can automate tasks and provide different\nfunctionalities to a user. Such automation is enabled by the `Turing-complete'\nnature of the programming language (Solidity) in which SCs are written. This\nalso opens up different vulnerabilities and bugs in SCs that malicious actors\nexploit to carry out malicious or illegal activities on the cryptocurrency\nplatform. In this work, we study the correlation between malicious activities\nand the vulnerabilities present in SCs and find that some malicious activities\nare correlated with certain types of vulnerabilities. We then develop and study\nthe feasibility of a scoring mechanism that corresponds to the severity of the\nvulnerabilities present in SCs to determine if it is a relevant feature to\nidentify suspicious SCs. We analyze the utility of severity score towards\ndetection of suspicious SCs using unsupervised machine learning (ML) algorithms\nacross different temporal granularities and identify behavioral changes. In our\nexperiments with on-chain SCs, we were able to find a total of 1094 benign SCs\nacross different granularities which behave similar to malicious SCs, with the\ninclusion of the smart contract vulnerability scores in the feature set.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 04:25:23 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Agarwal", "Rachit", ""], ["Thapliyal", "Tanmay", ""], ["Shukla", "Sandeep Kumar", ""]]}, {"id": "2106.13423", "submitter": "Han Xie", "authors": "Han Xie, Jing Ma, Li Xiong, Carl Yang", "title": "Federated Graph Classification over Non-IID Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as an important paradigm for training machine\nlearning models in different domains. For graph-level tasks such as graph\nclassification, graphs can also be regarded as a special type of data samples,\nwhich can be collected and stored in separate local systems. Similar to other\ndomains, multiple local systems, each holding a small set of graphs, may\nbenefit from collaboratively training a powerful graph mining model, such as\nthe popular graph neural networks (GNNs). To provide more motivation towards\nsuch endeavors, we analyze real-world graphs from different domains to confirm\nthat they indeed share certain graph properties that are statistically\nsignificant compared with random graphs. However, we also find that different\nsets of graphs, even from the same domain or same dataset, are non-IID\nregarding both graph structures and node features. To handle this, we propose a\ngraph clustered federated learning (GCFL) framework that dynamically finds\nclusters of local systems based on the gradients of GNNs, and theoretically\njustify that such clusters can reduce the structure and feature heterogeneity\namong graphs owned by the local systems. Moreover, we observe the gradients of\nGNNs to be rather fluctuating in GCFL which impedes high-quality clustering,\nand design a gradient sequence-based clustering mechanism based on dynamic time\nwarping (GCFL+). Extensive experimental results and in-depth analysis\ndemonstrate the effectiveness of our proposed frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 04:25:29 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 14:04:43 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Xie", "Han", ""], ["Ma", "Jing", ""], ["Xiong", "Li", ""], ["Yang", "Carl", ""]]}, {"id": "2106.13465", "submitter": "Jeremie Gaidamour", "authors": "J\\'er\\'emie Gaidamour (IECL), Dimitri Lecas (IDRIS),\n  Pierre-Fran\\c{c}ois Lavall\\'ee (IDRIS)", "title": "Introducing OpenMP Tasks into the HYDRO Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HYDRO mini-application has been successfully used as a research vehicle\nin previous PRACE projects [6]. In this paper, we evaluate the benefits of the\ntasking model introduced in recent OpenMP standards [9]. We have developed a\nnew version of HYDRO using the concept of OpenMP tasks and this implementation\nis compared to already existing and optimized OpenMP versions of HYDRO.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 07:27:59 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gaidamour", "J\u00e9r\u00e9mie", "", "IECL"], ["Lecas", "Dimitri", "", "IDRIS"], ["Lavall\u00e9e", "Pierre-Fran\u00e7ois", "", "IDRIS"]]}, {"id": "2106.13524", "submitter": "Jagruti Sahoo", "authors": "Jagruti Sahoo", "title": "Cost-efficient, QoS and Security aware Placement of Smart Farming IoT\n  Applications in Cloud-Fog Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart farming is a recent innovation in the agriculture sector that can\nimprove the agricultural yield by using smarter, automated, and data driven\nfarm processes that interact with IoT devices deployed on farms. A cloud-fog\ninfrastructure provides an effective platform to execute IoT applications.\nWhile fog computing satisfies the real-time processing need of delay-sensitive\nIoT services by bringing virtualized services closer to the IoT devices, cloud\ncomputing allows execution of applications with higher computational\nrequirements. The deployment of IoT applications is a critical challenge as\ncloud and fog nodes vary in terms of their resource availability and use\ndifferent cost models. Moreover, diversity in resource, quality of service\n(QoS) and security requirements of IoT applications make the problem even more\ncomplex. In this paper, we model IoT application placement as an optimization\nproblem that aims at minimizing the cost while satisfying the QoS and security\nconstraints. The problem is formulated using Integer Linear Programming (ILP).\nThe ILP model is evaluated for a small-scale scenario. The evaluation shows the\nimpact of QoS and security requirement on the cost. We also study the impact of\nrelaxing security constraint on the placement decision.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:32:19 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 10:39:10 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Sahoo", "Jagruti", ""]]}, {"id": "2106.13590", "submitter": "Jamal Toutouh", "authors": "Jamal Toutouh and Erik Hemberg and Una-May O'Reilly", "title": "Fostering Diversity in Spatial Evolutionary Generative Adversarial\n  Networks", "comments": "Accepted to be presented during Conference of the Spanish Association\n  of Artificial Intelligence (CAEPIA 2021). arXiv admin note: substantial text\n  overlap with arXiv:1905.12702", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversary networks (GANs) suffer from training pathologies such as\ninstability and mode collapse, which mainly arise from a lack of diversity in\ntheir adversarial interactions. Co-evolutionary GAN (CoE-GAN) training\nalgorithms have shown to be resilient to these pathologies. This article\nintroduces Mustangs, a spatially distributed CoE-GAN, which fosters diversity\nby using different loss functions during the training. Experimental analysis on\nMNIST and CelebA demonstrated that Mustangs trains statistically more accurate\ngenerators.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 12:40:36 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Toutouh", "Jamal", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "2106.13591", "submitter": "Stephen Wolfram", "authors": "Stephen Wolfram", "title": "The Problem of Distributed Consensus: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM nlin.CG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A survey is given of approaches to the problem of distributed consensus,\nfocusing particularly on methods based on cellular automata and related\nsystems. A variety of new results are given, as well as a history of the field\nand an extensive bibliography. Distributed consensus is of current relevance in\na new generation of blockchain-related systems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:03:26 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wolfram", "Stephen", ""]]}, {"id": "2106.13645", "submitter": "Masudul Quraishi", "authors": "Michael Riera, Masudul Hassan Quraishi, Erfan Bank Tavakoli, Fengbo\n  Ren", "title": "FLASH 1.0: A Software Framework for Rapid Parallel Deployment and\n  Enhancing Host Code Portability in Heterogeneous Computing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present FLASH 1.0, a C++-based software framework for rapid\nparallel deployment and enhancing host code portability in heterogeneous\ncomputing. FLASH takes a novel approach in describing kernels and dynamically\ndispatching them in a hardware-agnostic manner. FLASH features truly\nhardware-agnostic frontend interfaces, which not only unify the compile-time\ncontrol flow but also enforces a portability-optimized code organization that\nimposes a demarcation between computational (performance-critical) and\nfunctional (non-performance-critical) codes as well as the separation of\nhardware-specific and hardware-agnostic codes in the host application. We use\nstatic code analysis to measure the hardware independence ratio of popular HPC\napplications and show that up to 99.72% code portability can be achieved with\nFLASH. Similarly, we measure the complexity of state-of-the-art portable\nprogramming models and show that a code reduction of up to 2.2x can be achieved\nfor two common HPC kernels while maintaining 100% code portability with a\nnormalized framework overhead between 1% - 13% of the total kernel runtime. The\ncodes are available at https://github.com/PSCLab-ASU/FLASH.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 13:57:16 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Riera", "Michael", ""], ["Quraishi", "Masudul Hassan", ""], ["Tavakoli", "Erfan Bank", ""], ["Ren", "Fengbo", ""]]}, {"id": "2106.13673", "submitter": "Xinwei Zhang", "authors": "Xinwei Zhang, Xiangyi Chen, Mingyi Hong, Zhiwei Steven Wu and Jinfeng\n  Yi", "title": "Understanding Clipping for Federated Learning: Convergence and\n  Client-Level Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing privacy protection has been one of the primary motivations of\nFederated Learning (FL). Recently, there has been a line of work on\nincorporating the formal privacy notion of differential privacy with FL. To\nguarantee the client-level differential privacy in FL algorithms, the clients'\ntransmitted model updates have to be clipped before adding privacy noise. Such\nclipping operation is substantially different from its counterpart of gradient\nclipping in the centralized differentially private SGD and has not been\nwell-understood. In this paper, we first empirically demonstrate that the\nclipped FedAvg can perform surprisingly well even with substantial data\nheterogeneity when training neural networks, which is partly because the\nclients' updates become similar for several popular deep architectures. Based\non this key observation, we provide the convergence analysis of a differential\nprivate (DP) FedAvg algorithm and highlight the relationship between clipping\nbias and the distribution of the clients' updates. To the best of our\nknowledge, this is the first work that rigorously investigates theoretical and\nempirical issues regarding the clipping operation in FL algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 14:47:19 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhang", "Xinwei", ""], ["Chen", "Xiangyi", ""], ["Hong", "Mingyi", ""], ["Wu", "Zhiwei Steven", ""], ["Yi", "Jinfeng", ""]]}, {"id": "2106.13859", "submitter": "Marcin Copik", "authors": "Marcin Copik, Konstantin Taranov, Alexandru Calotoiu, Torsten Hoefler", "title": "RFaaS: RDMA-Enabled FaaS Platform for Serverless High-Performance\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rigid MPI programming model and batch scheduling dominate\nhigh-performance computing. While clouds brought new levels of elasticity into\nthe world of computing, supercomputers still suffer from low resource\nutilization rates. To enhance supercomputing clusters with the benefits of\nserverless computing, a modern cloud programming paradigm for pay-as-you-go\nexecution of stateless functions, we present rFaaS, the first RDMA-aware\nFunction-as-a-Service (FaaS) platform. With hot invocations and decentralized\nfunction placement, we overcome the major performance limitations of FaaS\nsystems and provide low-latency remote invocations in multi-tenant\nenvironments. We evaluate the new serverless system through a series of\nmicrobenchmarks and show that remote functions execute with negligible\nperformance overheads. We demonstrate how serverless computing can bring\nelastic resource management into MPI-based high-performance applications.\nOverall, our results show that MPI applications can benefit from modern cloud\nprogramming paradigms to guarantee high performance at lower resource costs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 19:36:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Copik", "Marcin", ""], ["Taranov", "Konstantin", ""], ["Calotoiu", "Alexandru", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2106.13972", "submitter": "Margaret Lawson", "authors": "Margaret Lawson, William Gropp, Jay Lofstead", "title": "Exploring Spatial Indexing for Accelerated Feature Retrieval in HPC", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the critical role that range queries play in analysis and\nvisualization for HPC applications, there has been no comprehensive analysis of\nindices that are designed to accelerate range queries and the extent to which\nthey are viable in an HPC setting. In this state of the practice paper we\npresent the first such evaluation, examining 20 open-source C and C++ libraries\nthat support range queries. Contributions of this paper include answering the\nfollowing questions: which of the implementations are viable in an HPC setting,\nhow do these libraries compare in terms of build time, query time, memory\nusage, and scalability, what are other trade-offs between these\nimplementations, is there a single overall best solution, and when does a brute\nforce solution offer the best performance? We also share key insights learned\nduring this process that can assist both HPC application scientists and spatial\nindex developers.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 08:50:29 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lawson", "Margaret", ""], ["Gropp", "William", ""], ["Lofstead", "Jay", ""]]}, {"id": "2106.14038", "submitter": "Yuedan Chen", "authors": "Yuedan Chen, M. Tamer \\\"Ozsu, Guoqing Xiao, Zhuo Tang, Kenli Li", "title": "GSmart: An Efficient SPARQL Query Engine Using Sparse Matrix Algebra --\n  Full Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient execution of SPARQL queries over large RDF datasets is a topic of\nconsiderable interest due to increased use of RDF to encode data. Most of this\nwork has followed either relational or graph-based approaches. In this paper,\nwe propose an alternative query engine, called gSmart, based on matrix algebra.\nThis approach can potentially better exploit the computing power of\nhigh-performance heterogeneous architectures that we target. gSmart\nincorporates: (1) grouped incident edge-based SPARQL query evaluation, in which\nall unevaluated edges of a vertex are evaluated together using a series of\nmatrix operations to fully utilize query constraints and narrow down the\nsolution space; (2) a graph query planner that determines the order in which\nvertices in query graphs should be evaluated; (3) memory- and\ncomputation-efficient data structures including the light-weight sparse matrix\n(LSpM) storage for RDF data and the tree-based representation for evaluation\nresults; (4) a multi-stage data partitioner to map the incident edge-based\nquery evaluation into heterogeneous HPC architectures and develop multi-level\nparallelism; and (5) a parallel executor that uses the fine-grained processing\nscheme, pre-pruning technique, and tree-pruning technique to lower inter-node\ncommunication and enable high throughput. Evaluations of gSmart on a CPU+GPU\nHPC architecture show execution time speedups of up to 46920.00x compared to\nthe existing SPARQL query engines on a single node machine. Additionally,\ngSmart on the Tianhe-1A supercomputer achieves a maximum speedup of 6.90x\nscaling from 2 to 16 CPU+GPU nodes.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 15:03:34 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chen", "Yuedan", ""], ["\u00d6zsu", "M. Tamer", ""], ["Xiao", "Guoqing", ""], ["Tang", "Zhuo", ""], ["Li", "Kenli", ""]]}, {"id": "2106.14075", "submitter": "Changxin Liu", "authors": "Changxin Liu, Zirui Zhou, Jian Pei, Yong Zhang, Yang Shi", "title": "Decentralized Composite Optimization in Stochastic Networks: A Dual\n  Averaging Approach with Linear Convergence", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized optimization, particularly the class of decentralized composite\nconvex optimization (DCCO) problems, has found many applications. Due to\nubiquitous communication congestion and random dropouts in practice, it is\nhighly desirable to design decentralized algorithms that can handle stochastic\ncommunication networks. However, most existing algorithms for DCCO only work in\ntime-invariant networks and cannot be extended to stochastic networks because\nthey inherently need knowledge of network topology $\\textit{a priori}$. In this\npaper, we propose a new decentralized dual averaging (DDA) algorithm that can\nsolve DCCO in stochastic networks. Under a rather mild condition on stochastic\nnetworks, we show that the proposed algorithm attains $\\textit{global linear\nconvergence}$ if each local objective function is strongly convex. Our\nalgorithm substantially improves the existing DDA-type algorithms as the latter\nwere only known to converge $\\textit{sublinearly}$ prior to our work. The key\nto achieving the improved rate is the design of a novel dynamic averaging\nconsensus protocol for DDA, which intuitively leads to more accurate local\nestimates of the global dual variable. To the best of our knowledge, this is\nthe first linearly convergent DDA-type decentralized algorithm and also the\nfirst algorithm that attains global linear convergence for solving DCCO in\nstochastic networks. Numerical results are also presented to support our design\nand analysis.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 18:29:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Changxin", ""], ["Zhou", "Zirui", ""], ["Pei", "Jian", ""], ["Zhang", "Yong", ""], ["Shi", "Yang", ""]]}, {"id": "2106.14079", "submitter": "Seyed Morteza Nabavinejad", "authors": "Seyed Morteza Nabavinejad and Behzad Salami", "title": "On the Impact of Device-Level Techniques on Energy-Efficiency of Neural\n  Network Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Energy-efficiency is a key concern for neural network applications. To\nalleviate this issue, hardware acceleration using FPGAs or GPUs can provide\nbetter energy-efficiency than general-purpose processors. However, further\nimprovement of the energy-efficiency of such accelerators will be extremely\nbeneficial specially to deploy neural network in power-constrained edge\ncomputing environments. In this paper, we experimentally explore the potential\nof device-level energy-efficiency techniques (e.g.,supply voltage underscaling,\nfrequency scaling, and data quantization) for representative off-the-shelf\nFPGAs compared to GPUs. Frequency scaling in both platforms can improve the\npower and energy consumption but with performance overhead, e.g.,in GPUs it\nimproves the power consumption and GOPs/J by up to 34% and 28%, respectively.\nHowever, leveraging reduced-precision instructions improves power (up to 13%),\nenergy (up to 20%), and performance (up to 7%) simultaneously, with negligible\nreduction in accuracy of neural network accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 20:00:22 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nabavinejad", "Seyed Morteza", ""], ["Salami", "Behzad", ""]]}, {"id": "2106.14100", "submitter": "Ahmed M. Abdelmoniem", "authors": "Ahmed M. Abdelmoniem and Brahim Bensaou", "title": "Implementation and Evaluation of Data Center Congestion Controller with\n  Switch Assistance", "comments": "arXiv admin note: text overlap with arXiv:2012.00339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we provide the design and implementation of a switch-assisted\ncongestion control algorithm for data center networks (DCNs). In particular, we\nprovide a prototype of the switch-driven congestion control algorithm and\ndeploy it in a real data center. The prototype is based on few simple\nmodifications to the switch software. The modifications imposed by the\nalgorithm on the switch are to enable the switch to modify the TCP\nreceive-window field in the packet headers. By doing so, the algorithm can\nenforce a pre-calculated (or target rate) to limit the sending rate at the\nsources. Therefore, the algorithm requires no modifications to the TCP source\nor receiver code which considered out of the DCN operators' control (e.g., in\nthe public cloud where the VM is maintained by the tenant). This paper\ndescribes in detail two implementations, one as a Linux kernel module and the\nsecond as an added feature to the well-known software switch, Open vSwitch.\nThen we present evaluation results based on experiments of the deployment of\nboth designs in a small testbed to demonstrate the effectiveness of the\nproposed technique in achieving high throughput, good fairness, and short flow\ncompletion times for delay-sensitive flows.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 22:18:35 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Abdelmoniem", "Ahmed M.", ""], ["Bensaou", "Brahim", ""]]}, {"id": "2106.14126", "submitter": "GuangMeng Zhou", "authors": "Guangmeng Zhou, Ke Xu, Qi Li, Yang Liu, Yi Zhao", "title": "AdaptCL: Efficient Collaborative Learning with Dynamic and Adaptive\n  Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-party collaborative learning, the parameter server sends a global\nmodel to each data holder for local training and then aggregates committed\nmodels globally to achieve privacy protection. However, both the dragger issue\nof synchronous collaborative learning and the staleness issue of asynchronous\ncollaborative learning make collaborative learning inefficient in real-world\nheterogeneous environments. We propose a novel and efficient collaborative\nlearning framework named AdaptCL, which generates an adaptive sub-model\ndynamically from the global base model for each data holder, without any prior\ninformation about worker capability. All workers (data holders) achieve\napproximately identical update time as the fastest worker by equipping them\nwith capability-adapted pruned models. Thus the training process can be\ndramatically accelerated. Besides, we tailor the efficient pruned rate learning\nalgorithm and pruning approach for AdaptCL. Meanwhile, AdaptCL provides a\nmechanism for handling the trade-off between accuracy and time overhead and can\nbe combined with other techniques to accelerate training further. Empirical\nresults show that AdaptCL introduces little computing and communication\noverhead. AdaptCL achieves time savings of more than 41\\% on average and\nimproves accuracy in a low heterogeneous environment. In a highly heterogeneous\nenvironment, AdaptCL achieves a training speedup of 6.2x with a slight loss of\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 02:41:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Guangmeng", ""], ["Xu", "Ke", ""], ["Li", "Qi", ""], ["Liu", "Yang", ""], ["Zhao", "Yi", ""]]}, {"id": "2106.14149", "submitter": "Xu Wang Dr", "authors": "Xu Wang (1), Wei Ni (2), Xuan Zha (3), Guangsheng Yu (1), Ren Ping Liu\n  (1), Nektarios Georgalas (4), Andrew Reeves (4) ((1) Global Big Data\n  Technologies Centre, University of Technology Sydney, Australia, (2) Data61,\n  CSIRO, Australia, (3) China Academy of Information and Communications\n  Technology, Beijing, China, (4) Applied Research, British Telecom,\n  Martlesham, UK)", "title": "Capacity Analysis of Public Blockchain", "comments": null, "journal-ref": null, "doi": "10.1016/j.comcom.2021.06.019", "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As distributed ledgers, blockchains run consensus protocols which trade\ncapacity for consistency, especially in non-ideal networks with incomplete\nconnectivity and erroneous links. Existing studies on the tradeoff between\ncapacity and consistency are only qualitative or rely on specific assumptions.\nThis paper presents discrete-time Markov chain models to quantify the capacity\nof Proof-of-Work based public blockchains in non-ideal networks. The\ncomprehensive model is collapsed to be ergodic under the eventual consistency\nof blockchains, achieving tractability and efficient evaluations of blockchain\ncapacity. A closed-form expression for the capacity is derived in the case of\ntwo miners. Another important aspect is that we extend the ergodic model to\nanalyze the capacity under strong consistency, evaluating the robustness of\nblockchains against double-spending attacks. Validated by simulations, the\nproposed models are accurate and reveal the effect of link quality and the\ndistribution of mining rates on blockchain capacity and the ratio of stale\nblocks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 05:38:13 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Xu", ""], ["Ni", "Wei", ""], ["Zha", "Xuan", ""], ["Yu", "Guangsheng", ""], ["Liu", "Ren Ping", ""], ["Georgalas", "Nektarios", ""], ["Reeves", "Andrew", ""]]}, {"id": "2106.14265", "submitter": "Leon Witt", "authors": "Leon Witt, Usama Zafar, KuoYeh Shen, Felix Sattler, Dan Li, Wojciech\n  Samek", "title": "Reward-Based 1-bit Compressed Federated Distillation on Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advent of various forms of Federated Knowledge Distillation (FD)\npaves the way for a new generation of robust and communication-efficient\nFederated Learning (FL), where mere soft-labels are aggregated, rather than\nwhole gradients of Deep Neural Networks (DNN) as done in previous FL schemes.\nThis security-per-design approach in combination with increasingly performant\nInternet of Things (IoT) and mobile devices opens up a new realm of\npossibilities to utilize private data from industries as well as from\nindividuals as input for artificial intelligence model training. Yet in\nprevious FL systems, lack of trust due to the imbalance of power between\nworkers and a central authority, the assumption of altruistic worker\nparticipation and the inability to correctly measure and compare contributions\nof workers hinder this technology from scaling beyond small groups of already\nentrusted entities towards mass adoption. This work aims to mitigate the\naforementioned issues by introducing a novel decentralized federated learning\nframework where heavily compressed 1-bit soft-labels, resembling 1-hot label\npredictions, are aggregated on a smart contract. In a context where workers'\ncontributions are now easily comparable, we modify the Peer Truth Serum for\nCrowdsourcing mechanism (PTSC) for FD to reward honest participation based on\npeer consistency in an incentive compatible fashion. Due to heavy reductions of\nboth computational complexity and storage, our framework is a fully\non-blockchain FL system that is feasible on simple smart contracts and\ntherefore blockchain agnostic. We experimentally test our new framework and\nvalidate its theoretical properties.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:51:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Witt", "Leon", ""], ["Zafar", "Usama", ""], ["Shen", "KuoYeh", ""], ["Sattler", "Felix", ""], ["Li", "Dan", ""], ["Samek", "Wojciech", ""]]}, {"id": "2106.14312", "submitter": "Matthew Ding", "authors": "Matthew Ding", "title": "An Analysis of Multi-hop Iterative Approximate Byzantine Consensus with\n  Local Communication", "comments": "6 pages. arXiv admin note: text overlap with arXiv:2010.05098", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative Approximate Byzantine Consensus (IABC) is a fundamental problem of\nfault-tolerant distributed computing where machines seek to achieve approximate\nconsensus to arbitrary exactness in the presence of Byzantine failures. We\npresent a novel algorithm for this problem, named Relay-IABC, which relies on\nthe usage of a multi-hop relayed messaging system and crytographically secure\nmessage signatures. The use of signatures and relays allows the strict\nnecessary network conditions of traditional IABC algorithms to be circumvented.\nIn addition, we show evidence that Relay-IABC achieves faster convergence than\ntraditional algorithms even under these strict network conditions with both\ntheoretical analysis and experimental results.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 19:16:47 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ding", "Matthew", ""]]}, {"id": "2106.14332", "submitter": "Weile Wei", "authors": "Joseph Huber, Weile Wei, Giorgis Georgakoudis, Johannes Doerfert,\n  Oscar Hernandez", "title": "A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci cs.AR cs.CL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a methodology for using LLVM-based tools to tune the\nDCA++ (dynamical clusterapproximation) application that targets the new ARM\nA64FX processor. The goal is to describethe changes required for the new\narchitecture and generate efficient single instruction/multiple data(SIMD)\ninstructions that target the new Scalable Vector Extension instruction set.\nDuring manualtuning, the authors used the LLVM tools to improve code\nparallelization by using OpenMP SIMD,refactored the code and applied\ntransformation that enabled SIMD optimizations, and ensured thatthe correct\nlibraries were used to achieve optimal performance. By applying these code\nchanges, codespeed was increased by 1.98X and 78 GFlops were achieved on the\nA64FX processor. The authorsaim to automatize parts of the efforts in the\nOpenMP Advisor tool, which is built on top of existingand newly introduced LLVM\ntooling.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 22:38:16 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Huber", "Joseph", ""], ["Wei", "Weile", ""], ["Georgakoudis", "Giorgis", ""], ["Doerfert", "Johannes", ""], ["Hernandez", "Oscar", ""]]}, {"id": "2106.14347", "submitter": "Pradeep Dogga", "authors": "Pradeep Dogga (1), Karthik Narasimhan (2), Anirudh Sivaraman (3), Shiv\n  Kumar Saini (4), George Varghese (1), Ravi Netravali (2) ((1) UCLA, (2)\n  Princeton University, (3) NYU, (4) Adobe Research, India)", "title": "Revelio: ML-Generated Debugging Queries for Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major difficulty in debugging distributed systems lies in manually\ndetermining which of the many available debugging tools to use and how to query\nits logs. Our own study of a production debugging workflow confirms the\nmagnitude of this burden. This paper explores whether a machine-learning model\ncan assist developers in distributed systems debugging. We present Revelio, a\ndebugging assistant which takes user reports and system logs as input, and\noutputs debugging queries that developers can use to find a bug's root cause.\nThe key challenges lie in (1) combining inputs of different types (e.g.,\nnatural language reports and quantitative logs) and (2) generalizing to unseen\nfaults. Revelio addresses these by employing deep neural networks to uniformly\nembed diverse input sources and potential queries into a high-dimensional\nvector space. In addition, it exploits observations from production systems to\nfactorize query generation into two computationally and statistically simpler\nlearning tasks. To evaluate Revelio, we built a testbed with multiple\ndistributed applications and debugging tools. By injecting faults and training\non logs and reports from 800 Mechanical Turkers, we show that Revelio includes\nthe most helpful query in its predicted list of top-3 relevant queries 96% of\nthe time. Our developer study confirms the utility of Revelio.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 00:23:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Dogga", "Pradeep", ""], ["Narasimhan", "Karthik", ""], ["Sivaraman", "Anirudh", ""], ["Saini", "Shiv Kumar", ""], ["Varghese", "George", ""], ["Netravali", "Ravi", ""]]}, {"id": "2106.14357", "submitter": "Jorge Francisco Barreras", "authors": "Francisco Barreras, Mikhail Hayhoe, Hamed Hassani, Victor M. Preciado", "title": "AutoEKF: Scalable System Identification for COVID-19 Forecasting from\n  Large-Scale GPS Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an Extended Kalman Filter framework for system identification and\ncontrol of a stochastic high-dimensional epidemic model. The scale and severity\nof the COVID-19 emergency have highlighted the need for accurate forecasts of\nthe state of the pandemic at a high resolution. Mechanistic compartmental\nmodels are widely used to produce such forecasts and assist in the design of\ncontrol and relief policies. Unfortunately, the scale and stochastic nature of\nmany of these models often makes the estimation of their parameters difficult.\nWith the goal of calibrating a high dimensional COVID-19 model using low-level\nmobility data, we introduce a method for tractable maximum likelihood\nestimation that combines tools from Bayesian inference with scalable\noptimization techniques from machine learning. The proposed approach uses\nautomatic backward-differentiation to directly compute the gradient of the\nlikelihood of COVID-19 incidence and death data. The likelihood of the\nobservations is estimated recursively using an Extended Kalman Filter and can\nbe easily optimized using gradient-based methods to compute maximum likelihood\nestimators. Our compartmental model is trained using GPS mobility data that\nmeasures the mobility patterns of millions of mobile phones across the United\nStates. We show that, after calibrating against incidence and deaths data from\nthe city of Philadelphia, our model is able to produce an accurate 30-day\nforecast of the evolution of the pandemic.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 00:49:07 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Barreras", "Francisco", ""], ["Hayhoe", "Mikhail", ""], ["Hassani", "Hamed", ""], ["Preciado", "Victor M.", ""]]}, {"id": "2106.14402", "submitter": "Aydin Buluc", "authors": "Ariful Azad, Oguz Selvitopi, Md Taufique Hussain, John R. Gilbert,\n  Aydin Buluc", "title": "Combinatorial BLAS 2.0: Scaling combinatorial algorithms on\n  distributed-memory systems", "comments": "To appear in IEEE Transactions on Parallel and Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.PF math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial algorithms such as those that arise in graph analysis, modeling\nof discrete systems, bioinformatics, and chemistry, are often hard to\nparallelize. The Combinatorial BLAS library implements key computational\nprimitives for rapid development of combinatorial algorithms in\ndistributed-memory systems. During the decade since its first introduction, the\nCombinatorial BLAS library has evolved and expanded significantly.\n  This paper details many of the key technical features of Combinatorial BLAS\nversion 2.0, such as communication avoidance, hierarchical parallelism via\nin-node multithreading, accelerator support via GPU kernels, generalized\nsemiring support, implementations of key data structures and functions, and\nscalable distributed I/O operations for human-readable files. Our paper also\npresents several rules of thumb for choosing the right data structures and\nfunctions in Combinatorial BLAS 2.0, under various common application\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 05:32:37 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Azad", "Ariful", ""], ["Selvitopi", "Oguz", ""], ["Hussain", "Md Taufique", ""], ["Gilbert", "John R.", ""], ["Buluc", "Aydin", ""]]}, {"id": "2106.14423", "submitter": "Alessio Netti", "authors": "Alessio Netti, Michael Ott, Carla Guillen, Daniele Tafani and Martin\n  Schulz", "title": "Operational Data Analytics in Practice: Experiences from Design to\n  Deployment in Production HPC Environments", "comments": "Preliminary version of the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As HPC systems grow in complexity, efficient and manageable operation is\nincreasingly critical. Many centers are thus starting to explore the use of\nOperational Data Analytics (ODA) techniques, which extract knowledge from\nmassive amounts of monitoring data and use it for control and visualization\npurposes. As ODA is a multi-faceted problem, much effort has gone into\nresearching its separate aspects: however, accounts of production ODA\nexperiences are still hard to come across.\n  In this work we aim to bridge the gap between ODA research and production use\nby presenting our experiences with ODA in production, involving in particular\nthe control of cooling infrastructures and visualization of job data on two HPC\nsystems. We cover the entire development process, from design to deployment,\nhighlighting our insights in an effort to drive the community forward. We rely\non open-source tools, which make for a generic ODA framework suitable for most\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:03:08 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Netti", "Alessio", ""], ["Ott", "Michael", ""], ["Guillen", "Carla", ""], ["Tafani", "Daniele", ""], ["Schulz", "Martin", ""]]}, {"id": "2106.14479", "submitter": "Xia Jiang", "authors": "Xia Jiang, Xianlin Zeng, Jian Sun, Jie Chen", "title": "Distributed stochastic gradient tracking algorithm with variance\n  reduction for non-convex optimization", "comments": "11pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a distributed stochastic algorithm with variance\nreduction for general smooth non-convex finite-sum optimization, which has wide\napplications in signal processing and machine learning communities. In\ndistributed setting, large number of samples are allocated to multiple agents\nin the network. Each agent computes local stochastic gradient and communicates\nwith its neighbors to seek for the global optimum. In this paper, we develop a\nmodified variance reduction technique to deal with the variance introduced by\nstochastic gradients. Combining gradient tracking and variance reduction\ntechniques, this paper proposes a distributed stochastic algorithm, GT-VR, to\nsolve large-scale non-convex finite-sum optimization over multi-agent networks.\nA complete and rigorous proof shows that the GT-VR algorithm converges to\nfirst-order stationary points with $O(\\frac{1}{k})$ convergence rate. In\naddition, we provide the complexity analysis of the proposed algorithm.\nCompared with some existing first-order methods, the proposed algorithm has a\nlower $\\mathcal{O}(PM\\epsilon^{-1})$ gradient complexity under some mild\ncondition. By comparing state-of-the-art algorithms and GT-VR in experimental\nsimulations, we verify the efficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:43:30 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 01:09:49 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jiang", "Xia", ""], ["Zeng", "Xianlin", ""], ["Sun", "Jian", ""], ["Chen", "Jie", ""]]}, {"id": "2106.14503", "submitter": "Pravin Chandran", "authors": "Pravin Chandran, Raghavendra Bhat, Avinash Chakravarthi, Srikanth\n  Chandar", "title": "Weight Divergence Driven Divide-and-Conquer Approach for Optimal\n  Federated Learning from non-IID Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning allows training of data stored in distributed devices\nwithout the need for centralizing training data, thereby maintaining data\nprivacy. Addressing the ability to handle data heterogeneity (non-identical and\nindependent distribution or non-IID) is a key enabler for the wider deployment\nof Federated Learning. In this paper, we propose a novel Divide-and-Conquer\ntraining methodology that enables the use of the popular FedAvg aggregation\nalgorithm by overcoming the acknowledged FedAvg limitations in non-IID\nenvironments. We propose a novel use of Cosine-distance based Weight Divergence\nmetric to determine the exact point where a Deep Learning network can be\ndivided into class agnostic initial layers and class-specific deep layers for\nperforming a Divide and Conquer training. We show that the methodology achieves\ntrained model accuracy at par (and in certain cases exceeding) with numbers\nachieved by state-of-the-art Aggregation algorithms like FedProx, FedMA, etc.\nAlso, we show that this methodology leads to compute and bandwidth\noptimizations under certain documented conditions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 09:34:20 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 02:35:38 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chandran", "Pravin", ""], ["Bhat", "Raghavendra", ""], ["Chakravarthi", "Avinash", ""], ["Chandar", "Srikanth", ""]]}, {"id": "2106.14956", "submitter": "Berkay Turan", "authors": "Berkay Turan, Cesar A. Uribe, Hoi-To Wai, Mahnoosh Alizadeh", "title": "Robust Distributed Optimization With Randomly Corrupted Gradients", "comments": "17 pages, 3 figures, submitted to IEEE TSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a first-order distributed optimization algorithm\nthat is provably robust to Byzantine failures-arbitrary and potentially\nadversarial behavior, where all the participating agents are prone to failure.\nWe model each agent's state over time as a two-state Markov chain that\nindicates Byzantine or trustworthy behaviors at different time instants. We set\nno restrictions on the maximum number of Byzantine agents at any given time. We\ndesign our method based on three layers of defense: 1) Temporal gradient\naveraging, 2) robust aggregation, and 3) gradient normalization. We study two\nsettings for stochastic optimization, namely Sample Average Approximation and\nStochastic Approximation, and prove that for strongly convex and smooth\nnon-convex cost functions, our algorithm achieves order-optimal statistical\nerror and convergence rates.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 19:45:25 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Turan", "Berkay", ""], ["Uribe", "Cesar A.", ""], ["Wai", "Hoi-To", ""], ["Alizadeh", "Mahnoosh", ""]]}, {"id": "2106.14995", "submitter": "Youngdae Kim", "authors": "Youngdae Kim and Fran\\c{c}ois Pacaud and Kibaek Kim and Mihai Anitescu", "title": "Leveraging GPU batching for scalable nonlinear programming through\n  massive Lagrangian decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the implementation of a trust-region Newton algorithm ExaTron for\nbound-constrained nonlinear programming problems, fully running on multiple\nGPUs. Without data transfers between CPU and GPU, our implementation has\nachieved the elimination of a major performance bottleneck under a memory-bound\nsituation, particularly when solving many small problems in batch. We discuss\nthe design principles and implementation details for our kernel function and\ncore operations. Different design choices are justified by numerical\nexperiments. By using the application of distributed control of alternating\ncurrent optimal power flow, where a large problem is decomposed into many\nsmaller nonlinear programs using a Lagrangian approach, we demonstrate\ncomputational performance of ExaTron on the Summit supercomputer at Oak\nRidgeNational Laboratory. Our numerical results show the linear scaling with\nrespect to the batch size and the number of GPUs and more than 35 times speedup\non 6 GPUs than on 40 CPUs available on a single node.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 21:39:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kim", "Youngdae", ""], ["Pacaud", "Fran\u00e7ois", ""], ["Kim", "Kibaek", ""], ["Anitescu", "Mihai", ""]]}, {"id": "2106.15048", "submitter": "Mehrdad Kiamari", "authors": "Mehrdad Kiamari and Bhaskar Krishnamachari", "title": "Bottleneck Time Minimization for Distributed Iterative Processes:\n  Speeding Up Gossip-Based Federated Learning on Networked Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel task scheduling scheme for accelerating computational\napplications involving distributed iterative processes that are executed on\nnetworked computing resources. Such an application consists of multiple tasks,\neach of which outputs data at each iteration to be processed by neighboring\ntasks; these dependencies between the tasks can be represented as a directed\ngraph. We first mathematically formulate the problem as a Binary Quadratic\nProgram (BQP), accounting for both computation and communication costs. We show\nthat the problem is NP-hard. We then relax the problem as a Semi-Definite\nProgram (SDP) and utilize a randomized rounding technique based on sampling\nfrom a suitably-formulated multi-variate Gaussian distribution. Furthermore, we\nderive the expected value of bottleneck time. Finally, we apply our proposed\nscheme on gossip-based federated learning as an application of iterative\nprocesses. Through numerical evaluations on the MNIST and CIFAR-10 datasets, we\nshow that our proposed approach outperforms well-known scheduling techniques\nfrom distributed computing. In particular, for arbitrary settings, we show that\nit reduces bottleneck time by $91\\%$ compared to HEFT and $84\\%$ compared to\nthroughput HEFT.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 01:28:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kiamari", "Mehrdad", ""], ["Krishnamachari", "Bhaskar", ""]]}, {"id": "2106.15050", "submitter": "Mabrook Al-Rakhami Mr.", "authors": "Mabrook S. Al-Rakhami, Abdu Gumaei, Sk. Md. Mizanur Rahman and Atif\n  Al-Amri", "title": "Decentralized Blockchain-based model for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain technology is among the fastest-growing technologies in the world\ntoday. It has been adopted in diverse areas but mostly in financial systems,\nsuch as Bitcoin cryptocurrency. Therefore, it is a niche that has attracted\ninterest from researchers from various fields, including computer science.\nOther areas where Blockchain is being embraced are the Smart Grid and Internet\nof Things (IoT) technologies, among others. While it is all good and improving\nmany areas of applications, Blockchain still has some shortcomings. For\nexample, it is not designed for high scalability when accommodating normal\ntransactions. On the other hand, a parallel technology that has diverse\napplications in distributed networks better known as edge computing has\nemerged. Its main advantage is that it increases the speed of pf processes\nwithin those networks. However, like Blockchain, edge computing has its\nshortcomings. Its security systems and management systems have been found to be\nwanting. Hence the idea to integrate the two technologies and take advantage of\ntheir strengths. A blend of the two would lead to advanced network servers,\nhuge data storage, and heightened security in transactions. However, this\nintegration will best happen when some measures are taken. For example, there\nis a need to address scalability, resource management satisfactorily, and the\nsecurity of the systems. To solve the integration problem, a decentralized\nBlockchain-based model of Edge computing is proposed in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 01:35:37 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Al-Rakhami", "Mabrook S.", ""], ["Gumaei", "Abdu", ""], ["Rahman", "Sk. Md. Mizanur", ""], ["Al-Amri", "Atif", ""]]}, {"id": "2106.15112", "submitter": "Deepika Saxena", "authors": "Deepika Saxena and Ashutosh Kumar Singh", "title": "workload forecasting and resource management models based on machine\n  learning for cloud computing environments", "comments": "17 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The workload prediction and resource allocation significantly play an\ninevitable role in production of an efficient cloud environment. The proactive\nestimation of future workload followed by decision of resource allocation have\nbecome a prior solution to handle other in-built challenges like the\nunder/over-loading of physical machines, resource wastage, Quality-of-Services\n(QoS) violations, load balancing,VM migration and many more. In this context,\nthe paper presents a comprehensive survey of workload forecasting and\npredictive resource management models in cloud environment. A conceptual\nframework for workload forecasting and resource management, categorization of\nexisting machine learning based resources allocation techniques, and major\nchallenges of inefficient distribution of physical resource distribution are\ndiscussed pertaining to cloud computing. Thereafter, a thorough survey of\nexisting state-of-the-art contributions empowering machine learning based\napproaches in the field of cloud workload prediction and resource management\nare rendered. Finally, the paper explores and concludes various emerging\nchallenges and future research directions concerning elastic resource\nmanagement in cloud environment.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 06:23:30 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Saxena", "Deepika", ""], ["Singh", "Ashutosh Kumar", ""]]}, {"id": "2106.15151", "submitter": "Jongwook Woo", "authors": "Dalyapraz Dauletbak, Junghoon Heo, Sooyoung Kim, Yeon Pyo Kim,\n  Jongwook Woo", "title": "Scalable Traffic Predictive Analysis using GPU in Big Data", "comments": "5 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper adopts parallel computing systems for predictive analysis in both\nCPU and GPU leveraging Spark Big Data platform. The traffic dataset is adopted\nto predict the traffic jams in Los Angeles County. It is collected from a\npopular platform in the USA for tracking information on the road using the\ndevice information and reports shared by the users. Large-scale traffic data\nset can be stored and processed using both GPU and CPU in this Scalable Big\nData systems. The major contribution of this paper is to improve the\nperformance of machine learning in distributed parallel computing systems with\nGPU to predict the traffic congestion. We show that the parallel computing can\nbe achieve using both GPU and CPU with the existing Apache Spark platform. Our\nmethod can be applicable to other large scale datasets in different domains.\nThe process modeling, as well as results, are interpreted using computing time\nand metrics: AUC, Precision and Recall. It should help the traffic management\nin Smart City.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:12:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Dauletbak", "Dalyapraz", ""], ["Heo", "Junghoon", ""], ["Kim", "Sooyoung", ""], ["Kim", "Yeon Pyo", ""], ["Woo", "Jongwook", ""]]}, {"id": "2106.15216", "submitter": "Pengkun Yang", "authors": "Lili Su, Jiaming Xu, Pengkun Yang", "title": "Achieving Statistical Optimality of Federated Learning: Beyond\n  Stationary Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a promising framework that has great potentials in\nprivacy preservation and in lowering the computation load at the cloud. FedAvg\nand FedProx are two widely adopted algorithms. However, recent work raised\nconcerns on these two methods: (1) their fixed points do not correspond to the\nstationary points of the original optimization problem, and (2) the common\nmodel found might not generalize well locally.\n  In this paper, we alleviate these concerns. Towards this, we adopt the\nstatistical learning perspective yet allow the distributions to be\nheterogeneous and the local data to be unbalanced. We show, in the general\nkernel regression setting, that both FedAvg and FedProx converge to the\nminimax-optimal error rates. Moreover, when the kernel function has a finite\nrank, the convergence is exponentially fast. Our results further analytically\nquantify the impact of the model heterogeneity and characterize the federation\ngain - the reduction of the estimation error for a worker to join the federated\nlearning compared to the best local estimator. To the best of our knowledge, we\nare the first to show the achievability of minimax error rates under FedAvg and\nFedProx, and the first to characterize the gains in joining FL. Numerical\nexperiments further corroborate our theoretical findings on the statistical\noptimality of FedAvg and FedProx and the federation gains.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 09:59:43 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Su", "Lili", ""], ["Xu", "Jiaming", ""], ["Yang", "Pengkun", ""]]}, {"id": "2106.15315", "submitter": "Neil Agarwal", "authors": "Neil Agarwal, Ravi Netravali", "title": "Boggart: Accelerating Retrospective Video Analytics via Model-Agnostic\n  Ingest Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delivering fast responses to retrospective queries on video datasets is\ndifficult due to the large number of frames to consider and the high costs of\nrunning convolutional neural networks (CNNs) on each one. A natural solution is\nto perform a subset of the necessary computations ahead of time, as video is\ningested. However, existing ingest-time systems require knowledge of the\nspecific CNN that will be used in future queries -- a challenging requisite\ngiven the evergrowing space of CNN architectures and training\ndatasets/methodologies.\n  This paper presents Boggart, a retrospective video analytics system that\ndelivers ingest-time speedups in a model-agnostic manner. Our underlying\ninsight is that traditional computer vision (CV) algorithms are capable of\nperforming computations that can be used to accelerate diverse queries with\nwide-ranging CNNs. Building on this, at ingest-time, Boggart carefully employs\na variety of motion tracking algorithms to identify potential objects and their\ntrajectories across frames. Then, at query-time, Boggart uses several novel\ntechniques to collect the smallest sample of CNN results required to meet the\ntarget accuracy: (1) a clustering strategy to efficiently unearth the\ninevitable discrepancies between CV- and CNN-generated outputs, and (2) a set\nof accuracy-preserving propagation techniques to safely extend sampled results\nalong each trajectory. Across many videos, CNNs, and queries Boggart\nconsistently meets accuracy targets while using CNNs sparingly (on 3-54% of\nframes).\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:21:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Agarwal", "Neil", ""], ["Netravali", "Ravi", ""]]}, {"id": "2106.15335", "submitter": "Pierre Tholoniat", "authors": "Tao Luo, Mingen Pan, Pierre Tholoniat, Asaf Cidon, Roxana Geambasu,\n  Mathias L\\'ecuyer", "title": "Privacy Budget Scheduling", "comments": "Extended version of a paper presented at the 15th USENIX Symposium on\n  Operating Systems Design and Implementation (OSDI '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models trained on personal data have been shown to leak\ninformation about users. Differential privacy (DP) enables model training with\na guaranteed bound on this leakage. Each new model trained with DP increases\nthe bound on data leakage and can be seen as consuming part of a global privacy\nbudget that should not be exceeded. This budget is a scarce resource that must\nbe carefully managed to maximize the number of successfully trained models.\n  We describe PrivateKube, an extension to the popular Kubernetes datacenter\norchestrator that adds privacy as a new type of resource to be managed\nalongside other traditional compute resources, such as CPU, GPU, and memory.\nThe abstractions we design for the privacy resource mirror those defined by\nKubernetes for traditional resources, but there are also major differences. For\nexample, traditional compute resources are replenishable while privacy is not:\na CPU can be regained after a model finishes execution while privacy budget\ncannot. This distinction forces a re-design of the scheduler. We present DPF\n(Dominant Private Block Fairness) -- a variant of the popular Dominant Resource\nFairness (DRF) algorithm -- that is geared toward the non-replenishable privacy\nresource but enjoys similar theoretical properties as DRF.\n  We evaluate PrivateKube and DPF on microbenchmarks and an ML workload on\nAmazon Reviews data. Compared to existing baselines, DPF allows training more\nmodels under the same global privacy guarantee. This is especially true for DPF\nover R\\'enyi DP, a highly composable form of DP.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:43:47 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Luo", "Tao", ""], ["Pan", "Mingen", ""], ["Tholoniat", "Pierre", ""], ["Cidon", "Asaf", ""], ["Geambasu", "Roxana", ""], ["L\u00e9cuyer", "Mathias", ""]]}, {"id": "2106.15508", "submitter": "Sourabh Kulkarni", "authors": "Sourabh Kulkarni, Csaba Andras Moritz", "title": "Efficient State-space Exploration in Massively Parallel Simulation Based\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Simulation-based Inference (SBI) is a widely used set of algorithms to learn\nthe parameters of complex scientific simulation models. While primarily run on\nCPUs in HPC clusters, these algorithms have been shown to scale in performance\nwhen developed to be run on massively parallel architectures such as GPUs.\nWhile parallelizing existing SBI algorithms provides us with performance gains,\nthis might not be the most efficient way to utilize the achieved parallelism.\nThis work proposes a new algorithm, that builds on an existing SBI method -\nApproximate Bayesian Computation with Sequential Monte Carlo(ABC-SMC). This new\nalgorithm is designed to utilize the parallelism not only for performance gain,\nbut also toward qualitative benefits in the learnt parameters. The key idea is\nto replace the notion of a single 'step-size' hyperparameter, which governs how\nthe state space of parameters is explored during learning, with step-sizes\nsampled from a tuned Beta distribution. This allows this new ABC-SMC algorithm\nto more efficiently explore the state-space of the parameters being learnt. We\ntest the effectiveness of the proposed algorithm to learn parameters for an\nepidemiology model running on a Tesla T4 GPU. Compared to the parallelized\nstate-of-the-art SBI algorithm, we get similar quality results in $\\sim 100$x\nfewer simulations and observe ~80x lower run-to-run variance across 10\nindependent trials.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:39:26 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kulkarni", "Sourabh", ""], ["Moritz", "Csaba Andras", ""]]}, {"id": "2106.15531", "submitter": "Umberto Ferraro Petrillo", "authors": "Giuseppe Cattaneo, Umberto Ferraro Petrillo, Raffaele Giancarlo,\n  Francesco Palini, Chiara Romualdi", "title": "The Power of Alignment-Free Histogram-based Functions: a Comprehensive\n  Genome Scale Experimental Analysis -- Version 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Alignment-free (AF, for short) distance/similarity functions are\na key tool for sequence analysis. Experimental studies on real datasets abound\nand, to some extent, there are also studies regarding their control of false\npositive rate (Type I error). However, assessment of their power, i.e., their\nability to identify true similarity, has been limited to some members or\nvariants of the \\Dd family and the asymptotic theoretic results have been\ncomplemented by experimental studies on short sequences, not adequate for\ncurrent genome-scale applications. Such a State of the Art is methodologically\nproblematic, since information regarding a key feature such as power is either\nmissing or limited.\n  Results: By concentrating on histogram-based AF functions, we perform the\nfirst coherent and uniform evaluation of the power of those functions,\ninvolving also Type I error for completeness. The experiments carried out are\nextensive, as we use two Alternative models of important genomic features (CIS\nRegulatory Modules and Horizontal Gene Transfer), sequence lengths from a few\nthousand to millions and different values of $k$. As a result, and using power,\nwe provide a characterization of those AF functions that is novel and\ninformative. Indeed, we identify weak and strong points of each function\nconsidered, which may be used as a guide to choose one for analysis tasks. In\nsynthesis, and quite remarkably, of the fifteen functions that we have\nconsidered, only four stand out. Finally, in order to encourage the use of our\nmethodology for validation of future AF functions, the Big Data platform\nsupporting it is public.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 06:26:39 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Cattaneo", "Giuseppe", ""], ["Petrillo", "Umberto Ferraro", ""], ["Giancarlo", "Raffaele", ""], ["Palini", "Francesco", ""], ["Romualdi", "Chiara", ""]]}, {"id": "2106.15549", "submitter": "Avah Banerjee", "authors": "Avah Banerjee, Guoli Ding, Maxwell Reeser", "title": "Distributed Matrix Tiling Using A Hypergraph Labeling Formulation", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Partitioning large matrices is an important problem in distributed linear\nalgebra computing (used in ML among others). Briefly, our goal is to perform a\nsequence of matrix algebra operations in a distributed manner (whenever\npossible) on these large matrices. However, not all partitioning schemes work\nwell with different matrix algebra operations and their implementations\n(algorithms). This is a type of data tiling problem. In this work we consider a\ntheoretical model for a version of the matrix tiling problem in the setting of\nhypergraph labeling. We prove some hardness results and give a theoretical\ncharacterization of its complexity on random instances. Additionally we develop\na greedy algorithm and experimentally show its efficacy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:35:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Banerjee", "Avah", ""], ["Ding", "Guoli", ""], ["Reeser", "Maxwell", ""]]}, {"id": "2106.15554", "submitter": "Jennifer Welch", "authors": "Hagit Attiya, Constantin Enea, Jennifer L. Welch", "title": "Linearizable Implementations Suffice for Termination of Randomized\n  Concurrent Programs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strong adversaries obtain additional power when a linearizable object is\nsubstituted instead of an atomic object in a concurrent program. This paper\nsuggests a novel approach to blunting this additional power, without relying on\nstrongly linearizable implementations. Instead, a simple modification of some\nexisting linearizable implementations is proposed with the property that if a\nconcurrent program has non-zero termination probability when used with atomic\nobjects, then it also has non-zero termination probability when it is used with\nthe modified linearizable implementations. Our results apply to the ABD\nimplementation of a shared register in asynchronous message-passing systems and\nalso to AAD+ linearizable snapshots in asynchronous shared-memory systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:45:32 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Attiya", "Hagit", ""], ["Enea", "Constantin", ""], ["Welch", "Jennifer L.", ""]]}, {"id": "2106.15555", "submitter": "David Quaresma", "authors": "David Ferreira Quaresma, Thiago Emmanuel Pereira, Daniel Fireman", "title": "Validation of a simulation model for FaaS performance benchmarking using\n  predictive validation", "comments": "The final paper of bachelor's degree, 8 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the paper Controlling Garbage Collection and Request Admission to Improve\nPerformance of FaaS Applications, we verified and evaluated the impact of\nmemory management mechanics of programming languages in the context of\nFunctions as a Service (FaaS) via simulation experiments. The results of this\nstudy pointed to an impact of up to 11.68% on the response time of requests\nwhen a garbage collector procedure was executed during the execution of a\nCPU-bound function. As future work, we listed a few threats to the validity of\nthe results attained, and among them, we cited the validation of the simulation\nmodel used. The validation of the model is important because it validates the\nresults generated in the simulation experiments, which ensures realistic\nresults. In this work, we proposed and executed a validation to the simulation\nmodel used in the previous work. To do so, we run measurement experiments in a\npublic FaaS platform and simulation experiments of the same scenarios using the\nsame simulator of the previous paper. Then, we validate the simulator by\ncomparing the results obtained in both experiments to ensure that the\nsimulation result and the measurement one are equivalent.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:46:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Quaresma", "David Ferreira", ""], ["Pereira", "Thiago Emmanuel", ""], ["Fireman", "Daniel", ""]]}, {"id": "2106.15565", "submitter": "Daniele De Sensi PhD", "authors": "Daniele De Sensi, Salvatore Di Girolamo, Saleh Ashkboos, Shigang Li,\n  Torsten Hoefler", "title": "Flare: Flexible In-Network Allreduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The allreduce operation is one of the most commonly used communication\nroutines in distributed applications. To improve its bandwidth and to reduce\nnetwork traffic, this operation can be accelerated by offloading it to network\nswitches, that aggregate the data received from the hosts, and send them back\nthe aggregated result. However, existing solutions provide limited\ncustomization opportunities and might provide suboptimal performance when\ndealing with custom operators and data types, with sparse data, or when\nreproducibility of the aggregation is a concern. To deal with these problems,\nin this work we design a flexible programmable switch by using as a building\nblock PsPIN, a RISC-V architecture implementing the sPIN programming model. We\nthen design, model, and analyze different algorithms for executing the\naggregation on this architecture, showing performance improvements compared to\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:58:32 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["De Sensi", "Daniele", ""], ["Di Girolamo", "Salvatore", ""], ["Ashkboos", "Saleh", ""], ["Li", "Shigang", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2106.15689", "submitter": "Blesson Varghese", "authors": "Ayesha Abdul Majeed and Peter Kilpatrick and Ivor Spence and Blesson\n  Varghese", "title": "NEUKONFIG: Reducing Edge Service Downtime When Repartitioning DNNs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) may be partitioned across the edge and the cloud\nto improve the performance efficiency of inference. DNN partitions are\ndetermined based on operational conditions such as network speed. When\noperational conditions change DNNs will need to be repartitioned to maintain\nthe overall performance. However, repartitioning using existing approaches,\nsuch as Pause and Resume, will incur a service downtime on the edge. This paper\npresents the NEUKONFIG framework that identifies the service downtime incurred\nwhen repartitioning DNNs and proposes approaches for reducing edge service\ndowntime. The proposed approaches are based on 'Dynamic Switching' in which,\nwhen the network speed changes and given an existing edge-cloud pipeline, a new\nedge-cloud pipeline is initialised with new DNN partitions. Incoming inference\nrequests are switched to the new pipeline for processing data. Two dynamic\nswitching scenarios are considered: when a second edge-cloud pipeline is always\nrunning and when a second pipeline is only initialised when the network speed\nchanges. Experimental studies are carried out on a lab-based testbed to\ndemonstrate that Dynamic Switching reduces the downtime by at least an order of\nmagnitude when compared to a baseline using Pause and Resume that has a\ndowntime of 6 seconds. A trade-off in the edge service downtime and memory\nrequired is noted. The Dynamic Switching approach that requires the same amount\nof memory as the baseline reduces the edge service downtime to 0.6 seconds and\nto less than 1 millisecond in the best case when twice the amount of memory as\nthe baseline is available.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:42:02 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Majeed", "Ayesha Abdul", ""], ["Kilpatrick", "Peter", ""], ["Spence", "Ivor", ""], ["Varghese", "Blesson", ""]]}, {"id": "2106.15718", "submitter": "Hamed Moasses", "authors": "Hamed Moasses, Abdulbaghi Ghaderzadeh, Keyhan Khamforoosh", "title": "HetEng: An Improved Distributed Energy Efficient Clustering Scheme for\n  Heterogeneous IoT Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network lifetime is always a challenging issue in battery-powered networks\ndue to the difficulty of recharging or replacing nodes in some scenarios.\nClustering methods are a promising approach to tackle this challenge and\nprolong lifetime by efficiently distributing tasks among nodes in the cluster.\nThe present study aimed to improve energy consumption in heterogeneous IoT\ndevices using an energy-aware clustering method. In a heterogeneous IoT\nnetwork, nodes (i.e., battery-powered IoT devices) can have a variety of energy\nprofiles and communication capabilities. Most of the existing clustering\nalgorithms have neglected the heterogeneity of energy capacity among nodes and\nassumed that they are of the same energy level. In this work, we present\nHetEng, a Cluster Head (CH) selection process that extended an existing\nclustering algorithm, named Smart-BEEM. To this end, we proposed a statistical\napproach that distributes energy consumption among highly energetic nodes in\nthe network topology by constantly changing the CH role between the nodes based\non their real energy levels (in joules). Experimental results showed that\nHetEng resulted in a 6.6% increase of alive nodes and 3% improvement in\nresidual energy among the nodes in comparison with SmartBEEM. Moreover, our\nmethod reduced the total number of iterations by 1% on average.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 20:47:58 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Moasses", "Hamed", ""], ["Ghaderzadeh", "Abdulbaghi", ""], ["Khamforoosh", "Keyhan", ""]]}, {"id": "2106.15911", "submitter": "Michal Merta", "authors": "Raphael Watschinger, Michal Merta, G\\\"unther Of, Jan Zapletal", "title": "A parallel fast multipole method for a space-time boundary element\n  method for the heat equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to the parallelization of the parabolic fast\nmultipole method for a space-time boundary element method for the heat\nequation. We exploit the special temporal structure of the involved operators\nto provide an efficient distributed parallelization with respect to time and\nwith a one-directional communication pattern. On top, we apply a task-based\nshared memory parallelization and SIMD vectorization. In the numerical tests we\nobserve high efficiencies of our parallelization approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:57:52 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Watschinger", "Raphael", ""], ["Merta", "Michal", ""], ["Of", "G\u00fcnther", ""], ["Zapletal", "Jan", ""]]}, {"id": "2106.16002", "submitter": "Ziqin Chen", "authors": "Ziqin Chen, Ji Ma, Shu Liang, Li Li", "title": "Distributed Nash Equilibrium Seeking under Quantization Communication", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates Nash equilibrium (NE) seeking problems for\nnoncooperative games over multi-agent networks with finite bandwidth\ncommunication. A distributed quantized algorithm is presented, which consists\nof local gradient play, distributed decision estimating, and adaptive\nquantization. Exponential convergence of the algorithm is established, and a\nrelationship between the convergence rate and the bandwidth is quantitatively\nanalyzed. Finally, a simulation of an energy consumption game is presented to\nvalidate the proposed results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:51:31 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chen", "Ziqin", ""], ["Ma", "Ji", ""], ["Liang", "Shu", ""], ["Li", "Li", ""]]}, {"id": "2106.16064", "submitter": "Guyue Huang", "authors": "Guyue Huang, Guohao Dai, Yu Wang, Yufei Ding, Yuan Xie", "title": "Efficient Sparse Matrix Kernels based on Adaptive Workload-Balancing and\n  Parallel-Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sparse matrix-vector and matrix-matrix multiplication (SpMV and SpMM) are\nfundamental in both conventional (graph analytics, scientific computing) and\nemerging (sparse DNN, GNN) domains. Workload-balancing and parallel-reduction\nare widely-used design principles for efficient SpMV. However, prior work fails\nto resolve how to implement and adaptively use the two principles for SpMV/MM.\nTo overcome this obstacle, we first complete the implementation space with\noptimizations by filling three missing pieces in prior work, including: (1) We\nshow that workload-balancing and parallel-reduction can be combined through a\nsegment-reduction algorithm implemented with SIMD-shuffle primitives. (2) We\nshow that parallel-reduction can be implemented in SpMM through loading the\ndense-matrix rows with vector memory operations. (3) We show that vectorized\nloading of sparse rows, being a part of the benefit of parallel-reduction, can\nco-exist with sequential-reduction in SpMM through temporally caching\nsparse-matrix elements in the shared memory. In terms of adaptive use, we\nanalyze how the benefit of two principles change with two characteristics from\nthe input data space: the diverse sparsity pattern and dense-matrix width. We\nfind the benefit of the two principles fades along with the increased total\nworkload, i.e. the increased dense-matrix width. We also identify, for SpMV and\nSpMM, different sparse-matrix features that impact workload-balancing\neffectiveness. Our design consistently exceeds cuSPARSE by 1.07-1.57x on\ndifferent GPUs and dense matrix width, and the kernel selection rules involve\n5-12% performance loss compared with optimal choices. Our kernel is being\nintegrated into popular graph learning frameworks to accelerate GNN training.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:45:47 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Huang", "Guyue", ""], ["Dai", "Guohao", ""], ["Wang", "Yu", ""], ["Ding", "Yufei", ""], ["Xie", "Yuan", ""]]}, {"id": "2106.16140", "submitter": "Ying Zhang", "authors": "Ying Zhang", "title": "Revisiting Time, Clocks, and Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sub-nanosecond precision clock synchronization over the packet network has\nbeen achieved by the White Rabbit protocol for a decade. However, few computer\nsystems utilize such a technique. We try to attract more interest in the clock\nsynchronization problem. We first introduce the basics of clock and\nsynchronization in the time and frequency discipline. Then we revisit several\nrelated works, such as Google's Spanner, Huygens, FARMv2, DTP, and Sundial,\nexplain why these works could be improved. Finally, we briefly discuss an\nindependent time network approach towards low-cost and high-precision\nsynchronization.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:21:58 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhang", "Ying", ""]]}]