[{"id": "1807.00151", "submitter": "Ricardo P\\'erez-Marco", "authors": "C. Grunspan, R. P\\'erez-Marco", "title": "Ant routing algorithm for the Lightning Network", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a decentralized routing algorithm that can be implemented in\nBitcoin Lightning Network. All nodes in the network contribute equally to path\nsearching. The algorithm is inspired from ant path searching algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 10:33:02 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 10:41:51 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Grunspan", "C.", ""], ["P\u00e9rez-Marco", "R.", ""]]}, {"id": "1807.00217", "submitter": "Kamel Abdelouahab", "authors": "Kamel Abdelouahab, Fran\\c{c}ois Berry, Maxime Pelcat", "title": "The Challenge of Multi-Operand Adders in CNNs on FPGAs: How not to solve\n  it!", "comments": "Proceedings of the International Conference on Embedded Computer\n  Systems: Architectures, Modeling, and Simulation - SAMOS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are computationally intensive algorithms\nthat currently require dedicated hardware to be executed. In the case of\nFPGA-Based accelerators, we point-out in this work the challenge of\nMulti-Operand Adders (MOAs) and their high resource utilization in an FPGA\nimplementation of a CNN. To address this challenge, two optimization\nstrategies, that rely on time-multiplexing and approximate computing, are\ninvestigated. At first glance, the two strategies looked promising to reduce\nthe footprint of a given architectural mapping, but when synthesized on the\ndevice, none of them gave the expected results. Experimental sections analyze\nthe reasons of these unexpected results.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 18:57:43 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Abdelouahab", "Kamel", ""], ["Berry", "Fran\u00e7ois", ""], ["Pelcat", "Maxime", ""]]}, {"id": "1807.00220", "submitter": "Nitish Mital", "authors": "Nitish Mital, Katina Kralevska, Deniz Gunduz and Cong Ling", "title": "Storage-Repair Bandwidth Trade-off for Wireless Caching with Partial\n  Failure and Broadcast Repair", "comments": "Conference version of this paper has been submitted for review in ITW\n  2018. This submission includes the proof of theorem 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repair of multiple partially failed cache nodes is studied in a distributed\nwireless content caching system, where $r$ out of a total of $n$ cache nodes\nlose part of their cached data. Broadcast repair of failed cache contents at\nthe network edge is studied; that is, the surviving cache nodes transmit\nbroadcast messages to the failed ones, which are then used, together with the\nsurviving data in their local cache memories, to recover the lost content. The\ntrade-off between the storage capacity and the repair bandwidth is derived. It\nis shown that utilizing the broadcast nature of the wireless medium and the\nsurviving cache contents at partially failed nodes significantly reduces the\nrequired repair bandwidth per node.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 19:33:37 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Mital", "Nitish", ""], ["Kralevska", "Katina", ""], ["Gunduz", "Deniz", ""], ["Ling", "Cong", ""]]}, {"id": "1807.00298", "submitter": "JunPing Wang", "authors": "JunPing Wang, WenSheng Zhang, Ian Thomas, ShiHui Duan, YouKang Shi", "title": "Multi-Task Generative Adversarial Nets with Shared Memory for\n  Cross-Domain Coordination Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Generating sequential decision process from huge amounts of measured process\ndata is a future research direction for collaborative factory automation,\nmaking full use of those online or offline process data to directly design\nflexible make decisions policy, and evaluate performance. The key challenges\nfor the sequential decision process is to online generate sequential\ndecision-making policy directly, and transferring knowledge across tasks\ndomain. Most multi-task policy generating algorithms often suffer from\ninsufficient generating cross-task sharing structure at discrete-time nonlinear\nsystems with applications. This paper proposes the multi-task generative\nadversarial nets with shared memory for cross-domain coordination control,\nwhich can generate sequential decision policy directly from raw sensory input\nof all of tasks, and online evaluate performance of system actions in\ndiscrete-time nonlinear systems. Experiments have been undertaken using a\nprofessional flexible manufacturing testbed deployed within a smart factory of\nWeichai Power in China. Results on three groups of discrete-time nonlinear\ncontrol tasks show that our proposed model can availably improve the\nperformance of task with the help of other related tasks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 09:07:04 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Wang", "JunPing", ""], ["Zhang", "WenSheng", ""], ["Thomas", "Ian", ""], ["Duan", "ShiHui", ""], ["Shi", "YouKang", ""]]}, {"id": "1807.00312", "submitter": "Ralf-Peter Mundani", "authors": "Christoph Ertl (1), Ralf-Peter Mundani (1), Ernst Rank (1) ((1)\n  Technische Universit\\\"at M\\\"unchen, Munich, Germany)", "title": "Ensuring domain consistency in an adaptive framework with distributed\n  topology for fluid flow simulations", "comments": "8 pages, 5 figures, submitted to IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-tier parallel computing clusters continue to accumulate more and more\ncomputational power with more and better CPUs and Networks. This allows,\nespecially for environmental simulations, computations with larger domain sizes\nand better resolution. One of the challenges becoming increasingly important is\nthe decomposition and distribution of the overall work load. State-of-the-art\nparallel codes usually use solutions that involve complete knowledge of the\ndomain topology, which will lead to communication and memory bottlenecks when\ncomputing very large domains. To meet this challenge, the authors propose a new\nstrategy for decentralised domain management, based on a proven hierarchic data\nstructure. On the way of developing a framework where individual sub-domains\nonly have local knowledge of their surroundings, this contribution describes\nthe communication patterns used in ensuring a consistent domain, without the\nneed for expensive global broadcast messages. Furthermore, the routines\nnecessary to deal with adaptive changes in domain topology, due to refinement,\ncoarsening and migration of sub-domains to different computational resources,\nare discussed in detail.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 11:10:35 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Ertl", "Christoph", ""], ["Mundani", "Ralf-Peter", ""], ["Rank", "Ernst", ""]]}, {"id": "1807.00324", "submitter": "Mohammad Shojafar", "authors": "Mohammad M. Tajiki, Mohammad Shojafar, Behzad Akbari, Stefano Salsano,\n  Mauro Conti, Mukesh Singhal", "title": "Joint Failure Recovery, Fault Prevention, and Energy-efficient Resource\n  Management for Real-time SFC in Fog-supported SDN", "comments": "24 pages, 10 figures, 3 tables, Computer Networks Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on the problems of traffic engineering, failure\nrecovery, fault prevention, and Service Function Chain (SFC) with reliability\nand energy consumption constraints in Software Defined Networks (SDN). These\ntypes of deployments use Fog computing as an emerging paradigm to manage the\ndistributed small-size traffic flows passing through the SDN-enabled switches\n(possibly Fog Nodes). The main aim of this integration is to support service\ndelivery in real-time, failure recovery, and fault-awareness in an SFC context.\nFirstly, we present an architecture for Failure Recovery and Fault Prevention\ncalled FRFP; this is a multi-tier structure in which the real-time traffic\nflows pass through SDN-enabled switches to jointly decrease the network\nside-effects of flow rerouting and energy consumption of the Fog Nodes. We then\nmathematically formulate an optimization problem called the Optimal\nFog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a\nnear-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding\nproblem in polynomial time. In this way, the energy consumption and the\nreliability of the selected paths are optimized, while the Quality of Service\nconstraints are met and the network congestion is minimized. In a reliability\ncontext, the focus of this work is on fault prevention; however, since we use a\nreallocation technique, the proposed scheme can be used as a failure recovery\nscheme. We compare the performance of HFES and OFES in terms of power\nconsumption, average path length, fault probability, network side-effects, link\nutilization, and Fog Node utilization. Additionally, we analyze the\ncomputational complexity of HFES. We use a real-world network topology to\nevaluate our algorithm. The simulation results show that the heuristic\nalgorithm is applicable to large-scale networks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 12:31:45 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Tajiki", "Mohammad M.", ""], ["Shojafar", "Mohammad", ""], ["Akbari", "Behzad", ""], ["Salsano", "Stefano", ""], ["Conti", "Mauro", ""], ["Singhal", "Mukesh", ""]]}, {"id": "1807.00331", "submitter": "Michael Blondin", "authors": "Michael Blondin, Javier Esparza, Anton\\'in Ku\\v{c}era", "title": "Automatic Analysis of Expected Termination Time for Population Protocols", "comments": "Extended version of CONCUR 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a formal model of sensor networks consisting of\nidentical mobile devices. Two devices can interact and thereby change their\nstates. Computations are infinite sequences of interactions in which the\ninteracting devices are chosen uniformly at random.\n  In well designed population protocols, for every initial configuration of\ndevices, and for every computation starting at this configuration, all devices\neventually agree on a consensus value. We address the problem of automatically\ncomputing a parametric bound on the expected time the protocol needs to reach\nthis consensus. We present the first algorithm that, when successful, outputs a\nfunction f(n) such that the expected time to consensus is bound by O(f(n)),\nwhere n is the number of devices executing the protocol. We experimentally show\nthat our algorithm terminates and provides good bounds for many of the\nprotocols found in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 13:49:05 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Blondin", "Michael", ""], ["Esparza", "Javier", ""], ["Ku\u010dera", "Anton\u00edn", ""]]}, {"id": "1807.00368", "submitter": "Francesco Pace", "authors": "Francesco Pace, Dimitrios Milios, Damiano Carra, Daniele Venzano and\n  Pietro Michiardi", "title": "A Data-Driven Approach to Dynamically Adjust Resource Allocation for\n  Compute Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, data-centers are largely under-utilized because resource allocation\nis based on reservation mechanisms which ignore actual resource utilization.\nIndeed, it is common to reserve resources for peak demand, which may occur only\nfor a small portion of the application life time. As a consequence, cluster\nresources often go under-utilized.\n  In this work, we propose a mechanism that improves cluster utilization, thus\ndecreasing the average turnaround time, while preventing application failures\ndue to contention in accessing finite resources such as RAM. Our approach\nmonitors resource utilization and employs a data-driven approach to resource\ndemand forecasting, featuring quantification of uncertainty in the predictions.\nUsing demand forecast and its confidence, our mechanism modulates cluster\nresources assigned to running applications, and reduces the turnaround time by\nmore than one order of magnitude while keeping application failures under\ncontrol. Thus, tenants enjoy a responsive system and providers benefit from an\nefficient cluster utilization.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 18:28:20 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Pace", "Francesco", ""], ["Milios", "Dimitrios", ""], ["Carra", "Damiano", ""], ["Venzano", "Daniele", ""], ["Michiardi", "Pietro", ""]]}, {"id": "1807.00394", "submitter": "Ralf-Peter Mundani", "authors": "Ralf-Peter Mundani (1), Marko Ljucovi\\'c (2), Ernst Rank (1) ((1)\n  Technische Universit\\\"at M\\\"unchen, Munich, Germany, (2) Western Michigan\n  University, Kalamazoo, MI, USA)", "title": "Framework for the hybrid parallelisation of simulation codes", "comments": "9 pages, 3 figures, Proceedings of the Second International\n  Conference on Parallel, Distributed, Grid and Cloud Computing for Engineering\n  (2011)", "journal-ref": null, "doi": "10.4203/ccp.95.53", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing efficient hybrid parallel code is tedious, error-prone, and requires\ngood knowledge of both parallel programming and multithreading such as MPI and\nOpenMP, resp. Therefore, we present a framework which is based on a job model\nthat allows the user to incorporate his sequential code with manageable effort\nand code modifications in order to be executed in parallel on clusters or\nsupercomputers built from modern multi-core CPUs. The primary application\ndomain of this framework are simulation codes from engineering disciplines as\nthose are in many cases still sequential and due to their memory and runtime\ndemands prominent candidates for parallelisation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 20:53:57 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Mundani", "Ralf-Peter", ""], ["Ljucovi\u0107", "Marko", ""], ["Rank", "Ernst", ""]]}, {"id": "1807.00581", "submitter": "Ralf-Peter Mundani", "authors": "Ralf-Peter Mundani (1), Alexander D\\\"uster (1), Jovana Kne\\v{z}evi\\'c\n  (2), Andreas Niggl (3), Ernst Rank (1) ((1) Technische Universit\\\"at\n  M\\\"unchen, Munich, Germany, (2) University of Belgrade, Belgrade, Serbia, (3)\n  SOFiSTiK AG, Oberschleissheim, Germany)", "title": "Dynamic load balancing strategies for hierarchical p-FEM solvers", "comments": "8 pages, 2 figures", "journal-ref": "Recent Advances in Parallel Virtual Machine and Message Passing\n  Interface, Lecture Notes in Computer Science 5759 (2009) 305-312", "doi": "10.1007/978-3-642-03770-2_37", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equation systems resulting from a p-version FEM discretisation typically\nrequire a special treatment as iterative solvers are not very efficient here.\nApplying hierarchical concepts based on a nested dissection approach allow for\nboth the design of sophisticated solvers as well as for advanced\nparallelisation strategies. To fully exploit the underlying computing power of\nparallel systems, dynamic load balancing strategies become an essential\ncomponent.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 10:29:09 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Mundani", "Ralf-Peter", ""], ["D\u00fcster", "Alexander", ""], ["Kne\u017eevi\u0107", "Jovana", ""], ["Niggl", "Andreas", ""], ["Rank", "Ernst", ""]]}, {"id": "1807.00769", "submitter": "Ralf-Peter Mundani", "authors": "Jovana Kne\\v{z}evi\\'c (1), Ralf-Peter Mundani (1), Ernst Rank (1) ((1)\n  Technische Universit\\\"at M\\\"unchen, Munich, Germany)", "title": "A high-performance interactive computing framework for engineering\n  applications", "comments": "23 pages, 9 figures", "journal-ref": "Advanced Computing, Lecture Notes in Computational Science and\n  Engineering 93 (2013) 177-199", "doi": "10.1007/978-3-642-38762-3_9", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To harness the potential of advanced computing technologies, efficient (real\ntime) analysis of large amounts of data is as essential as are front-line\nsimulations. In order to optimise this process, experts need to be supported by\nappropriate tools that allow to interactively guide both the computation and\ndata exploration of the underlying simulation code. The main challenge is to\nseamlessly feed the user requirements back into the simulation.\nState-of-the-art attempts to achieve this, have resulted in the insertion of\nso-called check- and break-points at fixed places in the code. Depending on the\nsize of the problem, this can still compromise the benefits of such an attempt,\nthus, preventing the experience of real interactive computing. To leverage the\nconcept for a broader scope of applications, it is essential that a user\nreceives an immediate response from the simulation to his or her changes. Our\ngeneric integration framework, targeted to the needs of the computational\nengineering domain, supports distributed computations as well as on-the-fly\nvisualisation in order to reduce latency and enable a high degree of\ninteractivity with only minor code modifications. Namely, the regular course of\nthe simulation coupled to our framework is interrupted in small, cyclic\nintervals followed by a check for updates. When new data is received, the\nsimulation restarts automatically with the updated settings (boundary\nconditions, simulation parameters, etc.). To obtain rapid, albeit approximate\nfeedback from the simulation in case of perpetual user interaction, a\nmulti-hierarchical approach is advantageous. Within several different\nengineering test cases, we will demonstrate the flexibility and the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 16:08:03 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kne\u017eevi\u0107", "Jovana", ""], ["Mundani", "Ralf-Peter", ""], ["Rank", "Ernst", ""]]}, {"id": "1807.00851", "submitter": "Konstantinos Psychas", "authors": "Konstantinos Psychas, Javad Ghaderi", "title": "On Non-Preemptive VM Scheduling in the Cloud", "comments": "29 pages", "journal-ref": "POMACS, Volume 1, Issue 2, December 2017, Article No. 35", "doi": "10.1145/3154493", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of scheduling VMs (Virtual Machines) in a distributed\nserver platform, motivated by cloud computing applications. The VMs arrive\ndynamically over time to the system, and require a certain amount of resources\n(e.g. memory, CPU, etc) for the duration of their service. To avoid costly\npreemptions, we consider non-preemptive scheduling: Each VM has to be assigned\nto a server which has enough residual capacity to accommodate it, and once a VM\nis assigned to a server, its service \\textit{cannot} be disrupted (preempted).\nPrior approaches to this problem either have high complexity, require\nsynchronization among the servers, or yield queue sizes/delays which are\nexcessively large. We propose a non-preemptive scheduling algorithm that\nresolves these issues. In general, given an approximation algorithm to Knapsack\nwith approximation ratio $r$, our scheduling algorithm can provide $r\\beta$\nfraction of the throughput region for $\\beta < r$. In the special case of a\ngreedy approximation algorithm to Knapsack, we further show that this condition\ncan be relaxed to $\\beta<1$. The parameters $\\beta$ and $r$ can be tuned to\nprovide a tradeoff between achievable throughput, delay, and computational\ncomplexity of the scheduling algorithm. Finally extensive simulation results\nusing both synthetic and real traffic traces are presented to verify the\nperformance of our algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 18:27:28 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Psychas", "Konstantinos", ""], ["Ghaderi", "Javad", ""]]}, {"id": "1807.00955", "submitter": "Zixuan Zhang", "authors": "Michael Zargham, Zixuan Zhang, Victor Preciado", "title": "A State-Space Modeling Framework for Engineering Blockchain-Enabled\n  Economic Systems", "comments": "8 pages, 2 figures, The Ninth International Conference on Complex\n  Systems Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized Ledger Technology, popularized by the Bitcoin network, aims to\nkeep track of a ledger of valid transactions between agents of a virtual\neconomy without a central institution for coordination. In order to keep track\nof a faithful and accurate list of transactions, the ledger is broadcast and\nreplicated across machines in a peer-to-peer network. To enforce validity of\ntransactions in the ledger (i.e., no negative balance or double spending), the\nnetwork as a whole coordinates to accept or reject new transactions based on a\nset of rules aiming to detect and block operations of malicious agents (i.e.,\nByzantine attacks). Consensus protocols are particularly important to\ncoordinate operation of the network, since they are used to reconcile\npotentially conflicting versions of the ledger. Regardless of architecture and\nconsensus mechanism used, resulting economic networks remain largely similar,\nwith economic agents driven by incentives under a set of rules. Due to the\nintense activity in this area, proper mathematical frameworks to model and\nanalyze behavior of blockchain-enabled systems are essential. In this paper, we\naddress this need and provide the following contributions: (i) we establish a\nformal framework, with tools from dynamical systems theory, to mathematically\ndescribe core concepts in blockchain-enabled networks, (ii) we apply this\nframework to the Bitcoin network and recover its key properties, and (iii) we\nconnect our modeling framework with powerful tools from control engineering,\nsuch as Lyapunov-like functions, to properly engineer economic systems with\nprovable properties. Apart from the aforementioned contributions, the\nmathematical framework herein proposed lays a foundation for engineering more\ngeneral economic systems built on emerging Turing complete networks, such as\nthe Ethereum network, through which complex alternative economic models are\nexplored.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 02:40:26 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Zargham", "Michael", ""], ["Zhang", "Zixuan", ""], ["Preciado", "Victor", ""]]}, {"id": "1807.00976", "submitter": "Ranesh Kumar Naha", "authors": "Ranesh Kumar Naha, Saurabh Garg, Dimitrios Georgakopoulos, Prem\n  Prakash Jayaraman, Longxiang Gao, Yong Xiang and Rajiv Ranjan", "title": "Fog Computing: Survey of Trends, Architectures, Requirements, and\n  Research Directions", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2018.2866491", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging technologies like the Internet of Things (IoT) require latency-aware\ncomputation for real-time application processing. In IoT environments,\nconnected things generate a huge amount of data, which are generally referred\nto as big data. Data generated from IoT devices are generally processed in a\ncloud infrastructure because of the on-demand services and scalability features\nof the cloud computing paradigm. However, processing IoT application requests\non the cloud exclusively is not an efficient solution for some IoT\napplications, especially time-sensitive ones. To address this issue, Fog\ncomputing, which resides in between cloud and IoT devices, was proposed. In\ngeneral, in the Fog computing environment, IoT devices are connected to Fog\ndevices. These Fog devices are located in close proximity to users and are\nresponsible for intermediate computation and storage. Fog computing research is\nstill in its infancy, and taxonomy-based investigation into the requirements of\nFog infrastructure, platform, and applications mapped to current research is\nstill required. This paper starts with an overview of Fog computing in which\nthe definition of Fog computing, research trends, and the technical differences\nbetween Fog and cloud are reviewed. Then, we investigate numerous proposed Fog\ncomputing architecture and describe the components of these architectures in\ndetail. From this, the role of each component will be defined, which will help\nin the deployment of Fog computing. Next, a taxonomy of Fog computing is\nproposed by considering the requirements of the Fog computing paradigm. We also\ndiscuss existing research works and gaps in resource allocation and scheduling,\nfault tolerance, simulation tools, and Fog-based microservices. Finally, by\naddressing the limitations of current research works, we present some open\nissues, which will determine the future research direction.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 04:44:56 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Naha", "Ranesh Kumar", ""], ["Garg", "Saurabh", ""], ["Georgakopoulos", "Dimitrios", ""], ["Jayaraman", "Prem Prakash", ""], ["Gao", "Longxiang", ""], ["Xiang", "Yong", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1807.01014", "submitter": "Jiajian Xiao", "authors": "Jiajian Xiao, Philipp Andelfinger, David Eckhoff, Wentong Cai, Alois\n  Knoll", "title": "A Survey on Agent-based Simulation using Hardware Accelerators", "comments": "Submitted for review to ACM Computing Surveys on 24/05/2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to decelerating gains in single-core CPU performance, computationally\nexpensive simulations are increasingly executed on highly parallel hardware\nplatforms. Agent-based simulations, where simulated entities act with a certain\ndegree of autonomy, frequently provide ample opportunities for parallelisation.\nThus, a vast variety of approaches proposed in the literature demonstrated\nconsiderable performance gains using hardware platforms such as many-core CPUs\nand GPUs, merged CPU-GPU chips as well as FPGAs. Typically, a combination of\ntechniques is required to achieve high performance for a given simulation\nmodel, putting substantial burden on modellers. To the best of our knowledge,\nno systematic overview of techniques for agent-based simulations on hardware\naccelerators has been given in the literature. To close this gap, we provide an\noverview and categorisation of the literature according to the applied\ntechniques. Since at the current state of research, challenges such as the\npartitioning of a model for execution on heterogeneous hardware are still a\nlargely manual process, we sketch directions for future research towards\nautomating the hardware mapping and execution. This survey targets modellers\nseeking an overview of suitable hardware platforms and execution techniques for\na specific simulation model, as well as methodology researchers interested in\npotential research gaps requiring further exploration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:21:07 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Xiao", "Jiajian", ""], ["Andelfinger", "Philipp", ""], ["Eckhoff", "David", ""], ["Cai", "Wentong", ""], ["Knoll", "Alois", ""]]}, {"id": "1807.01070", "submitter": "Christian Konrad", "authors": "Artur Czumaj, Christian Konrad", "title": "Detecting cliques in CONGEST networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting network structures plays a central role in\ndistributed computing. One of the fundamental problems studied in this area is\nto determine whether for a given graph $H$, the input network contains a\nsubgraph isomorphic to $H$ or not. We investigate this problem for $H$ being a\nclique $K_{l}$ in the classical distributed CONGEST model, where the\ncommunication topology is the same as the topology of the underlying network,\nand with limited communication bandwidth on the links.\n  Our first and main result is a lower bound, showing that detecting $K_{l}$\nrequires $\\Omega(\\sqrt{n} / b)$ communication rounds, for every $4 \\le l \\le\n\\sqrt{n}$, and $\\Omega(n / (l b))$ rounds for every $l \\ge \\sqrt{n}$, where $b$\nis the bandwidth of the communication links. This result is obtained by using a\nreduction to the set disjointness problem in the framework of two-party\ncommunication complexity.\n  We complement our lower bound with a two-party communication protocol for\nlisting all cliques in the input graph, which up to constant factors\ncommunicates the same number of bits as our lower bound for $K_4$ detection.\nThis demonstrates that our lower bound cannot be improved using the two-party\ncommunication framework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 10:28:36 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Czumaj", "Artur", ""], ["Konrad", "Christian", ""]]}, {"id": "1807.01147", "submitter": "Vaneet Aggarwal", "authors": "Abubakr Alabbasi and Vaneet Aggarwal and Tian Lan and Yu Xiang and\n  Moo-Ryong Ra and Yih-Farn R. Chen", "title": "FastTrack: Minimizing Stalls for CDN-based Over-the-top Video Streaming\n  Systems", "comments": "18 pages. arXiv admin note: text overlap with arXiv:1806.09466,\n  arXiv:1703.08348", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic for internet video streaming has been rapidly increasing and is\nfurther expected to increase with the higher definition videos and IoT\napplications, such as 360 degree videos and augmented virtual reality\napplications. While efficient management of heterogeneous cloud resources to\noptimize the quality of experience is important, existing work in this problem\nspace often left out important factors. In this paper, we present a model for\ndescribing a today's representative system architecture for video streaming\napplications, typically composed of a centralized origin server and several CDN\nsites. Our model comprehensively considers the following factors: limited\ncaching spaces at the CDN sites, allocation of CDN for a video request, choice\nof different ports from the CDN, and the central storage and bandwidth\nallocation. With the model, we focus on minimizing a performance metric, stall\nduration tail probability (SDTP), and present a novel, yet efficient, algorithm\nto solve the formulated optimization problem. The theoretical bounds with\nrespect to the SDTP metric are also analyzed and presented. Our extensive\nsimulation results demonstrate that the proposed algorithms can significantly\nimprove the SDTP metric, compared to the baseline strategies. Small-scale video\nstreaming system implementation in a real cloud environment further validates\nour results.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 10:24:28 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Alabbasi", "Abubakr", ""], ["Aggarwal", "Vaneet", ""], ["Lan", "Tian", ""], ["Xiang", "Yu", ""], ["Ra", "Moo-Ryong", ""], ["Chen", "Yih-Farn R.", ""]]}, {"id": "1807.01226", "submitter": "J\\'er\\'emie Decouchant", "authors": "David Kozhaya and J\\'er\\'emie Decouchant and Paulo Esteves-Verissimo", "title": "RT-ByzCast: Byzantine-Resilient Real-Time Reliable Broadcast", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's cyber-physical systems face various impediments to achieving their\nintended goals, namely, communication uncertainties and faults, relative to the\nincreased integration of networked and wireless devices, hinder the synchronism\nneeded to meet real-time deadlines. Moreover, being critical, these systems are\nalso exposed to significant security threats. This threat combination increases\nthe risk of physical damage. This paper addresses these problems by studying\nhow to build the first real-time Byzantine reliable broadcast protocol (RTBRB)\ntolerating network uncertainties, faults, and attacks. Previous literature\ndescribes either real-time reliable broadcast protocols, or asynchronous (non\nreal-time) Byzantine~ones.\n  We first prove that it is impossible to implement RTBRB using traditional\ndistributed computing paradigms, e.g., where the error/failure detection\nmechanisms of processes are decoupled from the broadcast algorithm itself, even\nwith the help of the most powerful failure detectors. We circumvent this\nimpossibility by proposing RT-ByzCast, an algorithm based on aggregating\ndigital signatures in a sliding time-window and on empowering processes with\nself-crashing capabilities to mask and bound losses. We show that RT-ByzCast\n(i) operates in real-time by proving that messages broadcast by correct\nprocesses are delivered within a known bounded delay, and (ii) is reliable by\ndemonstrating that correct processes using our algorithm crash themselves with\na negligible probability, even with message loss rates as high as 60%.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 15:04:46 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Kozhaya", "David", ""], ["Decouchant", "J\u00e9r\u00e9mie", ""], ["Esteves-Verissimo", "Paulo", ""]]}, {"id": "1807.01295", "submitter": "Ralf-Peter Mundani", "authors": "John N. Jomo (1), Nils Zander (1), Mohamed Elhaddad (1), Ali \\\"Ozcan\n  (1), Stefan Kollmannsberger (1), Ralf-Peter Mundani (1), Ernst Rank (1 and 2)\n  ((1) Technische Universit\\\"at M\\\"unchen, Munich, Germany, (2) Institute for\n  Advanced Study, Technische Universit\\\"at M\\\"unchen, Garching, Germany)", "title": "Parallelization of the multi-level hp-adaptive finite cell method", "comments": "24 pages, 16 figures", "journal-ref": "Computers & Mathematics with Applications 74 (2017) 126-142", "doi": "10.1016/j.camwa.2017.01.004", "report-no": null, "categories": "cs.DC math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-level hp-refinement scheme is a powerful extension of the finite\nelement method that allows local mesh adaptation without the trouble of\nconstraining hanging nodes. This is achieved through hierarchical high-order\noverlay meshes, a hp-scheme based on spatial refinement by superposition. An\nefficient parallelization of this method using standard domain decomposition\napproaches in combination with ghost elements faces the challenge of a large\nbasis function support resulting from the overlay structure and is in many\ncases not feasible. In this contribution, a parallelization strategy for the\nmulti-level hp-scheme is presented that is adapted to the scheme's simple\nhierarchical structure. By distributing the computational domain among\nprocesses on the granularity of the active leaf elements and utilizing shared\nmesh data structures, good parallel performance is achieved, as redundant\ncomputations on ghost elements are avoided. We show the scheme's parallel\nscalability for problems with a few hundred elements per process. Furthermore,\nthe scheme is used in conjunction with the finite cell method to perform\nnumerical simulations on domains of complex shape.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 17:27:50 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Jomo", "John N.", "", "1 and 2"], ["Zander", "Nils", "", "1 and 2"], ["Elhaddad", "Mohamed", "", "1 and 2"], ["\u00d6zcan", "Ali", "", "1 and 2"], ["Kollmannsberger", "Stefan", "", "1 and 2"], ["Mundani", "Ralf-Peter", "", "1 and 2"], ["Rank", "Ernst", "", "1 and 2"]]}, {"id": "1807.01340", "submitter": "Peng Wei", "authors": "Jason Cong, Zhenman Fang, Yuchen Hao, Peng Wei, Cody Hao Yu, Chen\n  Zhang, Peipei Zhou", "title": "Best-Effort FPGA Programming: A Few Steps Can Go a Long Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA-based heterogeneous architectures provide programmers with the ability\nto customize their hardware accelerators for flexible acceleration of many\nworkloads. Nonetheless, such advantages come at the cost of sacrificing\nprogrammability. FPGA vendors and researchers attempt to improve the\nprogrammability through high-level synthesis (HLS) technologies that can\ndirectly generate hardware circuits from high-level language descriptions.\nHowever, reading through recent publications on FPGA designs using HLS, one\noften gets the impression that FPGA programming is still hard in that it leaves\nprogrammers to explore a very large design space with many possible\ncombinations of HLS optimization strategies.\n  In this paper we make two important observations and contributions. First, we\ndemonstrate a rather surprising result: FPGA programming can be made easy by\nfollowing a simple best-effort guideline of five refinement steps using HLS. We\nshow that for a broad class of accelerator benchmarks from MachSuite, the\nproposed best-effort guideline improves the FPGA accelerator performance by\n42-29,030x. Compared to the baseline CPU performance, the FPGA accelerator\nperformance is improved from an average 292.5x slowdown to an average 34.4x\nspeedup. Moreover, we show that the refinement steps in the best-effort\nguideline, consisting of explicit data caching, customized pipelining,\nprocessing element duplication, computation/communication overlapping and\nscratchpad reorganization, correspond well to the best practice guidelines for\nmulticore CPU programming. Although our best-effort guideline may not always\nlead to the optimal solution, it substantially simplifies the FPGA programming\neffort, and will greatly support the wide adoption of FPGA-based acceleration\nby the software programming community.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 18:34:47 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Cong", "Jason", ""], ["Fang", "Zhenman", ""], ["Hao", "Yuchen", ""], ["Wei", "Peng", ""], ["Yu", "Cody Hao", ""], ["Zhang", "Chen", ""], ["Zhou", "Peipei", ""]]}, {"id": "1807.01341", "submitter": "Josh Borrow", "authors": "Josh Borrow, Richard G. Bower, Peter W. Draper, Pedro Gonnet, Matthieu\n  Schaller", "title": "SWIFT: Maintaining weak-scalability with a dynamic range of $10^4$ in\n  time-step size to harness extreme adaptivity", "comments": null, "journal-ref": "Proceedings of the 13th SPHERIC International Workshop, Galway,\n  Ireland, June 26-28 2018, pp. 44-51", "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmological simulations require the use of a multiple time-stepping scheme.\nWithout such a scheme, cosmological simulations would be impossible due to\ntheir high level of dynamic range; over eleven orders of magnitude in density.\nSuch a large dynamic range leads to a range of over four orders of magnitude in\ntime-step, which presents a significant load-balancing challenge. In this work,\nthe extreme adaptivity that cosmological simulations present is tackled in\nthree main ways through the use of the code SWIFT. First, an adaptive mesh is\nused to ensure that only the relevant particles are interacted in a given\ntime-step. Second, task-based parallelism is used to ensure efficient\nload-balancing within a single node, using pthreads and SIMD vectorisation.\nFinally, a domain decomposition strategy is presented, using the graph domain\ndecomposition library METIS, that bisects the work that must be performed by\nthe simulation between nodes using MPI. These three strategies are shown to\ngive SWIFT near-perfect weak-scaling characteristics, only losing 25%\nperformance when scaling from 1 to 4096 cores on a representative problem,\nwhilst being more than 30x faster than the de-facto standard Gadget-2 code.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 18:37:58 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Borrow", "Josh", ""], ["Bower", "Richard G.", ""], ["Draper", "Peter W.", ""], ["Gonnet", "Pedro", ""], ["Schaller", "Matthieu", ""]]}, {"id": "1807.01409", "submitter": "Chidchanok Choksuchat", "authors": "Chantana Chantrapornchai and Chidchanok Choksuchat", "title": "TripleID-Q: RDF Query Processing Framework using GPU", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TPDS.2018.2814567", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resource Description Framework (RDF) data represents information linkage\naround the Internet. It uses Inter- nationalized Resources Identifier (IRI)\nwhich can be referred to external information. Typically, an RDF data is\nserialized as a large text file which contains millions of relationships. In\nthis work, we propose a framework based on TripleID-Q, for query processing of\nlarge RDF data in a GPU. The key elements of the framework are 1) a compact\nrepresentation suitable for a Graphics Processing Unit (GPU) and 2) its simple\nrepresentation conversion method which optimizes the preprocessing overhead.\nTogether with the framework, we propose parallel algorithms which utilize\nthousands of GPU threads to look for specific data for a given query as well as\nto perform basic query operations such as union, join, and filter. The TripleID\nrepresentation is smaller than the original representation 3-4 times. Querying\nfrom TripleID using a GPU is up to 108 times faster than using the traditional\nRDF tool. The speedup can be more than 1,000 times over the traditional RDF\nstore when processing a complex query with union and join of many subqueries.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 00:30:02 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Chantrapornchai", "Chantana", ""], ["Choksuchat", "Chidchanok", ""]]}, {"id": "1807.01439", "submitter": "Sarathkumar Rangarajan", "authors": "Sarathkumar Rangarajan", "title": "Qos-Based Web Service Discovery And Selection Using Machine Learning", "comments": "8 Pages, 3 Figures", "journal-ref": "Rangarajan, S. (2018) Qos-based web service discovery and\n  selection using machine learning. EAI Endorsed Transactions on Scalable\n  Information Systems 18. doi:10.4108/eai.29-5-2018.154809", "doi": "10.4108/eai.29-5-2018.154809", "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In service computing, the same target functions can be achieved by multiple\nWeb services from different providers. Due to the functional similarities, the\nclient needs to consider the non-functional criteria. However, Quality of\nService provided by the developer suffers from scarcity and lack of\nreliability. In addition, the reputation of the service providers is an\nimportant factor, especially those with little experience, to select a service.\nMost of the previous studies were focused on the user's feedbacks for\njustifying the selection. Unfortunately, not all the users provide the feedback\nunless they had extremely good or bad experience with the service. In this\nvision paper, we propose a novel architecture for the web service discovery and\nselection. The core component is a machine learning based methodology to\npredict the QoS properties using source code metrics. The credibility value and\nprevious usage count are used to determine the reputation of the service.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 03:39:10 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Rangarajan", "Sarathkumar", ""]]}, {"id": "1807.01566", "submitter": "Umberto Ferraro Petrillo", "authors": "Umberto Ferraro Petrillo, Mara Sorella, Giuseppe Cattaneo, Raffaele\n  Giancarlo, Simona Rombo", "title": "Analyzing Big Datasets of Genomic Sequences: Fast and Scalable\n  Collection of k-mer Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed approaches based on the map-reduce programming paradigm have\nstarted to be proposed in the bioinformatics domain, due to the large amount of\ndata produced by the next-generation sequencing techniques. However, the use of\nmap-reduce and related Big Data technologies and frameworks (e.g., Apache\nHadoop and Spark) does not necessarily produce satisfactory results, in terms\nof both efficiency and effectiveness. We discuss how the development of\ndistributed and Big Data management technologies has affected the analysis of\nlarge datasets of biological sequences. Moreover, we show how the choice of\ndifferent parameter configurations and the careful engineering of the software\nwith respect to the specific framework under consideration may be crucial in\norder to achieve good performance, especially on very large amounts of data. We\nchoose k-mers counting as a case study for our analysis, and Spark as the\nframework to implement FastKmer, a novel approach for the extraction of k-mer\nstatistics from large collection of biological sequences, with arbitrary values\nof k. One of the most relevant contributions of FastKmer is the introduction of\na module for balancing the statistics aggregation workload over the nodes of a\ncomputing cluster, in order to overcome data skew while allowing for a fully\nexploitation of the underly- ing distributed architecture. We also present the\nresults of a comparative experimental analysis showing that our approach is\ncurrently the fastest among the ones based on Big Data technologies, while\nexhibiting a very good scalability. We provide evidence that the usage of\ntechnologies such as Hadoop or Spark for the analysis of big datasets of\nbiological sequences is productive only if the architectural details and the\npeculiar aspects of the considered framework are carefully taken into account\nfor the algorithm design and implementation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 13:23:09 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Petrillo", "Umberto Ferraro", ""], ["Sorella", "Mara", ""], ["Cattaneo", "Giuseppe", ""], ["Giancarlo", "Raffaele", ""], ["Rombo", "Simona", ""]]}, {"id": "1807.01751", "submitter": "Fabian Gieseke", "authors": "Malte von Mehren and Fabian Gieseke and Jan Verbesselt and Sabina\n  Rosca and St\\'ephanie Horion and Achim Zeileis", "title": "Massively-Parallel Break Detection for Satellite Data", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of remote sensing is nowadays faced with huge amounts of data.\nWhile this offers a variety of exciting research opportunities, it also yields\nsignificant challenges regarding both computation time and space requirements.\nIn practice, the sheer data volumes render existing approaches too slow for\nprocessing and analyzing all the available data. This work aims at accelerating\nBFAST, one of the state-of-the-art methods for break detection given satellite\nimage time series. In particular, we propose a massively-parallel\nimplementation for BFAST that can effectively make use of modern parallel\ncompute devices such as GPUs. Our experimental evaluation shows that the\nproposed GPU implementation is up to four orders of magnitude faster than the\nexisting publicly available implementation and up to ten times faster than a\ncorresponding multi-threaded CPU execution. The dramatic decrease in running\ntime renders the analysis of significantly larger datasets possible in seconds\nor minutes instead of hours or days. We demonstrate the practical benefits of\nour implementations given both artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 19:19:11 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["von Mehren", "Malte", ""], ["Gieseke", "Fabian", ""], ["Verbesselt", "Jan", ""], ["Rosca", "Sabina", ""], ["Horion", "St\u00e9phanie", ""], ["Zeileis", "Achim", ""]]}, {"id": "1807.01842", "submitter": "Qi Zhang", "authors": "Qi Zhang, Ling Liu, Calton Pu, Qiwei Dou, Liren Wu, Wei Zhou", "title": "A Comparative Study of Containers and Virtual Machines in Big Data\n  Environment", "comments": "Accepted by 2018 IEEE International Conference On Cloud Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Container technique is gaining increasing attention in recent years and has\nbecome an alternative to traditional virtual machines. Some of the primary\nmotivations for the enterprise to adopt the container technology include its\nconvenience to encapsulate and deploy applications, lightweight operations, as\nwell as efficiency and flexibility in resources sharing. However, there still\nlacks an in-depth and systematic comparison study on how big data applications,\nsuch as Spark jobs, perform between a container environment and a virtual\nmachine environment. In this paper, by running various Spark applications with\ndifferent configurations, we evaluate the two environments from many\ninteresting aspects, such as how convenient the execution environment can be\nset up, what are makespans of different workloads running in each setup, how\nefficient the hardware resources, such as CPU and memory, are utilized, and how\nwell each environment can scale. The results show that compared with virtual\nmachines, containers provide a more easy-to-deploy and scalable environment for\nbig data workloads. The research work in this paper can help practitioners and\nresearchers to make more informed decisions on tuning their cloud environment\nand configuring the big data applications, so as to achieve better performance\nand higher resources utilization.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 04:27:35 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Zhang", "Qi", ""], ["Liu", "Ling", ""], ["Pu", "Calton", ""], ["Dou", "Qiwei", ""], ["Wu", "Liren", ""], ["Zhou", "Wei", ""]]}, {"id": "1807.01854", "submitter": "Tianwei Zhang", "authors": "Jakub Szefer and Tianwei Zhang and Ruby B. Lee", "title": "Practical and Scalable Security Verification of Secure Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new and practical framework for security verification of secure\narchitectures. Specifically, we break the verification task into external\nverification and internal verification. External verification considers the\nexternal protocols, i.e. interactions between users, compute servers, network\nentities, etc. Meanwhile, internal verification considers the interactions\nbetween hardware and software components within each server. This verification\nframework is general-purpose and can be applied to a stand-alone server, or a\nlarge-scale distributed system. We evaluate our verification method on the\nCloudMonatt and HyperWall architectures as examples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 05:57:17 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Szefer", "Jakub", ""], ["Zhang", "Tianwei", ""], ["Lee", "Ruby B.", ""]]}, {"id": "1807.02253", "submitter": "Avishek Ghosh", "authors": "Avishek Ghosh and Kannan Ramchandran", "title": "Faster Data-access in Large-scale Systems: Network-scale Latency\n  Analysis under General Service-time Distributions", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud storage systems with a large number of servers, files are typically\nnot stored in single servers. Instead, they are split, replicated (to ensure\nreliability in case of server malfunction) and stored in different servers. We\nanalyze the mean latency of such a split-and-replicate cloud storage system\nunder general sub-exponential service time. We present a novel scheduling\nscheme that utilizes the load-balancing policy of the \\textit{power of $d$\n$(\\geq 2)$} choices. An alternative to split-and-replicate is to use\nerasure-codes, and recently, it has been observed that they can reduce latency\nin data access (see \\cite{longbo_delay} for details). We argue that under high\nredundancy (integer redundancy factor strictly greater than or equal to 2)\nregime, the mean latency of a coded system is upper bounded by that of a\nsplit-and-replicate system (with same replication factor) and the gap between\nthese two is small. We validate this claim numerically under different service\ndistributions such as exponential, shift plus exponential and the heavy-tailed\nWeibull distribution and compare the mean latency to that of an\nunsplit-replicated system. We observe that the coded system outperforms the\nunsplit-replication system by at least $20\\%$. Furthermore, we consider the\nmean latency for an erasure coded system with low redundancy (fractional\nredundancy factor between 1 and 2), a scenario which is more pragmatic, given\nthe storage constraints (\\cite{rashmi_thesis}). However under this regime, we\nrestrict ourselves to the special case of exponential service time distribution\nand use the randomized load balancing policy namely \\textit{batch-sampling}. We\nobtain an upper bound on mean delay that depends on the order statistics of the\nqueue lengths, which, we further smooth out via a discrete to continuous\napproximation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 04:29:17 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Ghosh", "Avishek", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1807.02492", "submitter": "Keke Zhai", "authors": "Keke Zhai, Tania Banerjee, David Zwick, Jason Hackl and Sanjay Ranka", "title": "Dynamic Load Balancing for Compressible Multiphase Turbulence", "comments": "This paper has been accepted by ACM International Conference on\n  Supercomputing (ICS) 2018", "journal-ref": null, "doi": "10.1145/3205289.3205304", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CMT-nek is a new scientific application for performing high fidelity\npredictive simulations of particle laden explosively dispersed turbulent flows.\nCMT-nek involves detailed simulations, is compute intensive and is targeted to\nbe deployed on exascale platforms. The moving particles are the main source of\nload imbalance as the application is executed on parallel processors. In a\ndemonstration problem, all the particles are initially in a closed container\nuntil a detonation occurs and the particles move apart. If all processors get\nan equal share of the fluid domain, then only some of the processors get\nsections of the domain that are initially laden with particles, leading to\ndisparate load on the processors. In order to eliminate load imbalance in\ndifferent processors and to speedup the makespan, we present different load\nbalancing algorithms for CMT-nek on large scale multi-core platforms consisting\nof hundred of thousands of cores. The detailed process of the load balancing\nalgorithms are presented. The performance of the different load balancing\nalgorithms are compared and the associated overheads are analyzed. Evaluations\non the application with and without load balancing are conducted and these show\nthat with load balancing, simulation time becomes faster by a factor of up to\n$9.97$.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 17:05:44 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Zhai", "Keke", ""], ["Banerjee", "Tania", ""], ["Zwick", "David", ""], ["Hackl", "Jason", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1807.02515", "submitter": "Gihan Janith Mendis Imbulgoda Liyangahawatte", "authors": "Gihan J. Mendis, Yifu Wu, Jin Wei, Moein Sabounchi, and Rigoberto\n  Roche'", "title": "Blockchain as a Service: A Decentralized and Secure Computing Paradigm", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": "10.1109/TETC.2020.2983007", "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the advances in machine learning, data-driven analysis tools have\nbecome valuable solutions for various applications. However, there still remain\nessential challenges to develop effective data-driven methods because of the\nneed to acquire a large amount of data and to have sufficient computing power\nto handle the data. In many instances these challenges are addressed by relying\non a dominant cloud computing vendor, but, although commercial cloud vendors\nprovide valuable platforms for data analytics, they can suffer from a lack of\ntransparency, security, and privacy-perservation. Furthermore, reliance on\ncloud servers prevents applying big data analytics in environments where the\ncomputing power is scattered. To address these challenges, a decentralize,\nsecure, and privacy-preserving computing paradigm is proposed to enable an\nasynchronized cooperative computing process amongst scattered and untrustworthy\ncomputing nodes that may have limited computing power and computing\nintelligence. This paradigm is designed by exploring blockchain, decentralized\nlearning, homomorphic encryption, and software defined networking(SDN)\ntechniques. The performance of the proposed paradigm is evaluated via different\nscenarios in the simulation section.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 20:03:38 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 02:53:24 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 15:37:43 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Mendis", "Gihan J.", ""], ["Wu", "Yifu", ""], ["Wei", "Jin", ""], ["Sabounchi", "Moein", ""], ["Roche'", "Rigoberto", ""]]}, {"id": "1807.02562", "submitter": "Stefano Markidis Prof.", "authors": "Steven Wei-der Chien and Stefano Markidis and Rami Karim and Erwin\n  Laure and Sai Narasimhamurthy", "title": "Exploring Scientific Application Performance Using Large Scale Object\n  Storage", "comments": "Preprint submitted to WOPSSS workshop at ISC 2018", "journal-ref": null, "doi": "10.1007/978-3-030-02465-9_8", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major performance and scalability bottlenecks in large scientific\napplications is parallel reading and writing to supercomputer I/O systems. The\nusage of parallel file systems and consistency requirements of POSIX, that all\nthe traditional HPC parallel I/O interfaces adhere to, pose limitations to the\nscalability of scientific applications. Object storage is a widely used storage\ntechnology in cloud computing and is more frequently proposed for HPC workload\nto address and improve the current scalability and performance of I/O in\nscientific applications. While object storage is a promising technology, it is\nstill unclear how scientific applications will use object storage and what the\nmain performance benefits will be. This work addresses these questions, by\nemulating an object storage used by a traditional scientific application and\nevaluating potential performance benefits. We show that scientific applications\ncan benefit from the usage of object storage on large scales.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 20:49:18 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Chien", "Steven Wei-der", ""], ["Markidis", "Stefano", ""], ["Karim", "Rami", ""], ["Laure", "Erwin", ""], ["Narasimhamurthy", "Sai", ""]]}, {"id": "1807.02846", "submitter": "Paolo Missier", "authors": "Paolo Missier and Shaimaa Bajoudah and Angelo Capossele and Andrea\n  Gaglione and Michele Nati", "title": "Mind My Value: a decentralized infrastructure for fair and trusted IoT\n  data trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet of Things (IoT) data are increasingly viewed as a new form of\nmassively distributed and large scale digital assets, which are continuously\ngenerated by millions of connected devices. The real value of such assets can\nonly be realized by allowing IoT data trading to occur on a marketplace that\nrewards every single producer and consumer, at a very granular level.\nCrucially, we believe that such a marketplace should not be owned by anybody,\nand should instead fairly and transparently self-enforce a well defined set of\ngovernance rules. In this paper we address some of the technical challenges\ninvolved in realizing such a marketplace. We leverage emerging blockchain\ntechnologies to build a decentralized, trusted, transparent and open\narchitecture for IoT traffic metering and contract compliance, on top of the\nlargely adopted IoT brokered data infrastructure. We discuss an Ethereum-based\nprototype implementation and experimentally evaluate the overhead cost\nassociated with Smart Contract transactions, concluding that a viable business\nmodel can indeed be associated with our technical approach.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 16:20:50 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Missier", "Paolo", ""], ["Bajoudah", "Shaimaa", ""], ["Capossele", "Angelo", ""], ["Gaglione", "Andrea", ""], ["Nati", "Michele", ""]]}, {"id": "1807.03103", "submitter": "Arezoo Khatibi", "authors": "Arezoo Khatibi, Omid Khatibi", "title": "Criteria for the CloudSim Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  CPU is undoubtedly the most important resource of the computer system. Recent\nadvances in software and system architecture have increased processing\ncomplexity, as computing is now distributed and parallel. CloudSim represents\nthe complexity of an application in terms of its computational requirements.\nCloudSim [9] is a complete solution for simulating Cloud Computing environments\nand building test beds for provisioning algorithms. This paper analyzes and\nevaluates the performance of cloud environment modeling using CloudSim. We\ndescribe the CloudSim architecture and then investigate the new models and\ntechniques in CloudSim.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 18:12:55 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 16:48:31 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Khatibi", "Arezoo", ""], ["Khatibi", "Omid", ""]]}, {"id": "1807.03104", "submitter": "Xiaoran Xu", "authors": "Keith Cooper, Xiaoran Xu", "title": "Efficient Characterization of Hidden Processor Memory Hierarchies", "comments": "14 pages, International Conference on Computational Science 2018", "journal-ref": null, "doi": "10.1007/978-3-319-93713-7_27", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A processor's memory hierarchy has a major impact on the performance of\nrunning code. However, computing platforms, where the actual hardware\ncharacteristics are hidden from both the end user and the tools that mediate\nexecution, such as a compiler, a JIT and a runtime system, are used more and\nmore, for example, performing large scale computation in cloud and cluster.\nEven worse, in such environments, a single computation may use a collection of\nprocessors with dissimilar characteristics. Ignorance of the\nperformance-critical parameters of the underlying system makes it difficult to\nimprove performance by optimizing the code or adjusting runtime-system\nbehaviors; it also makes application performance harder to understand.\n  To address this problem, we have developed a suite of portable tools that can\nefficiently derive many of the parameters of processor memory hierarchies, such\nas levels, effective capacity and latency of caches and TLBs, in a matter of\nseconds. The tools use a series of carefully considered experiments to produce\nand analyze cache response curves automatically. The tools are inexpensive\nenough to be used in a variety of contexts that may include install time,\ncompile time or runtime adaption, or performance understanding tools.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 07:26:16 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Cooper", "Keith", ""], ["Xu", "Xiaoran", ""]]}, {"id": "1807.03110", "submitter": "Gowri Sankar Ramachandran Dr", "authors": "Gowri Sankar Ramachandran, Kwame-Lante Wright, and Bhaskar\n  Krishnamachari", "title": "Trinity: A Distributed Publish/Subscribe Broker with Blockchain-based\n  Immutability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-of-Things (IoT) and Supply Chain monitoring applications rely on\nmessaging protocols for exchanging data. Contemporary IoT deployments widely\nuse the publish-subscribe messaging model because of its resource-efficiency.\nHowever, the systems with publish-subscribe messaging model employ a\ncentralized architecture, wherein the data from all the devices in the\napplication network flows via a central broker to the subscribers. Such a\ncentralized architecture make publish-subscribe messaging model susceptible to\na central point of failure. Besides, it provides an opportunity for the\norganization that owns the broker to tamper with the data. In this work, we\ncontribute Trinity, a novel distributed publish-subscribe broker with\nblockchain-based immutability. Trinity distributes the data published to one of\nthe brokers in the network to all the brokers in the network. The distributed\ndata is stored in an immutable ledger through the use of the blockchain\ntechnology. Furthermore, Trinity executes smart contracts to validate the data\nbefore saving the data on the blockchain. Through the use of a blockchain\nnetwork, Trinity can guarantee persistence, ordering, and immutability across\ntrust boundaries. Our evaluation results show that Trinity consumes minimal\nresources, and the use of smart contracts enable the stakeholders to automate\nthe data management processes. To the best of our knowledge, Trinity is the\nfirst framework that combines the components of the blockchain technology with\nthe publish-subscribe messaging model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:31:43 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Ramachandran", "Gowri Sankar", ""], ["Wright", "Kwame-Lante", ""], ["Krishnamachari", "Bhaskar", ""]]}, {"id": "1807.03112", "submitter": "Florina Ciorba", "authors": "Florina M. Ciorba", "title": "The importance and need for system monitoring and analysis in HPC\n  operations and research", "comments": "10 pages, 3 figures, 52 footnotes (URLs instead of references)", "journal-ref": "Proceedings of the 3rd bwHPC-Symposium: Heidelberg 2016", "doi": "10.11588/heibooks.308.418", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, system monitoring and analysis are discussed in terms of their\nsignificance and benefits for operations and research in the field of\nhigh-performance computing (HPC). HPC systems deliver unique insights to\ncomputational scientists from different disciplines. It is argued that research\nin HPC is also computational in nature, given the massive amounts of monitoring\ndata collected at various levels of an HPC system. The vision of a\ncomprehensive system model developed based on holistic monitoring and analysis\nis also presented. The goal and expected outcome of such a model is an improved\nunderstanding of the intricate interactions between today's software and\nhardware, and their diverse usage patterns. The associated modeling,\nmonitoring, and analysis challenges are reviewed and discussed. The envisioned\ncomprehensive system model will provide the ability to design future systems\nthat are better understood before use, easier to maintain and monitor, more\nefficient, more reliable, and, therefore, more productive. The paper is\nconcluded with a number of recommendations towards realizing the envisioned\nsystem model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 09:56:13 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Ciorba", "Florina M.", ""]]}, {"id": "1807.03144", "submitter": "Lisbeth Fajstrup", "authors": "Lisbeth Fajstrup", "title": "Cut-off Theorems for the PV-model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove cut-off results for deadlocks and serializability of a $PV$-thread\n$T$ run in parallel with itself: For a $PV$ thread $T$ which accesses a set\n$\\mathcal{R}$ of resources, each with a maximal capacity\n$\\kappa:\\mathcal{R}\\to\\mathbb{N}$, the PV-program $T^n$, where $n$ copies of\n$T$ are run in parallel, is deadlock free for all $n$ if and only if $T^M$ is\ndeadlock free where $M=\\Sigma_{r\\in\\mathcal{R}}\\kappa(r)$. This is a sharp\nbound: For all $\\kappa:\\mathcal{R}\\to\\mathbb{N}$ and finite $\\mathcal{R}$ there\nis a thread $T$ using these resources such that $T^M$ has a deadlock, but $T^n$\ndoes not for $n<M$. Moreover, we prove a more general theorem: There are no\ndeadlocks in $p=T1|T2|\\cdots |Tn$ if and only if there are no deadlocks in\n$T_{i_1}|T_{i_2}|\\cdots |T_{i_M}$ for any subset $\\{i_1,\\ldots,i_M\\}\\subset\n[1:n]$. For $\\kappa(r)\\equiv 1$, $T^n$ is serializable for all $n$ if and only\nif $T^2$ is serializable. For general capacities, we define a local obstruction\nto serializability. There is no local obstruction to serializability in $T^n$\nfor all $n$ if and only if there is no local obstruction to serializability in\n$T^M$ for $M=\\Sigma_{r\\in\\mathcal{R}}\\kappa(r)+1$. The obstructions may be\nfound using a deadlock algorithm in $T^{M+1}$. These serializability results\nalso have a generalization: If there are no local obstructions to\nserializability in any of the $M$-dimensional sub programs,\n$T_{i_1}|T_{i_2}|\\cdots |T_{i_M}$, then $p$ is serializable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:48:46 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Fajstrup", "Lisbeth", ""]]}, {"id": "1807.03210", "submitter": "Michael Kamp", "authors": "Michael Kamp and Linara Adilova and Joachim Sicking and Fabian H\\\"uger\n  and Peter Schlicht and Tim Wirtz and Stefan Wrobel", "title": "Efficient Decentralized Deep Learning by Dynamic Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient protocol for decentralized training of deep neural\nnetworks from distributed data sources. The proposed protocol allows to handle\ndifferent phases of model training equally well and to quickly adapt to concept\ndrifts. This leads to a reduction of communication by an order of magnitude\ncompared to periodically communicating state-of-the-art approaches. Moreover,\nwe derive a communication bound that scales well with the hardness of the\nserialized learning problem. The reduction in communication comes at almost no\ncost, as the predictive performance remains virtually unchanged. Indeed, the\nproposed protocol retains loss bounds of periodically averaging schemes. An\nextensive empirical evaluation validates major improvement of the trade-off\nbetween model performance and communication which could be beneficial for\nnumerous decentralized learning applications, such as autonomous driving, or\nvoice recognition and image classification on mobile phones.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 15:01:51 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 18:45:10 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kamp", "Michael", ""], ["Adilova", "Linara", ""], ["Sicking", "Joachim", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Wirtz", "Tim", ""], ["Wrobel", "Stefan", ""]]}, {"id": "1807.03280", "submitter": "Chao Wang", "authors": "Shengjian Guo and Meng Wu and Chao Wang", "title": "Adversarial Symbolic Execution for Detecting Concurrency-Related Cache\n  Timing Leaks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timing characteristics of cache, a high-speed storage between the fast\nCPU and the slowmemory, may reveal sensitive information of a program, thus\nallowing an adversary to conduct side-channel attacks. Existing methods for\ndetecting timing leaks either ignore cache all together or focus only on\npassive leaks generated by the program itself, without considering leaks that\nare made possible by concurrently running some other threads. In this work, we\nshow that timing-leak-freedom is not a compositional property: a program that\nis not leaky when running alone may become leaky when interleaved with other\nthreads. Thus, we develop a new method, named adversarial symbolic execution,\nto detect such leaks. It systematically explores both the feasible program\npaths and their interleavings while modeling the cache, and leverages an SMT\nsolver to decide if there are timing leaks. We have implemented our method in\nLLVM and evaluated it on a set of real-world ciphers with 14,455 lines of C\ncode in total. Our experiments demonstrate both the efficiency of our method\nand its effectiveness in detecting side-channel leaks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 17:32:09 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Guo", "Shengjian", ""], ["Wu", "Meng", ""], ["Wang", "Chao", ""]]}, {"id": "1807.03383", "submitter": "Alvaro Tzul", "authors": "Alvaro Tzul", "title": "Multicore architecture and cache optimization techniques for solving\n  graph problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of era of Big Data and Internet of Things, there has been an\nexponential increase in the availability of large data sets. These data sets\nrequire in-depth analysis that provides intelligence for improvements in\nmethods for academia and industry. Majority of the data sets are represented\nand available in the form of graphs. Therefore, the problem at hand is to\naddress solving graph problems. Since the data sets are large, the time it\ntakes to analyze the data is significant. Hence, in this paper, we explore\ntechniques that can exploit existing multicore architecture to address the\nissue. Currently, most Central Processing Units have incorporated multicore\ndesign; in addition, co-processors such as Graphics Processing Units have large\nnumber of cores that can used to gain significant speedup. Therefore, in this\npaper techniques to exploit the advantages of multicore architecture is\nstudied.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 20:52:57 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Tzul", "Alvaro", ""]]}, {"id": "1807.03546", "submitter": "Oskar Schirmer", "authors": "Oskar Schirmer", "title": "Parallel Architecture Hardware and General Purpose Operating System\n  Co-design", "comments": "66 pages, 30 figures and tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because most optimisations to achieve higher computational performance\neventually are limited, parallelism that scales is required. Parallelised\nhardware alone is not sufficient, but software that matches the architecture is\nrequired to gain best performance. For decades now, hardware design has been\nguided by the basic design of existing software, to avoid the higher cost to\nredesign the latter. In doing so, however, quite a variety of superior concepts\nis excluded a priori. Consequently, co-design of both hardware and software is\ncrucial where highest performance is the goal. For special purpose application,\nthis co-design is common practice. For general purpose application, however, a\nprecondition for usability of a computer system is an operating system which is\nboth comprehensive and dynamic. As no such operating system has ever been\ndesigned, a sketch for a comprehensive dynamic operating system is presented,\nbased on a straightforward hardware architecture to demonstrate how design\ndecisions regarding software and hardware do coexist and harmonise.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 09:24:33 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Schirmer", "Oskar", ""]]}, {"id": "1807.03562", "submitter": "Kevin Beineke", "authors": "Kevin Beineke and Stefan Nothaas and Michael Schoettner", "title": "DXRAM's Fault-Tolerance Mechanisms Meet High Speed I/O Devices", "comments": "21 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-memory key-value stores provide consistent low-latency access to all\nobjects which is important for interactive large-scale applications like social\nmedia networks or online graph analytics and also opens up new application\nareas. But, when storing the data in RAM on thousands of servers one has to\nconsider server failures. Only a few in-memory key-value stores provide\nautomatic online recovery of failed servers. The most prominent example of\nthese systems is RAMCloud. Another system with sophisticated fault-tolerance\nmechanisms is DXRAM which is optimized for small data objects. In this report,\nwe detail the remote replication process which is based on logs, investigate\nselection strategies for the reorganization of these logs and evaluate the\nreorganization performance for sequential, random, zipf and hot-and-cold\ndistributions in DXRAM. This is also the first time DXRAM's backup system is\nevaluated with high speed I/O devices, specifically with 56 GBit/s InfiniBand\ninterconnect and PCI-e SSDs. Furthermore, we discuss the copyset replica\ndistribution to reduce the probability for data loss and the adaptations to the\noriginal approach for DXRAM.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 10:43:55 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 12:45:28 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Beineke", "Kevin", ""], ["Nothaas", "Stefan", ""], ["Schoettner", "Michael", ""]]}, {"id": "1807.03577", "submitter": "Ali Mohammed", "authors": "Ali Mohammed and Florina M. Ciorba", "title": "SiL: An Approach for Adjusting Applications to Heterogeneous Systems\n  Under Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications consist of large and computationally-intensive loops.\nDynamic loop scheduling (DLS) techniques are used to load balance the execution\nof such applications. Load imbalance can be caused by variations in loop\niteration execution times due to problem, algorithmic, or systemic\ncharacteristics (also, perturbations). The following question motivates this\nwork: \"Given an application, a high-performance computing (HPC) system, and\nboth their characteristics and interplay, which DLS technique will achieve\nimproved performance under unpredictable perturbations?\" Existing work only\nconsiders perturbations caused by variations in the HPC system delivered\ncomputational speeds. However, perturbations in available network bandwidth or\nlatency are inevitable on production HPC systems. Simulator in the loop (SiL)\nis introduced, herein, as a new control-theoretic inspired approach to\ndynamically select DLS techniques that improve the performance of applications\non heterogeneous HPC systems under perturbations. The present work examines the\nperformance of six applications on a heterogeneous system under all above\nsystem perturbations. The SiL proof of concept is evaluated using simulation.\nThe performance results confirm the initial hypothesis that no single DLS\ntechnique can deliver best performance in all scenarios, while the SiL-based\nDLS selection delivered improved application performance in most experiments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 11:47:51 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 08:57:28 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Mohammed", "Ali", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1807.03578", "submitter": "Rajkumar Buyya", "authors": "Rajkumar Buyya, Maria A. Rodriguez, Adel Nadjaran Toosi, and Jaeman\n  Park", "title": "Cost-Efficient Orchestration of Containers in Clouds: A Vision,\n  Architectural Elements, and Future Directions", "comments": "11 pages, 4 figures", "journal-ref": "Proceedings of the Mathematics, Informatics, Science, and\n  Education International Conference (MISEIC 2018), Surabaya, Indonesia, July\n  21, 2018", "doi": "10.1088/1742-6596/1108/1/012001", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an architectural framework for the efficient\norchestration of containers in cloud environments. It centres around resource\nscheduling and rescheduling policies as well as autoscaling algorithms that\nenable the creation of elastic virtual clusters. In this way, the proposed\nframework enables the sharing of a computing environment between differing\nclient applications packaged in containers, including web services, offline\nanalytics jobs, and backend pre-processing tasks. The devised resource\nmanagement algorithms and policies will improve utilization of the available\nvirtual resources to reduce operational cost for the provider while satisfying\nthe resource needs of various types of applications. The proposed algorithms\nwill take factors that are previously omitted by other solutions into\nconsideration, including 1) the pricing models of the acquired resources, 2)\nand the fault-tolerability of the applications, and 3) the QoS requirements of\nthe running applications, such as the latencies and throughputs of the web\nservices and the deadline of the analytical and pre-processing jobs. The\nproposed solutions will be evaluated by developing a prototype platform based\non one of the existing container orchestration platforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 11:49:08 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Rodriguez", "Maria A.", ""], ["Toosi", "Adel Nadjaran", ""], ["Park", "Jaeman", ""]]}, {"id": "1807.03632", "submitter": "Stefano Markidis Prof.", "authors": "Sai Narasimhamurthy and Nikita Danilov and Sining Wu and Ganesan\n  Umanesan and Steven Wei-der Chien and Sergio Rivas-Gomez and Ivy Bo Peng and\n  Erwin Laure and Shaun de Witt and Dirk Pleiter and Stefano Markidis", "title": "The SAGE Project: a Storage Centric Approach for Exascale Computing", "comments": "Submitted to Computing Frontiers 2018. arXiv admin note: substantial\n  text overlap with arXiv:1805.00556", "journal-ref": null, "doi": "10.1145/3203217.3205341", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAGE (Percipient StorAGe for Exascale Data Centric Computing) is a European\nCommission funded project towards the era of Exascale computing. Its goal is to\ndesign and implement a Big Data/Extreme Computing (BDEC) capable infrastructure\nwith associated software stack. The SAGE system follows a \"storage centric\"\napproach as it is capable of storing and processing large data volumes at the\nExascale regime.\n  SAGE addresses the convergence of Big Data Analysis and HPC in an era of\nnext-generation data centric computing. This convergence is driven by the\nproliferation of massive data sources, such as large, dispersed scientific\ninstruments and sensors where data needs to be processed, analyzed and\nintegrated into simulations to derive scientific and innovative insights. A\nfirst prototype of the SAGE system has been been implemented and installed at\nthe Julich Supercomputing Center. The SAGE storage system consists of multiple\ntypes of storage device technologies in a multi-tier I/O hierarchy, including\nflash, disk, and non-volatile memory technologies. The main SAGE software\ncomponent is the Seagate Mero Object Storage that is accessible via the Clovis\nAPI and higher level interfaces. The SAGE project also includes scientific\napplications for the validation of the SAGE concepts.\n  The objective of this paper is to present the SAGE project concepts, the\nprototype of the SAGE platform and discuss the software architecture of the\nSAGE system.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 21:41:34 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Narasimhamurthy", "Sai", ""], ["Danilov", "Nikita", ""], ["Wu", "Sining", ""], ["Umanesan", "Ganesan", ""], ["Chien", "Steven Wei-der", ""], ["Rivas-Gomez", "Sergio", ""], ["Peng", "Ivy Bo", ""], ["Laure", "Erwin", ""], ["de Witt", "Shaun", ""], ["Pleiter", "Dirk", ""], ["Markidis", "Stefano", ""]]}, {"id": "1807.03662", "submitter": "Hobart Young", "authors": "Hao Dai, H Patrick Young, Thomas JS Durant, Guannan Gong, Mingming\n  Kang, Harlan M Krumholz, Wade L Schulz, Lixin Jiang", "title": "TrialChain: A Blockchain-Based Platform to Validate Data Integrity in\n  Large, Biomedical Research Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The governance of data used for biomedical research and clinical trials is an\nimportant requirement for generating accurate results. To improve the\nvisibility of data quality and analysis, we developed TrialChain, a\nblockchain-based platform that can be used to validate data integrity from\nlarge, biomedical research studies. We implemented a private blockchain using\nthe MultiChain platform and integrated it with a data science platform deployed\nwithin a large research center. An administrative web application was built\nwith Python to manage the platform, which was built with a microservice\narchitecture using Docker. The TrialChain platform was integrated during data\nacquisition into our existing data science platform. Using NiFi, data were\nhashed and logged within the local blockchain infrastructure. To provide public\nvalidation, the local blockchain state was periodically synchronized to the\npublic Ethereum network. The use of a combined private/public blockchain\nplatform allows for both public validation of results while maintaining\nadditional security and lower cost for blockchain transactions. Original data\nand modifications due to downstream analysis can be logged within TrialChain\nand data assets or results can be rapidly validated when needed using API calls\nto the platform. The TrialChain platform provides a data governance solution to\naudit the acquisition and analysis of biomedical research data. The platform\nprovides cryptographic assurance of data authenticity and can also be used to\ndocument data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 14:10:54 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Dai", "Hao", ""], ["Young", "H Patrick", ""], ["Durant", "Thomas JS", ""], ["Gong", "Guannan", ""], ["Kang", "Mingming", ""], ["Krumholz", "Harlan M", ""], ["Schulz", "Wade L", ""], ["Jiang", "Lixin", ""]]}, {"id": "1807.03755", "submitter": "Duarte Manuel Ribeiro Pinto M.Eng", "authors": "Duarte Pinto, Jo\\~ao Pedro Dias and Hugo Sereno Ferreira", "title": "Dynamic Allocation of Serverless Functions in IoT Environments", "comments": null, "journal-ref": null, "doi": "10.1109/EUC.2018.00008", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IoT area has grown significantly in the last few years and is expected to\nreach a gigantic amount of 50 billion devices by 2020. The appearance of\nserverless architectures, specifically highlighting FaaS, raises the question\nof the of using such in IoT environments. Combining IoT with a serverless\narchitectural design can be effective when trying to make use of the local\nprocessing power that exists in a local network of IoT devices and creating a\nfog layer that leverages computational capabilities that are closer to the\nend-user. In this approach, which is placed between the device and the\nserverless function, when a device requests for the execution of a serverless\nfunction will decide based on previous metrics of execution if the serverless\nfunction should be executed locally, in the fog layer of a local network of IoT\ndevices, or if it should be executed remotely, in one of the available cloud\nservers. Therefore, this approach allows to dynamically allocating functions to\nthe most suitable layer.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 16:59:01 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 13:00:46 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 22:34:36 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Pinto", "Duarte", ""], ["Dias", "Jo\u00e3o Pedro", ""], ["Ferreira", "Hugo Sereno", ""]]}, {"id": "1807.03847", "submitter": "Alexander Van Der Grinten", "authors": "Alexander van der Grinten and Elisabetta Bergamini and Oded Green and\n  David A. Bader and Henning Meyerhenke", "title": "Scalable Katz Ranking Computation in Large Static and Dynamic Graphs", "comments": "Published at ESA'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network analysis defines a number of centrality measures to identify the most\ncentral nodes in a network. Fast computation of those measures is a major\nchallenge in algorithmic network analysis. Aside from closeness and\nbetweenness, Katz centrality is one of the established centrality measures. In\nthis paper, we consider the problem of computing rankings for Katz centrality.\nIn particular, we propose upper and lower bounds on the Katz score of a given\nnode. While previous approaches relied on numerical approximation or heuristics\nto compute Katz centrality rankings, we construct an algorithm that iteratively\nimproves those upper and lower bounds until a correct Katz ranking is obtained.\nWe extend our algorithm to dynamic graphs while maintaining its correctness\nguarantees. Experiments demonstrate that our static graph algorithm outperforms\nboth numerical approaches and heuristics with speedups between 1.5x and 3.5x,\ndepending on the desired quality guarantees. Our dynamic graph algorithm\nimproves upon the static algorithm for update batches of less than 10000 edges.\nWe provide efficient parallel CPU and GPU implementations of our algorithms\nthat enable near real-time Katz centrality computation for graphs with hundreds\nof millions of nodes in fractions of seconds.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 20:18:40 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["van der Grinten", "Alexander", ""], ["Bergamini", "Elisabetta", ""], ["Green", "Oded", ""], ["Bader", "David A.", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1807.04151", "submitter": "Hui Sun", "authors": "Hui Sun, Wei Liu, Jianzhong Huang, Weisong Shi", "title": "Co-KV: A Collaborative Key-Value Store Using Near-Data Processing to\n  Improve Compaction for the LSM-tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-structured merge tree (LSM-tree) based key-value stores are widely\nemployed in large-scale storage systems. In the compaction of the key-value\nstore, SSTables are merged with overlapping key ranges and sorted for data\nqueries. This, however, incurs write amplification and thus degrades system\nperformance, especially under update-intensive workloads. Current optimization\nfocuses mostly on the reduction of the overload of compaction in the host, but\nrarely makes full use of computation in the device. To address these issues, we\npropose Co-KV, a Collaborative Key-Value store between the host and a near-data\nprocessing ( i.e., NDP) model based SSD to improve compaction. Co-KV offers\nthree benefits: (1) reducing write amplification by a compaction offloading\nscheme between host and device; (2) relieving the overload of compaction in the\nhost and leveraging computation in the SSD based on the NDP model; and (3)\nimproving the performance of LSM-tree based key-value stores under\nupdate-intensive workloads.\n  Extensive db_bench experiment show that Co-KV largely achieves a 2.0x overall\nthroughput improvement, and a write amplification reduction by up to 36.0% over\nthe state-of-the-art LevelDB. Under YCSB workloads, Co-KV increases the\nthroughput by 1.7x - 2.4x while decreases the write amplification and average\nlatency by up to 30.0% and 43.0%, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:20:15 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Sun", "Hui", ""], ["Liu", "Wei", ""], ["Huang", "Jianzhong", ""], ["Shi", "Weisong", ""]]}, {"id": "1807.04188", "submitter": "Thierry Moreau", "authors": "Thierry Moreau, Tianqi Chen, Luis Vega, Jared Roesch, Eddie Yan,\n  Lianmin Zheng, Josh Fromm, Ziheng Jiang, Luis Ceze, Carlos Guestrin, Arvind\n  Krishnamurthy", "title": "A Hardware-Software Blueprint for Flexible Deep Learning Specialization", "comments": "6 pages plus references, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized Deep Learning (DL) acceleration stacks, designed for a specific\nset of frameworks, model architectures, operators, and data types, offer the\nallure of high performance while sacrificing flexibility. Changes in\nalgorithms, models, operators, or numerical systems threaten the viability of\nspecialized hardware accelerators. We propose VTA, a programmable deep learning\narchitecture template designed to be extensible in the face of evolving\nworkloads. VTA achieves this flexibility via a parametrizable architecture,\ntwo-level ISA, and a JIT compiler. The two-level ISA is based on (1) a task-ISA\nthat explicitly orchestrates concurrent compute and memory tasks and (2) a\nmicrocode-ISA which implements a wide variety of operators with single-cycle\ntensor-tensor operations. Next, we propose a runtime system equipped with a JIT\ncompiler for flexible code-generation and heterogeneous execution that enables\neffective use of the VTA architecture. VTA is integrated and open-sourced into\nApache TVM, a state-of-the-art deep learning compilation stack that provides\nflexibility for diverse models and divergent hardware backends. We propose a\nflow that performs design space exploration to generate a customized hardware\narchitecture and software operator library that can be leveraged by mainstream\nlearning frameworks. We demonstrate our approach by deploying optimized deep\nlearning models used for object classification and style transfer on edge-class\nFPGAs.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:19:30 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 03:51:47 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 00:50:43 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Moreau", "Thierry", ""], ["Chen", "Tianqi", ""], ["Vega", "Luis", ""], ["Roesch", "Jared", ""], ["Yan", "Eddie", ""], ["Zheng", "Lianmin", ""], ["Fromm", "Josh", ""], ["Jiang", "Ziheng", ""], ["Ceze", "Luis", ""], ["Guestrin", "Carlos", ""], ["Krishnamurthy", "Arvind", ""]]}, {"id": "1807.04214", "submitter": "Seyyedali Hosseinalipour", "authors": "Seyyedali Hosseinalipour and Huaiyu Dai", "title": "A Two-Stage Auction Mechanism for Cloud Resource Allocation", "comments": "14 pages, 10 figures", "journal-ref": "IEEE Transactions on Cloud Computing, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent growth in the size of cloud computing business, handling the\ninteractions between customers and cloud providers has become more challenging.\nAuction theory has been proposed to model these interactions due to its\nsimplicity and a good match with real-world scenarios. In this paper, we\nconsider cloud of clouds networks (CCNs) with different types of servers along\nwith customers with heterogeneous demands. For each CCN, a CCN manager is\ndesignated to handle the cloud resources. A comprehensive framework is\nintroduced in which the process of resource gathering and allocation is\naddressed via two stages, where the first stage models the interactions between\ncustomers and CCN managers, and the second stage examines the interactions\nbetween CCN managers and private cloud providers (CPs). For the first stage, an\noptions-based sequential auction (OBSA) is adapted to the examined market,\nwhich is capable of providing truthfulness as the dominant strategy and\nresolving the entrance time problem. An analytical foundation for OBSAs is\npresented and multiple performance metrics are derived. For the second stage,\ntwo parallel markets are assumed: flat-price and auction-based market. A\ntheoretical framework for market analysis is provided and the bidding behavior\nof CCN managers is described.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:58:22 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 15:36:12 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Hosseinalipour", "Seyyedali", ""], ["Dai", "Huaiyu", ""]]}, {"id": "1807.04255", "submitter": "Adel Elmahdy", "authors": "Adel Elmahdy, Soheil Mohajer", "title": "On the Fundamental Limits of Coded Data Shuffling for Distributed\n  Machine Learning", "comments": "This work has been published in IEEE Transactions on Information\n  Theory. A preliminary version of this work was presented at IEEE\n  International Symposium on Information Theory (ISIT), Jun. 2018", "journal-ref": "IEEE Transactions on Information Theory, vol. 66, no. 5, pp.\n  3098-3131, May 2020", "doi": "10.1109/TIT.2020.2964547", "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the data shuffling problem in a distributed learning system, in\nwhich a master node is connected to a set of worker nodes, via a shared link,\nin order to communicate a set of files to the worker nodes. The master node has\naccess to a database of files. In every shuffling iteration, each worker node\nprocesses a new subset of files, and has excess storage to partially cache the\nremaining files, assuming the cached files are uncoded. The caches of the\nworker nodes are updated every iteration, and they should be designed to\nsatisfy any possible unknown permutation of the files in subsequent iterations.\nFor this problem, we characterize the exact load-memory trade-off for\nworst-case shuffling by deriving the minimum communication load for a given\nstorage capacity per worker node. As a byproduct, the exact load-memory\ntrade-off for any shuffling is characterized when the number of files is equal\nto the number of worker nodes. We propose a novel deterministic coded shuffling\nscheme, which improves the state of the art, by exploiting the cache memories\nto create coded functions that can be decoded by several worker nodes. Then, we\nprove the optimality of our proposed scheme by deriving a matching lower bound\nand showing that the placement phase of the proposed coded shuffling scheme is\noptimal over all shuffles.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 17:31:30 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 07:47:42 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Elmahdy", "Adel", ""], ["Mohajer", "Soheil", ""]]}, {"id": "1807.04345", "submitter": "Brenton Lessley", "authors": "Brenton Lessley", "title": "Data-Parallel Hashing Techniques for GPU Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hash tables are one of the most fundamental data structures for effectively\nstoring and accessing sparse data, with widespread usage in domains ranging\nfrom computer graphics to machine learning. This study surveys the\nstate-of-the-art research on data-parallel hashing techniques for emerging\nmassively-parallel, many-core GPU architectures. Key factors affecting the\nperformance of different hashing schemes are discovered and used to suggest\nbest practices and pinpoint areas for further research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 20:33:22 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Lessley", "Brenton", ""]]}, {"id": "1807.04449", "submitter": "Jonathan Scarlett", "authors": "Steffen Bondorf, Binbin Chen, Jonathan Scarlett, Haifeng Yu, and Yuda\n  Zhao", "title": "Cross-Sender Bit-Mixing Coding", "comments": "Published in the International Conference on Information Processing\n  in Sensor Networks (IPSN), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling to avoid packet collisions is a long-standing challenge in\nnetworking, and has become even trickier in wireless networks with multiple\nsenders and multiple receivers. In fact, researchers have proved that even {\\em\nperfect} scheduling can only achieve $\\mathbf{R} = O(\\frac{1}{\\ln N})$. Here\n$N$ is the number of nodes in the network, and $\\mathbf{R}$ is the {\\em medium\nutilization rate}. Ideally, one would hope to achieve $\\mathbf{R} = \\Theta(1)$,\nwhile avoiding all the complexities in scheduling. To this end, this paper\nproposes {\\em cross-sender bit-mixing coding} ({\\em BMC}), which does not rely\non scheduling. Instead, users transmit simultaneously on suitably-chosen slots,\nand the amount of overlap in different user's slots is controlled via coding.\nWe prove that in all possible network topologies, using BMC enables us to\nachieve $\\mathbf{R}=\\Theta(1)$. We also prove that the space and time\ncomplexities of BMC encoding/decoding are all low-order polynomials.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 07:27:56 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 08:52:23 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 04:05:26 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Bondorf", "Steffen", ""], ["Chen", "Binbin", ""], ["Scarlett", "Jonathan", ""], ["Yu", "Haifeng", ""], ["Zhao", "Yuda", ""]]}, {"id": "1807.04616", "submitter": "W. Cyrus Proctor", "authors": "W. Cyrus Proctor, Mike Packard, Anagha Jamthe, Richard Cardone, Joseph\n  Stubbs", "title": "Virtualizing the Stampede2 Supercomputer with Applications to HPC in the\n  Cloud", "comments": "6 pages, 0 figures, PEARC '18: Practice and Experience in Advanced\n  Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": null, "doi": "10.1145/3219104.3219131", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Methods developed at the Texas Advanced Computing Center (TACC) are described\nand demonstrated for automating the construction of an elastic, virtual cluster\nemulating the Stampede2 high performance computing (HPC) system. The cluster\ncan be built and/or scaled in a matter of minutes on the Jetstream self-service\ncloud system and shares many properties of the original Stampede2, including:\ni) common identity management, ii) access to the same file systems, iii)\nequivalent software application stack and module system, iv) similar job\nscheduling interface via Slurm.\n  We measure time-to-solution for a number of common scientific applications on\nour virtual cluster against equivalent runs on Stampede2 and develop an\napplication profile where performance is similar or otherwise acceptable. For\nsuch applications, the virtual cluster provides an effective form of \"cloud\nbursting\" with the potential to significantly improve overall turnaround time,\nparticularly when Stampede2 is experiencing long queue wait times. In addition,\nthe virtual cluster can be used for test and debug without directly impacting\nStampede2. We conclude with a discussion of how science gateways can leverage\nthe TACC Jobs API web service to incorporate this cloud bursting technique\ntransparently to the end user.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 14:09:26 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Proctor", "W. Cyrus", ""], ["Packard", "Mike", ""], ["Jamthe", "Anagha", ""], ["Cardone", "Richard", ""], ["Stubbs", "Joseph", ""]]}, {"id": "1807.04728", "submitter": "Jim Basney", "authors": "Alex Withers (NCSA), Brian Bockelman (University of Nebraska-Lincoln),\n  Derek Weitzel (University of Nebraska-Lincoln), Duncan Brown (Syracuse\n  University), Jeff Gaynor (NCSA), Jim Basney (NCSA), Todd Tannenbaum\n  (University of Wisconsin-Madison), Zach Miller (University of\n  Wisconsin-Madison)", "title": "SciTokens: Capability-Based Secure Access to Remote Scientific Data", "comments": "8 pages, 6 figures, PEARC '18: Practice and Experience in Advanced\n  Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": null, "doi": "10.1145/3219104.3219135", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The management of security credentials (e.g., passwords, secret keys) for\ncomputational science workflows is a burden for scientists and information\nsecurity officers. Problems with credentials (e.g., expiration, privilege\nmismatch) cause workflows to fail to fetch needed input data or store valuable\nscientific results, distracting scientists from their research by requiring\nthem to diagnose the problems, re-run their computations, and wait longer for\ntheir results. In this paper, we introduce SciTokens, open source software to\nhelp scientists manage their security credentials more reliably and securely.\nWe describe the SciTokens system architecture, design, and implementation\naddressing use cases from the Laser Interferometer Gravitational-Wave\nObservatory (LIGO) Scientific Collaboration and the Large Synoptic Survey\nTelescope (LSST) projects. We also present our integration with widely-used\nsoftware that supports distributed scientific computing, including HTCondor,\nCVMFS, and XrootD. SciTokens uses IETF-standard OAuth tokens for\ncapability-based secure access to remote scientific data. The access tokens\nconvey the specific authorizations needed by the workflows, rather than\ngeneral-purpose authentication impersonation credentials, to address the risks\nof scientific workflows running on distributed infrastructure including NSF\nresources (e.g., LIGO Data Grid, Open Science Grid, XSEDE) and public clouds\n(e.g., Amazon Web Services, Google Cloud, Microsoft Azure). By improving the\ninteroperability and security of scientific workflows, SciTokens 1) enables use\nof distributed computing for scientific domains that require greater data\nprotection and 2) enables use of more widely distributed computing resources by\nreducing the risk of credential abuse on remote systems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 17:06:19 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Withers", "Alex", "", "NCSA"], ["Bockelman", "Brian", "", "University of Nebraska-Lincoln"], ["Weitzel", "Derek", "", "University of Nebraska-Lincoln"], ["Brown", "Duncan", "", "Syracuse\n  University"], ["Gaynor", "Jeff", "", "NCSA"], ["Basney", "Jim", "", "NCSA"], ["Tannenbaum", "Todd", "", "University of Wisconsin-Madison"], ["Miller", "Zach", "", "University of\n  Wisconsin-Madison"]]}, {"id": "1807.04823", "submitter": "Wei Zhao", "authors": "Wei Zhao, Borzoo Bonakdarpour", "title": "Decentralized Multi-UAV Routing in the Presence of Disturbances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a decentralized and online path planning technique for a network\nof unmanned aerial vehicles (UAVs) in the presence of weather disturbances. In\nour problem setting, the group of UAVs are required to collaboratively visit a\nset of goals scattered in a 2-dimensional area. Each UAV will have to spend\nenergy to reach these goals, but due to unforeseen disturbances, the required\nenergy may vary over time and does not necessarily conform with the initial\nforecast and/or pre-computed optimal paths. Thus, we are dealing with two\nfundamental interrelated problems to find a global optimum at each point of\ntime: (1) energy consumption prediction based on disturbances and, hence,\nonline path replanning, and (2) distributed agreement among all UAVs to divide\nthe remaining unvisited goals based on their positions and energy requirements.\nOur approach consists of four main components: (i) a distributed algorithm that\nperiodically divides the unvisited goals among all the UAVs based on the\ncurrent energy requirements of the UAVs, (ii) a local (i.e., UAV-level)\n$\\AStar$-based algorithm that computes the {\\em desirable} path for each UAV to\nreach the nodes assigned to it, (iii) a local PID controller that {\\em\npredicts} the inputs to the UAV (i.e., thrust and moments), and (iv) a planner\nthat computes the required energy and the replanning time period. We validate\nour proposed solution through a rich set of simulations and show that our\napproach is significantly more efficient than a best-effort algorithm that\ndirects each idle UAV to visit the closest unvisited goal.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 21:11:46 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Zhao", "Wei", ""], ["Bonakdarpour", "Borzoo", ""]]}, {"id": "1807.04835", "submitter": "Sobhan Niknam", "authors": "Jiali Teddy Zhai, Sobhan Niknam, and Todor Stefanov", "title": "Modeling, Analysis, and Hard Real-time Scheduling of Adaptive Streaming\n  Applications", "comments": "Accepted for presentation at EMSOFT 2018 and for publication in IEEE\n  Transactions on Computer-Aided Design of Integrated Circuits and Systems\n  (TCAD) as part of the ESWEEK-TCAD special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-time systems, the application's behavior has to be predictable at\ncompile-time to guarantee timing constraints. However, modern streaming\napplications which exhibit adaptive behavior due to mode switching at run-time,\nmay degrade system predictability due to unknown behavior of the application\nduring mode transitions. Therefore, proper temporal analysis during mode\ntransitions is imperative to preserve system predictability. To this end, in\nthis paper, we initially introduce Mode Aware Data Flow (MADF) which is our new\npredictable Model of Computation (MoC) to efficiently capture the behavior of\nadaptive streaming applications. Then, as an important part of the operational\nsemantics of MADF, we propose the Maximum-Overlap Offset (MOO) which is our\nnovel protocol for mode transitions. The main advantage of this transition\nprotocol is that, in contrast to self-timed transition protocols, it avoids\ntiming interference between modes upon mode transitions. As a result, any mode\ntransition can be analyzed independently from the mode transitions that\noccurred in the past. Based on this transition protocol, we propose a hard\nreal-time analysis as well to guarantee timing constraints by avoiding\nprocessor overloading during mode transitions. Therefore, using this protocol,\nwe can derive a lower bound and an upper bound on the earliest starting time of\nthe tasks in the new mode during mode transitions in such a way that hard\nreal-time constraints are respected.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 21:37:08 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Zhai", "Jiali Teddy", ""], ["Niknam", "Sobhan", ""], ["Stefanov", "Todor", ""]]}, {"id": "1807.04900", "submitter": "Gregory Schwartzman", "authors": "Ran Ben-Basat, Ken-ichi Kawarabayashi, Gregory Schwartzman", "title": "Parameterized Distributed Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we initiate a thorough study of parameterized graph\noptimization problems in the distributed setting. In a parameterized problem,\nan algorithm decides whether a solution of size bounded by a \\emph{parameter}\n$k$ exists and if so, it finds one. We study fundamental problems, including\nMinimum Vertex Cover (MVC), Maximum Independent Set (MaxIS), Maximum Matching\n(MaxM), and many others, in both the LOCAL and CONGEST distributed computation\nmodels. We present lower bounds for the round complexity of solving\nparameterized problems in both models, together with optimal and near-optimal\nupper bounds.\n  Our results extend beyond the scope of parameterized problems. We show that\nany LOCAL $(1+\\epsilon)$-approximation algorithm for the above problems must\ntake $\\Omega(\\epsilon^{-1})$ rounds. Joined with the algorithm of [GKM17] and\nthe $\\Omega(\\sqrt{\\frac{\\log n}{\\log\\log n}})$ lower bound of [KMW16], this\nsettles the complexity of $(1+\\epsilon)$-approximating MVC, MaxM and MaxIS at\n$(\\epsilon^{-1}\\log n)^{\\Theta(1)}$. We also show that our parameterized\napproach reduces the runtime of exact and approximate CONGEST algorithms for\nMVC and MaxM if the optimal solution is small, without knowing its size\nbeforehand. Finally, we propose the first deterministic $o(n^2)$ rounds CONGEST\nalgorithms that approximate MVC and MaxM within a factor strictly smaller than\n$2$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 03:48:35 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 03:52:02 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ben-Basat", "Ran", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1807.04938", "submitter": "Zarko Milosevic", "authors": "Ethan Buchman, Jae Kwon and Zarko Milosevic", "title": "The latest gossip on BFT consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents Tendermint, a new protocol for ordering events in a\ndistributed network under adversarial conditions. More commonly known as\nByzantine Fault Tolerant (BFT) consensus or atomic broadcast, the problem has\nattracted significant attention in recent years due to the widespread success\nof blockchain-based digital currencies, such as Bitcoin and Ethereum, which\nsuccessfully solved the problem in a public setting without a central\nauthority. Tendermint modernizes classic academic work on the subject and\nsimplifies the design of the BFT algorithm by relying on a peer-to-peer gossip\nprotocol among nodes.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 06:53:53 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 09:01:15 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 10:21:27 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Buchman", "Ethan", ""], ["Kwon", "Jae", ""], ["Milosevic", "Zarko", ""]]}, {"id": "1807.04985", "submitter": "Julian Kunkel", "authors": "Julian M. Kunkel, Eugen Betke, Matt Bryson, Philip Carns, Rosemary\n  Francis, Wolfgang Frings, Roland Laifer, Sandra Mendez", "title": "Tools for Analyzing Parallel I/O", "comments": "Workshop paper: https://hps.vi4io.org/events/2018/iodc It will be\n  published with Springer LNCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel application I/O performance often does not meet user expectations.\nAdditionally, slight access pattern modifications may lead to significant\nchanges in performance due to complex interactions between hardware and\nsoftware. These challenges call for sophisticated tools to capture, analyze,\nunderstand, and tune application I/O. In this paper, we highlight advances in\nmonitoring tools to help address this problem. We also describe best practices,\nidentify issues in measurement and analysis, and provide practical approaches\nto translate parallel I/O analysis into actionable outcomes for users, facility\noperators, and researchers.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 09:37:38 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 08:34:14 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 12:17:17 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Kunkel", "Julian M.", ""], ["Betke", "Eugen", ""], ["Bryson", "Matt", ""], ["Carns", "Philip", ""], ["Francis", "Rosemary", ""], ["Frings", "Wolfgang", ""], ["Laifer", "Roland", ""], ["Mendez", "Sandra", ""]]}, {"id": "1807.05014", "submitter": "Ran Gelles", "authors": "Mark Braverman and Klim Efremenko and Ran Gelles and Michael A.\n  Yitayew", "title": "Optimal Short-Circuit Resilient Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fault-tolerant boolean formulas in which the output of a faulty\ngate is short-circuited to one of the gate's inputs. A recent result by Kalai\net al. (FOCS 2012) converts any boolean formula into a resilient formula of\npolynomial size that works correctly if less than a fraction $1/6$ of the gates\n(on every input-to-output path) are faulty. We improve the result of Kalai et\nal., and show how to efficiently fortify any boolean formula against a fraction\n$1/5$ of short-circuit gates per path, with only a polynomial blowup in size.\nWe additionally show that it is impossible to obtain formulas with higher\nresilience and sub-exponential growth in size.\n  Towards our results, we consider interactive coding schemes when noiseless\nfeedback is present; these produce resilient boolean formulas via a\nKarchmer-Wigderson relation. We develop a coding scheme that resists up to a\nfraction $1/5$ of corrupted transmissions in each direction of the interactive\nchannel. We further show that such a level of noise is maximal for coding\nschemes with sub-exponential blowup in communication. Our coding scheme takes a\nsurprising inspiration from Blockchain technology.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 11:29:05 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Braverman", "Mark", ""], ["Efremenko", "Klim", ""], ["Gelles", "Ran", ""], ["Yitayew", "Michael A.", ""]]}, {"id": "1807.05118", "submitter": "Richard Liaw", "authors": "Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E.\n  Gonzalez, Ion Stoica", "title": "Tune: A Research Platform for Distributed Model Selection and Training", "comments": "8 Pages, Presented at the 2018 ICML AutoML workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning algorithms are increasingly computationally\ndemanding, requiring specialized hardware and distributed computation to\nachieve high performance in a reasonable time frame. Many hyperparameter search\nalgorithms have been proposed for improving the efficiency of model selection,\nhowever their adaptation to the distributed compute environment is often\nad-hoc. We propose Tune, a unified framework for model selection and training\nthat provides a narrow-waist interface between training scripts and search\nalgorithms. We show that this interface meets the requirements for a broad\nrange of hyperparameter search algorithms, allows straightforward scaling of\nsearch to large clusters, and simplifies algorithm implementation. We\ndemonstrate the implementation of several state-of-the-art hyperparameter\nsearch algorithms in Tune. Tune is available at\nhttp://ray.readthedocs.io/en/latest/tune.html.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 15:00:17 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Liaw", "Richard", ""], ["Liang", "Eric", ""], ["Nishihara", "Robert", ""], ["Moritz", "Philipp", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""]]}, {"id": "1807.05139", "submitter": "Jennifer Welch", "authors": "Reginald Frank and Jennifer L. Welch", "title": "A Tight Lower Bound for Clock Synchronization in Odd-Ary M-Toroids", "comments": "5 pages, 4 figures, to appear as a brief announcement at 2018\n  International Symposium on Distributed Computing (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronizing clocks in a distributed system in which processes communicate\nthrough messages with uncertain delays is subject to inherent errors. Prior\nwork has shown upper and lower bounds on the best synchronization achievable in\na variety of network topologies and assumptions about the uncertainty on the\nmessage delays. However, until now there has not been a tight closed-form\nexpression for the optimal synchronization in $k$-ary $m$-cubes with\nwraparound, where $k$ is odd. In this paper, we prove a lower bound of\n$\\frac{1}{4}um\\left(k-\\frac{1}{k}\\right)$, where $k$ is the (odd) number of\nprocesses in the each of the $m$ dimensions, and $u$ is the uncertainty in\ndelay on every link. Our lower bound matches the previously known upper bound.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 15:28:47 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Frank", "Reginald", ""], ["Welch", "Jennifer L.", ""]]}, {"id": "1807.05308", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Ron Brightwell, Alan Edelman, Vijay Gadepally, Hayden\n  Jananthan, Michael Jones, Sam Madden, Peter Michaleas, Hamed Okhravi, Kevin\n  Pedretti, Albert Reuther, Thomas Sterling, Mike Stonebraker", "title": "TabulaROSA: Tabular Operating System Architecture for Massively Parallel\n  Heterogeneous Compute Engines", "comments": "8 pages, 6 figures, accepted at IEEE HPEC 2018", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547577", "report-no": null, "categories": "cs.DC cs.DB cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise in computing hardware choices is driving a reevaluation of operating\nsystems. The traditional role of an operating system controlling the execution\nof its own hardware is evolving toward a model whereby the controlling\nprocessor is distinct from the compute engines that are performing most of the\ncomputations. In this context, an operating system can be viewed as software\nthat brokers and tracks the resources of the compute engines and is akin to a\ndatabase management system. To explore the idea of using a database in an\noperating system role, this work defines key operating system functions in\nterms of rigorous mathematical semantics (associative array algebra) that are\ndirectly translatable into database operations. These operations possess a\nnumber of mathematical properties that are ideal for parallel operating systems\nby guaranteeing correctness over a wide range of parallel operations. The\nresulting operating system equations provide a mathematical specification for a\nTabular Operating System Architecture (TabulaROSA) that can be implemented on\nany platform. Simulations of forking in TabularROSA are performed using an\nassociative array implementation and compared to Linux on a 32,000+ core\nsupercomputer. Using over 262,000 forkers managing over 68,000,000,000\nprocesses, the simulations show that TabulaROSA has the potential to perform\noperating system functions on a massively parallel scale. The TabulaROSA\nsimulations show 20x higher performance as compared to Linux while managing\n2000x more processes in fully searchable tables.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 00:02:55 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kepner", "Jeremy", ""], ["Brightwell", "Ron", ""], ["Edelman", "Alan", ""], ["Gadepally", "Vijay", ""], ["Jananthan", "Hayden", ""], ["Jones", "Michael", ""], ["Madden", "Sam", ""], ["Michaleas", "Peter", ""], ["Okhravi", "Hamed", ""], ["Pedretti", "Kevin", ""], ["Reuther", "Albert", ""], ["Sterling", "Thomas", ""], ["Stonebraker", "Mike", ""]]}, {"id": "1807.05358", "submitter": "Zhihao Jia", "authors": "Zhihao Jia, Matei Zaharia, Alex Aiken", "title": "Beyond Data and Model Parallelism for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "Proceedings of the 35th International Conference on Machine Learning\n  (PMLR 80)", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational requirements for training deep neural networks (DNNs) have\ngrown to the point that it is now standard practice to parallelize training.\nExisting deep learning systems commonly use data or model parallelism, but\nunfortunately, these strategies often result in suboptimal parallelization\nperformance.\n  In this paper, we define a more comprehensive search space of parallelization\nstrategies for DNNs called SOAP, which includes strategies to parallelize a DNN\nin the Sample, Operation, Attribute, and Parameter dimensions. We also propose\nFlexFlow, a deep learning framework that uses guided randomized search of the\nSOAP space to find a fast parallelization strategy for a specific parallel\nmachine. To accelerate this search, FlexFlow introduces a novel execution\nsimulator that can accurately predict a parallelization strategy's performance\nand is three orders of magnitude faster than prior approaches that have to\nexecute each strategy. We evaluate FlexFlow with six real-world DNN benchmarks\non two GPU clusters and show that FlexFlow can increase training throughput by\nup to 3.8x over state-of-the-art approaches, even when including its search\ntime, and also improves scalability.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 08:44:31 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Jia", "Zhihao", ""], ["Zaharia", "Matei", ""], ["Aiken", "Alex", ""]]}, {"id": "1807.05432", "submitter": "Duc Tran", "authors": "Duc A. Tran and Quynh Vo", "title": "A Geo-Aware Server Assignment Problem for Mobile Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As mobile devices have become the preferred tool for communication, work, and\nentertainment, traffic at the edge of the network is growing more rapidly than\never. To improve user experience, commodity servers are deployed in the edge to\nform a decentralized network of mini datacenters each serving a localized\nregion. A challenge is how to place these servers geographically to maximize\nthe offloading benefit and be close to the users they respectively serve. We\nintroduce a formulation for this problem to serve applications that involve\npairwise communication between mobile devices at different geolocations. We\nexplore several heuristic solutions and compare them in an evaluation using\nboth real-world and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 19:12:53 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Tran", "Duc A.", ""], ["Vo", "Quynh", ""]]}, {"id": "1807.05506", "submitter": "Yong Wang", "authors": "Ziyan Gao, Yong Wang, Yifan Gao, Xingtian Ren", "title": "Multi-objective Non-cooperative Game Model for Cost-based Task\n  Scheduling in Computational Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task scheduling is an important and complex problem in computational grid. A\ncomputational grid often covers a range of different kinds of nodes, which\noffers a complex environment. There is a need to develop algorithms that can\ncapture this complexity as while as can be easily implemented and used to solve\na wide range of load-balancing scenarios. In this paper, we propose a game\ntheory based algorithm to the grid load balancing problem on the principle of\nminimizing the cost of the grid. The grid load-balancing problem is treated as\na non-cooperative game. The experiment results demonstrate that the game based\nalgorithm has a desirable capability to reduce the cost of the grid.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 08:00:52 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Gao", "Ziyan", ""], ["Wang", "Yong", ""], ["Gao", "Yifan", ""], ["Ren", "Xingtian", ""]]}, {"id": "1807.05623", "submitter": "Timur Bazhirov", "authors": "Protik Das, Mohammad Mohammadi, Timur Bazhirov", "title": "Accessible computational materials design with high fidelity and high\n  throughput", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cond-mat.other cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite multiple successful applications of high-throughput computational\nmaterials design from first principles, there is a number of factors that\ninhibit its future adoption. Of particular importance are limited ability to\nprovide high fidelity in a reliable manner and limited accessibility to\nnon-expert users. We present example applications of a novel approach, where\nhigh-fidelity first-principles simulation techniques, Density Functional Theory\nwith Hybrid Screened Exchange (HSE) and GW approximation, are standardized and\nmade available online in an accessible and repeatable setting. We apply this\napproach to extract electronic band gaps and band structures for a diverse set\nof 71 materials ranging from pure elements to III-V and II-VI compounds,\nternary oxides and alloys. We find that for HSE and G0W0, the average relative\nerror fits within 20%, whereas for conventional Generalized Gradient\nApproximation the error is 55%. For HSE we find the average calculation time on\nan up-to-date server centrally available from a public cloud provider to fit\nwithin 48 hours. This work provides a cost-effective, accessible and repeatable\npractical recipe for performing high-fidelity first-principles calculations of\nelectronic materials in a high-throughput manner.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 22:04:59 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Das", "Protik", ""], ["Mohammadi", "Mohammad", ""], ["Bazhirov", "Timur", ""]]}, {"id": "1807.05626", "submitter": "Emanuele Natale", "authors": "Andrea Clementi, Luciano Gual\\`a, Emanuele Natale, Francesco Pasquale,\n  Giacomo Scornavacca, Luca Trevisan", "title": "Consensus Needs Broadcast in Noiseless Models but can be Exponentially\n  Easier in the Presence of Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus and Broadcast are two fundamental problems in distributed\ncomputing, whose solutions have several applications. Intuitively, Consensus\nshould be no harder than Broadcast, and this can be rigorously established in\nseveral models. Can Consensus be easier than Broadcast?\n  In models that allow noiseless communication, we prove a reduction of (a\nsuitable variant of) Broadcast to binary Consensus, that preserves the\ncommunication model and all complexity parameters such as randomness, number of\nrounds, communication per round, etc., while there is a loss in the success\nprobability of the protocol. Using this reduction, we get, among other\napplications, the first logarithmic lower bound on the number of rounds needed\nto achieve Consensus in the uniform GOSSIP model on the complete graph. The\nlower bound is tight and, in this model, Consensus and Broadcast are\nequivalent.\n  We then turn to distributed models with noisy communication channels that\nhave been studied in the context of some bio-inspired systems. In such models,\nonly one noisy bit is exchanged when a communication channel is established\nbetween two nodes, and so one cannot easily simulate a noiseless protocol by\nusing error-correcting codes. An $\\Omega(\\epsilon^{-2} n)$ lower bound on the\nnumber of rounds needed for Broadcast is proved by Boczkowski et al. [PLOS\nComp. Bio. 2018] in one such model (noisy uniform PULL, where $\\epsilon$ is a\nparameter that measures the amount of noise). In such model, we prove a new\n$\\Theta(\\epsilon^{-2} n \\log n)$ bound for Broadcast and a\n$\\Theta(\\epsilon^{-2} \\log n)$ bound for binary Consensus, thus establishing an\nexponential gap between the number of rounds necessary for Consensus versus\nBroadcast.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 22:47:38 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Clementi", "Andrea", ""], ["Gual\u00e0", "Luciano", ""], ["Natale", "Emanuele", ""], ["Pasquale", "Francesco", ""], ["Scornavacca", "Giacomo", ""], ["Trevisan", "Luca", ""]]}, {"id": "1807.05642", "submitter": "John Feser", "authors": "Peter Ahrens, John Feser, Robin Hui", "title": "LATE Ain'T Earley: A Faster Parallel Earley Parser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the LATE algorithm, an asynchronous variant of the Earley\nalgorithm for parsing context-free grammars. The Earley algorithm is naturally\ntask-based, but is difficult to parallelize because of dependencies between the\ntasks. We present the LATE algorithm, which uses additional data structures to\nmaintain information about the state of the parse so that work items may be\nprocessed in any order. This property allows the LATE algorithm to be sped up\nusing task parallelism. We show that the LATE algorithm can achieve a 120x\nspeedup over the Earley algorithm on a natural language task.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 00:50:00 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ahrens", "Peter", ""], ["Feser", "John", ""], ["Hui", "Robin", ""]]}, {"id": "1807.05674", "submitter": "Sayaka Kamei", "authors": "Sayaka Kamei and Hirotsugu Kakugawa", "title": "An asynchronous message-passing distributed algorithm for the global\n  critical section problem", "comments": "This is a modified version of the conference paper in PDAA2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the global $(l,k)$-CS problem which is the problem of\ncontrolling the system in such a way that, at least $l$ and at most $k$\nprocesses must be in the CS at a time in the network. In this paper, a\ndistributed solution is proposed in the asynchronous message-passing model. Our\nsolution is a versatile composition method of algorithms for $l$-mutual\ninclusion and $k$-mutual exclusion. Its message complexity is $O(|Q|)$, where\n$|Q|$ is the maximum size for the quorum of a coterie used by the algorithm,\nwhich is typically $|Q| = \\sqrt{n}$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 04:28:51 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Kamei", "Sayaka", ""], ["Kakugawa", "Hirotsugu", ""]]}, {"id": "1807.05729", "submitter": "Clovis Ouedraogo", "authors": "Clovis Anicet Ouedraogo (LAAS-SARA), Samir Medjiah (LAAS-SARA),\n  Christophe Chassot (LAAS-SARA), Khalil Drira (LAAS-SARA)", "title": "Enhancing Middleware-based IoT Applications through Run-Time Pluggable\n  QoS Management Mechanisms. Application to a oneM2M compliant IoT Middleware", "comments": null, "journal-ref": "The 9th International Conference on Ambient Systems, Networks and\n  Technologies (ANT-2018), the 8th International Conference on Sustainable\n  Energy Information Technology (SEIT-2018), May 2018, Porto, Portugal. 130,\n  pp.619 - 627, 2018", "doi": "10.1016/j.procs.2018.04.112", "report-no": "Rapport LAAS n{\\textdegree} 18131", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, telecom and computer networks have witnessed new\nconcepts and technologies through Network Function Virtualization (NFV) and\nSoftware-Defined Networking (SDN). SDN, which allows applications to have a\ncontrol over the network, and NFV, which allows deploying network functions in\nvirtualized environments, are two paradigms that are increasingly used for the\nInternet of Things (IoT). This Internet (IoT) brings the promise to\ninterconnect billions of devices in the next few years rises several scientific\nchallenges in particular those of the satisfaction of the quality of service\n(QoS) required by the IoT applications. In order to address this problem, we\nhave identified two bottlenecks with respect to the QoS: the traversed networks\nand the intermediate entities that allows the application to interact with the\nIoT devices. In this paper, we first present an innovative vision of a \"network\nfunction\" with respect to their deployment and runtime environment. Then, we\ndescribe our general approach of a solution that consists in the dynamic,\nautonomous, and seamless deployment of QoS management mechanisms. We also\ndescribe the requirements for the implementation of such approach. Finally, we\npresent a redirection mechanism, implemented as a network function, allowing\nthe seamless control of the data path of a given middleware traffic. This\nmechanism is assessed through a use case related to vehicular transportation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 08:44:12 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ouedraogo", "Clovis Anicet", "", "LAAS-SARA"], ["Medjiah", "Samir", "", "LAAS-SARA"], ["Chassot", "Christophe", "", "LAAS-SARA"], ["Drira", "Khalil", "", "LAAS-SARA"]]}, {"id": "1807.05731", "submitter": "Clovis Ouedraogo", "authors": "Yassine Banouar (LAAS-SARA), Clovis Anicet Ouedraogo (LAAS-SARA),\n  Christophe Chassot (LAAS-SARA), Abdellah Zyane (UCA)", "title": "QoS management mechanisms for Enhanced Living Environments in IoT", "comments": "2017 IFIP/IEEE Symposium on Integrated Network and Service Management\n  (IM), May 2017, Lisbon, Portugal. IEEE, 2017", "journal-ref": null, "doi": "10.23919/INM.2017.7987454", "report-no": "Rapport LAAS n{\\textdegree} 17625", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) paradigm is expected to bring ubiquitous\nintelligence through new applications in order to enhance living and other\nenvironments. Several research and standardization studies are now focused on\nthe Middleware level of the underlying communication system. For this level,\nseveral challenges need to be considered, among them the Quality of Service\n(QoS) issue. The Autonomic Computing paradigm is now recognized as a promising\napproach to help communication and other systems to self-adapt when the context\nis changing. With the aim to promote the vision of an autonomic\nMiddleware-level QoS management for IoT-based systems, this paper proposes a\nset of QoS-oriented mechanisms that can be dynamically executed at the\nMiddleware level to correct QoS degradation. The benefits of the proposed\nmechanisms are also illustrated for a concrete case of Enhanced Living\nEnvironment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 08:49:04 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Banouar", "Yassine", "", "LAAS-SARA"], ["Ouedraogo", "Clovis Anicet", "", "LAAS-SARA"], ["Chassot", "Christophe", "", "LAAS-SARA"], ["Zyane", "Abdellah", "", "UCA"]]}, {"id": "1807.06070", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Rakhi Garg, P K Mishra", "title": "Performance Optimization of MapReduce-based Apriori Algorithm on Hadoop\n  Cluster", "comments": "24 pages, 5 figures, 12 tables, 5 algorithms", "journal-ref": "Computers & Electrical Engineering 2018; 67: 348-364", "doi": "10.1016/j.compeleceng.2017.10.008", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many techniques have been proposed to implement the Apriori algorithm on\nMapReduce framework but only a few have focused on performance improvement. FPC\n(Fixed Passes Combined-counting) and DPC (Dynamic Passes Combined-counting)\nalgorithms combine multiple passes of Apriori in a single MapReduce phase to\nreduce the execution time. In this paper, we propose improved MapReduce based\nApriori algorithms VFPC (Variable Size based Fixed Passes Combined-counting)\nand ETDPC (Elapsed Time based Dynamic Passes Combined-counting) over FPC and\nDPC. Further, we optimize the multi-pass phases of these algorithms by skipping\npruning step in some passes, and propose Optimized-VFPC and Optimized-ETDPC\nalgorithms. Quantitative analysis reveals that counting cost of additional\nun-pruned candidates produced due to skipped-pruning is less significant than\nreduction in computation cost due to the same. Experimental results show that\nVFPC and ETDPC are more robust and flexible than FPC and DPC whereas their\noptimized versions are more efficient in terms of execution time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 19:30:50 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Singh", "Sudhakar", ""], ["Garg", "Rakhi", ""], ["Mishra", "P K", ""]]}, {"id": "1807.06179", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Ronghua Xu, Deeraj Nagothu, Yu Chen, Alexander\n  Aved, Erik Blasch", "title": "Real-Time Index Authentication for Event-Oriented Surveillance Video\n  Query using Blockchain", "comments": "Submitted to the 1st International Workshop on BLockchain Enabled\n  Sustainable Smart Cities (BLESS 2018), In Conjunction with the 4th IEEE\n  Annual International Smart Cities Conference (ISC2 2018), Kansas City, MO,\n  USA, Sept. 16-19, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information from surveillance video is essential for situational awareness\n(SAW). Nowadays, a prohibitively large amount of surveillance data is being\ngenerated continuously by ubiquitously distributed video sensors. It is very\nchallenging to immediately identify the objects of interest or zoom in\nsuspicious actions from thousands of video frames. Making the big data\nindexable is critical to tackle this problem. It is ideal to generate pattern\nindexes in a real-time, on-site manner on the video streaming instead of\ndepending on the batch processing at the cloud centers. The modern\nedge-fog-cloud computing paradigm allows implementation of time sensitive tasks\nat the edge of the network. The on-site edge devices collect the information\nsensed in format of frames and extracts useful features. The near-site fog\nnodes conduct the contextualization and classification of the features. The\nremote cloud center is in charge of more data intensive and computing intensive\ntasks. However, exchanging the index information among devices in different\nlayers raises security concerns where an adversary can capture or tamper with\nfeatures to mislead the surveillance system. In this paper, a blockchain\nenabled scheme is proposed to protect the index data through an encrypted\nsecure channel between the edge and fog nodes. It reduces the chance of attacks\non the small edge and fog devices. The feasibility of the proposal is validated\nthrough intensive experimental analysis.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 02:02:39 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Xu", "Ronghua", ""], ["Nagothu", "Deeraj", ""], ["Chen", "Yu", ""], ["Aved", "Alexander", ""], ["Blasch", "Erik", ""]]}, {"id": "1807.06193", "submitter": "Maria Rodriguez", "authors": "Maria A. Rodriguez and Rajkumar Buyya", "title": "Container-based Cluster Orchestration Systems: A Taxonomy and Future\n  Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containers, enabling lightweight environment and performance isolation, fast\nand flexible deployment, and fine-grained resource sharing, have gained\npopularity in better application management and deployment in addition to\nhardware virtualization. They are being widely used by organizations to deploy\ntheir increasingly diverse workloads derived from modern-day applications such\nas web services, big data, and IoT in either proprietary clusters or private\nand public cloud data centers. This has led to the emergence of container\norchestration platforms, which are designed to manage the deployment of\ncontainerized applications in large-scale clusters. These systems are capable\nof running hundreds of thousands of jobs across thousands of machines. To do so\nefficiently, they must address several important challenges including\nscalability, fault-tolerance and availability, efficient resource utilization,\nand request throughput maximization among others. This paper studies these\nmanagement systems and proposes a taxonomy that identifies different mechanisms\nthat can be used to meet the aforementioned challenges. The proposed\nclassification is then applied to various state-of-the-art systems leading to\nthe identification of open research challenges and gaps in the literature\nintended as future directions for researchers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 03:13:57 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 20:53:48 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Rodriguez", "Maria A.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1807.06251", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari and Jara Uitto", "title": "Sparsifying Distributed Algorithms with Ramifications in Massively\n  Parallel Computation and Centralized Local Computation", "comments": "This is a shortened version of the abstract. Please see the pdf for\n  the full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for sparsifying distributed algorithms and exhibit how\nit leads to improvements that go past known barriers in two algorithmic\nsettings of large-scale graph processing: Massively Parallel Computation (MPC),\nand Local Computation Algorithms (LCA).\n  - MPC with Strongly Sublinear Memory: Recently, there has been growing\ninterest in obtaining MPC algorithms that are faster than their classic $O(\\log\nn)$-round parallel counterparts for problems such as MIS, Maximal Matching,\n2-Approximation of Minimum Vertex Cover, and $(1+\\epsilon)$-Approximation of\nMaximum Matching. Currently, all such MPC algorithms require\n$\\tilde{\\Omega}(n)$ memory per machine. Czumaj et al. [STOC'18] were the first\nto handle $\\tilde{\\Omega}(n)$ memory, running in $O((\\log\\log n)^2)$ rounds. We\nobtain $\\tilde{O}(\\sqrt{\\log \\Delta})$-round MPC algorithms for all these four\nproblems that work even when each machine has memory $n^{\\alpha}$ for any\nconstant $\\alpha\\in (0, 1)$. Here, $\\Delta$ denotes the maximum degree. These\nare the first sublogarithmic-time algorithms for these problems that break the\nlinear memory barrier.\n  - LCAs with Query Complexity Below the Parnas-Ron Paradigm: Currently, the\nbest known LCA for MIS has query complexity $\\Delta^{O(\\log \\Delta)} poly(\\log\nn)$, by Ghaffari [SODA'16]. As pointed out by Rubinfeld, obtaining a query\ncomplexity of $poly(\\Delta\\log n)$ remains a central open question. Ghaffari's\nbound almost reaches a $\\Delta^{\\Omega\\left(\\frac{\\log \\Delta}{\\log\\log\n\\Delta}\\right)}$ barrier common to all known MIS LCAs, which simulate\ndistributed algorithms by learning the local topology, \\`{a} la Parnas-Ron\n[TCS'07]. This barrier follows from the $\\Omega(\\frac{\\log \\Delta}{\\log\\log\n\\Delta})$ distributed lower bound of Kuhn, et al. [JACM'16]. We break this\nbarrier and obtain an MIS LCA with query complexity $\\Delta^{O(\\log\\log\n\\Delta)} poly(\\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 07:01:03 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Uitto", "Jara", ""]]}, {"id": "1807.06417", "submitter": "Amit Saha", "authors": "Johnu George, Ramdoot Pydipaty, Xinyuan Huang, Amit Saha, Debo Dutta,\n  Gary Wang and Uma Gangumalla", "title": "Tiered Object Storage using Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most data intensive applications often access only a few fields of the\nobjects they are operating on. Since NVM provides fast, byte-addressable access\nto durable memory, it is possible to access various fields of an object stored\nin NVM directly without incurring any serialization and deserialization cost.\nThis paper proposes a novel tiered object storage model that modifies a data\nstructure such that only a chosen subset of fields of the data structure are\nstored in NVM, while the remaining fields are stored in a cheaper (and a\ntraditional) storage layer such as HDDs/SSDs. We introduce a novel\nlinear-programming based optimization framework for deciding the field\nplacement. Our proof of concept demonstrates that a tiered object storage model\nimproves the execution time of standard operations by up to 50\\% by avoiding\nthe cost of serialization/deserialization and by reducing the memory footprint\nof operations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 17:52:32 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["George", "Johnu", ""], ["Pydipaty", "Ramdoot", ""], ["Huang", "Xinyuan", ""], ["Saha", "Amit", ""], ["Dutta", "Debo", ""], ["Wang", "Gary", ""], ["Gangumalla", "Uma", ""]]}, {"id": "1807.06431", "submitter": "Sreeja Nair", "authors": "Sreeja Nair and Marc Shapiro", "title": "Improving the \"Correct Eventual Consistency\" Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-9191", "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Preserving invariants while designing distributed applications under weak\nconsistency models is difficult. The CEC (Correct Eventual Consistency Tool) is\nmeant to aid the application designer in this task. It provides information\nabout the errors during concurrent operations and suggestions on how and where\nto synchronize operations. This report presents two features of the tool:\nproviding a counterexample for debugging and concurrency control suggestions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 13:47:40 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Nair", "Sreeja", ""], ["Shapiro", "Marc", ""]]}, {"id": "1807.06434", "submitter": "Mohamed Abdelfattah", "authors": "Mohamed S. Abdelfattah, David Han, Andrew Bitar, Roberto DiCecco,\n  Shane OConnell, Nitika Shanker, Joseph Chu, Ian Prins, Joshua Fender, Andrew\n  C. Ling, Gordon R. Chiu", "title": "DLA: Compiler and FPGA Overlay for Neural Network Inference Acceleration", "comments": "Accepted in the International Conference on Field-Programmable Logic\n  and Applications (FPL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlays have shown significant promise for field-programmable gate-arrays\n(FPGAs) as they allow for fast development cycles and remove many of the\nchallenges of the traditional FPGA hardware design flow. However, this often\ncomes with a significant performance burden resulting in very little adoption\nof overlays for practical applications. In this paper, we tailor an overlay to\na specific application domain, and we show how we maintain its full\nprogrammability without paying for the performance overhead traditionally\nassociated with overlays. Specifically, we introduce an overlay targeted for\ndeep neural network inference with only ~1% overhead to support the control and\nreprogramming logic using a lightweight very-long instruction word (VLIW)\nnetwork. Additionally, we implement a sophisticated domain specific graph\ncompiler that compiles deep learning languages such as Caffe or Tensorflow to\neasily target our overlay. We show how our graph compiler performs\narchitecture-driven software optimizations to significantly boost performance\nof both convolutional and recurrent neural networks (CNNs/RNNs) - we\ndemonstrate a 3x improvement on ResNet-101 and a 12x improvement for long\nshort-term memory (LSTM) cells, compared to naive implementations. Finally, we\ndescribe how we can tailor our hardware overlay, and use our graph compiler to\nachieve ~900 fps on GoogLeNet on an Intel Arria 10 1150 - the fastest ever\nreported on comparable FPGAs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 15:25:56 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Abdelfattah", "Mohamed S.", ""], ["Han", "David", ""], ["Bitar", "Andrew", ""], ["DiCecco", "Roberto", ""], ["OConnell", "Shane", ""], ["Shanker", "Nitika", ""], ["Chu", "Joseph", ""], ["Prins", "Ian", ""], ["Fender", "Joshua", ""], ["Ling", "Andrew C.", ""], ["Chiu", "Gordon R.", ""]]}, {"id": "1807.06534", "submitter": "Ralf-Peter Mundani", "authors": "Christoph Ertl (1), J\\'er\\^ome Frisch (2), Ralf-Peter Mundani (1) ((1)\n  Technische Universit\\\"at M\\\"unchen, Munich, Germany, (2) RWTH Aachen\n  University, Aachen, Germany)", "title": "Design and optimisation of an efficient HDF5 I/O kernel for massive\n  parallel fluid flow simulations", "comments": "24 pages, 8 figures", "journal-ref": "Concurrency and Computation: Practice and Experience 29 (2017)", "doi": "10.1002/cpe.4165", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more massive parallel codes running on several hundreds of thousands\nof cores enter the computational science and engineering domain, allowing\nhigh-fidelity computations on up to trillions of unknowns for very detailed\nanalyses of the underlying problems. During such runs, typically gigabytes of\ndata are being produced, hindering both efficient storage and (interactive)\ndata exploration. Here, advanced approaches based on inherently distributed\ndata formats such as HDF5 become necessary in order to avoid long latencies\nwhen storing the data and to support fast (random) access when retrieving the\ndata for visual processing. Avoiding file locking and using collective\nbuffering, write bandwidths to a single file close to the theoretical peak on a\nmodern supercomputing cluster were achieved. The structure of the output file\nsupports a very fast interactive visualisation and introduces additional\nsteering functionality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 17:40:39 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ertl", "Christoph", ""], ["Frisch", "J\u00e9r\u00f4me", ""], ["Mundani", "Ralf-Peter", ""]]}, {"id": "1807.06624", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Seth Pettie, Hengjie Zhang", "title": "Distributed Triangle Detection via Expander Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved distributed algorithms for triangle detection and its\nvariants in the CONGEST model. We show that Triangle Detection, Counting, and\nEnumeration can be solved in $\\tilde{O}(n^{1/2})$ rounds. In contrast, the\nprevious state-of-the-art bounds for Triangle Detection and Enumeration were\n$\\tilde{O}(n^{2/3})$ and $\\tilde{O}(n^{3/4})$, respectively, due to Izumi and\nLeGall (PODC 2017).\n  The main technical novelty in this work is a distributed graph partitioning\nalgorithm. We show that in $\\tilde{O}(n^{1-\\delta})$ rounds we can partition\nthe edge set of the network $G=(V,E)$ into three parts $E=E_m\\cup E_s\\cup E_r$\nsuch that\n  (a) Each connected component induced by $E_m$ has minimum degree\n$\\Omega(n^\\delta)$ and conductance $\\Omega(1/\\text{poly} \\log(n))$. As a\nconsequence the mixing time of a random walk within the component is\n$O(\\text{poly} \\log(n))$.\n  (b) The subgraph induced by $E_s$ has arboricity at most $n^{\\delta}$.\n  (c) $|E_r| \\leq |E|/6$.\n  All of our algorithms are based on the following generic framework, which we\nbelieve is of interest beyond this work. Roughly, we deal with the set $E_s$ by\nan algorithm that is efficient for low-arboricity graphs, and deal with the set\n$E_r$ using recursive calls. For each connected component induced by $E_m$, we\nare able to simulate congested clique algorithms with small overhead by\napplying a routing algorithm due to Ghaffari, Kuhn, and Su (PODC 2017) for high\nconductance graphs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 18:59:09 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Pettie", "Seth", ""], ["Zhang", "Hengjie", ""]]}, {"id": "1807.06629", "submitter": "Hao Yu", "authors": "Hao Yu and Sen Yang and Shenghuo Zhu", "title": "Parallel Restarted SGD with Faster Convergence and Less Communication:\n  Demystifying Why Model Averaging Works for Deep Learning", "comments": "No change has been made on the technical proof since V1. V2 changes\n  the title to emphasize its value in deep learning; polishes the writing; and\n  adds numerical simulations. This version further corrects a few typos in V2\n  posted a few days ago. A short version of this paper is accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed training of deep neural networks, parallel mini-batch SGD is\nwidely used to speed up the training process by using multiple workers. It uses\nmultiple workers to sample local stochastic gradient in parallel, aggregates\nall gradients in a single server to obtain the average, and update each\nworker's local model using a SGD update with the averaged gradient. Ideally,\nparallel mini-batch SGD can achieve a linear speed-up of the training time\n(with respect to the number of workers) compared with SGD over a single worker.\nHowever, such linear scalability in practice is significantly limited by the\ngrowing demand for gradient communication as more workers are involved. Model\naveraging, which periodically averages individual models trained over parallel\nworkers, is another common practice used for distributed training of deep\nneural networks since (Zinkevich et al. 2010) (McDonald, Hall, and Mann 2010).\nCompared with parallel mini-batch SGD, the communication overhead of model\naveraging is significantly reduced. Impressively, tremendous experimental works\nhave verified that model averaging can still achieve a good speed-up of the\ntraining time as long as the averaging interval is carefully controlled.\nHowever, it remains a mystery in theory why such a simple heuristic works so\nwell. This paper provides a thorough and rigorous theoretical study on why\nmodel averaging can work as well as parallel mini-batch SGD with significantly\nless communication overhead.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 19:14:17 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 09:09:49 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 07:57:46 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Yu", "Hao", ""], ["Yang", "Sen", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1807.06639", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Brian Bockelman, Derek Weitzel, David Swanson", "title": "Discovering Job Preemptions in the Open Science Grid", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/3219104.3229282", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Open Science Grid(OSG) is a world-wide computing system which facilitates\ndistributed computing for scientific research. It can distribute a\ncomputationally intensive job to geo-distributed clusters and process job's\ntasks in parallel. For compute clusters on the OSG, physical resources may be\nshared between OSG and cluster's local user-submitted jobs, with local jobs\npreempting OSG-based ones. As a result, job preemptions occur frequently in\nOSG, sometimes significantly delaying job completion time.\n  We have collected job data from OSG over a period of more than 80 days. We\npresent an analysis of the data, characterizing the preemption patterns and\ndifferent types of jobs. Based on observations, we have grouped OSG jobs into 5\ncategories and analyze the runtime statistics for each category. we further\nchoose different statistical distributions to estimate probability density\nfunction of job runtime for different classes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 19:42:54 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Zhang", "Zhe", ""], ["Bockelman", "Brian", ""], ["Weitzel", "Derek", ""], ["Swanson", "David", ""]]}, {"id": "1807.06701", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi, Richard\n  M. Karp", "title": "Massively Parallel Symmetry Breaking on Sparse Graphs: MIS and Maximal\n  Matching", "comments": "A merger of this paper and the independent and concurrent paper\n  [arxiv:1807.05374] appeared at PODC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of modern parallel paradigms such as MapReduce, Hadoop, or Spark,\nhas attracted a significant attention to the Massively Parallel Computation\n(MPC) model over the past few years, especially on graph problems. In this\nwork, we consider symmetry breaking problems of maximal independent set (MIS)\nand maximal matching (MM), which are among the most intensively studied\nproblems in distributed/parallel computing, in MPC.\n  These problems are known to admit efficient MPC algorithms if the space per\nmachine is near-linear in $n$, the number of vertices in the graph. This space\nrequirement however, as observed in the literature, is often significantly\nlarger than we can afford; especially when the input graph is sparse. In a\nsharp contrast, in the truly sublinear regime of $n^{1-\\Omega(1)}$ space per\nmachine, all the known algorithms take $\\log^{\\Omega(1)} n$ rounds which is\nconsidered inefficient.\n  Motivated by this shortcoming, we parametrize our algorithms by the\narboricity $\\alpha$ of the input graph, which is a well-received measure of its\nsparsity. We show that both MIS and MM admit $O(\\sqrt{\\log \\alpha}\\cdot\\log\\log\n\\alpha + \\log^2\\log n)$ round algorithms using $O(n^\\epsilon)$ space per\nmachine for any constant $\\epsilon \\in (0, 1)$ and using $\\widetilde{O}(m)$\ntotal space. Therefore, for the wide range of sparse graphs with small\narboricity---such as minor-free graphs, bounded-genus graphs or bounded\ntreewidth graphs---we get an $O(\\log^2 \\log n)$ round algorithm which\nexponentially improves prior algorithms.\n  By known reductions, our results also imply a $(1+\\epsilon)$-approximation of\nmaximum cardinality matching, a $(2+\\epsilon)$-approximation of maximum\nweighted matching, and a 2-approximation of minimum vertex cover with\nessentially the same round complexity and memory requirements.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 23:06:22 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 03:53:29 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 17:46:22 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Derakhshan", "Mahsa", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Karp", "Richard M.", ""]]}, {"id": "1807.07422", "submitter": "Pietro Danzi", "authors": "Pietro Danzi, Anders E. Kal{\\o}r, \\v{C}edomir Stefanovi\\'c, Petar\n  Popovski", "title": "Delay and Communication Tradeoffs for Blockchain Systems with\n  Lightweight IoT Clients", "comments": "This paper has been submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging blockchain protocols provide a decentralized architecture that\nis suitable of supporting Internet of Things (IoT) interactions. However,\nkeeping a local copy of the blockchain ledger is infeasible for low-power and\nmemory-constrained devices. For this reason, they are equipped with lightweight\nsoftware implementations that only download the useful data structures, e.g.\nstate of accounts, from the blockchain network, when they are updated. In this\npaper, we consider and analyze a novel scheme, implemented by the nodes of the\nblockchain network, which aggregates the blockchain data in periodic updates\nand further reduces the communication cost of the connected IoT devices. We\nshow that the aggregation period should be selected based on the channel\nquality, the offered rate, and the statistics of updates of the useful data\nstructures. The results, obtained for the Ethereum protocol, illustrate the\nbenefits of the aggregation scheme in terms of a reduced duty cycle of the\ndevice, particularly for low signal-to-noise ratios, and the overall reduction\nof the amount of information transmitted in downlink (e.g., from the wireless\nbase station to the IoT device). A potential application of the proposed scheme\nis to let the IoT device request more information than actually needed, hence\nincreasing its privacy, while keeping the communication cost constant. In\nconclusion, our work is the first to provide rigorous guidelines for the design\nof lightweight blockchain protocols with wireless connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 13:47:03 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 19:56:43 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 07:19:31 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Danzi", "Pietro", ""], ["Kal\u00f8r", "Anders E.", ""], ["Stefanovi\u0107", "\u010cedomir", ""], ["Popovski", "Petar", ""]]}, {"id": "1807.07487", "submitter": "Yu Chen", "authors": "Deeraj Nagothu, Ronghua Xu, Seyed Yahya Nikouei, Yu Chen", "title": "A Microservice-enabled Architecture for Smart Surveillance using\n  Blockchain Technology", "comments": "Submitted as a position paper to the 1st International Workshop on\n  BLockchain Enabled Sustainable Smart Cities (BLESS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the smart surveillance system enhanced by the Internet of Things (IoT)\ntechnology becomes an essential part of Smart Cities, it also brings new\nconcerns in security of the data. Compared to the traditional surveillance\nsystems that is built following a monolithic architecture to carry out lower\nlevel operations, such as monitoring and recording, the modern surveillance\nsystems are expected to support more scalable and decentralized solutions for\nadvanced video stream analysis at the large volumes of distributed edge\ndevices. In addition, the centralized architecture of the conventional\nsurveillance systems is vulnerable to single point of failure and privacy\nbreach owning to the lack of protection to the surveillance feed. This position\npaper introduces a novel secure smart surveillance system based on\nmicroservices architecture and blockchain technology. Encapsulating the video\nanalysis algorithms as various independent microservices not only isolates the\nvideo feed from different sectors, but also improve the system availability and\nrobustness by decentralizing the operations. The blockchain technology securely\nsynchronizes the video analysis databases among microservices across\nsurveillance domains, and provides tamper proof of data in the trustless\nnetwork environment. Smart contract enabled access authorization strategy\nprevents any unauthorized user from accessing the microservices and offers a\nscalable, decentralized and fine-grained access control solution for smart\nsurveillance systems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 15:19:38 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Nagothu", "Deeraj", ""], ["Xu", "Ronghua", ""], ["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""]]}, {"id": "1807.07724", "submitter": "Ben Blamey", "authors": "Ben Blamey and Andreas Hellander and Salman Toor", "title": "Apache Spark Streaming, Kafka and HarmonicIO: A Performance Benchmark\n  and Architecture Comparison for Enterprise and Scientific Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a benchmark of stream processing throughput comparing\nApache Spark Streaming (under file-, TCP socket- and Kafka-based stream\nintegration), with a prototype P2P stream processing framework, HarmonicIO.\nMaximum throughput for a spectrum of stream processing loads are measured,\nspecifically, those with large message sizes (up to 10MB), and heavy CPU loads\n-- more typical of scientific computing use cases (such as microscopy), than\nenterprise contexts. A detailed exploration of the performance characteristics\nwith these streaming sources, under varying loads, reveals an interplay of\nperformance trade-offs, uncovering the boundaries of good performance for each\nframework and streaming source integration. We compare with theoretic bounds in\neach case. Based on these results, we suggest which frameworks and streaming\nsources are likely to offer good performance for a given load. Broadly, the\nadvantages of Spark's rich feature set comes at a cost of sensitivity to\nmessage size in particular -- common stream source integrations can perform\npoorly in the 1MB-10MB range. The simplicity of HarmonicIO offers more robust\nperformance in this region, especially for raw CPU utilization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 07:39:07 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 14:41:42 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 13:03:42 GMT"}, {"version": "v4", "created": "Thu, 19 Dec 2019 09:35:06 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Blamey", "Ben", ""], ["Hellander", "Andreas", ""], ["Toor", "Salman", ""]]}, {"id": "1807.07801", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts", "title": "Finding Structure in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is the first part of the author's habilitation thesis (HDR),\ndefended on June 4, 2018 at the University of Bordeaux. Given the nature of\nthis document, the contributions that involve the author have been emphasized;\nhowever, these four chapters were specifically written for distribution to a\nlarger audience. We hope they can serve as a broad introduction to the domain\nof highly dynamic networks, with a focus on temporal graph concepts and their\ninteraction with distributed computing.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 11:57:37 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Casteigts", "Arnaud", ""]]}, {"id": "1807.07814", "submitter": "Albert Reuther PhD", "authors": "Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William\n  Arcand, David Bestor, Bill Bergeron, Vijay Gadepally, Michael Houle, Matthew\n  Hubbell, Michael Jones, Anna Klein, Lauren Milechin, Julia Mullen, Andrew\n  Prout, Antonio Rosa, Charles Yee, Peter Michaleas", "title": "Interactive Supercomputing on 40,000 Cores for Machine Learning and Data\n  Analysis", "comments": "6 pages, 7 figures, IEEE High Performance Extreme Computing\n  Conference 2018", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547629", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive massively parallel computations are critical for machine learning\nand data analysis. These computations are a staple of the MIT Lincoln\nLaboratory Supercomputing Center (LLSC) and has required the LLSC to develop\nunique interactive supercomputing capabilities. Scaling interactive machine\nlearning frameworks, such as TensorFlow, and data analysis environments, such\nas MATLAB/Octave, to tens of thousands of cores presents many technical\nchallenges - in particular, rapidly dispatching many tasks through a scheduler,\nsuch as Slurm, and starting many instances of applications with thousands of\ndependencies. Careful tuning of launches and prepositioning of applications\novercome these challenges and allow the launching of thousands of tasks in\nseconds on a 40,000-core supercomputer. Specifically, this work demonstrates\nlaunching 32,000 TensorFlow processes in 4 seconds and launching 262,000 Octave\nprocesses in 40 seconds. These capabilities allow researchers to rapidly\nexplore novel machine learning architecture and data analysis algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 12:42:40 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""], ["Byun", "Chansup", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julia", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Michaleas", "Peter", ""]]}, {"id": "1807.07901", "submitter": "Chryssis Georgiou", "authors": "Chryssis Georgiou, Robert Gustafsson, Andreas Lindhe, and Elad M.\n  Schiller", "title": "Self-stabilization Overhead: an Experimental Case Study on Coded Atomic\n  Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared memory emulation can be used as a fault-tolerant and highly available\ndistributed storage solution or as a low-level synchronization primitive.\nAttiya, Bar-Noy, and Dolev were the first to propose a single-writer,\nmulti-reader linearizable register emulation where the register is replicated\nto all servers. Recently, Cadambe et al. proposed the Coded Atomic Storage\n(CAS) algorithm, which uses erasure coding for achieving data redundancy with\nmuch lower communication cost than previous algorithmic solutions.\n  Although CAS can tolerate server crashes, it was not designed to recover from\nunexpected, transient faults, without the need of external (human)\nintervention. In this respect, Dolev, Petig, and Schiller have recently\ndeveloped a self-stabilizing version of CAS, which we call CASSS. As one would\nexpect, self-stabilization does not come as a free lunch; it introduces,\nmainly, communication overhead for detecting inconsistencies and stale\ninformation. So, one would wonder whether the overhead introduced by\nself-stabilization would nullify the gain of erasure coding.\n  To answer this question, we have implemented and experimentally evaluated the\nCASSS algorithm on PlanetLab; a planetary scale distributed infrastructure. The\nevaluation shows that our implementation of CASSS scales very well in terms of\nthe number of servers, the number of concurrent clients, as well as the size of\nthe replicated object. More importantly, it shows (a) to have only a constant\noverhead compared to the traditional CAS algorithm (which we also implement)\nand (b) the recovery period (after the last occurrence of a transient fault) is\nas fast as a few client (read/write) operations. Our results suggest that CASSS\ndoes not significantly impact efficiency while dealing with automatic recovery\nfrom transient faults and bounded size of needed resources.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 15:38:53 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 16:28:18 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Georgiou", "Chryssis", ""], ["Gustafsson", "Robert", ""], ["Lindhe", "Andreas", ""], ["Schiller", "Elad M.", ""]]}, {"id": "1807.07928", "submitter": "Vivienne Sze", "authors": "Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, Vivienne Sze", "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on\n  Mobile Devices", "comments": "accepted for publication in IEEE Journal on Emerging and Selected\n  Topics in Circuits and Systems. This extended version on arXiv also includes\n  Eyexam in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent trend in DNN development is to extend the reach of deep learning\napplications to platforms that are more resource and energy constrained, e.g.,\nmobile devices. These endeavors aim to reduce the DNN model size and improve\nthe hardware processing efficiency, and have resulted in DNNs that are much\nmore compact in their structures and/or have high data sparsity. These compact\nor sparse models are different from the traditional large ones in that there is\nmuch more variation in their layer shapes and sizes, and often require\nspecialized hardware to exploit sparsity for performance improvement. Thus,\nmany DNN accelerators designed for large DNNs do not perform well on these\nmodels. In this work, we present Eyeriss v2, a DNN accelerator architecture\ndesigned for running compact and sparse DNNs. To deal with the widely varying\nlayer shapes and sizes, it introduces a highly flexible on-chip network, called\nhierarchical mesh, that can adapt to the different amounts of data reuse and\nbandwidth requirements of different data types, which improves the utilization\nof the computation resources. Furthermore, Eyeriss v2 can process sparse data\ndirectly in the compressed domain for both weights and activations, and\ntherefore is able to improve both processing speed and energy efficiency with\nsparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65nm CMOS\nprocess achieves a throughput of 1470.6 inferences/sec and 2560.3 inferences/J\nat a batch size of 1, which is 12.6x faster and 2.5x more energy efficient than\nthe original Eyeriss running MobileNet. We also present an analysis methodology\ncalled Eyexam that provides a systematic way of understanding the performance\nlimits for DNN processors as a function of specific characteristics of the DNN\nmodel and accelerator design; it applies these characteristics as sequential\nsteps to increasingly tighten the bound on the performance limits.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 16:12:07 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 19:56:26 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chen", "Yu-Hsin", ""], ["Yang", "Tien-Ju", ""], ["Emer", "Joel", ""], ["Sze", "Vivienne", ""]]}, {"id": "1807.08099", "submitter": "Adel Nadjaran Toosi", "authors": "Ehsan Nadjaran Toosi and Adel Nadjaran Toosi and Reza Godaz and\n  Rajkumar Buyya", "title": "Integrated IoT and Cloud Environment for Fingerprint Recognition", "comments": "8 pages, 5 figures, Proceedings of the International Conference on\n  Fog Computing and Internet of Things (ICFCIOT 2017), Hyderabad, India,\n  December 21-22, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data applications involving the analysis of large datasets becomes a\ncritical part of many emerging paradigms such as smart cities, social networks\nand modern security systems. Cloud computing has developed as a mainstream for\nhosting big data applications by its ability to provide the illusion of\ninfinite resources. However, harnessing cloud resources for large-scale big\ndata computation is application specific to a large extent. In this paper, we\npropose a system for large-scale fingerprint matching application using Aneka,\na platform or developing scalable applications on the Cloud. We present the\ndesign and implementation of our proposed system and conduct experiments to\nevaluate its performance using resources from Microsoft Azure. Experimental\nresults demonstrate that matching time for biometric information such as\nfingerprints in large-scale databases can be reduced substantially using our\nproposed system.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 07:20:21 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Toosi", "Ehsan Nadjaran", ""], ["Toosi", "Adel Nadjaran", ""], ["Godaz", "Reza", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1807.08267", "submitter": "Florin Stoica", "authors": "Florin Stoica, Laura Florentina Stoica", "title": "Generating an ATL Model Checker using an Attribute Grammar", "comments": "18 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use attribute grammars as a formal approach for model\ncheckers development. Our aim is to design an ATL (Alternating-Time Temporal\nLogic) model checker from a context-free grammar which generates the language\nof the ATL formulas. An attribute grammar may be informally defined as a\ncontext-free grammar which is extended with a set of attributes and a\ncollection of semantic rules. We use an ATL attribute grammar for specifying an\noperational semantics of the language of the ATL formulas by defining a\ntranslation into the language which describes the set of states from the ATL\nmodel where the corresponding ATL formulas are satisfied. We provide a formal\ndefinition for an attribute grammar used as input for Another Tool for Language\nRecognition (ANTLR) to generate an ATL model checker. Also, the technique of\nimplementing the semantic actions in ANTLR is presented, which is the concept\nof connection between attribute evaluation in the grammar that generates the\nlanguage of ATL formulas and algebraic compiler implementation that represents\nthe ATL model checker. The original implementation of the model checking\nalgorithm is based on Relational Databases and Web Services. Several database\nsystems and Web Services technologies were used for evaluating the system\nperformance in verification of large ATL models.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 10:18:12 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:54:54 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Stoica", "Florin", ""], ["Stoica", "Laura Florentina", ""]]}, {"id": "1807.08549", "submitter": "Mark Burgess", "authors": "Paul Borrill, Mark Burgess, Alan Karp, and Atsushi Kasuya", "title": "Spacetime-Entangled Networks (I) Relativity and Observability of\n  Stepwise Consensus", "comments": "Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus protocols can be an effective tool for synchronizing small amounts\nof data over small regions. We describe the concept and implementation of\nentangled links, applied to data transmission, using the framework of Promise\nTheory as a tool to help bring certainty to distributed consensus. Entanglement\ndescribes co-dependent evolution of state. Networks formed by entanglement of\nagents keep certain promises: they deliver sequential messages, end-to-end, in\norder, and with atomic confirmation of delivery to both ends of the link. These\nproperties can be used recursively to assure a hierarchy of conditional\npromises at any scale. This is a useful property where a consensus of state or\n`common knowledge' is required. We intentionally straddle theory and\nimplementation in this discussion.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 11:59:53 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 07:36:31 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Borrill", "Paul", ""], ["Burgess", "Mark", ""], ["Karp", "Alan", ""], ["Kasuya", "Atsushi", ""]]}, {"id": "1807.08654", "submitter": "Evan Bollig", "authors": "Evan F. Bollig, James C. Wilgenbusch", "title": "From Bare Metal to Virtual: Lessons Learned when a Supercomputing\n  Institute Deploys its First Cloud", "comments": "8 pages, 5 figures, PEARC '18: Practice and Experience in Advanced\n  Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": null, "doi": "10.1145/3219104.3219164", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As primary provider for research computing services at the University of\nMinnesota, the Minnesota Supercomputing Institute (MSI) has long been\nresponsible for serving the needs of a user-base numbering in the thousands.\n  In recent years, MSI---like many other HPC centers---has observed a growing\nneed for self-service, on-demand, data-intensive research, as well as the\nemergence of many new controlled-access datasets for research purposes. In\nlight of this, MSI constructed a new on-premise cloud service, named Stratus,\nwhich is architected from the ground up to easily satisfy data-use agreements\nand fill four gaps left by traditional HPC. The resulting OpenStack cloud,\nconstructed from HPC-specific compute nodes and backed by Ceph storage, is\ndesigned to fully comply with controls set forth by the NIH Genomic Data\nSharing Policy.\n  Herein, we present twelve lessons learned during the ambitious sprint to take\nStratus from inception and into production in less than 18 months. Important,\nand often overlooked, components of this timeline included the development of\nnew leadership roles, staff and user training, and user support documentation.\nAlong the way, the lessons learned extended well beyond the technical\nchallenges often associated with acquiring, configuring, and maintaining\nlarge-scale systems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 14:54:53 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Bollig", "Evan F.", ""], ["Wilgenbusch", "James C.", ""]]}, {"id": "1807.08657", "submitter": "Evan Bollig", "authors": "Evan F. Bollig, Graham T. Allan, Benjamin J. Lynch, Yectli A. Huerta,\n  Mathew Mix, Edward A. Munsell, Raychel M. Benson, Brent Swartz", "title": "Leveraging OpenStack and Ceph for a Controlled-Access Data Cloud", "comments": "7 pages, 5 figures, PEARC '18: Practice and Experience in Advanced\n  Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": null, "doi": "10.1145/3219104.3219165", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While traditional HPC has and continues to satisfy most workflows, a new\ngeneration of researchers has emerged looking for sophisticated, scalable,\non-demand, and self-service control of compute infrastructure in a cloud-like\nenvironment. Many also seek safe harbors to operate on or store sensitive\nand/or controlled-access data in a high capacity environment.\n  To cater to these modern users, the Minnesota Supercomputing Institute\ndesigned and deployed Stratus, a locally-hosted cloud environment powered by\nthe OpenStack platform, and backed by Ceph storage. The subscription-based\nservice complements existing HPC systems by satisfying the following unmet\nneeds of our users: a) on-demand availability of compute resources, b)\nlong-running jobs (i.e., $> 30$ days), c) container-based computing with\nDocker, and d) adequate security controls to comply with controlled-access data\nrequirements.\n  This document provides an in-depth look at the design of Stratus with respect\nto security and compliance with the NIH's controlled-access data policy.\nEmphasis is placed on lessons learned while integrating OpenStack and Ceph\nfeatures into a so-called \"walled garden\", and how those technologies\ninfluenced the security design. Many features of Stratus, including tiered\nsecure storage with the introduction of a controlled-access data \"cache\",\nfault-tolerant live-migrations, and fully integrated two-factor authentication,\ndepend on recent OpenStack and Ceph features.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 15:01:25 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Bollig", "Evan F.", ""], ["Allan", "Graham T.", ""], ["Lynch", "Benjamin J.", ""], ["Huerta", "Yectli A.", ""], ["Mix", "Mathew", ""], ["Munsell", "Edward A.", ""], ["Benson", "Raychel M.", ""], ["Swartz", "Brent", ""]]}, {"id": "1807.08703", "submitter": "Andrew Prout", "authors": "Andrew Prout, William Arcand, David Bestor, Bill Bergeron, Chansup\n  Byun, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna\n  Klein, Peter Michaleas, Lauren Milechin, Julie Mullen, Antonio Rosa,\n  Siddharth Samsi, Charles Yee, Albert Reuther, Jeremy Kepner", "title": "Measuring the Impact of Spectre and Meltdown", "comments": null, "journal-ref": null, "doi": "10.1109/HPEC.2018.8547554", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Spectre and Meltdown flaws in modern microprocessors represent a new\nclass of attacks that have been difficult to mitigate. The mitigations that\nhave been proposed have known performance impacts. The reported magnitude of\nthese impacts varies depending on the industry sector and expected workload\ncharacteristics. In this paper, we measure the performance impact on several\nworkloads relevant to HPC systems. We show that the impact can be significant\non both synthetic and realistic workloads. We also show that the performance\npenalties are difficult to avoid even in dedicated systems where security is a\nlesser concern.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:31:28 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Prout", "Andrew", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1807.08738", "submitter": "Krzysztof Nowicki", "authors": "Krzysztof Nowicki", "title": "Random Sampling Applied to the MST Problem in the Node Congested Clique\n  Model", "comments": "simplified and corrected version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Congested Clique model proposed by Lotker et al.[SICOMP'05] was\nintroduced in order to provide a simple abstraction for overlay networks.\nCongested Clique is a model of distributed (or parallel) computing, in which\nthere are $n$ players with unique identifiers from set [n], which perform\ncomputations in synchronous rounds. Each round consists of the phase of\nunlimited local computation and the communication phase. While communicating,\neach pair of players is allowed to exchange a single message of size $O(\\log\nn)$ bits.\n  Since, in a single round, each player can communicate with even $\\Theta(n)$\nother players, the model seems to be to powerful to imitate bandwidth\nrestriction emerging from the underlying network. In this paper we study a\nrestricted version of the Congested Clique model, the Node Congested Clique\n(NCC) model, proposed by Augustine et al.[arxiv1805], in which a player is\nallowed to send/receive only $O(\\log n)$ messages per communication phase.\n  More precisely, we provide communication primitives that improve the round\ncomplexity of the MST algorithm by Augustine et al. [arxiv1805] to $O(\\log^3\nn)$ rounds, and give an $O(\\log^2 n)$ round algorithm solving the Spanning\nForest (SF) problem. Furthermore, we present an approach based on the random\nsampling technique by Karger et al.[JACM'95] that gives an $O(\\log^2 n \\log\n\\Delta / \\log \\log n)$ round algorithm for the Minimum Spanning Forest (MSF)\nproblem. Besides the faster SF/ MSF algorithms we consider the key\ncontributions to be\n  - an efficient implementation of basic protocols in the NCC model\n  - a tighter analysis of a special case of the sampling approach by Karger et\nal.[JACM'95] and related results by Pemmaraju and Sardeshmukh [FSTTCS'16]\n  - efficient k-sparse recovery data structure that requires $O((k +\\log n)\\log\nn\\log k)$ bits and provides recovery procedure that requires $O((k +\\log n)\\log\nk)$ steps\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 17:35:28 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 14:38:16 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Nowicki", "Krzysztof", ""]]}, {"id": "1807.08745", "submitter": "Krzysztof Onak", "authors": "Krzysztof Onak", "title": "Round Compression for Parallel Graph Algorithms in Strongly Sublinear\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massive Parallel Computation (MPC) model is a theoretical framework for\npopular parallel and distributed platforms such as MapReduce, Hadoop, or Spark.\nWe consider the task of computing a large matching or small vertex cover in\nthis model when the space per machine is $n^\\delta$ for $\\delta \\in (0,1)$,\nwhere $n$ is the number of vertices in the input graph. A direct simulation of\nclassic PRAM and distributed algorithms from the 1980s results in algorithms\nthat require at least a logarithmic number of MPC rounds. We give the first\nalgorithm that breaks this logarithmic barrier and runs in $\\tilde O(\\sqrt{\\log\nn})$ rounds, as long as the total space is at least slightly superlinear in the\nnumber of vertices.\n  The result is obtained by repeatedly compressing several rounds of a natural\npeeling algorithm to a logarithmically smaller number of MPC rounds. Each time\nwe show that it suffices to consider a low-degree subgraph, in which local\nneighborhoods can be explored with exponential speedup. Our techniques are\nrelatively simple and can also be used to accelerate the simulation of\ndistributed algorithms for bounded-degree graphs and finding a maximal\nindependent set in bounded-arboricity graphs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 17:49:03 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Onak", "Krzysztof", ""]]}, {"id": "1807.08824", "submitter": "Udit Agarwal", "authors": "Udit Agarwal and Vijaya Ramachandran", "title": "A Deterministic Distributed Algorithm for Weighted All Pairs Shortest\n  Paths Through Pipelining", "comments": "Minor correction in Citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new pipelined approach to compute all pairs shortest paths\n(APSP) in a directed graph with nonnegative integer edge weights (including\nzero weights) in the CONGEST model in the distributed setting. Our\ndeterministic distributed algorithm computes shortest paths of distance at most\n$\\Delta$ for all pairs of vertices in at most $2 n \\sqrt{\\Delta} + 2n$ rounds,\nand more generally, it computes h-hop shortest paths for k sources in\n$2\\sqrt{nkh} + n + k$ rounds. The algorithm is simple, and it has some novel\nfeatures and a nontrivial analysis.It uses only the directed edges in the graph\nfor communication. This algorithm can be used as a base within asymptotically\nfaster algorithms that match or improve on the current best deterministic bound\nof $\\tilde{O}(n^{3/2})$ rounds for this problem when edge weights are $O(n)$ or\nshortest path distances are $\\tilde{O}(n^{3/2})$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 20:51:42 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 15:48:52 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 20:21:14 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1807.08870", "submitter": "Jeferson Santiago da Silva", "authors": "Jeferson Santiago da Silva, Thibaut Stimpfling, Thomas Luinaud, Bachir\n  Fradj and Bochra Boughzala", "title": "One for All, All for One: A Heterogeneous Data Plane for Flexible P4\n  Processing", "comments": "Poster accepted for publication at the 1st P4 European Workshop\n  (P4EU), Cambridge, UK, September 24, 2018", "journal-ref": null, "doi": "10.1109/ICNP.2018.00063", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The P4 community has recently put significant effort to increase the\ndiversity of targets on which P4 programs can be implemented. These include\nfixed function and programmable ASICs, FPGAs, NICs, and CPUs. However, P4\nprograms are written according to the set of functionalities supported by the\ntarget for which they are compiled. For instance, a P4 program targeting a\nprogrammable ASIC cannot be extended with user-defined processing modules,\nwhich limits the flexibility and the abstraction of P4 programs.\n  To address these shortcomings, we propose a heterogeneous P4 programmable\ndata plane comprised of different targets that together appear as a single\nlogical unit. The proposed data plane broadens the range of functionalities\navailable to P4 programmers by combining the strength of each target. We\ndemonstrate the feasibility of the proposed P4 data plane by coupling an FPGA\nwith a soft switch which emulates a programmable ASIC. The proposed data plane\nis demonstrated with the implementation of a simplified L2 switch. The emulated\nASIC match-table capacity is extended by the FPGA by an order of magnitude.The\nFPGA also integrates a proprietary module using a P4 extern.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 01:03:45 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["da Silva", "Jeferson Santiago", ""], ["Stimpfling", "Thibaut", ""], ["Luinaud", "Thomas", ""], ["Fradj", "Bachir", ""], ["Boughzala", "Bochra", ""]]}, {"id": "1807.08887", "submitter": "Minjie Wang", "authors": "Minjie Wang, Chien-chin Huang, Jinyang Li", "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning", "comments": "Revision for Eurosys'19", "journal-ref": null, "doi": "10.1145/3302424.3303953", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Tofu, a system that partitions very large DNN models\nacross multiple GPU devices to reduce per-GPU memory footprint. Tofu is\ndesigned to partition a dataflow graph of fine-grained tensor operators in\norder to work transparently with a general-purpose deep learning platform like\nMXNet. In order to automatically partition each operator, we propose to\ndescribe the semantics of an operator in a simple language which represents\ntensors as lambda functions mapping from tensor coordinates to values. To\noptimally partition different operators in a dataflow graph, Tofu uses a\nrecursive search algorithm that minimizes the total communication cost. Our\nexperiments on an 8-GPU machine show that Tofu enables the training of very\nlarge CNN and RNN models. It also achieves 25% - 400% speedup over alternative\napproaches to train very large models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 02:57:28 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 23:59:26 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Wang", "Minjie", ""], ["Huang", "Chien-chin", ""], ["Li", "Jinyang", ""]]}, {"id": "1807.09161", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Renato L. de F. Cunha, Eduardo R. Rodrigues, Matheus Palhares Viana,\n  Dario Augusto Borges Oliveira", "title": "An argument in favor of strong scaling for deep neural networks with\n  small datasets", "comments": "8 pages, 5 figures, Presented at HPML 2018 -\n  http://hpml2018.github.io/", "journal-ref": null, "doi": "10.1109/CAHPC.2018.8645881", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the popularization of deep learning frameworks and\nlarge datasets, researchers have started parallelizing their models in order to\ntrain faster. This is crucially important, because they typically explore many\nhyperparameters in order to find the best ones for their applications. This\nprocess is time consuming and, consequently, speeding up training improves\nproductivity. One approach to parallelize deep learning models followed by many\nresearchers is based on weak scaling. The minibatches increase in size as new\nGPUs are added to the system. In addition, new learning rates schedules have\nbeen proposed to fix optimization issues that occur with large minibatch sizes.\nIn this paper, however, we show that the recommendations provided by recent\nwork do not apply to models that lack large datasets. In fact, we argument in\nfavor of using strong scaling for achieving reliable performance in such cases.\nWe evaluated our approach with up to 32 GPUs and show that weak scaling not\nonly does not have the same accuracy as the sequential model, it also fails to\nconverge most of time. Meanwhile, strong scaling has good scalability while\nhaving exactly the same accuracy of a sequential implementation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:48:19 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 12:23:39 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 22:59:03 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Cunha", "Renato L. de F.", ""], ["Rodrigues", "Eduardo R.", ""], ["Viana", "Matheus Palhares", ""], ["Oliveira", "Dario Augusto Borges", ""]]}, {"id": "1807.09220", "submitter": "Rui Tian", "authors": "Rui Tian", "title": "Detecting gasoline in small public places based on wireless sensor\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fire accidents often cause unpredictable catastrophic losses. At present,\nexisting fire prevention measures in public places are mostly based on the\nemergency treatments after the fire, which have limited protection capability\nwhen the fire spreads rapidly, especially for the flammable liquid explosion\naccident. Based on the gas sensor network, this paper proposes a detection\nframework as well as detail technologies to detect flammable liquid existing in\nsmall spaces. We propose to use sensor network to detect the flammable liquids\nthrough monitoring the concentrations of the target liquid vapor diffused in\nthe air. Experiment results show that, the proposed surveillant system can\ndetect the gasoline components in small space with high sensitivity while\nmaintaining very low false detection rates to external interferences.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 06:08:38 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Tian", "Rui", ""]]}, {"id": "1807.09250", "submitter": "Hadi Mardani Kamali", "authors": "Hadi Mardani Kamali", "title": "Using Multi-Core HW/SW Co-design Architecture for Accelerating K-means\n  Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capability of classifying and clustering a desired set of data is an\nessential part of building knowledge from data. However, as the size and\ndimensionality of input data increases, the run-time for such clustering\nalgorithms is expected to grow superlinearly, making it a big challenge when\ndealing with BigData. K-mean clustering is an essential tool for many big data\napplications including data mining, predictive analysis, forecasting studies,\nand machine learning. However, due to large size (volume) of Big-Data, and\nlarge dimensionality of its data points, even the application of a simple\nk-mean clustering may become extremely time and resource demanding. Specially\nwhen it is necessary to have a fast and modular dataset analysis flow. In this\npaper, we demonstrate that using a two-level filtering algorithm based on\nbinary kd-tree structure is able to decrease the time of convergence in K-means\nalgorithm for large datasets. The two-level filtering algorithm based on binary\nkd-tree structure evolves the SW to naturally divide the classification into\nsmaller data sets, based on the number of available cores and size of logic\navailable in a target FPGA. The empirical result on this two-level structure\nover multi-core FPGA-based architecture provides 330X speed-up compared to a\nconventional software-only solution.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 17:38:42 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Kamali", "Hadi Mardani", ""]]}, {"id": "1807.09524", "submitter": "Lukas Gianinazzi", "authors": "Barbara Geissmann, Lukas Gianinazzi", "title": "Parallel Minimum Cuts in Near-linear Work and Low Depth", "comments": null, "journal-ref": "SPAA '18: 30th ACM Symposium on Parallelism in Algorithms and\n  Architectures, July 16-18, 2018, Vienna, Austria. ACM, New York, NY, USA", "doi": "10.1145/3210377.3210393", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first near-linear work and poly-logarithmic depth algorithm\nfor computing a minimum cut in a graph, while previous parallel algorithms with\npoly-logarithmic depth required at least quadratic work in the number of\nvertices. In a graph with $n$ vertices and $m$ edges, our algorithm computes\nthe correct result with high probability in $O(m {\\log}^4 n)$ work and\n$O({\\log}^3 n)$ depth. This result is obtained by parallelizing a data\nstructure that aggregates weights along paths in a tree and by exploiting the\nconnection between minimum cuts and approximate maximum packings of spanning\ntrees. In addition, our algorithm improves upon bounds on the number of cache\nmisses incurred to compute a minimum cut.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 10:48:03 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 12:51:01 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 15:29:49 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Geissmann", "Barbara", ""], ["Gianinazzi", "Lukas", ""]]}, {"id": "1807.09632", "submitter": "Eduardo Ponce Mojica", "authors": "Eduardo Ponce, Brittany Stephenson, Suzanne Lenhart, Judy Day, Gregory\n  D. Peterson", "title": "PaPaS: A Portable, Lightweight, and Generic Framework for Parallel\n  Parameter Studies", "comments": "8 pages, 6 figures, PEARC '18: Practice and Experience in Advanced\n  Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": null, "doi": "10.1145/3219104.3229289", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current landscape of scientific research is widely based on modeling and\nsimulation, typically with complexity in the simulation's flow of execution and\nparameterization properties. Execution flows are not necessarily\nstraightforward since they may need multiple processing tasks and iterations.\nFurthermore, parameter and performance studies are common approaches used to\ncharacterize a simulation, often requiring traversal of a large parameter\nspace. High-performance computers offer practical resources at the expense of\nusers handling the setup, submission, and management of jobs. This work\npresents the design of PaPaS, a portable, lightweight, and generic workflow\nframework for conducting parallel parameter and performance studies. Workflows\nare defined using parameter files based on keyword-value pairs syntax, thus\nremoving from the user the overhead of creating complex scripts to manage the\nworkflow. A parameter set consists of any combination of environment variables,\nfiles, partial file contents, and command line arguments. PaPaS is being\ndeveloped in Python 3 with support for distributed parallelization using SSH,\nbatch systems, and C++ MPI. The PaPaS framework will run as user processes, and\ncan be used in single/multi-node and multi-tenant computing systems. An example\nsimulation using the BehaviorSpace tool from NetLogo and a matrix multiply\nusing OpenMP are presented as parameter and performance studies, respectively.\nThe results demonstrate that the PaPaS framework offers a simple method for\ndefining and managing parameter studies, while increasing resource utilization.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 14:36:35 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Ponce", "Eduardo", ""], ["Stephenson", "Brittany", ""], ["Lenhart", "Suzanne", ""], ["Day", "Judy", ""], ["Peterson", "Gregory D.", ""]]}, {"id": "1807.09651", "submitter": "Pradeep Subedi", "authors": "Pradeep Subedi, Philip E. Davis, J. J. Villalobos, Ivan Rodero, and\n  Manish Parashar", "title": "Using Intel Optane Devices for In-situ Data Staging in HPC Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging non-volatile memory technologies (NVRAM) offer alternatives to hard\ndrives that are persistent, while providing similar latencies to DRAM. Intel\nrecently released the Optane drive, which features 3D XPoint memory technology.\nThis device can be deployed as an SSD or as persistent memory. In this paper,\nwe provide a performance comparison between Optane (SSD DC4800X) and NVMe (SSD\nDC3700) drives as block devices. We study the performance from two\nperspectives: 1) Benchmarking of drives using FIO workloads, and 2) Assessing\nthe impact of using Optane over NVMe within the DataSpaces framework for\nin-memory data staging to support in-situ scientific workflows.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 23:04:16 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Subedi", "Pradeep", ""], ["Davis", "Philip E.", ""], ["Villalobos", "J. J.", ""], ["Rodero", "Ivan", ""], ["Parashar", "Manish", ""]]}, {"id": "1807.09667", "submitter": "Linpeng Tang", "authors": "Linpeng Tang, Yida Wang, Theodore L. Willke, Kai Li", "title": "Scheduling Computation Graphs of Deep Learning Models on Manycore CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a deep learning model, efficient execution of its computation graph is\nkey to achieving high performance. Previous work has focused on improving the\nperformance for individual nodes of the computation graph, while ignoring the\nparallelization of the graph as a whole. However, we observe that running\nmultiple operations simultaneously without interference is critical to\nefficiently perform parallelizable small operations. The attempt of executing\nthe computation graph in parallel in deep learning frameworks usually involves\nmuch resource contention among concurrent operations, leading to inferior\nperformance on manycore CPUs. To address these issues, in this paper, we\npropose Graphi, a generic and high-performance execution engine to efficiently\nexecute a computation graph in parallel on manycore CPUs. Specifically, Graphi\nminimizes the interference on both software/hardware resources, discovers the\nbest parallel setting with a profiler, and further optimizes graph execution\nwith the critical-path first scheduling. Our experiments show that the parallel\nexecution consistently outperforms the sequential one. The training times on\nfour different neural networks with Graphi are 2.1x to 9.5x faster than those\nwith TensorFlow on a 68-core Intel Xeon Phi processor.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:28:38 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Tang", "Linpeng", ""], ["Wang", "Yida", ""], ["Willke", "Theodore L.", ""], ["Li", "Kai", ""]]}, {"id": "1807.09686", "submitter": "Tom Morgan", "authors": "Michael Mitzenmacher, Tom Morgan", "title": "Directory Reconciliation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the theoretical study of directory reconciliation, a\ngeneralization of document exchange, in which Alice and Bob each have different\nversions of a set of documents that they wish to synchronize. This problem is\ndesigned to capture the setting of synchronizing different versions of file\ndirectories, while allowing for changes of file names and locations without\nsignificant expense. We present protocols for efficiently solving directory\nreconciliation based on a reduction to document exchange under edit distance\nwith block moves, as well as protocols combining techniques for reconciling\nsets of sets with document exchange protocols. Along the way, we develop a new\nprotocol for document exchange under edit distance with block moves inspired by\nnoisy binary search in graphs, which uses only $O(k \\log n)$ bits of\ncommunication at the expense of $O(k \\log n)$ rounds of communication.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:04:01 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Morgan", "Tom", ""]]}, {"id": "1807.09694", "submitter": "Tom Morgan", "authors": "Michael Mitzenmacher, Tom Morgan", "title": "Robust Set Reconciliation via Locality Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider variations of set reconciliation problems where two parties,\nAlice and Bob, each hold a set of points in a metric space, and the goal is for\nBob to conclude with a set of points that is close to Alice's set of points in\na well-defined way. This setting has been referred to as robust set\nreconciliation. More specifically, in one variation we examine the goal is for\nBob to end with a set of points that is close to Alice's in earth mover's\ndistance, and in another the goal is for Bob to have a point that is close to\neach of Alice's. The first problem has been studied before; our results scale\nbetter with the dimension of the space. The second problem appears new.\n  Our primary novelty is utilizing Invertible Bloom Lookup Tables in\ncombination with locality sensitive hashing. This combination allows us to cope\nwith the geometric setting in a communication-efficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:13:48 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Morgan", "Tom", ""]]}, {"id": "1807.09929", "submitter": "Michael Milligan", "authors": "Michael Milligan", "title": "Jupyter as Common Technology Platform for Interactive HPC Services", "comments": "6 pages, 2 figures, in PEARC '18: Proceedings of Practice and\n  Experience in Advanced Research Computing, July 22--26, 2018, Pittsburgh, PA,\n  USA", "journal-ref": null, "doi": "10.1145/3219104.3219162", "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minnesota Supercomputing Institute has implemented Jupyterhub and the\nJupyter notebook server as a general-purpose point-of-entry to interactive high\nperformance computing services. This mode of operation runs counter to\ntraditional job-oriented HPC operations, but offers significant advantages for\nease-of-use, data exploration, prototyping, and workflow development. From the\nuser perspective, these features bring the computing cluster nearer to parity\nwith emerging cloud computing options. On the other hand, retreating from\nfully-scheduled, job-based resource allocation poses challenges for resource\navailability and utilization efficiency, and can involve tools and technologies\noutside the typical core competencies of a supercomputing center's operations\nstaff. MSI has attempted to mitigate these challenges by adopting Jupyter as a\ncommon technology platform for interactive services, capable of providing\ncommand-line, graphical, and workflow-oriented access to HPC resources while\nstill integrating with job scheduling systems and using existing compute\nresources. This paper will describe the mechanisms that MSI has put in place,\nadvantages for research and instructional uses, and lessons learned.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 02:43:27 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Milligan", "Michael", ""]]}, {"id": "1807.10056", "submitter": "Alessio Netti", "authors": "Alessio Netti, Zeynep Kiziltan, Ozalp Babaoglu, Alina Sirbu, Andrea\n  Bartolini and Andrea Borghesi", "title": "FINJ: A Fault Injection Tool for HPC Systems", "comments": "To be presented at the 11th Resilience Workshop in the 2018 Euro-Par\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FINJ, a high-level fault injection tool for High-Performance\nComputing (HPC) systems, with a focus on the management of complex experiments.\nFINJ provides support for custom workloads and allows generation of anomalous\nconditions through the use of fault-triggering executable programs. FINJ can\nalso be integrated seamlessly with most other lower-level fault injection\ntools, allowing users to create and monitor a variety of highly-complex and\ndiverse fault conditions in HPC systems that would be difficult to recreate in\npractice. FINJ is suitable for experiments involving many, potentially\ninteracting nodes, making it a very versatile design and evaluation tool.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 10:32:05 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 14:02:09 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Netti", "Alessio", ""], ["Kiziltan", "Zeynep", ""], ["Babaoglu", "Ozalp", ""], ["Sirbu", "Alina", ""], ["Bartolini", "Andrea", ""], ["Borghesi", "Andrea", ""]]}, {"id": "1807.10461", "submitter": "Nicolas Gastineau", "authors": "Nicolas Gastineau (Le2i), Wahabou Abdou (UBFC), Nader Mbarek (LaBRI),\n  Olivier Togni (Le2i)", "title": "Distributed leader election and computation of local identifiers for\n  programmable matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The context of this paper is programmable matter, which consists of a set of\ncomputational elements, called particles, in an infinite graph. The considered\ninfinite graphs are the square, triangular and king grids. Each particle\noccupies one vertex, can communicate with the adjacent particles, has the same\nclockwise direction and knows the local positions of neighborhood particles.\nUnder these assumptions, we describe a new leader election algorithm affecting\na variable to the particles, called the k-local identifier, in such a way that\nparticles at close distance have each a different k-local identifier. For all\nthe presented algorithms, the particles only need a O(1)-memory space.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:20:34 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Gastineau", "Nicolas", "", "Le2i"], ["Abdou", "Wahabou", "", "UBFC"], ["Mbarek", "Nader", "", "LaBRI"], ["Togni", "Olivier", "", "Le2i"]]}, {"id": "1807.10507", "submitter": "Adam Barker", "authors": "Nnamdi Ekwe-Ekwe and Adam Barker", "title": "Location, Location, Location: Exploring Amazon EC2 Spot Instance Pricing\n  Across Geographical Regions - Extended Version", "comments": "Extended version of CCGrid 2018 paper entitled \"Location, Location,\n  Location: Exploring Amazon EC2 Spot Instance Pricing Across Geographical\n  Regions\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is becoming an almost ubiquitous part of the computing\nlandscape. For many companies today, moving their entire infrastructure and\nworkloads to the cloud reduces complexity, time to deployment, and saves money.\nSpot Instances, a subset of Amazon's cloud computing infrastructure (EC2),\nexpands on this. They allow a user to bid on spare compute capacity in Amazon's\ndata centres at heavily discounted prices. If demand was ever to increase such\nthat the user's maximum bid is exceeded, their instance is terminated.\n  In this paper, we conduct one of the first detailed analyses of how location\naffects the overall cost of deployment of a spot instance. We analyse pricing\ndata across all available Amazon Web Services regions for 60 days for a variety\nof spot instance types. We relate the data we find to the overall AWS region as\nwell as to the Availability Zone within that region.\n  We conclude that location does play a critical role in spot instance pricing\nand also that pricing differs depending on the granularity of that location -\nfrom a more coarse-grained AWS region to a more fine-grained Availability Zone\nwithin a region. We relate the pricing differences we find to the price's\nreliability, confirming whether one can be confident in the prices reported and\nsubsequently, in the ensuing bids one makes.\n  We conclude by showing that it is possible to run workloads on Spot Instances\nachieving both a very low risk of termination as well as paying very low\namounts per hour.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 09:35:15 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Ekwe-Ekwe", "Nnamdi", ""], ["Barker", "Adam", ""]]}, {"id": "1807.10727", "submitter": "Michal Wlodarczyk", "authors": "Jakub {\\L}\\k{a}cki, Vahab Mirrokni, Micha{\\l} W{\\l}odarczyk", "title": "Connected Components at Scale via Local Contractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental tool in hierarchical graph clustering, computing connected\ncomponents has been a central problem in large-scale data mining. While many\nknown algorithms have been developed for this problem, they are either not\nscalable in practice or lack strong theoretical guarantees on the parallel\nrunning time, that is, the number of communication rounds. So far, the best\nproven guarantee is $\\Oh(\\log n)$, which matches the running time in the PRAM\nmodel.\n  In this paper, we aim to design a distributed algorithm for this problem that\nworks well in theory and practice. In particular, we present a simple algorithm\nbased on contractions and provide a scalable implementation of it in MapReduce.\nOn the theoretical side, in addition to showing $\\Oh(\\log n)$ convergence for\nall graphs, we prove an $\\Oh(\\log \\log n)$ parallel running time with high\nprobability for a certain class of random graphs. We work in the MPC model that\ncaptures popular parallel computing frameworks, such as MapReduce, Hadoop or\nSpark.\n  On the practical side, we show that our algorithm outperforms the\nstate-of-the-art MapReduce algorithms. To confirm its scalability, we report\nempirical results on graphs with several trillions of edges.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 16:53:45 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["Mirrokni", "Vahab", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1807.10749", "submitter": "Sergio Boixo", "authors": "Igor L. Markov, Aneeqa Fatima, Sergei V. Isakov and Sergio Boixo", "title": "Quantum Supremacy Is Both Closer and Farther than It Appears", "comments": "32 pages, 3 figures, 1. A new section on how to simulate sampling. 2.\n  New comparisons with simulators developed by other groups. Edited for clarity", "journal-ref": "DAC 2020", "doi": null, "report-no": null, "categories": "quant-ph cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As quantum computers improve in the number of qubits and fidelity, the\nquestion of when they surpass state-of-the-art classical computation for a\nwell-defined computational task is attracting much attention. The leading\ncandidate task for this milestone entails sampling from the output distribution\ndefined by a random quantum circuit. We develop a massively-parallel simulation\ntool Rollright that does not require inter-process communication (IPC) or\nproprietary hardware. We also develop two ways to trade circuit fidelity for\ncomputational speedups, so as to match the fidelity of a given quantum computer\n--- a task previously thought impossible. We report massive speedups for the\nsampling task over prior software from Microsoft, IBM, Alibaba and Google, as\nwell as supercomputer and GPU-based simulations. By using publicly available\nGoogle Cloud Computing, we price such simulations and enable comparisons by\ntotal cost across hardware platforms. We simulate approximate sampling from the\noutput of a circuit with 7x8 qubits and depth 1+40+1 by producing one million\nbitstring probabilities with fidelity 0.5%, at an estimated cost of $35184. The\nsimulation costs scale linearly with fidelity, and using this scaling we\nestimate that extending circuit depth to 1+48+1 increases costs to one million\ndollars. Scaling the simulation to 10M bitstring probabilities needed for\nsampling 1M bitstrings helps comparing simulation to quantum computers. We\ndescribe refinements in benchmarks that slow down leading simulators, halving\nthe circuit depth that can be simulated within the same time.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 17:58:05 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 08:02:36 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 22:11:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Markov", "Igor L.", ""], ["Fatima", "Aneeqa", ""], ["Isakov", "Sergei V.", ""], ["Boixo", "Sergio", ""]]}, {"id": "1807.10792", "submitter": "Ioannis Papapanagiotou", "authors": "Ioannis Papapanagiotou and Vinay Chella", "title": "NDBench: Benchmarking Microservices at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software vendors often report performance numbers for the sweet spot or\nrunning on specialized hardware with specific workload parameters and without\nrealistic failures. Accurate benchmarks at the persistence layer are crucial,\nas failures may cause unrecoverable errors such as data loss, inconsistency or\ncorruption. To accurately evaluate data stores and other microservices at\nNetflix, we developed Netflix Data Benchmark (NDBench), a Cloud benchmark tool.\nIt can be deployed in a loosely-coupled fashion with the ability to dynamically\nchange the benchmark parameters at runtime so we can rapidly iterate on\ndifferent tests and failure modes. NDBench offers pluggable patterns and loads,\nsupport for pluggable client APIs, and was designed to run continually. This\ndesign enabled us to test long-running maintenance jobs that may affect the\nperformance, test numerous different systems under adverse conditions, and\nuncover long-term issues like memory leaks or heap pressure.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 18:42:59 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Papapanagiotou", "Ioannis", ""], ["Chella", "Vinay", ""]]}, {"id": "1807.11022", "submitter": "Ahmet Husainov A.", "authors": "Ahmet A. Husainov", "title": "Optimum Depth of the Bounded Pipeline", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is devoted to studying the performance of a computational pipeline,\nthe number of simultaneously executing stages of which at each time is bounded\nfrom above by a fixed number. A look at the restriction as a structural hazard\nmakes it possible to construct an analytical model for calculating the\nprocessing time of a given input data amount. Using this model, led to a\nformula for calculating the optimal depth of a bounded pipeline for a given\nvolume of input data. The formula shows that the optimal depth can get large\nchanges for small changes in the amount of data. To eliminate this disadvantage\nand to obtain a more convenient formula for optimal depth, a pipeline with a\nsingle random hazard is constructed, the mathematical expectation of a random\nvalue of the processing time of which approximates the analytical model of the\nbounded pipeline. In addition, a pipeline with two hazards has been built, the\nanalytical model of which allowed obtaining formulas for calculating the\noptimal depth of a bounded pipeline with restart for a given amount of data. To\ncheck whether the proposed analytical models are consistent with the\nexperiments to calculate the processing time, two methods of computer\nsimulation of bounded pipelines are used, the first of which is constructed as\na multi-threaded application, and the second is based on the theory of free\npartially commutative monoids.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 08:13:23 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Husainov", "Ahmet A.", ""]]}, {"id": "1807.11205", "submitter": "Xianyan Jia", "authors": "Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu\n  Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, Tiegang Chen,\n  Guangxiao Hu, Shaohuai Shi, Xiaowen Chu", "title": "Highly Scalable Deep Learning Training System with Mixed-Precision:\n  Training ImageNet in Four Minutes", "comments": "arXiv admin note: text overlap with arXiv:1803.03383 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronized stochastic gradient descent (SGD) optimizers with data\nparallelism are widely used in training large-scale deep neural networks.\nAlthough using larger mini-batch sizes can improve the system scalability by\nreducing the communication-to-computation ratio, it may hurt the generalization\nability of the models. To this end, we build a highly scalable deep learning\ntraining system for dense GPU clusters with three main contributions: (1) We\npropose a mixed-precision training method that significantly improves the\ntraining throughput of a single GPU without losing accuracy. (2) We propose an\noptimization approach for extremely large mini-batch size (up to 64k) that can\ntrain CNN models on the ImageNet dataset without losing accuracy. (3) We\npropose highly optimized all-reduce algorithms that achieve up to 3x and 11x\nspeedup on AlexNet and ResNet-50 respectively than NCCL-based training on a\ncluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the\nstate-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes\nand achieved 74.9\\% top-1 test accuracy, and another KNL-based system with 2048\nIntel KNLs spent 20 minutes and achieved 75.4\\% accuracy. Our training system\ncan achieve 75.8\\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40\nGPUs. When training AlexNet with 95 epochs, our system can achieve 58.7\\% top-1\ntest accuracy within 4 minutes, which also outperforms all other existing\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 07:40:44 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Jia", "Xianyan", ""], ["Song", "Shutao", ""], ["He", "Wei", ""], ["Wang", "Yangzihao", ""], ["Rong", "Haidong", ""], ["Zhou", "Feihu", ""], ["Xie", "Liqiang", ""], ["Guo", "Zhenyu", ""], ["Yang", "Yuanzhou", ""], ["Yu", "Liwei", ""], ["Chen", "Tiegang", ""], ["Hu", "Guangxiao", ""], ["Shi", "Shaohuai", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1807.11248", "submitter": "Gerard Par\\'is", "authors": "Pedro Garc\\'ia L\\'opez, Marc S\\'anchez-Artigas, Gerard Par\\'is, Daniel\n  Barcelona Pons, \\'Alvaro Ruiz Ollobarren, David Arroyo Pinto", "title": "Comparison of FaaS Orchestration Systems", "comments": "6 pages, 2 figures, title changed, 4th International Workshop on\n  Serverless Computing (UCC Companion 2018)", "journal-ref": null, "doi": "10.1109/UCC-Companion.2018.00049", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the appearance of Amazon Lambda in 2014, all major cloud providers have\nembraced the Function as a Service (FaaS) model, because of its enormous\npotential for a wide variety of applications. As expected (and also desired),\nthe competition is fierce in the serverless world, and includes aspects such as\nthe run-time support for the orchestration of serverless functions. In this\nregard, the three major production services are currently Amazon Step Functions\n(December 2016), Azure Durable Functions (June 2017), and IBM Composer (October\n2017), still young and experimental projects with a long way ahead. In this\narticle, we will compare and analyze these three serverless orchestration\nsystems under a common evaluation framework. We will study their architectures,\nprogramming and billing models, and their effective support for parallel\nexecution, among others. Through a series of experiments, we will also evaluate\nthe run-time overhead of the different infrastructures for different types of\nworkflows.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 09:23:09 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 09:27:57 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["L\u00f3pez", "Pedro Garc\u00eda", ""], ["S\u00e1nchez-Artigas", "Marc", ""], ["Par\u00eds", "Gerard", ""], ["Pons", "Daniel Barcelona", ""], ["Ollobarren", "\u00c1lvaro Ruiz", ""], ["Pinto", "David Arroyo", ""]]}, {"id": "1807.11557", "submitter": "Xiong Zheng", "authors": "Xiong Zheng, Changyong Hu, Vijay K. Garg", "title": "Lattice Agreement in Message Passing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the lattice agreement problem and the generalized lattice\nagreement problem in distributed message passing systems. In the lattice\nagreement problem, given input values from a lattice, processes have to\nnon-trivially decide output values that lie on a chain. We consider the lattice\nagreement problem in both synchronous and asynchronous systems. For synchronous\nlattice agreement, we present two algorithms which run in $\\log f$ and $\\min\n\\{O(\\log^2 h(L)), O(\\log^2 f)\\}$ rounds, respectively, where $h(L)$ denotes the\nheight of the {\\em input sublattice} $L$, $f < n$ is the number of crash\nfailures the system can tolerate, and $n$ is the number of processes in the\nsystem. These algorithms have significant better round complexity than\npreviously known algorithms. The algorithm by Attiya et al.\n\\cite{attiya1995atomic} takes $\\log n$ synchronous rounds, and the algorithm by\nMavronicolasa \\cite{mavronicolasabound} takes $\\min \\{O(h(L)), O(\\sqrt{f})\\}$\nrounds. For asynchronous lattice agreement, we propose an algorithm which has\ntime complexity of $2 \\cdot \\min \\{h(L), f + 1\\}$ message delays which improves\non the previously known time complexity of $O(n)$ message delays.\n  The generalized lattice agreement problem defined by Faleiro et al in\n\\cite{faleiro2012generalized} is a generalization of the lattice agreement\nproblem where it is applied for the replicated state machine. We propose an\nalgorithm which guarantees liveness when a majority of the processes are\ncorrect in asynchronous systems. Our algorithm requires $\\min \\{O(h(L)),\nO(f)\\}$ units of time in the worst case which is better than $O(n)$ units of\ntime required by the algorithm of Faleiro et al. \\cite{faleiro2012generalized}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 20:25:16 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zheng", "Xiong", ""], ["Hu", "Changyong", ""], ["Garg", "Vijay K.", ""]]}, {"id": "1807.11607", "submitter": "Wolfgang Fink", "authors": "Tzyy-Juin Kao and Wolfgang Fink", "title": "Pareto-Optimization Framework for Automated Network-on-Chip Design", "comments": "25 pages (1.5 line spacing), 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of multi-core processors, network-on-chip design has been key\nin addressing network performances, such as bandwidth, power consumption, and\ncommunication delays when dealing with on-chip communication between the\nincreasing number of processor cores. As the numbers of cores increase, network\ndesign becomes more complex. Therefore, there is a critical need in soliciting\ncomputer aid in determining network configurations that afford optimal\nperformance given resources and design constraints. We propose a\nPareto-optimization framework that explores the space of possible network\nconfigurations to determine optimal network latencies, power consumption, and\nthe corresponding link allocations. For a given number of routers, average\nnetwork latency and power consumption as example performance objectives can be\ndisplayed in form of Pareto-optimal fronts, thus not only offering a design\ntool, but also enabling trade-off studies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 23:22:46 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Kao", "Tzyy-Juin", ""], ["Fink", "Wolfgang", ""]]}, {"id": "1807.11830", "submitter": "Federico Simmross-Wattenberg", "authors": "Federico Simmross-Wattenberg, Manuel Rodr\\'iguez-Cayetano, Javier\n  Royuela-del-Val, Elena Mart\\'in-Gonz\\'alez, Elisa Moya-S\\'aez, Marcos\n  Mart\\'in-Fern\\'andez and Carlos Alberola-L\\'opez", "title": "OpenCLIPER: an OpenCL-based C++ Framework for Overhead-Reduced Medical\n  Image Processing and Reconstruction on Heterogeneous Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image processing is often limited by the computational cost of the\ninvolved algorithms. Whereas dedicated computing devices (GPUs in particular)\nexist and do provide significant efficiency boosts, they have an extra cost of\nuse in terms of housekeeping tasks (device selection and initialization, data\nstreaming, synchronization with the CPU and others), which may hinder\ndevelopers from using them. This paper describes an OpenCL-based framework that\nis capable of handling dedicated computing devices seamlessly and that allows\nthe developer to concentrate on image processing tasks.\n  The framework handles automatically device discovery and initialization, data\ntransfers to and from the device and the file system and kernel loading and\ncompiling. Data structures need to be defined only once independently of the\ncomputing device; code is unique, consequently, for every device, including the\nhost CPU. Pinned memory/buffer mapping is used to achieve maximum performance\nin data transfers.\n  Code fragments included in the paper show how the computing device is almost\nimmediately and effortlessly available to the users algorithms, so they can\nfocus on productive work. Code required for device selection and\ninitialization, data loading and streaming and kernel compilation is minimal\nand systematic. Algorithms can be thought of as mathematical operators (called\nprocesses), with input, output and parameters, and they may be chained one\nafter another easily and efficiently. Also for efficiency, processes can have\ntheir initialization work split from their core workload, so process chains and\nloops do not incur in performance penalties. Algorithm code is independent of\nthe device type targeted.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 14:15:07 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Simmross-Wattenberg", "Federico", ""], ["Rodr\u00edguez-Cayetano", "Manuel", ""], ["Royuela-del-Val", "Javier", ""], ["Mart\u00edn-Gonz\u00e1lez", "Elena", ""], ["Moya-S\u00e1ez", "Elisa", ""], ["Mart\u00edn-Fern\u00e1ndez", "Marcos", ""], ["Alberola-L\u00f3pez", "Carlos", ""]]}, {"id": "1807.11878", "submitter": "Ant\\'onio Sim\\~oes", "authors": "Ant\\'onio Sim\\~oes and Jo\\~ao Xavier", "title": "FADE: Fast and Asymptotically efficient Distributed Estimator for\n  dynamic networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2901355", "report-no": null, "categories": "cs.SY cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set of agents that wish to estimate a vector of parameters of\ntheir mutual interest. For this estimation goal, agents can sense and\ncommunicate. When sensing, an agent measures (in additive gaussian noise)\nlinear combinations of the unknown vector of parameters. When communicating, an\nagent can broadcast information to a few other agents, by using the channels\nthat happen to be randomly at its disposal at the time.\n  To coordinate the agents towards their estimation goal, we propose a novel\nalgorithm called FADE (Fast and Asymptotically efficient Distributed\nEstimator), in which agents collaborate at discrete time-steps; at each\ntime-step, agents sense and communicate just once, while also updating their\nown estimate of the unknown vector of parameters.\n  FADE enjoys five attractive features: first, it is an intuitive estimator,\nsimple to derive; second, it withstands dynamic networks, that is, networks\nwhose communication channels change randomly over time; third, it is strongly\nconsistent in that, as time-steps play out, each agent's local estimate\nconverges (almost surely) to the true vector of parameters; fourth, it is both\nasymptotically unbiased and efficient, which means that, across time, each\nagent's estimate becomes unbiased and the mean-square error (MSE) of each\nagent's estimate vanishes to zero at the same rate of the MSE of the optimal\nestimator at an almighty central node; fifth, and most importantly, when\ncompared with a state-of-art consensus+innovation (CI) algorithm, it yields\nestimates with outstandingly lower mean-square errors, for the same number of\ncommunications -- for example, in a sparsely connected network model with 50\nagents, we find through numerical simulations that the reduction can be\ndramatic, reaching several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 15:46:16 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Sim\u00f5es", "Ant\u00f3nio", ""], ["Xavier", "Jo\u00e3o", ""]]}]