[{"id": "1804.00284", "submitter": "Ilnur Khuziev", "authors": "Khuziev Ilnur", "title": "Fast tree numeration in networks with synchronized time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present a protocol for building dense numeration in\nnetwork with unknown topology. Additionally to a unique number each node as\nresult of the protocol will get information about a spanning tree. This\nspanning tree is constructed during BFS search from the leader node. This\nproperty of the numeration can be useful in other tasks, as example we present\na protocol for searching bridges in network. The time of numeration building in\nour protocol is linear in network size, simple informational lower bounds also\nlinear (it is required at least linear number of bits for code tree structure).\nIn bridges searching problem our protocol also heats lower linear bound: in\nresult each node knows about all bridges.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 11:24:14 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Ilnur", "Khuziev", ""]]}, {"id": "1804.00358", "submitter": "Hamed Omidvar", "authors": "Hamed Omidvar and Massimo Franceschetti", "title": "Evolution and Limiting Configuration of a Long-Range Schelling-Type Spin\n  System", "comments": "arXiv admin note: text overlap with arXiv:1811.10677\n  (arXiv:1811.10677 is an extension of this work by the same authors and these\n  works share many parts.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC cs.SI math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a long-range interacting particle system in which binary\nparticles -- whose initial states are chosen uniformly at random -- are located\nat the nodes of a flat torus $(\\mathbb{Z}/h\\mathbb{Z})^2$. Each node of the\ntorus is connected to all the nodes located in an $l_\\infty$-ball of radius $w$\nin the toroidal space centered at itself and we assume that $h$ is\nexponentially larger than $w^2$. Based on the states of the neighboring\nparticles and on the value of a common intolerance threshold $\\tau$, every\nparticle is labeled \"stable,\" or \"unstable.\" Every unstable particle that can\nbecome stable by flipping its state is labeled \"p-stable.\" Finally, unstable\nparticles that remained p-stable for a random, independent and identically\ndistributed waiting time, flip their state and become stable. When the waiting\ntimes have an exponential distribution and $\\tau \\le 1/2$, this model is\nequivalent to a Schelling model of self-organized segregation in an open\nsystem, a zero-temperature Ising model with Glauber dynamics, or an\nAsynchronous Cellular Automaton (ACA) with extended Moore neighborhoods. We\nfirst prove a shape theorem for the spreading of the \"affected\" nodes of a\ngiven state -- namely nodes on which a particle of a given state would be\np-stable. As $w \\rightarrow \\infty$, this spreading starts with high\nprobability (w.h.p.) from any $l_\\infty$-ball in the torus having radius $w/2$\nand containing only affected nodes, and continues for a time that is at least\nexponential in the cardinalilty of the neighborhood of interaction $N =\n(2w+1)^2$. Second, we show that when the process reaches a limiting\nconfiguration and no more state changes occur, for all ${\\tau \\in\n(\\tau^*,1-\\tau^*) \\setminus \\{1/2\\}}$ where ${\\tau^* \\approx 0.488}$, w.h.p.\nany particle is contained in a large \"monochromatic ball\" of cardinality\nexponential in $N$.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 23:47:58 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 23:44:03 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 22:31:48 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 02:01:58 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Omidvar", "Hamed", ""], ["Franceschetti", "Massimo", ""]]}, {"id": "1804.00399", "submitter": "Hung Dang", "authors": "Hung Dang, Tien Tuan Anh Dinh, Dumitrel Loghin, Ee-Chien Chang, Qian\n  Lin, Beng Chin Ooi", "title": "Towards Scaling Blockchain Systems via Sharding", "comments": "This is an updated version of the Chain of Trust: Can Trusted\n  Hardware Help Scaling Blockchains? paper. This version is to be appeared in\n  SIGMOD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing blockchain systems scale poorly because of their distributed\nconsensus protocols. Current attempts at improving blockchain scalability are\nlimited to cryptocurrency. Scaling blockchain systems under general workloads\n(i.e., non-cryptocurrency applications) remains an open question. In this work,\nwe take a principled approach to apply sharding, which is a well-studied and\nproven technique to scale out databases, to blockchain systems in order to\nimprove their transaction throughput at scale. This is challenging, however,\ndue to the fundamental difference in failure models between databases and\nblockchain. To achieve our goal, we first enhance the performance of Byzantine\nconsensus protocols, by doing so we improve individual shards' throughput.\nNext, we design an efficient shard formation protocol that leverages a trusted\nrandom beacon to securely assign nodes into shards. We rely on trusted\nhardware, namely Intel SGX, to achieve high performance for both consensus and\nshard formation protocol. Third, we design a general distributed transaction\nprotocol that ensures safety and liveness even when transaction coordinators\nare malicious. Finally, we conduct an extensive evaluation of our design both\non a local cluster and on Google Cloud Platform. The results show that our\nconsensus and shard formation protocols outperform state-of-the-art solutions\nat scale. More importantly, our sharded blockchain reaches a high throughput\nthat can handle Visa-level workloads, and is the largest ever reported in a\nrealistic environment.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 05:33:18 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 03:47:45 GMT"}, {"version": "v3", "created": "Sat, 11 Aug 2018 02:44:04 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2019 05:03:53 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Dang", "Hung", ""], ["Dinh", "Tien Tuan Anh", ""], ["Loghin", "Dumitrel", ""], ["Chang", "Ee-Chien", ""], ["Lin", "Qian", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1804.00478", "submitter": "Leeor Peled", "authors": "Leeor Peled, Uri Weiser, Yoav Etsion", "title": "A neural network memory prefetcher using semantic locality", "comments": "Newer version under conference submission", "journal-ref": "ACM Trans. Archit. Code Optim., Vol. 16, No. 4, Article 37,\n  Publication date: October 2019", "doi": "10.1145/3345000", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate memory prefetching is paramount for processor performance, and\nmodern processors employ various techniques to identify and prefetch different\nmemory access patterns. While most modern prefetchers target spatio-temporal\npatterns by matching memory addresses that are accessed in close proximity\n(either in space or time), the recently proposed concept of semantic locality\nviews locality as an artifact of the algorithmic level and searches for\ncorrelations between memory accesses and program state. While this approach was\nshown to be effective, capturing semantic locality requires significant\nassociative learning capabilities. In this paper we utilize neural networks for\nthis task. We leverage recent advances in machine learning to propose a neural\nnetwork prefetcher. We show that by observing program context, this prefetcher\ncan learn distinct memory access patterns that cannot be covered by other\nstate-of-the-art prefetchers. We evaluate the neural network prefetcher over\nSPEC2006, Graph500, and several microbenchmarks. We show that the prefetcher\ncan deliver an average speedup of 30% for SPEC2006 (up to 2.7x) and up to 4.6x\nover kernels. We also present a high-level design of our prefetcher, explore\nthe power, energy and area limitations, and propose several optimizations for\nfeasibility. We believe that this line of research can further improve the\nefficiency of such neural networks and allow harnessing them for additional\nmicro-architectural predictions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 12:56:04 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 15:14:09 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Peled", "Leeor", ""], ["Weiser", "Uri", ""], ["Etsion", "Yoav", ""]]}, {"id": "1804.00623", "submitter": "Renzo Andri", "authors": "Renzo Andri, Lukas Cavigelli, Davide Rossi, Luca Benini", "title": "Hyperdrive: A Multi-Chip Systolically Scalable Binary-Weight CNN\n  Inference Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive results in computer vision and\nmachine learning. Unfortunately, state-of-the-art networks are extremely\ncompute and memory intensive which makes them unsuitable for mW-devices such as\nIoT end-nodes. Aggressive quantization of these networks dramatically reduces\nthe computation and memory footprint. Binary-weight neural networks (BWNs)\nfollow this trend, pushing weight quantization to the limit. Hardware\naccelerators for BWNs presented up to now have focused on core efficiency,\ndisregarding I/O bandwidth and system-level efficiency that are crucial for\ndeployment of accelerators in ultra-low power devices. We present Hyperdrive: a\nBWN accelerator dramatically reducing the I/O bandwidth exploiting a novel\nbinary-weight streaming approach, which can be used for arbitrarily sized\nconvolutional neural network architecture and input resolution by exploiting\nthe natural scalability of the compute units both at chip-level and\nsystem-level by arranging Hyperdrive chips systolically in a 2D mesh while\nprocessing the entire feature map together in parallel. Hyperdrive achieves 4.3\nTOp/s/W system-level efficiency (i.e., including I/Os)---3.1x higher than\nstate-of-the-art BWN accelerators, even if its core uses resource-intensive\nFP16 arithmetic for increased robustness.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 17:35:42 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 13:25:50 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 11:15:37 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Andri", "Renzo", ""], ["Cavigelli", "Lukas", ""], ["Rossi", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "1804.00695", "submitter": "Mehmet Deveci", "authors": "Mehmet Deveci, Simon D. Hammond, Michael M. Wolf, and Sivasankaran\n  Rajamanickam", "title": "Sparse Matrix-Matrix Multiplication on Multilevel Memory Architectures :\n  Algorithms and Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": "SAND2018-3428 R", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architectures with multiple classes of memory media are becoming a common\npart of mainstream supercomputer deployments. So called multi-level memories\noffer differing characteristics for each memory component including variation\nin bandwidth, latency and capacity. This paper investigates the performance of\nsparse matrix multiplication kernels on two leading high-performance computing\narchitectures -- Intel's Knights Landing processor and NVIDIA's Pascal GPU. We\ndescribe a data placement method and a chunking-based algorithm for our kernels\nthat exploits the existence of the multiple memory spaces in each hardware\nplatform. We evaluate the performance of these methods w.r.t. standard\nalgorithms using the auto-caching mechanisms. Our results show that standard\nalgorithms that exploit cache reuse performed as well as multi-memory-aware\nalgorithms for architectures such as KNLs where the memory subsystems have\nsimilar latencies. However, for architectures such as GPUs where memory\nsubsystems differ significantly in both bandwidth and latency,\nmulti-memory-aware methods are crucial for good performance. In addition, our\nnew approaches permit the user to run problems that require larger capacities\nthan the fastest memory of each compute node without depending on the\nsoftware-managed cache mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 18:53:45 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Deveci", "Mehmet", ""], ["Hammond", "Simon D.", ""], ["Wolf", "Michael M.", ""], ["Rajamanickam", "Sivasankaran", ""]]}, {"id": "1804.00701", "submitter": "Virendra Marathe", "authors": "Virendra Marathe, Achin Mishra, Amee Trivedi, Yihe Huang, Faisal\n  Zaghloul, Sanidhya Kashyap, Margo Seltzer, Tim Harris, Steve Byan, Bill\n  Bridge, Dave Dice", "title": "Persistent Memory Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comprehensive analysis of performance trade offs\nbetween implementation choices for transaction runtime systems on persistent\nmemory. We compare three implementations of transaction runtimes: undo logging,\nredo logging, and copy-on-write. We also present a memory allocator that plugs\ninto these runtimes. Our microbenchmark based evaluation focuses on\nunderstanding the interplay between various factors that contribute to\nperformance differences between the three runtimes -- read/write access\npatterns of workloads, size of the persistence domain (portion of the memory\nhierarchy where the data is effectively persistent), cache locality, and\ntransaction runtime bookkeeping overheads. No single runtime emerges as a clear\nwinner. We confirm our analysis in more realistic settings of three \"real\nworld\" applications we developed with our transactional API: (i) a key-value\nstore we implemented from scratch, (ii) a SQLite port, and (iii) a persistified\nversion of memcached, a popular key-value store. These findings are not only\nconsistent with our microbenchmark analysis, but also provide additional\ninteresting insights into other factors (e.g. effects of multithreading and\nsynchronization) that affect application performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 22:46:42 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Marathe", "Virendra", ""], ["Mishra", "Achin", ""], ["Trivedi", "Amee", ""], ["Huang", "Yihe", ""], ["Zaghloul", "Faisal", ""], ["Kashyap", "Sanidhya", ""], ["Seltzer", "Margo", ""], ["Harris", "Tim", ""], ["Byan", "Steve", ""], ["Bridge", "Bill", ""], ["Dice", "Dave", ""]]}, {"id": "1804.00702", "submitter": "Rodrigo Bruno", "authors": "Rodrigo Bruno, Duarte Patr\\'icio, Jos\\'e Sim\\~ao, Lu\\'is Veiga and\n  Paulo Ferreira", "title": "ROLP: Runtime Object Lifetime Profiling for Big Data Memory Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low latency services such as credit-card fraud detection and website targeted\nadvertisement rely on Big Data platforms (e.g., Lucene, Graphchi, Cassandra)\nwhich run on top of memory managed runtimes, such as the JVM. These platforms,\nhowever, suffer from unpredictable and unacceptably high pause times due to\ninadequate memory management decisions (e.g., allocating objects with very\ndifferent lifetimes next to each other, resulting in memory fragmentation).\nThis leads to long and frequent application pause times, breaking Service Level\nAgreements (SLAs). This problem has been previously identified and results show\nthat current memory management techniques are ill-suited for applications that\nhold in memory massive amounts of middle to long-lived objects (which is the\ncase for a wide spectrum of Big Data applications).\n  Previous works try to reduce such application pauses by allocating objects\noff-heap or in special allocation regions/generations, thus alleviating the\npressure on memory management. However, all these solutions require a\ncombination of programmer effort and knowledge, source code access, or off-line\nprofiling, with clear negative impact on programmer productivity and/or\napplication performance.\n  This paper presents ROLP, a runtime object lifetime profiling system. ROLP\nprofiles application code at runtime in order to identify which allocation\ncontexts create objects with middle to long lifetimes, given that such objects\nneed to be handled differently (regarding short-lived ones). This profiling\ninformation greatly improves memory management decisions, leading to long tail\nlatencies reduction of up to 51% for Lucene, 85% for GraphChi, and 60% for\nCassandra, with negligible throughput and memory overhead. ROLP is implemented\nfor the OpenJDK 8 HotSpot JVM and it does not require any programmer effort or\nsource code access.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 16:53:44 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Bruno", "Rodrigo", ""], ["Patr\u00edcio", "Duarte", ""], ["Sim\u00e3o", "Jos\u00e9", ""], ["Veiga", "Lu\u00eds", ""], ["Ferreira", "Paulo", ""]]}, {"id": "1804.00703", "submitter": "Rasoul Rahmani", "authors": "R. Rahmani and I. Moser and M. Seyedmahmoudian", "title": "A Complete Model for Modular Simulation of Data Centre Power Load", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centres are very fast growing structures with significant contribution\nto the world's energy consumption. Reducing the energy consumption of data\ncentres is easier when the components that comprise a data centre and their\nrespective energy consumption are known. A complete model for a modular design\nof a data centre and a technique for simulating each module's energy\nconsumption are presented. Detailed power consumption modelling for each\ncomponent as well as their interactions are the merits of this model. Unlike\nexisting research, the present modular simulation model can take different\ndesign structures of data centres into account and provide us with hourly power\nconsumption profiles for each component. The impacts of environmental\nparameters such as temperature and humidity are also investigated and\nincorporated into the model. The flexibility, scalability, comprehensiveness\nand modularity of this model provides researchers and designers with a powerful\ntool for energy analysis, management and planning of data centres with\ndifferent designs and locations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 00:21:20 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Rahmani", "R.", ""], ["Moser", "I.", ""], ["Seyedmahmoudian", "M.", ""]]}, {"id": "1804.00704", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Naoto Hoshikawa, Hirofumi Noguchi, Tatsuya Demizu and\n  Misao Kataoka", "title": "A study of coordination logic description and execution for dynamic\n  device coordination services", "comments": "2 pages, in Japanese, The 2018 IEICE General Conference, BS-4-1, Mar.\n  2018", "journal-ref": "The 2018 IEICE General Conference, BS-4-1, Mar. 2018. (c) 2018\n  IEICE", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, IoT technologies have been progressed, and many devices are\nconnected to networks. Previously, IoT services were developed by vertical\nintegration style. But now Open IoT concept has attracted attentions which\nachieves various IoT services by integrating horizontal separated devices and\nservices. For Open IoT era, we have proposed the Tacit Computing technology to\ndiscover the devices with necessary data for users on demand and use them\ndynamically. Although Tacit Computing can discover and use the device based on\nthe situation, a study of coordination logic description is insufficient when\nmultiple devices are coordinated. In this paper, we study coordination logic\ndescription and execution to coordinate multiple devices dynamically. We\ncompare methods with ideas to convert abstract description to specific\ninterface access means at execution time, study merits and demerits and propose\nan appropriate method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 15:00:50 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Yamato", "Yoji", ""], ["Hoshikawa", "Naoto", ""], ["Noguchi", "Hirofumi", ""], ["Demizu", "Tatsuya", ""], ["Kataoka", "Misao", ""]]}, {"id": "1804.00705", "submitter": "Soramichi Akiyama", "authors": "Shinsuke Hamada, Soramichi Akiyama, Mitaro Namiki", "title": "Reactive NaN Repair for Applying Approximate Memory to Numerical\n  Applications", "comments": "Presented in the 8th Workshop on Systems for Multi-core and\n  Heterogeneous Architectures (SFMA), co-located with EuroSys'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in the AI and HPC fields require much memory capacity, and the\namount of energy consumed by main memory of server machines is ever increasing.\nEnergy consumption of main memory can be greatly reduced by applying\napproximate computing in exchange for increased bit error rates. AI and HPC\napplications are to some extent robust to bit errors because small numerical\nerrors are amortized by their iterative nature. However, a single occurrence of\na NaN due to bit-flips corrupts the whole calculation result. The issue is that\nfixing every bit-flip using ECC incurs too much overhead because the bit error\nrate is much higher than in normal environments. We propose a low-overhead\nmethod to fix NaNs when approximate computing is applied to main memory. The\nmain idea is to reactively repair NaNs while leaving other non-fatal numerical\nerrors as-is to reduce the overhead. We implemented a prototype by leveraging\nfloating-point exceptions of x86 CPUs, and the preliminary evaluations showed\nthat our method incurs negligible overhead.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 03:52:05 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Hamada", "Shinsuke", ""], ["Akiyama", "Soramichi", ""], ["Namiki", "Mitaro", ""]]}, {"id": "1804.00706", "submitter": "Cheng Tan", "authors": "Guanwen Zhong, Akshat Dubey, Tan Cheng, Tulika Mitra", "title": "Synergy: A HW/SW Framework for High Throughput CNNs on Embedded\n  Heterogeneous SoC", "comments": "34 pages, submitted to ACM Transactions on Embedded Computing Systems\n  (TECS)", "journal-ref": "TECS, 18 (2019) 13-39", "doi": "10.1145/3301278", "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been widely deployed in diverse\napplication domains. There has been significant progress in accelerating both\ntheir training and inference using high-performance GPUs, FPGAs, and custom\nASICs for datacenter-scale environments. The recent proliferation of mobile and\nIoT devices have necessitated real-time, energy-efficient deep neural network\ninference on embedded-class, resource-constrained platforms. In this context,\nwe present {\\em Synergy}, an automated, hardware-software co-designed,\npipelined, high-throughput CNN inference framework on embedded heterogeneous\nsystem-on-chip (SoC) architectures (Xilinx Zynq). {\\em Synergy} leverages,\nthrough multi-threading, all the available on-chip resources, which includes\nthe dual-core ARM processor along with the FPGA and the NEON SIMD engines as\naccelerators. Moreover, {\\em Synergy} provides a unified abstraction of the\nheterogeneous accelerators (FPGA and NEON) and can adapt to different network\nconfigurations at runtime without changing the underlying hardware accelerator\narchitecture by balancing workload across accelerators through work-stealing.\n{\\em Synergy} achieves 7.3X speedup, averaged across seven CNN models, over a\nwell-optimized software-only solution. {\\em Synergy} demonstrates substantially\nbetter throughput and energy-efficiency compared to the contemporary CNN\nimplementations on the same SoC architecture.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 16:02:45 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Zhong", "Guanwen", ""], ["Dubey", "Akshat", ""], ["Cheng", "Tan", ""], ["Mitra", "Tulika", ""]]}, {"id": "1804.00742", "submitter": "Jing Zhong", "authors": "Jing Zhong, Roy D. Yates and Emina Soljanin", "title": "Minimizing Content Staleness in Dynamo-Style Replicated Storage Systems", "comments": "INFOCOM AoI Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency in data storage systems requires any read operation to return the\nmost recent written version of the content. In replicated storage systems,\nconsistency comes at the price of delay due to large-scale write and read\noperations. Many applications with low latency requirements tolerate data\nstaleness in order to provide high availability and low operation latency.\nUsing age of information as the staleness metric, we examine a data updating\nsystem in which real-time content updates are replicated and stored in a\nDynamo-style quorum- based distributed system. A source sends updates to all\nthe nodes in the system and waits for acknowledgements from the earliest subset\nof nodes, known as a write quorum. An interested client fetches the update from\nanother set of nodes, defined as a read quorum. We analyze the staleness-delay\ntradeoff in replicated storage by varying the write quorum size. With a larger\nwrite quorum, an instantaneous read is more likely to get the latest update\nwritten by the source. However, the age of the content written to the system is\nmore likely to become stale as the write quorum size increases. For shifted\nexponential distributed write delay, we derive the age optimized write quorum\nsize that balances the likelihood of reading the latest update and the\nfreshness of the latest update written by the source.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 21:45:43 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zhong", "Jing", ""], ["Yates", "Roy D.", ""], ["Soljanin", "Emina", ""]]}, {"id": "1804.00914", "submitter": "Marc Shapiro", "authors": "Marc Shapiro (DELYS, Inria, LIP6), Pierre Sutra", "title": "Database Consistency Models", "comments": null, "journal-ref": "Sherif Sakr; Albert Zomaya. Encyclopedia of Big Data Technologies,\n  Springer, 2017, 978-3-319-63962-8", "doi": "10.1007/978-3-319-63962-8\\_203-1", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data store allows application processes to put and get data from a shared\nmemory. In general, a data store cannot be modelled as a strictly sequential\nprocess. Applications observe non-sequential behaviours, called anomalies. The\nset of pos- sible behaviours, and conversely of possible anomalies, constitutes\nthe consistency model of the data store.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 11:33:18 GMT"}], "update_date": "2018-06-24", "authors_parsed": [["Shapiro", "Marc", "", "DELYS, Inria, LIP6"], ["Sutra", "Pierre", ""]]}, {"id": "1804.00947", "submitter": "Giorgi Nadiradze", "authors": "Dan Alistarh, Syed Kamran Haider, Raphael K\\\"ubler and Giorgi\n  Nadiradze", "title": "The Transactional Conflict Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transactional conflict problem arises in transactional systems whenever\ntwo or more concurrent transactions clash on a data item.\n  While the standard solution to such conflicts is to immediately abort one of\nthe transactions, some practical systems consider the alternative of delaying\nconflict resolution for a short interval, which may allow one of the\ntransactions to commit. The challenge in the transactional conflict problem is\nto choose the optimal length of this delay interval so as to minimize the\noverall running time penalty for the conflicting transactions. In this paper,\nwe propose a family of optimal online algorithms for the transactional conflict\nproblem.\n  Specifically, we consider variants of this problem which arise in different\nimplementations of transactional systems, namely \"requestor wins\" and\n\"requestor aborts\" implementations: in the former, the recipient of a coherence\nrequest is aborted, whereas in the latter, it is the requestor which has to\nabort. Both strategies are implemented by real systems.\n  We show that the requestor aborts case can be reduced to a classic instance\nof the ski rental problem, while the requestor wins case leads to a new version\nof this classical problem, for which we derive optimal deterministic and\nrandomized algorithms.\n  Moreover, we prove that, under a simplified adversarial model, our algorithms\nare constant-competitive with the offline optimum in terms of throughput.\n  We validate our algorithmic results empirically through a hardware simulation\nof hardware transactional memory (HTM), showing that our algorithms can lead to\nnon-trivial performance improvements for classic concurrent data structures.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 13:15:20 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Alistarh", "Dan", ""], ["Haider", "Syed Kamran", ""], ["K\u00fcbler", "Raphael", ""], ["Nadiradze", "Giorgi", ""]]}, {"id": "1804.01018", "submitter": "Giorgi Nadiradze", "authors": "Dan Alistarh, Trevor Brown, Justin Kopinsky, Jerry Z. Li and Giorgi\n  Nadiradze", "title": "Distributionally Linearizable Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relaxed concurrent data structures have become increasingly popular, due to\ntheir scalability in graph processing and machine learning applications.\nDespite considerable interest, there exist families of natural, high performing\nrandomized relaxed concurrent data structures, such as the popular MultiQueue\npattern for implementing relaxed priority queue data structures, for which no\nguarantees are known in the concurrent setting. Our main contribution is in\nshowing for the first time that, under a set of analytic assumptions, a family\nof relaxed concurrent data structures, including variants of MultiQueues, but\nalso a new approximate counting algorithm we call the MultiCounter, provides\nstrong probabilistic guarantees on the degree of relaxation with respect to the\nsequential specification, in arbitrary concurrent executions. We formalize\nthese guarantees via a new correctness condition called distributional\nlinearizability, tailored to concurrent implementations with randomized\nrelaxations. Our result is based on a new analysis of an asynchronous variant\nof the classic power-of-two-choices load balancing algorithm, in which\nplacement choices can be based on inconsistent, outdated information (this\nresult may be of independent interest). We validate our results empirically,\nshowing that the MultiCounter algorithm can implement scalable relaxed\ntimestamps, which in turn can improve the performance of the classic TL2\ntransactional algorithm by up to 3 times, for some settings of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 15:01:54 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Alistarh", "Dan", ""], ["Brown", "Trevor", ""], ["Kopinsky", "Justin", ""], ["Li", "Jerry Z.", ""], ["Nadiradze", "Giorgi", ""]]}, {"id": "1804.01138", "submitter": "Rajarshi Biswas", "authors": "Rajarshi Biswas, Xiaoyi Lu, Dhabaleswar K. Panda", "title": "Designing a Micro-Benchmark Suite to Evaluate gRPC for TensorFlow: Early\n  Experiences", "comments": "9 Pages, 14 Figures, This paper was presented at BPOE - 9 @ ASPLOS\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote procedure call (RPC) is the backbone of many modern distributed\nsystems. Google's gRPC is one of the most popular open source RPC frameworks\navailable in the community. gRPC is the main communication engine for Google's\nDeep Learning framework TensorFlow. TensorFlow primarily uses gRPC for\ncommunicating tensors and administrative tasks among different processes.\nTensor updates during the training phase are communication intensive and thus\nTensorFlow's performance is heavily dependent on the underlying network and the\nefficacy of the communication engine. Training deep learning models on\nTensorFlow can take significant time ranging from several minutes to several\nhours, even several days. Thus system researchers need to devote a lot of time\nto understand the impact of communication on the overall performance. Clearly,\nthere is lack of benchmarks available for system researchers. Therefore, we\npropose TF-gRPC-Bench micro-benchmark suite that enables system researchers to\nquickly understand the impact of the underlying network and communication\nruntime on deep learning workloads. To achieve this, we first analyze the\ncharacteristics of TensorFlow workload over gRPC by training popular deep\nlearning models. Then, we propose three micro-benchmarks that take account\nthese workload characteristics. In addition, we comprehensively evaluate gRPC\nwith TF-gRPC-Bench micro-benchmark suite on different clusters over Ethernet,\nIPoIB, and RDMA, and present the results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 19:20:33 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Biswas", "Rajarshi", ""], ["Lu", "Xiaoyi", ""], ["Panda", "Dhabaleswar K.", ""]]}, {"id": "1804.01276", "submitter": "Muktikanta Sa", "authors": "Muktikanta Sa", "title": "Maintenance of Strongly Connected Component in Shared-memory Graph", "comments": "29 pages, 4 figures, Accepted in the Conference NETYS-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an on-line fully dynamic algorithm for maintaining\nstrongly connected component of a directed graph in a shared memory\narchitecture. The edges and vertices are added or deleted concurrently by fixed\nnumber of threads. To the best of our knowledge, this is the first work to\npropose using linearizable concurrent directed graph and is build using both\nordered and unordered list-based set. We provide an empirical comparison\nagainst sequential and coarse-grained. The results show our algorithm's\nthroughput is increased between 3 to 6x depending on different workload\ndistributions and applications. We believe that there are huge applications in\nthe on-line graph. Finally, we show how the algorithm can be extended to\ncommunity detection in on-line graph.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 07:41:48 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 14:57:41 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 07:56:58 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Sa", "Muktikanta", ""]]}, {"id": "1804.01308", "submitter": "Ran Ben Basat", "authors": "Ran Ben-Basat, Guy Even, Ken-ichi Kawarabayashi, Gregory Schwartzman", "title": "A Deterministic Distributed $2$-Approximation for Weighted Vertex Cover\n  in $O(\\log n\\log\\Delta / \\log^2\\log\\Delta)$ Rounds", "comments": "To appear in SIROCCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic distributed $2$-approximation algorithm for the\nMinimum Weight Vertex Cover problem in the CONGEST model whose round complexity\nis $O(\\log n \\log \\Delta / \\log^2 \\log \\Delta)$. This improves over the\ncurrently best known deterministic 2-approximation implied by [KVY94]. Our\nsolution generalizes the $(2+\\epsilon)$-approximation algorithm of [BCS17],\nimproving the dependency on $\\epsilon^{-1}$ from linear to logarithmic. In\naddition, for every $\\epsilon=(\\log \\Delta)^{-c}$, where $c\\geq 1$ is a\nconstant, our algorithm computes a $(2+\\epsilon)$-approximation in $O(\\log\n\\Delta / \\log \\log \\Delta)$~rounds (which is asymptotically optimal).\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 09:01:44 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 10:37:43 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 10:08:17 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Ben-Basat", "Ran", ""], ["Even", "Guy", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1804.01368", "submitter": "Koichi Wada", "authors": "Takashi Okumura, Koichi Wada, Xavier D\\'efago", "title": "Optimal Rendezvous ${\\mathcal L}$-Algorithms for Asynchronous Mobile\n  Robots with External-Lights", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Rendezvous problem for 2 autonomous mobile robots in\nasynchronous settings with persistent memory called light. It is well known\nthat Rendezvous is impossible in a basic model when robots have no lights, even\nif the system is semi-synchronous. On the other hand, Rendezvous is possible if\nrobots have lights of various types with a constant number of colors. If robots\ncan observe not only their own lights but also other robots' lights, their\nlights are called full-light. If robots can only observe the state of other\nrobots' lights, the lights are called external-light.\n  In this paper, we focus on robots with external-lights in asynchronous\nsettings and a particular class of algorithms (called L-algorithms), where an\nL-algorithm computes a destination based only on the current colors of\nobservable lights. When considering L-algorithms, Rendezvous can be solved by\nrobots with full-lights and 3 colors in general asynchronous settings (called\nASYNC) and the number of colors is optimal under these assumptions. In\ncontrast, there exists no L-algorithms in ASYNC with external-lights regardless\nof the number of colors. In this paper, we consider a fairly large subclass of\nASYNC in which Rendezvous can be solved by L-algorithms using external-lights\nwith a finite number of colors, and we show that the algorithms are optimal in\nthe number of colors they use.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 12:39:41 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Okumura", "Takashi", ""], ["Wada", "Koichi", ""], ["D\u00e9fago", "Xavier", ""]]}, {"id": "1804.01482", "submitter": "Erick Lavoie", "authors": "Erick Lavoie and Laurie Hendren", "title": "Personal Volunteer Computing", "comments": null, "journal-ref": null, "doi": "10.1145/3310273.3322819", "report-no": null, "categories": "cs.DC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose personal volunteer computing, a novel paradigm to encourage\ntechnical solutions that leverage personal devices, such as smartphones and\nlaptops, for personal applications that require significant computations, such\nas animation rendering and image processing. The paradigm requires no\ninvestment in additional hardware, relying instead on devices that are already\nowned by users and their community, and favours simple tools that can be\nimplemented part-time by a single developer. We show that samples of personal\ndevices of today are competitive with a top-of-the-line laptop from two years\nago. We also propose new directions to extend the paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 15:55:37 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 16:49:24 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Lavoie", "Erick", ""], ["Hendren", "Laurie", ""]]}, {"id": "1804.01626", "submitter": "Ittai Abraham", "authors": "Guy Golan Gueta and Ittai Abraham and Shelly Grossman and Dahlia\n  Malkhi and Benny Pinkas and Michael K. Reiter and Dragos-Adrian Seredinschi\n  and Orr Tamir and Alin Tomescu", "title": "SBFT: a Scalable and Decentralized Trust Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SBFT is a state of the art Byzantine fault tolerant permissioned blockchain\nsystem that addresses the challenges of scalability, decentralization and\nworld-scale geo-replication. SBFTis optimized for decentralization and can\neasily handle more than 200 active replicas in a real world-scale deployment.\nWe evaluate \\sysname in a world-scale geo-replicated deployment with 209\nreplicas withstanding f=64 Byzantine failures. We provide experiments that show\nhow the different algorithmic ingredients of \\sysname increase its performance\nand scalability. The results show that SBFT simultaneously provides almost 2x\nbetter throughput and about 1.5x better latency relative to a highly optimized\nsystem that implements the PBFT protocol. To achieve this performance\nimprovement, SBFT uses a combination of four ingredients: using collectors and\nthreshold signatures to reduce communication to linear, using an optimistic\nfast path, reducing client communication and utilizing redundant servers for\nthe fast path.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 23:46:43 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 11:00:07 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 07:06:34 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gueta", "Guy Golan", ""], ["Abraham", "Ittai", ""], ["Grossman", "Shelly", ""], ["Malkhi", "Dahlia", ""], ["Pinkas", "Benny", ""], ["Reiter", "Michael K.", ""], ["Seredinschi", "Dragos-Adrian", ""], ["Tamir", "Orr", ""], ["Tomescu", "Alin", ""]]}, {"id": "1804.01698", "submitter": "Aydin Buluc", "authors": "Yusuke Nagasaka, Satoshi Matsuoka, Ariful Azad, Ayd{\\i}n Bulu\\c{c}", "title": "High-performance sparse matrix-matrix products on Intel KNL and\n  multicore architectures", "comments": "12 pages (extended version of conference paper)", "journal-ref": "In 47th International Conference on Parallel Processing Workshops\n  (ICPPW), 2018", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-matrix multiplication (SpGEMM) is a computational primitive\nthat is widely used in areas ranging from traditional numerical applications to\nrecent big data analysis and machine learning. Although many SpGEMM algorithms\nhave been proposed, hardware specific optimizations for multi- and many-core\nprocessors are lacking and a detailed analysis of their performance under\nvarious use cases and matrices is not available. We firstly identify and\nmitigate multiple bottlenecks with memory management and thread scheduling on\nIntel Xeon Phi (Knights Landing or KNL). Specifically targeting multi- and\nmany-core processors, we develop a hash-table-based algorithm and optimize a\nheap-based shared-memory SpGEMM algorithm. We examine their performance\ntogether with other publicly available codes. Different from the literature,\nour evaluation also includes use cases that are representative of real graph\nalgorithms, such as multi-source breadth-first search or triangle counting. Our\nhash-table and heap-based algorithms are showing significant speedups from\nlibraries in the majority of the cases while different algorithms dominate the\nother scenarios with different matrix size, sparsity, compression factor and\noperation type. We wrap up in-depth evaluation results and make a recipe to\ngive the best SpGEMM algorithm for target scenario. A critical finding is that\nhash-table-based SpGEMM gets a significant performance boost if the nonzeros\nare not required to be sorted within each row of the output matrix.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 07:23:14 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 15:56:16 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Nagasaka", "Yusuke", ""], ["Matsuoka", "Satoshi", ""], ["Azad", "Ariful", ""], ["Bulu\u00e7", "Ayd\u0131n", ""]]}, {"id": "1804.01911", "submitter": "Alessandro Gabbana", "authors": "E. Calore, A. Gabbana, S.F. Schifano, R. Tripiccione", "title": "Energy-efficiency evaluation of Intel KNL for HPC workloads", "comments": null, "journal-ref": null, "doi": "10.3233/978-1-61499-843-3-733", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption is increasingly becoming a limiting factor to the design\nof faster large-scale parallel systems, and development of energy-efficient and\nenergy-aware applications is today a relevant issue for HPC code-developer\ncommunities. In this work we focus on energy performance of the Knights Landing\n(KNL) Xeon Phi, the latest many-core architecture processor introduced by Intel\ninto the HPC market. We take into account the 64-core Xeon Phi 7230, and\nanalyze its energy performance using both the on-chip MCDRAM and the regular\nDDR4 system memory as main storage for the application data-domain. As a\nbenchmark application we use a Lattice Boltzmann code heavily optimized for\nthis architecture and implemented using different memory data layouts to store\nits lattice. We assessthen the energy consumption using different memory\ndata-layouts, kind of memory (DDR4 or MCDRAM) and number of threads per core.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 15:29:22 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Calore", "E.", ""], ["Gabbana", "A.", ""], ["Schifano", "S. F.", ""], ["Tripiccione", "R.", ""]]}, {"id": "1804.01918", "submitter": "Alessandro Gabbana", "authors": "Enrico Calore, Alessandro Gabbana, Sebastiano Fabio Schifano, Raffaele\n  Tripiccione", "title": "Early Experience on Using Knights Landing Processors for Lattice\n  Boltzmann Applications", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-78024-5_45", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Knights Landing (KNL) is the codename for the latest generation of Intel\nprocessors based on Intel Many Integrated Core (MIC) architecture. It relies on\nmassive thread and data parallelism, and fast on-chip memory. This processor\noperates in standalone mode, booting an off-the-shelf Linux operating system.\nThe KNL peak performance is very high - approximately 3 Tflops in double\nprecision and 6 Tflops in single precision - but sustained performance depends\ncritically on how well all parallel features of the processor are exploited by\nreal-life applications. We assess the performance of this processor for Lattice\nBoltzmann codes, widely used in computational fluid-dynamics. In our OpenMP\ncode we consider several memory data-layouts that meet the conflicting\ncomputing requirements of distinct parts of the application, and sustain a\nlarge fraction of peak performance. We make some performance comparisons with\nother processors and accelerators, and also discuss the impact of the various\nmemory layouts on energy efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 15:47:04 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Calore", "Enrico", ""], ["Gabbana", "Alessandro", ""], ["Schifano", "Sebastiano Fabio", ""], ["Tripiccione", "Raffaele", ""]]}, {"id": "1804.01942", "submitter": "Habib Saissi", "authors": "Habib Saissi, Marco Serafini and Neeraj Suri", "title": "Scaling Out Acid Applications with Operation Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OLTP applications with high workloads that cannot be served by a single\nserver need to scale out to multiple servers. Typically, scaling out entails\nassigning a different partition of the application state to each server. But\ndata partitioning is at odds with preserving the strong consistency guarantees\nof ACID transactions, a fundamental building block of many OLTP applications.\nThe more we scale out and spread data across multiple servers, the more\nfrequent distributed transactions accessing data at different servers will be.\nWith a large number of servers, the high cost of distributed transactions makes\nscaling out ineffective or even detrimental.\n  In this paper we propose Operation Partitioning, a novel paradigm to scale\nout OLTP applications that require ACID guarantees. Operation Partitioning\nindirectly partitions data across servers by partitioning the application's\noperations through static analysis. This partitioning of operations yields to a\nlock-free Conveyor Belt protocol for distributed coordination, which can scale\nout unmodified applications running on top of unmodified database management\nsystems. We implement the protocol in a system called Elia and use it to scale\nout two applications, TPC-W and RUBiS. Our experiments show that Elia can\nincrease maximum throughput by up to 4.2x and reduce latency by up to 58.6x\ncompared to MySQL Cluster while at the same time providing a stronger isolation\nguarantee (serializability instead of read committed).\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 16:31:58 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Saissi", "Habib", ""], ["Serafini", "Marco", ""], ["Suri", "Neeraj", ""]]}, {"id": "1804.02068", "submitter": "Yang Song", "authors": "Yang Song and Olivier Alavoine and Bill Lin", "title": "SARA: Self-Aware Resource Allocation for Heterogeneous MPSoCs", "comments": "Accepted by the 55th annual Design Automation Conference 2018\n  (DAC'18)", "journal-ref": null, "doi": "10.1145/3195970.3196110", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern heterogeneous MPSoCs, the management of shared memory resources is\ncrucial in delivering end-to-end QoS. Previous frameworks have either focused\non singular QoS targets or the allocation of partitionable resources among CPU\napplications at relatively slow timescales. However, heterogeneous MPSoCs\ntypically require instant response from the memory system where most resources\ncannot be partitioned. Moreover, the health of different cores in a\nheterogeneous MPSoC is often measured by diverse performance objectives. In\nthis work, we propose a Self-Aware Resource Allocation (SARA) framework for\nheterogeneous MPSoCs. Priority-based adaptation allows cores to use different\ntarget performance and self-monitor their own intrinsic health. In response,\nthe system allocates non-partitionable resources based on priorities. The\nproposed framework meets a diverse range of QoS demands from heterogeneous\ncores.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 21:49:49 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Song", "Yang", ""], ["Alavoine", "Olivier", ""], ["Lin", "Bill", ""]]}, {"id": "1804.02425", "submitter": "Meng Ma", "authors": "Meng Ma, Athanasios N. Nikolakopoulos and Georgios B. Giannakis", "title": "Fast Decentralized Optimization over Networks", "comments": "fix error in remark 4; clean up algorithms 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work introduces the hybrid consensus alternating direction method\nof multipliers (H-CADMM), a novel framework for optimization over networks\nwhich unifies existing distributed optimization approaches, including the\ncentralized and the decentralized consensus ADMM. H-CADMM provides a flexible\ntool that leverages the underlying graph topology in order to achieve a\ndesirable sweet-spot between node-to-node communication overhead and rate of\nconvergence -- thereby alleviating known limitations of both C-CADMM and\nD-CADMM. A rigorous analysis of the novel method establishes linear convergence\nrate, and also guides the choice of parameters to optimize this rate. The novel\nhybrid update rules of H-CADMM lend themselves to \"in-network acceleration\"\nthat is shown to effect considerable -- and essentially \"free-of-charge\" --\nperformance boost over the fully decentralized ADMM. Comprehensive numerical\ntests validate the analysis and showcase the potential of the method in\ntackling efficiently, widely useful learning tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 19:05:47 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 16:43:15 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Ma", "Meng", ""], ["Nikolakopoulos", "Athanasios N.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1804.02486", "submitter": "Bernard Meade", "authors": "Bernard F Meade and Christopher J Fluke", "title": "Evaluating virtual hosted desktops for graphics-intensive astronomy", "comments": "20 pages, 13 figures, 4 tables", "journal-ref": null, "doi": "10.1016/j.ascom.2018.04.002", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualisation of data is critical to understanding astronomical phenomena.\nToday, many instruments produce datasets that are too big to be downloaded to a\nlocal computer, yet many of the visualisation tools used by astronomers are\ndeployed only on desktop computers. Cloud computing is increasingly used to\nprovide a computation and simulation platform in astronomy, but it also offers\ngreat potential as a visualisation platform. Virtual hosted desktops, with\ngraphics processing unit (GPU) acceleration, allow interactive,\ngraphics-intensive desktop applications to operate co-located with astronomy\ndatasets stored in remote data centres. By combining benchmarking and user\nexperience testing, with a cohort of 20 astronomers, we investigate the\nviability of replacing physical desktop computers with virtual hosted desktops.\nIn our work, we compare two Apple MacBook computers (one old and one new,\nrepresenting hardware and opposite ends of the useful lifetime) with two\nvirtual hosted desktops: one commercial (Amazon Web Services) and one in a\nprivate research cloud (the Australian Nectar Research Cloud). For\ntwo-dimensional image-based tasks and graphics-intensive three-dimensional\noperations -- typical of astronomy visualisation workflows -- we found that\nbenchmarks do not necessarily provide the best indication of performance. When\ncompared to typical laptop computers, virtual hosted desktops can provide a\nbetter user experience, even with lower performing graphics cards. We also\nfound that virtual hosted desktops are equally simple to use, provide greater\nflexibility in choice of configuration, and may actually be a more\ncost-effective option for typical usage profiles.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 00:13:56 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 22:44:04 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Meade", "Bernard F", ""], ["Fluke", "Christopher J", ""]]}, {"id": "1804.02513", "submitter": "Hasan Heydari Gharehbolagh", "authors": "Hasan Heydari, S. Mahmoud Taheri, Kaveh Kavousi", "title": "Distributed Maximal Independent Set on Scale-Free Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distributed maximal independent set (MIS) is investigated on\ninhomogeneous random graphs with power-law weights by which the scale-free\nnetworks can be produced. Such a particular problem has been solved on graphs\nwith $n$ vertices by state-of-the-art algorithms with the time complexity of\n$O(\\log{n})$. We prove that for a scale-free network with power-law exponent\n$\\beta > 3$, the induced subgraph is constructed by vertices with degrees\nlarger than $\\log{n}\\log^{*}{n}$ is a scale-free network with $\\beta' = 2$,\nalmost surely (a.s.). Then, we propose a new algorithm that computes an MIS on\nscale-free networks with the time complexity of\n$O(\\frac{\\log{n}}{\\log{\\log{n}}})$ a.s., which is better than $O(\\log{n})$.\nFurthermore, we prove that on scale-free networks with $\\beta \\geq 3$, the\narboricity and degeneracy are less than $2^{log^{1/3}n}$ with high probability\n(w.h.p.). Finally, we prove that the time complexity of finding an MIS on\nscale-free networks with $\\beta\\geq 3$ is $O(log^{2/3}n)$ w.h.p.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 06:06:42 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 19:51:01 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Heydari", "Hasan", ""], ["Taheri", "S. Mahmoud", ""], ["Kavousi", "Kaveh", ""]]}, {"id": "1804.02729", "submitter": "Haoran Sun", "authors": "Haoran Sun and Mingyi Hong", "title": "Distributed Non-Convex First-Order Optimization and Information\n  Processing: Lower Complexity Bounds and Rate Optimal Algorithms", "comments": "Accepted by TSP", "journal-ref": null, "doi": "10.1109/TSP.2019.2943230", "report-no": null, "categories": "math.OC cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of popular distributed non-convex optimization problems,\nin which agents connected by a network $\\mathcal{G}$ collectively optimize a\nsum of smooth (possibly non-convex) local objective functions. We address the\nfollowing question: if the agents can only access the gradients of local\nfunctions, what are the fastest rates that any distributed algorithms can\nachieve, and how to achieve those rates.\n  First, we show that there exist difficult problem instances, such that it\ntakes a class of distributed first-order methods at least\n$\\mathcal{O}(1/\\sqrt{\\xi(\\mathcal{G})} \\times \\bar{L} /{\\epsilon})$\ncommunication rounds to achieve certain $\\epsilon$-solution [where\n$\\xi(\\mathcal{G})$ denotes the spectral gap of the graph Laplacian matrix, and\n$\\bar{L}$ is some Lipschitz constant]. Second, we propose (near) optimal\nmethods whose rates match the developed lower rate bound (up to a polylog\nfactor). The key in the algorithm design is to properly embed the classical\npolynomial filtering techniques into modern first-order algorithms. To the best\nof our knowledge, this is the first time that lower rate bounds and optimal\nmethods have been developed for distributed non-convex optimization problems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 17:46:18 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 05:31:28 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 19:48:01 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 03:38:47 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Sun", "Haoran", ""], ["Hong", "Mingyi", ""]]}, {"id": "1804.02817", "submitter": "Tiantian Wang", "authors": "Tiantian Wang and Zhuzhong Qian and Sanglu Lu", "title": "PingAn: An Insurance Scheme for Job Acceleration in Geo-distributed Big\n  Data Analytics System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-distributed data analysis in a cloud-edge system is emerging as a daily\ndemand. Out of saving time in wide area data transfer, some tasks are dispersed\nto the edges. However, due to limited computing, overload interference and\ncluster-level unreachable troubles, efficient execution in the edges is hard,\nwhich obstructs the guarantee on the efficiency and reliability of jobs.\nLaunching copies across clusters can be an insurance on a task's completion.\nConsidering cluster heterogeneity and accompanying remote data fetch, cluster\nselection of copies affects execution quality, as different insuring plans\ndrive different revenues. For providing On-Line-Real-Time analysis results, a\nsystem needs to insure the geo-distributed resource for the arriving jobs. Our\nchallenge is to achieve the optimal revenue by dynamically weighing the gains\ndue to insurance against the loss of occupying extra resource for insuring.\n  To this end, we design PingAn, an online insurance algorithm promising\n$(1+\\varepsilon)\\!\\!-\\!speed \\\no(\\frac{1}{\\varepsilon^2+\\varepsilon})\\!\\!-\\!competitive$ in sum of the job\nflowtimes via cross-cluster copying for tasks. PingAn shares resource among the\nanterior fraction of jobs with the least unprocessed datasize and the fraction\nis adjustable to fit the system load condition. After sharing, PingAn\nconcretely insures for tasks following efficiency-first reliability-aware\nprinciple to optimize the revenue of copies on jobs' performance. Trace-driven\nsimulations demonstrate that PingAn can reduce the average job flowtimes by at\nleast $14\\%$ than the state-of-the-art speculation mechanisms. We also build\nPingAn in Spark on Yarn System to verify its practicality and generality.\nExperiments show that PingAn can reduce the average job flowtimes by up to\n$40\\%$ comparing to the default Spark execution.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 05:16:35 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 01:39:52 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Wang", "Tiantian", ""], ["Qian", "Zhuzhong", ""], ["Lu", "Sanglu", ""]]}, {"id": "1804.02917", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall and Fr\\'ed\\'eric Magniez", "title": "Sublinear-Time Quantum Computation of the Diameter in CONGEST Networks", "comments": "21 pages; to appear in PODC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the diameter is one of the most central problems in\ndistributed computation. In the standard CONGEST model, in which two adjacent\nnodes can exchange $O(\\log n)$ bits per round (here $n$ denotes the number of\nnodes of the network), it is known that exact computation of the diameter\nrequires $\\tilde \\Omega(n)$ rounds, even in networks with constant diameter. In\nthis paper we investigate quantum distributed algorithms for this problem in\nthe quantum CONGEST model, where two adjacent nodes can exchange $O(\\log n)$\nquantum bits per round. Our main result is a $\\tilde O(\\sqrt{nD})$-round\nquantum distributed algorithm for exact diameter computation, where $D$ denotes\nthe diameter. This shows a separation between the computational power of\nquantum and classical algorithms in the CONGEST model. We also show an\nunconditional lower bound $\\tilde \\Omega(\\sqrt{n})$ on the round complexity of\nany quantum algorithm computing the diameter, and furthermore show a tight\nlower bound $\\tilde \\Omega(\\sqrt{nD})$ for any distributed quantum algorithm in\nwhich each node can use only $\\textrm{poly}(\\log n)$ quantum bits of memory.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 11:24:24 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 00:12:56 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1804.02963", "submitter": "Mehdi Fatan Serj", "authors": "Mahnaz Khojand, Mehdi Fatan Serj, Sevin Ashrafi, Vahideh Namaki", "title": "Predicting Dynamic Replication based on Fuzzy System in Data Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data grid replication is an effective method to achieve efficient and fault\ntolerant data access while reducing access latency and bandwidth consumption in\ngrids. Since we have storage limitation, a replica should be created in the\nbest site. Through evaluation of previously suggested algorithms, we understand\nthat by blind creation of replications on different sites after each demand, we\nmay be able to improve algorithm regarding response time. In practice, however,\nmost of the created replications will never be used and existing resources in\nGrid will be wasted through the creation of unused replications. In this paper,\nwe propose a new dynamic replication algorithm called Predictive Fuzzy\nReplication (PFR). PFR not only redefines the Balanced Ant Colony Optimization\n(BACO) algorithm, which is used for job scheduling in grids, but also uses it\nfor replication in appropriate sites in the data grid. The new algorithm\nconsiders the history usage of files, files size, the level of the sites and\nfree available space for replication and tries to predict future needs and pre\nreplicates them in the resources that are more suitable or decides which\nreplica should be deleted if there is not enough space for replicating. This\nalgorithm considers the related files of the replicated file and replicates\nthem considering their own history. PFR acts more efficiently than Cascading\nmethod, which is one of the algorithms in optimized use of existing replicas.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 13:16:15 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 17:41:55 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Khojand", "Mahnaz", ""], ["Serj", "Mehdi Fatan", ""], ["Ashrafi", "Sevin", ""], ["Namaki", "Vahideh", ""]]}, {"id": "1804.03256", "submitter": "Martin Wilhelm", "authors": "Martin Wilhelm", "title": "Restructuring expression dags for efficient parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the field of robust geometric computation it is often necessary to make\nexact decisions based on inexact floating-point arithmetic. One common approach\nis to store the computation history in an arithmetic expression dag and to\nre-evaluate the expression with increasing precision until an exact decision\ncan be made. We show that exact-decisions number types based on expression dags\ncan be evaluated faster in practice through parallelization on multiple cores.\nWe compare the impact of several restructuring methods for the expression dag\non its running time in a parallel environment.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 22:08:13 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Wilhelm", "Martin", ""]]}, {"id": "1804.03326", "submitter": "Zhe Zhang", "authors": "Guilherme Amadio, Brian Bockelman, Philippe Canal, Danilo Piparo,\n  Enric Tejedor, Zhe Zhang", "title": "Increasing Parallelism in the ROOT I/O Subsystem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When processing large amounts of data, the rate at which reading and writing\ncan take place is a critical factor. High energy physics data processing\nrelying on ROOT is no exception. The recent parallelisation of LHC experiments'\nsoftware frameworks and the analysis of the ever increasing amount of collision\ndata collected by experiments further emphasized this issue underlying the need\nof increasing the implicit parallelism expressed within the ROOT I/O. In this\ncontribution we highlight the improvements of the ROOT I/O subsystem which\ntargeted a satisfactory scaling behaviour in a multithreaded context. The\neffect of parallelism on the individual steps which are chained by ROOT to read\nand write data, namely (de)compression, (de)serialisation, access to storage\nbackend, are discussed. Performance measurements are discussed through real\nlife examples coming from CMS production workflows on traditional server\nplatforms and highly parallel architectures such as Intel Xeon Phi.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 03:22:20 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Amadio", "Guilherme", ""], ["Bockelman", "Brian", ""], ["Canal", "Philippe", ""], ["Piparo", "Danilo", ""], ["Tejedor", "Enric", ""], ["Zhang", "Zhe", ""]]}, {"id": "1804.03327", "submitter": "Carl Yang", "authors": "Carl Yang, Aydin Buluc, John D. Owens", "title": "Implementing Push-Pull Efficiently in GraphBLAS", "comments": "11 pages, 7 figures, International Conference on Parallel Processing\n  (ICPP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We factor Beamer's push-pull, also known as direction-optimized\nbreadth-first-search (DOBFS) into 3 separable optimizations, and analyze them\nfor generalizability, asymptotic speedup, and contribution to overall speedup.\nWe demonstrate that masking is critical for high performance and can be\ngeneralized to all graph algorithms where the sparsity pattern of the output is\nknown a priori. We show that these graph algorithm optimizations, which\ntogether constitute DOBFS, can be neatly and separably described using linear\nalgebra and can be expressed in the GraphBLAS linear-algebra-based framework.\nWe provide experimental evidence that with these optimizations, a DOBFS\nexpressed in a linear-algebra-based graph framework attains competitive\nperformance with state-of-the-art graph frameworks on the GPU and on a\nmulti-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 03:33:03 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 18:46:27 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 16:54:00 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Yang", "Carl", ""], ["Buluc", "Aydin", ""], ["Owens", "John D.", ""]]}, {"id": "1804.03436", "submitter": "Mauro Ianni", "authors": "Romolo Marotta, Mauro Ianni, Alessandro Pellegrini, Andrea Scarselli,\n  Francesco Quaglia", "title": "A Non-blocking Buddy System for Scalable Memory Allocation on Multi-core\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common implementations of core memory allocation components, like the Linux\nbuddy system, handle concurrent allocation/release requests by synchronizing\nthreads via spin-locks. This approach is clearly not prone to scale with large\nthread counts, a problem that has been addressed in the literature by\nintroducing layered allocation services or replicating the core allocators-the\nbottom most ones within the layered architecture. Both these solutions tend to\nreduce the pressure of actual concurrent accesses to each individual core\nallocator. In this article we explore an alternative approach to scalability of\nmemory allocation/release, which can be still combined with those literature\nproposals. Conflict detection relies on conventional atomic machine\ninstructions in the Read-Modify-Write (RMW) class. Furthermore, beyond\nimproving scalability and performance, it can also avoid wasting clock cycles\nfor spin-lock operations by threads that could in principle carry out their\nmemory allocation/release in full concurrency. Thus, it is resilient to\nperformance degradation---in face of concurrent accesses---independently of the\ncurrent level of fragmentation of the handled memory blocks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:25:16 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 12:12:08 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Marotta", "Romolo", ""], ["Ianni", "Mauro", ""], ["Pellegrini", "Alessandro", ""], ["Scarselli", "Andrea", ""], ["Quaglia", "Francesco", ""]]}, {"id": "1804.03441", "submitter": "Andrea Biagioni", "authors": "Roberto Ammendola, Andrea Biagioni, Fabrizio Capuani, Paolo Cretaro,\n  Giulia De Bonis, Francesca Lo Cicero, Alessandro Lonardo, Michele Martinelli,\n  Pier Stanislao Paolucci, Elena Pastorelli, Luca Pontisso, Francesco Simula,\n  Piero Vicini", "title": "The Brain on Low Power Architectures - Efficient Simulation of Cortical\n  Slow Waves and Asynchronous States", "comments": null, "journal-ref": "(2018) Advances in Parallel Computing, 32, pp. 760-769", "doi": "10.3233/978-1-61499-843-3-760", "report-no": null, "categories": "cs.DC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient brain simulation is a scientific grand challenge, a\nparallel/distributed coding challenge and a source of requirements and\nsuggestions for future computing architectures. Indeed, the human brain\nincludes about 10^15 synapses and 10^11 neurons activated at a mean rate of\nseveral Hz. Full brain simulation poses Exascale challenges even if simulated\nat the highest abstraction level. The WaveScalES experiment in the Human Brain\nProject (HBP) has the goal of matching experimental measures and simulations of\nslow waves during deep-sleep and anesthesia and the transition to other brain\nstates. The focus is the development of dedicated large-scale\nparallel/distributed simulation technologies. The ExaNeSt project designs an\nARM-based, low-power HPC architecture scalable to million of cores, developing\na dedicated scalable interconnect system, and SWA/AW simulations are included\namong the driving benchmarks. At the joint between both projects is the INFN\nproprietary Distributed and Plastic Spiking Neural Networks (DPSNN) simulation\nengine. DPSNN can be configured to stress either the networking or the\ncomputation features available on the execution platforms. The simulation\nstresses the networking component when the neural net - composed by a\nrelatively low number of neurons, each one projecting thousands of synapses -\nis distributed over a large number of hardware cores. When growing the number\nof neurons per core, the computation starts to be the dominating component for\nshort range connections. This paper reports about preliminary performance\nresults obtained on an ARM-based HPC prototype developed in the framework of\nthe ExaNeSt project. Furthermore, a comparison is given of instantaneous power,\ntotal energy consumption, execution time and energetic cost per synaptic event\nof SWA/AW DPSNN simulations when executed on either ARM- or Intel-based server\nplatforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:39:25 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Capuani", "Fabrizio", ""], ["Cretaro", "Paolo", ""], ["De Bonis", "Giulia", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Paolucci", "Pier Stanislao", ""], ["Pastorelli", "Elena", ""], ["Pontisso", "Luca", ""], ["Simula", "Francesco", ""], ["Vicini", "Piero", ""]]}, {"id": "1804.03582", "submitter": "Fabian Reiter", "authors": "Olivier Carton, Bruno Guillon, and Fabian Reiter", "title": "Counter Machines and Distributed Automata: A Story about Exchanging\n  Space and Time", "comments": "15 pages (+ 13 pages of appendices), 5 figures; To appear in the\n  proceedings of AUTOMATA 2018;", "journal-ref": null, "doi": "10.1007/978-3-319-92675-9_2", "report-no": null, "categories": "cs.FL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the equivalence of two classes of counter machines and one class of\ndistributed automata. Our counter machines operate on finite words, which they\nread from left to right while incrementing or decrementing a fixed number of\ncounters. The two classes differ in the extra features they offer: one allows\nto copy counter values, whereas the other allows to compute copyless sums of\ncounters. Our distributed automata, on the other hand, operate on directed path\ngraphs that represent words. All nodes of a path synchronously execute the same\nfinite-state machine, whose state diagram must be acyclic except for\nself-loops, and each node receives as input the state of its direct\npredecessor. These devices form a subclass of linear-time one-way cellular\nautomata.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:12:40 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Carton", "Olivier", ""], ["Guillon", "Bruno", ""], ["Reiter", "Fabian", ""]]}, {"id": "1804.03807", "submitter": "Jan Verschelde", "authors": "Jan Verschelde", "title": "A Blackbox Polynomial System Solver on Parallel Shared Memory Computers", "comments": "Accepted for publication in the proceedings of CASC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.SC math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A numerical irreducible decomposition for a polynomial system provides\nrepresentations for the irreducible factors of all positive dimensional\nsolution sets of the system, separated from its isolated solutions. Homotopy\ncontinuation methods are applied to compute a numerical irreducible\ndecomposition. Load balancing and pipelining are techniques in a parallel\nimplementation on a computer with multicore processors. The application of the\nparallel algorithms is illustrated on solving the cyclic $n$-roots problems, in\nparticular for $n = 8, 9$, and~12.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 04:41:49 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 19:57:36 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Verschelde", "Jan", ""]]}, {"id": "1804.03889", "submitter": "Jan Ko\\v{z}usznik", "authors": "Jan Ko\\v{z}usznik", "title": "Mobile Device Synchronisation with Central Database based on Data\n  Relevance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed applications are broadly used due the existence of mobile devices\nas are mobile phones, tablets and chrome books. They are often based on an\narchitecture client-server. A server part contains a central storage where all\napplication data are stored. A specific user accesses data using a client part\nof an application. The client part of the application can store data in its\nlocal storage to enable an offline mode. In the past, we were developing such a\nclient-server software system and we realized that it doesn't exists a general\nway how to define, search and provide into a client part only those data that\nare important or relevant to a particular user. The aim of this article is a\ndefinition of the problem and a creation of its general solution, not how to\nachieve the most effective and complete data synchronization. Complete\nsynchronization of data that has been already theoretically and practically\nsolved on a general level and it is impractical for our case\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 09:22:55 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Ko\u017eusznik", "Jan", ""]]}, {"id": "1804.03893", "submitter": "Andrea Biagioni", "authors": "Roberto Ammendola, Andrea Biagioni, Fabrizio Capuani, Paolo Cretaro,\n  Giulia De Bonis, Francesca Lo Cicero, Alessandro Lonardo, Michele Martinelli,\n  Pier Stanislao Paolucci, Elena Pastorelli, Luca Pontisso, Francesco Simula,\n  Piero Vicini", "title": "Large Scale Low Power Computing System - Status of Network Design in\n  ExaNeSt and EuroExa Projects", "comments": null, "journal-ref": "(2018) Advances in Parallel Computing, 32, pp. 750-759", "doi": "10.3233/978-1-61499-843-3-750", "report-no": null, "categories": "cs.DC cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of the next generation computing platform at ExaFlops scale\nrequires to solve new technological challenges mainly related to the impressive\nnumber (up to 10^6) of compute elements required. This impacts on system power\nconsumption, in terms of feasibility and costs, and on system scalability and\ncomputing efficiency. In this perspective analysis, exploration and evaluation\nof technologies characterized by low power, high efficiency and high degree of\ncustomization is strongly needed. Among the various European initiative\ntargeting the design of ExaFlops system, ExaNeSt and EuroExa are EU-H2020\nfunded initiatives leveraging on high end MPSoC FPGAs. Last generation MPSoC\nFPGAs can be seen as non-mainstream but powerful HPC Exascale enabling\ncomponents thanks to the integration of embedded multi-core, ARM-based low\npower CPUs and a huge number of hardware resources usable to co-design\napplication oriented accelerators and to develop a low latency high bandwidth\nnetwork architecture. In this paper we introduce ExaNet the FPGA-based,\nscalable, direct network architecture of ExaNeSt system. ExaNet allow us to\nexplore different interconnection topologies, to evaluate advanced routing\nfunctions for congestion control and fault tolerance and to design specific\nhardware components for acceleration of collective operations. After a brief\nintroduction of the motivations and goals of ExaNeSt and EuroExa projects, we\nwill report on the status of network architecture design and its\nhardware/software testbed adding preliminary bandwidth and latency\nachievements.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 09:32:02 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Capuani", "Fabrizio", ""], ["Cretaro", "Paolo", ""], ["De Bonis", "Giulia", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Paolucci", "Pier Stanislao", ""], ["Pastorelli", "Elena", ""], ["Pontisso", "Luca", ""], ["Simula", "Francesco", ""], ["Vicini", "Piero", ""]]}, {"id": "1804.03903", "submitter": "Gokhan Sagirlar Dr", "authors": "Gokhan Sagirlar, Barbara Carminati, Elena Ferrari, John D. Sheehan,\n  Emanuele Ragnoli", "title": "Hybrid-IoT: Hybrid Blockchain Architecture for Internet of Things - PoW\n  Sub-blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  From its early days the Internet of Things (IoT) has evolved into a\ndecentralized system of cooperating smart objects with the requirement, among\nothers, of achieving distributed consensus. Yet, current IoT platform solutions\nare centralized cloud based computing infrastructures, manifesting a number of\nsignificant disadvantages, such as, among others, high cloud server maintenance\ncosts, weakness for supporting time-critical IoT applications, security and\ntrust issues. Enabling blockchain technology into IoT can help to achieve a\nproper distributed consensus based IoT system that overcomes those\ndisadvantages. While this is an ideal match, it is still a challenging\nendeavor. In this paper we take a first step towards that goal by designing\nHybrid-IoT, a hybrid blockchain architecture for IoT. In Hybrid-IoT, subgroups\nof IoT devices form PoW blockchains, referred to as PoW sub-blockchains. Then,\nthe connection among the PoW sub-blockchains employs a BFT inter-connector\nframework, such as Polkadot or Cosmos. In this paper we focus on the PoW\nsub-blockchains formation, guided by a set of guidelines based on a set of\ndimensions, metrics and bounds. In order to prove the validity of the approach\nwe carry a performance and security evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 09:50:07 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 11:01:24 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 07:38:26 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Sagirlar", "Gokhan", ""], ["Carminati", "Barbara", ""], ["Ferrari", "Elena", ""], ["Sheehan", "John D.", ""], ["Ragnoli", "Emanuele", ""]]}, {"id": "1804.04031", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Sudarshan Raghunathan, Akshaya Annavajhala, Danil\n  Kirsanov, Eduardo de Leon, Eli Barzilay, Ilya Matiach, Joe Davison, Maureen\n  Busch, Miruna Oprescu, Ratan Sur, Roope Astala, Tong Wen, ChangYoung Park", "title": "Flexible and Scalable Deep Learning with MMLSpark", "comments": null, "journal-ref": "Proceedings of Machine Learning Research 82 (2017) 11-22, 4th\n  International Conference on Predictive Applications and APIs", "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we detail a novel open source library, called MMLSpark, that\ncombines the flexible deep learning library Cognitive Toolkit, with the\ndistributed computing framework Apache Spark. To achieve this, we have\ncontributed Java Language bindings to the Cognitive Toolkit, and added several\nnew components to the Spark ecosystem. In addition, we also integrate the\npopular image processing library OpenCV with Spark, and present a tool for the\nautomated generation of PySpark wrappers from any SparkML estimator and use\nthis tool to expose all work to the PySpark ecosystem. Finally, we provide a\nlarge library of tools for working and developing within the Spark ecosystem.\nWe apply this work to the automated classification of Snow Leopards from camera\ntrap images, and provide an end to end solution for the non-profit conservation\norganization, the Snow Leopard Trust.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 14:55:35 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Hamilton", "Mark", ""], ["Raghunathan", "Sudarshan", ""], ["Annavajhala", "Akshaya", ""], ["Kirsanov", "Danil", ""], ["de Leon", "Eduardo", ""], ["Barzilay", "Eli", ""], ["Matiach", "Ilya", ""], ["Davison", "Joe", ""], ["Busch", "Maureen", ""], ["Oprescu", "Miruna", ""], ["Sur", "Ratan", ""], ["Astala", "Roope", ""], ["Wen", "Tong", ""], ["Park", "ChangYoung", ""]]}, {"id": "1804.04149", "submitter": "Maged Eljazzar", "authors": "M. M. Eljazzar, and M. A. Amr, S. S. Kassem, and M. Ezzat", "title": "Merging supply chain and blockchain technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology has been playing a major role in our lives. One definition for\ntechnology is all the knowledge, products, processes, tools,methods and systems\nemployed in the creation of goods or in providing services.This makes\ntechnological innovations raise the competitiveness between organizations that\ndepend on supply chain and logistics in the global market. With increasing\ncompetitiveness, new challenges arise due to lack of information and assets\ntractability. This paper introduces three scenarios for solving these\nchallenges using the Blockchain technology. In this work, Blockchain technology\ntargets two main issues within the supply chain, namely, data transparency and\nresource sharing. These issues are reflected into the organizations strategies\nand plans.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 09:15:19 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 09:37:58 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Eljazzar", "M. M.", ""], ["Amr", "M. A.", ""], ["Kassem", "S. S.", ""], ["Ezzat", "M.", ""]]}, {"id": "1804.04178", "submitter": "Mahdi Boroujeni", "authors": "Mahdi Boroujeni and Soheil Ehsani and Mohammad Ghodsi and\n  MohammadTaghi HajiAghayi and Saeed Seddighin", "title": "Approximating Edit Distance in Truly Subquadratic Time: Quantum and\n  MapReduce", "comments": "A preliminary version of this paper was presented at SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance between two strings is defined as the smallest number of\ninsertions, deletions, and substitutions that need to be made to transform one\nof the strings to another one. Approximating edit distance in subquadratic time\nis \"one of the biggest unsolved problems in the field of combinatorial pattern\nmatching\". Our main result is a quantum constant approximation algorithm for\ncomputing the edit distance in truly subquadratic time. More precisely, we give\nan $O(n^{1.858})$ quantum algorithm that approximates the edit distance within\na factor of $7$. We further extend this result to an $O(n^{1.781})$ quantum\nalgorithm that approximates the edit distance within a larger constant factor.\n  Our solutions are based on a framework for approximating edit distance in\nparallel settings. This framework requires as black box an algorithm that\ncomputes the distances of several smaller strings all at once. For a quantum\nalgorithm, we reduce the black box to \\textit{metric estimation} and provide\nefficient algorithms for approximating it. We further show that this framework\nenables us to approximate edit distance in distributed settings. To this end,\nwe provide a MapReduce algorithm to approximate edit distance within a factor\nof $3$, with sublinearly many machines and sublinear memory. Also, our\nalgorithm runs in a logarithmic number of rounds.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 19:20:04 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 20:43:21 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Boroujeni", "Mahdi", ""], ["Ehsani", "Soheil", ""], ["Ghodsi", "Mohammad", ""], ["HajiAghayi", "MohammadTaghi", ""], ["Seddighin", "Saeed", ""]]}, {"id": "1804.04481", "submitter": "Nils-Arne Dreier", "authors": "Christian Engwer, Mirco Altenbernd, Nils-Arne Dreier, Dominik\n  G\\\"oddeke", "title": "A high-level C++ approach to manage local errors, asynchrony and faults\n  in an MPI application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C++ advocates exceptions as the preferred way to handle unexpected behaviour\nof an implementation in the code. This does not integrate well with the error\nhandling of MPI, which more or less always results in program termination in\ncase of MPI failures. In particular, a local C++ exception can currently lead\nto a deadlock due to unfinished communication requests on remote hosts. At the\nsame time, future MPI implementations are expected to include an API to\ncontinue computations even after a hard fault (node loss), i.e. the worst\npossible unexpected behaviour.\n  In this paper we present an approach that adds extended exception propagation\nsupport to C++ MPI programs. Our technique allows to propagate local exceptions\nto remote hosts to avoid deadlocks, and to map MPI failures on remote hosts to\nlocal exceptions. A use case of particular interest are asynchronous 'local\nfailure local recovery' resilience approaches. Our prototype implementation\nuses MPI-3.0 features only. In addition we present a dedicated implementation,\nwhich integrates seamlessly with MPI-ULFM, i.e. the most prominent proposal for\nextending MPI towards fault tolerance.\n  Our implementation is available at\nhttps://gitlab.dune-project.org/christi/test-mpi-exceptions .\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 12:55:42 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 07:17:02 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Engwer", "Christian", ""], ["Altenbernd", "Mirco", ""], ["Dreier", "Nils-Arne", ""], ["G\u00f6ddeke", "Dominik", ""]]}, {"id": "1804.04659", "submitter": "Daning Cheng", "authors": "Cheng Daning, Xia Fen, Li Shigang, Zhang Yunquan", "title": "Asynch-SGBDT: Asynchronous Parallel Stochastic Gradient Boosting\n  Decision Tree based on Parameters Server", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In AI research and industry, machine learning is the most widely used tool.\nOne of the most important machine learning algorithms is Gradient Boosting\nDecision Tree, i.e. GBDT whose training process needs considerable\ncomputational resources and time. To shorten GBDT training time, many works\ntried to apply GBDT on Parameter Server. However, those GBDT algorithms are\nsynchronous parallel algorithms which fail to make full use of Parameter\nServer. In this paper, we examine the possibility of using asynchronous\nparallel methods to train GBDT model and name this algorithm as asynch-SGBDT\n(asynchronous parallel stochastic gradient boosting decision tree). Our\ntheoretical and experimental results indicate that the scalability of\nasynch-SGBDT is influenced by the sample diversity of datasets, sampling rate,\nstep length and the setting of GBDT tree. Experimental results also show\nasynch-SGBDT training process reaches a linear speedup in asynchronous parallel\nmanner when datasets and GBDT trees meet high scalability requirements.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 14:06:05 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 04:26:26 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 01:57:44 GMT"}, {"version": "v4", "created": "Thu, 18 Jul 2019 06:50:05 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Daning", "Cheng", ""], ["Fen", "Xia", ""], ["Shigang", "Li", ""], ["Yunquan", "Zhang", ""]]}, {"id": "1804.04731", "submitter": "Robail Yasrab Dr.", "authors": "Robail Yasrab", "title": "MPSM: Multi-prospective PaaS Security Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has brought a revolution in the field of information\ntechnology and improving the efficiency of computational resources. It offers\ncomputing as a service enabling huge cost and resource efficiency. Despite its\nadvantages, certain security issues still hinder organizations and enterprises\nfrom it being adopted. This study mainly focused on the security of\nPlatform-as-a-Service (PaaS) as well as the most critical security issues that\nwere documented regarding PaaS infrastructure. The prime outcome of this study\nwas a security model proposed to mitigate security vulnerabilities of PaaS.\nThis security model consists of a number of tools, techniques and guidelines to\nmitigate and neutralize security issues of PaaS. The security vulnerabilities\nalong with mitigation strategies were discussed to offer a deep insight into\nPaaS security for both vendor and client that may facilitate future design to\nimplement secure PaaS platforms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 21:13:20 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Yasrab", "Robail", ""]]}, {"id": "1804.04737", "submitter": "Demetrios Coutinho Mr.", "authors": "Demetrios Coutinho, Felipe O. Lins e Silva, Daniel Aloise, Samuel and\n  Xavier-de-Souza", "title": "A Scalable Shared-Memory Parallel Simplex for Large-Scale Linear\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Simplex tableau has been broadly used and investigated in the industry\nand academia. With the advent of the big data era, ever larger problems are\nposed to be solved in ever larger machines whose architecture type did not\nexist in the conception of this algorithm. In this paper, we present a\nshared-memory parallel implementation of the Simplex tableau algorithm for\ndense large-scale Linear Programming (LP) problems for use in modern multi-core\narchitectures. We present the general scheme and explain the strategies taken\nto parallelize each step of the standard simplex algorithm, emphasizing the\nsolutions found to solve performance bottlenecks. We analyzed the speedup and\nthe parallel efficiency for the proposed implementation relative to the\nstandard Simplex algorithm using a shared-memory system with 64 processing\ncores. The experiments were performed for several different problems, with up\nto 8192 variables and constraints, in their primal and dual formulations. The\nresults show that the performance is mostly much better when we use the\nformulation with more variables than inequality constraints. Also, they show\nthat the parallelization strategies applied to avoid bottlenecks lead the\nimplementation to scale well with the problem size and the core count up to a\ncertain limit of problem size. Further analysis showed that this scaling limit\nwas an effect of resource limitation. Even though, our implementation was able\nto reach speedups in the order of 19x.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 21:59:40 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 19:52:48 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Coutinho", "Demetrios", ""], ["Silva", "Felipe O. Lins e", ""], ["Aloise", "Daniel", ""], ["Samuel", "", ""], ["Xavier-de-Souza", "", ""]]}, {"id": "1804.04773", "submitter": "Warut Suksompong", "authors": "Warut Suksompong, Charles E. Leiserson, Tao B. Schardl", "title": "On the Efficiency of Localized Work Stealing", "comments": "13 pages, 1 figure", "journal-ref": "Information Processing Letters, 116(2):100-106 (2016)", "doi": "10.1016/j.ipl.2015.10.002", "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a variant of the work-stealing algorithm that we call\nthe localized work-stealing algorithm. The intuition behind this variant is\nthat because of locality, processors can benefit from working on their own\nwork. Consequently, when a processor is free, it makes a steal attempt to get\nback its own work. We call this type of steal a steal-back. We show that the\nexpected running time of the algorithm is $T_1/P+O(T_\\infty P)$, and that under\nthe \"even distribution of free agents assumption\", the expected running time of\nthe algorithm is $T_1/P+O(T_\\infty\\lg P)$. In addition, we obtain another\nrunning-time bound based on ratios between the sizes of serial tasks in the\ncomputation. If $M$ denotes the maximum ratio between the largest and the\nsmallest serial tasks of a processor after removing a total of $O(P)$ serial\ntasks across all processors from consideration, then the expected running time\nof the algorithm is $T_1/P+O(T_\\infty M)$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 02:17:57 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Suksompong", "Warut", ""], ["Leiserson", "Charles E.", ""], ["Schardl", "Tao B.", ""]]}, {"id": "1804.04806", "submitter": "Yosuke Oyama", "authors": "Yosuke Oyama, Tal Ben-Nun, Torsten Hoefler, Satoshi Matsuoka", "title": "{\\mu}-cuDNN: Accelerating Deep Learning Frameworks with Micro-Batching", "comments": "11 pages, 14 figures. Part of the content have been published in IPSJ\n  SIG Technical Report, Vol. 2017-HPC-162, No. 22, pp. 1-9, 2017. (DOI:\n  http://id.nii.ac.jp/1001/00184814)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NVIDIA cuDNN is a low-level library that provides GPU kernels frequently used\nin deep learning. Specifically, cuDNN implements several equivalent convolution\nalgorithms, whose performance and memory footprint may vary considerably,\ndepending on the layer dimensions. When an algorithm is automatically selected\nby cuDNN, the decision is performed on a per-layer basis, and thus it often\nresorts to slower algorithms that fit the workspace size constraints. We\npresent {\\mu}-cuDNN, a transparent wrapper library for cuDNN, which divides\nlayers' mini-batch computation into several micro-batches. Based on Dynamic\nProgramming and Integer Linear Programming, {\\mu}-cuDNN enables faster\nalgorithms by decreasing the workspace requirements. At the same time,\n{\\mu}-cuDNN keeps the computational semantics unchanged, so that it decouples\nstatistical efficiency from the hardware efficiency safely. We demonstrate the\neffectiveness of {\\mu}-cuDNN over two frameworks, Caffe and TensorFlow,\nachieving speedups of 1.63x for AlexNet and 1.21x for ResNet-18 on P100-SXM2\nGPU. These results indicate that using micro-batches can seamlessly increase\nthe performance of deep learning, while maintaining the same memory footprint.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 07:20:44 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Oyama", "Yosuke", ""], ["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1804.05039", "submitter": "Robail Yasrab Dr.", "authors": "Robail Yasrab", "title": "Mitigating Docker Security Issues", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is very easy to run applications in Docker. Docker offers an ecosystem\nthat offers a platform for application packaging, distributing and managing\nwithin containers. However, Docker platform is yet not matured. Presently,\nDocker is less secured as compare to virtual machines (VM) and most of the\nother cloud technologies. The key of reason of Docker inadequate security\nprotocols is containers sharing of Linux kernel, which can lead to risk of\nprivileged escalations. This research is going to outline some major security\nvulnerabilities at Docker and counter solutions to neutralize such attacks.\nThere are variety of security attacks like insider and outsider. This research\nwill outline both types of attacks and their mitigations strategies. Taking\nsome precautionary measures can save from huge disasters. This research will\nalso present Docker secure deployment guidelines. These guidelines will suggest\ndifferent configurations to deploy Docker containers in a more secure way.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 17:10:17 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Yasrab", "Robail", ""]]}, {"id": "1804.05271", "submitter": "Shiqiang Wang", "authors": "Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung,\n  Christian Makaya, Ting He, Kevin Chan", "title": "Adaptive Federated Learning in Resource Constrained Edge Computing\n  Systems", "comments": "This version (excluding appendices) has been accepted for publication\n  in the IEEE Journal on Selected Areas in Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging technologies and applications including Internet of Things (IoT),\nsocial networking, and crowd-sourcing generate large amounts of data at the\nnetwork edge. Machine learning models are often built from the collected data,\nto enable the detection, classification, and prediction of future events. Due\nto bandwidth, storage, and privacy concerns, it is often impractical to send\nall the data to a centralized location. In this paper, we consider the problem\nof learning model parameters from data distributed across multiple edge nodes,\nwithout sending raw data to a centralized place. Our focus is on a generic\nclass of machine learning models that are trained using gradient-descent based\napproaches. We analyze the convergence bound of distributed gradient descent\nfrom a theoretical point of view, based on which we propose a control algorithm\nthat determines the best trade-off between local update and global parameter\naggregation to minimize the loss function under a given resource budget. The\nperformance of the proposed algorithm is evaluated via extensive experiments\nwith real datasets, both on a networked prototype system and in a larger-scale\nsimulated environment. The experimentation results show that our proposed\napproach performs near to the optimum with various machine learning models and\ndifferent data distributions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 20:21:48 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 21:24:35 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 03:42:14 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wang", "Shiqiang", ""], ["Tuor", "Tiffany", ""], ["Salonidis", "Theodoros", ""], ["Leung", "Kin K.", ""], ["Makaya", "Christian", ""], ["He", "Ting", ""], ["Chan", "Kevin", ""]]}, {"id": "1804.05300", "submitter": "Ashraf Shahin", "authors": "Ashraf A. Shahin", "title": "A Collective Neurodynamic Approach to Survivable Virtual Network\n  Embedding", "comments": null, "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), 9(3), 2018", "doi": "10.14569/IJACSA.2018.090309", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network virtualization has attracted significant amount of attention in the\nlast few years as one of the key features of cloud computing. Network\nvirtualization allows multiple virtual networks to share physical resources of\nsingle substrate network. However, sharing substrate network resources\nincreases impact of single substrate resource failure. One of the commonly\napplied mechanisms to protect against such failures is provisioning redundant\nsubstrate resources for each virtual network to be used to recover affected\nvirtual resources. However, redundant resources decreases cloud revenue by\nincreasing virtual network embedding cost. In this paper, a collective\nneurodynamic approach has been proposed to reduce amount of provisioned\nredundant resources and reduce cost of embedding virtual networks. The proposed\napproach has been evaluated by using simulation and compared against some\nexisting survivable virtual network embedding techniques.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 03:07:41 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Shahin", "Ashraf A.", ""]]}, {"id": "1804.05349", "submitter": "Jerzy Proficz", "authors": "Jerzy Proficz", "title": "Improving all-reduce collective operations for imbalanced process\n  arrival patterns", "comments": "Published in Journal of Supercomputing, 22 pages, 12 figures and 5\n  tables", "journal-ref": null, "doi": "10.1007/s11227-018-2356-z", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two new algorithms for the all-reduce operation, optimized for imbalanced\nprocess arrival patterns (PAPs) are presented: (i) sorted linear tree (SLT),\n(ii) pre-reduced ring (PRR) as well as a new way of on-line PAP detection,\nincluding process arrival time (PAT) estimations and their distribution between\ncooperating processes was introduced. The idea, pseudo-code, implementation\ndetails, benchmark for performance evaluation and a real case example for\nmachine learning are provided. The results of the experiments were described\nand analyzed, showing that the proposed solution has high scalability and\nimproved performance in comparison with the usually used ring and Rabenseifner\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 12:34:35 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Proficz", "Jerzy", ""]]}, {"id": "1804.05358", "submitter": "Suzhen Wang", "authors": "Suzhen Wang, Jingjing Luo, Yuan-Hsun Lo, Wing Shing Wong", "title": "Forwarding and Optical Indices in an All-Optical BCube Network", "comments": "8 pages, 4 figures, a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BCube is a highly scalable and cost-effective networking topology, which has\nbeen widely applied to modular datacenters. Optical technologies based on\nWavelength Division Multiplexing (WDM) are gaining popularity for Data Center\nNetworks (DCNs) due to their technological strengths such as low communication\nlatency, low power consumption, and high link bandwidth. Therefore, it is worth\ninvestigating optical techniques into the BCube architecture for future DCNs.\nFor this purpose, we study the forwarding and optical indices in an all-optical\nBCube network. Consider an all-optical BCube network in which every host sets\nup a connection with every other host. The optical index is the minimum number\nof wavelengths required by the network to support such a host-to-host traffic,\nunder the restriction that each connection is assigned a wavelength that\nremains constant in the network. A routing is a set of directed paths specified\nfor all host pairs. By defining the maximum link load of a routing as the\nmaximum number of paths passing through any link, the forwarding index is\nmeasured to be the minimum of maximum link load over all possible routings. The\nforwarding index turns out to be a natural lower bound of the optical index. In\nthis paper, we first compute the forwarding index of an all-optical BCube\nnetwork. Then, we derive an upper bound of the optical index by providing an\noblivious routing and wavelength assignment (RWA) schemes, which attains the\nlower bound given by the forwarding index in some small cases. Finally, a\ntighter upper bound is obtained by means of the chromatic numbers in Graph\nTheory.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 13:44:59 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Wang", "Suzhen", ""], ["Luo", "Jingjing", ""], ["Lo", "Yuan-Hsun", ""], ["Wong", "Wing Shing", ""]]}, {"id": "1804.05395", "submitter": "Jay Billings", "authors": "Jay Jay Billings", "title": "Applying Distributed Ledgers to Manage Workflow Provenance", "comments": "Submitted to P-RECS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing provenance across workflow management systems automatically is not\ncurrently possible, but the value of such a capability is high since it could\ngreatly reduce the amount of duplicated workflows, accelerate the discovery of\nnew knowledge, and verify the integrity of past and present analyses. Although\nnumerous technological challenges exist to efficiently share provenance\ninformation across workflow management systems, permissioned distributed\nledgers could surmount many of them. The primary benefit of permissioned\ndistributed ledgers over other technologies is that their distribution is over\na peer-to-peer network that encodes transactions across the network into an\nimmutable hash list and achieves consensus on the validity of the new data\nthrough a common consensus mechanism. This work discusses provenance and\ndistributed ledgers on their own and then presents an argument that distributed\nledgers naturally satisfy many of the requirements of workflow provenance, that\nprovenance information can exist in the ledger in multiple ways, and that a\nnumber of novel research areas exist based on this strategy.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 17:57:09 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Billings", "Jay Jay", ""]]}, {"id": "1804.05441", "submitter": "Udit Agarwal", "authors": "Udit Agarwal, Vijaya Ramachandran, Valerie King, Matteo Pontecorvi", "title": "A Deterministic Distributed Algorithm for Exact Weighted All-Pairs\n  Shortest Paths in $\\tilde{O}(n^{3/2})$ Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic distributed algorithm to compute all-pairs\nshortest paths(APSP) in an edge-weighted directed or undirected graph. Our\nalgorithm runs in $\\tilde{O}(n^{3/2})$ rounds in the Congest model, where $n$\nis the number of nodes in the graph. This is the first $o(n^2)$ rounds\ndeterministic distributed algorithm for the weighted APSP problem. Our\nalgorithm is fairly simple and incorporates a deterministic distributed\nalgorithm we develop for computing a `blocker set' \\cite{King99}, which has\nbeen used earlier in sequential dynamic computation of APSP.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 22:17:54 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""], ["King", "Valerie", ""], ["Pontecorvi", "Matteo", ""]]}, {"id": "1804.05554", "submitter": "Bert Moons", "authors": "Bert Moons, Daniel Bankman, Lita Yang, Boris Murmann, Marian Verhelst", "title": "BinarEye: An Always-On Energy-Accuracy-Scalable Binary CNN Processor\n  With All Memory On Chip in 28nm CMOS", "comments": "Presented at the 2018 IEEE Custom Integrated Circuits Conference\n  (CICC). Presentation is available here:\n  https://www.researchgate.net/publication/324452819_Presentation_on_Binareye_at_CICC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces BinarEye: a digital processor for always-on Binary\nConvolutional Neural Networks. The chip maximizes data reuse through a Neuron\nArray exploiting local weight Flip-Flops. It stores full network models and\nfeature maps and hence requires no off-chip bandwidth, which leads to a 230\n1b-TOPS/W peak efficiency. Its 3 levels of flexibility - (a) weight\nreconfiguration, (b) a programmable network depth and (c) a programmable\nnetwork width - allow trading energy for accuracy depending on the task's\nrequirements. BinarEye's full system input-to-label energy consumption ranges\nfrom 14.4uJ/f for 86% CIFAR-10 and 98% owner recognition down to 0.92uJ/f for\n94% face detection at up to 1700 frames per second. This is 3-12-70x more\nefficient than the state-of-the-art at on-par accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 08:51:29 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Moons", "Bert", ""], ["Bankman", "Daniel", ""], ["Yang", "Lita", ""], ["Murmann", "Boris", ""], ["Verhelst", "Marian", ""]]}, {"id": "1804.05635", "submitter": "Li-Hsing Yen", "authors": "Li-Hsing Yen and Guang-Hong Sun", "title": "Decentralized Combinatorial Auctions for Multi-Unit Resource Allocation", "comments": "Theorem 1 is questionable and needs revision. Others parts should\n  also be modified accorrdingly. Before the new version is ready, this version\n  should not be referred", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auction has been used to allocate resources or tasks to processes, machines\nor other autonomous entities in distributed systems. When different bidders\nhave different demands and valuations on different types of resources or tasks,\nthe auction becomes a combinatorial auction (CA), for which finding an optimal\nauction result that maximizes total winning bid is NP-hard. Many time-efficient\napproximations to this problem work with a bid ranking function (BRF). However,\nexisting approximations are all centralized and mostly for single-unit\nresource. In this paper, we propose the first decentralized CA schemes for\nmulti-unit resources. It includes a BRF-based winner determination scheme that\nenables every agent to locally compute a critical bid value for her to win the\nCA and accordingly take her best response to other agent's win declaration. It\nalso includes a critical-value-based pricing scheme for each winner to locally\ncompute her payment. We analyze stabilization, correctness, and consistency\nproperties of the proposed approach. Simulation results confirms that the\nproposed approach identifies exactly the same set of winners as the centralized\ncounterpart regardless of initial bid setting, but at the cost of lower total\nwinning bid and payment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 12:19:50 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 08:44:55 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Yen", "Li-Hsing", ""], ["Sun", "Guang-Hong", ""]]}, {"id": "1804.05714", "submitter": "Cunxi Yu", "authors": "Cunxi Yu and Houping Xiao and Giovanni De Micheli", "title": "Developing Synthesis Flows Without Human Knowledge", "comments": "To appear in 2018 55th Design Automation Conference (DAC'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design flows are the explicit combinations of design transformations,\nprimarily involved in synthesis, placement and routing processes, to accomplish\nthe design of Integrated Circuits (ICs) and System-on-Chip (SoC). Mostly, the\nflows are developed based on the knowledge of the experts. However, due to the\nlarge search space of design flows and the increasing design complexity,\ndeveloping Intellectual Property (IP)-specific synthesis flows providing high\nQuality of Result (QoR) is extremely challenging. This work presents a fully\nautonomous framework that artificially produces design-specific synthesis flows\nwithout human guidance and baseline flows, using Convolutional Neural Network\n(CNN). The demonstrations are made by successfully designing logic synthesis\nflows of three large scaled designs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 14:56:09 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 12:53:41 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 15:55:49 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Yu", "Cunxi", ""], ["Xiao", "Houping", ""], ["De Micheli", "Giovanni", ""]]}, {"id": "1804.05839", "submitter": "Jason (Jinquan) Dai", "authors": "Jason Dai, Yiheng Wang, Xin Qiu, Ding Ding, Yao Zhang, Yanzhang Wang,\n  Xianyan Jia, Cherry Zhang, Yan Wan, Zhichao Li, Jiao Wang, Shengsheng Huang,\n  Zhongyuan Wu, Yang Wang, Yuhao Yang, Bowen She, Dongjie Shi, Qi Lu, Kai\n  Huang, Guoqiong Song", "title": "BigDL: A Distributed Deep Learning Framework for Big Data", "comments": "In ACM Symposium of Cloud Computing conference (SoCC) 2019", "journal-ref": null, "doi": "10.1145/3357223.3362707", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents BigDL (a distributed deep learning framework for Apache\nSpark), which has been used by a variety of users in the industry for building\ndeep learning applications on production big data platforms. It allows deep\nlearning applications to run on the Apache Hadoop/Spark cluster so as to\ndirectly process the production data, and as a part of the end-to-end data\nanalysis pipeline for deployment and management. Unlike existing deep learning\nframeworks, BigDL implements distributed, data parallel training directly on\ntop of the functional compute model (with copy-on-write and coarse-grained\noperations) of Spark. We also share real-world experience and \"war stories\" of\nusers that have adopted BigDL to address their challenges(i.e., how to easily\nbuild end-to-end data analysis and deep learning pipelines for their production\ndata).\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 12:04:03 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 03:21:14 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 02:57:37 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2019 13:12:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dai", "Jason", ""], ["Wang", "Yiheng", ""], ["Qiu", "Xin", ""], ["Ding", "Ding", ""], ["Zhang", "Yao", ""], ["Wang", "Yanzhang", ""], ["Jia", "Xianyan", ""], ["Zhang", "Cherry", ""], ["Wan", "Yan", ""], ["Li", "Zhichao", ""], ["Wang", "Jiao", ""], ["Huang", "Shengsheng", ""], ["Wu", "Zhongyuan", ""], ["Wang", "Yang", ""], ["Yang", "Yuhao", ""], ["She", "Bowen", ""], ["Shi", "Dongjie", ""], ["Lu", "Qi", ""], ["Huang", "Kai", ""], ["Song", "Guoqiong", ""]]}, {"id": "1804.05890", "submitter": "Maotong Xu", "authors": "Maotong Xu, Sultan Alamro, Tian Lan, Suresh Subramaniam", "title": "Chronos: A Unifying Optimization Framework for Speculative Execution of\n  Deadline-critical MapReduce Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meeting desired application deadlines in cloud processing systems such as\nMapReduce is crucial as the nature of cloud applications is becoming\nincreasingly mission-critical and deadline-sensitive. It has been shown that\nthe execution times of MapReduce jobs are often adversely impacted by a few\nslow tasks, known as stragglers, which result in high latency and deadline\nviolations. While a number of strategies have been developed in existing work\nto mitigate stragglers by launching speculative or clone task attempts, none of\nthem provides a quantitative framework that optimizes the speculative execution\nfor offering guaranteed Service Level Agreements (SLAs) to meet application\ndeadlines. In this paper, we bring several speculative scheduling strategies\ntogether under a unifying optimization framework, called Chronos, which defines\na new metric, Probability of Completion before Deadlines (PoCD), to measure the\nprobability that MapReduce jobs meet their desired deadlines. We systematically\nanalyze PoCD for popular strategies including Clone, Speculative-Restart, and\nSpeculative-Resume, and quantify their PoCD in closed-form. The result\nilluminates an important tradeoff between PoCD and the cost of speculative\nexecution, measured by the total (virtual) machine time required under\ndifferent strategies. We propose an optimization problem to jointly optimize\nPoCD and execution cost in different strategies, and develop an algorithmic\nsolution that is guaranteed to be optimal. Chronos is prototyped on Hadoop\nMapReduce and evaluated against three baseline strategies using both\nexperiments and trace-driven simulations, achieving 50% net utility increase\nwith up to 80% PoCD and 88% cost improvements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 18:50:35 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Xu", "Maotong", ""], ["Alamro", "Sultan", ""], ["Lan", "Tian", ""], ["Subramaniam", "Suresh", ""]]}, {"id": "1804.06062", "submitter": "Alok Singh", "authors": "Alok Singh, Eric Stephan, Malachi Schram, Ilkay Altintas", "title": "Deep Learning on Operational Facility Data Related to Large-Scale\n  Distributed Area Scientific Workflows", "comments": null, "journal-ref": "2017 IEEE 13th International Conference on e-Science, 2017, pp.\n  586 to 591", "doi": "10.1109/eScience.2017.94", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distributed computing platforms provide a robust mechanism to perform\nlarge-scale computations by splitting the task and data among multiple\nlocations, possibly located thousands of miles apart geographically. Although\nsuch distribution of resources can lead to benefits, it also comes with its\nassociated problems such as rampant duplication of file transfers increasing\ncongestion, long job completion times, unexpected site crashing, suboptimal\ndata transfer rates, unpredictable reliability in a time range, and suboptimal\nusage of storage elements. In addition, each sub-system becomes a potential\nfailure node that can trigger system wide disruptions. In this vision paper, we\noutline our approach to leveraging Deep Learning algorithms to discover\nsolutions to unique problems that arise in a system with computational\ninfrastructure that is spread over a wide area. The presented vision, motivated\nby a real scientific use case from Belle II experiments, is to develop\nmultilayer neural networks to tackle forecasting, anomaly detection and\noptimization challenges in a complex and distributed data movement environment.\nThrough this vision based on Deep Learning principles, we aim to achieve\nreduced congestion events, faster file transfer rates, and enhanced site\nreliability.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 06:29:56 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 19:43:16 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Singh", "Alok", ""], ["Stephan", "Eric", ""], ["Schram", "Malachi", ""], ["Altintas", "Ilkay", ""]]}, {"id": "1804.06087", "submitter": "Wei Wang", "authors": "Wei Wang and Sheng Wang and Jinyang Gao and Meihui Zhang and Gang Chen\n  and Teck Khim Ng and Beng Chin Ooi", "title": "Rafiki: Machine Learning as an Analytics Service System", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics is gaining massive momentum in the last few years.\nApplying machine learning models to big data has become an implicit requirement\nor an expectation for most analysis tasks, especially on high-stakes\napplications.Typical applications include sentiment analysis against reviews\nfor analyzing on-line products, image classification in food logging\napplications for monitoring user's daily intake and stock movement prediction.\nExtending traditional database systems to support the above analysis is\nintriguing but challenging. First, it is almost impossible to implement all\nmachine learning models in the database engines. Second, expertise knowledge is\nrequired to optimize the training and inference procedures in terms of\nefficiency and effectiveness, which imposes heavy burden on the system users.\nIn this paper, we develop and present a system, called Rafiki, to provide the\ntraining and inference service of machine learning models, and facilitate\ncomplex analytics on top of cloud platforms. Rafiki provides distributed\nhyper-parameter tuning for the training service, and online ensemble modeling\nfor the inference service which trades off between latency and accuracy.\nExperimental results confirm the efficiency, effectiveness, scalability and\nusability of Rafiki.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 07:54:55 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wang", "Wei", ""], ["Wang", "Sheng", ""], ["Gao", "Jinyang", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Ng", "Teck Khim", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1804.06127", "submitter": "Vincenzo Bonifaci", "authors": "Luca Becchetti, Vincenzo Bonifaci, Emanuele Natale", "title": "Pooling or Sampling: Collective Dynamics for Electrical Flow Estimation", "comments": null, "journal-ref": "Proceedings of the 17th International Conference on Autonomous\n  Agents and MultiAgent Systems, AAMAS 2018", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of electrical flows is a crucial primitive for many recently\nproposed optimization algorithms on weighted networks. While typically\nimplemented as a centralized subroutine, the ability to perform this task in a\nfully decentralized way is implicit in a number of biological systems. Thus, a\nnatural question is whether this task can provably be accomplished in an\nefficient way by a network of agents executing a simple protocol.\n  We provide a positive answer, proposing two distributed approaches to\nelectrical flow computation on a weighted network: a deterministic process\nmimicking Jacobi's iterative method for solving linear systems, and a\nrandomized token diffusion process, based on revisiting a classical random walk\nprocess on a graph with an absorbing node. We show that both processes converge\nto a solution of Kirchhoff's node potential equations, derive bounds on their\nconvergence rates in terms of the weights of the network, and analyze their\ntime and message complexity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 09:37:10 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Becchetti", "Luca", ""], ["Bonifaci", "Vincenzo", ""], ["Natale", "Emanuele", ""]]}, {"id": "1804.06197", "submitter": "Moshe Sulamy", "authors": "Dor Bank, Moshe Sulamy, Eyal Waserman", "title": "Reaching Distributed Equilibrium with Limited ID Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the relation between the size of the id space and the number of\nrational agents in a network under which equilibrium in distributed algorithms\nis possible. When the number of agents in the network is not a-priori known, a\nsingle agent may duplicate to gain an advantage, pretending to be more than one\nagent. However, when the id space is limited, each duplication involves a risk\nof being caught. By comparing the risk against the advantage, given an id space\nof size $L$, we provide a method of calculating the minimal threshold $t$, the\nrequired number of agents in the network, such that the algorithm is in\nequilibrium. That is, it is the minimal value of $t$ such that if agents\na-priori know that $n \\geq t$ then the algorithm is in equilibrium. We\ndemonstrate this method by applying it to two problems, Leader Election and\nKnowledge Sharing, as well as providing a constant-time approximation $t\n\\approx \\frac{L}{5}$ of the minimal threshold for Leader Election.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 12:26:31 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 06:58:27 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Bank", "Dor", ""], ["Sulamy", "Moshe", ""], ["Waserman", "Eyal", ""]]}, {"id": "1804.06231", "submitter": "James Willis", "authors": "James S. Willis, Matthieu Schaller, Pedro Gonnet, Richard G. Bower,\n  Peter W. Draper", "title": "An Efficient SIMD Implementation of Pseudo-Verlet Lists for Neighbour\n  Interactions in Particle-Based Codes", "comments": "10 pages, 3 figures. Proceedings of the ParCo 2017 conference,\n  Bologna, Italy, September 12-15th, 2017", "journal-ref": "Advances in Parallel Computing, Volume 32: Parallel Computing is\n  Everywhere (2018)", "doi": "10.3233/978-1-61499-843-3-507", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In particle-based simulations, neighbour finding (i.e finding pairs of\nparticles to interact within a given range) is the most time consuming part of\nthe computation. One of the best such algorithms, which can be used for both\nMolecular Dynamics (MD) and Smoothed Particle Hydrodynamics (SPH) simulations,\nis the pseudo-Verlet list algorithm. This algorithm, however, does not\nvectorise trivially, and hence makes it difficult to exploit SIMD-parallel\narchitectures. In this paper, we present several novel modifications as well as\na vectorisation strategy for the algorithm which lead to overall speed-ups over\nthe scalar version of the algorithm of 2.24x for the AVX instruction set (SIMD\nwidth of 8), 2.43x for AVX2, and 4.07x for AVX-512 (SIMD width of 16).\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 13:31:28 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Willis", "James S.", ""], ["Schaller", "Matthieu", ""], ["Gonnet", "Pedro", ""], ["Bower", "Richard G.", ""], ["Draper", "Peter W.", ""]]}, {"id": "1804.06404", "submitter": "Saman Biookaghazadeh", "authors": "Saman Biookaghazadeh, Fengbo Ren, Ming Zhao", "title": "Are FPGAs Suitable for Edge Computing?", "comments": "6 pages, HotEdge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The rapid growth of Internet-of-things (IoT) and artificial intelligence\napplications have called forth a new computing paradigm--edge computing. In\nthis paper, we study the suitability of deploying FPGAs for edge computing from\nthe perspectives of throughput sensitivity to workload size, architectural\nadaptiveness to algorithm characteristics, and energy efficiency. This goal is\naccomplished by conducting comparison experiments on an Intel Arria 10 GX1150\nFPGA and an Nvidia Tesla K40m GPU. The experiment results imply that the key\nadvantages of adopting FPGAs for edge computing over GPUs are three-fold: 1)\nFPGAs can provide a consistent throughput invariant to the size of application\nworkload, which is critical to aggregating individual service requests from\nvarious IoT sensors; (2) FPGAs offer both spatial and temporal parallelism at a\nfine granularity and a massive scale, which guarantees a consistently high\nperformance for accelerating both high-concurrency and high-dependency\nalgorithms; and (3) FPGAs feature 3-4 times lower power consumption and up to\n30.7 times better energy efficiency, offering better thermal stability and\nlower energy cost per functionality.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 19:16:09 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Biookaghazadeh", "Saman", ""], ["Ren", "Fengbo", ""], ["Zhao", "Ming", ""]]}, {"id": "1804.06462", "submitter": "Christina Delimitrou", "authors": "Francisco Romero, Christina Delimitrou", "title": "Mage: Online Interference-Aware Scheduling in Multi-Scale Heterogeneous\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity has grown in popularity both at the core and server level as a\nway to improve both performance and energy efficiency. However, despite these\nbenefits, scheduling applications in heterogeneous machines remains\nchallenging. Additionally, when these heterogeneous resources accommodate\nmultiple applications to increase utilization, resources are prone to\ncontention, destructive interference, and unpredictable performance. Existing\nsolutions examine heterogeneity either across or within a server, leading to\nmissed performance and efficiency opportunities. We present Mage, a practical\ninterference-aware runtime that optimizes performance and efficiency in systems\nwith intra- and inter-server heterogeneity. Mage leverages fast and online data\nmining to quickly explore the space of application placements, and determine\nthe one that minimizes destructive interference between co-resident\napplications. Mage continuously monitors the performance of active\napplications, and, upon detecting QoS violations, it determines whether\nalternative placements would prove more beneficial, taking into account any\noverheads from migration. Across 350 application mixes on a heterogeneous CMP,\nMage improves performance by 38% and up to 2x compared to a greedy scheduler.\nAcross 160 mixes on a heterogeneous cluster, Mage improves performance by 30%\non average and up to 52% over the greedy scheduler, and by 11% over the\ncombination of Paragon [15] for inter- and intra-server heterogeneity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 20:30:16 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Romero", "Francisco", ""], ["Delimitrou", "Christina", ""]]}, {"id": "1804.06468", "submitter": "Chien-Sheng Yang", "authors": "Chien-Sheng Yang, Ramtin Pedarsani, A. Salman Avestimehr", "title": "Communication-Aware Scheduling of Serial Tasks for Dispersed Computing", "comments": "accepted to appear in IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in development of in-network dispersed computing\nparadigms that leverage the computing capabilities of heterogeneous resources\ndispersed across the network for processing massive amount of data is collected\nat the edge of the network. We consider the problem of task scheduling for such\nnetworks, in a dynamic setting in which arriving computation jobs are modeled\nas chains, with nodes representing tasks, and edges representing precedence\nconstraints among tasks. In our proposed model, motivated by significant\ncommunication costs in dispersed computing environments, the communication\ntimes are taken into account. More specifically, we consider a network where\nservers are capable of serving all task types, and sending the results of\nprocessed tasks from one server to another server results in some communication\ndelay that makes the design of optimal scheduling policy significantly more\nchallenging than classical queueing networks. As the main contributions of the\npaper, we first characterize the capacity region of the network, then propose a\nnovel virtual queueing network encoding the state of the network. Finally, we\npropose a Max-Weight type scheduling policy, and considering the virtual\nqueueing network in the fluid limit, we use a Lyapunov argument to show that\nthe policy is throughput-optimal.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 21:10:15 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 17:34:31 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Yang", "Chien-Sheng", ""], ["Pedarsani", "Ramtin", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1804.06568", "submitter": "Xianghui Mao", "authors": "Xianghui Mao, Kun Yuan, Yubin Hu, Yuantao Gu, Ali H. Sayed, and Wotao\n  Yin", "title": "Walkman: A Communication-Efficient Random-Walk Algorithm for\n  Decentralized Optimization", "comments": "Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses consensus optimization problems in a multi-agent\nnetwork, where all agents collaboratively find a minimizer for the sum of their\nprivate functions. We develop a new decentralized algorithm in which each agent\ncommunicates only with its neighbors.\n  State-of-the-art decentralized algorithms use communications between either\nall pairs of adjacent agents or a random subset of them at each iteration.\nAnother class of algorithms uses a random walk incremental strategy, which\nsequentially activates a succession of nodes; these incremental algorithms\nrequire diminishing step sizes to converge to the solution, so their\nconvergence is relatively slow.\n  In this work, we propose a random walk algorithm that uses a fixed step size\nand converges faster than the existing random walk incremental algorithms. Our\nalgorithm is also communication efficient. Each iteration uses only one link to\ncommunicate the latest information for an agent to another. Since this\ncommunication rule mimics a man walking around the network, we call our new\nalgorithm Walkman. We establish convergence for convex and nonconvex\nobjectives. For decentralized least squares, we derive a linear rate of\nconvergence and obtain a better communication complexity than those of other\ndecentralized algorithms. Numerical experiments verify our analysis results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 06:26:38 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 18:47:01 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 22:54:05 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 08:53:21 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Mao", "Xianghui", ""], ["Yuan", "Kun", ""], ["Hu", "Yubin", ""], ["Gu", "Yuantao", ""], ["Sayed", "Ali H.", ""], ["Yin", "Wotao", ""]]}, {"id": "1804.06826", "submitter": "Daniele Scarpazza", "authors": "Zhe Jia, Marco Maggioni, Benjamin Staiger, Daniele P. Scarpazza", "title": "Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking", "comments": "Technical report. First Edition. April 18th, 2018. 66 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year, novel NVIDIA GPU designs are introduced. This rapid architectural\nand technological progression, coupled with a reluctance by manufacturers to\ndisclose low-level details, makes it difficult for even the most proficient GPU\nsoftware designers to remain up-to-date with the technological advances at a\nmicroarchitectural level. To address this dearth of public,\nmicroarchitectural-level information on the novel NVIDIA GPUs, independent\nresearchers have resorted to microbenchmarks-based dissection and discovery.\nThis has led to a prolific line of publications that shed light on instruction\nencoding, and memory hierarchy's geometry and features at each level. Namely,\nresearch that describes the performance and behavior of the Kepler, Maxwell and\nPascal architectures. In this technical report, we continue this line of\nresearch by presenting the microarchitectural details of the NVIDIA Volta\narchitecture, discovered through microbenchmarks and instruction set\ndisassembly. Additionally, we compare quantitatively our Volta findings against\nits predecessors, Kepler, Maxwell and Pascal.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 17:25:13 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Jia", "Zhe", ""], ["Maggioni", "Marco", ""], ["Staiger", "Benjamin", ""], ["Scarpazza", "Daniele P.", ""]]}, {"id": "1804.06836", "submitter": "Drew Stone", "authors": "Drew Stone", "title": "Delayed Blockchain Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the parallels between game theory and consensus, it makes sense to\nintelligently design blockchain or DAG protocols with an\nincentive-compatible-first mentality. To that end, we propose a new blockchain\nor DAG protocol enhancement based on delayed rewards. We devise a new method\nfor imposing slashing conditions on miner behavior, using their delayed rewards\nas stake in a Proof of Work system. Using fraud proofs, we can slash malicious\nminer behavior and reward long-lived, honest behavior.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 17:50:09 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Stone", "Drew", ""]]}, {"id": "1804.06926", "submitter": "Leyuan Wang", "authors": "Leyuan Wang, Yangzihao Wang, Carl Yang and John D. Owens", "title": "A Comparative Study on Exact Triangle Counting Algorithms on the GPU", "comments": "7 pages, 6 figures and 2 tables", "journal-ref": null, "doi": "10.1145/2915516.2915521", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement exact triangle counting in graphs on the GPU using three\ndifferent methodologies: subgraph matching to a triangle pattern; programmable\ngraph analytics, with a set-intersection approach; and a matrix formulation\nbased on sparse matrix-matrix multiplies. All three deliver best-of-class\nperformance over CPU implementations and over comparable GPU implementations,\nwith the graph-analytic approach achieving the best performance due to its\nability to exploit efficient filtering steps to remove unnecessary work and its\nhigh-performance set-intersection core.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 21:51:59 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Wang", "Leyuan", ""], ["Wang", "Yangzihao", ""], ["Yang", "Carl", ""], ["Owens", "John D.", ""]]}, {"id": "1804.07017", "submitter": "Sandra Catalan", "authors": "Sandra Catal\\'an, Adri\\'an Castell\\'o, Francisco D. Igual, Rafael\n  Rodr\\'iguez-S\\'anchez, Enrique S. Quintana-Ort\\'i", "title": "Programming Parallel Dense Matrix Factorizations with Look-Ahead and\n  OpenMP", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a parallelization strategy for dense matrix factorization\n(DMF) algorithms, using OpenMP, that departs from the legacy (or conventional)\nsolution, which simply extracts concurrency from a multithreaded version of\nBLAS. This approach is also different from the more sophisticated\nruntime-assisted implementations, which decompose the operation into tasks and\nidentify dependencies via directives and runtime support. Instead, our strategy\nattains high performance by explicitly embedding a static look-ahead technique\ninto the DMF code, in order to overcome the performance bottleneck of the panel\nfactorization, and realizing the trailing update via a cache-aware\nmulti-threaded implementation of the BLAS. Although the parallel algorithms are\nspecified with a highlevel of abstraction, the actual implementation can be\neasily derived from them, paving the road to deriving a high performance\nimplementation of a considerable fraction of LAPACK functionality on any\nmulticore platform with an OpenMP-like runtime.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 07:14:36 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Catal\u00e1n", "Sandra", ""], ["Castell\u00f3", "Adri\u00e1n", ""], ["Igual", "Francisco D.", ""], ["Rodr\u00edguez-S\u00e1nchez", "Rafael", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""]]}, {"id": "1804.07047", "submitter": "Leye Wang", "authors": "Leye Wang, Wenbin Liu, Daqing Zhang, Yasha Wang, En Wang, Yongjian\n  Yang", "title": "Cell Selection with Deep Reinforcement Learning in Sparse Mobile\n  Crowdsensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Mobile CrowdSensing (MCS) is a novel MCS paradigm where data inference\nis incorporated into the MCS process for reducing sensing costs while its\nquality is guaranteed. Since the sensed data from different cells (sub-areas)\nof the target sensing area will probably lead to diverse levels of inference\ndata quality, cell selection (i.e., choose which cells of the target area to\ncollect sensed data from participants) is a critical issue that will impact the\ntotal amount of data that requires to be collected (i.e., data collection\ncosts) for ensuring a certain level of quality. To address this issue, this\npaper proposes a Deep Reinforcement learning based Cell selection mechanism for\nSparse MCS, called DR-Cell. First, we properly model the key concepts in\nreinforcement learning including state, action, and reward, and then propose to\nuse a deep recurrent Q-network for learning the Q-function that can help decide\nwhich cell is a better choice under a certain state during cell selection.\nFurthermore, we leverage the transfer learning techniques to reduce the amount\nof data required for training the Q-function if there are multiple correlated\nMCS tasks that need to be conducted in the same target area. Experiments on\nvarious real-life sensing datasets verify the effectiveness of DR-Cell over the\nstate-of-the-art cell selection mechanisms in Sparse MCS by reducing up to 15%\nof sensed cells with the same data inference quality guarantee.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 09:21:06 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 06:18:41 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Wang", "Leye", ""], ["Liu", "Wenbin", ""], ["Zhang", "Daqing", ""], ["Wang", "Yasha", ""], ["Wang", "En", ""], ["Yang", "Yongjian", ""]]}, {"id": "1804.07051", "submitter": "Xiaojing Chen", "authors": "Xiaojing Chen, Wei Ni, Tianyi Chen, Iain B. Collings, Xin Wang, Ren\n  Ping Liu and Georgios B. Giannakis", "title": "Multi-Timescale Online Optimization of Network Function Virtualization\n  for Service Chaining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Function Virtualization (NFV) can cost-efficiently provide network\nservices by running different virtual network functions (VNFs) at different\nvirtual machines (VMs) in a correct order. This can result in strong couplings\nbetween the decisions of the VMs on the placement and operations of VNFs. This\npaper presents a new fully decentralized online approach for optimal placement\nand operations of VNFs. Building on a new stochastic dual gradient method, our\napproach decouples the real-time decisions of VMs, asymptotically minimizes the\ntime-average cost of NFV, and stabilizes the backlogs of network services with\na cost-backlog tradeoff of $[\\epsilon,1/\\epsilon]$, for any $\\epsilon > 0$. Our\napproach can be relaxed into multiple timescales to have VNFs (re)placed at a\nlarger timescale and hence alleviate service interruptions. While proved to\npreserve the asymptotic optimality, the larger timescale can slow down the\noptimal placement of VNFs. A learn-and-adapt strategy is further designed to\nspeed the placement up with an improved tradeoff\n$[\\epsilon,\\log^2(\\epsilon)/{\\sqrt{\\epsilon}}]$. Numerical results show that\nthe proposed method is able to reduce the time-average cost of NFV by 30\\% and\nreduce the queue length (or delay) by 83\\%, as compared to existing benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 09:27:03 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Chen", "Xiaojing", ""], ["Ni", "Wei", ""], ["Chen", "Tianyi", ""], ["Collings", "Iain B.", ""], ["Wang", "Xin", ""], ["Liu", "Ren Ping", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1804.07078", "submitter": "Cezara Dr\\u{a}goi", "authors": "Andrei Damien, Cezara Dragoi, Alexandru Militaru, and Josef Widder", "title": "Reducing asynchrony to synchronized rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous computation models simplify the design and the verification of\nfault-tolerant distributed systems. For efficiency reasons such systems are\ndesigned and implemented using an asynchronous semantics. In this paper, we\nbridge the gap between these two worlds. We introduce a (synchronous)\nround-based computational model and we prove a reduction for a class of\nasynchronous protocols to our new model. The reduction is based on properties\nof the code that can be checked with sequential methods. We apply the reduction\nto state machine replication systems, such as, Paxos, Zab, and Viewstamped\nReplication.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 10:50:11 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 13:42:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Damien", "Andrei", ""], ["Dragoi", "Cezara", ""], ["Militaru", "Alexandru", ""], ["Widder", "Josef", ""]]}, {"id": "1804.07131", "submitter": "Maria Predari", "authors": "Roland Glantz, Maria Predari, Henning Meyerhenke", "title": "Topology-induced Enhancement of Mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method to enhance a mapping $\\mu(\\cdot)$ of a\nparallel application's computational tasks to the processing elements (PEs) of\na parallel computer.\n  The idea behind our method \\mswap is to enhance such a mapping by drawing on\nthe observation that many topologies take the form of a partial cube.\n  This class of graphs includes all rectangular and cubic meshes, any such\ntorus with even extensions in each dimension, all hypercubes, and all trees.\n  Following previous work, we represent the parallel application and the\nparallel computer by graphs $G_a = (V_a, E_a)$ and $G_p = (V_p, E_p)$.\n  $G_p$ being a partial cube allows us to label its vertices, the PEs, by\nbitvectors such that the cost of exchanging one unit of information between two\nvertices $u_p$ and $v_p$ of $G_p$ amounts to the Hamming distance between the\nlabels of $u_p$ and $v_p$.\n  By transferring these bitvectors from $V_p$ to $V_a$ via $\\mu^{-1}(\\cdot)$\nand extending them to be unique on $V_a$, we can enhance $\\mu(\\cdot)$ by\nswapping labels of $V_a$ in a new way.\n  Pairs of swapped labels are local \\wrt the PEs, but not \\wrt $G_a$. Moreover,\npermutations of the bitvectors' entries give rise to a plethora of hierarchies\non the PEs. Through these hierarchies we turn \\mswap into a hierarchical method\nfor improving $\\mu(\\cdot)$ that is complementary to state-of-the-art methods\nfor computing $\\mu(\\cdot)$ in the first place.\n  In our experiments we use \\mswap to enhance mappings of complex networks onto\nrectangular meshes and tori with 256 and 512 nodes, as well as hypercubes with\n256 nodes. It turns out that common quality measures of mappings derived from\nstate-of-the-art algorithms can be improved considerably.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 13:08:39 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Glantz", "Roland", ""], ["Predari", "Maria", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1804.07356", "submitter": "Enrique Fynn", "authors": "Enrique Fynn, Fernando Pedone", "title": "Challenges and pitfalls of partitioning blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has received much attention in recent years. This immense\npopularity has raised a number of concerns, scalability of blockchain systems\nbeing a common one. In this paper, we seek to understand how Ethereum, a\nwell-established blockchain system, would respond to sharding. Sharding is a\nprevalent technique to increase the scalability of distributed systems. To\nunderstand how sharding would affect Ethereum, we model Ethereum blockchain as\na graph and evaluate five methods to partition the graph. We analyze the\nresults using three metrics: the balance among shards, the number of\ntransactions that would involve multiple shards, and the amount of data that\nwould be relocated across shards upon a repartitioning of the system.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 20:03:30 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 13:12:35 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Fynn", "Enrique", ""], ["Pedone", "Fernando", ""]]}, {"id": "1804.07391", "submitter": "Mansoor Ahmed-Rengers", "authors": "Mansoor Ahmed-Rengers and Kari Kostiainen", "title": "Don't Mine, Wait in Line: Fair and Efficient Blockchain Consensus with\n  Robust Round Robin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof-of-Stake systems randomly choose, on each round, one of the\nparticipants as a consensus leader that extends the chain with the next block\nsuch that the selection probability is proportional to the owned stake.\nHowever, distributed random number generation is notoriously difficult. Systems\nthat derive randomness from the previous blocks are completely insecure;\nsolutions that provide secure random selection are inefficient due to their\nhigh communication complexity; and approaches that balance security and\nperformance exhibit selection bias. When block creation is rewarded with new\nstake, even a minor bias can have a severe cumulative effect.\n  In this paper, we propose Robust Round Robin, a new consensus scheme that\naddresses this selection problem. We create reliable long-term identities by\nbootstrapping from an existing infrastructure, such as Intel's SGX processors,\nor by mining them starting from an initial fair distribution. For leader\nselection we use a deterministic approach. On each round, we select a set of\nthe previously created identities as consensus leader candidates in round robin\nmanner. Because simple round-robin alone is vulnerable to attacks and offers\npoor liveness, we complement such deterministic selection policy with a\nlightweight endorsement mechanism that is an interactive protocol between the\nleader candidates and a small subset of other system participants. Our solution\nhas low good efficiency as it requires no expensive distributed randomness\ngeneration and it provides block creation fairness which is crucial in\ndeployments that reward it with new stake.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 22:15:30 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 14:10:06 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 22:23:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ahmed-Rengers", "Mansoor", ""], ["Kostiainen", "Kari", ""]]}, {"id": "1804.07494", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff", "title": "Parallel Quicksort without Pairwise Element Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quicksort is an instructive classroom approach to parallel sorting on\ndistributed memory parallel computers with many opportunities for illustrating\nspecific implementation alternatives and tradeoffs with common communication\ninterfaces like MPI. The (two) standard distributed memory Quicksort\nimplementations exchange partitioned data elements at each level of the\nQuicksort recursion. In this note, we show that this is not necessary: It\nsuffices to distribute only the chosen pivots, and postpone element\nredistribution to the bottom of the recursion. This reduces the total volume of\ndata exchanged from $O(n\\log p)$ to $O(n)$, $n$ being the total number of\nelements to be sorted and $p$ a power-of-two number of processors, by trading\noff against a total of $O(p)$ additional pivot element distributions. Based on\nthis observation, we describe new, \\emph{exchange-free} implementation variants\nof parallel Quicksort and of Wagar's HyperQuicksort.\n  We have implemented the discussed four different Quicksort variations in MPI,\nand show that with good pivot selection, Quicksort without pairwise element\nexchange can be significantly faster than standard implementations on\nmoderately large problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 08:48:47 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 11:50:51 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1804.07501", "submitter": "Julien Peloton", "authors": "Julien Peloton, Christian Arnault, St\\'ephane Plaszczynski", "title": "FITS Data Source for Apache Spark", "comments": "9 pages, 6 figures. Package available at\n  https://github.com/astrolabsoftware/spark-fits Accepted in Computing and\n  Software for Big Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of Apache Spark, a cluster computing\nframework, for analyzing data from future LSST-like galaxy surveys. Apache\nSpark attempts to address big data problems have hitherto proved successful in\nthe industry, but its use in the astronomical community still remains limited.\nWe show how to manage complex binary data structures handled in astrophysics\nexperiments such as binary tables stored in FITS files, within a distributed\nenvironment. To this purpose, we first designed and implemented a Spark\nconnector to handle sets of arbitrarily large FITS files, called spark-fits.\nThe user interface is such that a simple file \"drag-and-drop\" to a cluster\ngives full advantage of the framework. We demonstrate the very high scalability\nof spark-fits using the LSST fast simulation tool, CoLoRe, and present the\nmethodologies for measuring and tuning the performance bottlenecks for the\nworkloads, scaling up to terabytes of FITS data on the Cloud@VirtualData,\nlocated at Universit\\'e Paris Sud. We also evaluate its performance on Cori, a\nHigh-Performance Computing system located at NERSC, and widely used in the\nscientific community.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 09:08:10 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 19:03:02 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Peloton", "Julien", ""], ["Arnault", "Christian", ""], ["Plaszczynski", "St\u00e9phane", ""]]}, {"id": "1804.07571", "submitter": "Ludwig Dierks", "authors": "Ludwig Dierks, Ian A. Kash, Sven Seuken", "title": "On the cluster admission problem for cloud computing", "comments": "This paper is a significantly extended version of work that was\n  published as a 6-page extended abstract in the proceedings of the 14th\n  Workshop on the Economics of Networks, Systems and Computation (NetEcon'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing providers face the problem of matching heterogeneous customer\nworkloads to resources that will serve them. This is particularly challenging\nif customers, who are already running a job on a cluster, scale their resource\nusage up and down over time. The provider therefore has to continuously decide\nwhether she can add additional workloads to a given cluster or if doing so\nwould impact existing workloads' ability to scale. Currently, this is often\ndone using simple threshold policies to reserve large parts of each cluster,\nwhich leads to low efficiency (i.e., low average utilization of the cluster).\nWe propose more sophisticated policies for controlling admission to a cluster\nand demonstrate that they significantly increase cluster utilization. We first\nintroduce the cluster admission problem and formalize it as a constrained\nPartially Observable Markov Decision Process (POMDP). As it is infeasible to\nsolve the POMDP optimally, we then systematically design admission policies\nthat estimate moments of each workload's distribution of future resource usage.\nVia extensive simulations grounded in a trace from Microsoft Azure, we show\nthat our admission policies lead to a substantial improvement over the simple\nthreshold policy. We then show that substantial further gains are possible if\nhigh-quality information is available about arriving workloads. Based on this,\nwe propose an information elicitation approach to incentivize users to provide\nthis information and simulate its effects.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 12:11:26 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 17:50:59 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 10:05:07 GMT"}, {"version": "v4", "created": "Mon, 3 Dec 2018 12:24:59 GMT"}, {"version": "v5", "created": "Tue, 4 Dec 2018 14:29:08 GMT"}, {"version": "v6", "created": "Fri, 21 Jun 2019 09:34:47 GMT"}, {"version": "v7", "created": "Mon, 6 Apr 2020 16:04:41 GMT"}, {"version": "v8", "created": "Fri, 14 Aug 2020 16:04:35 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Dierks", "Ludwig", ""], ["Kash", "Ian A.", ""], ["Seuken", "Sven", ""]]}, {"id": "1804.07598", "submitter": "Ivo Sbalzarini", "authors": "Pietro Incardona, Antonio Leo, Yaroslav Zaluzhnyi, Rajesh Ramaswamy,\n  Ivo F. Sbalzarini", "title": "OpenFPM: A scalable open framework for particle and particle-mesh codes\n  on parallel computers", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2019.03.007", "report-no": null, "categories": "cs.DC cs.MS cs.SE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable and efficient numerical simulations continue to gain importance, as\ncomputation is firmly established as the third pillar of discovery, alongside\ntheory and experiment. Meanwhile, the performance of computing hardware grows\nthrough increasing heterogeneous parallelism, enabling simulations of ever more\ncomplex models. However, efficiently implementing scalable codes on\nheterogeneous, distributed hardware systems becomes the bottleneck. This\nbottleneck can be alleviated by intermediate software layers that provide\nhigher-level abstractions closer to the problem domain, hence allowing the\ncomputational scientist to focus on the simulation. Here, we present OpenFPM,\nan open and scalable framework that provides an abstraction layer for numerical\nsimulations using particles and/or meshes. OpenFPM provides transparent and\nscalable infrastructure for shared-memory and distributed-memory\nimplementations of particles-only and hybrid particle-mesh simulations of both\ndiscrete and continuous models, as well as non-simulation codes. This\ninfrastructure is complemented with portable implementations of frequently used\nnumerical routines, as well as interfaces to third-party libraries. We present\nthe architecture and design of OpenFPM, detail the underlying abstractions, and\nbenchmark the framework in applications ranging from Smoothed-Particle\nHydrodynamics (SPH) to Molecular Dynamics (MD), Discrete Element Methods (DEM),\nVortex Methods, stencil codes, high-dimensional Monte Carlo sampling (CMA-ES),\nand Reaction-Diffusion solvers, comparing it to the current state of the art\nand existing software frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 13:18:50 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Incardona", "Pietro", ""], ["Leo", "Antonio", ""], ["Zaluzhnyi", "Yaroslav", ""], ["Ramaswamy", "Rajesh", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1804.07647", "submitter": "Max Friese", "authors": "Max J. Friese and Thorsten Ehlers and Dirk Nowotka", "title": "Estimating Latencies of Task Sequences in Multi-Core Automotive ECUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of a cyber-physical system's reaction to a stimulus typically\ninvolves the execution of several tasks. The delay between stimulus and\nreaction thus depends on the interaction of these tasks and is subject to\ntiming constraints. Such constraints exist for a number of reasons and range\nfrom possible impacts on customer experiences to safety requirements. We\npresent a technique to determine end-to-end latencies of such task sequences.\nThe technique is demonstrated on the example of electronic control units (ECUs)\nin automotive embedded real-time systems. Our approach is able to deal with\nmulti-core architectures and supports four different activation patterns,\nincluding interrupts. It is the first formal analysis approach making use of\nload assumptions in order to exclude infeasible data propagation paths without\nthe knowledge of worst-case execution times or worst-case response times. We\nemploy a constraint programming solver to compute bounds on end-to-end\nlatencies.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 16:43:31 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Friese", "Max J.", ""], ["Ehlers", "Thorsten", ""], ["Nowotka", "Dirk", ""]]}, {"id": "1804.07682", "submitter": "Maxim Gonchar", "authors": "Anna Fatkina, Maxim Gonchar, Liudmila Kolupaeva, Dmitry Naumov,\n  Konstantin Treskov", "title": "CUDA Support in GNA Data Analysis Framework", "comments": "12 pages, 7 figures, ICCSA 2018, submitted to Lecture Notes in\n  Computer Science (Springer Verlag)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usage of GPUs as co-processors is a well-established approach to accelerate\ncostly algorithms operating on matrices and vectors.\n  We aim to further improve the performance of the Global Neutrino Analysis\nframework (GNA) by adding GPU support in a way that is transparent to the end\nuser. To achieve our goal we use CUDA, a state of the art technology providing\nGPGPU programming methods.\n  In this paper we describe new features of GNA related to CUDA support. Some\nspecific framework features that influence GPGPU integration are also\nexplained. The paper investigates the feasibility of GPU technology application\nand shows an example of the achieved acceleration of an algorithm implemented\nwithin framework. Benchmarks show a significant performance increase when using\nGPU transformations.\n  The project is currently in the developmental phase. Our plans include\nimplementation of the set of transformations necessary for the data analysis in\nthe GNA framework and tests of the GPU expediency in the complete analysis\nchain.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 15:42:21 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Fatkina", "Anna", ""], ["Gonchar", "Maxim", ""], ["Kolupaeva", "Liudmila", ""], ["Naumov", "Dmitry", ""], ["Treskov", "Konstantin", ""]]}, {"id": "1804.07747", "submitter": "Polyvios Pratikakis", "authors": "Iacovos Kolokasis, Polyvios Pratikakis", "title": "Cut to Fit: Tailoring the Partitioning to the Computation", "comments": "14 pages, 6 figures, has been submitted for review", "journal-ref": null, "doi": null, "report-no": "FORTH TR-469-MARCH-2018", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Graph Analytics applications are very often built using off-the-shelf\nanalytics frameworks. These, however, are profiled and optimized for the\ngeneral case and have to perform for all kinds of graphs. This paper\ninvestigates how knowledge of the application and the dataset can help optimize\nperformance with minimal effort. We concentrate on the impact of partitioning\nstrategies on the performance of computations on social graphs. We evaluate six\ngraph partitioning algorithms on a set of six social graphs, using four\nstandard graph algorithms by measuring a set of five partitioning metrics.\n  We analyze the performance of each partitioning strategy with respect to (i)\nthe properties of the graph dataset, (ii) each analytics computation,of\npartitions. We discover that communication cost is the best predictor of\nperformance for most -but not all- analytics computations. We also find that\nthe best partitioning strategy for a particular kind of algorithm may not be\nthe best for another, and that optimizing for the general case of all\nalgorithms may not select the optimal partitioning strategy for a given graph\nalgorithm. We conclude with insights on selecting the right data partitioning\nstrategy, which has significant impact on the performance of large graph\nanalytics computations; certainly enough to warrant optimization of the\npartitioning strategy to the computation and to the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 17:44:03 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Kolokasis", "Iacovos", ""], ["Pratikakis", "Polyvios", ""]]}, {"id": "1804.07815", "submitter": "Lorenzo Clemente", "authors": "Lorenzo Clemente", "title": "Decidability of Timed Communicating Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the reachability problem for networks of timed communicating\nprocesses. Each process is a timed automaton communicating with other processes\nby exchanging messages over unbounded FIFO channels. Messages carry clocks\nwhich are checked at the time of transmission and reception with suitable\ntiming constraints. Each automaton can only access its set of local clocks and\nmessage clocks of sent/received messages. Time is dense and all clocks evolve\nat the same rate. Our main contribution is a complete characterisation of\ndecidable and undecidable communication topologies generalising and unifying\nprevious work. From a technical point of view, we use quantifier elimination\nand a reduction to counter automata with registers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 20:23:05 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Clemente", "Lorenzo", ""]]}, {"id": "1804.07824", "submitter": "Yan Xu", "authors": "Patrick Koch, Oleg Golovidov, Steven Gardner, Brett Wujek, Joshua\n  Griffin, Yan Xu", "title": "Autotune: A Derivative-free Optimization Framework for Hyperparameter\n  Tuning", "comments": "10 Pages, 9 figures, accept by KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219837", "report-no": null, "categories": "cs.LG cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applications often require hyperparameter tuning. The\nhyperparameters usually drive both the efficiency of the model training process\nand the resulting model quality. For hyperparameter tuning, machine learning\nalgorithms are complex black-boxes. This creates a class of challenging\noptimization problems, whose objective functions tend to be nonsmooth,\ndiscontinuous, unpredictably varying in computational expense, and include\ncontinuous, categorical, and/or integer variables. Further, function\nevaluations can fail for a variety of reasons including numerical difficulties\nor hardware failures. Additionally, not all hyperparameter value combinations\nare compatible, which creates so called hidden constraints. Robust and\nefficient optimization algorithms are needed for hyperparameter tuning. In this\npaper we present an automated parallel derivative-free optimization framework\ncalled \\textbf{Autotune}, which combines a number of specialized sampling and\nsearch methods that are very effective in tuning machine learning models\ndespite these challenges. Autotune provides significantly improved models over\nusing default hyperparameter settings with minimal user interaction on\nreal-world applications. Given the inherent expense of training numerous\ncandidate models, we demonstrate the effectiveness of Autotune's search methods\nand the efficient distributed and parallel paradigms for training and tuning\nmodels, and also discuss the resource trade-offs associated with the ability to\nboth distribute the training process and parallelize the tuning process.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 20:56:33 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 18:20:17 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Koch", "Patrick", ""], ["Golovidov", "Oleg", ""], ["Gardner", "Steven", ""], ["Wujek", "Brett", ""], ["Griffin", "Joshua", ""], ["Xu", "Yan", ""]]}, {"id": "1804.07981", "submitter": "Moreno Marzolla", "authors": "Moreno Marzolla", "title": "Parallel Implementations of Cellular Automata for Traffic Models", "comments": null, "journal-ref": "In: Mauri G., El Yacoubi S., Dennunzio A., Nishinari K., Manzoni\n  L. (eds) Cellular Automata. ACRI 2018. Lecture Notes in Computer Science, vol\n  11115. Springer", "doi": "10.1007/978-3-319-99813-8_46", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Biham-Middleton-Levine (BML) traffic model is a simple two-dimensional,\ndiscrete Cellular Automaton (CA) that has been used to study self-organization\nand phase transitions arising in traffic flows. From the computational point of\nview, the BML model exhibits the usual features of discrete CA, where the state\nof the automaton are updated according to simple rules that depend on the state\nof each cell and its neighbors. In this paper we study the impact of various\noptimizations for speeding up CA computations by using the BML model as a case\nstudy. In particular, we describe and analyze the impact of several parallel\nimplementations that rely on CPU features, such as multiple cores or SIMD\ninstructions, and on GPUs. Experimental evaluation provides quantitative\nmeasures of the payoff of each technique in terms of speedup with respect to a\nplain serial implementation. Our findings show that the performance gap between\nCPU and GPU implementations of the BML traffic model can be reduced by clever\nexploitation of all CPU features.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 15:24:44 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Marzolla", "Moreno", ""]]}, {"id": "1804.08133", "submitter": "Abhishek Dubey", "authors": "Scott Eisele and Aron Laszka and Anastasia Mavridou and Abhishek Dubey", "title": "SolidWorx: A Resilient and Trustworthy Transactive Platform for Smart\n  and Connected Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things and data sciences are fueling the development of\ninnovative solutions for various applications in Smart and Connected\nCommunities (SCC). These applications provide participants with the capability\nto exchange not only data but also resources, which raises the concerns of\nintegrity, trust, and above all the need for fair and optimal solutions to the\nproblem of resource allocation. This exchange of information and resources\nleads to a problem where the stakeholders of the system may have limited trust\nin each other. Thus, collaboratively reaching consensus on when, how, and who\nshould access certain resources becomes problematic. This paper presents\nSolidWorx, a blockchain-based platform that provides key mechanisms required\nfor arbitrating resource consumption across different SCC applications in a\ndomain-agnostic manner. For example, it introduces and implements a\nhybrid-solver pattern, where complex optimization computation is handled\noff-blockchain while solution validation is performed by a smart contract. To\nensure correctness, the smart contract of SolidWorx is generated and verified.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 16:33:56 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 12:59:51 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Eisele", "Scott", ""], ["Laszka", "Aron", ""], ["Mavridou", "Anastasia", ""], ["Dubey", "Abhishek", ""]]}, {"id": "1804.08230", "submitter": "Wei Li", "authors": "Wei Li", "title": "Adapting Blockchain Technology for Scientific Computing", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain stores information into a chain of \"blocks\", whose integrity is\nusually guaranteed by Proof of Work (PoW). In many blockchain applications\n(including cryptocurrencies), users compete with each other to win the\nownership of the blocks, a process commonly referred as \"mining\". Mining\nactivities consume huge amount of power, while the outcome appears to be\nuseless besides validating a block. Here we discuss the requirements of\ndesigning a new PoW algorithm. We also propose a PoW scheme to help solve\nhigh-dimension, non-linear optimization problems. Simulation experiments of\nblockchains generated by three miners solved an instance of Traveling Salesman\nProblem (TSP), a well-known NP-hard problem. The revised scheme enables us to\naddress difficult scientific questions as a byproduct of mining.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 02:50:07 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 15:26:09 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Li", "Wei", ""]]}, {"id": "1804.08265", "submitter": "Samrat Mukhopadhyay", "authors": "Samrat Mukhopadhyay and Mrityunjoy Chakraborty", "title": "Deterministic and Randomized Diffusion based Iterative Generalized Hard\n  Thresholding (DiFIGHT) for Distributed Sparse Signal Recovery", "comments": "11 pages, 4 figures, Updated some analysis, Added a few more\n  explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a distributed iterated hard thresholding algorithm\ntermed DiFIGHT over a network that is built on the diffusion mechanism and also\npropose a modification of the proposed algorithm, termed MoDiFIGHT, that has\nlow complexity in terms of communication in the network. We additionally\npropose four different strategies termed RP, RNP, RGP$_r$, and RGNP$_r$ that\nare used to randomly select a subset of nodes that are subsequently activated\nto take part in the distributed algorithm, so as to reduce the mean number of\ncommunications during the run of the distributed algorithm. We present\ntheoretical estimates of the long run communication per unit time for these\ndifferent strategies, when used by the two proposed algorithms. Also, we\npresent analysis of the two proposed algorithms and provide provable bounds on\ntheir recovery performance with or without using the random node selection\nstrategies. Finally we use numerical studies to show that both when the random\nstrategies are used as well as when they are not used, the proposed algorithms\ndisplay performances far superior to distributed IHT algorithm using consensus\nmechanism .\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 07:13:47 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 16:37:20 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Mukhopadhyay", "Samrat", ""], ["Chakraborty", "Mrityunjoy", ""]]}, {"id": "1804.08378", "submitter": "Nicolas Weber", "authors": "Nicolas Weber, Florian Schmidt, Mathias Niepert, Felipe Huici", "title": "BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First\n  Parallelism", "comments": "Technical Report, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network frameworks such as PyTorch and TensorFlow are the workhorses\nof numerous machine learning applications ranging from object recognition to\nmachine translation. While these frameworks are versatile and straightforward\nto use, the training of and inference in deep neural networks is resource\n(energy, compute, and memory) intensive. In contrast to recent works focusing\non algorithmic enhancements, we introduce BrainSlug, a framework that\ntransparently accelerates neural network workloads by changing the default\nlayer-by-layer processing to a depth-first approach, reducing the amount of\ndata required by the computations and thus improving the performance of the\navailable hardware caches. BrainSlug achieves performance improvements of up to\n41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the\nuser as they do not require hardware changes and only need tiny adjustments to\nthe software.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 12:49:04 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Weber", "Nicolas", ""], ["Schmidt", "Florian", ""], ["Niepert", "Mathias", ""], ["Huici", "Felipe", ""]]}, {"id": "1804.08548", "submitter": "Christopher Musco", "authors": "Frederik Mallmann-Trenn, Cameron Musco, and Christopher Musco", "title": "Eigenvector Computation and Community Detection in Asynchronous Gossip\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple distributed algorithm for computing adjacency matrix\neigenvectors for the communication graph in an asynchronous gossip model. We\nshow how to use this algorithm to give state-of-the-art asynchronous community\ndetection algorithms when the communication graph is drawn from the\nwell-studied stochastic block model. Our methods also apply to a natural\nalternative model of randomized communication, where nodes within a community\ncommunicate more frequently than nodes in different communities. Our analysis\nsimplifies and generalizes prior work by forging a connection between\nasynchronous eigenvector computation and Oja's algorithm for streaming\nprincipal component analysis. We hope that our work serves as a starting point\nfor building further connections between the analysis of stochastic iterative\nmethods, like Oja's algorithm, and work on asynchronous and gossip-type\nalgorithms for distributed computation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 16:39:39 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 00:16:00 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Mallmann-Trenn", "Frederik", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1804.08733", "submitter": "Charith Mendis", "authors": "Charith Mendis and Saman Amarasinghe", "title": "goSLP: Globally Optimized Superword Level Parallelism Framework", "comments": "Published at OOPSLA 2018", "journal-ref": "OOPSLA 2018", "doi": "10.1145/3276480", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern microprocessors are equipped with single instruction multiple data\n(SIMD) or vector instruction sets which allow compilers to exploit superword\nlevel parallelism (SLP), a type of fine-grained parallelism. Current SLP\nauto-vectorization techniques use heuristics to discover vectorization\nopportunities in high-level language code. These heuristics are fragile, local\nand typically only present one vectorization strategy that is either accepted\nor rejected by a cost model. We present goSLP, a novel SLP auto-vectorization\nframework which solves the statement packing problem in a pairwise optimal\nmanner. Using an integer linear programming (ILP) solver, goSLP searches the\nentire space of statement packing opportunities for a whole function at a time,\nwhile limiting total compilation time to a few minutes. Furthermore, goSLP\noptimally solves the vector permutation selection problem using dynamic\nprogramming. We implemented goSLP in the LLVM compiler infrastructure,\nachieving a geometric mean speedup of 7.58% on SPEC2017fp, 2.42% on SPEC2006fp\nand 4.07% on NAS benchmarks compared to LLVM's existing SLP auto-vectorizer.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:59:12 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 14:23:11 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Mendis", "Charith", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1804.08822", "submitter": "Jimmy Lin", "authors": "Kareem El Gebaly and Jimmy Lin", "title": "In-Browser Split-Execution Support for Interactive Analytics in the\n  Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The canonical analytics architecture today consists of a browser connected to\na backend in the cloud. In all deployments that we are aware of, the browser is\nsimply a dumb rendering endpoint. As an alternative, this paper explores\nsplit-execution architectures that push analytics capabilities into the\nbrowser. We show that, by taking advantage of typed arrays and asm.js, it is\npossible to build an analytical RDBMS in JavaScript that runs in a browser,\nachieving performance rivaling native databases. To support interactive data\nexploration, our Afterburner prototype automatically generates local\nmaterialized views from a backend database that are then shipped to the browser\nto facilitate subsequent interactions seamlessly and efficiently. We compare\nthis architecture to several alternative deployments, experimentally\ndemonstrating performance parity, while at the same time providing additional\nadvantages in terms of administrative and operational simplicity.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 03:02:54 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Gebaly", "Kareem El", ""], ["Lin", "Jimmy", ""]]}, {"id": "1804.08856", "submitter": "Oleg Malafeyev", "authors": "O.A.Malafeyev, S.A.Nemnyugin", "title": "Parallel computing as a congestion game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game-theoretical approach to the analysis of parallel algorithms is proposed.\nThe approach is based on presentation of the parallel computing as a congestion\ngame. In the game processes compete for resources such as core of a central\nprocessing unit and a communication subsystem. There are players, resources and\npayoffs (time delays) of players which depend on resources usage. Comparative\nanalysis of various optimality principles in the proposed model may be\nperformed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 06:34:11 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Malafeyev", "O. A.", ""], ["Nemnyugin", "S. A.", ""]]}, {"id": "1804.09107", "submitter": "David R. Matos", "authors": "David R. Matos, Nuno Neves, Alysson Bessani", "title": "SITAN: Services for Fault-Tolerant Ad Hoc Networks with Unknown\n  Participants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of mobile devices with various capabilities (e.g., smartphones\nand tablets), together with their ability to collaborate in impromptu ad hoc\nnetworks, opens new opportunities for the design of innovative distributed\napplications. The development of these applications needs to address several\ndifficulties, such as the unreliability of the network, the imprecise set of\nparticipants, or the presence of malicious nodes. In this paper we describe a\nmiddleware, called SITAN, that offers a number of communication, group\nmembership and coordination services specially conceived for these settings.\nThese services are implemented by a stack of Byzantine fault-tolerant\nprotocols, enabling applications that are built on top of them to operate\ncorrectly despite the uncertainty of the environment. The protocol stack was\nimplemented in Android and NS-3, which allowed the experimentation in\nrepresentative scenarios. Overall, the results show that the protocols are able\nto finish their execution within a small time window, which is acceptable for\nvarious kinds of applications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 15:50:55 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 10:57:53 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Matos", "David R.", ""], ["Neves", "Nuno", ""], ["Bessani", "Alysson", ""]]}, {"id": "1804.09136", "submitter": "Christina Delimitrou", "authors": "Yu Gan, Meghna Pancholi, Dailun Cheng, Siyuan Hu, Yuan He, Christina\n  Delimitrou", "title": "Seer: Leveraging Big Data to Navigate the Increasing Complexity of Cloud\n  Debugging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance unpredictability in cloud services leads to poor user experience,\ndegraded availability, and has revenue ramifications. Detecting performance\ndegradation a posteriori helps the system take corrective action, but does not\navoid the QoS violations. Detecting QoS violations after the fact is even more\ndetrimental when a service consists of hundreds of thousands of loosely-coupled\nmicroservices, since performance hiccups can quickly propagate across the\ndependency graph of microservices. In this work we focus on anticipating QoS\nviolations in cloud settings to mitigate performance unpredictability to begin\nwith. We propose Seer, a cloud runtime that leverages the massive amount of\ntracing data cloud systems collect over time and a set of practical learning\ntechniques to signal upcoming QoS violations, as well as identify the\nmicroservice(s) causing them. Once an imminent QoS violation is detected Seer\nuses machine-level hardware events to determine the cause of the QoS violation,\nand adjusts the resource allocations to prevent it. In local clusters with 10\n40-core servers and 200-instance clusters on GCE running diverse cloud\nmicroservices, we show that Seer correctly anticipates QoS violations 91% of\nthe time, and attributes the violation to the correct microservice in 89% of\ncases. Finally, Seer detects QoS violations early enough for a corrective\naction to almost always be applied successfully.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 16:59:07 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Gan", "Yu", ""], ["Pancholi", "Meghna", ""], ["Cheng", "Dailun", ""], ["Hu", "Siyuan", ""], ["He", "Yuan", ""], ["Delimitrou", "Christina", ""]]}, {"id": "1804.09152", "submitter": "Rhaleb Zayer", "authors": "Rhaleb Zayer, Daniel Mlakar, Markus Steinberger, Hans-Peter Seidel", "title": "Layered Fields for Natural Tessellations on Surfaces", "comments": "Natural tessellations, surface fields, Voronoi diagrams, Lloyd's\n  algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mimicking natural tessellation patterns is a fascinating multi-disciplinary\nproblem. Geometric methods aiming at reproducing such partitions on surface\nmeshes are commonly based on the Voronoi model and its variants, and are often\nfaced with challenging issues such as metric estimation, geometric, topological\ncomplications, and most critically parallelization. In this paper, we introduce\nan alternate model which may be of value for resolving these issues. We drop\nthe assumption that regions need to be separated by lines. Instead, we regard\nregion boundaries as narrow bands and we model the partition as a set of smooth\nfunctions layered over the surface. Given an initial set of seeds or regions,\nthe partition emerges as the solution of a time dependent set of partial\ndifferential equations describing concurrently evolving fronts on the surface.\nOur solution does not require geodesic estimation, elaborate numerical solvers,\nor complicated bookkeeping data structures. The cost per time-iteration is\ndominated by the multiplication and addition of two sparse matrices. Extension\nof our approach in a Lloyd's algorithm fashion can be easily achieved and the\nextraction of the dual mesh can be conveniently preformed in parallel through\nmatrix algebra. As our approach relies mainly on basic linear algebra kernels,\nit lends itself to efficient implementation on modern graphics hardware.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 17:20:30 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Zayer", "Rhaleb", ""], ["Mlakar", "Daniel", ""], ["Steinberger", "Markus", ""], ["Seidel", "Hans-Peter", ""]]}, {"id": "1804.09267", "submitter": "Yu Chen", "authors": "Ronghua Xu, Yu Chen, Erik Blasch, and Genshe Chen", "title": "BlendCAC: A BLockchain-ENabled Decentralized Capability-based Access\n  Control for IoTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of Internet of Things (IoTs) allows heterogeneous embedded\nsmart devices to collaboratively provide smart services with or without human\nintervention. While leveraging the large scale IoT based applications like\nSmart Gird or Smart Cities, IoTs also incur more concerns on privacy and\nsecurity. Among the top security challenges that IoTs face, access\nauthorization is critical in resource sharing and information protection. One\nof the weaknesses in today's access control (AC) is the centralized\nauthorization server, which can be the performance bottleneck or the single\npoint of failure. In this paper, BlendCAC, a blockchain enabled decentralized\ncapability based AC is proposed for the security of IoTs. The BlendCAC aims at\nan effective access control processes to devices, services and information in\nlarge scale IoT systems. Based on the blockchain network, a capability\ndelegation mechanism is suggested for access permission propagation. A robust\nidentity based capability token management strategy is proposed, which takes\nadvantage of smart contract for registering, propagation and revocation of the\naccess authorization. In the proposed BlendCAC scheme, IoT devices are their\nown master to control their resources instead of being supervised by a\ncentralized authority. Implemented and tested on a Raspberry Pi device and on a\nlocal private blockchain network, our experimental results demonstrate the\nfeasibility of the proposed BlendCAC approach to offer a decentralized,\nscalable, lightweight and fine grained AC solution to IoT systems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 21:44:39 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Xu", "Ronghua", ""], ["Chen", "Yu", ""], ["Blasch", "Erik", ""], ["Chen", "Genshe", ""]]}, {"id": "1804.09324", "submitter": "Abhirup  Chakraborty", "authors": "Abhirup Chakraborty", "title": "Processing Database Joins over a Shared-Nothing System of Multicore\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To process a large volume of data, modern data management systems use a\ncollection of machines connected through a network. This paper looks into the\nfeasibility of scaling up such a shared-nothing system while processing a\ncompute- and communication-intensive workload---processing distributed joins.\nBy exploiting multiple processing cores within the individual machines, we\nimplement a system to process database joins that parallelizes computation\nwithin each node, pipelines the computation with communication, parallelizes\nthe communication by allowing multiple simultaneous data transfers\n(send/receive), and removes synchronization barriers (a scalability bottleneck\nin a distributed data processing system). Our experimental results show that\nusing only four threads per node the framework achieves a 3.5x gains in\nintra-node performance while compared with a single-threaded counterpart.\nMoreover, with the join processing workload the cluster-wide performance (and\nspeedup) is observed to be dictated by the intra-node computational loads; this\nproperty brings a near-linear speedup with increasing nodes in the system, a\nfeature much desired in modern large-scale data processing system.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 02:30:43 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Chakraborty", "Abhirup", ""]]}, {"id": "1804.09442", "submitter": "Julian Sch\\\"utte", "authors": "Julian Sch\\\"utte, Gerd Brost, Sascha Wessel", "title": "Der Trusted Connector im Industrial Data Space", "comments": "in German. IDS, IoT, Industrial Data Space, Edge Device, Usage\n  Control", "journal-ref": null, "doi": null, "report-no": "TR-AISEC-04-20", "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitalization affects all industrial domains and causes disruption of\nvarious business models. Especially in domains such as logistics and\nmanufacturing, inter-connected devices and near-realtime exchange of sensor\ndata across enterprises allows to speed up processes, reduce costs and respond\nto customer's needs. However, the advent of the Industrial Internet of Things\n(IIoT) also raises challenges with respect to security and privacy of sensitive\nand personal data that is created by sensors and processed by services hosted\nin different administrative domains. The Industrial Data Space initiative\naddresses these challenges and proposes a secure edge gateway platform named\n\"Trusted Connector\". In this report, we introduce the main security building\nblocks of the Trusted Connector and point out how they help protecting\nbusiness-critical data and preserving the user's privacy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 09:23:30 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Sch\u00fctte", "Julian", ""], ["Brost", "Gerd", ""], ["Wessel", "Sascha", ""]]}, {"id": "1804.09494", "submitter": "Venkatesan Chakaravarthy", "authors": "Venkatesan T. Chakaravarthy, Jee W. Choi, Douglas J. Joseph, Prakash\n  Murali, Shivmaran S. Pandian, Yogish Sabharwal, Dheeraj Sreedhar", "title": "On Optimizing Distributed Tucker Decomposition for Sparse Tensors", "comments": "Abridged version of the paper to appear in the proceedings of ICS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tucker decomposition generalizes the notion of Singular Value\nDecomposition (SVD) to tensors, the higher dimensional analogues of matrices.\nWe study the problem of constructing the Tucker decomposition of sparse tensors\non distributed memory systems via the HOOI procedure, a popular iterative\nmethod. The scheme used for distributing the input tensor among the processors\n(MPI ranks) critically influences the HOOI execution time. Prior work has\nproposed different distribution schemes: an offline scheme based on\nsophisticated hypergraph partitioning method and simple, lightweight\nalternatives that can be used real-time. While the hypergraph based scheme\ntypically results in faster HOOI execution time, being complex, the time taken\nfor determining the distribution is an order of magnitude higher than the\nexecution time of a single HOOI iteration. Our main contribution is a\nlightweight distribution scheme, which achieves the best of both worlds. We\nshow that the scheme is near-optimal on certain fundamental metrics associated\nwith the HOOI procedure and as a result, near-optimal on the computational load\n(FLOPs). Though the scheme may incur higher communication volume, the\ncomputation time is the dominant factor and as the result, the scheme achieves\nbetter performance on the overall HOOI execution time. Our experimental\nevaluation on large real-life tensors (having up to 4 billion elements) shows\nthat the scheme outperforms the prior schemes on the HOOI execution time by a\nfactor of up to 3x. On the other hand, its distribution time is comparable to\nthe prior lightweight schemes and is typically lesser than the execution time\nof a single HOOI iteration.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 11:59:53 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 00:47:09 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chakaravarthy", "Venkatesan T.", ""], ["Choi", "Jee W.", ""], ["Joseph", "Douglas J.", ""], ["Murali", "Prakash", ""], ["Pandian", "Shivmaran S.", ""], ["Sabharwal", "Yogish", ""], ["Sreedhar", "Dheeraj", ""]]}, {"id": "1804.09536", "submitter": "Mikael Mortensen", "authors": "Lisandro Dalcin, Mikael Mortensen, David E Keyes", "title": "Fast parallel multidimensional FFT using advanced MPI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for performing global redistributions of\nmultidimensional arrays essential to parallel fast Fourier (or similar)\ntransforms. Traditional methods use standard all-to-all collective\ncommunication of contiguous memory buffers, thus necessary requiring local data\nrealignment steps intermixed in-between redistribution and transform steps.\nInstead, our method takes advantage of subarray datatypes and generalized\nall-to-all scatter/gather from the MPI-2 standard to communicate discontiguous\nmemory buffers, effectively eliminating the need for local data realignments.\nDespite generalized all-to-all communication of discontiguous data being\ngenerally slower, our proposal economizes in local work. For a range of strong\nand weak scaling tests, we found the overall performance of our method to be on\npar and often better than well-established libraries like MPI-FFTW, P3DFFT, and\n2DECOMP&FFT. We provide compact routines implemented at the highest possible\nlevel using the MPI bindings for the C programming language. These routines\napply to any global redistribution, over any two directions of a\nmultidimensional array, decomposed on arbitrary Cartesian processor grids (1D\nslabs, 2D pencils, or even higher-dimensional decompositions). The high level\nimplementation makes the code easy to read, maintain, and eventually extend.\nOur approach enables for future speedups from optimizations in the internal\ndatatype handling engines within MPI implementations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 13:20:53 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Dalcin", "Lisandro", ""], ["Mortensen", "Mikael", ""], ["Keyes", "David E", ""]]}, {"id": "1804.09738", "submitter": "Hang Liu", "authors": "Hang Liu, Yufei Ding, Da Zheng, Seung Woo Son, Da Yan", "title": "Challenges Towards Deploying Data Intensive Scientific Applications on\n  Extreme Heterogeneity Supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinking transistors, which powered the advancement of computing in the past\nhalf century, has stalled due to power wall; now extreme heterogeneity is\npromised to be the next driving force to feed the needs of ever-increasingly\ndiverse scientific domains. To unlock the potentials of such supercomputers, we\nidentify eight potential challenges in three categories: First, one needs fast\ndata movement since extreme heterogeneity will inevitably complicate the\ncommunication circuits -- thus hampering the data movement. Second, we need to\nintelligently schedule suitable hardware for corresponding applications/stages.\nThird, we have to lower the programming complexity in order to encourage the\nadoption of heterogeneous computing.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 18:16:59 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Liu", "Hang", ""], ["Ding", "Yufei", ""], ["Zheng", "Da", ""], ["Son", "Seung Woo", ""], ["Yan", "Da", ""]]}, {"id": "1804.09764", "submitter": "Langshi Chen", "authors": "Langshi Chen, Bo Peng, Sabra Ossen, Anil Vullikanti, Madhav Marathe,\n  Lei Jiang, Judy Qiu", "title": "High-Performance Massive Subgraph Counting using Pipelined\n  Adaptive-Group Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counting aims to count the number of occurrences of a subgraph T\n(aka as a template) in a given graph G. The basic problem has found\napplications in diverse domains. The problem is known to be computationally\nchallenging - the complexity grows both as a function of T and G. Recent\napplications have motivated solving such problems on massive networks with\nbillions of vertices. In this chapter, we study the subgraph counting problem\nfrom a parallel computing perspective. We discuss efficient parallel algorithms\nfor approximately resolving subgraph counting problems by using the\ncolor-coding technique. We then present several system-level strategies to\nsubstantially improve the overall performance of the algorithm in massive\nsubgraph counting problems. We propose: 1) a novel pipelined Adaptive-Group\ncommunication pattern to improve inter-node scalability, 2) a fine-grained\npipeline design to effectively reduce the memory space of intermediate results,\n3) partitioning neighbor lists of subgraph vertices to achieve better thread\nconcurrency and workload balance. Experimentation on an Intel Xeon E5 cluster\nshows that our implementation achieves 5x speedup of performance compared to\nthe state-of-the-art work while reduces the peak memory utilization by a factor\nof 2 on large templates of 12 to 15 vertices and input graphs of 2 to 5\nbillions of edges.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 19:25:14 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Chen", "Langshi", ""], ["Peng", "Bo", ""], ["Ossen", "Sabra", ""], ["Vullikanti", "Anil", ""], ["Marathe", "Madhav", ""], ["Jiang", "Lei", ""], ["Qiu", "Judy", ""]]}, {"id": "1804.09791", "submitter": "Sinong Wang", "authors": "Sinong Wang, Jiashang Liu, Ness Shroff, Pengyu Yang", "title": "Fundamental Limits of Coded Linear Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large scale distributed linear transform problems, coded computation plays\nan important role to effectively deal with \"stragglers\" (distributed\ncomputations that may get delayed due to few slow or faulty processors). We\npropose a coded computation strategy, referred to as diagonal code, that\nachieves the optimum recovery threshold and the optimum computation load. This\nis the first code that simultaneously achieves two-fold optimality in coded\ndistributed linear transforms. Furthermore, by leveraging the idea from random\nproposal graph theory, we design two random codes that can guarantee optimum\nrecovery threshold with high probability but with much less computation load.\nThese codes provide order-wise improvement over the state-of-the-art. Moreover,\nthe experimental results show significant improvement compared to both uncoded\nand existing coding schemes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 20:31:01 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Wang", "Sinong", ""], ["Liu", "Jiashang", ""], ["Shroff", "Ness", ""], ["Yang", "Pengyu", ""]]}, {"id": "1804.09798", "submitter": "Karen Devine", "authors": "Mehmet Deveci, Karen D. Devine, Kevin Pedretti, Mark A. Taylor,\n  Sivasankaran Rajamanickam, Umit V.Catalyurek", "title": "Geometric Partitioning and Ordering Strategies for Task Mapping on\n  Parallel Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": "SAND2018-4335R", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for mapping applications' MPI tasks to cores of a\nparallel computer such that applications' communication time is reduced. We\naddress the case of sparse node allocation, where the nodes assigned to a job\nare not necessarily located in a contiguous block nor within close proximity to\neach other in the network, although our methods generalize to contiguous\nallocations as well. The goal is to assign tasks to cores so that\ninterdependent tasks are performed by \"nearby\" cores, thus lowering the\ndistance messages must travel, the amount of congestion in the network, and the\noverall cost of communication. Our new method applies a geometric partitioning\nalgorithm to both the tasks and the processors, and assigns task parts to the\ncorresponding processor parts. We also present a number of algorithmic\noptimizations that exploit specific features of the network or application. We\nshow that, for the structured finite difference mini-application MiniGhost, our\nmapping methods reduced communication time up to 75% relative to MiniGhost's\ndefault mapping on 128K cores of a Cray XK7 with sparse allocation. For the\natmospheric modeling code E3SM/HOMME, our methods reduced communication time up\nto 31% on 32K cores of an IBM BlueGene/Q with contiguous allocation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 20:58:41 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Deveci", "Mehmet", ""], ["Devine", "Karen D.", ""], ["Pedretti", "Kevin", ""], ["Taylor", "Mark A.", ""], ["Rajamanickam", "Sivasankaran", ""], ["Catalyurek", "Umit V.", ""]]}, {"id": "1804.10001", "submitter": "Taro Sekiyama", "authors": "Taro Sekiyama, Takashi Imamichi, Haruki Imai, Rudy Raymond", "title": "Profile-guided memory optimization for deep neural networks", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen deep neural networks (DNNs) becoming wider and deeper\nto achieve better performance in many applications of AI. Such DNNs however\nrequire huge amounts of memory to store weights and intermediate results (e.g.,\nactivations, feature maps, etc.) in propagation. This requirement makes it\ndifficult to run the DNNs on devices with limited, hard-to-extend memory,\ndegrades the running time performance, and restricts the design of network\nmodels. We address this challenge by developing a novel profile-guided memory\noptimization to efficiently and quickly allocate memory blocks during the\npropagation in DNNs. The optimization utilizes a simple and fast heuristic\nalgorithm based on the two-dimensional rectangle packing problem. Experimenting\nwith well-known neural network models, we confirm that our method not only\nreduces the memory consumption by up to $49.5\\%$ but also accelerates training\nand inference by up to a factor of four thanks to the rapidity of the memory\nallocation and the ability to use larger mini-batch sizes.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 11:42:39 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Sekiyama", "Taro", ""], ["Imamichi", "Takashi", ""], ["Imai", "Haruki", ""], ["Raymond", "Rudy", ""]]}, {"id": "1804.10013", "submitter": "Federico Matteo Ben\\v{c}i\\'c", "authors": "Federico Matteo Ben\\v{c}i\\'c and Ivana Podnar \\v{Z}arko", "title": "Distributed Ledger Technology: Blockchain Compared to Directed Acyclic\n  Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays, blockchain is becoming a synonym for distributed ledger technology.\nHowever, blockchain is only one of the specializations in the field and is\ncurrently well-covered in existing literature, but mostly from a cryptographic\npoint of view. Besides blockchain technology, a new paradigm is gaining\nmomentum: directed acyclic graphs. The contribution presented in this paper is\ntwofold. Firstly, the paper analyzes distributed ledger technology with an\nemphasis on the features relevant to distributed systems. Secondly, the paper\nanalyses the usage of directed acyclic graph paradigm in the context of\ndistributed ledgers, and compares it with the blockchain-based solutions. The\ntwo paradigms are compared using representative implementations: Bitcoin,\nEthereum and Nano. We examine representative solutions in terms of the applied\ndata structures for maintaining the ledger, consensus mechanisms, transaction\nconfirmation confidence, ledger size, and scalability.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 12:13:10 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Ben\u010di\u0107", "Federico Matteo", ""], ["\u017darko", "Ivana Podnar", ""]]}, {"id": "1804.10028", "submitter": "John Klein", "authors": "John Klein, Mahmoud Albardan, Benjamin Guedj and Olivier Colot", "title": "Decentralized learning with budgeted network load using Gaussian copulas\n  and classifier ensembles", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-43823-4_26", "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a network of learners which address the same classification task\nbut must learn from different data sets. The learners cannot share data but\ninstead share their models. Models are shared only one time so as to preserve\nthe network load. We introduce DELCO (standing for Decentralized Ensemble\nLearning with COpulas), a new approach allowing to aggregate the predictions of\nthe classifiers trained by each learner. The proposed method aggregates the\nbase classifiers using a probabilistic model relying on Gaussian copulas.\nExperiments on logistic regressor ensembles demonstrate competing accuracy and\nincreased robustness in case of dependent classifiers. A companion python\nimplementation can be downloaded at https://github.com/john-klein/DELCO\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 12:53:58 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 07:33:39 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 07:38:13 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Klein", "John", ""], ["Albardan", "Mahmoud", ""], ["Guedj", "Benjamin", ""], ["Colot", "Olivier", ""]]}, {"id": "1804.10140", "submitter": "Lili Su", "authors": "Lili Su and Jiaming Xu", "title": "Securing Distributed Gradient Descent in High Dimensional Statistical\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider unreliable distributed learning systems wherein the training data\nis kept confidential by external workers, and the learner has to interact\nclosely with those workers to train a model. In particular, we assume that\nthere exists a system adversary that can adaptively compromise some workers;\nthe compromised workers deviate from their local designed specifications by\nsending out arbitrarily malicious messages.\n  We assume in each communication round, up to $q$ out of the $m$ workers\nsuffer Byzantine faults. Each worker keeps a local sample of size $n$ and the\ntotal sample size is $N=nm$. We propose a secured variant of the gradient\ndescent method that can tolerate up to a constant fraction of Byzantine\nworkers, i.e., $q/m = O(1)$. Moreover, we show the statistical estimation error\nof the iterates converges in $O(\\log N)$ rounds to $O(\\sqrt{q/N} +\n\\sqrt{d/N})$, where $d$ is the model dimension. As long as $q=O(d)$, our\nproposed algorithm achieves the optimal error rate $O(\\sqrt{d/N})$. Our results\nare obtained under some technical assumptions. Specifically, we assume\nstrongly-convex population risk. Nevertheless, the empirical risk (sample\nversion) is allowed to be non-convex. The core of our method is to robustly\naggregate the gradients computed by the workers based on the filtering\nprocedure proposed by Steinhardt et al. On the technical front, deviating from\nthe existing literature on robustly estimating a finite-dimensional mean\nvector, we establish a {\\em uniform} concentration of the sample covariance\nmatrix of gradients, and show that the aggregated gradient, as a function of\nmodel parameter, converges uniformly to the true gradient function. To get a\nnear-optimal uniform concentration bound, we develop a new matrix concentration\ninequality, which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:09:51 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 19:25:25 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 17:21:17 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Su", "Lili", ""], ["Xu", "Jiaming", ""]]}, {"id": "1804.10166", "submitter": "Choiru Za'in", "authors": "Choiru Za'in and Mahardhika Pratama and Andri Ashfahani and Eric\n  Pardede and Huang Sheng", "title": "Big Data Analytic based on Scalable PANFIS for RFID Localization", "comments": "7 pages and 3 figures, IEEE SMC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RFID technology has gained popularity to address localization problem in the\nmanufacturing shopfloor due to its affordability and easiness in deployment.\nThis technology is used to track the manufacturing object location to increase\nthe production efficiency. However, the data used for localization task is not\neasy to analyze because it is generated from the non-stationary environment. It\nalso continuously arrive over time and yields the large-volume of data.\nTherefore, an advanced big data analytic is required to overcome this problem.\nWe propose a distributed big data analytic framework based on PANFIS (Scalable\nPANFIS), where PANFIS is an evolving algorithm which has capability to learn\ndata stream in the single pass mode. Scalable PANFIS can learn big data stream\nby processing many chunks/partitions of data stream. Scalable PANFIS is also\nequipped with rule structure merging to eliminate the redundancy among rules.\nScalable PANFIS is validated by measuring its performance against single PANFIS\nand other Spark scalable machine learning algorithms. The result shows that\nScalable PANFIS performs running time more than 20 times faster than single\nPANFIS. The rule merging process in Scalable PANFIS shows that there is no\nsignificant reduction of accuracy in classification task with 96.67 percent of\naccuracy in comparison with single PANFIS of 98.71 percent. Scalable PANFIS\nalso generally outperforms some Spark MLib machine learnings to classify RFID\ndata with the comparable speed in running time.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:45:29 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Za'in", "Choiru", ""], ["Pratama", "Mahardhika", ""], ["Ashfahani", "Andri", ""], ["Pardede", "Eric", ""], ["Sheng", "Huang", ""]]}, {"id": "1804.10223", "submitter": "Jeff Pool", "authors": "Feiwen Zhu, Jeff Pool, Michael Andersch, Jeremy Appleyard, Fung Xie", "title": "Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are powerful tools for solving\nsequence-based problems, but their efficacy and execution time are dependent on\nthe size of the network. Following recent work in simplifying these networks\nwith model pruning and a novel mapping of work onto GPUs, we design an\nefficient implementation for sparse RNNs. We investigate several optimizations\nand tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight\nlayout. With these optimizations, we achieve speedups of over 6x over the next\nbest algorithm for a hidden layer of size 2304, batch size of 4, and a density\nof 30%. Further, our technique allows for models of over 5x the size to fit on\na GPU for a speedup of 2x, enabling larger networks to help advance the\nstate-of-the-art. We perform case studies on NMT and speech recognition tasks\nin the appendix, accelerating their recurrent layers by up to 3x.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 18:18:57 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Zhu", "Feiwen", ""], ["Pool", "Jeff", ""], ["Andersch", "Michael", ""], ["Appleyard", "Jeremy", ""], ["Xie", "Fung", ""]]}, {"id": "1804.10331", "submitter": "Ankur Mallick", "authors": "Ankur Mallick, Malhar Chaudhari, Utsav Sheth, Ganesh Palanikumar,\n  Gauri Joshi", "title": "Rateless Codes for Near-Perfect Load Balancing in Distributed\n  Matrix-Vector Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale machine learning and data mining applications require computer\nsystems to perform massive matrix-vector and matrix-matrix multiplication\noperations that need to be parallelized across multiple nodes. The presence of\nstraggling nodes -- computing nodes that unpredictably slowdown or fail -- is a\nmajor bottleneck in such distributed computations. Ideal load balancing\nstrategies that dynamically allocate more tasks to faster nodes require\nknowledge or monitoring of node speeds as well as the ability to quickly move\ndata. Recently proposed fixed-rate erasure coding strategies can handle\nunpredictable node slowdown, but they ignore partial work done by straggling\nnodes thus resulting in a lot of redundant computation. We propose a\n\\emph{rateless fountain coding} strategy that achieves the best of both worlds\n-- we prove that its latency is asymptotically equal to ideal load balancing,\nand it performs asymptotically zero redundant computations. Our idea is to\ncreate linear combinations of the $m$ rows of the matrix and assign these\nencoded rows to different worker nodes. The original matrix-vector product can\nbe decoded as soon as slightly more than $m$ row-vector products are\ncollectively finished by the nodes. We conduct experiments in three computing\nenvironments: local parallel computing, Amazon EC2, and Amazon Lambda, which\nshow that rateless coding gives as much as $3\\times$ speed-up over uncoded\nschemes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 03:41:04 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 15:06:01 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 02:15:33 GMT"}, {"version": "v4", "created": "Wed, 31 Oct 2018 23:45:53 GMT"}, {"version": "v5", "created": "Wed, 30 Oct 2019 21:39:14 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Mallick", "Ankur", ""], ["Chaudhari", "Malhar", ""], ["Sheth", "Utsav", ""], ["Palanikumar", "Ganesh", ""], ["Joshi", "Gauri", ""]]}, {"id": "1804.10355", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert, Ioana Cristescu (TAMIS)", "title": "History-Preserving Bisimulations on Reversible Calculus of Communicating\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  History-and hereditary history-preserving bisimulation (HPB and HHPB) are\nequivalences relations for denotational models of concurrency. Finding their\ncounterpart in process algebras is an open problem, with some partial\nsuccesses: there exists in calculus of communicating systems (CCS) an\nequivalence based on causal trees that corresponds to HPB. In Reversible CSS\n(RCCS), there is a bisimulation that corresponds to HHPB, but it considers only\nprocesses without auto-concurrency. We propose equivalences on CCS with\nauto-concurrency that correspond to HPB and HHPB, and their so-called \"weak\"\nvariants. The equivalences exploit not only reversibility but also the memory\nmechanism of RCCS.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 06:22:56 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "TAMIS"], ["Cristescu", "Ioana", "", "TAMIS"]]}, {"id": "1804.10597", "submitter": "Wojciech Golab", "authors": "Wojciech Golab", "title": "Recoverable Consensus in Shared Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herlihy's consensus hierarchy ranks the power of various synchronization\nprimitives for solving consensus in a model where asynchronous processes\ncommunicate through shared memory and fail by halting. This paper revisits the\nconsensus hierarchy in a model with crash-recovery failures, where the\nspecification of consensus, called \\emph{recoverable consensus} in this paper,\nis weakened by allowing non-terminating executions when a process fails\ninfinitely often. Two variations of this model are considered: independent\nfailures, and simultaneous (i.e., system-wide) failures. Several results are\nproved in this model: (i) We prove that any primitive at level two of Herlihy's\nhierarchy remains at level two if simultaneous crash-recovery failures are\nintroduced. This is accomplished by transforming (one instance of) any\n2-process conventional consensus algorithm to a 2-process recoverable consensus\nalgorithm. (ii) For any $n > 1$ and $f > 0$, we show how to use $f+1$ instances\nof any conventional $n$-process consensus algorithm and $\\Theta(f + n)$\nread/write registers to solve $n$-process recoverable consensus when\ncrash-recovery failures are independent, assuming that every execution contains\nat most $f$ such failures. (iii) Next, we prove for any $f > 0$ that any\n2-process recoverable consensus algorithm that uses TAS and read/writer\nregisters requires at least $f+1$ TAS objects, assuming that crash-recovery\nfailures are independent and every execution contains at most $f$ such\nfailures. (iv) Lastly, we generalize and strengthen (iii) by proving that any\nuniversal construction of $n$-process recoverable consensus from a type $T$\nwith consensus number $n$ and read/write registers requires at least $f+1$ base\nobjects of type $T$ in executions with up to $f$ failures.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 17:26:56 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 20:44:01 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Golab", "Wojciech", ""]]}, {"id": "1804.10642", "submitter": "Bichen Wu", "authors": "Kiseok Kwon, Alon Amid, Amir Gholami, Bichen Wu, Krste Asanovic, Kurt\n  Keutzer", "title": "Co-Design of Deep Neural Nets and Neural Net Accelerators for Embedded\n  Vision Applications", "comments": "This paper is trimmed to 6 pages to meet the conference requirement.\n  A longer version with more detailed discussion will be released afterwards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is arguably the most rapidly evolving research area in recent\nyears. As a result it is not surprising that the design of state-of-the-art\ndeep neural net models proceeds without much consideration of the latest\nhardware targets, and the design of neural net accelerators proceeds without\nmuch consideration of the characteristics of the latest deep neural net models.\nNevertheless, in this paper we show that there are significant improvements\navailable if deep neural net models and neural net accelerators are\nco-designed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 01:39:33 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kwon", "Kiseok", ""], ["Amid", "Alon", ""], ["Gholami", "Amir", ""], ["Wu", "Bichen", ""], ["Asanovic", "Krste", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1804.10694", "submitter": "R. Baghdadi", "authors": "Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo,\n  Abdurrahman Akkas, Yunming Zhang, Patricia Suriana, Shoaib Kamil, Saman\n  Amarasinghe", "title": "Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.00419", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.MS cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Tiramisu, a polyhedral framework designed to generate\nhigh performance code for multiple platforms including multicores, GPUs, and\ndistributed machines. Tiramisu introduces a scheduling language with novel\nextensions to explicitly manage the complexities that arise when targeting\nthese systems. The framework is designed for the areas of image processing,\nstencils, linear algebra and deep learning. Tiramisu has two main features: it\nrelies on a flexible representation based on the polyhedral model and it has a\nrich scheduling language allowing fine-grained control of optimizations.\nTiramisu uses a four-level intermediate representation that allows full\nseparation between the algorithms, loop transformations, data layouts, and\ncommunication. This separation simplifies targeting multiple hardware\narchitectures with the same algorithm. We evaluate Tiramisu by writing a set of\nimage processing, deep learning, and linear algebra benchmarks and compare them\nwith state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu\nmatches or outperforms existing compilers and libraries on different hardware\narchitectures, including multicore CPUs, GPUs, and distributed machines.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 21:28:44 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 19:58:57 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 21:24:44 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 02:41:00 GMT"}, {"version": "v5", "created": "Thu, 20 Dec 2018 16:25:40 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Baghdadi", "Riyadh", ""], ["Ray", "Jessica", ""], ["Romdhane", "Malek Ben", ""], ["Del Sozzo", "Emanuele", ""], ["Akkas", "Abdurrahman", ""], ["Zhang", "Yunming", ""], ["Suriana", "Patricia", ""], ["Kamil", "Shoaib", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1804.10919", "submitter": "Patrick Lambein-Monette", "authors": "Bernadette Charron-Bost, Patrick Lambein-Monette", "title": "Randomization and quantization for average consensus", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of problems in distributed control involve a networked system of\nautonomous agents cooperating to carry out some complex task in a decentralized\nfashion, e.g., orienting a flock of drones, or aggregating data from a network\nof sensors. Many of these complex tasks reduce to the computation of a global\nfunction of values privately held by the agents, such as the maximum or the\naverage. Distributed algorithms implementing these functions should rely on\nlimited assumptions on the topology of the network or the information available\nto the agents, reflecting the decentralized nature of the problem.\n  We present a randomized algorithm for computing the average in networks with\ndirected, time-varying communication topologies. With high probability, the\nsystem converges to an estimate of the average in linear time in the number of\nagents, provided that the communication topology remains strongly connected\nover time. This algorithm leverages properties of exponential random variables,\nwhich allows for approximating sums by computing minima. It is completely\ndecentralized, in the sense that it does not rely on agent identifiers, or\nglobal information of any kind. Besides, the agents do not need to know their\nout-degree; hence, our algorithm demonstrates how randomization can be used to\ncircumvent the impossibility result established in [1].\n  Using a logarithmic rounding rule, we show that this algorithm can be used\nunder the additional constraints of finite memory and channel capacity. We\nfurthermore extend the algorithm with a termination test, by which the agents\ncan decide irrevocably in finite time - rather than simply converge - on an\nestimate of the average. This terminating variant works under asynchronous\nstarts and yields linear decision times while still using quantized - albeit\nlarger - values.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 12:28:43 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Charron-Bost", "Bernadette", ""], ["Lambein-Monette", "Patrick", ""]]}, {"id": "1804.11026", "submitter": "Xiaoye Li", "authors": "Juliette Ugirumurera, Gabriel Gomes, Emily Porter, Xiaoye S. Li,\n  Alexandre Bayen", "title": "A unified software framework for solving traffic assignment problems", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a software framework for solving user equilibrium traffic\nassignment problems. The design is based on the formulation of the problem as a\nvariational inequality. The software implements these as well as several\nnumerical methods for find equilirbria. We compare the solutions obtained under\nseveral models: static, Merchant-Nemhauser, `CTM with instantaneous travel\ntime', and `CTM with actual travel time'. Some important differences are\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 02:24:55 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Ugirumurera", "Juliette", ""], ["Gomes", "Gabriel", ""], ["Porter", "Emily", ""], ["Li", "Xiaoye S.", ""], ["Bayen", "Alexandre", ""]]}, {"id": "1804.11115", "submitter": "Ali Mohammed", "authors": "Ali Mohammed, Ahmed Eleliemy, Florina M. Ciorba, Franziska Kasielke,\n  Ioana Banicescu", "title": "Experimental Verification and Analysis of Dynamic Loop Scheduling in\n  Scientific Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications are often irregular and characterized by large\ncomputationally-intensive parallel loops. Dynamic loop scheduling (DLS)\ntechniques improve the performance of computationally-intensive scientific\napplications via load balancing of their execution on high-performance\ncomputing (HPC) systems. Identifying the most suitable choices of data\ndistribution strategies, system sizes, and DLS techniques which improve the\nperformance of a given application, requires intensive assessment and a large\nnumber of exploratory native experiments (using real applications on real\nsystems), which may not always be feasible or practical due to associated time\nand costs. In such cases, simulative experiments are more appropriate for\nstudying the performance of applications. This motivates the question of How\nrealistic are the simulations of executions of scientific applications using\nDLS on HPC platforms? In the present work, a methodology is devised to answer\nthis question. It involves the experimental verification and analysis of the\nperformance of DLS in scientific applications. The proposed methodology is\nemployed for a computer vision application executing using four DLS techniques\non two different HPC plat- forms, both via native and simulative experiments.\nThe evaluation and analysis of the native and simulative results indicate that\nthe accuracy of the simulative experiments is strongly influenced by the\napproach used to extract the computational effort of the application (FLOP- or\ntime-based), the choice of application model representation into simulation\n(data or task parallel), and the available HPC subsystem models in the\nsimulator (multi-core CPUs, memory hierarchy, and network topology). The\nminimum and the maximum percent errors achieved between the native and the\nsimulative experiments are 0.95% and 8.03%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 10:53:10 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Mohammed", "Ali", ""], ["Eleliemy", "Ahmed", ""], ["Ciorba", "Florina M.", ""], ["Kasielke", "Franziska", ""], ["Banicescu", "Ioana", ""]]}, {"id": "1804.11239", "submitter": "Caiwen Ding", "authors": "Caiwen Ding, Ao Ren, Geng Yuan, Xiaolong Ma, Jiayu Li, Ning Liu, Bo\n  Yuan, Yanzhi Wang", "title": "Structured Weight Matrices-Based Hardware Accelerators in Deep Neural\n  Networks: FPGAs and ASICs", "comments": "6 pages, 7 figures, GLSVLSI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both industry and academia have extensively investigated hardware\naccelerations. In this work, to address the increasing demands in computational\ncapability and memory requirement, we propose structured weight matrices\n(SWM)-based compression techniques for both \\emph{field programmable gate\narray} (FPGA) and \\emph{application-specific integrated circuit} (ASIC)\nimplementations. In algorithm part, SWM-based framework adopts block-circulant\nmatrices to achieve a fine-grained tradeoff between accuracy and compression\nratio. The SWM-based technique can reduce computational complexity from\nO($n^2$) to O($n\\log n$) and storage complexity from O($n^2$) to O($n$) for\neach layer and both training and inference phases. For FPGA implementations on\ndeep convolutional neural networks (DCNNs), we achieve at least 152X and 72X\nimprovement in performance and energy efficiency, respectively using the\nSWM-based framework, compared with the baseline of IBM TrueNorth processor\nunder same accuracy constraints using the data set of MNIST, SVHN, and\nCIFAR-10. For FPGA implementations on long short term memory (LSTM) networks,\nthe proposed SWM-based LSTM can achieve up to 21X enhancement in performance\nand 33.5X gains in energy efficiency compared with the baseline accelerator.\nFor ASIC implementations, the SWM-based ASIC design exhibits impressive\nadvantages in terms of power, throughput, and energy efficiency. Experimental\nresults indicate that this method is greatly suitable for applying DNNs onto\nboth FPGAs and mobile/IoT devices.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 19:57:54 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Ding", "Caiwen", ""], ["Ren", "Ao", ""], ["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Li", "Jiayu", ""], ["Liu", "Ning", ""], ["Yuan", "Bo", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1804.11256", "submitter": "Ammar Qammaz", "authors": "Ammar Qammaz, Sokol Kosta, Nikolaos Kyriazis, Antonis Argyros", "title": "On the Feasibility of Real-Time 3D Hand Tracking using Edge GPGPU\n  Acceleration", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the case study of a non-intrusive porting of a monolithic\nC++ library for real-time 3D hand tracking, to the domain of edge-based\ncomputation. Towards a proof of concept, the case study considers a pair of\nworkstations, a computationally powerful and a computationally weak one. By\nwrapping the C++ library in Java container and by capitalizing on a Java-based\noffloading infrastructure that supports both CPU and GPGPU computations, we are\nable to establish automatically the required server-client workflow that best\naddresses the resource allocation problem in the effort to execute from the\nweak workstation. As a result, the weak workstation can perform well at the\ntask, despite lacking the sufficient hardware to do the required computations\nlocally. This is achieved by offloading computations which rely on GPGPU, to\nthe powerful workstation, across the network that connects them. We show the\nedge-based computation challenges associated with the information flow of the\nported algorithm, demonstrate how we cope with them, and identify what needs to\nbe improved for achieving even better performance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 15:00:40 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Qammaz", "Ammar", ""], ["Kosta", "Sokol", ""], ["Kyriazis", "Nikolaos", ""], ["Argyros", "Antonis", ""]]}, {"id": "1804.11268", "submitter": "Dingwen Tao", "authors": "Dingwen Tao, Sheng Di, Xin Liang, Zizhong Chen, Franck Cappello", "title": "Improving Performance of Iterative Methods by Lossy Checkponting", "comments": "14 pages, 10 figures, HPDC'18", "journal-ref": null, "doi": "10.1145/3208040.3208050", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative methods are commonly used approaches to solve large, sparse linear\nsystems, which are fundamental operations for many modern scientific\nsimulations. When the large-scale iterative methods are running with a large\nnumber of ranks in parallel, they have to checkpoint the dynamic variables\nperiodically in case of unavoidable fail-stop errors, requiring fast I/O\nsystems and large storage space. To this end, significantly reducing the\ncheckpointing overhead is critical to improving the overall performance of\niterative methods. Our contribution is fourfold. (1) We propose a novel lossy\ncheckpointing scheme that can significantly improve the checkpointing\nperformance of iterative methods by leveraging lossy compressors. (2) We\nformulate a lossy checkpointing performance model and derive theoretically an\nupper bound for the extra number of iterations caused by the distortion of data\nin lossy checkpoints, in order to guarantee the performance improvement under\nthe lossy checkpointing scheme. (3) We analyze the impact of lossy\ncheckpointing (i.e., extra number of iterations caused by lossy checkpointing\nfiles) for multiple types of iterative methods. (4)We evaluate the lossy\ncheckpointing scheme with optimal checkpointing intervals on a high-performance\ncomputing environment with 2,048 cores, using a well-known scientific\ncomputation package PETSc and a state-of-the-art checkpoint/restart toolkit.\nExperiments show that our optimized lossy checkpointing scheme can\nsignificantly reduce the fault tolerance overhead for iterative methods by\n23%~70% compared with traditional checkpointing and 20%~58% compared with\nlossless-compressed checkpointing, in the presence of system failures.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 15:12:32 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 15:56:32 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 01:59:59 GMT"}, {"version": "v4", "created": "Tue, 29 May 2018 02:50:36 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Tao", "Dingwen", ""], ["Di", "Sheng", ""], ["Liang", "Xin", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "1804.11312", "submitter": "Adri\\'an Bazaga", "authors": "Adrian Bazaga and Michal Pitonak", "title": "Performance Evaluation of an Algorithm-based Asynchronous\n  Checkpoint-Restart Fault Tolerant Application Using Mixed MPI/GPI-2", "comments": "Submitted to conference EuroMPI/USA'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the hardest challenges of the current Big Data landscape is the lack\nof ability to process huge volumes of information in an acceptable time. The\ngoal of this work, is to ascertain if it is useful to use typical Big Data\ntools to solve High Performance Computing problems, by exploring and comparing\na distributed computing framework implemented on a commodity cluster\narchitecture: the experiment will depend on the computational time required\nusing tools such as Apache Spark. This will be compared to \"equivalent more\ntraditional\" approaches such as using a distributed memory model with MPI on a\ndistributed file system such as HDFS (Hadoop Distributed File System) and\nnative C libraries that create an interface to encapsulate this file system\nfunctionalities, and using the GPI-2 implementation for the GASPI protocol and\nit's in-memory checkpointing library to provide an application with Fault\nTolerance features. To be more precise, we've chosen the K-means algorithm as\nexperiment, that will be ran on variable size datasets, and then we will\ncompare the computational run time and time resilience of both approaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 16:38:38 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 14:52:13 GMT"}, {"version": "v3", "created": "Sun, 6 May 2018 22:29:55 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Bazaga", "Adrian", ""], ["Pitonak", "Michal", ""]]}]