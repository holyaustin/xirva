[{"id": "2005.00081", "submitter": "Da Yan", "authors": "Guimu Guo, Da Yan, M. Tamer \\\"Ozsu, Zhe Jiang, Jalal Khalil", "title": "Scalable Mining of Maximal Quasi-Cliques: An Algorithm-System Codesign\n  Approach", "comments": "Guimu Guo and Da Yan are parallel first authors; this is the full\n  version of our PVLDB 2021 paper with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a user-specified minimum degree threshold $\\gamma$, a\n$\\gamma$-quasi-clique is a subgraph $g=(V_g,E_g)$ where each vertex $v\\in V_g$\nconnects to at least $\\gamma$ fraction of the other vertices (i.e., $\\lceil\n\\gamma\\cdot(|V_g|-1)\\rceil$ vertices) in $g$. Quasi-clique is one of the most\nnatural definitions for dense structures useful in finding communities in\nsocial networks and discovering significant biomolecule structures and\npathways. However, mining maximal quasi-cliques is notoriously expensive.\n  In this paper, we design parallel algorithms for mining maximal quasi-cliques\non G-thinker, a recent distributed framework targeting divide-and-conquer graph\nmining algorithms that decomposes the mining into compute-intensive tasks to\nfully utilize CPU cores. However, we found that directly using G-thinker\nresults in the straggler problem due to (i) the drastic load imbalance among\ndifferent tasks and (ii) the difficulty of predicting the task running time and\nthe time growth with task-subgraph size. We address these challenges by\nredesigning G-thinker's execution engine to prioritize long-running tasks for\nmining, and by utilizing a novel timeout strategy to effectively decompose the\nmining workloads of long-running tasks to improve load balancing. While this\nsystem redesign applies to many other expensive dense subgraph mining problems,\nthis paper verifies the idea by adapting the state-of-the-art quasi-clique\nalgorithm, Quick, to our redesigned G-thinker. We improve Quick by integrating\nnew pruning rules, and fixing some missed boundary cases that could lead to\nmissed results. Extensive experiments verify that our new solution scales well\nwith the number of CPU cores, achieving 201$\\times$ runtime speedup when mining\na graph with 3.77M vertices and 16.5M edges in a 16-node cluster.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 20:03:06 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 05:46:54 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 11:54:18 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2020 01:29:14 GMT"}, {"version": "v5", "created": "Fri, 11 Dec 2020 16:33:56 GMT"}, {"version": "v6", "created": "Tue, 16 Feb 2021 15:03:40 GMT"}, {"version": "v7", "created": "Sun, 21 Mar 2021 01:52:53 GMT"}, {"version": "v8", "created": "Mon, 10 May 2021 13:23:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Guo", "Guimu", ""], ["Yan", "Da", ""], ["\u00d6zsu", "M. Tamer", ""], ["Jiang", "Zhe", ""], ["Khalil", "Jalal", ""]]}, {"id": "2005.00124", "submitter": "Shigang Li", "authors": "Shigang Li, Tal Ben-Nun, Giorgi Nadiradze, Salvatore Di Girolamo,\n  Nikoli Dryden, Dan Alistarh, Torsten Hoefler", "title": "Breaking (Global) Barriers in Parallel Stochastic Optimization with\n  Wait-Avoiding Group Averaging", "comments": "Published in IEEE Transactions on Parallel and Distributed Systems\n  (IEEE TPDS), vol. 32, no. 7, pp. 1725-1739, 1 July 2021", "journal-ref": "in IEEE Transactions on Parallel and Distributed Systems, vol. 32,\n  no. 7, pp. 1725-1739, 1 July 2021", "doi": "10.1109/TPDS.2020.3040606", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning at scale is dominated by communication time. Distributing\nsamples across nodes usually yields the best performance, but poses scaling\nchallenges due to global information dissemination and load imbalance across\nuneven sample lengths. State-of-the-art decentralized optimizers mitigate the\nproblem, but require more iterations to achieve the same accuracy as their\nglobally-communicating counterparts. We present Wait-Avoiding Group Model\nAveraging (WAGMA) SGD, a wait-avoiding stochastic optimizer that reduces global\ncommunication via subgroup weight exchange. The key insight is a combination of\nalgorithmic changes to the averaging scheme and the use of a group allreduce\noperation. We prove the convergence of WAGMA-SGD, and empirically show that it\nretains convergence rates similar to Allreduce-SGD. For evaluation, we train\nResNet-50 on ImageNet; Transformer for machine translation; and deep\nreinforcement learning for navigation at scale. Compared with state-of-the-art\ndecentralized SGD variants, WAGMA-SGD significantly improves training\nthroughput (e.g., 2.1x on 1,024 GPUs for reinforcement learning), and achieves\nthe fastest time-to-solution (e.g., the highest score using the shortest\ntraining time for Transformer).\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:11:53 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 09:26:19 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2021 15:36:09 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Li", "Shigang", ""], ["Ben-Nun", "Tal", ""], ["Nadiradze", "Giorgi", ""], ["Di Girolamo", "Salvatore", ""], ["Dryden", "Nikoli", ""], ["Alistarh", "Dan", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2005.00224", "submitter": "Prashant Khanduri", "authors": "Prashant Khanduri, Pranay Sharma, Swatantra Kafle, Saikiran Bulusu,\n  Ketan Rajawat and Pramod K. Varshney", "title": "Distributed Stochastic Non-Convex Optimization: Momentum-Based Variance\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a distributed algorithm for stochastic non-convex\noptimization. We consider a worker-server architecture where a set of $K$\nworker nodes (WNs) in collaboration with a server node (SN) jointly aim to\nminimize a global, potentially non-convex objective function. The objective\nfunction is assumed to be the sum of local objective functions available at\neach WN, with each node having access to only the stochastic samples of its\nlocal objective function. In contrast to the existing approaches, we employ a\nmomentum based \"single loop\" distributed algorithm which eliminates the need of\ncomputing large batch size gradients to achieve variance reduction. We propose\ntwo algorithms one with \"adaptive\" and the other with \"non-adaptive\" learning\nrates. We show that the proposed algorithms achieve the optimal computational\ncomplexity while attaining linear speedup with the number of WNs. Specifically,\nthe algorithms reach an $\\epsilon$-stationary point $x_a$ with $\\mathbb{E}\\|\n\\nabla f(x_a) \\| \\leq \\tilde{O}(K^{-1/3}T^{-1/2} + K^{-1/3}T^{-1/3})$ in $T$\niterations, thereby requiring $\\tilde{O}(K^{-1} \\epsilon^{-3})$ gradient\ncomputations at each WN. Moreover, our approach does not assume identical data\ndistributions across WNs making the approach general enough for federated\nlearning applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 05:04:38 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Khanduri", "Prashant", ""], ["Sharma", "Pranay", ""], ["Kafle", "Swatantra", ""], ["Bulusu", "Saikiran", ""], ["Rajawat", "Ketan", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "2005.00263", "submitter": "Rohit Zambre", "authors": "Rohit Zambre, Aparna Chandramowlishwaran, Pavan Balaji", "title": "How I Learned to Stop Worrying About User-Visible Endpoints and Love MPI", "comments": "In Proceedings of the 34th ACM International Conference on\n  Supercomputing (ICS), Barcelona, Spain, June 2020", "journal-ref": null, "doi": "10.1145/3392717.3392773", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MPI+threads is gaining prominence as an alternative to the traditional MPI\neverywhere model in order to better handle the disproportionate increase in the\nnumber of cores compared with other on-node resources. However, the\ncommunication performance of MPI+threads can be 100x slower than that of MPI\neverywhere. Both MPI users and developers are to blame for this slowdown.\nTypically, MPI users do not expose logical communication parallelism.\nConsequently, MPI libraries use conservative approaches, such as a global\ncritical section, to maintain MPI's ordering constraints for MPI+threads, thus\nserializing access to parallel network resources and hurting performance.\n  To enhance MP+threads' communication performance, researchers have proposed\nMPI Endpoints as a user-visible extension to MPI-3.1. MPI Endpoints allows a\nsingle process to create multiple MPI ranks within a communicator. This could\nallow each thread to have a dedicated communication path to the network and\nimprove performance. The onus of mapping threads to endpoints, however, would\nthen be on domain scientists. We play the role of devil's advocate and question\nthe need for user-visible endpoints. We certainly agree that dedicated\ncommunication channels are critical. To what extent, however, can we hide these\nchannels inside the MPI library without modifying the MPI standard and thus\nunburden the user? More important, what functionality would we lose through\nsuch abstraction? This paper answers these questions through a new MPI-3.1\nimplementation that uses virtual communication interfaces (VCIs). VCIs abstract\nunderlying network contexts. When users expose parallelism through existing MPI\nmechanisms, the MPI library maps that parallelism to the VCIs, relieving domain\nscientists from endpoints. We identify cases where VCIs perform as well as\nuser-visible endpoints, as well as cases where such abstraction hurts\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 07:54:11 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Zambre", "Rohit", ""], ["Chandramowlishwaran", "Aparna", ""], ["Balaji", "Pavan", ""]]}, {"id": "2005.00270", "submitter": "Zeinab Nezami", "authors": "Zeinab Nezami, Kamran Zamanifar, Karim Djemame, and Evangelos\n  Pournaras", "title": "Decentralized Edge-to-Cloud Load-balancing: Service Placement for the\n  Internet of Things", "comments": "17 pages and 15 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3074962", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) requires a new processing paradigm that inherits\nthe scalability of the cloud while minimizing network latency using resources\ncloser to the network edge. Building up such flexibility within the\nedge-to-cloud continuum consisting of a distributed networked ecosystem of\nheterogeneous computing resources is challenging. Furthermore, IoT traffic\ndynamics and the rising demand for low-latency services foster the need for\nminimizing the response time and balanced service placement. Load-balancing for\nfog computing becomes a cornerstone for cost-effective system management and\noperations. This paper studies two optimization objectives and formulates a\ndecentralized load-balancing problem for IoT service placement: (global) IoT\nworkload balance and (local) quality of service (QoS), in terms of minimizing\nthe cost of deadline violation, service deployment, and unhosted services. The\nproposed solution, EPOS Fog, introduces a decentralized multi-agent system for\ncollective learning that utilizes edge-to-cloud nodes to jointly balance the\ninput workload across the network and minimize the costs involved in service\nexecution. The agents locally generate possible assignments of requests to\nresources and then cooperatively select an assignment such that their\ncombination maximizes edge utilization while minimizes service execution cost.\nExtensive experimental evaluation with realistic Google cluster workloads on\nvarious networks demonstrates the superior performance of EPOS Fog in terms of\nworkload balance and QoS, compared to approaches such as First Fit and\nexclusively Cloud-based. The results confirm that EPOS Fog reduces service\nexecution delay up to 25% and the load-balance of network nodes up to 90%. The\nfindings also demonstrate how distributed computational resources on the edge\ncan be utilized more cost-effectively by harvesting collective intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 08:42:41 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 07:40:42 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 07:52:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Nezami", "Zeinab", ""], ["Zamanifar", "Kamran", ""], ["Djemame", "Karim", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "2005.00338", "submitter": "Reem Abdel-Rahman Osman Mahmoud", "authors": "Reem Abdel-Rahman Osman, Omar H. Karam", "title": "Performance of Generalized Hypercubes in Dynamic Peer-to-Peer Networks", "comments": "The subscribed author submitted the paper without agreement with the\n  other author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly dynamic peer-to-peer networks are becoming very significant due to\ntheir wide range of applications. Although many structures were proposed to\ndeploy peer-to-peer networks, hypercube structures could grasp the researchers'\nattention for study because of their desirable properties. A lot of studies\nhave considered binary hypercubes for research. In this paper, we study more\ngeneralized topology of hypercubes, namely generalized hypercubes and measure\ntheir performance in dynamic networks where nodes can enter and leave the\nnetwork at any time. Python code was developed to simulate the networks and\nvisualize the results. In addition to being highly flexible and scalable,\ngeneralized hypercubes proved to have lower average path length and lower verge\ndensity than binary hypercubes.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:28:49 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 14:18:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Osman", "Reem Abdel-Rahman", ""], ["Karam", "Omar H.", ""]]}, {"id": "2005.00445", "submitter": "Ghafour Ahani", "authors": "Ghafour Ahani, Di Yuan", "title": "Optimal Scheduling of Age-centric Caching: Tractability and Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": "10348190", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of age of information (AoI) has become an important performance\nmetric in network and control systems. Information freshness, represented by\nAoI, naturally arises in the context of caching. We address optimal scheduling\nof cache updates for a time-slotted system where the contents vary in size.\nThere is limited capacity for the cache and for making content updates. Each\ncontent is associated with a utility function that is monotonically decreasing\nin the AoI. For this combinatorial optimization problem, we present the\nfollowing contributions. First, we provide theoretical results settling the\nboundary of problem tractability. In particular, by a reformulation using\nnetwork flows, we prove the boundary is essentially determined by whether or\nnot the contents are of equal size. Second, we derive an integer linear\nformulation for the problem, of which the optimal solution can be obtained for\nsmall-scale scenarios. Next, via a mathematical reformulation, we derive a\nscalable optimization algorithm using repeated column generation. In addition,\nthe algorithm computes a bound of global optimum, that can be used to assess\nthe performance of any scheduling solution. Performance evaluation of\nlarge-scale scenarios demonstrates the strengths of the algorithm in comparison\nto a greedy schedule. Finally, we extend the applicability of our work to\ncyclic scheduling.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 01:06:02 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Ahani", "Ghafour", ""], ["Yuan", "Di", ""]]}, {"id": "2005.00658", "submitter": "Gokhan Sagirlar Dr", "authors": "Gokhan Sagirlar and John D. Sheehan and Emanuele Ragnoli", "title": "On the Design of Co-operating Blockchains for IoT", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling blockchain technology into IoT can help to achieve a proper\ndistributed consensus based IoT system that overcomes disadvantages of today's\ncentralized infrastructures, such as, among others, high cloud server\nmaintenance costs, weakness for supporting time-critical IoT applications,\nsecurity and trust issues. However, meeting requirements posed by IoT in\nblockchain domain is not an easy endeavour. [1] proposes Hybrid-IoT, as a step\ntowards decentralizing IoT with the help of blockchain technology. Hybrid-IoT\nconsists of multiple PoW sub-blockchains to achieve distributed consensus among\nIoT devices and an inter-connector framework, to execute transactions between\nsub-blockchains. In this paper, we take the first step towards designing an\ninter-connector for multiple blockchains for IoT that is specifically tailored\nfor the Hybrid-IoT architecture. We also provide a detailed security\ndiscussion, in order to identify threats and we provide discussion on how to\ncope with threats.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 23:56:09 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Sagirlar", "Gokhan", ""], ["Sheehan", "John D.", ""], ["Ragnoli", "Emanuele", ""]]}, {"id": "2005.00794", "submitter": "Diego Pennino", "authors": "Diego Pennino, Maurizio Pizzonia, Andrea Vitaletti, Marco Zecchini", "title": "Binding of Endpoints to Identifiers by On-Chain Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, identity management (IdM) is used to associate a\nsubject public key with an endpoint at which the subject can be contacted\n(telephone number, email, etc.). In decentralized applications based on\ndistributed ledger technologies (DLTes), it is desirable for the IdM to be\ndecentralized as well. Currently, endpoints are either verified by who needs\nit, which is impractical in DLT-based applications, or by a centralized\nauthority, which contrasts with the spirit of DLTes. In this paper, we show two\nDLT-based protocols to prove the association between a subject and an endpoint\nin a decentralized manner, contributing in filling the gap of the current IdM\napproaches with respect to decentralization. Our protocols are compatible with\na wide variety of endpoints. We analyze the security of our protocols and\nevaluate their performance and cost against the common approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:07:42 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Pennino", "Diego", ""], ["Pizzonia", "Maurizio", ""], ["Vitaletti", "Andrea", ""], ["Zecchini", "Marco", ""]]}, {"id": "2005.00872", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "How deep the machine learning can be", "comments": "29 pages, 8 figures; submitted to book 'A closer look at deep\n  learning' by Nova", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today we live in the age of artificial intelligence and machine learning;\nfrom small startups to HW or SW giants, everyone wants to build machine\nintelligence chips, applications. The task, however, is hard: not only because\nof the size of the problem: the technology one can utilize (and the paradigm it\nis based upon) strongly degrades the chances to succeed efficiently. Today the\nsingle-processor performance practically reached the limits the laws of nature\nenable. The only feasible way to achieve the needed high computing performance\nseems to be parallelizing many sequentially working units. The laws of the\n(massively) parallelized computing, however, are different from those\nexperienced in connection with assembling and utilizing systems comprising\njust-a-few single processors. As machine learning is mostly based on the\nconventional computing (processors), we scrutinize the (known, but somewhat\nfaded) laws of the parallel computing, concerning AI. This paper attempts to\nreview some of the caveats, especially concerning scaling the computing\nperformance of the AI solutions.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 16:06:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "2005.00942", "submitter": "Umberto Ferraro Petrillo", "authors": "Umberto Ferraro Petrillo, Francesco Palini, Giuseppe Cattaneo,\n  Raffaele Giancarlo", "title": "Alignment-free Genomic Analysis via a Big Data Spark Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Alignment-free distance and similarity functions (AF functions,\nfor short) are a well established alternative to two and multiple sequence\nalignments for many genomic, metagenomic and epigenomic tasks. Due to\ndata-intensive applications, the computation of AF functions is a Big Data\nproblem, with the recent Literature indicating that the development of fast and\nscalable algorithms computing AF functions is a high-priority task. Somewhat\nsurprisingly, despite the increasing popularity of Big Data technologies in\nComputational Biology, the development of a Big Data platform for those tasks\nhas not been pursued, possibly due to its complexity. Results: We fill this\nimportant gap by introducing FADE, the first extensible, efficient and scalable\nSpark platform for Alignment-free genomic analysis. It supports natively\neighteen of the best performing AF functions coming out of a recent hallmark\nbenchmarking study. FADE development and potential impact comprises novel\naspects of interest. Namely, (a) a considerable effort of distributed\nalgorithms, the most tangible result being a much faster execution time of\nreference methods like MASH and FSWM; (b) a software design that makes FADE\nuser-friendly and easily extendable by Spark non-specialists; (c) its ability\nto support data- and compute-intensive tasks. About this, we provide a novel\nand much needed analysis of how informative and robust AF functions are, in\nterms of the statistical significance of their output. Our findings naturally\nextend the ones of the highly regarded benchmarking study, since the functions\nthat can really be used are reduced to a handful of the eighteen included in\nFADE.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:33:28 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 19:50:22 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 10:10:50 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Petrillo", "Umberto Ferraro", ""], ["Palini", "Francesco", ""], ["Cattaneo", "Giuseppe", ""], ["Giancarlo", "Raffaele", ""]]}, {"id": "2005.01026", "submitter": "Guodong Long Dr", "authors": "Ming Xie, Guodong Long, Tao Shen, Tianyi Zhou, Xianzhi Wang, Jing\n  Jiang", "title": "Multi-Center Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has received great attention for its capability to train a\nlarge-scale model in a decentralized manner without needing to access user data\ndirectly. It helps protect the users' private data from centralized collecting.\nUnlike distributed machine learning, federated learning aims to tackle non-IID\ndata from heterogeneous sources in various real-world applications, such as\nthose on smartphones. Existing federated learning approaches usually adopt a\nsingle global model to capture the shared knowledge of all users by aggregating\ntheir gradients, regardless of the discrepancy between their data\ndistributions. However, due to the diverse nature of user behaviors, assigning\nusers' gradients to different global models (i.e., centers) can better capture\nthe heterogeneity of data distributions across users. Our paper proposes a\nnovel multi-center aggregation mechanism for federated learning, which learns\nmultiple global models from the non-IID user data and simultaneously derives\nthe optimal matching between users and centers. We formulate the problem as a\njoint optimization that can be efficiently solved by a stochastic expectation\nmaximization (EM) algorithm. Our experimental results on benchmark datasets\nshow that our method outperforms several popular federated learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 09:14:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Xie", "Ming", ""], ["Long", "Guodong", ""], ["Shen", "Tao", ""], ["Zhou", "Tianyi", ""], ["Wang", "Xianzhi", ""], ["Jiang", "Jing", ""]]}, {"id": "2005.01038", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Joris Dugu\\'ep\\'eroux, Tristan Allard, Divyakant\n  Agrawal, Amr El Abbadi", "title": "SEPAR: Towards Regulating Future of Work Multi-Platform Crowdworking\n  Environments with Privacy Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdworking platforms provide the opportunity for diverse workers to execute\ntasks for different requesters. The popularity of the \"gig\" economy has given\nrise to independent platforms that provide competing and complementary\nservices. Workers as well as requesters with specific tasks may need to work\nfor or avail from the services of multiple platforms resulting in the rise of\nmulti-platform crowdworking systems. Recently, there has been increasing\ninterest by governmental, legal and social institutions to enforce regulations,\nsuch as minimal and maximal work hours, on crowdworking platforms. Platforms\nwithin multi-platform crowdworking systems, therefore, need to collaborate to\nenforce cross-platform regulations. While collaborating to enforce global\nregulations requires the transparent sharing of information about tasks and\ntheir participants, the privacy of all participants needs to be preserved. In\nthis paper, we propose an overall vision exploring the regulation, privacy, and\narchitecture dimensions for the future of work multi-platform crowdworking\nenvironments. We then present SEPAR, a multi-platform crowdworking system that\nenforces a large sub-space of practical global regulations on a set of\ndistributed independent platforms in a privacy-preserving manner. SEPAR,\nenforces privacy using lightweight and anonymous tokens, while transparency is\nachieved using fault-tolerant blockchains shared across multiple platforms. The\nprivacy guarantees of SEPAR against covert adversaries are formalized and\nthoroughly demonstrated, while the experiments reveal the efficiency of SEPAR\nin terms of performance and scalability.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 10:23:32 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 21:26:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Dugu\u00e9p\u00e9roux", "Joris", ""], ["Allard", "Tristan", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "2005.01108", "submitter": "Ethan Phillip LaRochelle", "authors": "Ethan P. M. LaRochelle, Pedro Arce and Brian W. Pogue", "title": "Monte Carlo modeling photon-tissue interaction using on-demand cloud\n  infrastructure", "comments": "13 pages; 3 figures; Keywords: Monte Carlo, cloud computing,\n  high-performance computing, medical physics, Cherenkov, luminescence, tissue\n  optics", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: This work advances a Monte Carlo (MC) method to combine ionizing\nradiation physics with optical physics, in a manner which was implicitly\ndesigned for deployment with the most widely accessible parallelization and\nportability possible. Methods: The current work updates a previously developed\noptical propagation plugin for GEANT4 architecture for medically oriented\nsimulations (GAMOS). Both virtual-machine (VM) and container based instances\nwere validated using previously published scripts, and improvements in\nexecution time using parallel simulations are demonstrated. A method to\nprogrammatically deploy multiple containers to achieve parallel execution using\nan on-demand cloud-based infrastructure is presented. Results: A\ncontainer-based GAMOS deployment is demonstrated using a multi-layer tissue\nmodel and both optical and X-ray source inputs. As an example, the model was\nsplit into 154 simulations which were run simultaneously on 64 separate\ncontainers across 4 servers. Conclusions: The container-based model provides\nthe ability to execute parallel simulations of applications which are not\ninherently thread-safe or GPU-optimized. In the current demonstration, this\nreduced the time by at most 97% compared to sequential execution. The code and\nexamples are available through an interactive online interface through links\nat: https://sites.dartmouth.edu/optmed/research-projects/monte-carlo-software/\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 15:01:04 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["LaRochelle", "Ethan P. M.", ""], ["Arce", "Pedro", ""], ["Pogue", "Brian W.", ""]]}, {"id": "2005.01445", "submitter": "Siva Kumar Sastry Hari", "authors": "Siva Kumar Sastry Hari, Paolo Rech, Timothy Tsai, Mark Stephenson,\n  Arslan Zulfiqar, Michael Sullivan, Philip Shirvani, Paul Racunas, Joel Emer,\n  Stephen W. Keckler", "title": "Estimating Silent Data Corruption Rates Using a Two-Level Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance and safety-critical system architects must accurately\nevaluate the application-level silent data corruption (SDC) rates of processors\nto soft errors. Such an evaluation requires error propagation all the way from\nparticle strikes on low-level state up to the program output. Existing\napproaches that rely on low-level simulations with fault injection cannot\nevaluate full applications because of their slow speeds, while\napplication-level accelerated fault testing in accelerated particle beams is\noften impractical. We present a new two-level methodology for application\nresilience evaluation that overcomes these challenges. The proposed approach\ndecomposes application failure rate estimation into (1) identifying how\nparticle strikes in low-level unprotected state manifest at the\narchitecture-level, and (2) measuring how such architecture-level\nmanifestations propagate to the program output. We demonstrate the\neffectiveness of this approach on GPU architectures. We also show that using\njust one of the two steps can overestimate SDC rates and produce different\ntrends---the composition of the two is needed for accurate reliability\nmodeling.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 00:09:47 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hari", "Siva Kumar Sastry", ""], ["Rech", "Paolo", ""], ["Tsai", "Timothy", ""], ["Stephenson", "Mark", ""], ["Zulfiqar", "Arslan", ""], ["Sullivan", "Michael", ""], ["Shirvani", "Philip", ""], ["Racunas", "Paul", ""], ["Emer", "Joel", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "2005.01461", "submitter": "Francisco Renato Ara\\'ujo", "authors": "Francisco Renato C. Ara\\'ujo", "title": "Evacua\\c{c}\\~ao de Dados em Nuvens \\'Opticas com base no SLA sob\n  Cen\\'ario de Desastre", "comments": "14 pages, in Portuguese, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularization of cloud computing has provided the emergence of large\nvolumes of data that are stored in Data Centers (DCs). These locations store\ndata of different types, origins, and priorities for their owners. The DCs are\nsubject to natural or man-made attacks. The attacks are diverse and happen\nquickly after their detection. Therefore, this paper proposes two techniques to\nevacuate data from threatened DCs to those who are outside the risk zone of the\nattack. The first technique is based on the Service Level Agreement (SLA) of\nthe data and the second one is based on the order that they arrive at the DC,\nusing the algorithm LIFO. Both techniques performed similarly on the amount of\nevacuated data and at the time for evacuation. However, the SLA policy\ndistributes the data on a priority scale according to the SLA, while the LIFO\npolicy ranks data on the same scale's priority.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 18:16:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ara\u00fajo", "Francisco Renato C.", ""]]}, {"id": "2005.01475", "submitter": "Mahdi Bohlouli", "authors": "Amin Keshavarzi, Abolfazl T. Haghighat, Mahdi Bohlouli", "title": "Research Challenges and Prospective Business Impacts of Cloud Computing:\n  A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's information technology (IT) era, a major part of the costs is\nbeing spent on computational needs. Enterprises are in efforts to increase\ntheir Return on Investment (ROI) and individuals are trying to reduce their\ncosts. In this regard, cloud computing which emerges as a fifth utility can\nreduce costs and enhance performance of IT solutions. A large number of\ncompanies and institutions are dealing with cloud related issues as a provider\nor user. Due to the fact that cloud computing services have been proposed in\nrecent years, organizations and individuals face with various challenges and\nproblems such as how to migrate applications and software platforms into cloud\nand how to ensure security of migrated applications and etc. Given that many\ndifferent definitions of cloud computing is presented in many publications and\nprojects, a concrete and clear definition for cloud computing considering its\ncharacteristics, models and services is provided in this paper. In addition,\ncurrent challenges and open issues in cloud computing is discussed in details\nand further recommendations and roadmaps for scientific activities and\nresearches as well as potentials for improvements in this area from scientific\nand commercial points of view are given in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 03:22:35 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Keshavarzi", "Amin", ""], ["Haghighat", "Abolfazl T.", ""], ["Bohlouli", "Mahdi", ""]]}, {"id": "2005.01574", "submitter": "Hao Zheng", "authors": "Yuting Cao, Parijat Mukherjee, Mahesh Ketkar, Jin Yang, Hao Zheng", "title": "Mining Message Flows using Recurrent Neural Networks for System-on-Chip\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive specifications are essential for various activities across the\nentire validation continuum for system-on-chip (SoC) designs. However,\nspecifications are often ambiguous, incomplete, or even contain inconsistencies\nor errors. This paper addresses this problem by developing a specification\nmining approach that automatically extracts sequential patterns from SoC\ntransaction-level traces such that the mined patterns collectively characterize\nsystem-level specifications for SoC designs. This approach exploits long\nshort-term memory (LSTM) networks trained with the collected SoC execution\ntraces to capture sequential dependencies among various communication events.\nThen, a novel algorithm is developed to efficiently extract sequential patterns\non system-level communications from the trained LSTM models. Several trace\nprocessing techniques are also proposed to enhance the mining performance. We\nevaluate the proposed approach on simulation traces of a non-trivial multi-core\nSoC prototype. Initial results show that the proposed approach is capable of\nextracting various patterns on system-level specifications from the highly\nconcurrent SoC execution traces.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:52:53 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Cao", "Yuting", ""], ["Mukherjee", "Parijat", ""], ["Ketkar", "Mahesh", ""], ["Yang", "Jin", ""], ["Zheng", "Hao", ""]]}, {"id": "2005.01584", "submitter": "Betis Baheri", "authors": "Betis Baheri and Qiang Guan", "title": "MARS: Multi-Scalable Actor-Critic Reinforcement Learning Scheduler", "comments": "12 pages, HPC, Cloud System, Scheduling, Scientific Workflow\n  Management, Multi-Scaleable, Reinforcement Learning, Deep Learning,\n  Actor-Critic, Asynchronous Actor-Critic, Algorithm, Cost-aware,\n  Cost-Efficient", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new scheduling algorithm MARS based on a\ncost-aware multi-scalable reinforcement learning approach, which serves as an\nintermediate layer between HPC resource manager and user application workflow,\nMARS ensembles the pre-generated models from users workflows and decides on the\nmost suitable strategy for optimization. A whole workflow application would be\nsplit into several optimized subtasks. Then based on a pre-defined resource\nmanagement plan. A reward will be generated after executing a scheduled task.\nLastly, MARS updates the Deep Neural Network (DNN) model for future use. MARS\nis designed to be able to optimize the existing models through the\nreinforcement mechanism. MARS can adapt to the shortage of training samples and\noptimize the performance by itself, especially through combining the small\ntasks together or switching between pre-built scheduling strategy such as\nBackfilling, SJF, etc, then choosing the most suitable approach. We tested MARS\nusing different real-world workflow traces. MARS can achieve between 5%-60%\nbetter performance while comparing to the other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:51:41 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Baheri", "Betis", ""], ["Guan", "Qiang", ""]]}, {"id": "2005.01662", "submitter": "Jonathan Schwartz", "authors": "Jonathan Schwartz, Huihuo Zheng, Marcus Hanwell, Yi Jiang, and Robert\n  Hovden", "title": "Dynamic Compressed Sensing for Real-Time Tomographic Reconstruction", "comments": null, "journal-ref": "Microsc Microanal 26 (2020) 2462-2465", "doi": "10.1017/S1431927620021674", "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron tomography has achieved higher resolution and quality at reduced\ndoses with recent advances in compressed sensing. Compressed sensing (CS)\ntheory exploits the inherent sparse signal structure to efficiently reconstruct\nthree-dimensional (3D) volumes at the nanoscale from undersampled measurements.\nHowever, the process bottlenecks 3D reconstruction with computation times that\nrun from hours to days. Here we demonstrate a framework for dynamic compressed\nsensing that produces a 3D specimen structure that updates in real-time as new\nspecimen projections are collected. Researchers can begin interpreting 3D\nspecimens as data is collected to facilitate high-throughput and interactive\nanalysis. Using scanning transmission electron microscopy (STEM), we show that\ndynamic compressed sensing accelerates the convergence speed by 3-fold while\nalso reducing its error by 27% for an Au/SrTiO3 nanoparticle specimen. Before a\ntomography experiment is completed, the 3D tomogram has interpretable structure\nwithin 33% of completion and fine details are visible as early as 66%. Upon\ncompletion of an experiment, a high-fidelity 3D visualization is produced\nwithout further delay. Additionally, reconstruction parameters that tune data\nfidelity can be manipulated throughout the computation without rerunning the\nentire process.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:14:36 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Schwartz", "Jonathan", ""], ["Zheng", "Huihuo", ""], ["Hanwell", "Marcus", ""], ["Jiang", "Yi", ""], ["Hovden", "Robert", ""]]}, {"id": "2005.01865", "submitter": "Iurii Shyshatskyi", "authors": "Iurii Shyshatsky, Vinod Manoharan, Taras Emelyanenko and Lucas Leger", "title": "JaxNet: Scalable Blockchain Network", "comments": "57 pages, 11 figures. Version of Oct 2020", "journal-ref": null, "doi": "10.6084/m9.figshare.13154420", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's world is organized based on merit and value. A single global currency\nthat's decentralized is needed for a global economy. Bitcoin is a partial\nsolution to this need, however it suffers from scalability problems which\nprevent it from being mass-adopted. Also, the deflationary nature of bitcoin\nmotivates people to hoard and speculate on them instead of using them for day\nto day transactions. We propose a scalable, decentralized cryptocurrency that\nis based on Proof of Work. The solution involves having parallel chains in a\nclosed network using a mechanism which rewards miners proportional to their\neffort in maintaining the network. The proposed design introduces a novel\napproach for solving the scalability problem in the blockchain network based on\nmerged mining.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:57:15 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 17:06:28 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Shyshatsky", "Iurii", ""], ["Manoharan", "Vinod", ""], ["Emelyanenko", "Taras", ""], ["Leger", "Lucas", ""]]}, {"id": "2005.01890", "submitter": "Florian Hofer", "authors": "Florian Hofer, Martin Sehr, Alberto Sangiovanni-Vincentelli and\n  Barbara Russo", "title": "Industrial Control via Application Containers:Maintaining determinism in\n  IAAS", "comments": "24 pages, 5 figures, 5 tables, submitted do Journal, under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 is changing fundamentally data collection, its storage and\nanalysis in industrial processes, enabling novel application such as flexible\nmanufacturing of highly customized products. Real-time control of these\nprocesses, however, has not yet realized its full potential in using the\ncollected data to drive further development. Indeed, typical industrial control\nsystems are tailored to the plant they need to control, making reuse and\nadaptation a challenge. In the past, the need to solve plant specific problems\novershadowed the benefits of physically isolating a control system from its\nplant. We believe that modern virtualization techniques, specifically\napplication containers, present a unique opportunity to decouple control from\nplants. This separation permits us to fully realize the potential for highly\ndistributed, and transferable industrial processes even with real-time\nconstraints arising from time-critical sub-processes. In this paper, we explore\nthe challenges and opportunities of shifting industrial control software from\ndedicated hardware to bare-metal servers or (edge) cloud computing platforms\nusing off-the-shelf technology. We present a migration architecture and show,\nusing a specifically developed orchestration tool, that containerized\napplications can run on shared resources without compromising scheduled\nexecution within given time constraints. Through latency and computational\nperformance experiments we explore limits of three system setups and summarize\nlessons learned.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 00:08:04 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Hofer", "Florian", ""], ["Sehr", "Martin", ""], ["Sangiovanni-Vincentelli", "Alberto", ""], ["Russo", "Barbara", ""]]}, {"id": "2005.01916", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao", "title": "Toward Equilibria and Solvability of Blockchain Pooling Strategies: A\n  Topological Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2015, Eyal proposed the first game-theoretical model for analyzing the\nequilibrium of blockchain pooling: when the blockchain pools are abstracted as\na non-cooperative game, two pools can reach a Nash equilibrium with a\nclosed-form formula; Moreover, an arbitrary number of pools still exhibit an\nequilibrium as long as the pools have an equal number of miners. Nevertheless,\nwhether an equilibrium exists for three or more pools of distinct sizes remains\nan open problem. To this end, this paper studies the equilibrium in a\nblockchain of arbitrary pools. First, we show that the equilibrium among $q$\nidentical pools, coinciding the result demonstrated by Eyal through game\ntheory, can be constructed using a topological approach. Second, if the pools\nare of different size, we show that (i) if the blockchain's pools exhibit two\ndistinct sizes, an equilibrium can be reached, and (ii) if the blockchain has\nat least three distinct pool sizes, there does not exist an equilibrium.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 02:25:33 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhao", "Dongfang", ""]]}, {"id": "2005.01945", "submitter": "Md Momin Al Aziz", "authors": "Toufique Morshed, Md Momin Al Aziz and Noman Mohammed", "title": "CPU and GPU Accelerated Fully Homomorphic Encryption", "comments": "Accepted in IEEE HOST'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully Homomorphic Encryption (FHE) is one of the most promising technologies\nfor privacy protection as it allows an arbitrary number of function\ncomputations over encrypted data. However, the computational cost of these FHE\nsystems limits their widespread applications. In this paper, our objective is\nto improve the performance of FHE schemes by designing efficient parallel\nframeworks. In particular, we choose Torus Fully Homomorphic Encryption (TFHE)\nas it offers exact results for an infinite number of boolean gate (e.g., AND,\nXOR) evaluations. We first extend the gate operations to algebraic circuits\nsuch as addition, multiplication, and their vector and matrix equivalents.\nSecondly, we consider the multi-core CPUs to improve the efficiency of both the\ngate and the arithmetic operations. Finally, we port the TFHE to the Graphics\nProcessing Units (GPU) and device novel optimizations for boolean and\narithmetic circuits employing the multitude of cores. We also experimentally\nanalyze both the CPU and GPU parallel frameworks for different numeric\nrepresentations (16 to 32-bit). Our GPU implementation outperforms the existing\ntechnique, and it achieves a speedup of 20x for any 32-bit boolean operation\nand 14.5x for multiplications.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 05:03:50 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Morshed", "Toufique", ""], ["Aziz", "Md Momin Al", ""], ["Mohammed", "Noman", ""]]}, {"id": "2005.02088", "submitter": "Wei Zhang", "authors": "Wei Zhang, Quan Chen, Kaihua Fu, Ningxin Zheng, Zhiyi Huang, Jingwen\n  Leng, Chao Li, Wenli Zheng, Minyi Guo", "title": "Towards QoS-Aware and Resource-Efficient GPU Microservices Based on\n  Spatial Multitasking GPUs In Datacenters", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While prior researches focus on CPU-based microservices, they are not\napplicable for GPU-based microservices due to the different contention\npatterns. It is challenging to optimize the resource utilization while\nguaranteeing the QoS for GPU microservices. We find that the overhead is caused\nby inter microservice communication, GPU resource contention and imbalanced\nthroughput within microservice pipeline. We propose Camelot, a runtime system\nthat manages GPU micorservices considering the above factors. In Camelot, a\nglobal memory-based communication mechanism enables onsite data sharing that\nsignificantly reduces the end-to-end latencies of user queries. We also propose\ntwo contention aware resource allocation policies that either maximize the peak\nsupported service load or minimize the resource usage at low load while\nensuring the required QoS. The two policies consider the microservice pipeline\neffect and the runtime GPU resource contention when allocating resources for\nthe microservices. Compared with state-of-the-art work, Camelot increases the\nsupported peak load by up to 64.5% with limited GPUs, and reduces 35% resource\nusage at low load while achieving the desired 99%-ile latency target.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 12:03:28 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhang", "Wei", ""], ["Chen", "Quan", ""], ["Fu", "Kaihua", ""], ["Zheng", "Ningxin", ""], ["Huang", "Zhiyi", ""], ["Leng", "Jingwen", ""], ["Li", "Chao", ""], ["Zheng", "Wenli", ""], ["Guo", "Minyi", ""]]}, {"id": "2005.02177", "submitter": "Yuanrui Dong", "authors": "Yuanrui Dong, Peng Zhao, Hanqiao Yu, Cong Zhao and Shusen Yang", "title": "CDC: Classification Driven Compression for Bandwidth Efficient\n  Edge-Cloud Collaborative Deep Learning", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 07:40:32 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Dong", "Yuanrui", ""], ["Zhao", "Peng", ""], ["Yu", "Hanqiao", ""], ["Zhao", "Cong", ""], ["Yang", "Shusen", ""]]}, {"id": "2005.02426", "submitter": "Zhishuai Guo", "authors": "Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li Shen, Wei Liu, Tianbao\n  Yang", "title": "Communication-Efficient Distributed Stochastic AUC Maximization with\n  Deep Neural Networks", "comments": null, "journal-ref": "37th International Conference on Machine Learning, 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study distributed algorithms for large-scale AUC\nmaximization with a deep neural network as a predictive model. Although\ndistributed learning techniques have been investigated extensively in deep\nlearning, they are not directly applicable to stochastic AUC maximization with\ndeep neural networks due to its striking differences from standard loss\nminimization problems (e.g., cross-entropy). Towards addressing this challenge,\nwe propose and analyze a communication-efficient distributed optimization\nalgorithm based on a {\\it non-convex concave} reformulation of the AUC\nmaximization, in which the communication of both the primal variable and the\ndual variable between each worker and the parameter server only occurs after\nmultiple steps of gradient-based updates in each worker. Compared with the\nnaive parallel version of an existing algorithm that computes stochastic\ngradients at individual machines and averages them for updating the model\nparameters, our algorithm requires a much less number of communication rounds\nand still achieves a linear speedup in theory. To the best of our knowledge,\nthis is the \\textbf{first} work that solves the {\\it non-convex concave\nmin-max} problem for AUC maximization with deep neural networks in a\ncommunication-efficient distributed manner while still maintaining the linear\nspeedup property in theory. Our experiments on several benchmark datasets show\nthe effectiveness of our algorithm and also confirm our theory.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 18:08:23 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 06:54:36 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Guo", "Zhishuai", ""], ["Liu", "Mingrui", ""], ["Yuan", "Zhuoning", ""], ["Shen", "Li", ""], ["Liu", "Wei", ""], ["Yang", "Tianbao", ""]]}, {"id": "2005.02496", "submitter": "Yu Chen", "authors": "Michael Rosenberg, John Henry Burns, Deeraj Nagothu, Yu Chen", "title": "Enabling Continuous Operations for UAVs with an Autonomous Service\n  Network Infrastructure", "comments": "2020 SPIE Defense + Commercial Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major restrictions on the practical applications of unmanned\naerial vehicles (UAV) is their incomplete self-sufficiency, which makes\ncontinuous operations infeasible without human oversights. The more oversight\nUAVs require, the less likely they are going to be commercially advantageous\nwhen compared to their alternatives. As an autonomous system, how much human\ninteraction is needed to function is one of the best indicators evaluating the\nlimitations and inefficiencies of the UAVs. Popular UAV related research areas,\nsuch as path planning and computer vision, have enabled substantial advances in\nthe ability of drones to act on their own. This research is dedicated to\nin-flight operations, in which there is not much reported effort to tackle the\nproblem from the aspect of the supportive infrastructure. In this paper, an\nAutonomous Service network infrastructure (AutoServe) is proposed. Aiming at\nincreasing the future autonomy of UAVs, the AutoServe system includes a\nservice-oriented landing platform and a customized communication protocol. This\nsupportive AutoServe infrastructure will autonomize many tasks currently done\nmanually by human operators, such as battery replacement. A proof-of-concept\nprototype has been built and the simulation experimental study validated the\ndesign.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 17:03:57 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Rosenberg", "Michael", ""], ["Burns", "John Henry", ""], ["Nagothu", "Deeraj", ""], ["Chen", "Yu", ""]]}, {"id": "2005.02503", "submitter": "Semih Yagli", "authors": "Semih Yagli, Alex Dytso, H. Vincent Poor", "title": "Information-Theoretic Bounds on the Generalization Error and Privacy\n  Leakage in Federated Learning", "comments": "Accepted for publication in Proceedings of 21st IEEE International\n  Workshop on Signal Processing Advances in Wireless Communications (SPAWC),\n  2020. arXiv version is 10pt font, 6 Pages. This is the same document as the\n  SPAWC version, except that the conference version is written with 9pt font to\n  meet the strict page margin requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms operating on mobile networks can be characterized\ninto three different categories. First is the classical situation in which the\nend-user devices send their data to a central server where this data is used to\ntrain a model. Second is the distributed setting in which each device trains\nits own model and send its model parameters to a central server where these\nmodel parameters are aggregated to create one final model. Third is the\nfederated learning setting in which, at any given time $t$, a certain number of\nactive end users train with their own local data along with feedback provided\nby the central server and then send their newly estimated model parameters to\nthe central server. The server, then, aggregates these new parameters, updates\nits own model, and feeds the updated parameters back to all the end users,\ncontinuing this process until it converges.\n  The main objective of this work is to provide an information-theoretic\nframework for all of the aforementioned learning paradigms. Moreover, using the\nprovided framework, we develop upper and lower bounds on the generalization\nerror together with bounds on the privacy leakage in the classical, distributed\nand federated learning settings.\n  Keywords: Federated Learning, Distributed Learning, Machine Learning, Model\nAggregation.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:23:45 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Yagli", "Semih", ""], ["Dytso", "Alex", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2005.02510", "submitter": "Shantanu Sharma", "authors": "Peeyush Gupta, Sharad Mehrotra, Nisha Panwar, Shantanu Sharma, Nalini\n  Venkatasubramanian, Guoxi Wang", "title": "Quest: Practical and Oblivious Mitigation Strategies for COVID-19 using\n  WiFi Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contact tracing has emerged as one of the main mitigation strategies to\nprevent the spread of pandemics such as COVID-19. Recently, several efforts\nhave been initiated to track individuals, their movements, and interactions\nusing technologies, e.g., Bluetooth beacons, cellular data records, and\nsmartphone applications. Such solutions are often intrusive, potentially\nviolating individual privacy rights and are often subject to regulations (e.g.,\nGDPR and CCPR) that mandate the need for opt-in policies to gather and use\npersonal information. In this paper, we introduce Quest, a system that empowers\norganizations to observe individuals and spaces to implement policies for\nsocial distancing and contact tracing using WiFi connectivity data in a passive\nand privacy-preserving manner. The goal is to ensure the safety of employees\nand occupants at an organization, while protecting the privacy of all parties.\nQuest incorporates computationally- and information-theoretically-secure\nprotocols that prevent adversaries from gaining knowledge of an individual's\nlocation history (based on WiFi data); it includes support for accurately\nidentifying users who were in the vicinity of a confirmed patient, and then\ninforming them via opt-in mechanisms. Quest supports a range of privacy-enabled\napplications to ensure adherence to social distancing, monitor the flow of\npeople through spaces, identify potentially impacted regions, and raise\nexposure alerts. We describe the architecture, design choices, and\nimplementation of the proposed security/privacy techniques in Quest. We, also,\nvalidate the practicality of Quest and evaluate it thoroughly via an actual\ncampus-scale deployment at UC Irvine over a very large dataset of over 50M\ntuples.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:39:38 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Gupta", "Peeyush", ""], ["Mehrotra", "Sharad", ""], ["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Venkatasubramanian", "Nalini", ""], ["Wang", "Guoxi", ""]]}, {"id": "2005.02544", "submitter": "Young Geun Kim", "authors": "Young Geun Kim and Carole-Jean Wu", "title": "AutoScale: Optimizing Energy Efficiency of End-to-End Edge Inference\n  under Stochastic Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning inference is increasingly run at the edge. As the programming\nand system stack support becomes mature, it enables acceleration opportunities\nwithin a mobile system, where the system performance envelope is scaled up with\na plethora of programmable co-processors. Thus, intelligent services designed\nfor mobile users can choose between running inference on the CPU or any of the\nco-processors on the mobile system, or exploiting connected systems, such as\nthe cloud or a nearby, locally connected system. By doing so, the services can\nscale out the performance and increase the energy efficiency of edge mobile\nsystems. This gives rise to a new challenge - deciding when inference should\nrun where. Such execution scaling decision becomes more complicated with the\nstochastic nature of mobile-cloud execution, where signal strength variations\nof the wireless networks and resource interference can significantly affect\nreal-time inference performance and system energy efficiency. To enable\naccurate, energy-efficient deep learning inference at the edge, this paper\nproposes AutoScale. AutoScale is an adaptive and light-weight execution scaling\nengine built upon the custom-designed reinforcement learning algorithm. It\ncontinuously learns and selects the most energy-efficient inference execution\ntarget by taking into account characteristics of neural networks and available\nsystems in the collaborative cloud-edge execution environment while adapting to\nthe stochastic runtime variance. Real system implementation and evaluation,\nconsidering realistic execution scenarios, demonstrate an average of 9.8 and\n1.6 times energy efficiency improvement for DNN edge inference over the\nbaseline mobile CPU and cloud offloading, while meeting the real-time\nperformance and accuracy requirement.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 00:30:29 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Kim", "Young Geun", ""], ["Wu", "Carole-Jean", ""]]}, {"id": "2005.02664", "submitter": "Fan He", "authors": "Fan He, Kexin Lv, Jie Yang, Xiaolin Huang", "title": "One-shot Distibuted Algorithm for PCA with RBF Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes a one-shot algorithm for feature-distributed kernel PCA.\nOur algorithm is inspired by the dual relationship between sample-distributed\nand feature-distributed scenario. This interesting relationship makes it\npossible to establish distributed kernel PCA for feature-distributed cases from\nideas in distributed PCA in sample-distributed scenario. In theoretical part,\nwe analyze the approximation error for both linear and RBF kernels. The result\nsuggests that when eigenvalues decay fast, the proposed algorithm gives high\nquality results with low communication cost. This result is also verified by\nnumerical experiments, showing the effectiveness of our algorithm in practice.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 09:07:50 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 02:17:46 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 07:11:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["He", "Fan", ""], ["Lv", "Kexin", ""], ["Yang", "Jie", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2005.03094", "submitter": "Katherine Barabash", "authors": "Anna Levin, Shelly Garion, Elliot K. Kolodner, Dean H. Lorenz,\n  Katherine Barabash, Mike Kugler, Niall McShane", "title": "AIOps for a Cloud Object Storage Service", "comments": "5 pages", "journal-ref": "2019 IEEE International Congress on Big Data (BigDataCongress)", "doi": "10.1109/BigDataCongress.2019.00036", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing reliance on the ubiquitous availability of IT systems and\nservices, these systems become more global, scaled, and complex to operate. To\nmaintain business viability, IT service providers must put in place reliable\nand cost efficient operations support. Artificial Intelligence for IT\nOperations (AIOps) is a promising technology for alleviating operational\ncomplexity of IT systems and services. AIOps platforms utilize big data,\nmachine learning and other advanced analytics technologies to enhance IT\noperations with proactive actionable dynamic insight.\n  In this paper we share our experience applying the AIOps approach to a\nproduction cloud object storage service to get actionable insights into\nsystem's behavior and health. We describe a real-life production cloud scale\nservice and its operational data, present the AIOps platform we have created,\nand show how it has helped us resolving operational pain points.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 19:29:49 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Levin", "Anna", ""], ["Garion", "Shelly", ""], ["Kolodner", "Elliot K.", ""], ["Lorenz", "Dean H.", ""], ["Barabash", "Katherine", ""], ["Kugler", "Mike", ""], ["McShane", "Niall", ""]]}, {"id": "2005.03156", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Andreas Kipf, Darren Engwirda, Navin Vembar, Michael\n  Jones, Lauren Milechin, Vijay Gadepally, Chris Hill, Tim Kraska, William\n  Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew Hubbell,\n  Michael Houle, Andrew Kirby, Anna Klein, Julie Mullen, Andrew Prout, Albert\n  Reuther, Antonio Rosa, Sid Samsi, Charles Yee, Peter Michaleas", "title": "Fast Mapping onto Census Blocks", "comments": "8 pages, 7 figures, 55 references; accepted to IEEE HPEC 2020", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286157", "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pandemic measures such as social distancing and contact tracing can be\nenhanced by rapidly integrating dynamic location data and demographic data.\nProjecting billions of longitude and latitude locations onto hundreds of\nthousands of highly irregular demographic census block polygons is\ncomputationally challenging in both research and deployment contexts. This\npaper describes two approaches labeled \"simple\" and \"fast\". The simple approach\ncan be implemented in any scripting language (Matlab/Octave, Python, Julia, R)\nand is easily integrated and customized to a variety of research goals. This\nsimple approach uses a novel combination of hierarchy, sparse bounding boxes,\npolygon crossing-number, vectorization, and parallel processing to achieve\n100,000,000+ projections per second on 100 servers. The simple approach is\ncompact, does not increase data storage requirements, and is applicable to any\ncountry or region. The fast approach exploits the thread, vector, and memory\noptimizations that are possible using a low-level language (C++) and achieves\nsimilar performance on a single server. This paper details these approaches\nwith the goal of enabling the broader community to quickly integrate location\nand demographic data.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 22:07:05 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 17:19:44 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kepner", "Jeremy", ""], ["Kipf", "Andreas", ""], ["Engwirda", "Darren", ""], ["Vembar", "Navin", ""], ["Jones", "Michael", ""], ["Milechin", "Lauren", ""], ["Gadepally", "Vijay", ""], ["Hill", "Chris", ""], ["Kraska", "Tim", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Kirby", "Andrew", ""], ["Klein", "Anna", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Reuther", "Albert", ""], ["Rosa", "Antonio", ""], ["Samsi", "Sid", ""], ["Yee", "Charles", ""], ["Michaleas", "Peter", ""]]}, {"id": "2005.03292", "submitter": "Peter Hillmann", "authors": "Mario Golling, Robert Koch, Peter Hillmann, Rick Hofstede, Frank\n  Tietze", "title": "YANG2UML: Bijective Transformation and Simplification of YANG to UML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.DC cs.NI cs.PL cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Networking is currently revolutionizing computer networking\nby decoupling the network control (control plane) from the forwarding functions\n(data plane) enabling the network control to become directly programmable and\nthe underlying infrastructure to be abstracted for applications and network\nservices. Next to the well-known OpenFlow protocol, the XML-based NETCONF\nprotocol is also an important means for exchanging configuration information\nfrom a management platform and is nowadays even part of OpenFlow. In\ncombination with NETCONF, YANG is the corresponding protocol that defines the\nassociated data structures supporting virtually all network configuration\nprotocols. YANG itself is a semantically rich language, which -- in order to\nfacilitate familiarization with the relevant subject -- is often visualized to\ninvolve other experts or developers and to support them by their daily work\n(writing applications which make use of YANG). In order to support this\nprocess, this paper presents an novel approach to optimize and simplify YANG\ndata models to assist further discussions with the management and\nimplementations (especially of interfaces) to reduce complexity. Therefore, we\nhave defined a bidirectional mapping of YANG to UML and developed a tool that\nrenders the created UML diagrams. This combines the benefits to use the formal\nlanguage YANG with automatically maintained UML diagrams to involve other\nexperts or developers, closing the gap between technically improved data models\nand their human readability.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:29:49 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Golling", "Mario", ""], ["Koch", "Robert", ""], ["Hillmann", "Peter", ""], ["Hofstede", "Rick", ""], ["Tietze", "Frank", ""]]}, {"id": "2005.03300", "submitter": "Aydin Buluc", "authors": "Alok Tripathy, Katherine Yelick, Aydin Buluc", "title": "Reducing Communication in Graph Neural Network Training", "comments": "To appear in International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are powerful and flexible neural networks that\nuse the naturally sparse connectivity information of the data. GNNs represent\nthis connectivity as sparse matrices, which have lower arithmetic intensity and\nthus higher communication costs compared to dense matrices, making GNNs harder\nto scale to high concurrencies than convolutional or fully-connected neural\nnetworks.\n  We introduce a family of parallel algorithms for training GNNs and show that\nthey can asymptotically reduce communication compared to previous parallel GNN\ntraining methods. We implement these algorithms, which are based on 1D, 1.5D,\n2D, and 3D sparse-dense matrix multiplication, using torch.distributed on\nGPU-equipped clusters. Our algorithms optimize communication across the full\nGNN training pipeline. We train GNNs on over a hundred GPUs on multiple\ndatasets, including a protein network with over a billion edges.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:45:09 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 06:33:01 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 20:35:32 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Tripathy", "Alok", ""], ["Yelick", "Katherine", ""], ["Buluc", "Aydin", ""]]}, {"id": "2005.03314", "submitter": "Fei Song", "authors": "Fei Song, Khaled Zaouk, Chenghao Lyu, Arnab Sinha, Qi Fan, Yanlei\n  Diao, Prashant Shenoy", "title": "Boosting Cloud Data Analytics using Multi-Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics in the cloud has become an integral part of enterprise\nbusinesses. Big data analytics systems, however, still lack the ability to take\nuser performance goals and budgetary constraints for a task, collectively\nreferred to as task objectives, and automatically configure an analytic job to\nachieve these objectives. This paper presents a data analytics optimizer that\ncan automatically determine a cluster configuration with a suitable number of\ncores as well as other system parameters that best meet the task objectives. At\na core of our work is a principled multi-objective optimization (MOO) approach\nthat computes a Pareto optimal set of job configurations to reveal tradeoffs\nbetween different user objectives, recommends a new job configuration that best\nexplores such tradeoffs, and employs novel optimizations to enable such\nrecommendations within a few seconds. We present efficient incremental\nalgorithms based on the notion of a Progressive Frontier for realizing our MOO\napproach and implement them into a Spark-based prototype. Detailed experiments\nusing benchmark workloads show that our MOO techniques provide a 2-50x speedup\nover existing MOO methods, while offering good coverage of the Pareto frontier.\nWhen compared to Ottertune, a state-of-the-art performance tuning system, our\napproach recommends configurations that yield 26\\%-49\\% reduction of running\ntime of the TPCx-BB benchmark while adapting to different application\npreferences on multiple objectives.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 08:21:31 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Song", "Fei", ""], ["Zaouk", "Khaled", ""], ["Lyu", "Chenghao", ""], ["Sinha", "Arnab", ""], ["Fan", "Qi", ""], ["Diao", "Yanlei", ""], ["Shenoy", "Prashant", ""]]}, {"id": "2005.03555", "submitter": "Philipp J. Meyer", "authors": "Michael Blondin, Javier Esparza, Martin Helfrich, Anton\\'in\n  Ku\\v{c}era, Philipp J. Meyer", "title": "Checking Qualitative Liveness Properties of Replicated Systems with\n  Stochastic Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sound and complete method for the verification of qualitative\nliveness properties of replicated systems under stochastic scheduling. These\nare systems consisting of a finite-state program, executed by an unknown number\nof indistinguishable agents, where the next agent to make a move is determined\nby the result of a random experiment. We show that if a property of such a\nsystem holds, then there is always a witness in the shape of a Presburger stage\ngraph: a finite graph whose nodes are Presburger-definable sets of\nconfigurations. Due to the high complexity of the verification problem\n(non-elementary), we introduce an incomplete procedure for the construction of\nPresburger stage graphs, and implement it on top of an SMT solver. The\nprocedure makes extensive use of the theory of well-quasi-orders, and of the\nstructural theory of Petri nets and vector addition systems. We apply our\nresults to a set of benchmarks, in particular to a large collection of\npopulation protocols, a model of distributed computation extensively studied by\nthe distributed computing community.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:41:06 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 14:47:24 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Blondin", "Michael", ""], ["Esparza", "Javier", ""], ["Helfrich", "Martin", ""], ["Ku\u010dera", "Anton\u00edn", ""], ["Meyer", "Philipp J.", ""]]}, {"id": "2005.04091", "submitter": "Fatima Zohra Benhamida", "authors": "Riyadh Baghdadi, Abdelkader Nadir Debbagh, Kamel Abdous, Fatima Zohra\n  Benhamida, Alex Renda, Jonathan Elliott Frankle, Michael Carbin and Saman\n  Amarasinghe", "title": "TIRAMISU: A Polyhedral Compiler for Dense and Sparse Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate a compiler that can optimize sparse and\nrecurrent neural networks, both of which are currently outside of the scope of\nexisting neural network compilers (sparse neural networks here stand for\nnetworks that can be accelerated with sparse tensor algebra techniques). Our\ndemonstration includes a mapping of sparse and recurrent neural networks to the\npolyhedral model along with an implementation of our approach in TIRAMISU, our\nstate-of-the-art polyhedral compiler. We evaluate our approach on a set of deep\nlearning benchmarks and compare our results with hand-optimized industrial\nlibraries. Our results show that our approach at least matches Intel MKL-DNN\nand in some cases outperforms it by 5x (on multicore-CPUs).\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:27:08 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Baghdadi", "Riyadh", ""], ["Debbagh", "Abdelkader Nadir", ""], ["Abdous", "Kamel", ""], ["Benhamida", "Fatima Zohra", ""], ["Renda", "Alex", ""], ["Frankle", "Jonathan Elliott", ""], ["Carbin", "Michael", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "2005.04092", "submitter": "Stefano Cereda", "authors": "Stefano Cereda, Gianluca Palermo, Paolo Cremonesi and Stefano Doni", "title": "A Collaborative Filtering Approach for the Automatic Tuning of Compiler\n  Optimisations", "comments": "To be published in the 21st ACM SIGPLAN/SIGBED International\n  Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES\n  2020) PolyBench dataset available at:\n  https://github.com/stefanocereda/polybench_data cBench dataset available at:\n  https://github.com/amirjamez/COBAYN", "journal-ref": null, "doi": "10.1145/3372799.3394361", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the right compiler optimisations has a severe impact on programs'\nperformance. Still, the available optimisations keep increasing, and their\neffect depends on the specific program, making the task human intractable.\nResearchers proposed several techniques to search in the space of compiler\noptimisations. Some approaches focus on finding better search algorithms, while\nothers try to speed up the search by leveraging previously collected knowledge.\nThe possibility to effectively reuse previous compilation results inspired us\ntoward the investigation of techniques derived from the Recommender Systems\nfield. The proposed approach exploits previously collected knowledge and\nimproves its characterisation over time. Differently from current\nstate-of-the-art solutions, our approach is not based on performance counters\nbut relies on Reaction Matching, an algorithm able to characterise programs\nlooking at how they react to different optimisation sets. The proposed approach\nhas been validated using two widely used benchmark suites, cBench and\nPolyBench, including 54 different programs. Our solution, on average, extracted\n90% of the available performance improvement 10 iterations before current\nstate-of-the-art solutions, which corresponds to 40% fewer compilations and\nperformance tests to perform.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:07:48 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 07:53:01 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Cereda", "Stefano", ""], ["Palermo", "Gianluca", ""], ["Cremonesi", "Paolo", ""], ["Doni", "Stefano", ""]]}, {"id": "2005.04093", "submitter": "Aleks Ontman", "authors": "Joshua Porter, Aleks Ontman", "title": "Importing Relationships into a Running Graph Database Using Parallel\n  Processing", "comments": "5 pages, code provided on GitHub\n  https://github.com/Lnofeisone/graph-iterateRelationship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importing relationships into a running graph database using multiple threads\nrunning concurrently is a difficult task, as multiple threads cannot write\ninformation to the same node at the same time. Here we present an algorithm in\nwhich relationships are sorted into bins, then imported such that no two\nthreads ever access the same node concurrently. When this algorithm was\nimplemented as a procedure to run on the Neo4j graph database, it reduced the\ntime to import relationships by up to 69% when 32 threads were used.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:31:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Porter", "Joshua", ""], ["Ontman", "Aleks", ""]]}, {"id": "2005.04094", "submitter": "Zheng Wang", "authors": "Jianbin Fang, Chun Huang, Tao Tang, Zheng Wang", "title": "Parallel Programming Models for Heterogeneous Many-Cores : A Survey", "comments": "Accepted to be published at CCF Transactions on High Performance\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heterogeneous many-cores are now an integral part of modern computing systems\nranging from embedding systems to supercomputers. While heterogeneous many-core\ndesign offers the potential for energy-efficient high-performance, such\npotential can only be unlocked if the application programs are suitably\nparallel and can be made to match the underlying heterogeneous platform. In\nthis article, we provide a comprehensive survey for parallel programming models\nfor heterogeneous many-core architectures and review the compiling techniques\nof improving programmability and portability. We examine various software\noptimization techniques for minimizing the communicating overhead between\nheterogeneous computing devices. We provide a road map for a wide variety of\ndifferent research areas. We conclude with a discussion on open issues in the\narea and potential research directions. This article provides both an\naccessible introduction to the fast-moving area of heterogeneous programming\nand a detailed bibliography of its main achievements.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 13:39:05 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Fang", "Jianbin", ""], ["Huang", "Chun", ""], ["Tang", "Tao", ""], ["Wang", "Zheng", ""]]}, {"id": "2005.04095", "submitter": "Thanh Pham Dinh", "authors": "Pham Dinh Thanh, Huynh Thi Thanh Binh, Do Dinh Dac, Nguyen Binh Long,\n  Le Minh Hai Phong", "title": "A Heuristic Based on Randomized Greedy Algorithms for the Clustered\n  Shortest-Path Tree Problem", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2019.8790070", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomized Greedy Algorithms (RGAs) are interesting approaches to solve\nproblems whose structures are not well understood as well as problems in\ncombinatorial optimization which incorporate the random processes and the\ngreedy algorithms. This paper introduces a new algorithm that combines the\nmajor features of RGAs and Shortest Path Tree Algorithm (SPTA) to deal with the\nClustered Shortest-Path Tree Problem (CluSPT). In our algorithm, SPTA is used\nto determine the shortest path tree in each cluster while the combination\nbetween characteristics of the RGAs and search strategy of SPTA is used to\nconstructed the edges connecting clusters. To evaluate the performance of the\nproposed algorithm, Euclidean benchmarks are selected. The experimental\ninvestigations show the strengths of the proposed algorithm in comparison with\nsome existing algorithms. We also analyze the influence of the parameters on\nthe performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 08:34:58 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Thanh", "Pham Dinh", ""], ["Binh", "Huynh Thi Thanh", ""], ["Dac", "Do Dinh", ""], ["Long", "Nguyen Binh", ""], ["Phong", "Le Minh Hai", ""]]}, {"id": "2005.04096", "submitter": "In\\^es Pinto Gouveia", "authors": "In\\^es Pinto Gouveia, Marcus V\\\"olp and Paulo Esteves-Verissimo\n  (University of Luxembourg, Interdisciplinary Center for Security, Reliability\n  and Trust (SnT) - CritiX group)", "title": "Behind the Last Line of Defense -- Surviving SoC Faults and Intrusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, leveraging the enormous modular power, diversity and flexibility of\nmanycore systems-on-a-chip (SoCs) requires careful orchestration of complex\nresources, a task left to low-level software, e.g. hypervisors. In current\narchitectures, this software forms a single point of failure and worthwhile\ntarget for attacks: once compromised, adversaries gain access to all\ninformation and full control over the platform and the environment it controls.\nThis paper proposes Midir, an enhanced manycore architecture, effecting a\nparadigm shift from SoCs to distributed SoCs. Midir changes the way platform\nresources are controlled, by retrofitting tile-based fault containment through\nwell known mechanisms, while securing low-overhead quorum-based consensus on\nall critical operations, in particular privilege management and, thus,\nmanagement of containment domains. Allowing versatile redundancy management,\nMidir promotes resilience for all software levels, including at low level. We\nexplain this architecture, its associated algorithms and hardware mechanisms\nand show, for the example of a Byzantine fault tolerant microhypervisor, that\nit outperforms the highly efficient MinBFT by one order of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 19:49:17 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Gouveia", "In\u00eas Pinto", "", "University of Luxembourg, Interdisciplinary Center for Security, Reliability\n  and Trust"], ["V\u00f6lp", "Marcus", "", "University of Luxembourg, Interdisciplinary Center for Security, Reliability\n  and Trust"], ["Esteves-Verissimo", "Paulo", "", "University of Luxembourg, Interdisciplinary Center for Security, Reliability\n  and Trust"]]}, {"id": "2005.04097", "submitter": "Qiang Fan", "authors": "Qiang Fan, Jianan Bai, Hongxia Zhang, Yang Yi, Lingjia Liu", "title": "Delay-aware Resource Allocation in Fog-assisted IoT Networks Through\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog nodes in the vicinity of IoT devices are promising to provision low\nlatency services by offloading tasks from IoT devices to them. Mobile IoT is\ncomposed by mobile IoT devices such as vehicles, wearable devices and\nsmartphones. Owing to the time-varying channel conditions, traffic loads and\ncomputing loads, it is challenging to improve the quality of service (QoS) of\nmobile IoT devices. As task delay consists of both the transmission delay and\ncomputing delay, we investigate the resource allocation (i.e., including both\nradio resource and computation resource) in both the wireless channel and fog\nnode to minimize the delay of all tasks while their QoS constraints are\nsatisfied. We formulate the resource allocation problem into an integer\nnon-linear problem, where both the radio resource and computation resource are\ntaken into account. As IoT tasks are dynamic, the resource allocation for\ndifferent tasks are coupled with each other and the future information is\nimpractical to be obtained. Therefore, we design an on-line reinforcement\nlearning algorithm to make the sub-optimal decision in real time based on the\nsystem's experience replay data. The performance of the designed algorithm has\nbeen demonstrated by extensive simulation results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 05:07:39 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 19:45:24 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Fan", "Qiang", ""], ["Bai", "Jianan", ""], ["Zhang", "Hongxia", ""], ["Yi", "Yang", ""], ["Liu", "Lingjia", ""]]}, {"id": "2005.04098", "submitter": "Ahsan Javed Awan Dr", "authors": "Stefano Corda, Bram Veenboer, Ahsan Javed Awan, Akash Kumar, Roel\n  Jordans, and Henk Corporaal", "title": "Near Memory Acceleration on High Resolution Radio Astronomy Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern radio telescopes like the Square Kilometer Array (SKA) will need to\nprocess in real-time exabytes of radio-astronomical signals to construct a\nhigh-resolution map of the sky. Near-Memory Computing (NMC) could alleviate the\nperformance bottlenecks due to frequent memory accesses in a state-of-the-art\nradio-astronomy imaging algorithm. In this paper, we show that a sub-module\nperforming a two-dimensional fast Fourier transform (2D FFT) is memory bound\nusing CPI breakdown analysis on IBM Power9. Then, we present an NMC approach on\nFPGA for 2D FFT that outperforms a CPU by up to a factor of 120x and performs\ncomparably to a high-end GPU, while using less bandwidth and memory.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:33:36 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Corda", "Stefano", ""], ["Veenboer", "Bram", ""], ["Awan", "Ahsan Javed", ""], ["Kumar", "Akash", ""], ["Jordans", "Roel", ""], ["Corporaal", "Henk", ""]]}, {"id": "2005.04099", "submitter": "Roberto Pacheco", "authors": "Roberto G. Pacheco and Rodrigo S. Couto", "title": "Inference Time Optimization Using BranchyNet Partitioning", "comments": "8 pages, 11 figures, IEEE Symposium on Computers and Communications\n  2020", "journal-ref": null, "doi": "10.1109/ISCC50000.2020.9219647", "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) applications with edge computing presents a\ntrade-off between responsiveness and computational resources. On one hand, edge\ncomputing can provide high responsiveness deploying computational resources\nclose to end devices, which may be prohibitive for the majority of cloud\ncomputing services. On the other hand, DNN inference requires computational\npower to be executed, which may not be available on edge devices, but a cloud\nserver can provide it. To solve this problem (trade-off), we partition a DNN\nbetween edge device and cloud server, which means the first DNN layers are\nprocessed at the edge and the other layers at the cloud. This paper proposes an\noptimal partition of DNN, according to network bandwidth, computational\nresources of edge and cloud, and parameter inherent to data. Our proposal aims\nto minimize the inference time, to allow high responsiveness applications. To\nthis end, we show the equivalency between DNN partitioning problem and shortest\npath problem to find an optimal solution, using Dijkstra's algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:40:56 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 13:14:15 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Pacheco", "Roberto G.", ""], ["Couto", "Rodrigo S.", ""]]}, {"id": "2005.04174", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Evaluation of Automatic GPU and FPGA Offloading for Function Blocks of\n  Applications", "comments": "8 pages, 5 figures, in Japanese, IEICE Technical Report, SC2019-44", "journal-ref": null, "doi": null, "report-no": "IEICE Technical Report, SC2019-44, Mar. 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, systems using FPGAs, GPUs have increased due to their\nadvantages such as power efficiency compared to CPUs. However, use in systems\nsuch as FPGAs and GPUs requires understanding hardware-specific technical\nspecifications such as HDL and CUDA, which is a high hurdle. Based on this\nbackground, I previously proposed environment adaptive software that enables\nautomatic conversion, configuration, and high-performance operation of once\nwritten code according to the hardware to be placed. As an element of the\nconcept, I proposed a method to automatically offload loop statements of\napplication source code for CPU to FPGA and GPU. In this paper, I propose and\nevaluate a method for offloading a function block, which is a larger unit,\ninstead of individual loop statements in an application, to achieve higher\nspeed by automatic offloading to GPU and FPGA. I implement the proposed method\nand evaluate with existing applications offloading to GPU.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 20:49:39 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2005.04198", "submitter": "Gal Oren", "authors": "Gal Oren, Leonid Barenboim", "title": "Distributed K-Backup Placement and Applications to Virtual Memory in\n  Real-World Wireless Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Backup Placement problem in networks in the $\\mathcal{CONGEST}$\ndistributed setting considers a network graph $G = (V,E)$, in which the goal of\neach vertex $v \\in V$ is selecting a neighbor, such that the maximum number of\nvertices in $V$ that select the same vertex is minimized [Halldorsson et al.,\n2015]. Previous backup placement algorithms suffer from obliviousness to main\nfactors of real-world heterogeneous wireless network. Specifically, there is no\nconsideration of the nodes memory and storage capacities, and no reference to a\ncase in which nodes have different energy capacity, and thus can leave (or\njoin) the network at any time. These parameters are strongly correlated in\nwireless networks, as the load on different parts of the network can differ\ngreatly, thus requiring more communication, energy, memory and storage. In\norder to fit the real-world attributes of wireless networks, this work\naddresses a generalized version of the original problem, namely $K$-Backup\nPlacement, in which each vertex selects $K$ neighbors, for a positive parameter\n$K$. Our $K$-Backup Placement algorithm terminates within just one round. In\naddition we suggest two complementary algorithms which employ\n$K$-Backup-Placement to obtain efficient virtual memory schemes for wireless\nnetworks. The first algorithm divides the memory of each node to many small\nparts. Each vertex is assigned the memories of a large subset of its neighbors.\nThus more memory capacity for more vertices is gained, but with much\nfragmentation. The second algorithm requires greater round-complexity, but\nproduces larger virtual memory for each vertex without any fragmentation.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 17:39:11 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 21:12:10 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Oren", "Gal", ""], ["Barenboim", "Leonid", ""]]}, {"id": "2005.04215", "submitter": "Ryan Chard", "authors": "Ryan Chard, Yadu Babuji, Zhuozhao Li, Tyler Skluzacek, Anna Woodard,\n  Ben Blaiszik, Ian Foster, Kyle Chard", "title": "funcX: A Federated Function Serving Fabric for Science", "comments": "Accepted to ACM Symposium on High-Performance Parallel and\n  Distributed Computing (HPDC 2020). arXiv admin note: substantial text overlap\n  with arXiv:1908.04907", "journal-ref": null, "doi": "10.1145/3369583.3392683", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploding data volumes and velocities, new computational methods and\nplatforms, and ubiquitous connectivity demand new approaches to computation in\nthe sciences. These new approaches must enable computation to be mobile, so\nthat, for example, it can occur near data, be triggered by events (e.g.,\narrival of new data), be offloaded to specialized accelerators, or run remotely\nwhere resources are available. They also require new design approaches in which\nmonolithic applications can be decomposed into smaller components, that may in\nturn be executed separately and on the most suitable resources. To address\nthese needs we present funcX---a distributed function as a service (FaaS)\nplatform that enables flexible, scalable, and high performance remote function\nexecution. funcX's endpoint software can transform existing clouds, clusters,\nand supercomputers into function serving systems, while funcX's cloud-hosted\nservice provides transparent, secure, and reliable function execution across a\nfederated ecosystem of endpoints. We motivate the need for funcX with several\nscientific case studies, present our prototype design and implementation, show\noptimizations that deliver throughput in excess of 1 million functions per\nsecond, and demonstrate, via experiments on two supercomputers, that funcX can\nscale to more than more than 130000 concurrent workers.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 20:31:11 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Chard", "Ryan", ""], ["Babuji", "Yadu", ""], ["Li", "Zhuozhao", ""], ["Skluzacek", "Tyler", ""], ["Woodard", "Anna", ""], ["Blaiszik", "Ben", ""], ["Foster", "Ian", ""], ["Chard", "Kyle", ""]]}, {"id": "2005.04309", "submitter": "Yongge Wang", "authors": "Yongge Wang", "title": "Blockchain BFT Protocol for Complete Asynchronous Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethereum Research team has proposed a family of Casper blockchain consensus\nprotocols for Ethereum 2.0. It has been shown in the literature that Casper\nFriendly Finality Gadget (Casper FFG) for Ethereum 2.0's beacon network cannot\nachieve liveness property in partially synchronous networks such as the\nInternet environment. The \"Correct-by-Construction\" family of Casper blockchain\nconsensus protocols (CBC Casper) has been proposed as a finality gadget for the\nfuture release of Ethereum 2.0 blockchain. Unfortunately, neither constructive\nfinality rule nor satisfactory liveness property has been obtained for CBC\nCasper, and it is commonly believed that CBC Casper could not achieve liveness\nproperty in asynchronous networks. This paper provides the first probabilistic\nCBC Casper protocol that achieves liveness property against (n-1)/3 Byzantine\nparticipants in complete asynchronous networks.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 22:36:59 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 22:23:48 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Wang", "Yongge", ""]]}, {"id": "2005.04347", "submitter": "Aavaas Gajurel", "authors": "Aavaas Gajurel, Sushil J. Louis, Frederick C Harris", "title": "GPU Acceleration of Sparse Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use graphics processing units(GPU) to accelerate sparse and\narbitrary structured neural networks. Sparse networks have nodes in the network\nthat are not fully connected with nodes in preceding and following layers, and\narbitrary structure neural networks have different number of nodes in each\nlayers. Sparse Neural networks with arbitrary structures are generally created\nin the processes like neural network pruning and evolutionary machine learning\nstrategies. We show that we can gain significant speedup for full activation of\nsuch neural networks using graphical processing units. We do a prepossessing\nstep to determine dependency groups for all the nodes in a network, and use\nthat information to guide the progression of activation in the neural network.\nThen we compute activation for each nodes in its own separate thread in the\nGPU, which allows for massive parallelization. We use CUDA framework to\nimplement our approach and compare the results of sequential and GPU\nimplementations. Our results show that the activation of sparse neural networks\nlends very well to GPU acceleration and can help speed up machine learning\nstrategies which generate such networks or other processes that have similar\nstructure.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 02:18:31 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gajurel", "Aavaas", ""], ["Louis", "Sushil J.", ""], ["Harris", "Frederick C", ""]]}, {"id": "2005.04355", "submitter": "Xiaotian Hao", "authors": "Xiaotian Hao, Junqi Jin, Jianye Hao, Jin Li, Weixun Wang, Yi Ma,\n  Zhenzhe Zheng, Han Li, Jian Xu and Kun Gai", "title": "Learning to Accelerate Heuristic Searching for Large-Scale Maximum\n  Weighted b-Matching Problems in Online Advertising", "comments": "accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite b-matching is fundamental in algorithm design, and has been widely\napplied into economic markets, labor markets, etc. These practical problems\nusually exhibit two distinct features: large-scale and dynamic, which requires\nthe matching algorithm to be repeatedly executed at regular intervals. However,\nexisting exact and approximate algorithms usually fail in such settings due to\neither requiring intolerable running time or too much computation resource. To\naddress this issue, we propose \\texttt{NeuSearcher} which leverages the\nknowledge learned from previously instances to solve new problem instances.\nSpecifically, we design a multichannel graph neural network to predict the\nthreshold of the matched edges weights, by which the search region could be\nsignificantly reduced. We further propose a parallel heuristic search algorithm\nto iteratively improve the solution quality until convergence. Experiments on\nboth open and industrial datasets demonstrate that \\texttt{NeuSearcher} can\nspeed up 2 to 3 times while achieving exactly the same matching solution\ncompared with the state-of-the-art approximation approaches.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 02:48:23 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 07:37:56 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Hao", "Xiaotian", ""], ["Jin", "Junqi", ""], ["Hao", "Jianye", ""], ["Li", "Jin", ""], ["Wang", "Weixun", ""], ["Ma", "Yi", ""], ["Zheng", "Zhenzhe", ""], ["Li", "Han", ""], ["Xu", "Jian", ""], ["Gai", "Kun", ""]]}, {"id": "2005.04519", "submitter": "J\\'er\\'emie Decouchant", "authors": "Paulo Esteves-Verissimo, J\\'er\\'emie Decouchant, Marcus V\\\"olp,\n  Alireza Esfahani, Rafal Graczyk", "title": "PriLok: Citizen-protecting distributed epidemic tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Contact tracing is an important instrument for national health services to\nfight epidemics. As part of the COVID-19 situation, many proposals have been\nmade for scaling up contract tracing capacities with the help of smartphone\napplications, an important but highly critical endeavor due to the privacy\nrisks involved in such solutions. Extending our previously expressed concern,\nwe clearly articulate in this article, the functional and non-functional\nrequirements that any solution has to meet, when striving to serve, not mere\ncollections of individuals, but the whole of a nation, as required in face of\nsuch potentially dangerous epidemics. We present a critical information\ninfrastructure, PriLock, a fully-open preliminary architecture proposal and\ndesign draft for privacy preserving contact tracing, which we believe can be\nconstructed in a way to fulfill the former requirements. Our architecture\nleverages the existing regulated mobile communication infrastructure and builds\nupon the concept of \"checks and balances\", requiring a majority of independent\nplayers to agree to effect any operation on it, thus preventing abuse of the\nhighly sensitive information that must be collected and processed for efficient\ncontact tracing. This is enforced with a largely decentralised layout and\nhighly resilient state-of-the-art technology, which we explain in the paper,\nfinishing by giving a security, dependability and resilience analysis, showing\nhow it meets the defined requirements, even while the infrastructure is under\nattack.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 22:01:47 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 21:30:49 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Esteves-Verissimo", "Paulo", ""], ["Decouchant", "J\u00e9r\u00e9mie", ""], ["V\u00f6lp", "Marcus", ""], ["Esfahani", "Alireza", ""], ["Graczyk", "Rafal", ""]]}, {"id": "2005.04536", "submitter": "Alexis Asseman", "authors": "Alexis Asseman, Nicolas Antoine and Ahmet S. Ozcan", "title": "Accelerating Deep Neuroevolution on Distributed FPGAs for Reinforcement\n  Learning Problems", "comments": "12 pages. Submitted to ACM Journal on Emerging Technologies in\n  Computing Systems: Special Issue on Hardware and Algorithms for Efficient\n  Machine Learning", "journal-ref": null, "doi": "10.1145/3425500", "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning augmented by the representational power of deep neural\nnetworks, has shown promising results on high-dimensional problems, such as\ngame playing and robotic control. However, the sequential nature of these\nproblems poses a fundamental challenge for computational efficiency. Recently,\nalternative approaches such as evolutionary strategies and deep neuroevolution\ndemonstrated competitive results with faster training time on distributed CPU\ncores. Here, we report record training times (running at about 1 million frames\nper second) for Atari 2600 games using deep neuroevolution implemented on\ndistributed FPGAs. Combined hardware implementation of the game console, image\npre-processing and the neural network in an optimized pipeline, multiplied with\nthe system level parallelism enabled the acceleration. These results are the\nfirst application demonstration on the IBM Neural Computer, which is a custom\ndesigned system that consists of 432 Xilinx FPGAs interconnected in a 3D mesh\nnetwork topology. In addition to high performance, experiments also showed\nimprovement in accuracy for all games compared to the CPU-implementation of the\nsame algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 00:41:39 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Asseman", "Alexis", ""], ["Antoine", "Nicolas", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "2005.04680", "submitter": "Alexander Heinecke", "authors": "Dhiraj Kalamkar, Evangelos Georganas, Sudarshan Srinivasan, Jianping\n  Chen, Mikhail Shiryaev, Alexander Heinecke", "title": "Optimizing Deep Learning Recommender Systems' Training On CPU Cluster\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last two years, the goal of many researchers has been to squeeze\nthe last bit of performance out of HPC system for AI tasks. Often this\ndiscussion is held in the context of how fast ResNet50 can be trained.\nUnfortunately, ResNet50 is no longer a representative workload in 2020. Thus,\nwe focus on Recommender Systems which account for most of the AI cycles in\ncloud computing centers. More specifically, we focus on Facebook's DLRM\nbenchmark. By enabling it to run on latest CPU hardware and software tailored\nfor HPC, we are able to achieve more than two-orders of magnitude improvement\nin performance (110x) on a single socket compared to the reference CPU\nimplementation, and high scaling efficiency up to 64 sockets, while fitting\nultra-large datasets. This paper discusses the optimization techniques for the\nvarious operators in DLRM and which component of the systems are stressed by\nthese different operators. The presented techniques are applicable to a broader\nset of DL workloads that pose the same scaling challenges/characteristics as\nDLRM.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 14:40:16 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kalamkar", "Dhiraj", ""], ["Georganas", "Evangelos", ""], ["Srinivasan", "Sudarshan", ""], ["Chen", "Jianping", ""], ["Shiryaev", "Mikhail", ""], ["Heinecke", "Alexander", ""]]}, {"id": "2005.04883", "submitter": "Yael Doweck", "authors": "Yael Doweck, Ittay Eyal", "title": "Multi-Party Timed Commitments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of obtaining secret commitments from multiple parties and\nrevealing them after a certain time is useful for sealed-bid auctions, games,\nand other applications. Existing solutions, dating back to Rivest, Shamir and\nWagner, either do not scale or rely on synchrony for the commitment phase and\ntrust of $t/n$ parties. We formalize the problem of implementing such\ncommitments with a probabilistic delay and without the aforementioned\nassumptions as Multi-Party Timed Commitments (MPTC) and present a solution --\nthe Time-Capsule protocol. Like previous approaches, Time Capsule forms a\npuzzle whose solution reveals the committed values. But unlike previous\nsolutions, no party has an advantage in solving the puzzle, and individual\ncommitments cannot be revealed before the entire set is committed. A particular\napplication of MPTC realizes an advancement in the study of decentralized\nsystems. The state of the art in decentralized systems is manifested in\nblockchain systems that utilize Proof of Work to achieve censorship resistance.\nHowever, they are still vulnerable to frontrunning, an issue that is plaguing\noperational systems. By adapting Time Capsule, we allow it to be used for Proof\nof Work, preventing frontrunning by system operators and tuning the puzzle\ndifficulty using the blockchain mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 06:54:30 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 12:20:40 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Doweck", "Yael", ""], ["Eyal", "Ittay", ""]]}, {"id": "2005.04935", "submitter": "Vincenzo Gulisano", "authors": "Hannaneh Najdataei, Vincenzo Gulisano, Alessandro V. Papadopoulos,\n  Ivan Walulya, Marina Papatriantafilou, Philippas Tsigas", "title": "Performance Modeling and Vertical Autoscaling of Stream Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming analysis is widely used in cloud as well as edge infrastructures.\nIn these contexts, fine-grained application performance can be based on\naccurate modeling of streaming operators. This is especially beneficial for\ncomputationally expensive operators like adaptive stream joins that, being very\nsensitive to rate-varying data streams, would otherwise require costly frequent\nmonitoring.\n  We propose a dynamic model for the processing throughput and latency of\nadaptive stream joins that run with different parallelism degrees. The model is\npresented with progressive complexity, from a centralized non-deterministic up\nto a deterministic parallel stream join, describing how throughput and latency\ndynamics are influenced by various configuration parameters. The model is\ncatalytic for understanding the behavior of stream joins against different\nsystem deployments, as we show with our model-based autoscaling methodology to\nchange the parallelism degree of stream joins during the execution. Our\nthorough evaluation, for a broad spectrum of parameter, confirms the model can\nreliably predict throughput and latency metrics with a fairly high accuracy,\nwith the median error in estimation ranging from approximately 0.1% to 6.5%,\neven for an overloaded system. Furthermore, we show that our model allows to\nefficiently control adaptive stream joins by estimating the needed resources\nsolely based on the observed input load. In particular, we show it can be\nemployed to enable efficient autoscaling, even when big changes in the input\nload happen frequently (in the realm of seconds).\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 09:02:53 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Najdataei", "Hannaneh", ""], ["Gulisano", "Vincenzo", ""], ["Papadopoulos", "Alessandro V.", ""], ["Walulya", "Ivan", ""], ["Papatriantafilou", "Marina", ""], ["Tsigas", "Philippas", ""]]}, {"id": "2005.05036", "submitter": "Sally Elghamrawy Prof", "authors": "Manar A. Elmeiligy, Ali I. El Desouky, Sally M. Elghamrawy", "title": "A Multi-Dimensional Big Data Storing System for Generated COVID-19\n  Large-Scale Data using Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing outbreak of coronavirus disease (COVID-19) had burst out in Wuhan\nChina, specifically in December 2019. COVID-19 has caused by a new virus that\nhad not been identified in human previously. This was followed by a widespread\nand rapid spread of this epidemic throughout the world. Daily, the number of\nthe confirmed cases are increasing rapidly, number of the suspect increases,\nbased on the symptoms that accompany this disease, and unfortunately number of\nthe deaths also increase. Therefore, with these increases in number of cases\naround the world, it becomes hard to manage all these cases information with\ndifferent situations; if the patient either injured or suspect with which\nsymptoms that appeared on the patient. Therefore, there is a critical need to\nconstruct a multi-dimensional system to store and analyze the generated\nlarge-scale data. In this paper, a Comprehensive Storing System for COVID-19\ndata using Apache Spark (CSS-COVID) is proposed, to handle and manage the\nproblem caused by increasing the number of COVID-19 daily. CSS-COVID helps in\ndecreasing the processing time for querying and storing COVID-19 daily data.\nCSS-COVID consists of three stages, namely, inserting and indexing, storing,\nand querying stage. In the inserting stage, data is divided into subsets and\nthen index each subset separately. The storing stage uses set of storing-nodes\nto store data, while querying stage is responsible for handling the querying\nprocesses. Using Apache Spark in CSS-COVID leverages the performance of dealing\nwith large-scale data of the coronavirus disease injured whom increase daily. A\nset of experiments are applied, using real COVID-19 Datasets, to prove the\nefficiency of CSS-COVID in indexing large-scale data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:42:07 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Elmeiligy", "Manar A.", ""], ["Desouky", "Ali I. El", ""], ["Elghamrawy", "Sally M.", ""]]}, {"id": "2005.05044", "submitter": "Paolo Nesi", "authors": "C. Badii, P. Bellini, S. Bilotta, D. Bologna, D. Cenni, A. Difino, A.\n  Ipsaro Palesi, N. Mitolo, P. Nesi, G. Pantaleo, I. Paoli, M. Paolucci, M.\n  Soderi", "title": "Impact on Mobility and Environmental data of COVID-19 Lockdown on\n  Florence Area", "comments": "13 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  According to the changed operative conditions due to lockdown and successive\nreopening a number of facts can be analysed. The main effects have been\ndetected on: mobility, environment, social media and people flows. While in\nthis first report only mobility, transport and environment are reported. The\nanalysis performed identified a strong reduction of the mobility and transport\nactivities, and in the pollutants. The mobility reduction has been assessed to\nbe quite coherent with respect to what has been described by Google Global\nmobility report. On the other hand, in this paper a number of additional\naspects have been put in evidence providing detailed aspects on mobility and\nparking that allowed us to better analyze the impact of the reopening on an\neventual revamping of the infection. To this end, the collected data from the\nfield have been compared from those of google and some considerations with\nrespect to the Imperial college Report 20 have been derived. For the pollutant\naspects, a relevant reduction on most of them has been measured and rationales\nare reported.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 11:33:43 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Badii", "C.", ""], ["Bellini", "P.", ""], ["Bilotta", "S.", ""], ["Bologna", "D.", ""], ["Cenni", "D.", ""], ["Difino", "A.", ""], ["Palesi", "A. Ipsaro", ""], ["Mitolo", "N.", ""], ["Nesi", "P.", ""], ["Pantaleo", "G.", ""], ["Paoli", "I.", ""], ["Paolucci", "M.", ""], ["Soderi", "M.", ""]]}, {"id": "2005.05061", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh and \\'Ad\\'am J. Berki", "title": "Do we know the operating principles of our computers better than those\n  of our brain?", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing interest in understanding the behavior of the biological\nneural networks, and the increasing utilization of artificial neural networks\nin different fields and scales, both require a thorough understanding of how\nneuromorphic computing works. On the one side, the need to program those\nartificial neuron-like elements, and, on the other side, the necessity for a\nlarge number of such elements to cooperate, communicate and compute during\ntasks, need to be scrutinized to determine how efficiently conventional\ncomputing can assist in implementing such systems. Some electronic components\nbear a surprising resemblance to some biological structures. However, combining\nthem with components that work using different principles can result in systems\nwith very poor efficacy. The paper discusses how the conventional principles,\ncomponents and thinking about computing limit mimicking the biological systems.\nWe describe what changes will be necessary in the computing paradigms to get\ncloser to the marvelously efficient operation of biological neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 20:41:23 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""], ["Berki", "\u00c1d\u00e1m J.", ""]]}, {"id": "2005.05083", "submitter": "Binhang Yuan", "authors": "Binhang Yuan and Song Ge and Wenhui Xing", "title": "A Federated Learning Framework for Healthcare IoT devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) revolution has shown potential to give rise to\nmany medical applications with access to large volumes of healthcare data\ncollected by IoT devices. However, the increasing demand for healthcare data\nprivacy and security makes each IoT device an isolated island of data. Further,\nthe limited computation and communication capacity of wearable healthcare\ndevices restrict the application of vanilla federated learning. To this end, we\npropose an advanced federated learning framework to train deep neural networks,\nwhere the network is partitioned and allocated to IoT devices and a centralized\nserver. Then most of the training computation is handled by the powerful\nserver. The sparsification of activations and gradients significantly reduces\nthe communication overhead. Empirical study have suggested that the proposed\nframework guarantees a low accuracy loss, while only requiring 0.2% of the\nsynchronization traffic in vanilla federated learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 22:58:43 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yuan", "Binhang", ""], ["Ge", "Song", ""], ["Xing", "Wenhui", ""]]}, {"id": "2005.05268", "submitter": "Uzay \\c{C}etin", "authors": "Uzay Cetin and Yunus Emre Gundogmus", "title": "Feature Selection with Evolving, Fast and Slow Using Two Parallel\n  Genetic Algorithms", "comments": null, "journal-ref": "Conference: 2019 4th International Conference on Computer Science\n  and Engineering (UBMK)", "doi": "10.1109/UBMK.2019.8907165", "report-no": null, "categories": "cs.NE cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is one of the most challenging issues in machine learning,\nespecially while working with high dimensional data. In this paper, we address\nthe problem of feature selection and propose a new approach called Evolving\nFast and Slow. This new approach is based on using two parallel genetic\nalgorithms having high and low mutation rates, respectively. Evolving Fast and\nSlow requires a new parallel architecture combining an automatic system that\nevolves fast and an effortful system that evolves slow. With this architecture,\nexploration and exploitation can be done simultaneously and in unison. Evolving\nfast, with high mutation rate, can be useful to explore new unknown places in\nthe search space with long jumps; and Evolving Slow, with low mutation rate,\ncan be useful to exploit previously known places in the search space with short\nmovements. Our experiments show that Evolving Fast and Slow achieves very good\nresults in terms of both accuracy and feature elimination.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:10:39 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Cetin", "Uzay", ""], ["Gundogmus", "Yunus Emre", ""]]}, {"id": "2005.05295", "submitter": "Arthur Goldberg", "authors": "Arthur P. Goldberg (1) and David R. Jefferson (2) and John A. P. Sekar\n  (1) and Jonathan R. Karr (1) ((1) Icahn Institute for Data Science and\n  Genomic Technology, and Department of Genetics and Genomic Sciences, Icahn\n  School of Medicine at Mount Sinai, (2) Lawrence Livermore National\n  Laboratory)", "title": "Exact Parallelization of the Stochastic Simulation Algorithm for\n  Scalable Simulation of Large Biochemical Networks", "comments": "21 pages, 4 figures; 2020-05-20 submission: updated authors,\n  affiliations, emails, acknowledgments and layout", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.DC cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive simulations of the entire biochemistry of cells have great\npotential to help physicians treat disease and help engineers design biological\nmachines. But such simulations must model networks of millions of molecular\nspecies and reactions.\n  The Stochastic Simulation Algorithm (SSA) is widely used for simulating\nbiochemistry, especially systems with species populations small enough that\ndiscreteness and stochasticity play important roles. However, existing serial\nSSA methods are prohibitively slow for comprehensive networks, and existing\nparallel SSA methods, which use periodic synchronization, sacrifice accuracy.\n  To enable fast, accurate, and scalable simulations of biochemistry, we\npresent an exact parallel algorithm for SSA that partitions a biochemical\nnetwork into many SSA processes that simulate in parallel. Our parallel SSA\nalgorithm exactly coordinates the interactions among these SSA processes and\nthe species state they share by structuring the algorithm as a parallel\ndiscrete event simulation (DES) application and using an optimistic parallel\nDES simulator to synchronize the interactions. We anticipate that our method\nwill enable unprecedented biochemical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:56:21 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 21:27:01 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Goldberg", "Arthur P.", ""], ["Jefferson", "David R.", ""], ["Sekar", "John A. P.", ""], ["Karr", "Jonathan R.", ""]]}, {"id": "2005.05427", "submitter": "Armando Casta\\~neda", "authors": "Armando Casta\\~neda and Sergio Rajsbaum and Michel Raynal", "title": "Relaxed Queues and Stacks from Read/Write Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering asynchronous shared memory systems in which any number of\nprocesses may crash, this work identifies and formally defines relaxations of\nqueues and stacks that can be non-blocking or wait-free while being implemented\nusing only read/write operations. Set-linearizability and\nInterval-linearizability are used to specify the relaxations formally, and\nprecisely identify the subset of executions which preserve the original\nsequential behavior. The relaxations allow for an item to be returned more than\nonce by different operations, but only in case of concurrency; we call such a\nproperty multiplicity. The stack implementation is wait-free, while the queue\nimplementation is non-blocking. Interval-linearizability is used to describe a\nqueue with multiplicity, with the additional relaxation that a dequeue\noperation can return weak-empty, which means that the queue might be empty. We\npresent a read/write wait-free interval-linearizable algorithm of a concurrent\nqueue. As far as we know, this work is the first that provides formalizations\nof the notions of multiplicity and weak-emptiness, which can be implemented on\ntop of read/write registers only.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:49:20 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 16:35:10 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Casta\u00f1eda", "Armando", ""], ["Rajsbaum", "Sergio", ""], ["Raynal", "Michel", ""]]}, {"id": "2005.05510", "submitter": "Jing Pan", "authors": "Jing Pan, Wendao Liu, Jing Zhou", "title": "Benchmark Tests of Convolutional Neural Network and Graph Convolutional\n  Network on HorovodRunner Enabled Spark Clusters", "comments": "AAAI 2020 W8 Deep Learning on Graphs: Methodologies and Applications\n  Accepted Poster Number 23", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The freedom of fast iterations of distributed deep learning tasks is crucial\nfor smaller companies to gain competitive advantages and market shares from big\ntech giants. HorovodRunner brings this process to relatively accessible spark\nclusters. There have been, however, no benchmark tests on HorovodRunner per se,\nnor specifically graph convolutional network (GCN, hereafter), and very limited\nscalability benchmark tests on Horovod, the predecessor requiring custom built\nGPU clusters. For the first time, we show that Databricks' HorovodRunner\nachieves significant lift in scaling efficiency for the convolutional neural\nnetwork (CNN, hereafter) based tasks on both GPU and CPU clusters, but not the\noriginal GCN task. We also implemented the Rectified Adam optimizer for the\nfirst time in HorovodRunner.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:36:43 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Pan", "Jing", ""], ["Liu", "Wendao", ""], ["Zhou", "Jing", ""]]}, {"id": "2005.05601", "submitter": "Suman Sourav", "authors": "Barath Ashok, John Augustine, Aditya Mehekare, Sridhar Ragupathi,\n  Srikkanth Ramachandran, and Suman Sourav", "title": "Guarding a Polygon Without Losing Touch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical Art Gallery Problem first proposed by Klee in 1973\nfrom a mobile multi-agents perspective. Specifically, we require an optimally\nsmall number of agents (also called guards) to navigate and position themselves\nin the interior of an unknown simple polygon with $n$ vertices such that the\ncollective view of all the agents covers the polygon.\n  We consider the visibly connected setting wherein agents must remain\nconnected through line of sight links -- a requirement particularly relevant to\nmulti-agent systems. We first provide a centralized algorithm for the visibly\nconnected setting that runs in time $O(n)$, which is of course optimal. We then\nprovide algorithms for two different distributed settings. In the first\nsetting, agents can only perceive relative proximity (i.e., can tell which of a\npair of objects is closer) whereas they can perceive exact distances in the\nsecond setting. Our distributed algorithms work despite agents having no prior\nknowledge of the polygon. Furthermore, we provide lower bounds to show that our\ndistributed algorithms are near-optimal.\n  Our visibly connected guarding ensures that (i) the guards form a connected\nnetwork and (ii) the polygon is fully guarded. Consequently, this guarding\nprovides the distributed infrastructure to execute any geometric algorithm for\nthis polygon.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 08:15:32 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Ashok", "Barath", ""], ["Augustine", "John", ""], ["Mehekare", "Aditya", ""], ["Ragupathi", "Sridhar", ""], ["Ramachandran", "Srikkanth", ""], ["Sourav", "Suman", ""]]}, {"id": "2005.05758", "submitter": "Runbin Shi", "authors": "Runbin Shi, Peiyan Dong, Tong Geng, Yuhao Ding, Xiaolong Ma, Hayden\n  K.-H. So, Martin Herbordt, Ang Li and Yanzhi Wang", "title": "CSB-RNN: A Faster-than-Realtime RNN Acceleration Framework with\n  Compressed Structured Blocks", "comments": null, "journal-ref": null, "doi": "10.1145/3392717.3392749", "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have been widely adopted in temporal\nsequence analysis, where realtime performance is often in demand. However, RNNs\nsuffer from heavy computational workload as the model often comes with large\nweight matrices. Pruning schemes have been proposed for RNNs to eliminate the\nredundant (close-to-zero) weight values. On one hand, the non-structured\npruning methods achieve a high pruning rate but introducing computation\nirregularity (random sparsity), which is unfriendly to parallel hardware. On\nthe other hand, hardware-oriented structured pruning suffers from low pruning\nrate due to restricted constraints on allowable pruning structure. This paper\npresents CSB-RNN, an optimized full-stack RNN framework with a novel compressed\nstructured block (CSB) pruning technique. The CSB pruned RNN model comes with\nboth fine pruning granularity that facilitates a high pruning rate and regular\nstructure that benefits the hardware parallelism. To address the challenges in\nparallelizing the CSB pruned model inference with fine-grained structural\nsparsity, we propose a novel hardware architecture with a dedicated compiler.\nGaining from the architecture-compilation co-design, the hardware not only\nsupports various RNN cell types, but is also able to address the challenging\nworkload imbalance issue and therefore significantly improves the hardware\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 13:13:18 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Shi", "Runbin", ""], ["Dong", "Peiyan", ""], ["Geng", "Tong", ""], ["Ding", "Yuhao", ""], ["Ma", "Xiaolong", ""], ["So", "Hayden K. -H.", ""], ["Herbordt", "Martin", ""], ["Li", "Ang", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2005.05826", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi, Daniel McDonald and Rob Knight", "title": "Porting and optimizing UniFrac for GPUs", "comments": "4 pages, 3 figures, 4 tables", "journal-ref": null, "doi": "10.1145/3311790.3399614", "report-no": null, "categories": "cs.DC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  UniFrac is a commonly used metric in microbiome research for comparing\nmicrobiome profiles to one another (\"beta diversity\"). The recently implemented\nStriped UniFrac added the capability to split the problem into many independent\nsubproblems and exhibits near linear scaling. In this paper we describe steps\nundertaken in porting and optimizing Striped Unifrac to GPUs. We reduced the\nrun time of computing UniFrac on the published Earth Microbiome Project dataset\nfrom 13 hours on an Intel Xeon E5-2680 v4 CPU to 12 minutes on an NVIDIA Tesla\nV100 GPU, and to about one hour on a laptop with NVIDIA GTX 1050 (with minor\nloss in precision). Computing UniFrac on a larger dataset containing 113k\nsamples reduced the run time from over one month on the CPU to less than 2\nhours on the V100 and 9 hours on an NVIDIA RTX 2080TI GPU (with minor loss in\nprecision). This was achieved by using OpenACC for generating the GPU offload\ncode and by improving the memory access patterns. A BSD-licensed implementation\nis available, which produces a C shared library linkable by any programming\nlanguage.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:41:28 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Sfiligoi", "Igor", ""], ["McDonald", "Daniel", ""], ["Knight", "Rob", ""]]}, {"id": "2005.05836", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi", "title": "Demonstrating 100 Gbps in and out of the public Clouds", "comments": "4 pages, 6 figures, 3 tables", "journal-ref": null, "doi": "10.1145/3311790.3399612", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is increased awareness and recognition that public Cloud providers do\nprovide capabilities not found elsewhere, with elasticity being a major driver.\nThe value of elastic scaling is however tightly coupled to the capabilities of\nthe networks that connect all involved resources, both in the public Clouds and\nat the various research institutions. This paper presents results of\nmeasurements involving file transfers inside public Cloud providers, fetching\ndata from on-prem resources into public Cloud instances and fetching data from\npublic Cloud storage into on-prem nodes. The networking of the three major\nCloud providers, namely Amazon Web Services, Microsoft Azure and the Google\nCloud Platform, has been benchmarked. The on-prem nodes were managed by either\nthe Pacific Research Platform or located at the University of Wisconsin -\nMadison. The observed sustained throughput was of the order of 100 Gbps in all\nthe tests moving data in and out of the public Clouds and throughput reaching\ninto the Tbps range for data movements inside the public Cloud providers\nthemselves. All the tests used HTTP as the transfer protocol.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:56:05 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Sfiligoi", "Igor", ""]]}, {"id": "2005.05863", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley, Pierre Fraigniaud, Ivan Rapaport, \\'Eric R\\'emila,\n  Pedro Montealegre and Ioan Todinca", "title": "Compact Distributed Certification of Planar Graphs", "comments": "To appear at PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naor, Parter, and Yogev (SODA 2020) have recently demonstrated the existence\nof a \\emph{distributed interactive proof} for planarity (i.e., for certifying\nthat a network is planar), using a sophisticated generic technique for\nconstructing distributed IP protocols based on sequential IP protocols. The\ninteractive proof for planarity is based on a distributed certification of the\ncorrect execution of any given sequential linear-time algorithm for planarity\ntesting. It involves three interactions between the prover and the randomized\ndistributed verifier (i.e., it is a \\dMAM\\/ protocol), and uses small\ncertificates, on $O(\\log n)$ bits in $n$-node networks. We show that a single\ninteraction from the prover suffices, and randomization is unecessary, by\nproviding an explicit description of a \\emph{proof-labeling scheme} for\nplanarity, still using certificates on just $O(\\log n)$ bits. We also show that\nthere are no proof-labeling schemes -- in fact, even no \\emph{locally checkable\nproofs} -- for planarity using certificates on $o(\\log n)$ bits.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:34:04 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Feuilloley", "Laurent", ""], ["Fraigniaud", "Pierre", ""], ["Rapaport", "Ivan", ""], ["R\u00e9mila", "\u00c9ric", ""], ["Montealegre", "Pedro", ""], ["Todinca", "Ioan", ""]]}, {"id": "2005.05899", "submitter": "Guillermo Oyarzun", "authors": "R. Borrell and D. Dosimont and M. Garcia-Gasulla and G. Houzeaux and\n  O. Lehmkuhl and V. Mehta and H. Owen and M. Vazquez and G. Oyarzun", "title": "Heterogeneous CPU/GPU co-execution of CFD simulations on the POWER9\n  architecture: Application to airplane aerodynamics", "comments": null, "journal-ref": "Future Generation Computer Systems, Volume 107, 2020,Pages 31-48", "doi": "10.1016/j.future.2020.01.045", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High fidelity Computational Fluid Dynamics simulations are generally\nassociated with large computing requirements, which are progressively acute\nwith each new generation of supercomputers. However, significant research\nefforts are required to unlock the computing power of leading-edge systems,\ncurrently referred to as pre-Exascale systems, based on increasingly complex\narchitectures. In this paper, we present the approach implemented in the\ncomputational mechanics code Alya. We describe in detail the parallelization\nstrategy implemented to fully exploit the different levels of parallelism,\ntogether with a novel co-execution method for the efficient utilization of\nheterogeneous CPU/GPU architectures. The latter is based on a multi-code\nco-execution approach with a dynamic load balancing mechanism. The assessment\nof the performance of all the proposed strategies has been carried out for\nairplane simulations on the POWER9 architecture accelerated with NVIDIA Volta\nV100 GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 16:21:39 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 08:12:21 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 10:14:36 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Borrell", "R.", ""], ["Dosimont", "D.", ""], ["Garcia-Gasulla", "M.", ""], ["Houzeaux", "G.", ""], ["Lehmkuhl", "O.", ""], ["Mehta", "V.", ""], ["Owen", "H.", ""], ["Vazquez", "M.", ""], ["Oyarzun", "G.", ""]]}, {"id": "2005.05910", "submitter": "Antonio J. Pe\\v{n}a", "authors": "Sergio Iserte and Rafael Mayo and Enrique S. Quintana-Orti and Vicenc\n  Beltran and Antonio J. Pe\\~na", "title": "DMR API: Improving cluster productivity by turning applications into\n  malleable", "comments": null, "journal-ref": "S. Iserte, R. Mayo, E. S. Quintana-Orti, V. Beltran, and A. J.\n  Pe\\~na, \"DMR API: Improving cluster productivity by turning applications into\n  malleable\", Parallel Computing, Elsevier, vol. 78, pp. 54-66, Oct. 2018", "doi": "10.1016/j.parco.2018.07.006", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive workloads can change on--the--fly the configuration of their jobs,\nin terms of number of processes. In order to carry out these job\nreconfigurations, we have designed a methodology which enables a job to\ncommunicate with the resource manager and, through the runtime, to change its\nnumber of MPI ranks. The collaboration between both the workload\nmanager---aware of the queue of jobs and the resource allocation---and the\nparallel runtime---able to transparently handle the processes and the program\ndata---is crucial for our throughput-aware malleability methodology. Hence,\nwhen a job triggers a reconfiguration, the resource manager will check the\ncluster status and return an action: an expansion, if there are spare\nresources; a shrink, if queued jobs can be initiated; or none, if no change can\nimprove the global productivity. In this paper, we describe the internals of\nour framework and how it is capable of reducing the global workload completion\ntime along with providing a smarter usage of the underlying resources. For this\npurpose, we present a thorough study of the adaptive workloads processing by\nshowing the detailed behavior of our framework in representative experiments\nand the low overhead that our reconfiguration involves.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 16:41:55 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 22:31:28 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Iserte", "Sergio", ""], ["Mayo", "Rafael", ""], ["Quintana-Orti", "Enrique S.", ""], ["Beltran", "Vicenc", ""], ["Pe\u00f1a", "Antonio J.", ""]]}, {"id": "2005.05968", "submitter": "Minsoo Rhu", "authors": "Ranggi Hwang, Taehun Kim, Youngeun Kwon, Minsoo Rhu", "title": "Centaur: A Chiplet-based, Hybrid Sparse-Dense Accelerator for\n  Personalized Recommendations", "comments": "Accepted for publication at the 47th IEEE/ACM International Symposium\n  on Computer Architecture (ISCA-47), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations are the backbone machine learning (ML) algorithm\nthat powers several important application domains (e.g., ads, e-commerce, etc)\nserviced from cloud datacenters. Sparse embedding layers are a crucial building\nblock in designing recommendations yet little attention has been paid in\nproperly accelerating this important ML algorithm. This paper first provides a\ndetailed workload characterization on personalized recommendations and\nidentifies two significant performance limiters: memory-intensive embedding\nlayers and compute-intensive multi-layer perceptron (MLP) layers. We then\npresent Centaur, a chiplet-based hybrid sparse-dense accelerator that addresses\nboth the memory throughput challenges of embedding layers and the compute\nlimitations of MLP layers. We implement and demonstrate our proposal on an\nIntel HARPv2, a package-integrated CPU+FPGA device, which shows a 1.7-17.2x\nperformance speedup and 1.7-19.5x energy-efficiency improvement than\nconventional approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 07:53:35 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Hwang", "Ranggi", ""], ["Kim", "Taehun", ""], ["Kwon", "Youngeun", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2005.06043", "submitter": "Tarek Elgamal", "authors": "Tarek Elgamal, Klara Nahrstedt", "title": "Serdab: An IoT Framework for Partitioning Neural Networks Computation\n  across Multiple Enclaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Deep Neural Networks (DNN) and Edge Computing have made it\npossible to automatically analyze streams of videos from home/security cameras\nover hierarchical clusters that include edge devices, close to the video\nsource, as well as remote cloud compute resources. However, preserving the\nprivacy and confidentiality of users' sensitive data as it passes through\ndifferent devices remains a concern to most users. Private user data is subject\nto attacks by malicious attackers or misuse by internal administrators who may\nuse the data in activities that are not explicitly approved by the user. To\naddress this challenge, we present Serdab, a distributed orchestration\nframework for deploying deep neural network computation across multiple secure\nenclaves (e.g., Intel SGX). Secure enclaves provide a guarantee on the privacy\nof the data/code deployed inside it. However, their limited hardware resources\nmake them inefficient when solely running an entire deep neural network. To\nbridge this gap, Serdab presents a DNN partitioning strategy to distribute the\nlayers of the neural network across multiple enclave devices or across an\nenclave device and other hardware accelerators. Our partitioning strategy\nachieves up to 4.7x speedup compared to executing the entire neural network in\none enclave.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 20:51:47 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Elgamal", "Tarek", ""], ["Nahrstedt", "Klara", ""]]}, {"id": "2005.06087", "submitter": "Victoria Stodden", "authors": "Kyle Chard, Niall Gaffney, Mihael Hategan, Kacper Kowalik, Bertram\n  Ludaescher, Timothy McPhillips, Jarek Nabrzyski, Victoria Stodden, Ian\n  Taylor, Thomas Thelen, Matthew J. Turk, and Craig Willis", "title": "Toward Enabling Reproducibility for Data-Intensive Research using the\n  Whole Tale Platform", "comments": null, "journal-ref": "Advances in Parallel Computing 2020", "doi": "10.3233/APC200107", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole Tale http://wholetale.org is a web-based, open-source platform for\nreproducible research supporting the creation, sharing, execution, and\nverification of \"Tales\" for the scientific research community. Tales are\nexecutable research objects that capture the code, data, and environment along\nwith narrative and workflow information needed to re-create computational\nresults from scientific studies. Creating reproducible research objects that\nenable reproducibility, transparency, and re-execution for computational\nexperiments requiring significant compute resources or utilizing massive data\nis an especially challenging open problem. We describe opportunities,\nchallenges, and solutions to facilitating reproducibility for data- and\ncompute-intensive research, that we call \"Tales at Scale,\" using the Whole Tale\ncomputing platform. We highlight challenges and solutions in frontend\nresponsiveness needs, gaps in current middleware design and implementation,\nnetwork restrictions, containerization, and data access. Finally, we discuss\nchallenges in packaging computational experiment implementations for portable\ndata-intensive Tales and outline future work.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 23:23:24 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Chard", "Kyle", ""], ["Gaffney", "Niall", ""], ["Hategan", "Mihael", ""], ["Kowalik", "Kacper", ""], ["Ludaescher", "Bertram", ""], ["McPhillips", "Timothy", ""], ["Nabrzyski", "Jarek", ""], ["Stodden", "Victoria", ""], ["Taylor", "Ian", ""], ["Thelen", "Thomas", ""], ["Turk", "Matthew J.", ""], ["Willis", "Craig", ""]]}, {"id": "2005.06102", "submitter": "Leeor Peled", "authors": "Leeor Peled, Uri Weiser and Yoav Etsion", "title": "Semantic prefetching using forecast slices", "comments": "Under conference review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern prefetchers identify memory access patterns in order to predict future\naccesses. However, many applications exhibit irregular access patterns that do\nnot manifest spatio-temporal locality in the memory address space. Such\napplications usually do not fall under the scope of existing prefetching\ntechniques, which observe only the stream of addresses dispatched by the memory\nunit but not the code flows that produce them. Similarly, temporal correlation\nprefetchers detect recurring relations between accesses, but do not track the\nchain of causality in program code that manifested the memory locality.\nConversely, techniques that are code-aware are limited to the basic program\nfunctionality and are bounded by the machine depth. In this paper we show that\ncontextual analysis of the code flows that generate memory accesses can detect\nrecurring code patterns and expose their underlying semantics even for\nirregular access patterns. Moreover, program locality artifacts can be used to\nenhance the memory traversal code and predict future accesses. We present the\nsemantic prefetcher that analyzes programs at run-time and learns their memory\ndependency chains and address calculation flows. The prefetcher then constructs\nforecast slices and injects them at key points to trigger timely prefetching of\nfuture contextually-related iterations. We show how this approach takes the\nbest of both worlds, augmenting code injection with forecast functionality and\nrelying on context-based temporal correlation of code slices. This combination\nallows us to overcome critical memory latencies that are currently not covered\nby any other prefetcher. Our evaluation of the semantic prefetcher using an\nindustrial-grade, cycle-accurate x86 simulator shows that it improves\nperformance by 24% on average over SPEC 2006 (outliers up to 3.7x), and 16% on\naverage over SPEC 2017 (outliers up to 1.85x), using only ~6KB.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 01:24:23 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Peled", "Leeor", ""], ["Weiser", "Uri", ""], ["Etsion", "Yoav", ""]]}, {"id": "2005.06151", "submitter": "Robert Gardner Jr.", "authors": "Robert Gardner, Lincoln Bryant, Mark Neubauer, Frank Wuerthwein,\n  Judith Stephen and Andrew Chien", "title": "The Scalable Systems Laboratory: a Platform for Software Innovation for\n  HEP", "comments": null, "journal-ref": null, "doi": "10.1051/epjconf/202024505019", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Scalable Systems Laboratory (SSL), part of the IRIS-HEP Software\nInstitute, provides Institute participants and HEP software developers\ngenerally with a means to transition their R&D from conceptual toys to testbeds\nto production-scale prototypes. The SSL enables tooling, infrastructure, and\nservices supporting the innovation of novel analysis and data architectures,\ndevelopment of software elements and tool-chains, reproducible functional and\nscalability testing of service components, and foundational systems R&D for\naccelerated services developed by the Institute. The SSL is constructed with a\ncore team having expertise in scale testing and deployment of services across a\nwide range of cyberinfrastructure. The core team embeds and partners with other\nareas in the Institute, and with LHC and other HEP development and operations\nteams as appropriate, to define investigations and required service deployment\npatterns. We describe the approach and experiences with early application\ndeployments, including analysis platforms and intelligent data delivery\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 05:04:53 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Gardner", "Robert", ""], ["Bryant", "Lincoln", ""], ["Neubauer", "Mark", ""], ["Wuerthwein", "Frank", ""], ["Stephen", "Judith", ""], ["Chien", "Andrew", ""]]}, {"id": "2005.06154", "submitter": "Shantanu Sharma", "authors": "Sharad Mehrotra, Shantanu Sharma, Jeffrey D. Ullman, Dhrubajyoti\n  Ghosh, Peeyush Gupta", "title": "Panda: Partitioned Data Security on Outsourced Sensitive and\n  Non-sensitive Data", "comments": "This version has been accepted in ACM Transactions on Management\n  Information Systems. The final published version of this paper may differ\n  from this accepted version. A preliminary version of this paper\n  [arXiv:1812.09233] was accepted and presented in IEEE ICDE 2019", "journal-ref": null, "doi": "10.1145/3397521", "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite extensive research on cryptography, secure and efficient query\nprocessing over outsourced data remains an open challenge. This paper continues\nalong with the emerging trend in secure data processing that recognizes that\nthe entire dataset may not be sensitive, and hence, non-sensitivity of data can\nbe exploited to overcome limitations of existing encryption-based approaches.\nWe, first, provide a new security definition, entitled partitioned data\nsecurity for guaranteeing that the joint processing of non-sensitive data (in\ncleartext) and sensitive data (in encrypted form) does not lead to any leakage.\nThen, this paper proposes a new secure approach, entitled query binning (QB)\nthat allows secure execution of queries over non-sensitive and sensitive parts\nof the data. QB maps a query to a set of queries over the sensitive and\nnon-sensitive data in a way that no leakage will occur due to the joint\nprocessing over sensitive and non-sensitive data. In particular, we propose\nsecure algorithms for selection, range, and join queries to be executed over\nencrypted sensitive and cleartext non-sensitive datasets. Interestingly, in\naddition to improving performance, we show that QB actually strengthens the\nsecurity of the underlying cryptographic technique by preventing size,\nfrequency-count, and workload-skew attacks.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 05:27:18 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""], ["Ghosh", "Dhrubajyoti", ""], ["Gupta", "Peeyush", ""]]}, {"id": "2005.06332", "submitter": "Antonio J. Pe\\v{n}a", "authors": "Pedro Valero-Lara and Ra\\\"ul Sirvent and Antonio J. Pe\\~na and Jes\\'us\n  Labarta", "title": "MPI+OpenMP Tasking Scalability for Multi-Morphology Simulations of the\n  Human Brain", "comments": null, "journal-ref": "P. Valero-Lara, R. Sirvent, A. J. Pe\\~na, and J. Labarta.\n  \"MPI+OpenMP tasking scalability for multi-morphology simulations of the human\n  brain\", Parallel Computing, Elsevier, vol. 84, pp. 50-61, May 2019", "doi": "10.1016/j.parco.2019.03.006", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of the behavior of the human brain is one of the most\nambitious challenges today with a non-end of important applications. We can\nfind many different initiatives in the USA, Europe and Japan which attempt to\nachieve such a challenging target. In this work, we focus on the most important\nEuropean initiative (the Human Brain Project) and on one of the models\ndeveloped in this project. This tool simulates the spikes triggered in a neural\nnetwork by computing the voltage capacitance on the neurons' morphology, being\none of the most precise simulators today. In the present work, we have\nevaluated the use of MPI+OpenMP tasking on top of this framework. We prove that\nthis approach is able to achieve a good scaling even when computing a\nrelatively low workload (number of neurons) per node. One of our targets\nconsists of achieving not only a highly scalable implementation, but also to\ndevelop a tool with a high degree of abstraction without losing control and\nperformance by using \\emph{MPI+OpenMP} tasking. The main motivation of this\nwork is the evaluation of this cutting-edge simulation on multi-morphology\nneural networks. The simulation of a high number of neurons, which are\ncompletely different among them, is an important challenge. In fact, in the\nmulti-morphology simulations, we find an important unbalancing between the\nnodes, mainly due to the differences in the neurons, which causes an important\nunder-utilization of the available resources. In this work, the authors present\nand evaluate mechanisms to deal with this and reduce the time of this kind of\nsimulations considerably.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:15:24 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Valero-Lara", "Pedro", ""], ["Sirvent", "Ra\u00fcl", ""], ["Pe\u00f1a", "Antonio J.", ""], ["Labarta", "Jes\u00fas", ""]]}, {"id": "2005.06356", "submitter": "Jo\\~ao Barreto", "authors": "Paulo Silva, David Vav\\v{r}i\\v{c}ka, Jo\\~ao Barreto and Miguel Matos", "title": "Impact of Geo-distribution and Mining Pools on Blockchains: A Study of\n  Ethereum", "comments": "To appear in 50th IEEE/IFIP International Conference on Dependable\n  Systems and Networks (DSN), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the large adoption and economical impact of permissionless blockchains,\nthe complexity of the underlying systems and the adversarial environment in\nwhich they operate, it is fundamental to properly study and understand the\nemergent behavior and properties of these systems. We describe our experience\non a detailed, one-month study of the Ethereum network from several\ngeographically dispersed observation points. We leverage multiple geographic\nvantage points to assess the key pillars of Ethereum, namely geographical\ndispersion, network efficiency, blockchain efficiency and security, and the\nimpact of mining pools. Among other new findings, we identify previously\nundocumented forms of selfish behavior and show that the prevalence of powerful\nmining pools exacerbates the geographical impact on block propagation delays.\nFurthermore, we provide a set of open measurement and processing tools, as well\nas the data set of the collected measurements, in order to promote further\nresearch on understanding permissionless blockchains.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:03:50 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Silva", "Paulo", ""], ["Vav\u0159i\u010dka", "David", ""], ["Barreto", "Jo\u00e3o", ""], ["Matos", "Miguel", ""]]}, {"id": "2005.06361", "submitter": "Vicent Gim\\'enez Alventosa", "authors": "Vicent Gim\\'enez Alventosa, Germ\\'an Molt\\'o Mart\\'inez, J. Dami\\'an\n  Segrelles Quilis", "title": "RUPER-LB: Load balancing embarrasingly parallel applications in\n  unpredictable cloud environments", "comments": "13 pages, 9 figures, Added download link previously blinded due\n  double blinded review process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suitability of cloud computing has been studied by several authors to run\nscientific applications. However, the unpredictable performance fluctuations in\nthese environments hinders the migration of scientific applications to cloud\nproviders. To mitigate these effects, this work presents RUPER-LB, a load\nbalancer for loosely-coupled iterative parallel applications that runs on\ninfrastructures with disparate computing capabilities. The results obtained\nwith a real world simulation software, show the suitability of RUPER-LB to\nadapt this kind of applications to execution environments with variable\nperformance and highlight the convenience of its adoption.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:12:18 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 20:52:56 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Alventosa", "Vicent Gim\u00e9nez", ""], ["Mart\u00ednez", "Germ\u00e1n Molt\u00f3", ""], ["Quilis", "J. Dami\u00e1n Segrelles", ""]]}, {"id": "2005.06502", "submitter": "Gadi Taubenfeld", "authors": "Sabrina Rashid, Gadi Taubenfeld and Ziv Bar-Joseph", "title": "Genome-Wide Epigenetic Modifications as a Shared Memory Consensus\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed computing system is a collection of processors that communicate\neither by reading and writing from a shared memory or by sending messages over\nsome communication network. Most prior biologically inspired distributed\ncomputing algorithms rely on message passing as the communication model. Here\nwe show that in the process of genome-wide epigenetic modifications cells\nutilize their DNA as a shared memory system. We formulate a particular\nconsensus problem, called the epigenetic consensus problem, that cells attempt\nto solve using this shared memory model, and then present algorithms, derive\nexpected run time and discuss, analyze and simulate improved methods for\nsolving this problem. Analysis of real biological data indicates that the\ncomputational methods indeed reflect aspects of the biological process for\ngenome-wide epigenetic modifications.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:16:34 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Rashid", "Sabrina", ""], ["Taubenfeld", "Gadi", ""], ["Bar-Joseph", "Ziv", ""]]}, {"id": "2005.06528", "submitter": "Magnus M. Halldorsson", "authors": "Magnus M. Halldorsson, Fabian Kuhn, Yannic Maus", "title": "Distance-2 Coloring in the CONGEST Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give efficient randomized and deterministic distributed algorithms for\ncomputing a distance-$2$ vertex coloring of a graph $G$ in the CONGEST model.\nIn particular, if $\\Delta$ is the maximum degree of $G$, we show that there is\na randomized CONGEST model algorithm to compute a distance-$2$ coloring of $G$\nwith $\\Delta^2+1$ colors in $O(\\log\\Delta\\cdot\\log n)$ rounds. Further if the\nnumber of colors is slightly increased to $(1+\\epsilon)\\Delta^2$ for some\n$\\epsilon>1/{\\rm polylog}(n)$, we show that it is even possible to compute a\ndistance-$2$ coloring deterministically in polylog$(n)$ time in the CONGEST\nmodel. Finally, we give a $O(\\Delta^2 + \\log^* n)$-round deterministic CONGEST\nalgorithm to compute distance-$2$ coloring with $\\Delta^2+1$ colors.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:58:48 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "2005.06597", "submitter": "Javad Mohammadi", "authors": "Juye Kim and Carolyn Goodman and Javad Mohammadi", "title": "CarnegiePLUG: Prosumer-in-the-Loop simUlation Grid", "comments": "Accepted for presentation, IEEE 6th World Forum on Internet of Things\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Carnegie Mellon campus-wide CarnegiePLUG test-bed. This\ntest-bed in a hardware-in-the-loop simulator that enables large-scale sensing,\ncomputation and actuation over a network of heterogeneous energy producers and\nconsumers (prosuming) assets. CarnegiePLUG is a multi-agent framework that\nfacilitates plug-and-play integration of a wide range of virtual and physical\nsensors and controllers through a multi-layer architecture. Virtual\nfunctionalities are accessed in software environments that interface with\ncommunication layer of our test-bed. This paper discusses CarnegiePLUG layers,\nprovides implementation details and showcases potential applications of this\ntest-bed through preliminary results.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 16:46:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Kim", "Juye", ""], ["Goodman", "Carolyn", ""], ["Mohammadi", "Javad", ""]]}, {"id": "2005.06665", "submitter": "Diego Pennino", "authors": "Gianmaria Del Monte, Diego Pennino, Maurizio Pizzonia", "title": "Scaling Blockchains Without Giving up Decentralization and Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public blockchains should be able to scale with respect to the number of\nnodes and to the transactions workload. The blockchain scalability trilemma has\nbeen informally conjectured. This is related to scalability, security and\ndecentralization, stating that any improvement in one of these aspects should\nnegatively impact on at least one of the other twos. In fact, despite the large\nresearch and experimental effort, all known approaches turn out to be\ntradeoffs. We theoretically describe a new blockchain architecture that scales\nto arbitrarily high workload provided that a corresponding proportional\nincrement of nodes is provisioned. We show that, under reasonable assumptions,\nour approach does not require tradeoffs on security or decentralization. To the\nbest of our knowledge, this is the first result that disprove the trilemma\nconsidering the scalability of all architectural elements of a blockchain and\nnot only the consensus protocol. While our result is currently only theoretic,\nwe believe that ot our approach may stimulate significant practical\ncontributions.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 23:39:40 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:20:00 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Del Monte", "Gianmaria", ""], ["Pennino", "Diego", ""], ["Pizzonia", "Maurizio", ""]]}, {"id": "2005.06666", "submitter": "Suayb Arslan", "authors": "Suayb S. Arslan and Elif Haytaoglu", "title": "Guessing Cost: Applications to Distributed DataStorage and Repair in\n  Cellular Networks", "comments": "11 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of guessing cost (also referred to as the cost of guessing) is\nintroduced and an optimal strategy for the $\\rho$-th moment of guessing cost is\nprovided for a random variable taking values on a finite set whereby each\nchoice is associated with a positive finite cost value. Moreover, we drive\nasymptotically tight upper and lower bounds on the moments of cost of guessing\nproblem. Similar to previous studies on the standard guesswork, established\nbounds on the moments of guessing cost quantify the accumulated cost of guesses\nrequired for correctly identifying the unknown choice and are expressed in\nterms of the Renyi's entropy. Moreover, anew random variable is introduced to\nestablish connections between the guessing cost and the standard guesswork.\nBased on this observation, further improved bounds are conjectured in the\nnon-asymptotic region by borrowing ideas from recent works given for the\nstandard guesswork. Finally, we establish the guessing cost exponent in terms\nof the Renyi's entropy rate on the moments of the optimal guessing cost by\nconsidering a sequence of independent random variables. Finally, these bounds\nare shown to be very useful for bounding the overall repair bandwidth for\ndistributed data storage systems. Our particular application utilizes a sparse\ngraph code (low-density parity-check codes) in conjunction with a back-up\nmaster which is pretty common in cellular networks.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 23:42:42 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 13:32:31 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Arslan", "Suayb S.", ""], ["Haytaoglu", "Elif", ""]]}, {"id": "2005.06727", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi and Fabrizio Petrini", "title": "An Efficient Shared-memory Parallel Sinkhorn-Knopp Algorithm to Compute\n  the Word Mover's Distance", "comments": "10 pages, 1 page for reference, total 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Word Mover's Distance (WMD) is a metric that measures the semantic\ndissimilarity between two text documents by computing the cost of moving all\nwords of a source/query document to the most similar words of a target document\noptimally. Computing WMD between two documents is costly because it requires\nsolving an optimization problem that costs \\(O(V^3log(V))\\) where \\(V\\) is the\nnumber of unique words in the document. Fortunately, the WMD can be framed as\nthe Earth Mover's Distance (EMD) (also known as the Optimal Transportation\nDistance) for which it has been shown that the algorithmic complexity can be\nreduced to \\(O(V^2)\\) by adding an entropy penalty to the optimization problem\nand a similar idea can be adapted to compute WMD efficiently. Additionally, the\ncomputation can be made highly parallel by computing WMD of a single query\ndocument against multiple target documents at once (e.g., finding whether a\ngiven tweet is similar to any other tweets happened in a day). In this paper,\nwe present a shared-memory parallel Sinkhorn-Knopp Algorithm to compute the WMD\nof one document against many other documents by adopting the \\(O(V^2)\\) EMD\nalgorithm. We used algorithmic transformations to change the original dense\ncompute-heavy kernel to a sparse compute kernel and obtained \\(67\\times\\)\nspeedup using \\(96\\) cores on the state-of-the-art of Intel\\textregistered{}\n4-sockets Cascade Lake machine w.r.t. its sequential run. Our parallel\nalgorithm is over \\(700\\times\\) faster than the naive parallel python code that\ninternally uses optimized matrix library calls.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 05:30:18 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 23:06:41 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 20:35:08 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Petrini", "Fabrizio", ""]]}, {"id": "2005.06728", "submitter": "Yemao Xu Mr", "authors": "Yemao Xu and Dezun Dong and Weixia Xu and Xiangke Liao", "title": "OD-SGD: One-step Delay Stochastic Gradient Descent for Distributed\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of modern deep learning neural network calls for large amounts\nof computation, which is often provided by GPUs or other specific accelerators.\nTo scale out to achieve faster training speed, two update algorithms are mainly\napplied in the distributed training process, i.e. the Synchronous SGD algorithm\n(SSGD) and Asynchronous SGD algorithm (ASGD). SSGD obtains good convergence\npoint while the training speed is slowed down by the synchronous barrier. ASGD\nhas faster training speed but the convergence point is lower when compared to\nSSGD. To sufficiently utilize the advantages of SSGD and ASGD, we propose a\nnovel technology named One-step Delay SGD (OD-SGD) to combine their strengths\nin the training process. Therefore, we can achieve similar convergence point\nand training speed as SSGD and ASGD separately. To the best of our knowledge,\nwe make the first attempt to combine the features of SSGD and ASGD to improve\ndistributed training performance. Each iteration of OD-SGD contains a global\nupdate in the parameter server node and local updates in the worker nodes, the\nlocal update is introduced to update and compensate the delayed local weights.\nWe evaluate our proposed algorithm on MNIST, CIFAR-10 and ImageNet datasets.\nExperimental results show that OD-SGD can obtain similar or even slightly\nbetter accuracy than SSGD, while its training speed is much faster, which even\nexceeds the training speed of ASGD.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 05:33:36 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Xu", "Yemao", ""], ["Dong", "Dezun", ""], ["Xu", "Weixia", ""], ["Liao", "Xiangke", ""]]}, {"id": "2005.06773", "submitter": "Eduardo S\\'anchez Morales", "authors": "Eduardo S\\'anchez Morales, Richard Membarth, Andreas Gaull, Philipp\n  Slusallek, Tobias Dirndorfer, Alexander Kammenhuber, Christoph Lauer, and\n  Michael Botsch", "title": "Parallel Multi-Hypothesis Algorithm for Criticality Estimation in\n  Traffic and Collision Avoidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the current developments towards autonomous driving and vehicle active\nsafety, there is an increasing necessity for algorithms that are able to\nperform complex criticality predictions in real-time. Being able to process\nmulti-object traffic scenarios aids the implementation of a variety of\nautomotive applications such as driver assistance systems for collision\nprevention and mitigation as well as fall-back systems for autonomous vehicles.\n  We present a fully model-based algorithm with a parallelizable architecture.\nThe proposed algorithm can evaluate the criticality of complex, multi-modal\n(vehicles and pedestrians) traffic scenarios by simulating millions of\ntrajectory combinations and detecting collisions between objects. The algorithm\nis able to estimate upcoming criticality at very early stages, demonstrating\nits potential for vehicle safety-systems and autonomous driving applications.\nAn implementation on an embedded system in a test vehicle proves in a\nprototypical manner the compatibility of the algorithm with the hardware\npossibilities of modern cars. For a complex traffic scenario with 11 dynamic\nobjects, more than 86 million pose combinations are evaluated in 21 ms on the\nGPU of a Drive PX~2.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 07:42:49 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Morales", "Eduardo S\u00e1nchez", ""], ["Membarth", "Richard", ""], ["Gaull", "Andreas", ""], ["Slusallek", "Philipp", ""], ["Dirndorfer", "Tobias", ""], ["Kammenhuber", "Alexander", ""], ["Lauer", "Christoph", ""], ["Botsch", "Michael", ""]]}, {"id": "2005.06814", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert, Ioana Cristescu (HMS)", "title": "How Reversibility Can Solve Traditional Questions: The Example of\n  Hereditary History-Preserving Bisimulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.FL cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible computation opens up the possibility of overcoming some of the\nhardware's current physical limitations. It also offers theoretical insights,\nas it enriches multiple paradigms and models of computation, and sometimes\nretrospectively enlightens them. Concurrent reversible computation, for\ninstance, offered interesting extensions to the Calculus of Communicating\nSystems, but was still lacking a natural and pertinent bisimulation to study\nprocesses equivalences. Our paper formulates an equivalence exploiting the two\naspects of reversibility: backward moves and memory mechanisms. This\nbisimulation captures classical equivalences relations for denotational models\nof concurrency (History-and hereditary history-preserving bisimulation,\n(H)HPB), that were up to now only partially characterized by process algebras.\nThis result gives an insight on the expressiveness of reversibility, as both\nbackward moves and a memory mechanism-providing 'backward determinism'-are\nneeded to capture HHPB.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 08:55:07 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "HMS"], ["Cristescu", "Ioana", "", "HMS"]]}, {"id": "2005.06850", "submitter": "Thomas Hiessl", "authors": "Thomas Hiessl, Daniel Schall, Jana Kemnitz, Stefan Schulte", "title": "Industrial Federated Learning -- Requirements and System Design", "comments": "12 pages, accepted for https://www.paams.net/workshops/agedai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a very promising approach for improving\ndecentralized Machine Learning (ML) models by exchanging knowledge between\nparticipating clients without revealing private data. Nevertheless, FL is still\nnot tailored to the industrial context as strong data similarity is assumed for\nall FL tasks. This is rarely the case in industrial machine data with\nvariations in machine type, operational- and environmental conditions.\nTherefore, we introduce an Industrial Federated Learning (IFL) system\nsupporting knowledge exchange in continuously evaluated and updated FL cohorts\nof learning tasks with sufficient data similarity. This enables optimal\ncollaboration of business partners in common ML problems, prevents negative\nknowledge transfer, and ensures resource optimization of involved edge devices.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 10:07:48 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Hiessl", "Thomas", ""], ["Schall", "Daniel", ""], ["Kemnitz", "Jana", ""], ["Schulte", "Stefan", ""]]}, {"id": "2005.06913", "submitter": "Suryanarayana Murthy Durbhakula", "authors": "Suryanarayana Murthy Durbhakula", "title": "Parallel Minimum Spanning Tree Algorithms and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum Spanning Tree (MST) is an important graph algorithm that has wide\nranging applications in the areas of computer networks, VLSI routing, wireless\ncommunications among others. Today virtually every computer is built out of\nmulti-core processors. Hence it is important to take advantage of such parallel\ncomputing power by parallelizing existing algorithms and applications. Most of\nthe earlier work on parallelizing MST focused on algorithms for PRAM models.\nThere are two limitations to such studies. First, PRAM models assume infinite\nmemory bandwidth which is unrealistic. Second, PRAM model based algorithms\nrequire at least O(n) processors where n being total number of vertices. For\nlarge graphs this is infeasible. There are very few implementations which\ntarget real systems. In this paper I present and evaluate two new parallel MST\nalgorithms that are a variant of Parallel Boruvka algorithms: i) First\nalgorithm uses lock variables without spin-locks ii) Second algorithm uses only\natomic compare-and-swap (CAS) primitive. I evaluated the performance of these\nalgorithms on a six-core, 12-thread Intel system on various input graphs of\nsizes up to 1 million vertices. First algorithm showed a speedup of up to1.94\nover an un-optimized sequential algorithm and a speedup of up to 1.4 over an\noptimized sequential algorithm. Second algorithm showed a speedup of up to 2.03\nover an un-optimized sequential algorithm and a speedup of up to 1.403 over an\noptimized sequential algorithm. When second algorithm using CAS is compared\nwith the first algorithm second algorithm is found to be up to 1.15 times\nbetter than the first algorithm at four threads.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 12:30:27 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Durbhakula", "Suryanarayana Murthy", ""]]}, {"id": "2005.06958", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan and Mansaf Alam", "title": "Wearable Internet of Things for Personalized Healthcare Study of Trends\n  and Latent Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this age of heterogeneous systems, diverse technologies are integrated to\ncreate application-specific solutions. The recent upsurge in acceptance of\ntechnologies such as cloud computing and ubiquitous Internet has cleared the\npath for Internet of Things (IoT). Moreover, the increasing Internet\npenetration with the rising use of mobile devices has inspired an era of\ntechnology that allows interfacing of physical objects and connecting them to\nInternet for developing applications serving a wide range of purposes. Recent\ndevelopments in the area of wearable devices has led to the creation of another\nsegment in IoT, which can be conveniently referred to as Wearable Internet of\nThings (WIoT). Research in this area promises to personalize healthcare in\npreviously unimaginable ways by allowing individual tracking of wellness and\nhealth information. This chapter shall cover the different facets of Wearable\nInternet of Things (WIoT) and ways in which it is a key driving technology\nbehind the concept of personalized healthcare. It shall discuss the theoretical\naspects of WIoT, focusing on functionality, design and applicability. Moreover,\nit shall also elaborate on the role of wearable sensors, big data and cloud\ncomputing as enabling technologies for WIoT.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:04:08 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Khan", "Samiya", ""], ["Alam", "Mansaf", ""]]}, {"id": "2005.06963", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri and Sabuzima Nayak", "title": "A Survey on Large Scale Metadata Server for Big Data Storage", "comments": "Submitted to ACM for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big Data is defined as high volume of variety of data with an exponential\ndata growth rate. Data are amalgamated to generate revenue, which results a\nlarge data silo. Data are the oils of modern IT industries. Therefore, the data\nare growing at an exponential pace. The access mechanism of these data silos\nare defined by metadata. The metadata are decoupled from data server for\nvarious beneficial reasons. For instance, ease of maintenance. The metadata are\nstored in metadata server (MDS). Therefore, the study on the MDS is mandatory\nin designing of a large scale storage system. The MDS requires many parameters\nto augment with its architecture. The architecture of MDS depends on the demand\nof the storage system's requirements. Thus, MDS is categorized in various ways\ndepending on the underlying architecture and design methodology. The article\nsurveys on the various kinds of MDS architecture, designs, and methodologies.\nThis article emphasizes on clustered MDS (cMDS) and the reports are prepared\nbased on a) Bloom filter$-$based MDS, b) Client$-$funded MDS, c) Geo$-$aware\nMDS, d) Cache$-$aware MDS, e) Load$-$aware MDS, f) Hash$-$based MDS, and g)\nTree$-$based MDS. Additionally, the article presents the issues and challenges\nof MDS for mammoth sized data.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 20:49:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Patgiri", "Ripon", ""], ["Nayak", "Sabuzima", ""]]}, {"id": "2005.06964", "submitter": "Ripon Patgiri", "authors": "Sabuzima Nayak and Ripon Patgiri and Thoudam Doren Singh", "title": "Big Computing: Where are we heading?", "comments": "Published in EAI Endorsed Transactions on Scalable Information\n  Systems", "journal-ref": "EAI Endorsed Transactions on Scalable Information Systems, 2020", "doi": "10.4108/eai.13-7-2018.163972", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the overview of the current trends of Big data against\nthe computing scenario from different aspects. Some of the important aspect\nincludes the Exascale, the computing power and the kind of applications which\noffer the Big data. This starts with the current computing hardware constraint\nagainst the need of the rising Big data applications. We highlight the issues\nand challenges of energy requirement, software complexity, hardware failure,\nfault tolerant computing, and communication. As the complexity of computation\nis going to rise in the future. The paper also highlights the future direction\nof Big computing systems for Bioinformatics, social media, hardware and\nsoftware requirements, data intensive computation and then towards GPU era.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 12:32:56 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Nayak", "Sabuzima", ""], ["Patgiri", "Ripon", ""], ["Singh", "Thoudam Doren", ""]]}, {"id": "2005.07041", "submitter": "Navjot Singh", "authors": "Navjot Singh, Deepesh Data, Jemin George, Suhas Diggavi", "title": "SQuARM-SGD: Communication-Efficient Momentum SGD for Decentralized\n  Optimization", "comments": "50 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1910.14280", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study communication-efficient decentralized training of\nlarge-scale machine learning models over a network. We propose and analyze\nSQuARM-SGD, a decentralized training algorithm, employing momentum and\ncompressed communication between nodes regulated by a locally computable\ntriggering rule. In SQuARM-SGD, each node performs a fixed number of local SGD\n(stochastic gradient descent) steps using Nesterov's momentum and then sends\nsparisified and quantized updates to its neighbors only when there is a\nsignificant change in its model parameters since the last time communication\noccurred. We provide convergence guarantees of our algorithm for\nstrongly-convex and non-convex smooth objectives. We believe that ours is the\nfirst theoretical analysis for compressed decentralized SGD with momentum\nupdates. We show that SQuARM-SGD converges at rate\n$\\mathcal{O}\\left(\\frac{1}{nT}\\right)$ for strongly-convex objectives, while\nfor non-convex objectives it converges at rate\n$\\mathcal{O}\\left(\\frac{1}{\\sqrt{nT}}\\right)$, thus matching the convergence\nrate of \\emph{vanilla} distributed SGD in both these settings. We corroborate\nour theoretical understanding with experiments and compare the performance of\nour algorithm with the state-of-the-art, showing that without sacrificing much\non the accuracy, SQuARM-SGD converges at a similar rate while saving\nsignificantly in total communicated bits.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 02:11:14 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 22:28:32 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Singh", "Navjot", ""], ["Data", "Deepesh", ""], ["George", "Jemin", ""], ["Diggavi", "Suhas", ""]]}, {"id": "2005.07119", "submitter": "Kartik Jain", "authors": "Kartik Jain", "title": "Efficacy of the FDA nozzle benchmark and the lattice Boltzmann method\n  for the analysis of biomedical flows in transitional regime", "comments": null, "journal-ref": null, "doi": "10.1007/s11517-020-02188-8", "report-no": null, "categories": "physics.flu-dyn cs.DC nlin.CG physics.bio-ph", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Flows through medical devices as well as in anatomical vessels despite being\nat moderate Reynolds number may exhibit transitional or even turbulent\ncharacter. In order to validate numerical methods and codes used for biomedical\nflow computations, the U.S. food and drug administration (FDA) established an\nexperimental benchmark, which was a pipe with gradual contraction and sudden\nexpansion representing a nozzle. The experimental results for various Reynolds\nnumbers ranging from 500 to 6500 were publicly released. Previous and recent\ncomputational investigations of flow in the FDA nozzle found limitations in\nvarious CFD approaches and some even questioned the adequacy of the benchmark\nitself. This communication reports the results of a lattice Boltzmann method\n(LBM) based direct numerical simulation (DNS) approach applied to the FDA\nnozzle benchmark for transitional cases of Reynolds numbers 2000 and 3500. The\ngoal is to evaluate if a simple off the shelf LBM would predict the\nexperimental results without the use of complex models or synthetic turbulence\nat the inflow. LBM computations with various spatial and temporal resolutions\nare performed - in the extremities of 44 million to 2.8 billion lattice cells -\nconducted respectively on 32 CPU cores of a desktop to more than 300'000 cores\nof a modern supercomputer to explore and characterize miniscule flow details\nand quantify Kolmogorov scales. The LBM simulations transition to turbulence at\na Reynolds number 2000 like the FDA's experiments and acceptable agreement in\njet breakdown locations, average velocity, shear stress and pressure is found\nfor both the Reynolds numbers.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:39:01 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Jain", "Kartik", ""]]}, {"id": "2005.07136", "submitter": "Pradeeban Kathiravelu", "authors": "Pradeeban Kathiravelu, Peter Van Roy, Lu\\'is Veiga, Elhadj Benkhelifa", "title": "Latency-Sensitive Web Service Workflows: A Case for a Software-Defined\n  Internet", "comments": "Accepted for Publication at The Seventh International Conference on\n  Software Defined Systems (SDS-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet, at large, remains under the control of service providers and\nautonomous systems. The Internet of Things (IoT) and edge computing provide an\nincreasing demand and potential for more user control for their web service\nworkflows. Network Softwarization revolutionizes the network landscape in\nvarious stages, from building, incrementally deploying, and maintaining the\nenvironment. Software-Defined Networking (SDN) and Network Functions\nVirtualization (NFV) are two core tenets of network softwarization. SDN offers\na logically centralized control plane by abstracting away the control of the\nnetwork devices in the data plane. NFV virtualizes dedicated hardware\nmiddleboxes and deploys them on top of servers and data centers as network\nfunctions. Thus, network softwarization enables efficient management of the\nsystem by enhancing its control and improving the reusability of the network\nservices. In this work, we propose our vision for a Software-Defined Internet\n(SDI) for latency-sensitive web service workflows. SDI extends network\nsoftwarization to the Internet-scale, to enable a latency-aware user workflow\nexecution on the Internet.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:57:52 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Kathiravelu", "Pradeeban", ""], ["Van Roy", "Peter", ""], ["Veiga", "Lu\u00eds", ""], ["Benkhelifa", "Elhadj", ""]]}, {"id": "2005.07184", "submitter": "Swanand Kadhe", "authors": "Swanand Kadhe, O. Ozan Koyluoglu, and Kannan Ramchandran", "title": "Communication-Efficient Gradient Coding for Straggler Mitigation in\n  Distributed Learning", "comments": "Shorter version accepted in 2020 IEEE International Symposium on\n  Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distributed implementations of gradient-based methods, wherein a server\ndistributes gradient computations across worker machines, need to overcome two\nlimitations: delays caused by slow running machines called 'stragglers', and\ncommunication overheads. Recently, Ye and Abbe [ICML 2018] proposed a\ncoding-theoretic paradigm to characterize a fundamental trade-off between\ncomputation load per worker, communication overhead per worker, and straggler\ntolerance. However, their proposed coding schemes suffer from heavy decoding\ncomplexity and poor numerical stability. In this paper, we develop a\ncommunication-efficient gradient coding framework to overcome these drawbacks.\nOur proposed framework enables using any linear code to design the encoding and\ndecoding functions. When a particular code is used in this framework, its\nblock-length determines the computation load, dimension determines the\ncommunication overhead, and minimum distance determines the straggler\ntolerance. The flexibility of choosing a code allows us to gracefully trade-off\nthe straggler threshold and communication overhead for smaller decoding\ncomplexity and higher numerical stability. Further, we show that using a\nmaximum distance separable (MDS) code generated by a random Gaussian matrix in\nour framework yields a gradient code that is optimal with respect to the\ntrade-off and, in addition, satisfies stronger guarantees on numerical\nstability as compared to the previously proposed schemes. Finally, we evaluate\nour proposed framework on Amazon EC2 and demonstrate that it reduces the\naverage iteration time by 16% as compared to prior gradient coding schemes.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:57:13 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Kadhe", "Swanand", ""], ["Koyluoglu", "O. Ozan", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "2005.07262", "submitter": "Hans Dermot Doran", "authors": "Hans Dermot Doran", "title": "Voting Framework for Distributed Real-Time Ethernet based Dependable and\n  Safe Systems", "comments": "4 pages, 3 figures, conference - International Conference on Factory\n  Communication Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many industrial sectors such as factory automation and process control\nsensor redundancy is required to ensure reliable and highly-available\noperation. Measured values from N-redundant sensors are typically subjected to\nsome voting scheme to determine a value which is used in further processing. In\nthis paper we present a voting framework which allows the sensors and the\nvoting scheme to be configured at systemconfiguration time. The voting scheme\nis designed as a Real Time Ethernet profile. We describe the structure of the\nvoting system and the design and verification of the framework. We argue the\napplicability of this sub-system based on a successful prototype\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:05:13 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Doran", "Hans Dermot", ""]]}, {"id": "2005.07282", "submitter": "Roman Iakymchuk", "authors": "Roman Iakymchuk, Maria Barreda, Stef Graillat, Jose I. Aliaga, Enrique\n  S. Quintana-Orti", "title": "Reproducibility of Parallel Preconditioned Conjugate Gradient in Hybrid\n  Programming Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Preconditioned Conjugate Gradient method is often employed for the\nsolution of linear systems of equations arising in numerical simulations of\nphysical phenomena. While being widely used, the solver is also known for its\nlack of accuracy while computing the residual. In this article, we propose two\nalgorithmic solutions that originate from the ExBLAS project to enhance the\naccuracy of the solver as well as to ensure its reproducibility in a hybrid MPI\n+ OpenMP tasks programming environment. One is based on ExBLAS and preserves\nevery bit of information until the final rounding, while the other relies upon\nfloating-point expansions and, hence, expands the intermediate precision.\nInstead of converting the entire solver into its ExBLAS-related implementation,\nwe identify those parts that violate reproducibility/non-associativity, secure\nthem, and combine this with the sequential executions. These algorithmic\nstrategies are reinforced with programmability suggestions to assure\ndeterministic executions. Finally, we verify these approaches on two modern HPC\nsystems: both versions deliver reproducible number of iterations, residuals,\ndirect errors, and vector-solutions for the overhead of less than 37.7 % on 768\ncores.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 22:10:15 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Iakymchuk", "Roman", ""], ["Barreda", "Maria", ""], ["Graillat", "Stef", ""], ["Aliaga", "Jose I.", ""], ["Quintana-Orti", "Enrique S.", ""]]}, {"id": "2005.07338", "submitter": "Saurabh Bagchi", "authors": "Saurabh Bagchi, Tarek F. Abdelzaher, Ramesh Govindan, Prashant Shenoy,\n  Akanksha Atrey, Pradipta Ghosh, Ran Xu", "title": "New Frontiers in IoT: Networking, Systems, Reliability, and Security\n  Challenges", "comments": "Invited paper to the IEEE IoT Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of IoT has blossomed and is positively influencing many application\ndomains. In this paper, we bring out the unique challenges this field poses to\nresearch in computer systems and networking. The unique challenges arise from\nthe unique characteristics of IoT systems such as the diversity of application\ndomains where they are used and the increasingly demanding protocols they are\nbeing called upon to run (such as, video and LIDAR processing) on constrained\nresources (on-node and network). We show how these open challenges can benefit\nfrom foundations laid in other areas, such as, 5G cellular protocols, ML model\nreduction, and device-edge-cloud offloading. We then discuss the unique\nchallenges for reliability, security, and privacy posed by IoT systems due to\ntheir salient characteristics which include heterogeneity of devices and\nprotocols, dependence on the physical environment, and the close coupling with\nhumans. We again show how the open research challenges benefit from\nreliability, security, and privacy advancements in other areas. We conclude by\nproviding a vision for a desirable end state for IoT systems.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 03:23:58 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Bagchi", "Saurabh", ""], ["Abdelzaher", "Tarek F.", ""], ["Govindan", "Ramesh", ""], ["Shenoy", "Prashant", ""], ["Atrey", "Akanksha", ""], ["Ghosh", "Pradipta", ""], ["Xu", "Ran", ""]]}, {"id": "2005.07373", "submitter": "Reza Fathi", "authors": "Reza Fathi, Anisur Rahaman Molla, Gopal Pandurangan", "title": "Efficient Distributed Algorithms for the $K$-Nearest Neighbors Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$-nearest neighbors is a basic problem in machine learning with\nnumerous applications. In this problem, given a (training) set of $n$ data\npoints with labels and a query point $p$, we want to assign a label to $p$\nbased on the labels of the $K$-nearest points to the query. We study this\nproblem in the {\\em $k$-machine model}, (Note that parameter $k$ stands for the\nnumber of machines in the $k$-machine model and is independent of $K$-nearest\npoints.) a model for distributed large-scale data. In this model, we assume\nthat the $n$ points are distributed (in a balanced fashion) among the $k$\nmachines and the goal is to quickly compute answer given a query point to a\nmachine.\n  Our main result is a simple randomized algorithm in the $k$-machine model\nthat runs in $O(\\log K)$ communication rounds with high probability success\n(regardless of the number of machines $k$ and the number of points $n$). The\nmessage complexity of the algorithm is small taking only $O(k\\log K)$ messages.\nOur bounds are essentially the best possible for comparison-based algorithms\n(Algorithms that use only comparison operations ($\\leq, \\geq, =$) between\nelements to distinguish the ordering among them). This is due to the existence\nof a lower bound of $\\Omega(\\log n)$ communication rounds for finding the {\\em\nmedian} of $2n$ elements distributed evenly among two processors by Rodeh\n\\cite{rodeh}.\n  We also implemented our algorithm and show that it performs well compared to\nan algorithm (used in practice) that sends $K$ nearest points from each machine\nto a single machine which then computes the answer.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 06:24:43 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 01:52:20 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 02:53:00 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Fathi", "Reza", ""], ["Molla", "Anisur Rahaman", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "2005.07388", "submitter": "Michael Feldmann M. Sc.", "authors": "Michael Feldmann, Ardalan Khazraei, Christian Scheideler", "title": "Time- and Space-Optimal Clock Synchronization in the Beeping Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the clock synchronization problem in the (discrete) beeping\nmodel: Given a network of $n$ nodes with each node having a clock value\n$\\delta(v) \\in \\{0,\\ldots T-1\\}$, the goal is to synchronize the clock values\nof all nodes such that they have the same value in any round. As is standard in\nclock synchronization, we assume \\emph{arbitrary activations} for all nodes,\ni.e., the nodes start their protocol at an arbitrary round (not limited to\n$\\{0,\\ldots,T-1\\}$). We give an asymptotically optimal algorithm that runs in\n$4D + \\Bigl\\lfloor \\frac{D}{\\lfloor T/4 \\rfloor} \\Bigr \\rfloor \\cdot (T \\mod 4)\n= O(D)$ rounds, where $D$ is the diameter of the network. Once all nodes are in\nsync, they beep at the same round every $T$ rounds. The algorithm drastically\nimproves on the $O(T D)$-bound of [ACGL'13] (where $T$ is required to be at\nleast $4n$, so the bound is no better than $O(nD)$). Our algorithm is very\nsimple as nodes only have to maintain $3$ bits in addition to the $\\lceil \\log\nT \\rceil$ bits needed to maintain the clock. Furthermore we investigate the\ncomplexity of \\emph{self-stabilizing} solutions for the clock synchronization\nproblem: We first show lower bounds of $\\Omega(\\max\\{T,n\\})$ rounds on the\nruntime and $\\Omega(\\log(\\max\\{T,n\\}))$ bits of memory required for any such\nprotocol. Afterwards we present a protocol that runs in $O(\\max\\{T,n\\})$ rounds\nusing at most $O(\\log(\\max\\{T,n\\}))$ bits at each node, which is asymptotically\noptimal with regards to both, runtime and memory requirements.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 07:30:48 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Feldmann", "Michael", ""], ["Khazraei", "Ardalan", ""], ["Scheideler", "Christian", ""]]}, {"id": "2005.07423", "submitter": "Francesco d'Amore", "authors": "Francesco d'Amore (COATI), Andrea Clementi, Emanuele Natale (COATI)", "title": "Phase Transition of a Non-Linear Opinion Dynamics with Noisy\n  Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several real \\emph{Multi-Agent Systems} (MAS), it has been observed that\nonly weaker forms of\\emph{metastable consensus} are achieved, in which a large\nmajority of agents agree on some opinion while other opinions continue to be\nsupported by a (small) minority of agents. In this work, we take a step towards\nthe investigation of metastable consensus for complex (non-linear)\n\\emph{opinion dynamics} by considering the famous \\undecided dynamics in the\nbinary setting, which is known to reach consensus exponentially faster than the\n\\voter dynamics. We propose a simple form of uniform noise in which each\nmessage can change to another one with probability $p$ and we prove that the\npersistence of a \\emph{metastable consensus} undergoes a \\emph{phase\ntransition} for $p=\\frac 16$. In detail, below this threshold, we prove the\nsystem reaches with high probability a metastable regime where a large majority\nof agents keeps supporting the same opinion for polynomial time. Moreover, this\nopinion turns out to be the initial majority opinion, whenever the initial bias\nis slightly larger than its standard deviation.On the contrary, above the\nthreshold, we show that the information about the initial majority opinion is\n\"lost\" within logarithmic time even when the initial bias is\nmaximum.Interestingly, using a simple coupling argument, we show the\nequivalence between our noisy model above and the model where a subset of\nagents behave in a \\emph{stubborn} way.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:04:29 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["d'Amore", "Francesco", "", "COATI"], ["Clementi", "Andrea", "", "COATI"], ["Natale", "Emanuele", "", "COATI"]]}, {"id": "2005.07453", "submitter": "Giuseppe Antonio Di Luna", "authors": "Giuseppe Antonio Di Luna and Paola Flocchini and Giuseppe Prencipe and\n  Nicola Santoro", "title": "Tight Bounds for Black Hole Search in Dynamic Rings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we start the investigation of distributed computing by mobile\nagents in dangerous dynamic networks. The danger is posed by the presence in\nthe network of a black hole BH, a harmful site that destroys all incoming\nagents without leaving any trace. The problem of determining the location of\nthe black hole in a network, known as black hole search BHS, has been\nextensively studied in the literature, but always and only assuming that the\nnetwork is static. At the same time, the existing results on mobile agents\ncomputing in dynamic networks never consider the presence of harmful sites.\n  In this paper we start filling this research gap by studying black hole\nsearch in temporal rings, specifically focusing on 1-interval connectivity\nadversarial dynamics. The problem is solved if within finite time at least one\nagent survives and knows the location of BH. The main complexity parameters of\nBHS is the number of agents (called size) needed to solve the problem, and the\nnumber of moves (called cost) performed by the agents; in synchronous systems,\nsuch as temporal rings, an additional complexity measure is the amount of time\nuntil termination occurs.\n  Feasibility and complexity depend on many parameters; in particular: whether\nthe agents start from the same safe node or from possibly distinct safe\nlocations, the size $n$ of the ring, whether or not $n$ is known, and the type\nof inter-agent communication (whiteboards, tokens, face-to-face, visual). In\nthis paper, we provide a complete feasibility characterization for all\ninstances of those parameters; all our algorithms are size optimal.\nFurthermore, we establish lower bounds on the cost (i.e., the number of moves)\nand time of size-optimal solutions for all instances of those parameters and\nshow that our algorithms achieve those bound.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 10:06:01 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 19:43:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Di Luna", "Giuseppe Antonio", ""], ["Flocchini", "Paola", ""], ["Prencipe", "Giuseppe", ""], ["Santoro", "Nicola", ""]]}, {"id": "2005.07475", "submitter": "Martin Uhrin", "authors": "Martin Uhrin and Sebastiaan P. Huber", "title": "kiwiPy: Robust, high-volume, messaging for big-data and computational\n  science workflows", "comments": null, "journal-ref": "Journal of Open Source Software 5 2351 (2020)", "doi": "10.21105/joss.02351", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work we present kiwiPy, a Python library designed to support robust\nmessage based communication for high-throughput, big-data, applications while\nbeing general enough to be useful wherever high-volumes of messages need to be\ncommunicated in a predictable manner. KiwiPy relies on the RabbitMQ protocol,\nan industry standard message broker, while providing a simple and intuitive\ninterface that can be used in both multithreaded and coroutine based\napplications. To demonstrate some of kiwiPy's functionality we give examples\nfrom AiiDA, a high-throughput simulation platform, where kiwiPy is used as a\nkey component of the workflow engine.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 11:34:35 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Uhrin", "Martin", ""], ["Huber", "Sebastiaan P.", ""]]}, {"id": "2005.07543", "submitter": "Sumeet Gajjar", "authors": "Sumeet Gajjar and Saurabh Vaidya", "title": "Elastic execution of checkpointed MPI applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MPI applications begin with a fixed number of rank and, by default, the rank\nremains constant throughout the application's lifetime. The developer can\nchoose to increase the rank by dynamically spawning MPI processes. However\ndoing this manually adds complexity to the MPI application. Making the MPI\napplications malleable \\cite{b20} would allow HPC applications to have the same\nelasticity as that of cloud applications. We propose multiple approaches to\nchange the rank of an MPI program agnostic to the modification of the user\ncode. We use checkpointing as a tool to achieve mutability of rank by halting\nthe execution and resuming the MPI program with a new state. In this paper, we\nfocus on the scenario of increasing the rank of an MPI program using ExaMPI as\nthe implementation for MPI.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 13:47:47 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Gajjar", "Sumeet", ""], ["Vaidya", "Saurabh", ""]]}, {"id": "2005.07595", "submitter": "Khan Mohammad Rashedun-Naby", "authors": "Khan Mohammad Rashedun-Naby", "title": "A Peer-to-Peer Distributed Secured Sustainable Large Scale Identity\n  Document Verification System With BitTorrent Network And Hash Function", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Verifying identity documents from a large Central Identity Database (CIDB) is\nalways challenging and it get more challenging when we need to verify a large\nnumber of documents at the same time. Usually most of the time we setup a\ngateway server connected to the CIDB and it serve all the identity document\nverification requests. Though it work well, there are still chances that this\nmodel will collapse in high traffic. We obviously can tune the system to be\nsustainable, but the process is economically expensive. In this paper we\npropose an economically cheaper way to verify ID documents with private\nBitTorrent network and hash function.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:44:24 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Rashedun-Naby", "Khan Mohammad", ""]]}, {"id": "2005.07596", "submitter": "Mansaf Alam Dr", "authors": "Mohammad Moazum Wani, Samiya Khan and Mansaf Alam", "title": "IoT Based Traffic Management System for Ambulances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lack of efficient traffic control can lead to the loss of thousands of lives\ndue to ambulance not being able to reach the hospital in time. Also, with the\ncurrent annual growth of Vehicles being around 11% while the annual road\nextension remaining around 4% in developing countries such as India, the\nproblem is further worsening. So, to deal with this problem the paper presents\na novel, easy toimplement alternative for traffic management during emergency\nsituations requiring only three main devices: Arduino UNO, GPS neo 6M and SIM\n900A.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:12:48 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Wani", "Mohammad Moazum", ""], ["Khan", "Samiya", ""], ["Alam", "Mansaf", ""]]}, {"id": "2005.07597", "submitter": "Srinivasa Rao Aravilli", "authors": "Srinivasa Rao Aravilli", "title": "BAHULAM: Distributed Data Analytics on Secure Enclaves", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This is a survey of some of the currently available frameworks\n(opensource/commercial) in order to run distributed data applications(Hadoop,\nSpark) on secure enclaves. Intel, AMD, Amazon support secure enclaves on their\nsystems Intel-SGX, AMD Memory Encryption, AWS Nitro Enclaves respectively.\nKeystone is an open-source framework for architecting Trusted Execution\nEnvironments and isolation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 05:48:40 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Aravilli", "Srinivasa Rao", ""]]}, {"id": "2005.07598", "submitter": "Nathan George", "authors": "Nathan George", "title": "Literature Review and Implementation Overview: High Performance\n  Computing with Graphics Processing Units for Classroom and Research Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, I discuss the history and current state of GPU HPC systems.\nAlthough high-power GPUs have only existed a short time, they have found rapid\nadoption in deep learning applications. I also discuss an implementation of a\ncommodity-hardware NVIDIA GPU HPC cluster for deep learning research and\nacademic teaching use.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 17:49:13 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["George", "Nathan", ""]]}, {"id": "2005.07600", "submitter": "Vignesh Sriniviasakumar S.", "authors": "Vignesh S., Muthumanikandan V., Siddarth S., Sainath G", "title": "An Alternative C++ based HPC system for Hadoop MapReduce", "comments": "8 pages, 13 figures, 4 authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce is a technique used to vastly improve distributed processing of\ndata and can massively speed up computation. Hadoop and its MapReduce relies on\nJVM and Java which is expensive on memory. High Performance Computing based\nMapReduce framework could be used that can perform more memory-efficiently and\nfaster than the standard MapReduce. This paper explores an entirely C++ based\napproach to the MapReduce and its feasibility on multiple factors like\ndeveloper friendliness, deployment interface, efficiency and scalability. This\npaper also introduces Delayed Reduction and deployment techniques that can\nspeed up MapReduce in a compiled environment.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:31:04 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 17:56:14 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["S.", "Vignesh", ""], ["V.", "Muthumanikandan", ""], ["S.", "Siddarth", ""], ["G", "Sainath", ""]]}, {"id": "2005.07619", "submitter": "Mahmoud Eljammaly", "authors": "Miquel Pericas, Oscar Palomar, Vassilis Papaefstathiou and Mahmoud\n  Eljammaly", "title": "Proceedings of the Thirteenth International Workshop on Programmability\n  and Architectures for Heterogeneous Multicores (MULTIPROG-2020)", "comments": "This volume contains 3 full papers and 3 position papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the 13th International Workshop on\nProgrammability and Architectures for Heterogeneous Multicores. The workshop\nwas held in conjunction with the 16th International Conference on\nHigh-Performance and Embedded Architectures and Compilers (HiPEAC) in Bologna,\nItaly on January 20th, 2020.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 13:49:27 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Pericas", "Miquel", ""], ["Palomar", "Oscar", ""], ["Papaefstathiou", "Vassilis", ""], ["Eljammaly", "Mahmoud", ""]]}, {"id": "2005.07637", "submitter": "Janne H. Korhonen", "authors": "Klaus-Tycho Foerster and Janne H. Korhonen and Ami Paz and Joel\n  Rybicki and Stefan Schmid", "title": "Input-Dynamic Distributed Algorithms for Communication Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3447384", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a distributed task where the communication network is fixed but the\nlocal inputs given to the nodes of the distributed system may change over time.\nIn this work, we explore the following question: if some of the local inputs\nchange, can an existing solution be updated efficiently, in a dynamic and\ndistributed manner?\n  To address this question, we define the batch dynamic CONGEST model in which\nwe are given a bandwidth-limited communication network and a dynamic edge\nlabelling defines the problem input. The task is to maintain a solution to a\ngraph problem on the labeled graph under batch changes. We investigate, when a\nbatch of $\\alpha$ edge label changes arrive,\n  -- how much time as a function of $\\alpha$ we need to update an existing\nsolution, and\n  -- how much information the nodes have to keep in local memory between\nbatches in order to update the solution quickly.\n  Our work lays the foundations for the theory of input-dynamic distributed\nnetwork algorithms. We give a general picture of the complexity landscape in\nthis model, design both universal algorithms and algorithms for concrete\nproblems, and present a general framework for lower bounds. In particular, we\nderive non-trivial upper bounds for two selected, contrasting problems:\nmaintaining a minimum spanning tree and detecting cliques.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 16:49:20 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 13:52:25 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 17:34:49 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Foerster", "Klaus-Tycho", ""], ["Korhonen", "Janne H.", ""], ["Paz", "Ami", ""], ["Rybicki", "Joel", ""], ["Schmid", "Stefan", ""]]}, {"id": "2005.07741", "submitter": "Dongfang Zhao", "authors": "Abdullah Al-Mamun and Dongfang Zhao", "title": "Trustworthy Edge Computing through Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing draws a lot of recent research interests because of the\nperformance improvement by offloading many workloads from the remote data\ncenter to nearby edge nodes. Nonetheless, one open challenge of this emerging\nparadigm lies in the potential security issues on edge nodes and end devices,\ne.g., sensors and controllers. This paper proposes a cooperative protocol,\nnamely DEAN, across edge nodes to prevent data manipulation, and to allow fair\ndata sharing with quick recovery under resource constraints of limited storage,\ncomputing, and network capacity. Specifically, DEAN leverages a parallel\nmechanism equipped with three independent core components, effectively\nachieving low resource consumption while allowing secured parallel block\nprocessing on edge nodes. We have implemented a system prototype based on DEAN\nand experimentally verified its effectiveness with a comparison with three\npopular blockchain implementations: Ethereum, Parity, and Hyperledger Fabric.\nExperimental results show that the system prototype exhibits high resilience to\narbitrary failures: the percentile of trusty nodes is much higher than the\nrequired 50\\% in most cases. Performance-wise, DEAN-based blockchain\nimplementation outperforms the state-of-the-art blockchain systems with up to\n$25\\times$ higher throughput and $18\\times$ lower latency on 1,000 nodes.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 18:57:08 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Al-Mamun", "Abdullah", ""], ["Zhao", "Dongfang", ""]]}, {"id": "2005.07761", "submitter": "Joel Rybicki", "authors": "Sebastian Brandt and Barbara Keller and Joel Rybicki and Jukka Suomela\n  and Jara Uitto", "title": "Efficient Load-Balancing through Distributed Token Dropping", "comments": "19 pages, 3 figures, revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new graph problem, the token dropping game, and we show how to\nsolve it efficiently in a distributed setting. We use the token dropping game\nas a tool to design an efficient distributed algorithm for stable orientations\nand more generally for locally optimal semi-matchings. The prior work by\nCzygrinow et al. (DISC 2012) finds a stable orientation in $O(\\Delta^5)$ rounds\nin graphs of maximum degree $\\Delta$, while we improve it to $O(\\Delta^4)$ and\nalso prove a lower bound of $\\Omega(\\Delta)$.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 19:41:02 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 17:45:16 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Brandt", "Sebastian", ""], ["Keller", "Barbara", ""], ["Rybicki", "Joel", ""], ["Suomela", "Jukka", ""], ["Uitto", "Jara", ""]]}, {"id": "2005.07776", "submitter": "Amir Sonee", "authors": "Amir Sonee and Stefano Rini", "title": "Efficient Federated Learning over Multiple Access Channel with\n  Differential Privacy Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of federated learning (FL) through digital\ncommunication between clients and a parameter server (PS) over a multiple\naccess channel (MAC), also subject to differential privacy (DP) constraints, is\nstudied. More precisely, we consider the setting in which clients in a\ncentralized network are prompted to train a machine learning model using their\nlocal datasets. The information exchange between the clients and the PS takes\nplaces over a MAC channel and must also preserve the DP of the local datasets.\nAccordingly, the objective of the clients is to minimize the training loss\nsubject to (i) rate constraints for reliable communication over the MAC and\n(ii) DP constraint over the local datasets. For this optimization scenario, we\nproposed a novel consensus scheme in which digital distributed stochastic\ngradient descent (D-DSGD) is performed by each client. To preserve DP, a\ndigital artificial noise is also added by the users to the locally quantized\ngradients. The performance of the scheme is evaluated in terms of the\nconvergence rate and DP level for a given MAC capacity. The performance is\noptimized over the choice of the quantization levels and the artificial noise\nparameters. Numerical evaluations are presented to validate the performance of\nthe proposed scheme.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 20:38:04 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 14:39:45 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sonee", "Amir", ""], ["Rini", "Stefano", ""]]}, {"id": "2005.07798", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far, Emina Soljanin and Roy D. Yates", "title": "Data Freshness in Leader-Based Replicated Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leader-based data replication improves consistency in highly available\ndistributed storage systems via sequential writes to the leader nodes. After a\nwrite has been committed by the leaders, follower nodes are written by a\nmulticast mechanism and are only guaranteed to be eventually consistent. With\nAge of Information (AoI) as the freshness metric, we characterize how the\nnumber of leaders affects the freshness of the data retrieved by an\ninstantaneous read query. In particular, we derive the average age of a read\nquery for a deterministic model for the leader writing time and a probabilistic\nmodel for the follower writing time. We obtain a closed-form expression for the\naverage age for exponentially distributed follower writing time. Our numerical\nresults show that, depending on the relative speed of the write operation to\nthe two groups of nodes, there exists an optimal number of leaders which\nminimizes the average age of the retrieved data, and that this number increases\nas the relative speed of writing on leaders increases.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 21:56:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""], ["Yates", "Roy D.", ""]]}, {"id": "2005.07859", "submitter": "Ali Pourmiri", "authors": "Ali Pourmiri, Bernard Mans", "title": "Tight Analysis of Asynchronous Rumor Spreading in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asynchronous rumor algorithm spreading propagates a piece of information,\nthe so-called rumor, in a network. Starting with a single informed node, each\nnode is associated with an exponential time clock with rate $1$ and calls a\nrandom neighbor in order to possibly exchange the rumor. Spread time is the\nfirst time when all nodes of a network are informed with high probability. We\nconsider spread time of the algorithm in any dynamic evolving network,\n$\\mathcal{G}=\\{G^{(t)}\\}_{t=0}^{\\infty}$, which is a sequence of graphs exposed\nat discrete time step $t=0,1\\ldots$. We observe that besides the expansion\nprofile of a dynamic network, the degree distribution of nodes over time effect\nthe spread time. We establish upper bounds for the spread time in terms of\ngraph conductance and diligence. For a given connected simple graph $G=(V,E)$,\nthe diligence of cut set $E(S, \\overline{S})$ is defined as\n$\\rho(S)=\\min_{\\{u,v\\}\\in E(S,\\overline{S})}\\max\\{\\bar{d}/d_u, \\bar{d}/d_v\\}$\nwhere $d_u$ is the degree of $u$ and $\\bar{d}$ is the average degree of nodes\nin the one side of the cut with smaller volume (i.e.,\n${\\mathtt{vol}}{(S)}=\\sum_{u\\in S}d_u$). The diligence of $G$ is also defined\nas $\\rho(G)=\\min_{ \\emptyset\\neq S\\subset V}\\rho(S)$. We show that the spread\ntime of the algorithm in $\\mathcal{G}$ is bounded by $T$, where $T$ is the\nfirst time that $\\sum_{t=0}^T\\Phi(G^{(t)})\\cdot\\rho(G^{(t)})$ exceeds $C\\log\nn$, where $\\Phi(G^{(t)})$ denotes the conductance of $G^{(t)}$ and $C$ is a\nspecified constant. We also define the absolute diligence as\n$\\overline{\\rho}(G)=\\min_{\\{u,v\\}\\in E}\\max\\{1/d_u,1/d_v\\}$ and establish upper\nbound $T$ for the spread time in terms of absolute diligence, which is the\nfirst time when $\\sum_{t=0}^T\\lceil\\Phi(G^{(t)})\\rceil\\cdot\n\\overline{\\rho}(G^{(t)})\\ge 2n$. We present dynamic networks where the given\nupper bounds are almost tight.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 03:42:37 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Pourmiri", "Ali", ""], ["Mans", "Bernard", ""]]}, {"id": "2005.07866", "submitter": "Deepesh Data", "authors": "Deepesh Data and Suhas Diggavi", "title": "Byzantine-Resilient SGD in High Dimensions on Heterogeneous Data", "comments": "57 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed stochastic gradient descent (SGD) in the master-worker\narchitecture under Byzantine attacks. We consider the heterogeneous data model,\nwhere different workers may have different local datasets, and we do not make\nany probabilistic assumptions on data generation. At the core of our algorithm,\nwe use the polynomial-time outlier-filtering procedure for robust mean\nestimation proposed by Steinhardt et al. (ITCS 2018) to filter-out corrupt\ngradients. In order to be able to apply their filtering procedure in our {\\em\nheterogeneous} data setting where workers compute {\\em stochastic} gradients,\nwe derive a new matrix concentration result, which may be of independent\ninterest.\n  We provide convergence analyses for smooth strongly-convex and non-convex\nobjectives. We derive our results under the bounded variance assumption on\nlocal stochastic gradients and a {\\em deterministic} condition on datasets,\nnamely, gradient dissimilarity; and for both these quantities, we provide\nconcrete bounds in the statistical heterogeneous data model. We give a\ntrade-off between the mini-batch size for stochastic gradients and the\napproximation error. Our algorithm can tolerate up to $\\frac{1}{4}$ fraction\nByzantine workers. It can find approximate optimal parameters in the\nstrongly-convex setting exponentially fast and reach to an approximate\nstationary point in the non-convex setting with a linear speed, thus, matching\nthe convergence rates of vanilla SGD in the Byzantine-free setting.\n  We also propose and analyze a Byzantine-resilient SGD algorithm with gradient\ncompression, where workers send $k$ random coordinates of their gradients.\nUnder mild conditions, we show a $\\frac{d}{k}$-factor saving in communication\nbits as well as decoding complexity over our compression-free algorithm without\naffecting its convergence rate (order-wise) and the approximation error.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 04:15:27 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Data", "Deepesh", ""], ["Diggavi", "Suhas", ""]]}, {"id": "2005.07890", "submitter": "Zonghao Huang", "authors": "Zonghao Huang and Yanmin Gong", "title": "Differentially Private ADMM for Convex Distributed Learning: Improved\n  Accuracy via Multi-Step Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating Direction Method of Multipliers (ADMM) is a popular algorithm for\ndistributed learning, where a network of nodes collaboratively solve a\nregularized empirical risk minimization by iterative local computation\nassociated with distributed data and iterate exchanges. When the training data\nis sensitive, the exchanged iterates will cause serious privacy concern. In\nthis paper, we aim to propose a new differentially private distributed ADMM\nalgorithm with improved accuracy for a wide range of convex learning problems.\nIn our proposed algorithm, we adopt the approximation of the objective function\nin the local computation to introduce calibrated noise into iterate updates\nrobustly, and allow multiple primal variable updates per node in each\niteration. Our theoretical results demonstrate that our approach can obtain\nhigher utility by such multiple approximate updates, and achieve the error\nbounds asymptotic to the state-of-art ones for differentially private empirical\nrisk minimization.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 07:17:31 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Huang", "Zonghao", ""], ["Gong", "Yanmin", ""]]}, {"id": "2005.07917", "submitter": "Giovanni Viglietta", "authors": "Giuseppe A. Di Luna, Ryuhei Uehara, Giovanni Viglietta, and Yukiko\n  Yamauchi", "title": "Gathering on a Circle with Limited Visibility by Anonymous Oblivious\n  Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A swarm of anonymous oblivious mobile robots, operating in deterministic\nLook-Compute-Move cycles, is confined within a circular track. All robots agree\non the clockwise direction (chirality), they are activated by an adversarial\nsemi-synchronous scheduler (SSYNCH), and an active robot always reaches the\ndestination point it computes (rigidity). Robots have limited visibility: each\nrobot can see only the points on the circle that have an angular distance\nstrictly smaller than a constant $\\vartheta$ from the robot's current location,\nwhere $0<\\vartheta\\leq\\pi$ (angles are expressed in radians).\n  We study the Gathering problem for such a swarm of robots: that is, all\nrobots are initially in distinct locations on the circle, and their task is to\nreach the same point on the circle in a finite number of turns, regardless of\nthe way they are activated by the scheduler. Note that, due to the anonymity of\nthe robots, this task is impossible if the initial configuration is\nrotationally symmetric; hence, we have to make the assumption that the initial\nconfiguration be rotationally asymmetric.\n  We prove that, if $\\vartheta=\\pi$ (i.e., each robot can see the entire circle\nexcept its antipodal point), there is a distributed algorithm that solves the\nGathering problem for swarms of any size. By contrast, we also prove that, if\n$\\vartheta\\leq \\pi/2$, no distributed algorithm solves the Gathering problem,\nregardless of the size of the swarm, even under the assumption that the initial\nconfiguration is rotationally asymmetric and the visibility graph of the robots\nis connected.\n  The latter impossibility result relies on a probabilistic technique based on\nrandom perturbations, which is novel in the context of anonymous mobile robots.\nSuch a technique is of independent interest, and immediately applies to other\nPattern-Formation problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 09:12:39 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Di Luna", "Giuseppe A.", ""], ["Uehara", "Ryuhei", ""], ["Viglietta", "Giovanni", ""], ["Yamauchi", "Yukiko", ""]]}, {"id": "2005.08098", "submitter": "Zhi-Gang Liu", "authors": "Zhi-Gang Liu, Paul N. Whatmough, Matthew Mattina", "title": "Systolic Tensor Array: An Efficient Structured-Sparse GEMM Accelerator\n  for Mobile CNN Inference", "comments": "Accepted by IEEE Computer Architecture Letters on 3/4/2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) inference on mobile devices demands\nefficient hardware acceleration of low-precision (INT8) general matrix\nmultiplication (GEMM). The systolic array (SA) is a pipelined 2D array of\nprocessing elements (PEs), with very efficient local data movement, well suited\nto accelerating GEMM, and widely deployed in industry. In this work, we\ndescribe two significant improvements to the traditional SA architecture, to\nspecifically optimize for CNN inference. Firstly, we generalize the traditional\nscalar PE, into a Tensor-PE, which gives rise to a family of new Systolic\nTensor Array (STA) microarchitectures. The STA family increases intra-PE\noperand reuse and datapath efficiency, resulting in circuit area and power\ndissipation reduction of as much as 2.08x and 1.36x respectively, compared to\nthe conventional SA at iso-throughput with INT8 operands. Secondly, we extend\nthis design to support a novel block-sparse data format called density-bound\nblock (DBB). This variant (STA-DBB) achieves a 3.14x and 1.97x improvement over\nthe SA baseline at iso-throughput in area and power respectively, when\nprocessing specially-trained DBB-sparse models, while remaining fully backwards\ncompatible with dense models.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:47:56 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Liu", "Zhi-Gang", ""], ["Whatmough", "Paul N.", ""], ["Mattina", "Matthew", ""]]}, {"id": "2005.08466", "submitter": "Yao Chen", "authors": "Yao Chen, Xin Long, Jiong He, Yuhang Chen, Hongshi Tan, Zhenxiang\n  Zhang, Marianne Winslett, Deming Chen", "title": "HaoCL: Harnessing Large-scale Heterogeneous Processors Made Easy", "comments": "Accepted by ICDCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive adoption of Deep Learning (DL) and Graph Processing (GP) makes\nit a de facto requirement to build large-scale clusters of heterogeneous\naccelerators including GPUs and FPGAs. The OpenCL programming framework can be\nused on the individual nodes of such clusters but is not intended for\ndeployment in a distributed manner. Fortunately, the original OpenCL semantics\nnaturally fit into the programming environment of heterogeneous clusters. In\nthis paper, we propose a heterogeneity-aware OpenCL-like (HaoCL) programming\nframework to facilitate the programming of a wide range of scientific\napplications including DL and GP workloads on large-scale heterogeneous\nclusters. With HaoCL, existing applications can be directly deployed on\nheterogeneous clusters without any modifications to the original OpenCL source\ncode and without awareness of the underlying hardware topologies and\nconfigurations. Our experiments show that HaoCL imposes a negligible overhead\nin a distributed environment, and provides near-linear speedups on standard\nbenchmarks when computation or data size exceeds the capacity of a single node.\nThe system design and the evaluations are presented in this demo paper.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 05:52:39 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chen", "Yao", ""], ["Long", "Xin", ""], ["He", "Jiong", ""], ["Chen", "Yuhang", ""], ["Tan", "Hongshi", ""], ["Zhang", "Zhenxiang", ""], ["Winslett", "Marianne", ""], ["Chen", "Deming", ""]]}, {"id": "2005.08536", "submitter": "Alex Pellegrini", "authors": "Alex Pellegrini and Luca Zanolini", "title": "An Algebraic Model For Quorum Systems", "comments": "15 pages, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quorum systems are a key mathematical abstraction in distributed\nfault-tolerant computing for capturing trust assumptions. A quorum system is a\ncollection of subsets of all processes, called quorums, with the property that\neach pair of quorums have a non-empty intersection. They can be found at the\ncore of many reliable distributed systems, such as cloud computing platforms,\ndistributed storage systems and blockchains. In this paper we give a new\ninterpretation of quorum systems, starting with classical majority-based quorum\nsystems and extending this to Byzantine quorum systems. We propose an algebraic\nrepresentation of the theory underlying quorum systems making use of\nmultivariate polynomial ideals, incorporating properties of these systems, and\nstudying their algebraic varieties. To achieve this goal we will exploit\nproperties of Boolean Groebner bases. The nice nature of Boolean Groebner bases\nallows us to avoid part of the combinatorial computations required to check\nconsistency and availability of quorum systems. Our results provide a novel\napproach to test quorum systems properties from both algebraic and algorithmic\nperspectives.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:48:41 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 08:00:40 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Pellegrini", "Alex", ""], ["Zanolini", "Luca", ""]]}, {"id": "2005.08708", "submitter": "Istv\\'an Koren", "authors": "Dominik Adam Kus, Istv\\'an Koren, Ralf Klamma", "title": "A Link Generator for Increasing the Utility of OpenAPI-to-GraphQL\n  Translations", "comments": "WWW2020 Developer Track", "journal-ref": null, "doi": "10.13140/RG.2.2.33982.92488", "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standardized interfaces are the connecting link of today's distributed\nsystems, facilitating access to data services in the cloud. REST APIs have been\nprevalent over the last years, despite several issues like over- and\nunderfetching of resources. GraphQL enjoys rapid adoption, resolving these\nproblems by using statically typed queries. However, the redevelopment of\nservices to the new paradigm is costly. Therefore, several approaches for the\nsuccessive migration from REST to GraphQL have been proposed, many leveraging\nOpenAPI service descriptions. In this article, we present the findings of our\nempirical evaluation on the APIs.guru directory and identify several schema\ntranslation challenges. These include less expressive schema types in GraphQL,\nas well as missing meta information about related resources in OpenAPI. To this\nend, we developed the open source Link Generator, that analyzes OpenAPI\ndocuments and automatically adds links to increase translation utility. This\nfundamentally benefits around 34% of APIs in the APIs.guru directory. Our\nfindings and tool support contribute to the ongoing discussion about the\nmigration of REST APIs to GraphQL, and provide developers with valuable\ninsights into common pitfalls, to reduce friction during API transformation.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 13:35:22 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kus", "Dominik Adam", ""], ["Koren", "Istv\u00e1n", ""], ["Klamma", "Ralf", ""]]}, {"id": "2005.08739", "submitter": "Andriy Miranskyy", "authors": "Mohammad Saiful Islam and Andriy Miranskyy", "title": "Anomaly Detection in Cloud Components", "comments": "Accepted for publication in Proceedings of the IEEE International\n  Conference on Cloud Computing (CLOUD 2020). Fix dataset description", "journal-ref": null, "doi": "10.1109/CLOUD49709.2020.00008", "report-no": null, "categories": "cs.SE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud platforms, under the hood, consist of a complex inter-connected stack\nof hardware and software components. Each of these components can fail which\nmay lead to an outage. Our goal is to improve the quality of Cloud services\nthrough early detection of such failures by analyzing resource utilization\nmetrics. We tested Gated-Recurrent-Unit-based autoencoder with a likelihood\nfunction to detect anomalies in various multi-dimensional time series and\nachieved high performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:06:38 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 14:34:41 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Islam", "Mohammad Saiful", ""], ["Miranskyy", "Andriy", ""]]}, {"id": "2005.08795", "submitter": "Luca Zanolini", "authors": "Christian Cachin, Luca Zanolini", "title": "From Symmetric to Asymmetric Asynchronous Byzantine Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Consensus is arguably one of the most important notions in distributed\ncomputing. Among asynchronous, randomized, and signature-free implementations,\nthe protocols of Most\\'efaoui et al. (PODC 2014 and JACM 2015) represent a\nlandmark result, which has been extended later and taken up in practical\nsystems. The protocols achieve optimal resilience and takes, in expectation,\nonly a constant expected number of rounds of quadratic message complexity.\nRandomization is provided through a common-coin primitive. In traditional\nconsensus protocols, all involved processes adhere to a global, symmetric\nfailure model, typically only defined by bounds on the number of faulty\nprocesses. Motivated by applications to blockchains, however, more flexible\ntrust assumptions have recently been considered. In particular, with asymmetric\ntrust, a process is free to choose which other processes it trusts and which\nones might collude against it. This paper revisits the optimal asynchronous\nprotocol of Most\\'efaoui et al. and shows how to realize it with asymmetric\ntrust. The paper starts by pointing out in detail why some versions of this\nprotocol may violate liveness. Then it proposes a fix for the protocol that\ndoes not affect its properties, but lets it regain the simplicity of its\noriginal version (PODC 2014). At the same time, the paper shows how to realize\nrandomized signature-free asynchronous Byzantine consensus with asymmetric\nquorums. This results in an optimal consensus protocol with subjective,\nasymmetric trust and constant expected running time. It is suitable for\napplications to blockchains, for instance.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:16:56 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 15:16:49 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 15:59:19 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Cachin", "Christian", ""], ["Zanolini", "Luca", ""]]}, {"id": "2005.08854", "submitter": "Waheed Bajwa", "authors": "Matthew Nokleby, Haroon Raja, and Waheed U. Bajwa", "title": "Scaling-up Distributed Processing of Data Streams for Machine Learning", "comments": "45 pages, 9 figures; preprint of a journal paper published in\n  Proceedings of the IEEE (Special Issue on Optimization for Data-driven\n  Learning and Control)", "journal-ref": "Proc. of the IEEE, vol. 108, no. 11, pp. 1984-2012, Nov. 2020", "doi": "10.1109/JPROC.2020.3021381", "report-no": null, "categories": "cs.LG cs.DC eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging applications of machine learning in numerous areas involve\ncontinuous gathering of and learning from streams of data. Real-time\nincorporation of streaming data into the learned models is essential for\nimproved inference in these applications. Further, these applications often\ninvolve data that are either inherently gathered at geographically distributed\nentities or that are intentionally distributed across multiple machines for\nmemory, computational, and/or privacy reasons. Training of models in this\ndistributed, streaming setting requires solving stochastic optimization\nproblems in a collaborative manner over communication links between the\nphysical entities. When the streaming data rate is high compared to the\nprocessing capabilities of compute nodes and/or the rate of the communications\nlinks, this poses a challenging question: how can one best leverage the\nincoming data for distributed training under constraints on computing\ncapabilities and/or communications rate? A large body of research has emerged\nin recent decades to tackle this and related problems. This paper reviews\nrecently developed methods that focus on large-scale distributed stochastic\noptimization in the compute- and bandwidth-limited regime, with an emphasis on\nconvergence analysis that explicitly accounts for the mismatch between\ncomputation, communication and streaming rates. In particular, it focuses on\nmethods that solve: (i) distributed stochastic convex problems, and (ii)\ndistributed principal component analysis, which is a nonconvex problem with\ngeometric structure that permits global convergence. For such methods, the\npaper discusses recent advances in terms of distributed algorithmic designs\nwhen faced with high-rate streaming data. Further, it reviews guarantees\nunderlying these methods, which show there exist regimes in which systems can\nlearn from distributed, streaming data at order-optimal rates.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:28:54 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 23:48:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Nokleby", "Matthew", ""], ["Raja", "Haroon", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2005.08942", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Which scaling rule applies to Artificial Neural Networks", "comments": "14 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although an ANN is a biology-mimicking system, it is built from components\ndesigned/fabricated for use in conventional computing, and it is created by\nexperts trained in conventional computing; all of them are using the classic\ncomputing paradigm. As von Neumann in his classic \"First Draft\" warned, because\nthe data transfer time is neglected in the model he used, using a \"too fast\nprocessor\" vitiates the procedure, furthermore, that using his paradigm for\nimitating neuronal operations, is unsound. This means that at least doubly\nunsound to apply his paradigm to describe scaling ANNs. The common experience\nshows that making actively cooperating and communicating computing systems,\nusing segregated single processors, has severe performance limitations; which\nfact cannot be explained using his classic paradigm. The achievable payload\ncomputing performance of those systems sensitively depends on their workload\ntype, and this effect is only poorly known. The type of the workload that the\nAI-based systems generate, leads to an exceptionally low payload computational\nperformance. Unfortunately, the initial successes of demo systems that comprise\nonly a few \"neurons\" and solve simple tasks are misleading: the scaling of\nprocessor-based ANN systems is strongly nonlinear. The paper discusses some\nmajor limiting factors that affect their performance. It points out that for\nbuilding biology-mimicking large systems, it is inevitable to perform drastic\nchanges in the present computing paradigm. Namely, instead of neglecting the\ntransfer time, a proper method to consider it shall be developed. The temporal\nbehavior enables us to comprehend the technical implementation of computing\ncomponents and architectures.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 19:52:55 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 01:02:45 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 18:38:09 GMT"}, {"version": "v4", "created": "Sat, 11 Jul 2020 15:53:02 GMT"}, {"version": "v5", "created": "Sun, 26 Jul 2020 18:22:44 GMT"}, {"version": "v6", "created": "Sat, 26 Sep 2020 18:40:40 GMT"}, {"version": "v7", "created": "Sun, 18 Oct 2020 08:38:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "2005.09140", "submitter": "Mina Zaminkar", "authors": "Mina Zaminkar and Reza Fotohi", "title": "SoS-RPL: Securing Internet of Things Against Sinkhole Attack Using RPL\n  Protocol-Based Node Rating and Ranking Mechanism", "comments": "26 pages, 11 figures, 10 tables, Wireless Pers Commun (2020)", "journal-ref": null, "doi": "10.1007/s11277-020-07421-z", "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the Internet of Things (IoT) the internet scope is established by the\nintegration of physical things to classify themselves into mutual things. A\nphysical thing can be created by this inventive perception to signify itself in\nthe digital world. Regarding the physical things that are related to the\ninternet, it is worth noting that considering numerous theories and upcoming\npredictions, they mostly require protected structures, moreover, they are at\nrisk of several attacks. IoTs are endowed with particular routing disobedience\ncalled sinkhole attack owing to their distributed features. In these attacks, a\nmalicious node broadcasts illusive information regarding the routings to impose\nitself as a route towards specific nodes for the neighboring nodes and thus,\nattract data traffic. RPL (IP-V6 routing protocol for efficient and low-energy\nnetworks) is a standard routing protocol which is mainly employed in sensor\nnetworks and IoT. This protocol is called SoS-RPL consisting of two key\nsections of the sinkhole detection. In the first section rating and ranking the\nnodes in the RPL is carried out based on distance measurements. The second\nsection is in charge of discovering the misbehavior sources within the IoT\nnetwork through, the Average Packet Transmission RREQ (APT-RREQ). Here, the\ntechnique is assessed through wide simulations performed within the NS-3\nenvironment. Based on the results of the simulation, it is indicated that the\nIoT network behavior metrics are enhanced based on the detection rate,\nfalse-negative rate, false-positive rate, packet delivery rate, maximum\nthroughput, and packet loss rate.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 09:26:09 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zaminkar", "Mina", ""], ["Fotohi", "Reza", ""]]}, {"id": "2005.09148", "submitter": "Rong Ou", "authors": "Rong Ou", "title": "Out-of-Core GPU Gradient Boosting", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU-based algorithms have greatly accelerated many machine learning methods;\nhowever, GPU memory is typically smaller than main memory, limiting the size of\ntraining data. In this paper, we describe an out-of-core GPU gradient boosting\nalgorithm implemented in the XGBoost library. We show that much larger datasets\ncan fit on a given GPU, without degrading model accuracy or training time. To\nthe best of our knowledge, this is the first out-of-core GPU implementation of\ngradient boosting. Similar approaches can be applied to other machine learning\nalgorithms\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 00:41:00 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ou", "Rong", ""]]}, {"id": "2005.09521", "submitter": "Konrad von Kirchbach", "authors": "Sascha Hunold, Konrad von Kirchbach, Markus Lehr, Christian Schulz,\n  Jesper Larsson Tr\\\"aff", "title": "Efficient Process-to-Node Mapping Algorithms for Stencil Computations", "comments": "18 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good process-to-compute-node mappings can be decisive for well performing HPC\napplications. A special, important class of process-to-node mapping problems is\nthe problem of mapping processes that communicate in a sparse stencil pattern\nto Cartesian grids. By thoroughly exploiting the inherently present structure\nin this type of problem, we devise three novel distributed algorithms that are\nable to handle arbitrary stencil communication patterns effectively. We analyze\nthe expected performance of our algorithms based on an abstract model of inter-\nand intra-node communication. An extensive experimental evaluation on several\nHPC machines shows that our algorithms are up to two orders of magnitude faster\nin running time than a (sequential) high-quality general graph mapping tool,\nwhile obtaining similar results in communication performance. Furthermore, our\nalgorithms also achieve significantly better mapping quality compared to\nprevious state-of-the-art Cartesian grid mapping algorithms. This results in up\nto a threefold performance improvement of an MPI_Neighbor_alltoall exchange\noperation. Our new algorithms can be used to implement the MPI_Cart_create\nfunctionality.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:27:39 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 08:55:00 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Hunold", "Sascha", ""], ["von Kirchbach", "Konrad", ""], ["Lehr", "Markus", ""], ["Schulz", "Christian", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "2005.09571", "submitter": "Naser Hossein Motlagh", "authors": "Huber Flores and Naser Hossein Motlagh and Agustin Zuniga and Mohan\n  Liyanage and Monica Passananti and Sasu Tarkoma and Moustafa Youssef and\n  Petteri Nurmi", "title": "Toward Large-Scale Autonomous Monitoring and Sensing of Underwater\n  Pollutants", "comments": "10 pages, 4 figures, 15 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marine pollution is a growing worldwide concern, affecting health of marine\necosystems, human health, climate change, and weather patterns. To reduce\nunderwater pollution, it is critical to have access to accurate information\nabout the extent of marine pollutants as otherwise appropriate countermeasures\nand cleaning measures cannot be chosen. Currently such information is difficult\nto acquire as existing monitoring solutions are highly laborious or costly,\nlimited to specific pollutants, and have limited spatial and temporal\nresolution. In this article, we present a research vision of large-scale\nautonomous marine pollution monitoring that uses coordinated groups of\nautonomous underwater vehicles (AUV)s to monitor extent and characteristics of\nmarine pollutants. We highlight key requirements and reference technologies to\nestablish a research roadmap for realizing this vision. We also address the\nfeasibility of our vision, carrying out controlled experiments that address\nclassification of pollutants and collaborative underwater processing, two key\nresearch challenges for our vision.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 20:12:55 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Flores", "Huber", ""], ["Motlagh", "Naser Hossein", ""], ["Zuniga", "Agustin", ""], ["Liyanage", "Mohan", ""], ["Passananti", "Monica", ""], ["Tarkoma", "Sasu", ""], ["Youssef", "Moustafa", ""], ["Nurmi", "Petteri", ""]]}, {"id": "2005.09588", "submitter": "Udit Agarwal", "authors": "Udit Agarwal, Vijaya Ramachandran", "title": "Faster Deterministic All Pairs Shortest Paths in Congest Model", "comments": "An extended abstract of this paper will appear in the Proceedings of\n  the ACM Symposium on Parallel Algorithms and Architectures (SPAA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deterministic algorithm for distributed weighted all pairs\nshortest paths (APSP) in both undirected and directed graphs. Our algorithm\nruns in $\\tilde{O}(n^{4/3})$ rounds in the Congest models on graphs with\narbitrary edge weights, and it improves on the previous $\\tilde{O}(n^{3/2})$\nbound of Agarwal et al. [ARKP18]. The main components of our new algorithm are\na new faster technique for constructing blocker set deterministically and a new\npipelined method for deterministically propagating distance values from source\nnodes to the blocker set nodes in the network. Both of these techniques have\npotential applications to other distributed algorithms.\n  Our new deterministic algorithm for computing blocker set adapts the NC\napproximate hypergraph set cover algorithm in [BRS94] to the distributed\nconstruction of a blocker set. It follows the two-step process of first\ndesigning a randomized algorithm that uses only pairwise independence, and then\nderandomizes this algorithm using a sample space of linear size. This algorithm\nruns in almost the same number of rounds as the initial step in our APSP\nalgorithm that computes $h$-hops shortest paths, and significantly improves on\nthe deterministic blocker set algorithms in [ARKP18, AR19] by removing an\nadditional $n\\cdot |Q|$ term in the round bound, where Q is the blocker set.\n  The other new component in our APSP algorithm is a deterministic pipelined\napproach to propagate distance values from source nodes to blocker nodes. We\nuse a simple natural round-robin method for this step, and we show using a\nsuitable progress measure that it achieve the $\\tilde{O}(n^{4/3})$ bound on the\nnumber of rounds. It appears that the standard deterministic methods for\nefficiently broadcasting multiple values, and for sending or receiving messages\nusing the routing schedule in [HPDG+19,LSP19] do not apply to this setting.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:09:41 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "2005.09610", "submitter": "Ranvir Rana", "authors": "Ranvir Rana, Sreeram Kannan, David Tse, Pramod Viswanath", "title": "Free2Shard: Adaptive-adversary-resistant sharding via Dynamic Self\n  Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.GT cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Propelled by the growth of large-scale blockchain deployments, much recent\nprogress has been made in designing sharding protocols that achieve throughput\nscaling linearly in the number of nodes. However, existing protocols are not\nrobust to an adversary adaptively corrupting a fixed fraction of nodes. In this\npaper, we propose Free2Shard -- a new architecture that achieves near-linear\nscaling while being secure against a fully adaptive adversary.\n  The focal point of this architecture is a dynamic self-allocation algorithm\nthat lets users allocate themselves to shards in response to adversarial\naction, without requiring a central or cryptographic proof. This architecture\nhas several attractive features unusual for sharding protocols, including: (a)\nthe ability to handle the regime of large number of shards (relative to the\nnumber of nodes); (b) heterogeneous shard demands; (c) requiring only a small\nminority to follow the self-allocation; (d) asynchronous shard rotation; (e)\noperation in a purely identity-free proof-of-work setting. The key technical\ncontribution is a deep mathematical connection to the classical work of\nBlackwell in dynamic game theory.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:31:39 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Rana", "Ranvir", ""], ["Kannan", "Sreeram", ""], ["Tse", "David", ""], ["Viswanath", "Pramod", ""]]}, {"id": "2005.09942", "submitter": "Andr\\'e R. Brodtkorb", "authors": "Andr\\'e R. Brodtkorb, Anna Benedictow, Heiko Klein, Arve Kylling,\n  Agnes Nyiri, Alvaro Valdebenito, Espen Sollum", "title": "Estimating volcanic ash emissions using retrieved satellite ash columns\n  and inverse ash transport modelling", "comments": "17 pages, 11 figures. First public draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the inversion procedure being used operationally at the\nNorwegian Meteorological Institute for estimating ash emission rates from\nretrieved satellite ash column amounts and a priori knowledge.\n  The overall procedure consists of five stages:\n  (1) generate a priori emission estimates;\n  (2) run forward simulations with unit emissions;\n  (3) collocate/match observations with emission simulations;\n  (4) build system of linear equations; and\n  (5) solve overdetermined system.\n  We go through the mathematical foundations for the inversion procedure,\nperformance for synthetic cases, and performance for real-world cases. The\nnovelties of this paper includes pruning of the linear system of equations used\nin the inversion and inclusion of observations of ash cloud top altitude.\n  The source code used in this work is freely available under an open source\nlicense, and is possible to use for other similar applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 10:00:37 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Brodtkorb", "Andr\u00e9 R.", ""], ["Benedictow", "Anna", ""], ["Klein", "Heiko", ""], ["Kylling", "Arve", ""], ["Nyiri", "Agnes", ""], ["Valdebenito", "Alvaro", ""], ["Sollum", "Espen", ""]]}, {"id": "2005.09944", "submitter": "Yuichi Sudo", "authors": "Yuichi Sudo, Ryota Eguchi, Taisuke Izumi, Toshimitsu Masuzawa", "title": "Time-optimal Loosely-stabilizing Leader Election in Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the leader election problem in population protocol models. In\npragmatic settings of population protocols, self-stabilization is a highly\ndesired feature owing to its fault resilience and the benefit of initialization\nfreedom. However, the design of self-stabilizing leader election is possible\nonly under a strong assumption (i.e. the knowledge of the \\emph{exact} size of\na network) and rich computational resources (i.e. the number of states).\nLoose-stabilization, introduced by Sudo et al [Theoretical Computer Science,\n2012], is a promising relaxed concept of self-stabilization to address the\naforementioned issue. Loose-stabilization guarantees that starting from any\nconfiguration, the network will reach a safe configuration where a single\nleader exists within a short time, and thereafter it will maintain the single\nleader for a long time, but not forever. The main contribution of the paper is\na time-optimal loosely-stabilizing leader election protocol. While the shortest\nconvergence time achieved so far in loosely-stabilizing leader election is\n$O(\\log^3 n)$ parallel time, the proposed protocol with design parameter $\\tau\n\\ge 1$ attains $O(\\tau \\log n)$ parallel convergence time and\n$\\Omega(n^{\\tau})$ parallel holding time (i.e. the length of the period keeping\nthe unique leader), both in expectation. This protocol is time-optimal in the\nsense of both the convergence and holding times in expectation because any\nloosely-stabilizing leader election protocol with the same length of the\nholding time is known to require $\\Omega(\\tau \\log n)$ parallel time.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 10:02:33 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Sudo", "Yuichi", ""], ["Eguchi", "Ryota", ""], ["Izumi", "Taisuke", ""], ["Masuzawa", "Toshimitsu", ""]]}, {"id": "2005.10103", "submitter": "Hao Xu", "authors": "Hao Xu, Lei Zhang, Oluwakayode Onireti, Yang Fang, William Bill\n  Buchanan, Muhammad Ali Imran", "title": "BeepTrace: Blockchain-enabled Privacy-preserving Contact Tracing for\n  COVID-19 Pandemic and Beyond", "comments": null, "journal-ref": null, "doi": "10.1109/JIOT.2020.3025953", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The outbreak of COVID-19 pandemic has exposed an urgent need for effective\ncontact tracing solutions through mobile phone applications to prevent the\ninfection from spreading further. However, due to the nature of contact\ntracing, public concern on privacy issues has been a bottleneck to the existing\nsolutions, which is significantly affecting the uptake of contact tracing\napplications across the globe. In this paper, we present a blockchain-enabled\nprivacy-preserving contact tracing scheme: BeepTrace, where we propose to adopt\nblockchain bridging the user/patient and the authorized solvers to desensitize\nthe user ID and location information. Compared with recently proposed contract\ntracing solutions, our approach shows higher security and privacy with the\nadditional advantages of being battery friendly and globally accessible.\nResults show viability in terms of the required resource at both server and\nmobile phone perspectives. Through breaking the privacy concerns of the public,\nthe proposed BeepTrace solution can provide a timely framework for authorities,\ncompanies, software developers and researchers to fast develop and deploy\neffective digital contact tracing applications, to conquer COVID-19 pandemic\nsoon. Meanwhile, the open initiative of BeepTrace allows worldwide\ncollaborations, integrate existing tracing and positioning solutions with the\nhelp of blockchain technology.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:04:43 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 14:00:26 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Xu", "Hao", ""], ["Zhang", "Lei", ""], ["Onireti", "Oluwakayode", ""], ["Fang", "Yang", ""], ["Buchanan", "William Bill", ""], ["Imran", "Muhammad Ali", ""]]}, {"id": "2005.10141", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern and Xavier Vilaca", "title": "Rational Consensus", "comments": "Appears in Proceedings of the 35th Annual ACM Symposium on Principles\n  of Distributed Computing, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a game-theoretic analysis of consensus, assuming that processes\nare controlled by rational agents and may fail by crashing. We consider agents\nthat \\emph{care only about consensus}: that is, (a) an agent's utility depends\nonly on the consensus value achieved (and not, for example, on the number of\nmessages the agent sends) and (b) agents strictly prefer reaching consensus to\nnot reaching consensus. We show that, under these assumptions, there is no\n\\emph{ex post Nash Equilibrium}, even with only one failure. Roughly speaking,\nthis means that there must always exist a \\emph{failure pattern} (a description\nof who fails, when they fail, and which agents they do not send messages to in\nthe round that they fail) and initial preferences for which an agent can gain\nby deviating. On the other hand, if we assume that there is a distribution\n$\\pi$ on the failure patterns and initial preferences, then under minimal\nassumptions on $\\pi$, there is a Nash equilibrium that tolerates $f$ failures\n(i.e., $\\pi$ puts probability 1 on there being at most $f$ failures) if $f+1 <\nn$ (where $n$ is the total number of agents). Moreover, we show that a slight\nextension of the Nash equilibrium strategy is also a \\emph{sequential}\nequilibrium (under the same assumptions about the distribution $\\pi$).\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:39:55 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Vilaca", "Xavier", ""]]}, {"id": "2005.10413", "submitter": "Jonas H. M\\\"uller Kornd\\\"orfer", "authors": "Jonas H. M\\\"uller Kornd\\\"orfer and Mario Bielert and La\\'ercio L.\n  Pilla and Florina M. Ciorba", "title": "Mapping Matters: Application Process Mapping on 3-D Processor Topologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications' performance is influenced by the mapping of processes to\ncomputing nodes, the frequency and volume of exchanges among processing\nelements, the network capacity, and the routing protocol. A poor mapping of\napplication processes degrades performance and wastes resources. Process\nmapping is frequently ignored as an explicit optimization step since the system\ntypically offers a default mapping, users may lack awareness of their\napplications' communication behavior, and the opportunities for improving\nperformance through mapping are often unclear. This work studies the impact of\napplication process mapping on several processor topologies. We propose a\nworkflow that renders mapping as an explicit optimization step for parallel\napplications. We apply the workflow to a set of four applications, twelve\nmapping algorithms, and three direct network topologies. We assess the\nmappings' quality in terms of volume, frequency, and distance of exchanges\nusing metrics such as dilation (measured in hop$\\cdot$Byte). With a parallel\ntrace-based simulator, we predict the applications' execution on the three\ntopologies using the twelve mappings. We evaluate the impact of process mapping\non the applications' simulated performance in terms of execution and\ncommunication times and identify the mappings that achieve the highest\nperformance in both cases. To ensure the correctness of the simulations, we\ncompare the pre- and post-simulation results. This work emphasizes the\nimportance of process mapping as an explicit optimization step and offers a\nsolution for parallel applications to exploit the full potential of the\nallocated resources on a given system.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 01:28:40 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 23:24:30 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 14:28:33 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kornd\u00f6rfer", "Jonas H. M\u00fcller", ""], ["Bielert", "Mario", ""], ["Pilla", "La\u00e9rcio L.", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "2005.10435", "submitter": "HaiYing Wang", "authors": "Jun Yu, HaiYing Wang, Mingyao Ai and Huiming Zhang", "title": "Optimal Distributed Subsampling for Maximum Quasi-Likelihood Estimators\n  with Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonuniform subsampling methods are effective to reduce computational burden\nand maintain estimation efficiency for massive data. Existing methods mostly\nfocus on subsampling with replacement due to its high computational efficiency.\nIf the data volume is so large that nonuniform subsampling probabilities cannot\nbe calculated all at once, then subsampling with replacement is infeasible to\nimplement. This paper solves this problem using Poisson subsampling. We first\nderive optimal Poisson subsampling probabilities in the context of\nquasi-likelihood estimation under the A- and L-optimality criteria. For a\npractically implementable algorithm with approximated optimal subsampling\nprobabilities, we establish the consistency and asymptotic normality of the\nresultant estimators. To deal with the situation that the full data are stored\nin different blocks or at multiple locations, we develop a distributed\nsubsampling framework, in which statistics are computed simultaneously on\nsmaller partitions of the full data. Asymptotic properties of the resultant\naggregated estimator are investigated. We illustrate and evaluate the proposed\nstrategies through numerical experiments on simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:46:56 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 13:45:32 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 15:32:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yu", "Jun", ""], ["Wang", "HaiYing", ""], ["Ai", "Mingyao", ""], ["Zhang", "Huiming", ""]]}, {"id": "2005.10445", "submitter": "Sivan Toledo", "authors": "Yaniv Rubinpur and Sivan Toledo", "title": "Signal Processing for a Reverse-GPS Wildlife Tracking System: CPU and\n  GPU Implementation Experiences", "comments": "Revised (very slightly, changed a few sentences describing Figure 7)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present robust high-performance implementations of signal-processing tasks\nperformed by a high-throughput wildlife tracking system called ATLAS. The\nsystem tracks radio transmitters attached to wild animals by estimating the\ntime of arrival of radio packets to multiple receivers (base stations).\nTime-of-arrival estimation of wideband radio signals is computationally\nexpensive, especially in acquisition mode (when the time of transmission is not\nknown, not even approximately). These computations are a bottleneck that limits\nthe throughput of the system. We developed a sequential high-performance CPU\nimplementation of the computations a few years back, and more recencely a GPU\nimplementation. Both strive to balance performance with simplicity,\nmaintainability, and development effort, as most real-world codes do. The paper\nreports on the two implementations and carefully evaluates their performance.\nThe evaluations indicates that the GPU implementation dramatically improves\nperformance and power-performance relative to the sequential CPU implementation\nrunning on a desktop CPU typical of the computers in current base stations.\nPerformance improves by more than 50X on a high-end GPU and more than 4X with a\nGPU platform that consumes almost 5 times less power than the CPU platform.\nPerformance-per-Watt ratios also improve (by more than 16X), and so do the\nprice-performance ratios.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 03:28:52 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 08:37:40 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 07:15:52 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Rubinpur", "Yaniv", ""], ["Toledo", "Sivan", ""]]}, {"id": "2005.10460", "submitter": "Md. Redowan Mahmud", "authors": "Redowan Mahmud, Kotagiri Ramamohanarao and Rajkumar Buyya", "title": "Application Management in Fog Computing Environments: A Taxonomy, Review\n  and Future Directions", "comments": null, "journal-ref": "ACM Computing Surveys, 2020", "doi": "10.1145/3403955", "report-no": "Volume: 53, Issue: 4", "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) paradigm is being rapidly adopted for the\ncreation of smart environments in various domains. The IoT-enabled\nCyber-Physical Systems (CPSs) associated with smart city, healthcare, Industry\n4.0 and Agtech handle a huge volume of data and require data processing\nservices from different types of applications in real-time. The Cloud-centric\nexecution of IoT applications barely meets such requirements as the Cloud\ndatacentres reside at a multi-hop distance from the IoT devices. \\textit{Fog\ncomputing}, an extension of Cloud at the edge network, can execute these\napplications closer to data sources. Thus, Fog computing can improve\napplication service delivery time and resist network congestion. However, the\nFog nodes are highly distributed, heterogeneous and most of them are\nconstrained in resources and spatial sharing. Therefore, efficient management\nof applications is necessary to fully exploit the capabilities of Fog nodes. In\nthis work, we investigate the existing application management strategies in Fog\ncomputing and review them in terms of architecture, placement and maintenance.\nAdditionally, we propose a comprehensive taxonomy and highlight the research\ngaps in Fog-based application management. We also discuss a perspective model\nand provide future research directions for further improvement of application\nmanagement in Fog computing.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 04:43:44 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mahmud", "Redowan", ""], ["Ramamohanarao", "Kotagiri", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2005.10566", "submitter": "Ce Jin", "authors": "Mohsen Ghaffari, Ce Jin, Daan Nilis", "title": "A Massively Parallel Algorithm for Minimum Weight Vertex Cover", "comments": "To appear in SPAA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a massively parallel algorithm, with near-linear memory per\nmachine, that computes a $(2+\\varepsilon)$-approximation of minimum-weight\nvertex cover in $O(\\log\\log d)$ rounds, where $d$ is the average degree of the\ninput graph.\n  Our result fills the key remaining gap in the state-of-the-art MPC algorithms\nfor vertex cover and matching problems; two classic optimization problems,\nwhich are duals of each other. Concretely, a recent line of work---by Czumaj et\nal. [STOC'18], Ghaffari et al. [PODC'18], Assadi et al. [SODA'19], and Gamlath\net al. [PODC'19]---provides $O(\\log\\log n)$ time algorithms for\n$(1+\\varepsilon)$-approximate maximum weight matching as well as for\n$(2+\\varepsilon)$-approximate minimum cardinality vertex cover. However, the\nlatter algorithm does not work for the general weighted case of vertex cover,\nfor which the best known algorithm remained at $O(\\log n)$ time complexity.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 10:59:33 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Jin", "Ce", ""], ["Nilis", "Daan", ""]]}, {"id": "2005.10675", "submitter": "Hadrien Hendrikx", "authors": "Hadrien Hendrikx, Francis Bach, Laurent Massoulie", "title": "An Optimal Algorithm for Decentralized Finite Sum Optimization", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.11394", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern large-scale finite-sum optimization relies on two key aspects:\ndistribution and stochastic updates. For smooth and strongly convex problems,\nexisting decentralized algorithms are slower than modern accelerated\nvariance-reduced stochastic algorithms when run on a single machine, and are\ntherefore not efficient. Centralized algorithms are fast, but their scaling is\nlimited by global aggregation steps that result in communication bottlenecks.\nIn this work, we propose an efficient \\textbf{A}ccelerated\n\\textbf{D}ecentralized stochastic algorithm for \\textbf{F}inite \\textbf{S}ums\nnamed ADFS, which uses local stochastic proximal updates and decentralized\ncommunications between nodes. On $n$ machines, ADFS minimizes the objective\nfunction with $nm$ samples in the same time it takes optimal algorithms to\noptimize from $m$ samples on one machine. This scaling holds until a critical\nnetwork size is reached, which depends on communication delays, on the number\nof samples $m$, and on the network topology. We give a lower bound of\ncomplexity to show that ADFS is optimal among decentralized algorithms. To\nderive ADFS, we first develop an extension of the accelerated proximal\ncoordinate gradient algorithm to arbitrary sampling. Then, we apply this\ncoordinate descent algorithm to a well-chosen dual problem based on an\naugmented graph approach, leading to the general ADFS algorithm. We illustrate\nthe improvement of ADFS over state-of-the-art decentralized approaches with\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 12:55:51 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Hendrikx", "Hadrien", ""], ["Bach", "Francis", ""], ["Massoulie", "Laurent", ""]]}, {"id": "2005.10676", "submitter": "David Brayford", "authors": "David Brayford, Sofia Vallercorsa", "title": "Deploying Scientific AI Networks at Petaflop Scale on Secure Large Scale\n  HPC Production Systems with Containers", "comments": "The Platform for Advanced Scientific Computing (PASC) Conference\n  2020. arXiv admin note: text overlap with arXiv:1905.10090", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an ever-increasing need for computational power to train complex\nartificial intelligence (AI) & machine learning (ML) models to tackle large\nscientific problems. High performance computing (HPC) resources are required to\nefficiently compute and scale complex models across tens of thousands of\ncompute nodes. In this paper, we discuss the issues associated with the\ndeployment of machine learning frameworks on large scale secure HPC systems and\nhow we successfully deployed a standard machine learning framework on a secure\nlarge scale HPC production system, to train a complex three-dimensional\nconvolutional GAN (3DGAN), with petaflop performance. 3DGAN is an example from\nthe high energy physics domain, designed to simulate the energy pattern\nproduced by showers of secondary particles inside a particle detector on\nvarious HPC systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:41:21 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Brayford", "David", ""], ["Vallercorsa", "Sofia", ""]]}, {"id": "2005.10855", "submitter": "Vaneet Aggarwal", "authors": "Vaneet Aggarwal and Tian Lan", "title": "Modeling and Optimization of Latency in Erasure-coded Storage Systems", "comments": "Monograph for use by researchers interested in latency aspects of\n  distributed storage systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As consumers are increasingly engaged in social networking and E-commerce\nactivities, businesses grow to rely on Big Data analytics for intelligence, and\ntraditional IT infrastructures continue to migrate to the cloud and edge, these\ntrends cause distributed data storage demand to rise at an unprecedented speed.\nErasure coding has seen itself quickly emerged as a promising technique to\nreduce storage cost while providing similar reliability as replicated systems,\nwidely adopted by companies like Facebook, Microsoft and Google. However, it\nalso brings new challenges in characterizing and optimizing the access latency\nwhen erasure codes are used in distributed storage. The aim of this monograph\nis to provide a review of recent progress (both theoretical and practical) on\nsystems that employ erasure codes for distributed storage.\n  In this monograph, we will first identify the key challenges and taxonomy of\nthe research problems and then give an overview of different approaches that\nhave been developed to quantify and model latency of erasure-coded storage.\nThis includes recent work leveraging MDS-Reservation, Fork-Join, Probabilistic,\nand Delayed-Relaunch scheduling policies, as well as their applications to\ncharacterize access latency (e.g., mean, tail, asymptotic latency) of\nerasure-coded distributed storage systems. We will also extend the problem to\nthe case when users are streaming videos from erasure-coded distributed storage\nsystems. Next, we bridge the gap between theory and practice, and discuss\nlessons learned from prototype implementation. In particular, we will discuss\nexemplary implementations of erasure-coded storage, illuminate key design\ndegrees of freedom and tradeoffs, and summarize remaining challenges in\nreal-world storage systems such as in content delivery and caching. Open\nproblems for future research are discussed at the end of each chapter.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 18:45:22 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Aggarwal", "Vaneet", ""], ["Lan", "Tian", ""]]}, {"id": "2005.11026", "submitter": "Zhida Pan", "authors": "Xingwu Liu, Zhida Pan, Yuyi Wang", "title": "Target Location Problem for Multi-commodity Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by scheduling in Geo-distributed data analysis, we propose a target\nlocation problem for multi-commodity flow (LoMuF for short). Given commodities\nto be sent from their resources, LoMuF aims at locating their targets so that\nthe multi-commodity flow is optimized in some sense. LoMuF is a combination of\ntwo fundamental problems, namely, the facility location problem and the network\nflow problem. We study the hardness and algorithmic issues of the problem in\nvarious settings. The findings lie in three aspects. First, a series of\nNP-hardness and APX-hardness results are obtained, uncovering the inherent\ndifficulty in solving this problem. Second, we propose an approximation\nalgorithm for general undirected networks and an exact algorithm for undirected\ntrees, which naturally induce efficient approximation algorithms on directed\nnetworks. Third, we observe separations between directed networks and\nundirected ones, indicating that imposing direction on edges makes the problem\nstrictly harder. These results show the richness of the problem and pave the\nway to further studies.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:42:29 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Liu", "Xingwu", ""], ["Pan", "Zhida", ""], ["Wang", "Yuyi", ""]]}, {"id": "2005.11050", "submitter": "Mohsen Amini Salehi", "authors": "Ali Mokhtari, Chavit Denninnart, Mohsen Amini Salehi", "title": "Autonomous Task Dropping Mechanism to Achieve Robustness in\n  Heterogeneous Computing Systems", "comments": null, "journal-ref": "in 29th Heterogeneity in Computing Workshop (HCW 2019), in the\n  Proceedings of the IPDPS 2019 Workshops & PhD Forum (IPDPSW)", "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of a distributed computing system is defined as the ability to\nmaintain its performance in the presence of uncertain parameters. Uncertainty\nis a key problem in heterogeneous (and even homogeneous) distributed computing\nsystems that perturbs system robustness. Notably, the performance of these\nsystems is perturbed by uncertainty in both task execution time and arrival.\nAccordingly, our goal is to make the system robust against these uncertainties.\nConsidering task execution time as a random variable, we use probabilistic\nanalysis to develop an autonomous proactive task dropping mechanism to attain\nour robustness goal. Specifically, we provide a mathematical model that\nidentifies the optimality of a task dropping decision, so that the system\nrobustness is maximized. Then, we leverage the mathematical model to develop a\ntask dropping heuristic that achieves the system robustness within a feasible\ntime complexity. Although the proposed model is generic and can be applied to\nany distributed system, we concentrate on heterogeneous computing (HC) systems\nthat have a higher degree of exposure to uncertainty than homogeneous systems.\nExperimental results demonstrate that the autonomous proactive dropping\nmechanism can improve the system robustness by up to 20%.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 08:14:04 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Mokhtari", "Ali", ""], ["Denninnart", "Chavit", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2005.11054", "submitter": "Song Hua", "authors": "Song Hua, Shenbin Zhang, Bingfeng Pi, Jun Sun, Kazuhiro Yamashita,\n  Yoshihide Nomura", "title": "Reasonableness discussion and analysis for Hyperledger Fabric\n  configuration", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain, as a distributed ledger technology, becomes more and more popular\nin both industry and academia. Each peer in blockchain system maintains a copy\nof ledger and makes sure of data consistency through consensus protocol.\nBlockchain system can provide many benefits such as immutability, transparency\nand security. Hyperledger Fabric is permissioned blockchain platform hosted by\nLinux foundation. Fabric has various components such as peer, ordering service,\nchaincode and state database. The structure of Fabric network is very\ncomplicated to provide reliable permissioned blockchain service. Generally,\ndevelopers must deal with hundreds of parameters to configure a network. That\nwill cause many reasonableness problems in configurations. In this paper, we\nfocus on how to detect reasonableness problems in Fabric configurations.\nFirstly, we discuss and provide a reasonableness problem knowledge database\nbased on the perspectives of functionality, security and performance. Secondly,\nwe implemented a detect tool for reasonableness check to Fabric. Finally, we\ncollect 108 sample networks as the testing dataset in the experiment. The\nresult shows our tool can help developers to locate reasonableness problems and\nunderstand their network better.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 08:25:00 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Hua", "Song", ""], ["Zhang", "Shenbin", ""], ["Pi", "Bingfeng", ""], ["Sun", "Jun", ""], ["Yamashita", "Kazuhiro", ""], ["Nomura", "Yoshihide", ""]]}, {"id": "2005.11158", "submitter": "Christian G\\\"ottel", "authors": "Christian G\\\"ottel, Lars Nielsen, Niloofar Yazdani, Pascal Felber,\n  Daniel E. Lucani and Valerio Schiavoni", "title": "Hermes: Enabling Energy-efficient IoT Networks with Generalized\n  Deduplication", "comments": "This work was partially financed by the SCALE-IoT Project (Grant No.\n  7026-00042B) granted by the Independent Research Fund Denmark, by the Aarhus\n  Universitets Forskningsfond (AUFF) Starting Grant Project AUFF- 2017-FLS-7-1,\n  and Aarhus University's DIGIT Centre. European Commission Project: LEGaTO -\n  Low Energy Toolset for Heterogeneous Computing (EC-H2020-780681)", "journal-ref": "DEBS'20: Proceedings of the 14th ACM International Conference on\n  Distributed and Event-Based Systems (2020) 133-136", "doi": "10.1145/3401025.3404098", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of the Internet of Things (IoT), the ever growing number of\nconnected devices observed in recent years and foreseen for the next decade\nsuggests that more and more data will have to be transmitted over a network,\nbefore being processed and stored in data centers. Generalized deduplication\n(GD) is a novel technique to effectively reduce the data storage cost by\nidentifying similar data chunks, and able to gradually reduce the pressure from\nthe network infrastructure by limiting the data that needs to be transmitted.\n  This paper presents Hermes, an application-level protocol for the data-plane\nthat can operate over generalized deduplication, as well as over classic\ndeduplication. Hermes significantly reduces the data transmission traffic while\neffectively decreasing the energy footprint, a relevant matter to consider in\nthe context of IoT deployments. We fully implemented Hermes and evaluated its\nperformance using consumer-grade IoT devices (e.g., Raspberry Pi 4B models).\nOur results highlight several trade-offs that must be taken into account when\nconsidering real-world workloads.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 12:59:38 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 12:02:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["G\u00f6ttel", "Christian", ""], ["Nielsen", "Lars", ""], ["Yazdani", "Niloofar", ""], ["Felber", "Pascal", ""], ["Lucani", "Daniel E.", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2005.11259", "submitter": "Anna Queralt", "authors": "Rizkallah Touma, Anna Queralt, Toni Cortes", "title": "CAPre: Code-Analysis based Prefetching for Persistent Object Stores", "comments": null, "journal-ref": null, "doi": "10.1016/j.future.2019.10.023", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data prefetching aims to improve access times to data storage systems by\npredicting data records that are likely to be accessed by subsequent requests\nand retrieving them into a memory cache before they are needed. In the case of\nPersistent Object Stores, previous approaches to prefetching have been based on\npredictions made through analysis of the store's schema, which generates rigid\npredictions, or monitoring access patterns to the store while applications are\nexecuted, which introduces memory and/or computation overhead. In this paper,\nwe present CAPre, a novel prefetching system for Persistent Object Stores based\non static code analysis of object-oriented applications. CAPre generates the\npredictions at compile-time and does not introduce any overhead to the\napplication execution. Moreover, CAPre is able to predict large amounts of\nobjects that will be accessed in the near future, thus enabling the object\nstore to perform parallel prefetching if the objects are distributed, in a much\nmore aggressive way than in schema-based prediction algorithms. We integrate\nCAPre into a distributed Persistent Object Store and run a series of\nexperiments that show that it can reduce the execution time of applications\nfrom 9% to over 50%, depending on the nature of the application and its\npersistent data model.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 16:16:40 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 17:51:32 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Touma", "Rizkallah", ""], ["Queralt", "Anna", ""], ["Cortes", "Toni", ""]]}, {"id": "2005.11317", "submitter": "Mohsen Amini Salehi", "authors": "SM Zobaed, Raju Gottmukkala, Mohsen Amini Salehi", "title": "Privacy-Preserving Clustering of Unstructured Big Data for Cloud-Based\n  Enterprise Search Solutions", "comments": "arXiv admin note: text overlap with arXiv:1908.04960", "journal-ref": "ACM Transactions on Information Technology (ACM TOIT), May 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud-based enterprise search services (e.g., Amazon Kendra) are enchanting\nto big data owners by providing them with convenient search solutions over\ntheir enterprise big datasets. However, individuals and businesses that deal\nwith confidential big data (eg, credential documents) are reluctant to fully\nembrace such services, due to valid concerns about data privacy. Solutions\nbased on client-side encryption have been explored to mitigate privacy\nconcerns. Nonetheless, such solutions hinder data processing, specifically\nclustering, which is pivotal in dealing with different forms of big data. For\ninstance, clustering is critical to limit the search space and perform\nreal-time search operations on big datasets. To overcome the hindrance in\nclustering encrypted big data, we propose privacy-preserving clustering schemes\nfor three forms of unstructured encrypted big datasets, namely static,\nsemi-dynamic, and dynamic datasets. To preserve data privacy, the proposed\nclustering schemes function based on statistical characteristics of the data\nand determine (A) the suitable number of clusters and (B) appropriate content\nfor each cluster. Experimental results obtained from evaluating the clustering\nschemes on three different datasets demonstrate between 30% to 60% improvement\non the clusters' coherency compared to other clustering schemes for encrypted\ndata. Employing the clustering schemes in a privacy-preserving enterprise\nsearch system decreases its search time by up to 78%, while increases the\nsearch accuracy by up to 35%.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 08:42:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zobaed", "SM", ""], ["Gottmukkala", "Raju", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2005.11429", "submitter": "Aron Laszka", "authors": "Scott Eisele and Taha Eghtesad and Nicholas Troutman and Aron Laszka\n  and Abhishek Dubey", "title": "Mechanisms for Outsourcing Computation via a Decentralized Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of personal computing and IoT devices grows rapidly, so does\nthe amount of computational power that is available at the edge. Since many of\nthese devices are often idle, there is a vast amount of computational power\nthat is currently untapped, and which could be used for outsourcing\ncomputation. Existing solutions for harnessing this power, such as volunteer\ncomputing (e.g., BOINC), are centralized platforms in which a single\norganization or company can control participation and pricing. By contrast, an\nopen market of computational resources, where resource owners and resource\nusers trade directly with each other, could lead to greater participation and\nmore competitive pricing. To provide an open market, we introduce MODiCuM, a\ndecentralized system for outsourcing computation. MODiCuM deters participants\nfrom misbehaving-which is a key problem in decentralized systems-by resolving\ndisputes via dedicated mediators and by imposing enforceable fines. However,\nunlike other decentralized outsourcing solutions, MODiCuM minimizes\ncomputational overhead since it does not require global trust in mediation\nresults. We provide analytical results proving that MODiCuM can deter\nmisbehavior, and we evaluate the overhead of MODiCuM using experimental results\nbased on an implementation of our platform.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 00:00:19 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 16:18:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Eisele", "Scott", ""], ["Eghtesad", "Taha", ""], ["Troutman", "Nicholas", ""], ["Laszka", "Aron", ""], ["Dubey", "Abhishek", ""]]}, {"id": "2005.11491", "submitter": "Wes Lloyd", "authors": "Huazeng Deng, Ling-Hong Hung, Raymond Schooley, David Perez, Niharika\n  Arumilli, Ka Yee Yeung, Wes Lloyd", "title": "Profiling Resource Utilization of Bioinformatics Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a software tool, the Container Profiler, that measures and records\nthe resource usage of any containerized task. Our tool profiles the CPU,\nmemory, disk, and network utilization of a containerized job by collecting\nLinux operating system metrics at the virtual machine, container, and process\nlevels. The Container Profiler can produce utilization snapshots at multiple\ntime points, allowing for continuous monitoring of the resources consumed by a\ncontainer workflow.\n  To investigate the utility of the Container Profiler we profiled the resource\nutilization requirements of a multi-stage bioinformatics analytical workflow\n(RNA sequencing using unique molecular identifiers). We examined the collected\nprofile metrics and confirmed that they were consistent with the expected CPU,\ndisk, network resource utilization patterns for the different stages of the\nworkflow. We also quantified the profiling overhead and found that this was\nnegligible.\n  The Container Profiler is a useful tool that can be used to continuously\nmonitor the resource consumption of long and complex containerized workflows\nthat run locally or on the cloud. This can identify bottlenecks where more\nresources are needed to improve performance.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 08:42:57 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Deng", "Huazeng", ""], ["Hung", "Ling-Hong", ""], ["Schooley", "Raymond", ""], ["Perez", "David", ""], ["Arumilli", "Niharika", ""], ["Yeung", "Ka Yee", ""], ["Lloyd", "Wes", ""]]}, {"id": "2005.11608", "submitter": "Sheriffo Ceesay", "authors": "Sheriffo Ceesay, Adam Barker, Yuhui Lin", "title": "Benchmarking and Performance Modelling of MapReduce Communication\n  Pattern", "comments": "8 pages, 10 figures", "journal-ref": "2019 IEEE International Conference on Cloud Computing Technology\n  and Science (CloudCom)", "doi": "10.1109/CloudCom.2019.00029", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and predicting the performance of big data applications running\nin the cloud or on-premises could help minimise the overall cost of operations\nand provide opportunities in efforts to identify performance bottlenecks. The\ncomplexity of the low-level internals of big data frameworks and the ubiquity\nof application and workload configuration parameters makes it challenging and\nexpensive to come up with comprehensive performance modelling solutions.\n  In this paper, instead of focusing on a wide range of configurable\nparameters, we studied the low-level internals of the MapReduce communication\npattern and used a minimal set of performance drivers to develop a set of phase\nlevel parametric models for approximating the execution time of a given\napplication on a given cluster. Models can be used to infer the performance of\nunseen applications and approximate their performance when an arbitrary dataset\nis used as input. Our approach is validated by running empirical experiments in\ntwo setups. On average the error rate in both setups is plus or minus 10% from\nthe measured values.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 21:52:29 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ceesay", "Sheriffo", ""], ["Barker", "Adam", ""], ["Lin", "Yuhui", ""]]}, {"id": "2005.11796", "submitter": "Themistoklis Melissourgos", "authors": "Argyrios Deligkas, Themistoklis Melissourgos, Paul G. Spirakis", "title": "Walrasian Equilibria in Markets with Small Demands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of finding a Walrasian equilibrium in markets where\nthe agents have $k$-demand valuations. These valuations are an extension of\nunit-demand valuations where a bundle's value is the maximum of its\n$k$-subsets' values. For unit-demand agents, where the existence of a Walrasian\nequilibrium is guaranteed, we show that the problem is in quasi-NC. For $k=2$,\nwe show that it is NP-hard to decide if a Walrasian equilibrium exists even if\nthe valuations are fractionally subadditive (XOS), while for $k=3$ the hardness\ncarries over to budget-additive valuations. In addition, we give a\npolynomial-time algorithm for markets with 2-demand single-minded valuations,\nor unit-demand valuations.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 16:39:29 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 01:38:04 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 16:11:44 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Deligkas", "Argyrios", ""], ["Melissourgos", "Themistoklis", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "2005.11901", "submitter": "Renuga Kanagavelu", "authors": "Renuga Kanagavelu, Zengxiang Li, Juniarto Samsudin, Yechao Yang, Feng\n  Yang, Rick Siow Mong Goh, Mervyn Cheah, Praewpiraya Wiwatphonthana,\n  Khajonpong Akkarajitsakul, Shangguang Wangz", "title": "Two-Phase Multi-Party Computation Enabled Privacy-Preserving Federated\n  Learning", "comments": "This paper appears in the Proceedings of The 20th IEEE/ACM\n  International Symposium on Cluster, Cloud and Internet Computing(CCGrid\n  2020). Please feel free to contact us for questions or remarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Countries across the globe have been pushing strict regulations on the\nprotection of personal or private data collected. The traditional centralized\nmachine learning method, where data is collected from end-users or IoT devices,\nso that it can discover insights behind real-world data, may not be feasible\nfor many data-driven industry applications in light of such regulations. A new\nmachine learning method, coined by Google as Federated Learning (FL) enables\nmultiple participants to train a machine learning model collectively without\ndirectly exchanging data. However, recent studies have shown that there is\nstill a possibility to exploit the shared models to extract personal or\nconfidential data. In this paper, we propose to adopt Multi Party Computation\n(MPC) to achieve privacy-preserving model aggregation for FL. The MPC-enabled\nmodel aggregation in a peer-to-peer manner incurs high communication overhead\nwith low scalability. To address this problem, the authors proposed to develop\na two-phase mechanism by 1) electing a small committee and 2) providing\nMPC-enabled model aggregation service to a larger number of participants\nthrough the committee. The MPC enabled FL framework has been integrated in an\nIoT platform for smart manufacturing. It enables a set of companies to train\nhigh quality models collectively by leveraging their complementary data-sets on\ntheir own premises, without compromising privacy, model accuracy vis-a-vis\ntraditional machine learning methods and execution efficiency in terms of\ncommunication cost and execution time.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 03:05:05 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kanagavelu", "Renuga", ""], ["Li", "Zengxiang", ""], ["Samsudin", "Juniarto", ""], ["Yang", "Yechao", ""], ["Yang", "Feng", ""], ["Goh", "Rick Siow Mong", ""], ["Cheah", "Mervyn", ""], ["Wiwatphonthana", "Praewpiraya", ""], ["Akkarajitsakul", "Khajonpong", ""], ["Wangz", "Shangguang", ""]]}, {"id": "2005.12091", "submitter": "Tobias Weinzierl", "authors": "Philipp Samfass, Tobias Weinzierl, Benjamin Hazelwood, Michael Bader", "title": "TeaMPI -- Replication-based Resilience without the (Performance) Pain", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-50743-5_23", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In an era where we can not afford to checkpoint frequently, replication is a\ngeneric way forward to construct numerical simulations that can continue to run\neven if hardware parts fail. Yet, replication often is not employed on larger\nscales, as na\\\"ively mirroring a computation once effectively halves the\nmachine size, and as keeping replicated simulations consistent with each other\nis not trivial. We demonstrate for the ExaHyPE engine -- a task-based solver\nfor hyperbolic equation systems -- that it is possible to realise resiliency\nwithout major code changes on the user side, while we introduce a novel\nalgorithmic idea where replication reduces the time-to-solution. The redundant\nCPU cycles are not burned \"for nothing\". Our work employs a weakly consistent\ndata model where replicas run independently yet inform each other through\nheartbeat messages whether they are still up and running. Our key performance\nidea is to let the tasks of the replicated simulations share some of their\noutcomes, while we shuffle the actual task execution order per replica. This\nway, replicated ranks can skip some local computations and automatically start\nto synchronise with each other. Our experiments with a production-level seismic\nwave-equation solver provide evidence that this novel concept has the potential\nto make replication affordable for large-scale simulations in high-performance\ncomputing.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:12:35 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 13:00:32 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Samfass", "Philipp", ""], ["Weinzierl", "Tobias", ""], ["Hazelwood", "Benjamin", ""], ["Bader", "Michael", ""]]}, {"id": "2005.12249", "submitter": "Prashant Shenoy", "authors": "Pradeep Ambati, David Irwin, Prashant Shenoy", "title": "No Reservations: A First Look at Amazon's Reserved Instance Marketplace", "comments": "6 pages, 13 figures. This paper will appear in the Proceedings of the\n  12th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud), July 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud users can significantly reduce their cost (by up to 60\\%) by reserving\nvirtual machines (VMs) for long periods (1 or 3 years) rather than acquiring\nthem on demand. Unfortunately, reserving VMs exposes users to \\emph{demand\nrisk} that can increase cost if their expected future demand does not\nmaterialize. Since accurately forecasting demand over long periods is\nchallenging, users often limit their use of reserved VMs. To mitigate demand\nrisk, Amazon operates a Reserved Instance Marketplace (RIM) where users may\npublicly list the remaining time on their VM reservations for sale at a price\nthey set. The RIM enables users to limit demand risk by either selling VM\nreservations if their demand changes, or purchasing variable- and shorter-term\nVM reservations that better match their demand forecast horizon. Clearly, the\nRIM's potential to mitigate demand risk is a function of its price\ncharacteristics. However, to the best of our knowledge, historical RIM prices\nhave neither been made publicly available nor analyzed. To address the problem,\nwe have been monitoring and archiving RIM prices for 1.75 years across all 69\navailability zones and 22 regions in Amazon's Elastic Compute Cloud (EC2). This\npaper provides a first look at this data and its implications for\ncost-effectively provisioning cloud infrastructure.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 17:51:22 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ambati", "Pradeep", ""], ["Irwin", "David", ""], ["Shenoy", "Prashant", ""]]}, {"id": "2005.12271", "submitter": "Sally Elghamrawy Prof", "authors": "Eman A. Khashan, Ali I. Eldesouky, M. Fadel and Sally M. Elghamrawy", "title": "A Big Data Based Framework for Executing Complex Query Over COVID-19\n  Datasets (COVID-QF)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19's rapid global spread has driven innovative tools for Big Data\nAnalytics. These have guided organizations in all fields of the health industry\nto track and minimized the effects of virus. Researchers are required to detect\ncoronaviruses through artificial intelligence, machine learning, and natural\nlanguage processing, and to gain a complete understanding of the disease.\nCOVID-19 takes place in different countries in the world, with which only big\ndata application and the work of NOSQL databases are suitable. There is a great\nnumber of platforms used for processing NOSQL Databases model like: Spark, H2O\nand Hadoop HDFS/MapReduce, which are proper to control and manage the enormous\namount of data. Many challenges faced by large applications programmers,\nespecially those that work on the COVID-19 databases through hybrid data models\nthrough different APIs and query. In this context, this paper proposes a\nstorage framework to handle both SQL and NOSQL databases named (COVID-QF) for\nCOVID-19 datasets in order to treat and handle the problems caused by virus\nspreading worldwide by reducing treatment times. In case of NoSQL database,\nCOVID-QF uses Hadoop HDFS/Map Reduce and Apache Spark. The COVID-QF consists of\nthree Layers: data collection layer, storage layer, and query Processing layer.\nThe data is collected in the data collection layer. The storage layer divides\ndata into collection of data-saving and processing blocks, and it connects the\nConnector of the spark with different databases engine to reduce time of saving\nand retrieving. While the Processing layer executes the request query and sends\nresults. The proposed framework used three datasets increased for time for\nCOVID-19 data (COVID-19-Merging, COVID-19-inside-Hubei and COVID-19-ex-Hubei)\nto test experiments of this study. The results obtained insure the superiority\nof the COVID-QF framework.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 03:13:00 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Khashan", "Eman A.", ""], ["Eldesouky", "Ali I.", ""], ["Fadel", "M.", ""], ["Elghamrawy", "Sally M.", ""]]}, {"id": "2005.12326", "submitter": "Cong Wang", "authors": "Cong Wang, Yuanyuan Yang and Pengzhan Zhou", "title": "Towards Efficient Scheduling of Federated Mobile Devices under\n  Computational and Statistical Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Originated from distributed learning, federated learning enables\nprivacy-preserved collaboration on a new abstracted level by sharing the model\nparameters only. While the current research mainly focuses on optimizing\nlearning algorithms and minimizing communication overhead left by distributed\nlearning, there is still a considerable gap when it comes to the real\nimplementation on mobile devices. In this paper, we start with an empirical\nexperiment to demonstrate computation heterogeneity is a more pronounced\nbottleneck than communication on the current generation of battery-powered\nmobile devices, and the existing methods are haunted by mobile stragglers.\nFurther, non-identically distributed data across the mobile users makes the\nselection of participants critical to the accuracy and convergence. To tackle\nthe computational and statistical heterogeneity, we utilize data as a tuning\nknob and propose two efficient polynomial-time algorithms to schedule different\nworkloads on various mobile devices, when data is identically or\nnon-identically distributed. For identically distributed data, we combine\npartitioning and linear bottleneck assignment to achieve near-optimal training\ntime without accuracy loss. For non-identically distributed data, we convert it\ninto an average cost minimization problem and propose a greedy algorithm to\nfind a reasonable balance between computation time and accuracy. We also\nestablish an offline profiler to quantify the runtime behavior of different\ndevices, which serves as the input to the scheduling algorithms. We conduct\nextensive experiments on a mobile testbed with two datasets and up to 20\ndevices. Compared with the common benchmarks, the proposed algorithms achieve\n2-100x speedup epoch-wise, 2-7% accuracy gain and boost the convergence rate by\nmore than 100% on CIFAR10.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 18:21:51 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 18:12:30 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Cong", ""], ["Yang", "Yuanyuan", ""], ["Zhou", "Pengzhan", ""]]}, {"id": "2005.12364", "submitter": "Kezhi Wang", "authors": "Feibo Jiang and Li Dong and Kezhi Wang and Kun Yang and Cunhua Pan", "title": "Distributed Resource Scheduling for Large-Scale MEC Systems: A\n  Multi-Agent Ensemble Deep Reinforcement Learning with Imitation Acceleration", "comments": "Submitted for Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization of distributed resource scheduling to minimize\nthe sum of task latency and energy consumption for all the Internet of things\ndevices (IoTDs) in a large-scale mobile edge computing (MEC) system. To address\nthis problem, we propose a distributed intelligent resource scheduling (DIRS)\nframework, which includes centralized training relying on the global\ninformation and distributed decision making by each agent deployed in each MEC\nserver. More specifically, we first introduce a novel multi-agent\nensemble-assisted distributed deep reinforcement learning (DRL) architecture,\nwhich can simplify the overall neural network structure of each agent by\npartitioning the state space and also improve the performance of a single agent\nby combining decisions of all the agents. Secondly, we apply action refinement\nto enhance the exploration ability of the proposed DIRS framework, where the\nnear-optimal state-action pairs are obtained by a novel L\\'evy flight search.\nFinally, an imitation acceleration scheme is presented to pre-train all the\nagents, which can significantly accelerate the learning process of the proposed\nframework through learning the professional experience from a small amount of\ndemonstration data. Extensive simulations are conducted to demonstrate that the\nproposed DIRS framework is efficient and outperforms the existing benchmark\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:04:40 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Jiang", "Feibo", ""], ["Dong", "Li", ""], ["Wang", "Kezhi", ""], ["Yang", "Kun", ""], ["Pan", "Cunhua", ""]]}, {"id": "2005.12623", "submitter": "Julian Portmann", "authors": "Sebastian Brandt, Julian Portmann, Jara Uitto", "title": "Tight Bounds for Deterministic High-Dimensional Grid Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of exploring an oriented grid with autonomous agents\ngoverned by finite automata. In the case of a 2-dimensional grid, the question\nhow many agents are required to explore the grid, or equivalently, find a\nhidden treasure in the grid, is fully understood in both the synchronous and\nthe semi-synchronous setting. For higher dimensions, Dobrev, Narayanan,\nOpatrny, and Pankratov [ICALP'19] showed very recently that, surprisingly, a\n(small) constant number of agents suffices to find the treasure, independent of\nthe number of dimensions, thereby disproving a conjecture by Cohen, Emek,\nLouidor, and Uitto [SODA'17]. Dobrev et al. left as an open question whether\ntheir bounds on the number of agents can be improved. We answer this question\nin the affirmative for deterministic finite automata: we show that 3\nsynchronous and 4 semi-synchronous agents suffice to explore an $n$-dimensional\ngrid for any constant $n$. The bounds are optimal and notably, the matching\nlower bounds already hold in the 2-dimensional case.\n  Our techniques can also be used to make progress on other open questions\nasked by Dobrev et al.: we prove that 4 synchronous and 5 semi-synchronous\nagents suffice for polynomial-time exploration, and we show that, under a\nnatural assumption, 3 synchronous and 4 semi-synchronous agents suffice to\nexplore unoriented grids of arbitrary dimension (which, again, is tight).\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 10:48:00 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Brandt", "Sebastian", ""], ["Portmann", "Julian", ""], ["Uitto", "Jara", ""]]}, {"id": "2005.12648", "submitter": "Berenger Bramas", "authors": "Berenger Bramas (Inria, ICube, CAMUS), Quentin Bramas (ICube, UNISTRA)", "title": "On the improvement of the in-place merge algorithm parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present several improvements in the parallelization of the\nin-place merge algorithm, which merges two contiguous sorted arrays into one\nwith an O(T) space complexity (where T is the number of threads). The approach\ndivides the two arrays into as many pairs of partitions as there are threads\navailable; such that each thread can later merge a pair of partitions\nindependently of the others. We extend the existing method by proposing a new\nalgorithm to find the median of two partitions. Additionally, we provide a new\nstrategy to divide the input arrays where we minimize the data movement, but at\nthe cost of making this stage sequential. Finally, we provide the so-called\nlinear shifting algorithm that swaps two partitions in-place with contiguous\ndata access. We emphasize that our approach is straightforward to implement and\nthat it can also be used for external (out of place) merging. The results\ndemonstrate that it provides a significant speedup compared to sequential\nexecutions, when the size of the arrays is greater than a thousand elements.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 12:08:04 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Bramas", "Berenger", "", "Inria, ICube, CAMUS"], ["Bramas", "Quentin", "", "ICube, UNISTRA"]]}, {"id": "2005.12725", "submitter": "Ye Wang", "authors": "Ye Wang and Roger Wattenhofer", "title": "Asynchronous Byzantine Agreement in Incomplete Networks [Technical\n  Report]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Byzantine agreement problem is considered to be a core problem in\ndistributed systems. For example, Byzantine agreement is needed to build a\nblockchain, a totally ordered log of records. Blockchains are asynchronous\ndistributed systems, fault-tolerant against Byzantine nodes.\n  In the literature, the asynchronous byzantine agreement problem is studied in\na fully connected network model where every node can directly send messages to\nevery other node. This assumption is questionable in many real-world\nenvironments. In the reality, nodes might need to communicate by means of an\nincomplete network, and Byzantine nodes might not forward messages.\nFurthermore, Byzantine nodes might not behave correctly and, for example,\ncorrupt messages. Therefore, in order to truly understand Byzantine Agreement,\nwe need both ingredients: asynchrony and incomplete communication networks.\n  In this paper, we study the asynchronous Byzantine agreement problem in\nincomplete networks. A classic result by Danny Dolev proved that in a\ndistributed system with n nodes in the presence of f Byzantine nodes, the\nvertex connectivity of the system communication graph should be at least\n(2f+1). While Dolev's result was for synchronous deterministic systems, we\ndemonstrate that the same bound also holds for asynchronous randomized systems.\nWe show that the bound is tight by presenting a randomized algorithm, and a\nmatching lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 13:51:54 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wang", "Ye", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2005.12783", "submitter": "Antonio Fern\\'andez Anta", "authors": "Oluwasegun Ojo, Augusto Garc\\'ia-Agundez, Benjamin Girault, Harold\n  Hern\\'andez, Elisa Cabana, Amanda Garc\\'ia-Garc\\'ia, Payman Arabshahi, Carlos\n  Baquero, Paolo Casari, Ednaldo Jos\\'e Ferreira, Davide Frey, Chryssis\n  Georgiou, Mathieu Goessens, Anna Ishchenko, Ernesto Jim\\'enez, Oleksiy\n  Kebkal, Rosa Lillo, Raquel Menezes, Nicolas Nicolaou, Antonio Ortega, Paul\n  Patras, Julian C Roberts, Efstathios Stavrakis, Yuichi Tanaka, Antonio\n  Fern\\'andez Anta", "title": "CoronaSurveys: Using Surveys with Indirect Reporting to Estimate the\n  Incidence and Evolution of Epidemics", "comments": "Presented at The KDD Workshop on Humanitarian Mapping, San Diego,\n  California USA, August 24, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world is suffering from a pandemic called COVID-19, caused by the\nSARS-CoV-2 virus. National governments have problems evaluating the reach of\nthe epidemic, due to having limited resources and tests at their disposal. This\nproblem is especially acute in low and middle-income countries (LMICs). Hence,\nany simple, cheap and flexible means of evaluating the incidence and evolution\nof the epidemic in a given country with a reasonable level of accuracy is\nuseful. In this paper, we propose a technique based on (anonymous) surveys in\nwhich participants report on the health status of their contacts. This indirect\nreporting technique, known in the literature as network scale-up method,\npreserves the privacy of the participants and their contacts, and collects\ninformation from a larger fraction of the population (as compared to individual\nsurveys). This technique has been deployed in the CoronaSurveys project, which\nhas been collecting reports for the COVID-19 pandemic for more than two months.\nResults obtained by CoronaSurveys show the power and flexibility of the\napproach, suggesting that it could be an inexpensive and powerful tool for\nLMICs.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 11:58:23 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 12:18:50 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ojo", "Oluwasegun", ""], ["Garc\u00eda-Agundez", "Augusto", ""], ["Girault", "Benjamin", ""], ["Hern\u00e1ndez", "Harold", ""], ["Cabana", "Elisa", ""], ["Garc\u00eda-Garc\u00eda", "Amanda", ""], ["Arabshahi", "Payman", ""], ["Baquero", "Carlos", ""], ["Casari", "Paolo", ""], ["Ferreira", "Ednaldo Jos\u00e9", ""], ["Frey", "Davide", ""], ["Georgiou", "Chryssis", ""], ["Goessens", "Mathieu", ""], ["Ishchenko", "Anna", ""], ["Jim\u00e9nez", "Ernesto", ""], ["Kebkal", "Oleksiy", ""], ["Lillo", "Rosa", ""], ["Menezes", "Raquel", ""], ["Nicolaou", "Nicolas", ""], ["Ortega", "Antonio", ""], ["Patras", "Paul", ""], ["Roberts", "Julian C", ""], ["Stavrakis", "Efstathios", ""], ["Tanaka", "Yuichi", ""], ["Anta", "Antonio Fern\u00e1ndez", ""]]}, {"id": "2005.12821", "submitter": "Madhur Jain", "authors": "Madhur Jain", "title": "Study of Firecracker MicroVM", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Firecracker is a virtualization technology that makes use of Kernel Virtual\nMachine (KVM). Firecracker belongs to a new virtualization class named the\nmicro-virtual machines (MicroVMs). Using Firecracker, we can launch lightweight\nMicroVMs in non-virtualized environments in a fraction of a second, at the same\ntime offering the security and workload isolation provided by traditional VMs\nand also the resource efficiency that comes along with containers \\cite{b1}.\nFirecracker aims to provide a slimmed-down MicroVM, comprised of approximately\n50K lines of code in Rust and with a reduced attack surface for guest VMs. This\nreport will examine the internals of Firecracker and understand why Firecracker\nis the next big thing going forward in virtualization and cloud computing.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:56:06 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Jain", "Madhur", ""]]}, {"id": "2005.12873", "submitter": "Miyuru Dayarathna", "authors": "Miyuru Dayarathna and Toyotaro Suzumura", "title": "Benchmarking Graph Data Management and Processing Systems: A Survey", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of scalable, representative, and widely adopted benchmarks\nfor graph data systems have been a question for which answers has been sought\nfor decades. We conduct an in-depth study of the existing literature on\nbenchmarks for graph data management and processing, covering 20 different\nbenchmarks developed during the last 15 years. We categorize the benchmarks\ninto three areas focusing on benchmarks for graph processing systems, graph\ndatabase benchmarks, and bigdata benchmarks with graph processing workloads.\nThis systematic approach allows us to identify multiple issues existing in this\narea, including i) few benchmarks exist which can produce high workload\nscenarios, ii) no significant work done on benchmarking graph stream processing\nas well as graph based machine learning, iii) benchmarks tend to use\nconventional metrics despite new meaningful metrics have been around for years,\niv) increasing number of big data benchmarks appear with graph processing\nworkloads. Following these observations, we conclude the survey by describing\nkey challenges for future research on graph data systems benchmarking.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:07:29 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 17:35:16 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 17:17:53 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Dayarathna", "Miyuru", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "2005.12911", "submitter": "Yann Thierry-Mieg", "authors": "Yann Thierry-Mieg (SU, CNRS)", "title": "Symbolic and Structural Model-Checking", "comments": "Extended Journal version of ICATPN 2020 paper (pre-print)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brute-force model-checking consists in exhaustive exploration of the\nstate-space of a Petri net, and meets the dreaded state-space explosion\nproblem.\n  In contrast, this paper shows how to solve model-checking problems using a\ncombination of techniques that stay in complexity proportional to the size of\nthe net structure rather than to the state-space size.\n  We combine an SMT based over-approximation to prove that some behaviors are\nunfeasible, an under-approximation using memory-less sampling of runs to find\nwitness traces or counter-examples, and a set of structural reduction rules\nthat can simplify both the system and the property.\n  This approach was able to win by a clear margin the model-checking contest\n2020 for reachability queries as well as deadlock detection, thus demonstrating\nthe practical effectiveness and general applicability of the system of rules\npresented in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 08:44:36 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 08:55:49 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Thierry-Mieg", "Yann", "", "SU, CNRS"]]}, {"id": "2005.13076", "submitter": "Pablo Mart\\'inez", "authors": "Eduardo Jos\\'e G\\'omez-Hern\\'andez, Pablo Antonio Mart\\'inez, Biagio\n  Peccerillo, Sandro Bartolini, Jos\\'e Manuel Garc\\'ia, Gregorio Bernab\\'e", "title": "Using PHAST to port Caffe library: First experiences and lessons learned", "comments": "Presented at the 13th International Workshop on Programmability and\n  Architectures for Heterogeneous Multicores, 2020 (arXiv:2005.07619)", "journal-ref": null, "doi": null, "report-no": "MULTIPROG/2020/3", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance has always been a hot topic in computing. However, the viable\nways to achieve it have taken many forms in the different moments of computing\nhistory. Today, technological limits have pushed the adoption of increasingly\nparallel multi-core and many-core architectures and even the use of highly\nspecific hardware (aka Domain-Specific Architectures, or DSAs) to solve very\nspecific problems. In this new context, one major problem is how to develop\nsoftware once, and be able to run it on multiple accelerator architectures,\nseamlessly. Ideally aiming at a single programming model that can automatically\ntarget the code to different kinds of parallel architectures, allowing specific\ntuning with minimal, if any, changes to the source-code in order to seek\nperformance portability. A comprehensive solution to this is still lacking. In\nthis work, we present the use of the PHAST Library, which allows users to code\nonce, at a high level of abstraction and thus with high productivity, and\nautomatically targeting different parallel devices by changing the compilation\nprocess. As a case study, we have worked on the porting of the well-known\ndeep-learning Caffe framework. The framework has been split into different\nparts and some of them have been ported, obtaining a working straightforward\nimplementation that can be run on both CPUs and GPUs. We conclude discussing\nthe lessons learned during the porting process, and analyzing the obtained\nperformance in the perspective of completing the porting and expanding it to\nfuture consequent works.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 23:02:48 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 22:36:02 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["G\u00f3mez-Hern\u00e1ndez", "Eduardo Jos\u00e9", ""], ["Mart\u00ednez", "Pablo Antonio", ""], ["Peccerillo", "Biagio", ""], ["Bartolini", "Sandro", ""], ["Garc\u00eda", "Jos\u00e9 Manuel", ""], ["Bernab\u00e9", "Gregorio", ""]]}, {"id": "2005.13227", "submitter": "Ben Van Werkhoven", "authors": "Ben van Werkhoven (1), Willem Jan Palenstijn (2), Alessio Sclocco (1)\n  ((1) Netherlands eScience Center, (2) Centrum Wiskunde & Informatica)", "title": "Lessons learned in a decade of research software engineering GPU\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After years of using Graphics Processing Units (GPUs) to accelerate\nscientific applications in fields as varied as tomography, computer vision,\nclimate modeling, digital forensics, geospatial databases, particle physics,\nradio astronomy, and localization microscopy, we noticed a number of technical,\nsocio-technical, and non-technical challenges that Research Software Engineers\n(RSEs) may run into. While some of these challenges, such as managing different\nprogramming languages within a project, or having to deal with different memory\nspaces, are common to all software projects involving GPUs, others are more\ntypical of scientific software projects. Among these challenges we include\nchanging resolutions or scales, maintaining an application over time and making\nit sustainable, and evaluating both the obtained results and the achieved\nperformance. %In this paper, we present the challenges and lessons learned from\nresearch software engineering GPU applications.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 08:11:30 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["van Werkhoven", "Ben", "", "Netherlands eScience Center"], ["Palenstijn", "Willem Jan", "", "Centrum Wiskunde & Informatica"], ["Sclocco", "Alessio", "", "Netherlands eScience Center"]]}, {"id": "2005.13247", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Chengjian Liu, Wei Wang, Bo\n  Li", "title": "A Quantitative Survey of Communication Optimizations in Distributed Deep\n  Learning", "comments": "9 pages, 6 figures & tables. Code at:\n  https://github.com/HKBU-HPML/ddl-benchmarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, large and complex deep learning (DL) models are increasingly\ntrained in a distributed manner across multiple worker machines, in which\nextensive communications between workers pose serious scaling problems. In this\narticle, we present a quantitative survey of communication optimization\ntechniques for data parallel distributed DL. We first identify the major\ncommunication challenges and classify the existing solutions into three levels,\nnamely the learning algorithm, the system architecture, and the network\ninfrastructure. We present the state-of-the-art communication optimization\ntechniques and conduct a comparative study of seven common lossless distributed\nDL methods on a 32-GPU cluster with 100Gbps InfiniBand (IB). We show that (1)\nthe DL models with low model intensity (such as BERT and BERT-Large) are\ndifficult to scale out even with the best available lossless algorithm over\n100Gbps IB; (2) the system architecture and scheduling algorithms have a\ncritical impact on the scaling property. We conclude the article with\ndiscussions on the open issues for further investigations.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 09:12:48 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 07:05:33 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Shi", "Shaohuai", ""], ["Tang", "Zhenheng", ""], ["Chu", "Xiaowen", ""], ["Liu", "Chengjian", ""], ["Wang", "Wei", ""], ["Li", "Bo", ""]]}, {"id": "2005.13304", "submitter": "Gal Oren", "authors": "Idan Mosseri, Lee-or Alon, Re'em Harel and Gal Oren", "title": "ComPar: Optimized Multi-Compiler for Automatic OpenMP S2S\n  Parallelization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallelization schemes are essential in order to exploit the full benefits\nof multi-core architectures. In said architectures, the most comprehensive\nparallelization API is OpenMP. However, the introduction of correct and optimal\nOpenMP parallelization to applications is not always a simple task, due to\ncommon parallel management pitfalls, architecture heterogeneity and the current\nnecessity for human expertise in order to comprehend many fine details and\nabstract correlations. To ease this process, many automatic parallelization\ncompilers were created over the last decade. Harel et al. [2020] tested several\nsource-to-source compilers and concluded that each has its advantages and\ndisadvantages and no compiler is superior to all other compilers in all tests.\nThis indicates that a fusion of the compilers' best outputs under the best\nhyper-parameters for the current hardware setups can yield greater speedups. To\ncreate such a fusion, one should execute a computationally intensive\nhyper-parameter sweep, in which the performance of each option is estimated and\nthe best option is chosen. We created a novel parallelization source-to-source\nmulti-compiler named ComPar, which uses code segmentation-and-fusion with\nhyper-parameters tuning to achieve the best parallel code possible without any\nhuman intervention while maintaining the program's validity. In this paper we\npresent ComPar and analyze its results on NAS and PolyBench benchmarks. We\nconclude that although the resources ComPar requires to produce parallel code\nare greater than other source-to-source parallelization compilers - as it\ndepends on the number of parameters the user wishes to consider, and their\ncombinations - ComPar achieves superior performance overall compared to the\nserial code version and other tested parallelization compilers. ComPar is\npublicly available at: https://github.com/Scientific-Computing-Lab-NRCN/compar.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 12:02:05 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Mosseri", "Idan", ""], ["Alon", "Lee-or", ""], ["Harel", "Re'em", ""], ["Oren", "Gal", ""]]}, {"id": "2005.13332", "submitter": "Amit Kulkarni Mr.", "authors": "Amit Kulkarni, Monica Chiosa, Thomas B. Preu{\\ss}er, Kaan Kara, David\n  Sidler and Gustavo Alonso", "title": "HyperLogLog Sketch Acceleration on FPGA", "comments": "This paper was accepted as a full paper to FPL 2020. The latest/full\n  version of this paper is available:\n  https://ieeexplore.ieee.org/document/9221525", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sketches are a set of widely used approximated data summarizing\ntechniques. Their fundamental property is sub-linear memory complexity on the\ninput cardinality, an important aspect when processing streams or data sets\nwith a vast base domain (URLs, IP addresses, user IDs, etc.). Among the many\ndata sketches available, HyperLogLog has become the reference for cardinality\ncounting (how many distinct data items there are in a data set). Although it\ndoes not count every data item (to reduce memory consumption), it provides\nprobabilistic guarantees on the result, and it is, thus, often used to analyze\ndata streams. In this paper, we explore how to implement HyperLogLog on an FPGA\nto benefit from the parallelism available and the ability to process data\nstreams coming from high-speed networks. Our multi-pipelined high-cardinality\nHyperLogLog implementation delivers 1.8x higher throughput than an optimized\nHyperLogLog running on a dual-socket Intel Xeon E5-2630 v3 system with a total\nof 16 cores and 32 hyper-threads.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 07:14:45 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 07:54:48 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Kulkarni", "Amit", ""], ["Chiosa", "Monica", ""], ["Preu\u00dfer", "Thomas B.", ""], ["Kara", "Kaan", ""], ["Sidler", "David", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2005.13339", "submitter": "Ivan Homoliak Ph.D.", "authors": "Ivan Homoliak, Pawel Szalachowski", "title": "Aquareum: A Centralized Ledger Enhanced with Blockchain and Trusted\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed ledger systems (i.e., blockchains) have received a lot of\nattention recently. They promise to enable mutually untrusted participants to\nexecute transactions, while providing the immutability of the transaction\nhistory and censorship resistance. Although decentralized ledgers may become a\ndisruptive innovation, as of today, they suffer from scalability, privacy, or\ngovernance issues. Therefore, they are inapplicable for many important use\ncases, where interestingly, centralized ledger systems quietly gain adoption\nand find new use cases. Unfortunately, centralized ledgers have also several\ndrawbacks, like a lack of efficient verifiability or a higher risk of\ncensorship and equivocation.\n  In this paper, we present Aquareum, a novel framework for centralized ledgers\nremoving their main limitations. By combining a trusted execution environment\nwith a public blockchain platform, Aquareum provides publicly verifiable,\nnon-equivocating, censorship-evident, private, and high-performance ledgers.\nAquareum ledgers are integrated with a Turing-complete virtual machine,\nallowing arbitrary transaction processing logics, including tokens or\nclient-specified smart contracts. Aquareum is fully implemented and\ndeployment-ready, even with currently existing technologies.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:07:01 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Homoliak", "Ivan", ""], ["Szalachowski", "Pawel", ""]]}, {"id": "2005.13425", "submitter": "Martin Karp", "authors": "Martin Karp, Niclas Jansson, Artur Podobas, Philipp Schlatter, and\n  Stefano Markidis", "title": "Optimization of Tensor-product Operations in Nekbone on GPUs", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the CFD solver Nek5000, the computation is dominated by the evaluation of\nsmall tensor operations. Nekbone is a proxy app for Nek5000 and has previously\nbeen ported to GPUs with a mixed OpenACC and CUDA approach. In this work, we\ncontinue this effort and optimize the main tensor-product operation in Nekbone\nfurther. Our optimization is done in CUDA and uses a different, 2D, thread\nstructure to make the computations layer by layer. This enables us to use loop\nunrolling as well as utilize registers and shared memory efficiently. Our\nimplementation is then compared on both the Pascal and Volta GPU architectures\nto previous GPU versions of Nekbone as well as a measured roofline. The results\nshow that our implementation outperforms previous GPU Nekbone implementations\nby 6-10%. Compared to the measured roofline, we obtain 77 - 92% of the peak\nperformance for both Nvidia P100 and V100 GPUs for inputs with 1024 - 4096\nelements and polynomial degree 9.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 15:30:11 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Karp", "Martin", ""], ["Jansson", "Niclas", ""], ["Podobas", "Artur", ""], ["Schlatter", "Philipp", ""], ["Markidis", "Stefano", ""]]}, {"id": "2005.13457", "submitter": "Sergio Miguel Martin", "authors": "Sergio M. Martin, Daniel W\\\"alchli, Georgios Arampatzis, Athena E.\n  Economides, Petr Karnakov, Petros Koumoutsakos", "title": "Korali: Efficient and Scalable Software Framework for Bayesian\n  Uncertainty Quantification and Stochastic Optimization", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Korali, an open-source framework for large-scale Bayesian\nuncertainty quantification and stochastic optimization. The framework relies on\nnon-intrusive sampling of complex multiphysics models and enables their\nexploitation for optimization and decision-making. In addition, its distributed\nsampling engine makes efficient use of massively-parallel architectures while\nintroducing novel fault tolerance and load balancing mechanisms. We demonstrate\nthese features by interfacing Korali with existing high-performance software\nsuch as Aphros, Lammps (CPU-based), and Mirheo (GPU-based) and show efficient\nscaling for up to 512 nodes of the CSCS Piz Daint supercomputer. Finally, we\npresent benchmarks demonstrating that Korali outperforms related\nstate-of-the-art software frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:14:49 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 12:58:16 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Martin", "Sergio M.", ""], ["W\u00e4lchli", "Daniel", ""], ["Arampatzis", "Georgios", ""], ["Economides", "Athena E.", ""], ["Karnakov", "Petr", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "2005.13499", "submitter": "Petr Kuznetsov", "authors": "Petr Kuznetsov and Andrei Tonkikh", "title": "Asynchronous Reconfiguration with Byzantine Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicated services are inherently vulnerable to failures and security\nbreaches. In a long-running system, it is, therefore, indispensable to maintain\na \\emph{reconfiguration} mechanism that would replace faulty replicas with\ncorrect ones. An important challenge is to enable reconfiguration without\naffecting the availability and consistency of the replicated data: the clients\nshould be able to get correct service even when the set of service replicas is\nbeing updated.\n  In this paper, we address the problem of reconfiguration in the presence of\nByzantine failures: faulty replicas or clients may arbitrarily deviate from\ntheir expected behavior. We describe a generic technique for building\n\\emph{asynchronous} and \\emph{Byzantine fault-tolerant} reconfigurable objects:\nclients can manipulate the object data and issue reconfiguration calls without\nreaching consensus on the current configuration. With the help of\nforward-secure digital signatures, our solution makes sure that superseded and\npossibly compromised configurations are harmless, that slow clients cannot be\nfooled into reading stale data, and that Byzantine clients cannot cause a\ndenial of service by flooding the system with reconfiguration requests. Our\napproach is modular and based on \\emph{dynamic lattice agreement} abstraction,\nand we discuss how to extend it to enable Byzantine fault-tolerant\nimplementations of a large class of reconfigurable replicated services.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 17:19:24 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 06:48:04 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Tonkikh", "Andrei", ""]]}, {"id": "2005.13583", "submitter": "Isabella Ziccardi", "authors": "Andrea Clementi, Emanuele Natale, Isabella Ziccardi", "title": "Parallel Load Balancing on Constrained Client-Server Topologies", "comments": null, "journal-ref": "SPAA 2020: 32nd ACM Symposium on Parallelism in Algorithms and\n  Architectures Proceedings", "doi": "10.1145/3350755.3400232", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study parallel \\emph{Load Balancing} protocols for a client-server\ndistributed model defined as follows.\n  There is a set $\\sC$ of $n$ clients and a set $\\sS$ of $n$ servers where each\nclient has\n  (at most) a constant number $d \\geq 1$ of requests that must be assigned to\nsome server. The client set and the server one are connected to each other via\na fixed bipartite graph: the requests of client $v$ can only be sent to the\nservers in its neighborhood $N(v)$. The goal is to assign every client request\nso as to minimize the maximum load of the servers.\n  In this setting, efficient parallel protocols are available only for dense\ntopolgies. In particular, a simple symmetric, non-adaptive protocol achieving\nconstant maximum load has been recently introduced by Becchetti et al\n\\cite{BCNPT18} for regular dense bipartite graphs. The parallel completion time\nis $\\bigO(\\log n)$ and the overall work is $\\bigO(n)$, w.h.p.\n  Motivated by proximity constraints arising in some client-server systems, we\ndevise a simple variant of Becchetti et al's protocol \\cite{BCNPT18} and we\nanalyse it over almost-regular bipartite graphs where nodes may have\nneighborhoods of small size. In detail, we prove that, w.h.p., this new version\nhas a cost equivalent to that of Becchetti et al's protocol (in terms of\nmaximum load, completion time, and work complexity, respectively) on every\nalmost-regular bipartite graph with degree $\\Omega(\\log^2n)$.\n  Our analysis significantly departs from that in \\cite{BCNPT18} for the\noriginal protocol and requires to cope with non-trivial stochastic-dependence\nissues on the random choices of the algorithmic process which are due to the\nworst-case, sparse topology of the underlying graph.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 18:21:54 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Clementi", "Andrea", ""], ["Natale", "Emanuele", ""], ["Ziccardi", "Isabella", ""]]}, {"id": "2005.13628", "submitter": "Neal E. Young", "authors": "Christos Koufogiannakis, Neal E. Young", "title": "Distributed algorithms for covering, packing and maximum weighted\n  matching", "comments": null, "journal-ref": "Distributed Computing 24, 45--63 (2011)", "doi": "10.1007/s00446-011-0127-7", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives poly-logarithmic-round, distributed D-approximation\nalgorithms for covering problems with submodular cost and monotone covering\nconstraints (Submodular-cost Covering). The approximation ratio D is the\nmaximum number of variables in any constraint. Special cases include Covering\nMixed Integer Linear Programs (CMIP), and Weighted Vertex Cover (with D=2). Via\nduality, the paper also gives poly-logarithmic-round, distributed\nD-approximation algorithms for Fractional Packing linear programs (where D is\nthe maximum number of constraints in which any variable occurs), and for Max\nWeighted c-Matching in hypergraphs (where D is the maximum size of any of the\nhyperedges; for graphs D=2). The paper also gives parallel (RNC)\n2-approximation algorithms for CMIP with two variables per constraint and\nWeighted Vertex Cover. The algorithms are randomized. All of the approximation\nratios exactly match those of comparable centralized algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:20:04 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Koufogiannakis", "Christos", ""], ["Young", "Neal E.", ""]]}, {"id": "2005.13632", "submitter": "Farzin Houshmand", "authors": "Farzin Houshmand, Mohsen Lesani and Keval Vora", "title": "GraFS: Graph Analytics Fusion and Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph analytics elicits insights from large graphs to inform critical\ndecisions for business, safety and security. Several large-scale graph\nprocessing frameworks feature efficient runtime systems; however, they often\nprovide programming models that are low-level and subtly different from each\nother. Therefore, end users can find implementation and specially optimization\nof graph analytics time-consuming and error-prone. This paper regards the\nabstract interface of the graph processing frameworks as the instruction set\nfor graph analytics, and presents Grafs, a high-level declarative specification\nlanguage for graph analytics and a synthesizer that automatically generates\nefficient code for five high-performance graph processing frameworks. It\nfeatures novel semantics-preserving fusion transformations that optimize the\nspecifications and reduce them to three primitives: reduction over paths,\nmapping over vertices and reduction over vertices. Reductions over paths are\ncommonly calculated based on push or pull models that iteratively apply kernel\nfunctions at the vertices. This paper presents conditions, parametric in terms\nof the kernel functions, for the correctness and termination of the iterative\nmodels, and uses these conditions as specifications to automatically synthesize\nthe kernel functions. Experimental results show that the generated code matches\nor outperforms hand-optimized code, and that fusion accelerates execution.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:27:05 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Houshmand", "Farzin", ""], ["Lesani", "Mohsen", ""], ["Vora", "Keval", ""]]}, {"id": "2005.13669", "submitter": "Ryan Chard", "authors": "Jakob R. Elias, Ryan Chard, Joseph A. Libera, Ian Foster, Santanu\n  Chaudhuri", "title": "The Manufacturing Data and Machine Learning Platform: Enabling Real-time\n  Monitoring and Control of Scientific Experiments via IoT", "comments": "Two page demonstration paper. Accepted to WFIoT2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT devices and sensor networks present new opportunities for measuring,\nmonitoring, and guiding scientific experiments. Sensors, cameras, and\ninstruments can be combined to provide previously unachievable insights into\nthe state of ongoing experiments. However, IoT devices can vary greatly in the\ntype, volume, and velocity of data they generate, making it challenging to\nfully realize this potential. Indeed, synergizing diverse IoT data streams in\nnear-real time can require the use of machine learning (ML). In addition, new\ntools and technologies are required to facilitate the collection, aggregation,\nand manipulation of sensor data in order to simplify the application of ML\nmodels and in turn, fully realize the utility of IoT devices in laboratories.\nHere we will demonstrate how the use of the Argonne-developed Manufacturing\nData and Machine Learning (MDML) platform can analyze and use IoT devices in a\nmanufacturing experiment. MDML is designed to standardize the research and\noperational environment for advanced data analytics and AI-enabled automated\nprocess optimization by providing the infrastructure to integrate AI in\ncyber-physical systems for in situ analysis. We will show that MDML is capable\nof processing diverse IoT data streams, using multiple computing resources, and\nintegrating ML models to guide an experiment.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 21:32:44 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Elias", "Jakob R.", ""], ["Chard", "Ryan", ""], ["Libera", "Joseph A.", ""], ["Foster", "Ian", ""], ["Chaudhuri", "Santanu", ""]]}, {"id": "2005.13685", "submitter": "Ameer Haj-Ali", "authors": "Ameer Haj-Ali, Hasan Genc, Qijing Huang, William Moses, John\n  Wawrzynek, Krste Asanovi\\'c, Ion Stoica", "title": "ProTuner: Tuning Programs with Monte Carlo Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore applying the Monte Carlo Tree Search (MCTS) algorithm in a\nnotoriously difficult task: tuning programs for high-performance deep learning\nand image processing. We build our framework on top of Halide and show that\nMCTS can outperform the state-of-the-art beam-search algorithm. Unlike beam\nsearch, which is guided by greedy intermediate performance comparisons between\npartial and less meaningful schedules, MCTS compares complete schedules and\nlooks ahead before making any intermediate scheduling decision. We further\nexplore modifications to the standard MCTS algorithm as well as combining real\nexecution time measurements with the cost model. Our results show that MCTS can\noutperform beam search on a suite of 16 real benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 22:25:10 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Haj-Ali", "Ameer", ""], ["Genc", "Hasan", ""], ["Huang", "Qijing", ""], ["Moses", "William", ""], ["Wawrzynek", "John", ""], ["Asanovi\u0107", "Krste", ""], ["Stoica", "Ion", ""]]}, {"id": "2005.13744", "submitter": "Konstantinos Psychas", "authors": "Konstantinos Psychas and Javad Ghaderi", "title": "A Theory of Auto-Scaling for Resource Reservation in Cloud Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed server system consisting of a large number of\nservers, each with limited capacity on multiple resources (CPU, memory, disk,\netc.). Jobs with different rewards arrive over time and require certain amounts\nof resources for the duration of their service. When a job arrives, the system\nmust decide whether to admit it or reject it, and if admitted, in which server\nto schedule the job. The objective is to maximize the expected total reward\nreceived by the system. This problem is motivated by control of cloud computing\nclusters, in which, jobs are requests for Virtual Machines or Containers that\nreserve resources for various services, and rewards represent service priority\nof requests or price paid per time unit of service by clients. We study this\nproblem in an asymptotic regime where the number of servers and jobs' arrival\nrates scale by a factor $L$, as $L$ becomes large. We propose a resource\nreservation policy that asymptotically achieves at least $1/2$, and under\ncertain monotone property on jobs' rewards and resources, at least $1-1/e$ of\nthe optimal expected reward. The policy automatically scales the number of VM\nslots for each job type as the demand changes, and decides in which servers the\nslots should be created in advance, without the knowledge of traffic rates. It\neffectively tracks a low-complexity greedy packing of existing jobs in the\nsystem while maintaining only a small number, $g(L)=\\omega(\\log L)$, of\nreserved VM slots for high priority jobs that pack well.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 02:31:09 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Psychas", "Konstantinos", ""], ["Ghaderi", "Javad", ""]]}, {"id": "2005.13789", "submitter": "Yangzihao Wang", "authors": "Wanjing Wei, Yangzihao Wang, Pin Gao, Shijie Sun and Donghai Yu", "title": "A Distributed Multi-GPU System for Large-Scale Node Embedding at Tencent", "comments": "Submitted to IEEE Cluster 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scaling node embedding systems to efficiently process networks in real-world\napplications that often contain hundreds of billions of edges with\nhigh-dimension node features remains a challenging problem. In this paper we\npresent a high-performance multi-GPU node embedding system that uses hybrid\nmodel data parallel training. We propose a hierarchical data partitioning\nstrategy and an embedding training pipeline to optimize both communication and\nmemory usage on a GPU cluster. With the decoupled design of our random walk\nengine and embedding training engine, we can run both random walk and embedding\ntraining with high flexibility to fully utilize all computing resources on a\nGPU cluster. We evaluate the system on real-world and synthesized networks with\nvarious node embedding tasks. Using 40 NVIDIA V100 GPUs on a network with over\ntwo hundred billion edges and one billion nodes, our implementation requires\nonly 200 seconds to finish one training epoch. We also achieve 5.9x-14.4x\nspeedup on average over the current state-of-the-art multi-GPU single-node\nembedding system with competitive or better accuracy on open datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 05:54:38 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 04:36:17 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Wei", "Wanjing", ""], ["Wang", "Yangzihao", ""], ["Gao", "Pin", ""], ["Sun", "Shijie", ""], ["Yu", "Donghai", ""]]}, {"id": "2005.13823", "submitter": "Behnam Pourghassemi", "authors": "Behnam Pourghassemi (1), Chenghao Zhang (1), Joo Hwan Lee (2), Aparna\n  Chandramowlishwaran (1) ((1) University of California, Irvine, (2) Samsung\n  Semiconductor)", "title": "Brief Announcement: On the Limits of Parallelizing Convolutional Neural\n  Networks on GPUs", "comments": "3 pages, 1 figure, to be published in Proceedings of the 32nd ACM\n  Symposium on Parallelism in Algorithms and Architectures (SPAA '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are currently the platform of choice for training neural networks.\nHowever, training a deep neural network (DNN) is a time-consuming process even\non GPUs because of the massive number of parameters that have to be learned. As\na result, accelerating DNN training has been an area of significant research in\nthe last couple of years.\n  While earlier networks such as AlexNet had a linear dependency between layers\nand operations, state-of-the-art networks such as ResNet, PathNet, and\nGoogleNet have a non-linear structure that exhibits a higher level of\ninter-operation parallelism. However, popular deep learning (DL) frameworks\nsuch as TensorFlow and PyTorch launch the majority of neural network\noperations, especially convolutions, serially on GPUs and do not exploit this\ninter-op parallelism. In this brief announcement, we make a case for the need\nand potential benefit of exploiting this rich parallelism in state-of-the-art\nnon-linear networks for reducing the training time. We identify the challenges\nand limitations in enabling concurrent layer execution on GPU backends (such as\ncuDNN) of DL frameworks and propose potential solutions.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 07:51:22 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Pourghassemi", "Behnam", ""], ["Zhang", "Chenghao", ""], ["Lee", "Joo Hwan", ""], ["Chandramowlishwaran", "Aparna", ""]]}, {"id": "2005.13896", "submitter": "Peter Hillmann", "authors": "Peter Hillmann and Tobias Uhlig and Gabi Dreo Rodosek and Oliver Rose", "title": "Simulation and Optimization of Content Delivery Networks considering\n  User Profiles and Preferences of Internet Service Providers", "comments": null, "journal-ref": "Winter Simulation Conference 2016", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Content Delivery Network (CDN) is a dynamic and complex service system. It\ncauses a huge amount of traffic on the network infrastructure of Internet\nService Providers (ISPs). Oftentimes, CDN providers and ISPs struggle to find\nan efficient and appropriate way to cooperate for mutual benefits. This\nchallenge is key to push the quality of service (QoS) for the end-user. We\nmodel, simulate, and optimize the behavior of a CDN to provide cooperative\nsolutions and to improve the QoS. Therefor, we determine reasonable server\nlocations, balance the amount of servers and improve the user assignments to\nthe servers. These aspects influence run time effects like caching at the\nserver, response time and network load at specific links. Especially, user\nrequest history and profiles are considered to improve the overall performance.\nSince we consider multiple objectives, we aim to provide a diverse set of\npareto optimal solutions using simulation based optimization.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 10:40:16 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hillmann", "Peter", ""], ["Uhlig", "Tobias", ""], ["Rodosek", "Gabi Dreo", ""], ["Rose", "Oliver", ""]]}, {"id": "2005.13905", "submitter": "Peter Hillmann", "authors": "Peter Hillmann, Tobias Uhlig, Gabi Dreo Rodosek, Oliver Rose", "title": "Modeling the Location Selection of Mirror Servers in Content Delivery\n  Networks", "comments": "Conference on Services Computing 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a provider of a Content Delivery Network (CDN), the location selection of\nmirror servers is a complex optimization problem. Generally, the objective is\nto place the nodes centralized such that all customers have convenient access\nto the service according to their demands. It is an instance of the k-center\nproblem, which is proven to be NP-hard. Determining reasonable server locations\ndirectly influences run time effects and future service costs. We model,\nsimulate, and optimize the properties of a content delivery network.\nSpecifically, considering the server locations in a network infrastructure with\nprioritized customers and weighted connections. A simulation model for the\nservers is necessary to analyze the caching behavior in accordance to the\ntargeted customer requests. We analyze the problem and compare different\noptimization strategies. For our simulation, we employ various realistic\nscenarios and evaluate several performance indicators. Our new optimization\napproach shows a significant improvement. The presented results are generally\napplicable to other domains with k-center problems, e.g., the placement of\nmilitary bases, the planning and placement of facility locations, or data\nmining.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 11:06:57 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hillmann", "Peter", ""], ["Uhlig", "Tobias", ""], ["Rodosek", "Gabi Dreo", ""], ["Rose", "Oliver", ""]]}, {"id": "2005.14038", "submitter": "Jay H. Park", "authors": "Jay H. Park, Gyeongchan Yun, Chang M. Yi, Nguyen T. Nguyen, Seungmin\n  Lee, Jaesik Choi, Sam H. Noh, Young-ri Choi", "title": "HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU\n  Clusters through Integration of Pipelined Model Parallelism and Data\n  Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) models have continuously been growing in size in\norder to improve the accuracy and quality of the models. Moreover, for training\nof large DNN models, the use of heterogeneous GPUs is inevitable due to the\nshort release cycle of new GPU architectures. In this paper, we investigate how\nto enable training of large DNN models on a heterogeneous GPU cluster that\npossibly includes whimpy GPUs that, as a standalone, could not be used for\ntraining. We present a DNN training system, HetPipe (Heterogeneous Pipeline),\nthat integrates pipelined model parallelism (PMP) with data parallelism (DP).\nIn HetPipe, a group of multiple GPUs, called a virtual worker, processes\nminibatches in a pipelined manner, and multiple such virtual workers employ\ndata parallelism for higher performance. We also propose a novel parameter\nsynchronization model, which we refer to as Wave Synchronous Parallel (WSP) to\naccommodate both PMP and DP for virtual workers, and provide convergence proof\nof WSP. Our experimental results on a given heterogeneous setting show that\nwith HetPipe, DNN models converge up to 49% faster compared to the\nstate-of-the-art DP technique.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:12:27 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Park", "Jay H.", ""], ["Yun", "Gyeongchan", ""], ["Yi", "Chang M.", ""], ["Nguyen", "Nguyen T.", ""], ["Lee", "Seungmin", ""], ["Choi", "Jaesik", ""], ["Noh", "Sam H.", ""], ["Choi", "Young-ri", ""]]}, {"id": "2005.14080", "submitter": "Paolo Torroni", "authors": "Daniela Loreti and Marco Lippi and Paolo Torroni", "title": "Parallelizing Machine Learning as a Service for the End-User", "comments": null, "journal-ref": "Future Generation Computer Systems 105 (2020) 275-286", "doi": "10.1016/j.future.2019.11.042", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As ML applications are becoming ever more pervasive, fully-trained systems\nare made increasingly available to a wide public, allowing end-users to submit\nqueries with their own data, and to efficiently retrieve results. With\nincreasingly sophisticated such services, a new challenge is how to scale up to\nevergrowing user bases. In this paper, we present a distributed architecture\nthat could be exploited to parallelize a typical ML system pipeline. We propose\na case study consisting of a text mining service and discuss how the method can\nbe generalized to many similar applications. We demonstrate the significance of\nthe computational gain boosted by the distributed architecture by way of an\nextensive experimental evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:22:50 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 09:13:36 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Loreti", "Daniela", ""], ["Lippi", "Marco", ""], ["Torroni", "Paolo", ""]]}, {"id": "2005.14150", "submitter": "Yishai Oltchik", "authors": "Yishai Oltchik (ETH Zurich) and Oded Schwartz (Hebrew University of\n  Jerusalem)", "title": "Network Partitioning and Avoidable Contention", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network contention frequently dominates the run time of parallel algorithms\nand limits scaling performance. Most previous studies mitigate or eliminate\ncontention by utilizing one of several approaches: communication-minimizing\nalgorithms; hotspot-avoiding routing schemes; topology-aware task mapping; or\nimproving global network properties, such as bisection bandwidth,\nedge-expansion, partitioning, and network diameter. In practice, parallel jobs\noften use only a fraction of a host system. How do processor allocation\npolicies affect contention within a partition? We utilize edge-isoperimetric\nanalysis of network graphs to determine whether a network partition has optimal\ninternal bisection. Increasing the bisection allows a more efficient use of the\nnetwork resources, decreasing or completely eliminating the link contention. We\nfirst study torus networks and characterize partition geometries that maximize\ninternal bisection bandwidth. We examine the allocation policies of Mira and\nJUQUEEN, the two largest publicly-accessible Blue Gene/Q torus-based\nsupercomputers. Our analysis demonstrates that the bisection bandwidth of their\ncurrent partitions can often be improved by changing the partitions'\ngeometries. These can yield up to a X2 speedup for contention-bound workloads.\nBenchmarking experiments validate the predictions. Our analysis applies to\nallocation policies of other networks.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 17:10:32 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Oltchik", "Yishai", "", "ETH Zurich"], ["Schwartz", "Oded", "", "Hebrew University of\n  Jerusalem"]]}, {"id": "2005.14282", "submitter": "Rafael Belchior", "authors": "Rafael Belchior, Andr\\'e Vasconcelos, S\\'ergio Guerreiro, Miguel\n  Correia", "title": "A Survey on Blockchain Interoperability: Past, Present, and Future\n  Trends", "comments": "For any comments or suggestions, contact rafael.belchior AT\n  t\\'ecnico.ulisboa.pt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain interoperability is emerging as one of the crucial features of\nblockchain technology, but the knowledge necessary for achieving it is\nfragmented. This fact makes it challenging for academics and the industry to\nseamlessly achieve interoperability among blockchains.\n  Given the novelty and potential of this new domain, we conduct a literature\nreview on blockchain interoperability, by collecting 262 papers, and 70 grey\nliterature documents, constituting a corpus of 332 documents. From those 332\ndocuments, we systematically analyzed and discussed 80 documents, including\nboth peer-reviewed papers and grey literature.\n  Our review classifies studies in three categories: Cryptocurrency-directed\ninteroperability approaches, Blockchain Engines, and Blockchain Connectors.\nEach category is further divided into sub-categories based on defined criteria.\nWe discuss not only studies within each category and subcategory but also\nacross categories, providing a holistic overview of blockchain\ninteroperability, paving the way for systematic research in this domain. Our\nfindings show that blockchain interoperability has a much broader spectrum than\ncryptocurrencies.\n  The present survey leverages an interesting approach: we systematically\ncontacted the authors of grey literature papers and industry solutions to\nobtain an updated view of their work. Finally, this paper discusses supporting\ntechnologies, standards, use cases, open challenges, and provides several\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 20:36:21 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 12:35:09 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 12:54:02 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Belchior", "Rafael", ""], ["Vasconcelos", "Andr\u00e9", ""], ["Guerreiro", "S\u00e9rgio", ""], ["Correia", "Miguel", ""]]}, {"id": "2005.14410", "submitter": "Niklas K\\\"uhl", "authors": "Lucia Schuler and Somaya Jamil and Niklas K\\\"uhl", "title": "AI-based Resource Allocation: Reinforcement Learning for Adaptive\n  Auto-scaling in Serverless Environments", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has emerged as a compelling new paradigm of cloud\ncomputing models in recent years. It promises the user services at large scale\nand low cost while eliminating the need for infrastructure management. On cloud\nprovider side, flexible resource management is required to meet fluctuating\ndemand. It can be enabled through automated provisioning and deprovisioning of\nresources. A common approach among both commercial and open source serverless\ncomputing platforms is workload-based auto-scaling, where a designated\nalgorithm scales instances according to the number of incoming requests. In the\nrecently evolving serverless framework Knative a request-based policy is\nproposed, where the algorithm scales resources by a configured maximum number\nof requests that can be processed in parallel per instance, the so-called\nconcurrency. As we show in a baseline experiment, this predefined concurrency\nlevel can strongly influence the performance of a serverless application.\nHowever, identifying the concurrency configuration that yields the highest\npossible quality of service is a challenging task due to various factors, e.g.\nvarying workload and complex infrastructure characteristics, influencing\nthroughput and latency. While there has been considerable research into\nintelligent techniques for optimizing auto-scaling for virtual machine\nprovisioning, this topic has not yet been discussed in the area of serverless\ncomputing. For this reason, we investigate the applicability of a reinforcement\nlearning approach, which has been proven on dynamic virtual machine\nprovisioning, to request-based auto-scaling in a serverless framework. Our\nresults show that within a limited number of iterations our proposed model\nlearns an effective scaling policy per workload, improving the performance\ncompared to the default auto-scaling configuration.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 06:18:39 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Schuler", "Lucia", ""], ["Jamil", "Somaya", ""], ["K\u00fchl", "Niklas", ""]]}, {"id": "2005.14469", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Qiang Wang, Xiaowen Chu", "title": "Efficient Sparse-Dense Matrix-Matrix Multiplication on GPUs Using the\n  Customized Sparse Storage Format", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication of a sparse matrix to a dense matrix (SpDM) is widely used in\nmany areas like scientific computing and machine learning. However, existing\nworks under-look the performance optimization of SpDM on modern many-core\narchitectures like GPUs. The storage data structures help sparse matrices store\nin a memory-saving format, but they bring difficulties in optimizing the\nperformance of SpDM on modern GPUs due to irregular data access of the sparse\nstructure, which results in lower resource utilization and poorer performance.\nIn this paper, we refer to the roofline performance model of GPUs to design an\nefficient SpDM algorithm called GCOOSpDM, in which we exploit coalescent global\nmemory access, fast shared memory reuse and more operations per byte of global\nmemory traffic. Experiments are evaluated on three Nvidia GPUs (i.e., GTX 980,\nGTX Titan X Pascal and Tesla P100) with CUDA-8.0 using a large number of\nmatrices including a public dataset and randomly generated matrices.\nExperimental results show that GCOOSpDM achieves 1.5-8$\\times$ speedup over\nNvidia's library cuSPARSE in many matrices. We also analyze instruction-level\noperations on a particular GPU to understand the performance gap between\nGCOOSpDM and cuSPARSE. The profiled instructions confirm that cuSPARSE spends a\nlot of time on slow memory access (including DRAM access and L2 cache access),\nwhile GCOOSpDM transfers such slow memory access to faster shared memory, which\nmainly contributes to the performance gain. Results also show that GCOOSpDM\nwould outperform the dense algorithm (cuBLAS) with lower sparsity than cuSPARSE\non GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 09:36:01 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Shi", "Shaohuai", ""], ["Wang", "Qiang", ""], ["Chu", "Xiaowen", ""]]}]