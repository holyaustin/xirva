[{"id": "1404.0027", "submitter": "Denis Mestivier", "authors": "Neri Mickael and Denis Mestivier", "title": "An efficient GPU acceptance-rejection algorithm for the selection of the\n  next reaction to occur for Stochastic Simulation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The Stochastic Simulation Algorithm (SSA) has largely diffused in\nthe field of systems biology. This approach needs many realizations for\nestablishing statistical results on the system under study. It is very\ncomputationnally demanding, and with the advent of large models this burden is\nincreasing. Hence parallel implementation of SSA are needed to address these\nneeds.\n  At the very heart of the SSA is the selection of the next reaction to occur\nat each time step, and to the best of our knowledge all implementations are\nbased on an inverse transformation method. However, this method involves a\nrandom number of steps to select this next reaction and is poorly amenable to a\nparallel implementation.\n  Results: Here, we introduce a parallel acceptance-rejection algorithm to\nselect the K next reactions to occur. This algorithm uses a deterministic\nnumber of steps, a property well suited to a parallel implementation. It is\nsimple and small, accurate and scalable. We propose a Graphics Processing Unit\n(GPU) implementation and validate our algorithm with simulated propensity\ndistributions and the propensity distribution of a large model of yeast iron\nmetabolism. We show that our algorithm can handle thousands of selections of\nnext reaction to occur in parallel on the GPU, paving the way to massive SSA.\n  Availability: We present our GPU-AR algorithm that focuses on the very heart\nof the SSA. We do not embed our algorithm within a full implementation in order\nto stay pedagogical and allows its rapid implementation in existing software.\nWe hope that it will enable stochastic modelers to implement our algorithm with\nthe benefits of their own optimizations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 11:11:20 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Mickael", "Neri", ""], ["Mestivier", "Denis", ""]]}, {"id": "1404.0076", "submitter": "EPTCS", "authors": "Eugen Jiresch (Technische Universit\\\"at Wien)", "title": "Towards a GPU-based implementation of interaction nets", "comments": "In Proceedings DCM 2012, arXiv:1403.7579", "journal-ref": "EPTCS 143, 2014, pp. 41-53", "doi": "10.4204/EPTCS.143.4", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ingpu, a GPU-based evaluator for interaction nets that heavily\nutilizes their potential for parallel evaluation. We discuss advantages and\nchallenges of the ongoing implementation of ingpu and compare its performance\nto existing interaction nets evaluators.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:37:24 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Jiresch", "Eugen", "", "Technische Universit\u00e4t Wien"]]}, {"id": "1404.0085", "submitter": "EPTCS", "authors": "Carlos Alberto Ram\\'irez Restrepo, Jorge A. P\\'erez, Jes\\'us Aranda,\n  Juan Francisco D\\'iaz-Frias", "title": "Towards Formal Interaction-Based Models of Grid Computing\n  Infrastructures", "comments": "In Proceedings DCM 2013, arXiv:1403.7685", "journal-ref": "EPTCS 144, 2014, pp. 57-72", "doi": "10.4204/EPTCS.144.5", "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid computing (GC) systems are large-scale virtual machines, built upon a\nmassive pool of resources (processing time, storage, software) that often span\nmultiple distributed domains. Concurrent users interact with the grid by adding\nnew tasks; the grid is expected to assign resources to tasks in a fair,\ntrustworthy way. These distinctive features of GC systems make their\nspecification and verification a challenging issue. Although prior works have\nproposed formal approaches to the specification of GC systems, a precise\naccount of the interaction model which underlies resource sharing has not been\nyet proposed. In this paper, we describe ongoing work aimed at filling in this\ngap. Our approach relies on (higher-order) process calculi: these core\nlanguages for concurrency offer a compositional framework in which GC systems\ncan be precisely described and potentially reasoned about.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:39:11 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Restrepo", "Carlos Alberto Ram\u00edrez", ""], ["P\u00e9rez", "Jorge A.", ""], ["Aranda", "Jes\u00fas", ""], ["D\u00edaz-Frias", "Juan Francisco", ""]]}, {"id": "1404.0447", "submitter": "Lin Lin", "authors": "Mathias Jacquelin, Lin Lin, Chao Yang", "title": "PSelInv -- A Distributed Memory Parallel Algorithm for Selected\n  Inversion : the Symmetric Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an efficient parallel implementation of the selected inversion\nalgorithm for distributed memory computer systems, which we call\n\\texttt{PSelInv}. The \\texttt{PSelInv} method computes selected elements of a\ngeneral sparse matrix $A$ that can be decomposed as $A = LU$, where $L$ is\nlower triangular and $U$ is upper triangular. The implementation described in\nthis paper focuses on the case of sparse symmetric matrices. It contains an\ninterface that is compatible with the distributed memory parallel sparse direct\nfactorization \\texttt{SuperLU\\_DIST}. However, the underlying data structure\nand design of \\texttt{PSelInv} allows it to be easily combined with other\nfactorization routines such as \\texttt{PARDISO}. We discuss general\nparallelization strategies such as data and task distribution schemes. In\nparticular, we describe how to exploit the concurrency exposed by the\nelimination tree associated with the $LU$ factorization of $A$. We demonstrate\nthe efficiency and accuracy of \\texttt{PSelInv} by presenting a number of\nnumerical experiments. In particular, we show that \\texttt{PSelInv} can run\nefficiently on more than $4,000$ cores for a modestly sized matrix. We also\ndemonstrate how \\texttt{PSelInv} can be used to accelerate large-scale\nelectronic structure calculations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 03:48:45 GMT"}, {"version": "v2", "created": "Sat, 27 Dec 2014 17:54:15 GMT"}, {"version": "v3", "created": "Thu, 28 May 2015 21:43:52 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Jacquelin", "Mathias", ""], ["Lin", "Lin", ""], ["Yang", "Chao", ""]]}, {"id": "1404.0743", "submitter": "Geoffrey Irving", "authors": "Geoffrey Irving", "title": "Pentago is a First Player Win: Strongly Solving a Game Using Parallel\n  In-Core Retrograde Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present a strong solution of the board game pentago, computed using\nexhaustive parallel retrograde analysis in 4 hours on 98304 ($3 \\times 2^{15}$)\nthreads of NERSC's Cray Edison. At $3.0 \\times 10^{15}$ states, pentago is the\nlargest divergent game solved to date by two orders of magnitude, and the only\nexample of a nontrivial divergent game solved using retrograde analysis. Unlike\nprevious retrograde analyses, our computation was performed entirely in-core,\nwriting only a small portion of the results to disk; an out-of-core\nimplementation would have been much slower. Symmetry was used to reduce\nbranching factor and exploit instruction level parallelism. Despite a\ntheoretically embarrassingly parallel structure, asynchronous message passing\nwas required to fit the computation into available RAM, causing latency\nproblems on an older Cray machine. All code and data for the project are open\nsource, together with a website which combines database lookup and on-the-fly\ncomputation to interactively explore the strong solution.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 01:01:21 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 00:47:00 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Irving", "Geoffrey", ""]]}, {"id": "1404.0774", "submitter": "Md. Enamul Haque", "authors": "Md. Enamul Haque, Abdullah Al Kaisan, Mahmudur R Saniat, and Aminur\n  Rahman", "title": "GPU Accelerated Fractal Image Compression for Medical Imaging in\n  Parallel Computing Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we implemented both sequential and parallel version of fractal\nimage compression algorithms using CUDA (Compute Unified Device Architecture)\nprogramming model for parallelizing the program in Graphics Processing Unit for\nmedical images, as they are highly similar within the image itself. There are\nseveral improvement in the implementation of the algorithm as well. Fractal\nimage compression is based on the self similarity of an image, meaning an image\nhaving similarity in majority of the regions. We take this opportunity to\nimplement the compression algorithm and monitor the effect of it using both\nparallel and sequential implementation. Fractal compression has the property of\nhigh compression rate and the dimensionless scheme. Compression scheme for\nfractal image is of two kind, one is encoding and another is decoding. Encoding\nis very much computational expensive. On the other hand decoding is less\ncomputational. The application of fractal compression to medical images would\nallow obtaining much higher compression ratios. While the fractal magnification\nan inseparable feature of the fractal compression would be very useful in\npresenting the reconstructed image in a highly readable form. However, like all\nirreversible methods, the fractal compression is connected with the problem of\ninformation loss, which is especially troublesome in the medical imaging. A\nvery time consuming encoding pro- cess, which can last even several hours, is\nanother bothersome drawback of the fractal compression.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 06:27:12 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Haque", "Md. Enamul", ""], ["Kaisan", "Abdullah Al", ""], ["Saniat", "Mahmudur R", ""], ["Rahman", "Aminur", ""]]}, {"id": "1404.0780", "submitter": "Bernhard Haeupler", "authors": "Mohsen Ghaffari, Bernhard Haeupler, Majid Khabbazian", "title": "Randomized Broadcast in Radio Networks with Collision Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized distributed algorithm that in radio networks with\ncollision detection broadcasts a single message in $O(D + \\log^6 n)$ rounds,\nwith high probability. This time complexity is most interesting because of its\noptimal additive dependence on the network diameter $D$. It improves over the\ncurrently best known $O(D\\log\\frac{n}{D}\\,+\\,\\log^2 n)$ algorithms, due to\nCzumaj and Rytter [FOCS 2003], and Kowalski and Pelc [PODC 2003]. These\nalgorithms where designed for the model without collision detection and are\noptimal in that model. However, as explicitly stated by Peleg in his 2007\nsurvey on broadcast in radio networks, it had remained an open question whether\nthe bound can be improved with collision detection.\n  We also study distributed algorithms for broadcasting $k$ messages from a\nsingle source to all nodes. This problem is a natural and important\ngeneralization of the single-message broadcast problem, but is in fact\nconsiderably more challenging and less understood. We show the following\nresults: If the network topology is known to all nodes, then a $k$-message\nbroadcast can be performed in $O(D + k\\log n + \\log^2 n)$ rounds, with high\nprobability. If the topology is not known, but collision detection is\navailable, then a $k$-message broadcast can be performed in $O(D + k\\log n +\n\\log^6 n)$ rounds, with high probability. The first bound is optimal and the\nsecond is optimal modulo the additive $O(\\log^6 n)$ term.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 07:07:56 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Haeupler", "Bernhard", ""], ["Khabbazian", "Majid", ""]]}, {"id": "1404.1124", "submitter": "Yong Wang", "authors": "Kai Li, Yong Wang, Meilin Liu", "title": "A Task Allocation Schema Based on Response Time Optimization in Cloud\n  Computing", "comments": "arXiv admin note: substantial text overlap with arXiv:1403.5012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a newly emerging distributed computing which is evolved\nfrom Grid computing. Task scheduling is the core research of cloud computing\nwhich studies how to allocate the tasks among the physical nodes so that the\ntasks can get a balanced allocation or each task's execution cost decreases to\nthe minimum or the overall system performance is optimal. Unlike the previous\ntask slices' sequential execution of an independent task in the model of which\nthe target is processing time, we build a model that targets at the response\ntime, in which the task slices are executed in parallel. Then we give its\nsolution with a method based on an improved adjusting entropy function. At\nlast, we design a new task scheduling algorithm. Experimental results show that\nthe response time of our proposed algorithm is much lower than the\ngame-theoretic algorithm and balanced scheduling algorithm and compared with\nthe balanced scheduling algorithm, game-theoretic algorithm is not necessarily\nsuperior in parallel although its objective function value is better.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 01:24:25 GMT"}, {"version": "v2", "created": "Fri, 18 Apr 2014 03:24:18 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Li", "Kai", ""], ["Wang", "Yong", ""], ["Liu", "Meilin", ""]]}, {"id": "1404.1328", "submitter": "Da Wang", "authors": "Da Wang, Gauri Joshi, Gregory Wornell", "title": "Efficient Task Replication for Fast Response Times in Parallel\n  Computation", "comments": "Extended version of the 2-page paper accepted to ACM SIGMETRICS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One typical use case of large-scale distributed computing in data centers is\nto decompose a computation job into many independent tasks and run them in\nparallel on different machines, sometimes known as the \"embarrassingly\nparallel\" computation. For this type of computation, one challenge is that the\ntime to execute a task for each machine is inherently variable, and the overall\nresponse time is constrained by the execution time of the slowest machine. To\naddress this issue, system designers introduce task replication, which sends\nthe same task to multiple machines, and obtains result from the machine that\nfinishes first. While task replication reduces response time, it usually\nincreases resource usage. In this work, we propose a theoretical framework to\nanalyze the trade-off between response time and resource usage. We show that,\nwhile in general, there is a tension between response time and resource usage,\nthere exist scenarios where replicating tasks judiciously reduces completion\ntime and resource usage simultaneously. Given the execution time distribution\nfor machines, we investigate the conditions for a scheduling policy to achieve\noptimal performance trade-off, and propose efficient algorithms to search for\noptimal or near-optimal scheduling policies. Our analysis gives insights on\nwhen and why replication helps, which can be used to guide scheduler design in\nlarge-scale distributed computing systems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 18:05:45 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Wang", "Da", ""], ["Joshi", "Gauri", ""], ["Wornell", "Gregory", ""]]}, {"id": "1404.1814", "submitter": "George Lestaris", "authors": "G. Lestaris, I. Charalampidis, D. Berzano, J. Blomer, P. Buncic, G.\n  Ganis, R. Meusel", "title": "CernVM Online and Cloud Gateway: a uniform interface for CernVM\n  contextualization and deployment", "comments": "Conference paper at the 2013 Computing in High Energy Physics (CHEP)\n  Conference, Amsterdam", "journal-ref": null, "doi": "10.1088/1742-6596/513/3/032055", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a virtualized environment, contextualization is the process of configuring\na VM instance for the needs of various deployment use cases. Contextualization\nin CernVM can be done by passing a handwritten context to the user data field\nof cloud APIs, when running CernVM on the cloud, or by using CernVM web\ninterface when running the VM locally. CernVM Online is a publicly accessible\nweb interface that unifies these two procedures. A user is able to define,\nstore and share CernVM contexts using CernVM Online and then apply them either\nin a cloud by using CernVM Cloud Gateway or on a local VM with the single-step\npairing mechanism. CernVM Cloud Gateway is a distributed system that provides a\nsingle interface to use multiple and different clouds (by location or type,\nprivate or public). Cloud gateway has been so far integrated with OpenNebula,\nCloudStack and EC2 tools interfaces. A user, with access to a number of clouds,\ncan run CernVM cloud agents that will communicate with these clouds using their\ninterfaces, and then use one single interface to deploy and scale CernVM\nclusters. CernVM clusters are defined in CernVM Online and consist of a set of\nCernVM instances that are contextualized and can communicate with each other.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 15:11:38 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Lestaris", "G.", ""], ["Charalampidis", "I.", ""], ["Berzano", "D.", ""], ["Blomer", "J.", ""], ["Buncic", "P.", ""], ["Ganis", "G.", ""], ["Meusel", "R.", ""]]}, {"id": "1404.1836", "submitter": "Yazad Baldawala", "authors": "Sagar Tirodkar, Yazad Baldawala, Sagar Ulane, Ashok Jori", "title": "Improved 3-Dimensional Security in Cloud Computing", "comments": "6 Pages, 10 Figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V9(5):242-247, March 2014. Published by Seventh Sense Research Group", "doi": "10.14445/22312803/IJCTT-V9P145", "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a trending technology in the field of Information\nTechnology as it allows sharing of resources over a network. The reason Cloud\ncomputing gained traction so rapidly was because of its performance,\navailability and low cost among other features. Besides these features,\ncompanies are still refraining from binding their business with cloud computing\ndue to the fear of data leakage. The focus of this paper is on the problem of\ndata leakage. It proposes a framework which works in two phases. The first\nphase consists of data encryption and classification which is performed before\nstoring the data. In this phase, the client may want to encrypt his data prior\nto uploading. After encryption, data is classified using three parameters\nnamely Confidentiality [C], Integrity [I] and Availability [A]. With the help\nof proposed algorithm, criticality rating (Cr) of the data is calculated.\nAccording to the Cr, security will be provided on the basis of the 3 Dimensions\nproposed in this paper. The second phase consists of data retrieval by the\nclient. As per the concept of 3D, users who want to access their data need to\nbe authenticated, to avoid data from being compromised. Before every access to\ndata, the users identity is verified for authorization. After the user is\nauthorized for data access, if the data is encrypted, the user can decrypt the\nsame.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 16:25:15 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Tirodkar", "Sagar", ""], ["Baldawala", "Yazad", ""], ["Ulane", "Sagar", ""], ["Jori", "Ashok", ""]]}, {"id": "1404.2076", "submitter": "Saloni Jain", "authors": "Dr. Amit Agarwal, Saloni Jain", "title": "Efficient Optimal Algorithm of Task Scheduling in Cloud Computing\n  Environment", "comments": "6,1. published in IJCTT 2014 mARCH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is an emerging technology in distributed computing which\nfacilitates pay per model as per user demand and requirement.Cloud consist of a\ncollection of virtual machine which includes both computational and storage\nfacility. The primary aim of cloud computing is to provide efficient access to\nremote and geographically distributed resources. Cloud is developing day by day\nand faces many challenges, one of them is scheduling. Scheduling refers to a\nset of policies to control the order of work to be performed by a computer\nsystem. A good scheduler adapts its scheduling strategy according to the\nchanging environment and the type of task. In this research paper we presented\na Generalized Priority algorithm for efficient execution of task and comparison\nwith FCFS and Round Robin Scheduling. Algorithm should be tested in cloud Sim\ntoolkit and result shows that it gives better performance compared to other\ntraditional scheduling algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 10:21:03 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Agarwal", "Dr. Amit", ""], ["Jain", "Saloni", ""]]}, {"id": "1404.2157", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev, Ramon Alvarez", "title": "Leveraging VMware vCloud Director Virtual Applications (vApps) for\n  Operational Expense (OpEx) Efficiency", "comments": null, "journal-ref": "World of Computer Science and Information Technology Journal\n  (WCSIT)ISSN: 2221-0741 Vol. 3, No. 9, 156-163, 2013", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization technology has provided many benefits to organizations, but it\ncannot provide automation. This causes operational expenditure (OpEx)\ninefficiencies, which are solved by cloud computing (vCloud Director vApps).\nOrganizations have adopted virtualization technology to reduce IT costs and\nmeet business needs. In addition to improved CapEx efficiency, virtualization\nhas enabled organizations to respond to business needs faster. While\nvirtualization has dramatically optimized core IT infrastructures,\norganizations struggle to reduce OpEx costs. Because virtualization only\naddresses server consolidation, administrators are faced with the manual and\nresource-intensive day-to-day tasks of managing the rest of the data center:\nnetworking, storage, user management. This manuscript presents details on how\nleverage vApps based on a virtualized platform to improve CapEx efficiency in\ntoday s data center. The combination of virtualization and cloud computing can\ntransform the data center into a dynamic, scalable, and agile resource capable\nof achieving significant CapEx and OpEx cost savings.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 14:41:51 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Mirzoev", "Dr. Timur", ""], ["Alvarez", "Ramon", ""]]}, {"id": "1404.2167", "submitter": "Timur Mirzoev", "authors": "Timur Mirzoev", "title": "Employing Virtualization for Information Technology Education", "comments": "Technology Interface International Journal, Vol 12. No.1 Fall Winter\n  2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript presents teaching and curriculum design for Information\nTechnology classes. Today, students demand hands-on activities for the newest\ntechnologies. It is feasible to satisfy this appetite for exciting education by\nemploying server virtualization technologies to teach advanced concepts with\nextensive hands-on assignments. Through utilization of virtualized servers,\nstudents are able to deploy, secure and manage virtual machines and networks in\na contained environment. Various techniques, assessment tools and experiences\nwill be analyzed and presented by this manuscript. Previous teaching cases for\nInformation Systems or Information Technology classes are done using\nnon-commercial products, such as free VMware Server or VMware Player. Such\nproducts have very limited functionality in terms of networking, storage and\nresource management. Several advanced datacenter functions, such as Distributed\nPower Management (DPM), vMotion and others, are not available in desktop\nversions of that type of virtualization software. This manuscript introduces\nthe utilization of commercial software, such as vSphere 4.1, with full\ndatacenter functionality and operations for teaching Information Technology\nclasses of various levels.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 14:56:27 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Mirzoev", "Timur", ""]]}, {"id": "1404.2174", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev, Dr. Baijian Yang", "title": "Securing Virtualized Datacenters", "comments": null, "journal-ref": "International Journal Of Engineering Research and Innovation |\n  Vol. 2, No. 1, Spring 2010", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization is a very popular solution to many problems in datacenter\nmanagement. It offers increased utilization of existing system resources\nthrough effective consolidation, negating the need for more servers and\nadditional rack space. Furthermore, it offers essential capabilities in terms\nof disaster recovery and potential savings on energy and maintenance costs.\nHowever, these benefits may be tempered by the increased complexities of\nsecuring virtual infrastructure. Do the benefits of virtualization outweigh the\nrisks? In this study, the authors evaluated the functionalities of the basic\ncomponents of virtual datacenters, identified the major risks to the data\ninfrastructure, and present here several solutions for overcoming potential\nthreats to virtual infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 15:06:08 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Mirzoev", "Dr. Timur", ""], ["Yang", "Dr. Baijian", ""]]}, {"id": "1404.2176", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev", "title": "Synchronous replication of remote storage", "comments": null, "journal-ref": "Journal of Communication and Computer, Mar. 2009, Volume 6, No.3\n  (Serial No.52) ISSN 1548-7709", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage replication is one of the essential requirements for network\nenvironments. While many forms of Network Attached Storage (NAS), Storage Area\nNetworks (SAN) and other forms of network storage exist, there is a need for a\nreliable synchronous storage replication technique between distant sites (less\nthan 1 mile). Such technology allows setting new standards for network failover\nand failback systems for virtual servers; specifically, addressing the growing\nneed for effective disaster recovery (DR) planning. The purpose of this\nmanuscript is to identify newest technologies such as SAN/iQ and Storage\nVMotion that allow for remote storage synchronous replication for virtual\nservers. This study provides an analysis and a comparison of various SANs that\ncreate solutions for enterprise needs. Additionally, the interoperability of\nthese technologies with the industry s leading product VMware ESX Server will\nbe discussed.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 15:09:59 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Mirzoev", "Dr. Timur", ""]]}, {"id": "1404.2187", "submitter": "Ernie Cohen", "authors": "Ernie Cohen", "title": "Coherent Causal Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coherent causal memory (CCM) is causal memory in which prefixes of an\nexecution can be mapped to global memory states in a consistent way. While CCM\nrequires conflicting pairs of writes to be globally ordered, it allows writes\nto remain unordered with respect to both reads and nonconflicting writes.\nNevertheless, it supports assertional, state-based program reasoning using\ngeneralized Owicki-Gries proof outlines (where assertions can be attached to\nany causal program edge). Indeed, we show that from a reasoning standpoint, CCM\ndiffers from sequentially consistent (SC) memory only in that ghost code added\nby the user is not allowed to introduce new write-write races.\n  While CCM provides most of the formal reasoning leverage of SC memory, it is\nmuch more efficiently implemented. As an illustration, we describe a simple\nprogramming discipline that provides CCM on top of x86-TSO. The discipline is\nconsiderably more relaxed than the one needed to ensure SC; for example, it\nintroduces no burden whatsoever for programs in which at most one thread writes\nto any variable.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 22:58:50 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Cohen", "Ernie", ""]]}, {"id": "1404.2237", "submitter": "Lukasz Swierczewski", "authors": "Monika Kwiatkowska, Lukasz Swierczewski", "title": "Steganography - coding and intercepting the information from encoded\n  pictures in the absence of any initial information", "comments": "10 pages, 5 figures, 5 tables, LVEE 2014 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The work includes implementation and extraction algorithms capabilities test,\nwithout any additional data (starting position, the number of bits used, gap\nbetween the amount of data encoded) information from encoded files (mostly\nimages). The software is written using OpenMP standard [1], which allowed them\nto run on parallel computers. Performance tests were carried out on computers,\nBlue Gene/P [2], Blue Gene/Q [3] and the system consisting of four AMD Opteron\n6272 [4]. Source code is available under GNU GPL v3 license and are available\nin a repository OLib [5].\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 17:09:56 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Kwiatkowska", "Monika", ""], ["Swierczewski", "Lukasz", ""]]}, {"id": "1404.2259", "submitter": "Taylor T Johnson", "authors": "Luan Viet Nguyen and Taylor T. Johnson", "title": "Virtual Prototyping and Distributed Control for Solar Array with\n  Distributed Multilevel Inverter", "comments": "Preprint draft under review, submitted on March 18, 2014 to IEEE\n  Transactions on Energy Conversion, Special Issue: Advanced Distributed\n  Control of Energy Conversion Devices and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the virtual prototyping of a solar array with a\ngrid-tie implemented as a distributed inverter and controlled using distributed\nalgorithms. Due to the distributed control and inherent redundancy in the array\ncomposed of many panels and inverter modules, the virtual prototype exhibits\nfault-tolerance capabilities. The distributed identifier algorithm allows the\nsystem to keep track of the number of operating panels to appropriately\nregulate the DC voltage output of the panels using buck-boost converters, and\ndetermine appropriate switching times for H-bridges in the grid-tie. We\nevaluate the distributed inverter, its control strategy, and fault-tolerance\nthrough simulation in Simulink/Stateflow. Our virtual prototyping framework\nallows for generating arrays and grid-ties consisting of many panels, and we\nevaluate arrays of five to dozens of panels. Our analysis suggests the\nachievable total harmonic distortion (THD) of the system may allow for\noperating the array in spite of failures of the power electronics, control\nsoftware, and other subcomponents.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 19:40:47 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Nguyen", "Luan Viet", ""], ["Johnson", "Taylor T.", ""]]}, {"id": "1404.2261", "submitter": "Mehran Alidoost Nia", "authors": "Mehran Alidoost Nia, Aida Ghorbani and Reza Ebrahimi Atani", "title": "A Novel Anonymous Cloud Architecture Design; Providing Secure Online\n  Services and Electronic Payments", "comments": "in proceeding of the first international conference on Electronic\n  Commerce and Economy, Tehran, April 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymous cloud architecture provides secure environment for business and\nalso e-commerce approaches. By using this type of architecture, we can propose\nanonymous online applications. Customers who need secure and reliable online\nservices should pay for provided services. A big problem is electronic payment\nthat is needed for billing customers. But customer identity should be remained\nanonymous during and also after payment procedure. In this paper we propose a\nnovel and modified anonymous architecture that ensures customers that hide\ntheir identity from others. This architecture is used from common network\nprotocols and we eliminate Tor anonymous service from architecture design space\nbecause of independency. The here is introduced scalability parameter in\nanonymous cloud architecture design space. After all we compare proposed\narchitecture with other popular cloud architectures in this range and we obtain\nits advantages according to efficiency, security and anonymity.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 19:49:02 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Nia", "Mehran Alidoost", ""], ["Ghorbani", "Aida", ""], ["Atani", "Reza Ebrahimi", ""]]}, {"id": "1404.2266", "submitter": "James Roberts", "authors": "Thomas Bonald and James Roberts", "title": "Enhanced Cluster Computing Performance Through Proportional Fairness", "comments": "Submitted to Performance 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of cluster computing depends on how concurrent jobs share\nmultiple data center resource types like CPU, RAM and disk storage. Recent\nresearch has discussed efficiency and fairness requirements and identified a\nnumber of desirable scheduling objectives including so-called dominant resource\nfairness (DRF). We argue here that proportional fairness (PF), long recognized\nas a desirable objective in sharing network bandwidth between ongoing flows, is\npreferable to DRF. The superiority of PF is manifest under the realistic\nmodelling assumption that the population of jobs in progress is a stochastic\nprocess. In random traffic the strategy-proof property of DRF proves\nunimportant while PF is shown by analysis and simulation to offer a\nsignificantly better efficiency-fairness tradeoff.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 09:44:11 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Bonald", "Thomas", ""], ["Roberts", "James", ""]]}, {"id": "1404.2303", "submitter": "Pedro Gonnet", "authors": "Pedro Gonnet", "title": "Efficient and Scalable Algorithms for Smoothed Particle Hydrodynamics on\n  Hybrid Shared/Distributed-Memory Architectures", "comments": "Submitted to SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new fast and implicitly parallel approach to\nneighbour-finding in multi-resolution Smoothed Particle Hydrodynamics (SPH)\nsimulations. This new approach is based on hierarchical cell decompositions and\nsorted interactions, within a task-based formulation. It is shown to be faster\nthan traditional tree-based codes, and to scale better than domain\ndecomposition-based approaches on hybrid shared/distributed-memory parallel\narchitectures, e.g. clusters of multi-cores, achieving a $40\\times$ speedup\nover the Gadget-2 simulation code.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 20:45:22 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Gonnet", "Pedro", ""]]}, {"id": "1404.2340", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev", "title": "Disaster Recovery Using Virtual Machines", "comments": null, "journal-ref": "Kabardino-Balkarian State University Journal, Technical Sciences\n  Series, Edition 6. Publishing Department of KBSU. Nalchik 2008", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, the importance of having 100% uptime for businesses and industries is\nclear: financial reasons and often strict government regulations for certain\nindustries require 100% business continuity. The concept of business continuity\n(BC), as Microsoft defines it: the ability of an organization to continue to\nfunction even after a disastrous event, accomplished through the deployment of\nredundant hardware and software, the use of fault tolerant systems, as well as\na solid backup and recovery strategy, directly relates to an organization s\nability to quickly restore and deploy IT backups and business operations in a\nshort period of time.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 01:05:38 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Mirzoev", "Dr. Timur", ""]]}, {"id": "1404.2387", "submitter": "Bernhard Haeupler", "authors": "Mohsen Ghaffari, Bernhard Haeupler", "title": "Fast Structuring of Radio Networks for Multi-Message Communications", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-41527-2_34", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce collision free layerings as a powerful way to structure radio\nnetworks. These layerings can replace hard-to-compute BFS-trees in many\ncontexts while having an efficient randomized distributed construction. We\ndemonstrate their versatility by using them to provide near optimal distributed\nalgorithms for several multi-message communication primitives.\n  Designing efficient communication primitives for radio networks has a rich\nhistory that began 25 years ago when Bar-Yehuda et al. introduced fast\nrandomized algorithms for broadcasting and for constructing BFS-trees. Their\nBFS-tree construction time was $O(D \\log^2 n)$ rounds, where $D$ is the network\ndiameter and $n$ is the number of nodes. Since then, the complexity of a\nbroadcast has been resolved to be $T_{BC} = \\Theta(D \\log \\frac{n}{D} + \\log^2\nn)$ rounds. On the other hand, BFS-trees have been used as a crucial building\nblock for many communication primitives and their construction time remained a\nbottleneck for these primitives.\n  We introduce collision free layerings that can be used in place of BFS-trees\nand we give a randomized construction of these layerings that runs in nearly\nbroadcast time, that is, w.h.p. in $T_{Lay} = O(D \\log \\frac{n}{D} +\n\\log^{2+\\epsilon} n)$ rounds for any constant $\\epsilon>0$. We then use these\nlayerings to obtain: (1) A randomized algorithm for gathering $k$ messages\nrunning w.h.p. in $O(T_{Lay} + k)$ rounds. (2) A randomized $k$-message\nbroadcast algorithm running w.h.p. in $O(T_{Lay} + k \\log n)$ rounds. These\nalgorithms are optimal up to the small difference in the additive\npoly-logarithmic term between $T_{BC}$ and $T_{Lay}$. Moreover, they imply the\nfirst optimal $O(n \\log n)$ round randomized gossip algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 07:54:07 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Haeupler", "Bernhard", ""]]}, {"id": "1404.2632", "submitter": "Mohammad Shojafar", "authors": "Saeed Javanmardi, Mohammad Shojafar, Shahdad Shariatmadari, Sima S.\n  Ahrabi", "title": "FRTRUST: a fuzzy reputation based model for trust management in semantic\n  P2P grids", "comments": "12 Pages, 10 Figures, 3 Tables, InderScience, International Journal\n  of Grid and Utility Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Grid and peer-to-peer (P2P) networks are two ideal technologies for file\nsharing. A P2P grid is a special case of grid networks in which P2P\ncommunications are used for communication between nodes and trust management.\nUse of this technology allows creation of a network with greater distribution\nand scalability. Semantic grids have appeared as an expansion of grid networks\nin which rich resource metadata are revealed and clearly handled. In a semantic\nP2P grid, nodes are clustered into different groups based on the semantic\nsimilarities between their services. This paper proposes a reputation model for\ntrust management in a semantic P2P Grid. We use fuzzy theory, in a trust\noverlay network named FR TRUST that models the network structure and the\nstorage of reputation information. In fact we present a reputation collection\nand computation system for semantic P2P Grids. The system uses fuzzy theory to\ncompute a peer trust level, which can be either: Low, Medium, or High. Our\nexperimental results demonstrate that FR TRUST combines low (and therefore\ndesirable) a good computational complexity with high ranking accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 21:27:55 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:47:26 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Javanmardi", "Saeed", ""], ["Shojafar", "Mohammad", ""], ["Shariatmadari", "Shahdad", ""], ["Ahrabi", "Sima S.", ""]]}, {"id": "1404.2644", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet, Yingyu Liang, Alireza Bagheri Garakani,\n  Maria-Florina Balcan, Fei Sha", "title": "A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse\n  Learning", "comments": "Extended version of the SIAM Data Mining 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sparse combinations is a frequent theme in machine learning. In this\npaper, we study its associated optimization problem in the distributed setting\nwhere the elements to be combined are not centrally located but spread over a\nnetwork. We address the key challenges of balancing communication costs and\noptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)\nalgorithm. We obtain theoretical guarantees on the optimization error\n$\\epsilon$ and communication cost that do not depend on the total number of\ncombining elements. We further show that the communication cost of dFW is\noptimal by deriving a lower-bound on the communication cost required to\nconstruct an $\\epsilon$-approximate solution. We validate our theoretical\nanalysis with empirical studies on synthetic and real-world data, which\ndemonstrate that dFW outperforms both baselines and competing methods. We also\nstudy the performance of dFW when the conditions of our analysis are relaxed,\nand show that dFW is fairly robust.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 22:16:39 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 04:08:51 GMT"}, {"version": "v3", "created": "Mon, 12 Jan 2015 15:14:19 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Liang", "Yingyu", ""], ["Garakani", "Alireza Bagheri", ""], ["Balcan", "Maria-Florina", ""], ["Sha", "Fei", ""]]}, {"id": "1404.2670", "submitter": "Brendan Harding", "authors": "Brendan Harding, Markus Hegland, Jay Larson and James Southern", "title": "Scalable and Fault Tolerant Computation with the Sparse Grid Combination\n  Technique", "comments": "23 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues to develop a fault tolerant extension of the sparse grid\ncombination technique recently proposed in [B. Harding and M. Hegland, ANZIAM\nJ., 54 (CTAC2012), pp. C394-C411]. The approach is novel for two reasons, first\nit provides several levels in which one can exploit parallelism leading towards\nmassively parallel implementations, and second, it provides algorithm-based\nfault tolerance so that solutions can still be recovered if failures occur\nduring computation. We present a generalisation of the combination technique\nfrom which the fault tolerant algorithm is a consequence. Using a model for the\ntime between faults on each node of a high performance computer we provide\nbounds on the expected error for interpolation with this algorithm. Numerical\nexperiments on the scalar advection PDE demonstrate that the algorithm is\nresilient to faults on a real application. It is observed that the trade-off of\nrecovery time to decreased accuracy of the solution is suitably small. A\ncomparison with traditional checkpoint-restart methods applied to the\ncombination technique show that our approach is highly scalable with respect to\nthe number of faults.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 02:14:34 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Harding", "Brendan", ""], ["Hegland", "Markus", ""], ["Larson", "Jay", ""], ["Southern", "James", ""]]}, {"id": "1404.2697", "submitter": "Duane Wilson", "authors": "Duane Wilson, Giuseppe Ateniese", "title": "To Share or Not to Share in Client-Side Encrypted Clouds", "comments": null, "journal-ref": "Information Security, Lecture Notes in Computer Science Volume\n  8783, 2014, pp 401-412", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of cloud computing, a number of cloud providers have arisen\nto provide Storage-as-a-Service (SaaS) offerings to both regular consumers and\nbusiness organizations. SaaS (different than Software-as-a-Service in this\ncontext) refers to an architectural model in which a cloud provider provides\ndigital storage on their own infrastructure. Three models exist amongst SaaS\nproviders for protecting the confidentiality data stored in the cloud: 1) no\nencryption (data is stored in plain text), 2) server-side encryption (data is\nencrypted once uploaded), and 3) client-side encryption (data is encrypted\nprior to upload). This paper seeks to identify weaknesses in the third model,\nas it claims to offer 100% user data confidentiality throughout all data\ntransactions (e.g., upload, download, sharing) through a combination of Network\nTraffic Analysis, Source Code Decompilation, and Source Code Disassembly. The\nweaknesses we uncovered primarily center around the fact that the cloud\nproviders we evaluated were each operating in a Certificate Authority capacity\nto facilitate data sharing. In this capacity, they assume the role of both\ncertificate issuer and certificate authorizer as denoted in a Public-Key\nInfrastructure (PKI) scheme - which gives them the ability to view user data\ncontradicting their claims of 100% data confidentiality. We have collated our\nanalysis and findings in this paper and explore some potential solutions to\naddress these weaknesses in these sharing methods. The solutions proposed are a\ncombination of best practices associated with the use of PKI and other\ncryptographic primitives generally accepted for protecting the confidentiality\nof shared information.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 05:22:32 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 01:21:15 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Wilson", "Duane", ""], ["Ateniese", "Giuseppe", ""]]}, {"id": "1404.2739", "submitter": "Rathna Devi M", "authors": "M.Rathna Devi and A.Anju", "title": "Multiprocessor Scheduling of Dependent Tasks to Minimize Makespan and\n  Reliability Cost Using NSGA-II", "comments": "13 pages,7 figures", "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST), Vol.4, No.2, March 2014", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Algorithms developed for scheduling applications on heterogeneous\nmultiprocessor system focus on asingle objective such as execution time, cost\nor total data transmission time. However, if more than oneobjective (e.g.\nexecution cost and time, which may be in conflict) are considered, then the\nproblem becomes more challenging. This project is proposed to develop a\nmultiobjective scheduling algorithm using Evolutionary techniques for\nscheduling a set of dependent tasks on available resources in a multiprocessor\nenvironment which will minimize the makespan and reliability cost. A\nNon-dominated sorting Genetic Algorithm-II procedure has been developed to get\nthe pareto- optimal solutions. NSGA-II is a Elitist Evolutionary algorithm, and\nit takes the initial parental solution without any changes, in all iteration to\neliminate the problem of loss of some pareto-optimal solutions.NSGA-II uses\ncrowding distance concept to create a diversity of the solutions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 08:53:10 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Devi", "M. Rathna", ""], ["Anju", "A.", ""]]}, {"id": "1404.2772", "submitter": "Ravi Ranjan", "authors": "Ravi Ranjan and G. Sahoo", "title": "A New Clustering Approach for Anomaly Intrusion Detection", "comments": "10 pages with 3 figures,2 Tables This paper explains about clustering\n  methodology used in Data Mining field for Intrusion Detection in the area of\n  Network Security", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP),ISSN:2230-9608[Online],2231-007X[Print] Vol.4, No.2, March\n  2014, page(s): 29-38", "doi": "10.5121/ijdkp.2014.4203", "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in technology have made our work easier compare to earlier\ntimes. Computer network is growing day by day but while discussing about the\nsecurity of computers and networks it has always been a major concerns for\norganizations varying from smaller to larger enterprises. It is true that\norganizations are aware of the possible threats and attacks so they always\nprepare for the safer side but due to some loopholes attackers are able to make\nattacks. Intrusion detection is one of the major fields of research and\nresearchers are trying to find new algorithms for detecting intrusions.\nClustering techniques of data mining is an interested area of research for\ndetecting possible intrusions and attacks. This paper presents a new clustering\napproach for anomaly intrusion detection by using the approach of K-medoids\nmethod of clustering and its certain modifications. The proposed algorithm is\nable to achieve high detection rate and overcomes the disadvantages of K-means\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 11:22:17 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Ranjan", "Ravi", ""], ["Sahoo", "G.", ""]]}, {"id": "1404.2842", "submitter": "Xibo Jin", "authors": "Xibo Jin, Fa Zhang, Lin Wang, Songlin Hu, Biyu Zhou and Zhiyong Liu", "title": "A Joint Optimization of Operational Cost and Performance Interference in\n  Cloud Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual machine (VM) scheduling is an important technique to efficiently\noperate the computing resources in a data center. Previous work has mainly\nfocused on consolidating VMs to improve resource utilization and thus to\noptimize energy consumption. However, the interference between collocated VMs\nis usually ignored, which can result in very worse performance degradation to\nthe applications running in those VMs due to the contention of the shared\nresources. Based on this observation, we aim at designing efficient VM\nassignment and scheduling strategies where we consider optimizing both the\noperational cost of the data center and the performance degradation of running\napplications and then, we propose a general model which captures the inherent\ntradeoff between the two contradictory objectives. We present offline and\nonline solutions for this problem by exploiting the spatial and temporal\ninformation of VMs where VM scheduling is done by jointly consider the\ncombinations and the life-cycle overlapping of the VMs. Evaluation results show\nthat the proposed methods can generate efficient schedules for VMs, achieving\nlow operational cost while significantly reducing the performance degradation\nof applications in cloud data centers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 15:10:37 GMT"}, {"version": "v2", "created": "Sat, 19 Apr 2014 06:35:44 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Jin", "Xibo", ""], ["Zhang", "Fa", ""], ["Wang", "Lin", ""], ["Hu", "Songlin", ""], ["Zhou", "Biyu", ""], ["Liu", "Zhiyong", ""]]}, {"id": "1404.3166", "submitter": "Robert Brijder", "authors": "Robert Brijder", "title": "Minimal Output Unstable Configurations in Chemical Reaction Networks and\n  Deciders", "comments": "14 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the set of output stable configurations of chemical reaction\ndeciders (CRDs). It turns out that CRDs with only bimolecular reactions (which\nare almost equivalent to population protocols) have a special structure that\nallows for an algorithm to efficiently compute their finite set of minimal\noutput unstable configurations. As a consequence, a relatively large set of\nconfigurations may be efficiently checked for output stability.\n  We also provide a number of observations regarding the semilinearity result\nof Angluin et al. [Distrib. Comput., 2007] from the context of population\nprotocols (which is a central result for output stable CRDs). In particular, we\nobserve that the computation-friendly class of totally stable CRDs has equal\nexpressive power as the larger class of output stable CRDs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 17:48:50 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 09:32:30 GMT"}, {"version": "v3", "created": "Fri, 6 Jun 2014 12:19:11 GMT"}, {"version": "v4", "created": "Thu, 24 Jul 2014 08:34:03 GMT"}, {"version": "v5", "created": "Mon, 15 Jun 2015 09:46:29 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Brijder", "Robert", ""]]}, {"id": "1404.3272", "submitter": "Bapi Chatterjee", "authors": "Bapi Chatterjee, Nhan Nguyen and Philippas Tsigas", "title": "Efficient Lock-free Binary Search Trees", "comments": "15 pages, 3 figures, submitted to PODC", "journal-ref": null, "doi": null, "report-no": "TR - 2014:05, ISSN 1652-926X", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel algorithm for concurrent lock-free internal\nbinary search trees (BST) and implement a Set abstract data type (ADT) based on\nthat. We show that in the presented lock-free BST algorithm the amortized step\ncomplexity of each set operation - {\\sc Add}, {\\sc Remove} and {\\sc Contains} -\nis $O(H(n) + c)$, where, $H(n)$ is the height of BST with $n$ number of nodes\nand $c$ is the contention during the execution. Our algorithm adapts to\ncontention measures according to read-write load. If the situation is\nread-heavy, the operations avoid helping pending concurrent {\\sc Remove}\noperations during traversal, and, adapt to interval contention. However, for\nwrite-heavy situations we let an operation help pending {\\sc Remove}, even\nthough it is not obstructed, and so adapt to tighter point contention. It uses\nsingle-word compare-and-swap (\\texttt{CAS}) operations. We show that our\nalgorithm has improved disjoint-access-parallelism compared to similar existing\nalgorithms. We prove that the presented algorithm is linearizable. To the best\nof our knowledge this is the first algorithm for any concurrent tree data\nstructure in which the modify operations are performed with an additive term of\ncontention measure.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 09:59:02 GMT"}, {"version": "v2", "created": "Fri, 18 Apr 2014 08:59:26 GMT"}, {"version": "v3", "created": "Fri, 9 May 2014 15:39:21 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Chatterjee", "Bapi", ""], ["Nguyen", "Nhan", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1404.3318", "submitter": "Michele Scquizzato", "authors": "Gianfranco Bilardi, Andrea Pietracaprina, Geppino Pucci, Michele\n  Scquizzato, and Francesco Silvestri", "title": "Network-Oblivious Algorithms", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is proposed for the design and analysis of\n\\emph{network-oblivious algorithms}, namely, algorithms that can run unchanged,\nyet efficiently, on a variety of machines characterized by different degrees of\nparallelism and communication capabilities. The framework prescribes that a\nnetwork-oblivious algorithm be specified on a parallel model of computation\nwhere the only parameter is the problem's input size, and then evaluated on a\nmodel with two parameters, capturing parallelism granularity and communication\nlatency. It is shown that, for a wide class of network-oblivious algorithms,\noptimality in the latter model implies optimality in the Decomposable BSP\nmodel, which is known to effectively describe a wide and significant class of\nparallel platforms. The proposed framework can be regarded as an attempt to\nport the notion of obliviousness, well established in the context of cache\nhierarchies, to the realm of parallel computation. Its effectiveness is\nillustrated by providing optimal network-oblivious algorithms for a number of\nkey problems. Some limitations of the oblivious approach are also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 20:05:06 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Bilardi", "Gianfranco", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Scquizzato", "Michele", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1404.3321", "submitter": "Michael Schliephake", "authors": "Michael Schliephake and Erwin Laure", "title": "Performance Analysis of Irregular Collective Communication with the\n  Crystal Router Algorithm", "comments": "Conference EASC2014: Solving Software Challenges for Exascale", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve exascale performance it is important to detect potential\nbottlenecks and identify strategies to overcome them. For this, both\napplications and system software must be analysed and potentially improved. The\nEU FP7 project Collaborative Research into Exascale Systemware, Tools &\nApplications (CRESTA) chose the approach to co-design advanced simulation\napplications and system software as well as development tools. In this paper,\nwe present the results of a co-design activity focused on the simulation code\nNEK5000 that aims at performance improvements of collective communication\noperations. We have analysed the algorithms that form the core of NEK5000's\ncommunication module in order to assess its viability on recent computer\narchitectures before starting to improve its performance. Our results show that\nthe crystal router algorithm performs well in sparse, irregular collective\noperations for medium and large processor number but improvements for even\nlarger system sizes of the future will be needed. We sketch the needed\nimprovements, which will make the communication algorithms also beneficial for\nother applications that need to implement latency-dominated communication\nschemes with short messages. The latency-optimised communication operations\nwill also become used in a runtime-system providing dynamic load balancing,\nunder development within CRESTA.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 21:43:39 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 20:01:56 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Schliephake", "Michael", ""], ["Laure", "Erwin", ""]]}, {"id": "1404.3448", "submitter": "YuKun Zhong", "authors": "YuKun Zhong, BaoQiu Wang, JianBiao Lin, Chen Tao, Che Nian, Xie Wen", "title": "Solving The Longest Overlap Region Problem for Noncoding DNA Sequences\n  with GPU", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early hardware limitations of GPU (lack of synchronization primitives and\nlimited memory caching mechanisms) can make GPU-based computation inefficient.\nNow Bio-technologies bring more chances to Bioinformatics and Biological\nEngineering. Our paper introduces a way to solve the longest overlap region of\nnon-coding DNA sequences on using the Compute Unified Device Architecture\n(CUDA) platform Intel(R) Core(TM) i3- 3110m quad-core. Compared to standard CPU\nimplementation, CUDA performance proves the method of the longest overlap\nregion recognition of noncoding DNA is an efficient approach to\nhigh-performance bioinformatics applications. Studies show the fact that\nefficiency of GPU performance is more than 20 times speedup than that of CPU\nserial implementation. We believe our method gives a cost-efficient solution to\nthe bioinformatics community for solving longest overlap region recognition\nproblem and other related fields.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 02:00:37 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 00:58:29 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Zhong", "YuKun", ""], ["Wang", "BaoQiu", ""], ["Lin", "JianBiao", ""], ["Tao", "Chen", ""], ["Nian", "Che", ""], ["Wen", "Xie", ""]]}, {"id": "1404.3456", "submitter": "YuKun Zhong", "authors": "Yukun Zhong, ZhiWei He, XianHong Wang, XiongBin Cao", "title": "A Way For Accelerating The DNA Sequence Reconstruction Problem By CUDA", "comments": "7 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, we usually utilize the method of shotgun to cut a DNA sequence\ninto pieces and we have to reconstruct the original DNA sequence from the\npieces, those are widely used method for DNA assembly. Emerging DNA sequence\ntechnologies open up more opportunities for molecular biology. This paper\nintroduce a new method to improve the efficiency of reconstructing DNA sequence\nusing suffix array based on CUDA programming model. The experimental result\nshow the construction of suffix array using GPU is an more efficient approach\non Intel(R) Core(TM) i3-3110K quad-core and NVIDIA GeForce 610M GPU, and study\nshow the performance of our method is more than 20 times than that of CPU\nserial implementation. We believe our method give a cost-efficient solution to\nthe bioinformatics community.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 02:57:44 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Zhong", "Yukun", ""], ["He", "ZhiWei", ""], ["Wang", "XianHong", ""], ["Cao", "XiongBin", ""]]}, {"id": "1404.3766", "submitter": "Puxiao Han", "authors": "Puxiao Han, Ruixin Niu, and Mengqi Ren", "title": "Distributed Approximate Message Passing for Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient distributed approach for implementing the\napproximate message passing (AMP) algorithm, named distributed AMP (DAMP), is\ndeveloped for compressed sensing (CS) recovery in sensor networks with the\nsparsity K unknown. In the proposed DAMP, distributed sensors do not have to\nuse or know the entire global sensing matrix, and the burden of computation and\nstorage for each sensor is reduced. To reduce communications among the sensors,\na new data query algorithm, called global computation for AMP (GCAMP), is\nproposed. The proposed GCAMP based DAMP approach has exactly the same recovery\nsolution as the centralized AMP algorithm, which is proved theoretically in the\npaper. The performance of the DAMP approach is evaluated in terms of the\ncommunication cost saved by using GCAMP. For comparison purpose, thresholding\nalgorithm (TA), a well known distributed Top-K algorithm, is modified so that\nit also leads to the same recovery solution as the centralized AMP. Numerical\nresults demonstrate that the GCAMP based DAMP outperforms the Modified TA based\nDAMP, and reduces the communication cost significantly.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 22:05:50 GMT"}, {"version": "v2", "created": "Thu, 22 May 2014 21:40:08 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Han", "Puxiao", ""], ["Niu", "Ruixin", ""], ["Ren", "Mengqi", ""]]}, {"id": "1404.3861", "submitter": "Dionysios Logothetis", "authors": "Claudio Martella, Dionysios Logothetis, Andreas Loukas, Georgos\n  Siganos", "title": "Spinner: Scalable Graph Partitioning in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several organizations, like social networks, store and routinely analyze\nlarge graphs as part of their daily operation. Such graphs are typically\ndistributed across multiple servers, and graph partitioning is critical for\nefficient graph management. Existing partitioning algorithms focus on finding\ngraph partitions with good locality, but disregard the pragmatic challenges of\nintegrating partitioning into large-scale graph management systems deployed on\nthe cloud, such as dealing with the scale and dynamicity of the graph and the\ncompute environment.\n  In this paper, we propose Spinner, a scalable and adaptive graph partitioning\nalgorithm based on label propagation designed on top of the Pregel model.\nSpinner scales to massive graphs, produces partitions with locality and balance\ncomparable to the state-of-the-art and efficiently adapts the partitioning upon\nchanges. We describe our algorithm and its implementation in the Pregel\nprogramming model that makes it possible to partition billion-vertex graphs. We\nevaluate Spinner with a variety of synthetic and real graphs and show that it\ncan compute partitions with quality comparable to the state-of-the art. In\nfact, by using Spinner in conjunction with the Giraph graph processing engine,\nwe speed up different applications by a factor of 2 relative to standard hash\npartitioning.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 10:26:24 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 03:04:30 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Martella", "Claudio", ""], ["Logothetis", "Dionysios", ""], ["Loukas", "Andreas", ""], ["Siganos", "Georgos", ""]]}, {"id": "1404.3867", "submitter": "Yohsuke Murase", "authors": "Yohsuke Murase, Takeshi Uchitane, Nobuyasu Ito", "title": "A tool for parameter-space explorations", "comments": "4 pages, 5 figures, CSP 2014 conference", "journal-ref": "Physics Procedia 57C pp. 73-76 (2014)", "doi": "10.1016/j.phpro.2014.08.134", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A software for managing simulation jobs and results, named \"OACIS\", is\npresented. It controls a large number of simulation jobs executed in various\nremote servers, keeps these results in an organized way, and manages the\nanalyses on these results. The software has a web browser front end, and users\ncan submit various jobs to appropriate remote hosts from a web browser easily.\nAfter these jobs are finished, all the result files are automatically\ndownloaded from the computational hosts and stored in a traceable way together\nwith the logs of the date, host, and elapsed time of the jobs. Some\nvisualization functions are also provided so that users can easily grasp the\noverview of the results distributed in a high-dimensional parameter space.\nThus, OACIS is especially beneficial for the complex simulation models having\nmany parameters for which a lot of parameter searches are required. By using\nAPI of OACIS, it is easy to write a code that automates parameter selection\ndepending on the previous simulation results. A few examples of the automated\nparameter selection are also demonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 10:47:36 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Murase", "Yohsuke", ""], ["Uchitane", "Takeshi", ""], ["Ito", "Nobuyasu", ""]]}, {"id": "1404.3913", "submitter": "Loris Marchal", "authors": "Olivier Beaumont and Loris Marchal", "title": "Analysis of Dynamic Scheduling Strategies for Matrix Multiplication on\n  Heterogeneous Platforms", "comments": "Accepted for publication in HPDC 2014", "journal-ref": null, "doi": "10.1145/2600212.2600223", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous increase in the size and heterogeneity of supercomputers makes\nit very difficult to predict the performance of a scheduling algorithm.\nTherefore, dynamic solutions, where scheduling decisions are made at runtime\nhave overpassed static allocation strategies. The simplicity and efficiency of\ndynamic schedulers such as Hadoop are a key of the success of the MapReduce\nframework. Dynamic schedulers such as StarPU, PaRSEC or StarSs are also\ndeveloped for more constrained computations, e.g. task graphs coming from\nlinear algebra. To make their decisions, these runtime systems make use of some\nstatic information, such as the distance of tasks to the critical path or the\naffinity between tasks and computing resources (CPU, GPU,...) and of dynamic\ninformation, such as where input data are actually located. In this paper, we\nconcentrate on two elementary linear algebra kernels, namely the outer product\nand the matrix multiplication. For each problem, we propose several dynamic\nstrategies that can be used at runtime and we provide an analytic study of\ntheir theoretical performance. We prove that the theoretical analysis provides\nvery good estimate of the amount of communications induced by a dynamic\nstrategy and can be used in order to efficiently determine thresholds used in\ndynamic scheduler, thus enabling to choose among them for a given problem and\narchitecture.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 13:56:02 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Beaumont", "Olivier", ""], ["Marchal", "Loris", ""]]}, {"id": "1404.4152", "submitter": "Yongchao Liu", "authors": "Yongchao Liu, Bertil Schmidt", "title": "SWAPHI: Smith-Waterman Protein Database Search on Xeon Phi Coprocessors", "comments": "A short version of this paper has been accepted by the IEEE ASAP 2014\n  conference", "journal-ref": null, "doi": "10.1109/ASAP.2014.6868657", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximal sensitivity of the Smith-Waterman (SW) algorithm has enabled its\nwide use in biological sequence database search. Unfortunately, the high\nsensitivity comes at the expense of quadratic time complexity, which makes the\nalgorithm computationally demanding for big databases. In this paper, we\npresent SWAPHI, the first parallelized algorithm employing Xeon Phi\ncoprocessors to accelerate SW protein database search. SWAPHI is designed based\non the scale-and-vectorize approach, i.e. it boosts alignment speed by\neffectively utilizing both the coarse-grained parallelism from the many\nco-processing cores (scale) and the fine-grained parallelism from the 512-bit\nwide single instruction, multiple data (SIMD) vectors within each core\n(vectorize). By searching against the large UniProtKB/TrEMBL protein database,\nSWAPHI achieves a performance of up to 58.8 billion cell updates per second\n(GCUPS) on one coprocessor and up to 228.4 GCUPS on four coprocessors.\nFurthermore, it demonstrates good parallel scalability on varying number of\ncoprocessors, and is also superior to both SWIPE on 16 high-end CPU cores and\nBLAST+ on 8 cores when using four coprocessors, with the maximum speedup of\n1.52 and 1.86, respectively. SWAPHI is written in C++ language (with a set of\nSIMD intrinsics), and is freely available at http://swaphi.sourceforge.net.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 07:02:36 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 06:51:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liu", "Yongchao", ""], ["Schmidt", "Bertil", ""]]}, {"id": "1404.4161", "submitter": "Edoardo Di Napoli", "authors": "Mario Berljafa (1), Daniel Wortmann (2), Edoardo Di Napoli (2 and 3)\n  ((1) The University of Manchester, (2) Forschungszentrum Juelich, (3) AICES,\n  RWTH Aachen)", "title": "An Optimized and Scalable Eigensolver for Sequences of Eigenvalue\n  Problems", "comments": "23 Pages, 6 figures. First revision of an invited submission to\n  special issue of Concurrency and Computation: Practice and Experience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific applications the solution of non-linear differential\nequations are obtained through the set-up and solution of a number of\nsuccessive eigenproblems. These eigenproblems can be regarded as a sequence\nwhenever the solution of one problem fosters the initialization of the next. In\naddition, in some eigenproblem sequences there is a connection between the\nsolutions of adjacent eigenproblems. Whenever it is possible to unravel the\nexistence of such a connection, the eigenproblem sequence is said to be\ncorrelated. When facing with a sequence of correlated eigenproblems the current\nstrategy amounts to solving each eigenproblem in isolation. We propose a\nalternative approach which exploits such correlation through the use of an\neigensolver based on subspace iteration and accelerated with Chebyshev\npolynomials (ChFSI). The resulting eigensolver is optimized by minimizing the\nnumber of matrix-vector multiplications and parallelized using the Elemental\nlibrary framework. Numerical results show that ChFSI achieves excellent\nscalability and is competitive with current dense linear algebra parallel\neigensolvers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 08:09:56 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2014 11:59:36 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Berljafa", "Mario", "", "2 and 3"], ["Wortmann", "Daniel", "", "2 and 3"], ["Di Napoli", "Edoardo", "", "2 and 3"]]}, {"id": "1404.4250", "submitter": "Dmitry N. Kozlov", "authors": "Dmitry N. Kozlov", "title": "Witness structures and immediate snapshot complexes", "comments": "full paper version of the 1st part of the preprint arXiv:1402.4707;\n  to appear in DMTCS", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  3, Distributed Computing and Networking (November 28, 2017) dmtcs:4086", "doi": "10.23638/DMTCS-19-3-12", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce and study a new family of combinatorial simplicial\ncomplexes, which we call immediate snapshot complexes. Our construction and\nterminology is strongly motivated by theoretical distributed computing, as\nthese complexes are combinatorial models of the standard protocol complexes\nassociated to immediate snapshot read/write shared memory communication model.\nIn order to define the immediate snapshot complexes we need a new combinatorial\nobject, which we call a witness structure. These objects are indexing the\nsimplices in the immediate snapshot complexes, while a special operation on\nthem, called ghosting, describes the combinatorics of taking simplicial\nboundary. In general, we develop the theory of witness structures and use it to\nprove several combinatorial as well as topological properties of the immediate\nsnapshot complexes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 14:06:12 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 11:29:40 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2017 08:39:38 GMT"}, {"version": "v4", "created": "Fri, 24 Nov 2017 13:13:08 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Kozlov", "Dmitry N.", ""]]}, {"id": "1404.4540", "submitter": "Daniel Aguilar-Hidalgo", "authors": "Antonio C\\'ordoba and Daniel Aguilar-Hidalgo and M. Carmen Lemos", "title": "Collective computation in a network with distributed information", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a distributed information network in which each node has access to\nthe information contained in a limited set of nodes (its neighborhood) at a\ngiven time. A collective computation is carried out in which each node\ncalculates a value that implies all information contained in the network (in\nour case, the average value of a variable that can take different values in\neach network node). The neighborhoods can change dynamically by exchanging\nneighbors with other nodes. The results of this collective calculation show\nrapid convergence and good scalability with the network size. These results are\ncompared with those of a fixed network arranged as a square lattice, in which\nthe number of rounds to achieve a given accuracy is very high when the size of\nthe network increases. The results for the evolving networks are interpreted in\nlight of the properties of complex networks and are directly relevant to the\ndiameter and characteristic path length of the networks, which seem to express\n\"small world\" properties.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 14:30:11 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["C\u00f3rdoba", "Antonio", ""], ["Aguilar-Hidalgo", "Daniel", ""], ["Lemos", "M. Carmen", ""]]}, {"id": "1404.4653", "submitter": "Satya Sundeep Kambhampati", "authors": "Sundeep Kambhampati and Christopher Stewart", "title": "An Efficient and Balanced Platform for Data-Parallel Subsampling\n  Workloads", "comments": "70 Pages, 16 figures,Thesis, The Ohio State University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of internet services, data started growing faster than it can\nbe processed. To personalize user experience, this enormous data has to be\nprocessed in real time, in interactive fashion. In order to achieve faster data\nprocessing often a statistical method called subsampling. Subsampling workloads\ncompute statistics from a random subset of sample data (i.e., a subsample).\nData-parallel platforms group these samples into tasks; each task subsamples\nits data in parallel.\n  Current, state-of-the-art platforms such as Hadoop are built for large tasks\nthat run for long periods of time, but applications with smaller average task\nsizes suffer large overheads on these platforms. Tasks in subsampling workloads\nare sized to minimize the number of overall cache misses, and these tasks can\ncomplete in seconds. This technique can reduce the overall length of a\nmap-reduce job, but only when the savings from the cache miss rate reduction\nare not eclipsed by the platform overhead of task creation and data\ndistribution.\n  In this thesis, we propose a data-parallel platform with an efficient data\ndistribution component that breaks data-parallel subsampling workloads into\ncompute clusters with tiny tasks. Each tiny task completes in few hundreds of\nmilliseconds to seconds. Tiny tasks reduce processor cache misses caused by\nrandom subsampling, which speeds up per-task running time. However, they cause\nsignificant scheduling overheads and data distribution challenges. We propose a\ntask knee-pointing algorithm and a dynamic scheduler that schedules the tasks\nto worker nodes based on the availability and response times of the data nodes.\nWe compare our framework against various configurations of BashReduce and\nHadoop. A detailed discussion of tiny task approach on two workloads, EAGLET\nand Netflix movie rating is presented.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 21:13:43 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Kambhampati", "Sundeep", ""], ["Stewart", "Christopher", ""]]}, {"id": "1404.4767", "submitter": "Fabrice Rastello", "authors": "Venmugil Elango (CSE), Fabrice Rastello (INRIA Grenoble\n  Rh\\^one-Alpes), Louis-No\\\"el Pouchet (UCLA-CS), J. Ramanujam (ECE), P.\n  Sadayappan (CSE)", "title": "On Characterizing the Data Movement Complexity of Computational DAGs for\n  Parallel Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-8522", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology trends are making the cost of data movement increasingly dominant,\nboth in terms of energy and time, over the cost of performing arithmetic\noperations in computer systems. The fundamental ratio of aggregate data\nmovement bandwidth to the total computational power (also referred to the\nmachine balance parameter) in parallel computer systems is decreasing. It is\nthere- fore of considerable importance to characterize the inherent data\nmovement requirements of parallel algorithms, so that the minimal architectural\nbalance parameters required to support it on future systems can be well\nunderstood. In this paper, we develop an extension of the well-known red-blue\npebble game to develop lower bounds on the data movement complexity for the\nparallel execution of computational directed acyclic graphs (CDAGs) on parallel\nsystems. We model multi-node multi-core parallel systems, with the total\nphysical memory distributed across the nodes (that are connected through some\ninterconnection network) and in a multi-level shared cache hierarchy for\nprocessors within a node. We also develop new techniques for lower bound\ncharacterization of non-homogeneous CDAGs. We demonstrate the use of the\nmethodology by analyzing the CDAGs of several numerical algorithms, to develop\nlower bounds on data movement for their parallel execution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 12:42:26 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Elango", "Venmugil", "", "CSE"], ["Rastello", "Fabrice", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes"], ["Pouchet", "Louis-No\u00ebl", "", "UCLA-CS"], ["Ramanujam", "J.", "", "ECE"], ["Sadayappan", "P.", "", "CSE"]]}, {"id": "1404.4797", "submitter": "Christian Schulz", "authors": "Henning Meyerhenke, Peter Sanders, Christian Schulz", "title": "Parallel Graph Partitioning for Complex Networks", "comments": "Review article. Parallelization of our previous approach\n  arXiv:1402.3281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NE cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing large complex networks like social networks or web graphs has\nrecently attracted considerable interest. In order to do this in parallel, we\nneed to partition them into pieces of about equal size. Unfortunately, previous\nparallel graph partitioners originally developed for more regular mesh-like\nnetworks do not work well for these networks. This paper addresses this problem\nby parallelizing and adapting the label propagation technique originally\ndeveloped for graph clustering. By introducing size constraints, label\npropagation becomes applicable for both the coarsening and the refinement phase\nof multilevel graph partitioning. We obtain very high quality by applying a\nhighly parallel evolutionary algorithm to the coarsened graph. The resulting\nsystem is both more scalable and achieves higher quality than state-of-the-art\nsystems like ParMetis or PT-Scotch. For large complex networks the performance\ndifferences are very big. For example, our algorithm can partition a web graph\nwith 3.3 billion edges in less than sixteen seconds using 512 cores of a high\nperformance cluster while producing a high quality partition -- none of the\ncompeting systems can handle this graph on our system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 14:30:04 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 13:40:34 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2015 10:07:38 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Meyerhenke", "Henning", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1404.4821", "submitter": "Sergey Kovalchuk", "authors": "Sergey V. Kovalchuk, Artem V. Zakharchuk, Jiaqi Liao, Sergey V.\n  Ivanov, Alexander V. Boukhanovsky", "title": "A Technology for BigData Analysis Task Description using Domain-Specific\n  Languages", "comments": "To appear in Proceedings of the International Conference on\n  Computational Science (ICCS) 2014", "journal-ref": null, "doi": "10.1016/j.procs.2014.05.044", "report-no": null, "categories": "cs.DC cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents a technology for dynamic knowledge-based building of\nDomain-Specific Languages (DSL) to describe data-intensive scientific discovery\ntasks using BigData technology. The proposed technology supports high level\nabstract definition of analytic and simulation parts of the task as well as\nintegration into the composite scientific solutions. Automatic translation of\nthe abstract task definition enables seamless integration of various data\nsources within single solution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 15:44:29 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Kovalchuk", "Sergey V.", ""], ["Zakharchuk", "Artem V.", ""], ["Liao", "Jiaqi", ""], ["Ivanov", "Sergey V.", ""], ["Boukhanovsky", "Alexander V.", ""]]}, {"id": "1404.4910", "submitter": "Arko Provo Mukherjee", "authors": "Arko Provo Mukherjee and Srikanta Tirthapura", "title": "Enumerating Maximal Bicliques from a Large Graph using MapReduce", "comments": "A preliminary version of the paper was accepted at the Proceedings of\n  the 3rd IEEE International Congress on Big Data 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the enumeration of maximal bipartite cliques (bicliques) from a\nlarge graph, a task central to many practical data mining problems in social\nnetwork analysis and bioinformatics. We present novel parallel algorithms for\nthe MapReduce platform, and an experimental evaluation using Hadoop MapReduce.\nOur algorithm is based on clustering the input graph into smaller sized\nsubgraphs, followed by processing different subgraphs in parallel. Our\nalgorithm uses two ideas that enable it to scale to large graphs: (1) the\nredundancy in work between different subgraph explorations is minimized through\na careful pruning of the search space, and (2) the load on different reducers\nis balanced through the use of an appropriate total order among the vertices.\nOur evaluation shows that the algorithm scales to large graphs with millions of\nedges and tens of mil- lions of maximal bicliques. To our knowledge, this is\nthe first work on maximal biclique enumeration for graphs of this scale.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 01:43:52 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Mukherjee", "Arko Provo", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1404.4975", "submitter": "Vaneet Aggarwal", "authors": "Yu Xiang and Tian Lan and Vaneet Aggarwal and Yih-Farn R Chen", "title": "Joint Latency and Cost Optimization for Erasure-coded Data Center\n  Storage", "comments": "14 pages, presented in part at IFIP Performance, Oct 2014", "journal-ref": "IEEE/ACM Transactions on Networking, vol. 24, no. 4, pp. 2443-2457\n  (2016)", "doi": "10.1109/TNET.2015.2466453", "report-no": null, "categories": "cs.DC cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern distributed storage systems offer large capacity to satisfy the\nexponentially increasing need of storage space. They often use erasure codes to\nprotect against disk and node failures to increase reliability, while trying to\nmeet the latency requirements of the applications and clients. This paper\nprovides an insightful upper bound on the average service delay of such\nerasure-coded storage with arbitrary service time distribution and consisting\nof multiple heterogeneous files. Not only does the result supersede known delay\nbounds that only work for a single file or homogeneous files, it also enables a\nnovel problem of joint latency and storage cost minimization over three\ndimensions: selecting the erasure code, placement of encoded chunks, and\noptimizing scheduling policy. The problem is efficiently solved via the\ncomputation of a sequence of convex approximations with provable convergence.\nWe further prototype our solution in an open-source, cloud storage deployment\nover three geographically distributed data centers. Experimental results\nvalidate our theoretical delay analysis and show significant latency reduction,\nproviding valuable insights into the proposed latency-cost tradeoff in\nerasure-coded storage.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 17:25:55 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 04:34:02 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Xiang", "Yu", ""], ["Lan", "Tian", ""], ["Aggarwal", "Vaneet", ""], ["Chen", "Yih-Farn R", ""]]}, {"id": "1404.4982", "submitter": "Noy Rotbart", "authors": "S{\\o}ren Dahlgaard, Mathias B{\\ae}k Tejs Knudsen and Noy Rotbart", "title": "Dynamic and Multi-functional Labeling Schemes", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate labeling schemes supporting adjacency, ancestry, sibling, and\nconnectivity queries in forests. In the course of more than 20 years, the\nexistence of $\\log n + O(\\log \\log)$ labeling schemes supporting each of these\nfunctions was proven, with the most recent being ancestry [Fraigniaud and\nKorman, STOC '10]. Several multi-functional labeling schemes also enjoy lower\nor upper bounds of $\\log n + \\Omega(\\log \\log n)$ or $\\log n + O(\\log \\log n)$\nrespectively. Notably an upper bound of $\\log n + 5\\log \\log n$ for\nadjacency+siblings and a lower bound of $\\log n + \\log \\log n$ for each of the\nfunctions siblings, ancestry, and connectivity [Alstrup et al., SODA '03]. We\nimprove the constants hidden in the $O$-notation. In particular we show a $\\log\nn + 2\\log \\log n$ lower bound for connectivity+ancestry and\nconnectivity+siblings, as well as an upper bound of $\\log n + 3\\log \\log n +\nO(\\log \\log \\log n)$ for connectivity+adjacency+siblings by altering existing\nmethods.\n  In the context of dynamic labeling schemes it is known that ancestry requires\n$\\Omega(n)$ bits [Cohen, et al. PODS '02]. In contrast, we show upper and lower\nbounds on the label size for adjacency, siblings, and connectivity of $2\\log n$\nbits, and $3 \\log n$ to support all three functions. There exist efficient\nadjacency labeling schemes for planar, bounded treewidth, bounded arboricity\nand interval graphs. In a dynamic setting, we show a lower bound of $\\Omega(n)$\nfor each of those families.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 18:44:57 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Rotbart", "Noy", ""]]}, {"id": "1404.5458", "submitter": "Yuri Gordienko G.", "authors": "Yuri Gordienko, Lev Bekenov, Olexandr Gatsenko, Elena Zasimchuk,\n  Valentin Tatarenko", "title": "Complex Workflow Management and Integration of Distributed Computing\n  Resources by Science Gateway Portal for Molecular Dynamics Simulations in\n  Materials Science", "comments": "8 pages, 8 figures; Proc. of Third International Conference \"High\n  Performance Computing\" HPC-UA 2013 (Ukraine, Kyiv, October 7-11, 2013)\n  (http://hpc-ua.org/hpc-ua-13/proceedings). ISBN 978-966-7690-16-8 (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"IMP Science Gateway Portal\" (http://scigate.imp.kiev.ua) for complex\nworkflow management and integration of distributed computing resources (like\nclusters, service grids, desktop grids, clouds) is presented. It is created on\nthe basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for\nscience workflow operation and gUSE - for smooth integration of available\nresources for parallel and distributed computing in various heterogeneous\ndistributed computing infrastructures (DCI). The typical scientific workflow\nwith possible scenarios of its preparation and usage is considered. Several\ntypical science applications (scientific workflows) are considered for\nmolecular dynamics (MD) simulations of complex behavior of various\nnanostructures (nanoindentation of graphene layers, defect system relaxation in\nmetal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The\nadvantages and drawbacks of the solution are shortly analyzed in the context of\nits practical applications for MD simulations in materials science, physics and\nnanotechnologies with available heterogeneous DCIs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 11:34:04 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Gordienko", "Yuri", ""], ["Bekenov", "Lev", ""], ["Gatsenko", "Olexandr", ""], ["Zasimchuk", "Elena", ""], ["Tatarenko", "Valentin", ""]]}, {"id": "1404.5528", "submitter": "Mohammad Shojafar", "authors": "Saeed Javanmardi, Mohammad Shojafar, Danilo Amendola, Nicola\n  Cordeschi, Hongbo Liu, Ajith Abraham", "title": "Hybrid Genetic Algorithm for Cloud Computing Applications", "comments": "10 Pages, 5 figures, 1 table, IBICA2014, Accepted to publish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper with the aid of genetic algorithm and fuzzy theory, we present\na hybrid job scheduling approach, which considers the load balancing of the\nsystem and reduces total execution time and execution cost. We try to modify\nthe standard Genetic algorithm and to reduce the iteration of creating\npopulation with the aid of fuzzy theory. The main goal of this research is to\nassign the jobs to the resources with considering the VM MIPS and length of\njobs. The new algorithm assigns the jobs to the resources with considering the\njob length and resources capacities. We evaluate the performance of our\napproach with some famous cloud scheduling models. The results of the\nexperiments show the efficiency of the proposed approach in term of execution\ntime, execution cost and average Degree of Imbalance (DI).\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 15:33:39 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Javanmardi", "Saeed", ""], ["Shojafar", "Mohammad", ""], ["Amendola", "Danilo", ""], ["Cordeschi", "Nicola", ""], ["Liu", "Hongbo", ""], ["Abraham", "Ajith", ""]]}, {"id": "1404.5552", "submitter": "James Elliott", "authors": "James Elliott and Mark Hoemmen and Frank Mueller", "title": "Tolerating Silent Data Corruption in Opaque Preconditioners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate algorithm-based fault tolerance for silent, transient data\ncorruption in \"black-box\" preconditioners. We consider both additive Schwarz\ndomain decomposition with an ILU(k) subdomain solver, and algebraic multigrid,\nboth implemented in the Trilinos library. We evaluate faults that corrupt\npreconditioner results in both single and multiple MPI ranks. We then analyze\nhow our approach behaves when then application is scaled. Our technique is\nbased on a Selective Reliability approach that performs most operations in an\nunreliable mode, with only a few operations performed reliably. We also\ninvestigate two responses to faults and discuss the performance overheads\nimposed by each. For a non-symmetric problem solved using GMRES and ILU, we\nshow that at scale our fault tolerance approach incurs only 22% overhead for\nthe worst case. With detection techniques, we are able to reduce this overhead\nto 1.8% in the worst case.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 16:48:30 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Elliott", "James", ""], ["Hoemmen", "Mark", ""], ["Mueller", "Frank", ""]]}, {"id": "1404.5611", "submitter": "Yuri Gordienko G.", "authors": "Yuri Gordienko, Lev Bekenev, Olexandra Baskova, Olexander Gatsenko,\n  Elena Zasimchuk, Sergii Stirenko", "title": "IMP Science Gateway: from the Portal to the Hub of Virtual Experimental\n  Labs in Materials Science", "comments": "6 pages, 5 figures, 3 tables; 6th International Workshop on Science\n  Gateways, IWSG-2014 (Dublin, Ireland, 3-5 June, 2014). arXiv admin note:\n  substantial text overlap with arXiv:1404.5458", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Science gateway\" (SG) ideology means a user-friendly intuitive interface\nbetween scientists (or scientific communities) and different software\ncomponents + various distributed computing infrastructures (DCIs) (like grids,\nclouds, clusters), where researchers can focus on their scientific goals and\nless on peculiarities of software/DCI. \"IMP Science Gateway Portal\"\n(http://scigate.imp.kiev.ua) for complex workflow management and integration of\ndistributed computing resources (like clusters, service grids, desktop grids,\nclouds) is presented. It is created on the basis of WS-PGRADE and gUSE\ntechnologies, where WS-PGRADE is designed for science workflow operation and\ngUSE - for smooth integration of available resources for parallel and\ndistributed computing in various heterogeneous distributed computing\ninfrastructures (DCI). The typical scientific workflows with possible scenarios\nof its preparation and usage are presented. Several typical use cases for these\nscience applications (scientific workflows) are considered for molecular\ndynamics (MD) simulations of complex behavior of various nanostructures\n(nanoindentation of graphene layers, defect system relaxation in metal\nnanocrystals, thermal stability of boron nitride nanotubes, etc.). The user\nexperience is analyzed in the context of its practical applications for MD\nsimulations in materials science, physics and nanotechnologies with available\nheterogeneous DCIs. In conclusion, the \"science gateway\" approach - workflow\nmanager (like WS-PGRADE) + DCI resources manager (like gUSE)- gives opportunity\nto use the SG portal (like \"IMP Science Gateway Portal\") in a very promising\nway, namely, as a hub of various virtual experimental labs (different software\ncomponents + various requirements to resources) in the context of its practical\nMD applications in materials science, physics, chemistry, biology, and\nnanotechnologies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 11:55:17 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Gordienko", "Yuri", ""], ["Bekenev", "Lev", ""], ["Baskova", "Olexandra", ""], ["Gatsenko", "Olexander", ""], ["Zasimchuk", "Elena", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1404.5686", "submitter": "Liu Yue", "authors": "Yue Liu, Songlin Hu, Tilmann Rabl, Wantao Liu, Hans-Arno Jacobsen,\n  Kaifeng Wu, Jian Chen, Jintao Li", "title": "DGFIndex for Smart Grid: Enhancing Hive with a Cost-Effective\n  Multidimensional Range Index", "comments": "12 pages, VLDB 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Smart Grid applications, as the number of deployed electric smart meters\nincreases, massive amounts of valuable meter data is generated and collected\nevery day. To enable reliable data collection and make business decisions fast,\nhigh throughput storage and high-performance analysis of massive meter data\nbecome crucial for grid companies. Considering the advantage of high\nefficiency, fault tolerance, and price-performance of Hadoop and Hive systems,\nthey are frequently deployed as underlying platform for big data processing.\nHowever, in real business use cases, these data analysis applications typically\ninvolve multidimensional range queries (MDRQ) as well as batch reading and\nstatistics on the meter data. While Hive is high-performance at complex data\nbatch reading and analysis, it lacks efficient indexing techniques for MDRQ.\n  In this paper, we propose DGFIndex, an index structure for Hive that\nefficiently supports MDRQ for massive meter data. DGFIndex divides the data\nspace into cubes using the grid file technique. Unlike the existing indexes in\nHive, which stores all combinations of multiple dimensions, DGFIndex only\nstores the information of cubes. This leads to smaller index size and faster\nquery processing. Furthermore, with pre-computing user-defined aggregations of\neach cube, DGFIndex only needs to access the boundary region for aggregation\nquery. Our comprehensive experiments show that DGFIndex can save significant\ndisk space in comparison with the existing indexes in Hive and the query\nperformance with DGFIndex is 2-50 times faster than existing indexes in Hive\nand HadoopDB for aggregation query, 2-5 times faster than both for\nnon-aggregation query, 2-75 times faster than scanning the whole table in\ndifferent query selectivity.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 02:44:26 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 02:32:27 GMT"}, {"version": "v3", "created": "Wed, 9 Jul 2014 09:02:23 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Liu", "Yue", ""], ["Hu", "Songlin", ""], ["Rabl", "Tilmann", ""], ["Liu", "Wantao", ""], ["Jacobsen", "Hans-Arno", ""], ["Wu", "Kaifeng", ""], ["Chen", "Jian", ""], ["Li", "Jintao", ""]]}, {"id": "1404.5756", "submitter": "Salvatore Cuomo", "authors": "R. Farina, S. Dobricic, A. Storto, S. Masina and S. Cuomo", "title": "A Revised Scheme to Compute Horizontal Covariances in an Oceanographic\n  3D-VAR Assimilation System", "comments": "26 pages", "journal-ref": null, "doi": "10.1016/j.jcp.2015.01.003", "report-no": null, "categories": "cs.NA cs.CE cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improvement of an oceanographic three dimensional variational\nassimilation scheme (3D-VAR), named OceanVar, by introducing a recursive filter\n(RF) with the third order of accuracy (3rd-RF), instead of a RF with first\norder of accuracy (1st-RF), to approximate horizontal Gaussian covariances. An\nadvantage of the proposed scheme is that the CPU's time can be substantially\nreduced with benefits on the large scale applications. Experiments estimating\nthe impact of 3rd-RF are performed by assimilating oceanographic data in two\nrealistic oceanographic applications. The results evince benefits in terms of\nassimilation process computational time, accuracy of the Gaussian correlation\nmodeling, and show that the 3rd-RF is a suitable tool for operational data\nassimilation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 09:22:17 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Farina", "R.", ""], ["Dobricic", "S.", ""], ["Storto", "A.", ""], ["Masina", "S.", ""], ["Cuomo", "S.", ""]]}, {"id": "1404.5764", "submitter": "Yuri Gordienko G.", "authors": "Olexander Gatsenko, Lev Bekenev, Evgen Pavlov, Yuri G. Gordienko", "title": "From Quantity to Quality: Massive Molecular Dynamics Simulation of\n  Nanostructures under Plastic Deformation in Desktop and Service Grid\n  Distributed Computing Infrastructure", "comments": "13 pages, 11 pages (http://journals.agh.edu.pl/csci/article/view/106)", "journal-ref": "Computer Science 14 (1), 27-44 (2013)", "doi": "10.7494/csci.2013.14.1.27", "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed computing infrastructure (DCI) on the basis of BOINC and\nEDGeS-bridge technologies for high-performance distributed computing is used\nfor porting the sequential molecular dynamics (MD) application to its parallel\nversion for DCI with Desktop Grids (DGs) and Service Grids (SGs). The actual\nmetrics of the working DG-SG DCI were measured, and the normal distribution of\nhost performances, and signs of log-normal distributions of other\ncharacteristics (CPUs, RAM, and HDD per host) were found. The practical\nfeasibility and high efficiency of the MD simulations on the basis of DG-SG DCI\nwere demonstrated during the experiment with the massive MD simulations for the\nlarge quantity of aluminum nanocrystals ($\\sim10^2$-$10^3$). Statistical\nanalysis (Kolmogorov-Smirnov test, moment analysis, and bootstrapping analysis)\nof the defect density distribution over the ensemble of nanocrystals had shown\nthat change of plastic deformation mode is followed by the qualitative change\nof defect density distribution type over ensemble of nanocrystals. Some\nlimitations (fluctuating performance, unpredictable availability of resources,\netc.) of the typical DG-SG DCI were outlined, and some advantages (high\nefficiency, high speedup, and low cost) were demonstrated. Deploying on DG DCI\nallows to get new scientific $\\it{quality}$ from the simulated $\\it{quantity}$\nof numerous configurations by harnessing sufficient computational power to\nundertake MD simulations in a wider range of physical parameters\n(configurations) in a much shorter timeframe.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 09:42:50 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Gatsenko", "Olexander", ""], ["Bekenev", "Lev", ""], ["Pavlov", "Evgen", ""], ["Gordienko", "Yuri G.", ""]]}, {"id": "1404.5813", "submitter": "Dmitry N. Kozlov", "authors": "Dmitry N. Kozlov", "title": "Topology of the immediate snapshot complexes", "comments": "final version as it appears in Topology and it Applications, Article\n  number 5275. note: this paper is a full version of the second half of the\n  previous research announcement, which was posted as arXiv:1402.4707", "journal-ref": "Topology Appl. 178 (2014), 160-184", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The immediate snapshot complexes were introduced as combinatorial models for\nthe protocol complexes in the context of theoretical distributed computing. In\nthe previous work we have developed a formal language of witness structures in\norder to define and to analyze these complexes.\n  In this paper, we study topology of immediate snapshot complexes. It was\nknown that these complexes are always pure and that they are pseudomanifolds.\nHere we prove two further independent topological properties. First, we show\nthat immediate snapshot complexes are collapsible. Second, we show that these\ncomplexes are homeomorphic to closed balls. Specifically, given any immediate\nsnapshot complex $P(\\tr)$, we show that there exists a homeomorphism\n$\\varphi:\\da^{|\\supp\\tr|-1}\\ra P(\\tr)$, such that $\\varphi(\\sigma)$ is a\nsubcomplex of $P(\\tr)$, whenever $\\sigma$ is a simplex in the simplicial\ncomplex $\\da^{|\\supp\\tr|-1}$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 13:05:14 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 12:52:28 GMT"}, {"version": "v3", "created": "Mon, 24 Nov 2014 10:01:10 GMT"}, {"version": "v4", "created": "Tue, 26 Apr 2016 11:53:44 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Kozlov", "Dmitry N.", ""]]}, {"id": "1404.5997", "submitter": "Alex Krizhevsky", "authors": "Alex Krizhevsky", "title": "One weird trick for parallelizing convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a new way to parallelize the training of convolutional neural\nnetworks across multiple GPUs. The method scales significantly better than all\nalternatives when applied to modern convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 22:37:56 GMT"}, {"version": "v2", "created": "Sat, 26 Apr 2014 23:10:51 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Krizhevsky", "Alex", ""]]}, {"id": "1404.6218", "submitter": "Ashkan Tousimojarad Mr", "authors": "Ashkan Tousimojarad and Wim Vanderbauwhede", "title": "A Parallel Task-based Approach to Linear Algebra", "comments": "Final version as appeared in \"dx.doi.org/10.1109/ISPDC.2014.11\"", "journal-ref": "Tousimojarad, A., Vanderbauwhede, W.: A parallel task-based\n  approach to linear algebra. In: Parallel and Distributed Computing (ISPDC),\n  2014 IEEE 13th International Symposium on. pp. 59-66. IEEE (2014)", "doi": "10.1109/ISPDC.2014.11", "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processors with large numbers of cores are becoming commonplace. In order to\ntake advantage of the available resources in these systems, the programming\nparadigm has to move towards increased parallelism. However, increasing the\nlevel of concurrency in the program does not necessarily lead to better\nperformance. Parallel programming models have to provide flexible ways of\ndefining parallel tasks and at the same time, efficiently managing the created\ntasks. OpenMP is a widely accepted programming model for shared-memory\narchitectures. In this paper we highlight some of the drawbacks in the OpenMP\ntasking approach, and propose an alternative model based on the Glasgow\nParallel Reduction Machine (GPRM) programming framework. As the main focus of\nthis study, we deploy our model to solve a fundamental linear algebra problem,\nLU factorisation of sparse matrices. We have used the SparseLU benchmark from\nthe BOTS benchmark suite, and compared the results obtained from our model to\nthose of the OpenMP tasking approach. The TILEPro64 system has been used to run\nthe experiments. The results are very promising, not only because of the\nperformance improvement for this particular problem, but also because they\nverify the task management efficiency, stability, and flexibility of our model,\nwhich can be applied to solve problems in future many-core systems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:39:30 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 14:53:58 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 15:46:24 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Tousimojarad", "Ashkan", ""], ["Vanderbauwhede", "Wim", ""]]}, {"id": "1404.6415", "submitter": "Ana Mihut", "authors": "Anubis G. M. Rossetto, Cl\\'audio F. R. Geyer, Luciana Arantes, Pierre\n  Sens", "title": "The Impact Failure Detector", "comments": "EDCC-2014, Fast-Abstracts, failure detector, impact factor, trust\n  level", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new and flexible unreliable failure detector whose\noutput is related to the trust level of a set of processes. By expressing the\nrelevance of each process of the set by an impact factor value, our approach\nallows the tuning of the detector output, making possible a softer or stricter\nmonitoring. The idea behind our proposal is that, according to an acceptable\nmargin of failures and the impact factor assigned to processes, in some\nscenarios, the failure of some low impact processes may not change the user\nconfidence in the set of processes, while the crash of a high impact factor\nprocess may seriously affect it. We outline the application scenarios and the\nproposed unreliable failure detector, giving a detailed account of the concept\non which it is based.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 13:35:47 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Rossetto", "Anubis G. M.", ""], ["Geyer", "Cl\u00e1udio F. R.", ""], ["Arantes", "Luciana", ""], ["Sens", "Pierre", ""]]}, {"id": "1404.6561", "submitter": "Michael Borokhovich", "authors": "Chen Avin, Michael Borokhovich, Zvi Lotker, David Peleg", "title": "Distributed Computing on Core-Periphery Networks: Axiom-based Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by social networks and complex systems, we propose a core-periphery\nnetwork architecture that supports fast computation for many distributed\nalgorithms and is robust and efficient in number of links. Rather than\nproviding a concrete network model, we take an axiom-based design approach. We\nprovide three intuitive (and independent) algorithmic axioms and prove that any\nnetwork that satisfies all axioms enjoys an efficient algorithm for a range of\ntasks (e.g., MST, sparse matrix multiplication, etc.). We also show the\nminimality of our axiom set: for networks that satisfy any subset of the\naxioms, the same efficiency cannot be guaranteed for any deterministic\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 21:28:14 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 04:07:27 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Avin", "Chen", ""], ["Borokhovich", "Michael", ""], ["Lotker", "Zvi", ""], ["Peleg", "David", ""]]}, {"id": "1404.6681", "submitter": "Sparsh Mittal", "authors": "Sparsh Mittal", "title": "Power Management Techniques for Data Centers: A Survey", "comments": "Keywords: Data Centers, Power Management, Low-power Design, Energy\n  Efficiency, Green Computing, DVFS, Server Consolidation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing use of internet and exponential growth in amount of data to be\nstored and processed (known as 'big data'), the size of data centers has\ngreatly increased. This, however, has resulted in significant increase in the\npower consumption of the data centers. For this reason, managing power\nconsumption of data centers has become essential. In this paper, we highlight\nthe need of achieving energy efficiency in data centers and survey several\nrecent architectural techniques designed for power management of data centers.\nWe also present a classification of these techniques based on their\ncharacteristics. This paper aims to provide insights into the techniques for\nimproving energy efficiency of data centers and encourage the designers to\ninvent novel solutions for managing the large power dissipation of data\ncenters.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 20:39:09 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 14:58:38 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Mittal", "Sparsh", ""]]}, {"id": "1404.6687", "submitter": "Yin Sun", "authors": "Shengbo Chen, Yin Sun, Ulas C. Kozat, Longbo Huang, Prasun Sinha,\n  Guanfeng Liang, Xin Liu, Ness B. Shroff", "title": "When Queueing Meets Coding: Optimal-Latency Data Retrieving Scheme in\n  Storage Clouds", "comments": "Original accepted by IEEE Infocom 2014, 9 pages. Some statements in\n  the Infocom paper are corrected", "journal-ref": null, "doi": "10.1109/INFOCOM.2014.6848034", "report-no": null, "categories": "cs.IT cs.DC cs.NI cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of reducing the delay of downloading data\nfrom cloud storage systems by leveraging multiple parallel threads, assuming\nthat the data has been encoded and stored in the clouds using fixed rate\nforward error correction (FEC) codes with parameters (n, k). That is, each file\nis divided into k equal-sized chunks, which are then expanded into n chunks\nsuch that any k chunks out of the n are sufficient to successfully restore the\noriginal file. The model can be depicted as a multiple-server queue with\narrivals of data retrieving requests and a server corresponding to a thread.\nHowever, this is not a typical queueing model because a server can terminate\nits operation, depending on when other servers complete their service (due to\nthe redundancy that is spread across the threads). Hence, to the best of our\nknowledge, the analysis of this queueing model remains quite uncharted.\n  Recent traces from Amazon S3 show that the time to retrieve a fixed size\nchunk is random and can be approximated as a constant delay plus an i.i.d.\nexponentially distributed random variable. For the tractability of the\ntheoretical analysis, we assume that the chunk downloading time is i.i.d.\nexponentially distributed. Under this assumption, we show that any\nwork-conserving scheme is delay-optimal among all on-line scheduling schemes\nwhen k = 1. When k > 1, we find that a simple greedy scheme, which allocates\nall available threads to the head of line request, is delay optimal among all\non-line scheduling schemes. We also provide some numerical results that point\nto the limitations of the exponential assumption, and suggest further research\ndirections.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 21:32:13 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chen", "Shengbo", ""], ["Sun", "Yin", ""], ["Kozat", "Ulas C.", ""], ["Huang", "Longbo", ""], ["Sinha", "Prasun", ""], ["Liang", "Guanfeng", ""], ["Liu", "Xin", ""], ["Shroff", "Ness B.", ""]]}, {"id": "1404.6719", "submitter": "Parisa Jalili Marandi", "authors": "Parisa Jalili Marandi, Samuel Benz, Fernando Pedone, Ken Birman", "title": "Practical Experience Report: The Performance of Paxos in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This experience report presents the results of an extensive performance\nevaluation conducted using four open-source implementations of Paxos deployed\nin Amazon's EC2. Paxos is a fundamental algorithm for building fault-tolerant\nservices, at the core of state-machine replication. Implementations of Paxos\nare currently used in many prototypes and production systems in both academia\nand industry. Although all protocols surveyed in the paper implement Paxos,\nthey are optimized in a number of different ways, resulting in very different\nbehavior, as we show in the paper. We have considered a variety of\nconfigurations and failure-free and faulty executions. In addition to reporting\nour findings, we propose and assess additional optimizations to existing\nimplementations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 07:14:47 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Marandi", "Parisa Jalili", ""], ["Benz", "Samuel", ""], ["Pedone", "Fernando", ""], ["Birman", "Ken", ""]]}, {"id": "1404.6721", "submitter": "Parisa Jalili Marandi", "authors": "Parisa Jalili Marandi, Fernando Pedone", "title": "Optimistic Parallel State-Machine Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-machine replication, a fundamental approach to fault tolerance,\nrequires replicas to execute commands deterministically, which usually results\nin sequential execution of commands. Sequential execution limits performance\nand underuses servers, which are increasingly parallel (i.e., multicore). To\nnarrow the gap between state-machine replication requirements and the\ncharacteristics of modern servers, researchers have recently come up with\nalternative execution models. This paper surveys existing approaches to\nparallel state-machine replication and proposes a novel optimistic protocol\nthat inherits the scalable features of previous techniques. Using a replicated\nB+-tree service, we demonstrate in the paper that our protocol outperforms the\nmost efficient techniques by a factor of 2.4 times.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 07:34:36 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Marandi", "Parisa Jalili", ""], ["Pedone", "Fernando", ""]]}, {"id": "1404.6772", "submitter": "Ana Mihut", "authors": "Hermann Kopetz", "title": "Why a Global Time is Needed in a Dependable SoS", "comments": "EDCC-2014, EDSoS-2014, System-of-Systems, global time, clock,\n  synchronization, sparse time base, error detection, dependability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system-of-systems (SoS) is a large information processing system formed by\nthe integration of autonomous computer systems (called constituent systems,\nCS), physical machines and humans for the purpose of providing new synergistic\nservices and/or more efficient economic processes. In a number of applications,\ne.g robotics, the autonomous CSs must coordinate their actions in the temporal\ndomain to realize the desired objectives. In this paper we argue that the\nintroduction of a proper global physical time establishes a shared view about\nthe progress of physical time and helps to realize the temporal coordination of\nthe autonomous CSs. The available global time can also be used to simplify the\nsolution of many challenging problems within the SoS, such as distributed\nresource allocation, and helps to improve the dependability and fault-tolerance\nof the SoS.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:33:12 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Kopetz", "Hermann", ""]]}, {"id": "1404.6878", "submitter": "Wantao Liu", "authors": "Songlin Hu, Wantao Liu, Tilmann Rabl, Shuo Huang, Ying Liang, Zheng\n  Xiao, Hans-Arno Jacobsen, Xubin Pei, Jiye Wang", "title": "DualTable: A Hybrid Storage Model for Update Optimization in Hive", "comments": "accepted by industry session of ICDE2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hive is the most mature and prevalent data warehouse tool providing SQL-like\ninterface in the Hadoop ecosystem. It is successfully used in many Internet\ncompanies and shows its value for big data processing in traditional\nindustries. However, enterprise big data processing systems as in Smart Grid\napplications usually require complicated business logics and involve many data\nmanipulation operations like updates and deletes. Hive cannot offer sufficient\nsupport for these while preserving high query performance. Hive using the\nHadoop Distributed File System (HDFS) for storage cannot implement data\nmanipulation efficiently and Hive on HBase suffers from poor query performance\neven though it can support faster data manipulation.There is a project based on\nHive issue Hive-5317 to support update operations, but it has not been finished\nin Hive's latest version. Since this ACID compliant extension adopts same data\nstorage format on HDFS, the update performance problem is not solved.\n  In this paper, we propose a hybrid storage model called DualTable, which\ncombines the efficient streaming reads of HDFS and the random write capability\nof HBase. Hive on DualTable provides better data manipulation support and\npreserves query performance at the same time. Experiments on a TPC-H data set\nand on a real smart grid data set show that Hive on DualTable is up to 10 times\nfaster than Hive when executing update and delete operations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 06:43:36 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 05:45:45 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Hu", "Songlin", ""], ["Liu", "Wantao", ""], ["Rabl", "Tilmann", ""], ["Huang", "Shuo", ""], ["Liang", "Ying", ""], ["Xiao", "Zheng", ""], ["Jacobsen", "Hans-Arno", ""], ["Pei", "Xubin", ""], ["Wang", "Jiye", ""]]}, {"id": "1404.6929", "submitter": "Peter Elmer", "authors": "David Abdurachmanov, Peter Elmer, Giulio Eulisse, Paola Grosso, Curtis\n  Hillegas, Burt Holzman, Ruben L. Janssen, Sander Klous, Robert Knight,\n  Shahzad Muzaffar", "title": "Power-aware applications for scientific cluster and distributed\n  computing", "comments": "Submitted to proceedings of International Symposium on Grids and\n  Clouds (ISGC) 2014, 23-28 March 2014, Academia Sinica, Taipei, Taiwan", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aggregate power use of computing hardware is an important cost factor in\nscientific cluster and distributed computing systems. The Worldwide LHC\nComputing Grid (WLCG) is a major example of such a distributed computing\nsystem, used primarily for high throughput computing (HTC) applications. It has\na computing capacity and power consumption rivaling that of the largest\nsupercomputers. The computing capacity required from this system is also\nexpected to grow over the next decade. Optimizing the power utilization and\ncost of such systems is thus of great interest.\n  A number of trends currently underway will provide new opportunities for\npower-aware optimizations. We discuss how power-aware software applications and\nscheduling might be used to reduce power consumption, both as autonomous\nentities and as part of a (globally) distributed system. As concrete examples\nof computing centers we provide information on the large HEP-focused Tier-1 at\nFNAL, and the Tigress High Performance Computing Center at Princeton\nUniversity, which provides HPC resources in a university context.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 10:57:04 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 21:25:23 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Abdurachmanov", "David", ""], ["Elmer", "Peter", ""], ["Eulisse", "Giulio", ""], ["Grosso", "Paola", ""], ["Hillegas", "Curtis", ""], ["Holzman", "Burt", ""], ["Janssen", "Ruben L.", ""], ["Klous", "Sander", ""], ["Knight", "Robert", ""], ["Muzaffar", "Shahzad", ""]]}, {"id": "1404.7494", "submitter": "Kishan Gandhi bipinchandra", "authors": "Gandhi Kishan Bipinchandra, Prof. Rajanikanth Aluvalu, Dr.Ajay Shanker\n  Singh", "title": "Intelligent Resource Allocation Technique For Desktop-as-a-Service in\n  Cloud Environment", "comments": "6 pages, 3 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The specialty of desktop-as-a-service cloud computing is that user can access\ntheir desktop and can execute applications in virtual desktops on remote\nservers. Resource management and resource utilization are most significant in\nthe area of desktop-as-a-service, cloud computing; however, handling a large\namount of clients in the most efficient manner poses important challenges.\nEspecially deciding how many clients to handle on one server, and where to\nexecute the user applications at each time is important. This is because we\nhave to ensure maximum resource utilization along with user data\nconfidentiality, customer satisfaction, scalability, minimum Service level\nagreement (SLA) violation etc. Assigning too many users to one server leads to\ncustomer dissatisfaction, while assigning too little leads to higher\ninvestments costs. So we have taken into consideration these two situations\nalso. We study different aspects to optimize the resource usage and customer\nsatisfaction. Here in this paper We proposed Intelligent Resource Allocation\n(IRA) Technique which assures the above mentioned parameters like minimum SLA\nviolation. For this, priorities are assigned to user requests based on their\nSLA Factors in order to maintain their confidentiality. The results of the\npaper indicate that by applying IRA Technique to the already existing\noverbooking mechanism will improve the performance of the system with\nsignificant reduction in SLA violation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 19:17:32 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Bipinchandra", "Gandhi Kishan", ""], ["Aluvalu", "Prof. Rajanikanth", ""], ["Singh", "Dr. Ajay Shanker", ""]]}, {"id": "1404.7548", "submitter": "Ana Mihut", "authors": "Ryan Emerson, Paul Ezhilchelvan", "title": "Faster Transaction Commit even when Nodes Crash", "comments": "EDCC-2014, Fast-Abstracts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic broadcasts play a central role in serialisable in-memory transactions.\nBest performing ones block, when a node crashes, until a new view is installed.\nWe augment a new protocol for uninterrupted progress in the interim period.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 22:16:48 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 21:03:48 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Emerson", "Ryan", ""], ["Ezhilchelvan", "Paul", ""]]}, {"id": "1404.7559", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari", "title": "Near-Optimal Distributed Approximation of Minimum-Weight Connected\n  Dominating Set", "comments": "An extended abstract version of this result appears in the\n  proceedings of 41st International Colloquium on Automata, Languages, and\n  Programming (ICALP 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a near-optimal distributed approximation algorithm for\nthe minimum-weight connected dominating set (MCDS) problem. The presented\nalgorithm finds an $O(\\log n)$ approximation in $\\tilde{O}(D+\\sqrt{n})$ rounds,\nwhere $D$ is the network diameter and $n$ is the number of nodes.\n  MCDS is a classical NP-hard problem and the achieved approximation factor\n$O(\\log n)$ is known to be optimal up to a constant factor, unless P=NP.\nFurthermore, the $\\tilde{O}(D+\\sqrt{n})$ round complexity is known to be\noptimal modulo logarithmic factors (for any approximation), following [Das\nSarma et al.---STOC'11].\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 00:30:51 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Ghaffari", "Mohsen", ""]]}, {"id": "1404.7634", "submitter": "Arnaud Casteigts", "authors": "Matthieu Barjon, Arnaud Casteigts, Serge Chaumette, Colette Johnen,\n  Yessin M. Neggaz", "title": "Testing Temporal Connectivity in Sparse Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of testing whether a given dynamic graph is temporally\nconnected, {\\it i.e} a temporal path (also called a {\\em journey}) exists\nbetween all pairs of vertices. We consider a discrete version of the problem,\nwhere the topology is given as an evolving graph ${\\cal\nG}=\\{G_1,G_2,...,G_{k}\\}$ whose set of vertices is invariant and the set of\n(directed) edges varies over time. Two cases are studied, depending on whether\na single edge or an unlimited number of edges can be crossed in a same $G_i$\n(strict journeys {\\it vs} non-strict journeys).\n  In the case of {\\em strict} journeys, a number of existing algorithms\ndesigned for more general problems can be adapted. We adapt one of them to the\nabove formulation of the problem and characterize its running time complexity.\nThe parameters of interest are the length of the graph sequence $k=|{\\cal G}|$,\nthe maximum {\\em instant} density $\\mu=max(|E_i|)$, and the {\\em cumulated}\ndensity $m=|\\cup E_i|$. Our algorithm has a time complexity of $O(k\\mu n)$,\nwhere $n$ is the number of nodes. This complexity is compared to that of the\nother solutions: one is always more costly (keep in mind that is solves a more\ngeneral problem), the other one is more or less costly depending on the\ninterplay between instant density and cumulated density. The length $k$ of the\nsequence also plays a role. We characterize the key values of $k, \\mu$ and $m$\nfor which either algorithm should be used.\n  In the case of {\\em non-strict} journeys, for which no algorithm is known, we\nshow that some pre-processing of the input graph allows us to re-use the same\nalgorithm than before. By chance, these operations happens to cost again\n$O(k\\mu n)$ time, which implies that the second problem is not more difficult\nthan the first.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 08:53:27 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 14:13:39 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Barjon", "Matthieu", ""], ["Casteigts", "Arnaud", ""], ["Chaumette", "Serge", ""], ["Johnen", "Colette", ""], ["Neggaz", "Yessin M.", ""]]}, {"id": "1404.7671", "submitter": "Christoforos Raptopoulos Christoforos Raptopoulos", "authors": "George B. Mertzios, Sotiris E. Nikoletseas, Christoforos L.\n  Raptopoulos, Paul G. Spirakis", "title": "Determining Majority in Networks with Local Interactions and very Small\n  Local Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study here the problem of determining the majority type in an arbitrary\nconnected network, each vertex of which has initially two possible types. The\nvertices may have a few additional possible states and can interact in pairs\nonly if they share an edge. Any (population) protocol is required to stabilize\nin the initial majority. We first present and analyze a protocol with 4 states\nper vertex that always computes the initial majority value, under any fair\nscheduler. As we prove, this protocol is optimal, in the sense that there is no\npopulation protocol that always computes majority with fewer than 4 states per\nvertex. However this does not rule out the existence of a protocol with 3\nstates per vertex that is correct with high probability. To this end, we\nexamine a very natural majority protocol with 3 states per vertex, introduced\nin [Angluin et al. 2008] where its performance has been analyzed for the clique\ngraph. We study the performance of this protocol in arbitrary networks. We\nprove that, when the two initial states are put uniformly at random on the\nvertices, this protocol converges to the initial majority with probability\nhigher than the probability of converging to the initial minority. In contrast,\nwe present an infinite family of graphs, on which the protocol can fail whp,\neven when the difference between the initial majority and the initial minority\nis $n - \\Theta(\\ln{n})$. We also present another infinite family of graphs in\nwhich the protocol of Angluin et al. takes an expected exponential time to\nconverge. These two negative results build upon a very positive result\nconcerning the robustness of the protocol on the clique. Surprisingly, the\nresistance of the clique to failure causes the failure in general graphs. Our\ntechniques use new domination and coupling arguments for suitably defined\nprocesses whose dynamics capture the antagonism between the states involved.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 10:29:46 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Mertzios", "George B.", ""], ["Nikoletseas", "Sotiris E.", ""], ["Raptopoulos", "Christoforos L.", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "1404.7688", "submitter": "Matteo Dell'Amico Ph.D.", "authors": "Matteo Dell'Amico, Maurizio Filippone, Pietro Michiardi, Yves Roudier", "title": "On User Availability Prediction and Network Applications", "comments": "Accepted for publication in IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": "10.1109/TNET.2014.2321430", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User connectivity patterns in network applications are known to be\nheterogeneous, and to follow periodic (daily and weekly) patterns. In many\ncases, the regularity and the correlation of those patterns is problematic: for\nnetwork applications, many connected users create peaks of demand; in contrast,\nin peer-to-peer scenarios, having few users online results in a scarcity of\navailable resources. On the other hand, since connectivity patterns exhibit a\nperiodic behavior, they are to some extent predictable. This work shows how\nthis can be exploited to anticipate future user connectivity and to have\napplications proactively responding to it. We evaluate the probability that any\ngiven user will be online at any given time, and assess the prediction on\nsix-month availability traces from three different Internet applications.\nBuilding upon this, we show how our probabilistic approach makes it easy to\nevaluate and optimize the performance in a number of diverse network\napplication models, and to use them to optimize systems. In particular, we show\nhow this approach can be used in distributed hash tables, friend-to-friend\nstorage, and cache pre-loading for social networks, resulting in substantial\ngains in data availability and system efficiency at negligible costs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 11:24:26 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dell'Amico", "Matteo", ""], ["Filippone", "Maurizio", ""], ["Michiardi", "Pietro", ""], ["Roudier", "Yves", ""]]}, {"id": "1404.7760", "submitter": "Ana Mihut", "authors": "Wen Zeng, Chunyan Mu, Maciej Koutny, Paul Watson", "title": "A Flow Sensitive Security Model for Cloud Computing Systems", "comments": "EDCC-2014, EDSoS-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extent and importance of cloud computing is rapidly increasing due to the\never increasing demand for internet services and communications. Instead of\nbuilding individual information technology infrastructure to host databases or\nsoftware, a third party can host them in its large server clouds. Large\norganizations may wish to keep sensitive information on their more restricted\nservers rather than in the public cloud. This has led to the introduction of\nfederated cloud computing (FCC) in which both public and private cloud\ncomputing resources are used.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 15:23:49 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Zeng", "Wen", ""], ["Mu", "Chunyan", ""], ["Koutny", "Maciej", ""], ["Watson", "Paul", ""]]}]