[{"id": "0910.0097", "submitter": "Alexandre Vaniachine", "authors": "A. Vaniachine (for the ATLAS Collaboration)", "title": "Scalable Database Access Technologies for ATLAS Distributed Computing", "comments": "6 pages, 7 figures. To be published in the proceedings of DPF-2009,\n  Detroit, MI, July 2009, eConf C090726", "journal-ref": null, "doi": null, "report-no": "ANL-HEP-CP-09-085", "categories": "physics.ins-det cs.DB cs.DC hep-ex", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  ATLAS event data processing requires access to non-event data (detector\nconditions, calibrations, etc.) stored in relational databases. The\ndatabase-resident data are crucial for the event data reconstruction processing\nsteps and often required for user analysis. A main focus of ATLAS database\noperations is on the worldwide distribution of the Conditions DB data, which\nare necessary for every ATLAS data processing job. Since Conditions DB access\nis critical for operations with real data, we have developed the system where a\ndifferent technology can be used as a redundant backup. Redundant database\noperations infrastructure fully satisfies the requirements of ATLAS\nreprocessing, which has been proven on a scale of one billion database queries\nduring two reprocessing campaigns of 0.5 PB of single-beam and cosmics data on\nthe Grid. To collect experience and provide input for a best choice of\ntechnologies, several promising options for efficient database access in user\nanalysis were evaluated successfully. We present ATLAS experience with scalable\ndatabase access technologies and describe our approach for prevention of\ndatabase access bottlenecks in a Grid computing environment.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2009 07:20:54 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2009 13:55:12 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Vaniachine", "A.", "", "for the ATLAS Collaboration"]]}, {"id": "0910.0187", "submitter": "Ivan Voras", "authors": "Ivan Voras, Mario Zagar", "title": "Web-enabling Cache Daemon for Complex Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common basic techniques for improving the performance of web\napplications is caching frequently accessed data in fast data stores,\ncolloquially known as cache daemons. In this paper we present a cache daemon\nsuitable for storing complex data while maintaining fine-grained control over\ndata storage, retrieval and expiry. Data manipulation in this cache daemon is\nperformed via standard SQL statements so we call it SQLcached. It is a\npractical, usable solution already implemented in several large web sites.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2009 14:25:50 GMT"}], "update_date": "2009-10-02", "authors_parsed": [["Voras", "Ivan", ""], ["Zagar", "Mario", ""]]}, {"id": "0910.0317", "submitter": "Rdv Ijcsis", "authors": "Abbas Karimi, Faraneh Zarafshan, Adznan.b. Jantan, A.R Ramli, M.Iqbal\n  b.Saripan", "title": "A New Fuzzy Approach for Dynamic Load Balancing Algorithm", "comments": "5 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 1, pp. 01-05, October 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load balancing is the process of improving the Performance of a parallel and\ndistributed system through is distribution of load among the processors [1-2].\nMost of the previous work in load balancing and distributed decision making in\ngeneral, do not effectively take into account the uncertainty and inconsistency\nin state information but in fuzzy logic, we have advantage of using crisps\ninputs. In this paper, we present a new approach for implementing dynamic load\nbalancing algorithm with fuzzy logic, which can face to uncertainty and\ninconsistency of previous algorithms, further more our algorithm shows better\nresponse time than round robin and randomize algorithm respectively 30.84\npercent and 45.45 percent.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2009 03:32:09 GMT"}], "update_date": "2009-10-05", "authors_parsed": [["Karimi", "Abbas", ""], ["Zarafshan", "Faraneh", ""], ["Jantan", "Adznan. b.", ""], ["Ramli", "A. R", ""], ["Saripan", "M. Iqbal b.", ""]]}, {"id": "0910.0366", "submitter": "Daniel Cederman", "authors": "Daniel Cederman and Philippas Tsigas", "title": "Supporting Lock-Free Composition of Concurrent Data Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": "2009-10", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lock-free data objects offer several advantages over their blocking\ncounterparts, such as being immune to deadlocks and convoying and, more\nimportantly, being highly concurrent. But they share a common disadvantage in\nthat the operations they provide are difficult to compose into larger atomic\noperations while still guaranteeing lock-freedom. We present a lock-free\nmethodology for composing highly concurrent linearizable objects together by\nunifying their linearization points. This makes it possible to relatively\neasily introduce atomic lock-free move operations to a wide range of concurrent\nobjects. Experimental evaluation has shown that the operations originally\nsupported by the data objects keep their performance behavior under our\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2009 10:08:12 GMT"}], "update_date": "2009-10-05", "authors_parsed": [["Cederman", "Daniel", ""], ["Tsigas", "Philippas", ""]]}, {"id": "0910.0626", "submitter": "Mugurel Ionut Andreica", "authors": "Alexandru Costan, Corina Stratan, Eliana-Dina Tirsa, Mugurel Ionut\n  Andreica, Valentin Cristea", "title": "Towards a Grid Platform for Scientific Workflows Management", "comments": null, "journal-ref": "Proc. of the 17th Intl. Conf. on Control Systems and Computer\n  Science (CSCS), vol. 1, pp. 37-44, Bucharest, Romania, 26-29 May, 2009.\n  (ISSN: 2066-4451)", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workflow management systems allow the users to develop complex applications\nat a higher level, by orchestrating functional components without handling the\nimplementation details. Although a wide range of workflow engines are developed\nin enterprise environments, the open source engines available for scientific\napplications lack some functionalities or are too difficult to use for\nnon-specialists. Our purpose is to develop a workflow management platform for\ndistributed systems, that will provide features like an intuitive way to\ndescribe workflows, efficient data handling mechanisms and flexible fault\ntolerance support. We introduce here an architectural model for the workflow\nplatform, based on the ActiveBPEL workflow engine, which we propose to augment\nwith an additional set of components.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2009 18:28:16 GMT"}], "update_date": "2009-10-06", "authors_parsed": [["Costan", "Alexandru", ""], ["Stratan", "Corina", ""], ["Tirsa", "Eliana-Dina", ""], ["Andreica", "Mugurel Ionut", ""], ["Cristea", "Valentin", ""]]}, {"id": "0910.0663", "submitter": "Fei Wei", "authors": "Fei Wei, Huazhong Yang", "title": "Transmission line inspires a new distributed algorithm to solve linear\n  system of circuit", "comments": "This work was finished in Nov 2007. Recently we are preparing it for\n  IEEE Trans. CAD. More info, see my web page at\n  http://weifei00.googlepages.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transmission line, or wire, is always troublesome to integrated circuits\ndesigners, but it could be helpful to parallel computing researchers. This\npaper proposes the Virtual Transmission Method (VTM), which is a new\ndistributed and stationary iterative algorithm to solve the linear system\nextracted from circuit. It tears the circuit by virtual transmission lines to\nachieve distributed computing. For the symmetric positive definite (SPD) linear\nsystem, VTM is proved to be convergent. For the unsymmetrical linear system,\nnumerical experiments show that VTM is possible to achieve better convergence\nproperty than the traditional stationary algorithms. VTM could be accelerated\nby some preconditioning techniques, and the convergence speed of VTM is fast\nwhen its preconditioner is properly chosen.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 03:08:43 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2009 09:42:47 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2010 09:17:58 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2010 06:48:46 GMT"}, {"version": "v5", "created": "Tue, 7 Sep 2010 23:43:49 GMT"}, {"version": "v6", "created": "Sun, 31 Oct 2010 02:42:32 GMT"}, {"version": "v7", "created": "Thu, 9 Dec 2010 08:33:24 GMT"}], "update_date": "2010-12-10", "authors_parsed": [["Wei", "Fei", ""], ["Yang", "Huazhong", ""]]}, {"id": "0910.0708", "submitter": "Mugurel Ionut Andreica", "authors": "Ciprian Mihai Dobre, Florin Pop, Alexandru Costan, Mugurel Ionut\n  Andreica, Valentin Cristea", "title": "Robust Failure Detection Architecture for Large Scale Distributed\n  Systems", "comments": null, "journal-ref": "Proc. of the 17th Intl. Conf. on Control Systems and Computer\n  Science (CSCS), vol. 1, pp. 433-440, Bucharest, Romania, 26-29 May, 2009", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure detection is a fundamental building block for ensuring fault\ntolerance in large scale distributed systems. There are lots of approaches and\nimplementations in failure detectors. Providing flexible failure detection in\noff-the-shelf distributed systems is difficult. In this paper we present an\ninnovative solution to this problem. Our approach is based on adaptive,\ndecentralized failure detectors, capable of working asynchronous and\nindependent on the application flow. The proposed solution considers an\narchitecture for the failure detectors, based on clustering, the use of a\ngossip-based algorithm for detection at local level and the use of a\nhierarchical structure among clusters of detectors along which traffic is\nchanneled. The solution can scale to a large number of nodes, considers the QoS\nrequirements of both applications and resources, and includes fault tolerance\nand system orchestration mechanisms, added in order to asses the reliability\nand availability of distributed systems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 10:05:19 GMT"}], "update_date": "2009-10-06", "authors_parsed": [["Dobre", "Ciprian Mihai", ""], ["Pop", "Florin", ""], ["Costan", "Alexandru", ""], ["Andreica", "Mugurel Ionut", ""], ["Cristea", "Valentin", ""]]}, {"id": "0910.0928", "submitter": "EPTCS", "authors": "Ji\\v{r}\\'i Barnat (Masaryk University), Lubo\\v{s} Brim (Masaryk\n  University), Ivana \\v{C}ern\\'a (Masaryk University), Sven Dra\\v{z}an (Masaryk\n  University), Jana Fabrikov\\'a (Masaryk University), Jan L\\'an\\'ik (Masaryk\n  University), David \\v{S}afr\\'anek (Masaryk University), Hongwu Ma (University\n  of Edinburgh)", "title": "BioDiVinE: A Framework for Parallel Analysis of Biological Models", "comments": null, "journal-ref": "EPTCS 6, 2009, pp. 31-45", "doi": "10.4204/EPTCS.6.3", "report-no": null, "categories": "cs.CE cs.DC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel tool BioDiVinEfor parallel analysis of biological\nmodels is presented. The tool allows analysis of biological models specified in\nterms of a set of chemical reactions. Chemical reactions are transformed into a\nsystem of multi-affine differential equations. BioDiVinE employs techniques for\nfinite discrete abstraction of the continuous state space. At that level,\nparallel analysis algorithms based on model checking are provided. In the\npaper, the key tool features are described and their application is\ndemonstrated by means of a case study.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2009 06:25:34 GMT"}], "update_date": "2009-10-07", "authors_parsed": [["Barnat", "Ji\u0159\u00ed", "", "Masaryk University"], ["Brim", "Lubo\u0161", "", "Masaryk\n  University"], ["\u010cern\u00e1", "Ivana", "", "Masaryk University"], ["Dra\u017ean", "Sven", "", "Masaryk\n  University"], ["Fabrikov\u00e1", "Jana", "", "Masaryk University"], ["L\u00e1n\u00edk", "Jan", "", "Masaryk\n  University"], ["\u0160afr\u00e1nek", "David", "", "Masaryk University"], ["Ma", "Hongwu", "", "University\n  of Edinburgh"]]}, {"id": "0910.1585", "submitter": "Aaron D. Jaggard", "authors": "Aaron D. Jaggard, Michael Schapira, and Rebecca N. Wright", "title": "Distributed Computing with Adaptive Heuristics", "comments": "36 pages, four figures. Expands both technical results and discussion\n  of v1. Revised version will appear in the proceedings of Innovations in\n  Computer Science 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use ideas from distributed computing to study dynamic environments in\nwhich computational nodes, or decision makers, follow adaptive heuristics (Hart\n2005), i.e., simple and unsophisticated rules of behavior, e.g., repeatedly\n\"best replying\" to others' actions, and minimizing \"regret\", that have been\nextensively studied in game theory and economics. We explore when convergence\nof such simple dynamics to an equilibrium is guaranteed in asynchronous\ncomputational environments, where nodes can act at any time. Our research\nagenda, distributed computing with adaptive heuristics, lies on the borderline\nof computer science (including distributed computing and learning) and game\ntheory (including game dynamics and adaptive heuristics). We exhibit a general\nnon-termination result for a broad class of heuristics with bounded\nrecall---that is, simple rules of behavior that depend only on recent history\nof interaction between nodes. We consider implications of our result across a\nwide variety of interesting and timely applications: game theory, circuit\ndesign, social networks, routing and congestion control. We also study the\ncomputational and communication complexity of asynchronous dynamics and present\nsome basic observations regarding the effects of asynchrony on no-regret\ndynamics. We believe that our work opens a new avenue for research in both\ndistributed computing and game theory.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 22:39:39 GMT"}, {"version": "v2", "created": "Tue, 12 Oct 2010 19:35:08 GMT"}], "update_date": "2010-10-13", "authors_parsed": [["Jaggard", "Aaron D.", ""], ["Schapira", "Michael", ""], ["Wright", "Rebecca N.", ""]]}, {"id": "0910.1719", "submitter": "Federico Calzolari", "authors": "Federico Calzolari", "title": "High availability using virtualization", "comments": "PhD Thesis in Information Technology Engineering: Electronics,\n  Computer Science, Telecommunications, pp. 94, University of Pisa [Italy]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High availability has always been one of the main problems for a data center.\nTill now high availability was achieved by host per host redundancy, a highly\nexpensive method in terms of hardware and human costs. A new approach to the\nproblem can be offered by virtualization. Using virtualization, it is possible\nto achieve a redundancy system for all the services running on a data center.\nThis new approach to high availability allows to share the running virtual\nmachines over the servers up and running, by exploiting the features of the\nvirtualization layer: start, stop and move virtual machines between physical\nhosts. The system (3RC) is based on a finite state machine with hysteresis,\nproviding the possibility to restart each virtual machine over any physical\nhost, or reinstall it from scratch. A complete infrastructure has been\ndeveloped to install operating system and middleware in a few minutes. To\nvirtualize the main servers of a data center, a new procedure has been\ndeveloped to migrate physical to virtual hosts. The whole Grid data center\nSNS-PISA is running at the moment in virtual environment under the high\navailability system. As extension of the 3RC architecture, several storage\nsolutions have been tested to store and centralize all the virtual disks, from\nNAS to SAN, to grant data safety and access from everywhere. Exploiting\nvirtualization and ability to automatically reinstall a host, we provide a sort\nof host on-demand, where the action on a virtual machine is performed only when\na disaster occurs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 12:04:13 GMT"}], "update_date": "2009-10-12", "authors_parsed": [["Calzolari", "Federico", ""]]}, {"id": "0910.1852", "submitter": "N Vunka Jungum", "authors": "Mohammad Ali Jabraeil Jamali and Ahmad Khademzadeh", "title": "DAMQ-Based Schemes for Efficiently Using the Buffer Spaces of a NoC\n  Router", "comments": "\" International Journal of Computer Science Issues, IJCSI, Volume 4,\n  Issue 2, pp36-41, September 2009\"", "journal-ref": "M. A. J. Jamali and A. Khademzadeh, \" DAMQ-Based Schemes for\n  Efficiently Using the Buffer Spaces of a NoC Router\", International Journal\n  of Computer Science Issues, IJCSI, Volume 4, Issue 2, pp36-41, September 2009", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present high performance dynamically allocated multi-queue\n(DAMQ) buffer schemes for fault tolerance systems on chip applications that\nrequire an interconnection network. Two or four virtual channels shared the\nsame buffer space. On the message switching layer, we make improvement to boost\nsystem performance when there are faults involved in the components\ncommunication. The proposed schemes are when a node or a physical channel is\ndeemed as faulty, the previous hop node will terminate the buffer occupancy of\nmessages destined to the failed link. The buffer usage decisions are made at\nswitching layer without interactions with higher abstract layer, thus buffer\nspace will be released to messages destined to other healthy nodes quickly.\nTherefore, the buffer space will be efficiently used in case fault occurs at\nsome nodes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 20:24:57 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Jamali", "Mohammad Ali Jabraeil", ""], ["Khademzadeh", "Ahmad", ""]]}, {"id": "0910.1974", "submitter": "Rajkumar Buyya", "authors": "Rajkumar Buyya, Suraj Pandey, and Christian Vecchiola", "title": "Cloudbus Toolkit for Market-Oriented Cloud Computing", "comments": "21 pages, 6 figures, 2 tables, Conference paper", "journal-ref": "Proceeding of the 1st International Conference on Cloud Computing\n  (CloudCom 2009, Springer, Germany), Beijing, China, December 1-4, 2009", "doi": "10.1007/978-3-642-10665-1_4", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This keynote paper: (1) presents the 21st century vision of computing and\nidentifies various IT paradigms promising to deliver computing as a utility;\n(2) defines the architecture for creating market-oriented Clouds and computing\natmosphere by leveraging technologies such as virtual machines; (3) provides\nthoughts on market-based resource management strategies that encompass both\ncustomer-driven service management and computational risk management to sustain\nSLA-oriented resource allocation; (4) presents the work carried out as part of\nour new Cloud Computing initiative, called Cloudbus: (i) Aneka, a Platform as a\nService software system containing SDK (Software Development Kit) for\nconstruction of Cloud applications and deployment on private or public Clouds,\nin addition to supporting market-oriented resource management; (ii)\ninternetworking of Clouds for dynamic creation of federated computing\nenvironments for scaling of elastic applications; (iii) creation of 3rd party\nCloud brokering services for building content delivery networks and e-Science\napplications and their deployment on capabilities of IaaS providers such as\nAmazon along with Grid mashups; (iv) CloudSim supporting modelling and\nsimulation of Clouds for performance studies; (v) Energy Efficient Resource\nAllocation Mechanisms and Techniques for creation and management of Green\nClouds; and (vi) pathways for future research.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 06:26:29 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Pandey", "Suraj", ""], ["Vecchiola", "Christian", ""]]}, {"id": "0910.1979", "submitter": "Rajkumar Buyya", "authors": "Christian Vecchiola, Suraj Pandey, and Rajkumar Buyya", "title": "High-Performance Cloud Computing: A View of Scientific Applications", "comments": "13 pages, 9 figures, conference paper", "journal-ref": "Proceedings of the 10th International Symposium on Pervasive\n  Systems, Algorithms and Networks (I-SPAN 2009, IEEE CS Press, USA),\n  Kaohsiung, Taiwan, December 14-16, 2009", "doi": "10.1109/I-SPAN.2009.150", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computing often requires the availability of a massive number of\ncomputers for performing large scale experiments. Traditionally, these needs\nhave been addressed by using high-performance computing solutions and installed\nfacilities such as clusters and super computers, which are difficult to setup,\nmaintain, and operate. Cloud computing provides scientists with a completely\nnew model of utilizing the computing infrastructure. Compute resources, storage\nresources, as well as applications, can be dynamically provisioned (and\nintegrated within the existing infrastructure) on a pay per use basis. These\nresources can be released when they are no more needed. Such services are often\noffered within the context of a Service Level Agreement (SLA), which ensure the\ndesired Quality of Service (QoS). Aneka, an enterprise Cloud computing\nsolution, harnesses the power of compute resources by relying on private and\npublic Clouds and delivers to users the desired QoS. Its flexible and service\nbased infrastructure supports multiple programming paradigms that make Aneka\naddress a variety of different scenarios: from finance applications to\ncomputational science. As examples of scientific computing in the Cloud, we\npresent a preliminary case study on using Aneka for the classification of gene\nexpression data and the execution of fMRI brain imaging workflow.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 07:42:44 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Vecchiola", "Christian", ""], ["Pandey", "Suraj", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "0910.2004", "submitter": "Christian Schulz", "authors": "Manuel Holtgrewe, Peter Sanders, Christian Schulz", "title": "Engineering a Scalable High Quality Graph Partitioner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to parallel graph partitioning that scales to\nhundreds of processors and produces a high solution quality. For example, for\nmany instances from Walshaw's benchmark collection we improve the best known\npartitioning. We use the well known framework of multi-level graph\npartitioning. All components are implemented by scalable parallel algorithms.\nQuality improvements compared to previous systems are due to better\nprioritization of edges to be contracted, better approximation algorithms for\nidentifying matchings, better local search heuristics, and perhaps most\nnotably, a parallelization of the FM local search algorithm that works more\nlocally than previous approaches.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 14:36:07 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2010 14:35:26 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Holtgrewe", "Manuel", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "0910.2324", "submitter": "Bernhard Scholz", "authors": "Raymes Khoury, Bernd Burgstaller and Bernhard Scholz", "title": "Accelerating the Execution of Matrix Languages on the Cell Broadband\n  Engine Architecture", "comments": "61 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix languages, including MATLAB and Octave, are established standards for\napplications in science and engineering. They provide interactive programming\nenvironments that are easy to use due to their scripting languages with matrix\ndata types. Current implementations of matrix languages do not fully utilise\nhigh-performance, special-purpose chip architectures such as the IBM PowerXCell\nprocessor (Cell), which is currently used in the fastest computer in the world.\n  We present a new framework that extends Octave to harness the computational\npower of the Cell. With this framework the programmer is relieved of the burden\nof introducing explicit notions of parallelism. Instead the programmer uses a\nnew matrix data-type to execute matrix operations in parallel on the\nsynergistic processing elements (SPEs) of the Cell. We employ lazy evaluation\nsemantics for our new matrix data-type to obtain execution traces of matrix\noperations. Traces are converted to data dependence graphs; operations in the\ndata dependence graph are lowered (split into sub-matrices), scheduled and\nexecuted on the SPEs. Thereby we exploit (1) data parallelism, (2) instruction\nlevel parallelism, (3) pipeline parallelism and (4) task parallelism of matrix\nlanguage programs. We conducted extensive experiments to show the validity of\nour approach. Our Cell-based implementation achieves speedups of up to a factor\nof 12 over code run on recent Intel Core2 Quad processors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2009 08:54:18 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2009 23:55:21 GMT"}], "update_date": "2009-11-15", "authors_parsed": [["Khoury", "Raymes", ""], ["Burgstaller", "Bernd", ""], ["Scholz", "Bernhard", ""]]}, {"id": "0910.2582", "submitter": "Johannes Singler", "authors": "Mirko Rahn, Peter Sanders, Johannes Singler", "title": "Scalable Distributed-Memory External Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We engineer algorithms for sorting huge data sets on massively parallel\nmachines. The algorithms are based on the multiway merging paradigm. We first\noutline an algorithm whose I/O requirement is close to a lower bound. Thus, in\ncontrast to naive implementations of multiway merging and all other approaches\nknown to us, the algorithm works with just two passes over the data even for\nthe largest conceivable inputs. A second algorithm reduces communication\noverhead and uses more conventional specifications of the result at the cost of\nslightly increased I/O requirements. An implementation wins the well known\nsorting benchmark in several categories and by a large margin over its\ncompetitors.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2009 12:18:28 GMT"}], "update_date": "2009-10-15", "authors_parsed": [["Rahn", "Mirko", ""], ["Sanders", "Peter", ""], ["Singler", "Johannes", ""]]}, {"id": "0910.2743", "submitter": "Soummya Kar", "authors": "Usman A. Khan, Soummya Kar, and Jose M. F. Moura", "title": "DILAND: An Algorithm for Distributed Sensor Localization with Noisy\n  Distance Measurements", "comments": "Submitted to the IEEE Transactions on Signal Processing. Initial\n  submission on May 2009. 12 pages", "journal-ref": null, "doi": "10.1109/TSP.2009.2038423", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this correspondence, we present an algorithm for distributed sensor\nlocalization with noisy distance measurements (DILAND) that extends and makes\nthe DLRE more robust. DLRE is a distributed sensor localization algorithm in\n$\\mathbb{R}^m$ $(m\\geq1)$ introduced in \\cite{usman_loctsp:08}. DILAND operates\nwhen (i) the communication among the sensors is noisy; (ii) the communication\nlinks in the network may fail with a non-zero probability; and (iii) the\nmeasurements performed to compute distances among the sensors are corrupted\nwith noise. The sensors (which do not know their locations) lie in the convex\nhull of at least $m+1$ anchors (nodes that know their own locations.) Under\nminimal assumptions on the connectivity and triangulation of each sensor in the\nnetwork, this correspondence shows that, under the broad random phenomena\ndescribed above, DILAND converges almost surely (a.s.) to the exact sensor\nlocations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2009 22:31:39 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Khan", "Usman A.", ""], ["Kar", "Soummya", ""], ["Moura", "Jose M. F.", ""]]}, {"id": "0910.2942", "submitter": "Mugurel Ionut Andreica", "authors": "Florin Pop, Ciprian Mihai Dobre, Alexandru Costan, Mugurel Ionut\n  Andreica, Eliana-Dina Tirsa, Corina Stratan, Valentin Cristea", "title": "Critical Analysis of Middleware Architectures for Large Scale\n  Distributed Systems", "comments": null, "journal-ref": "Proc. of the 17th Intl. Conf. on Control Systems and Computer\n  Science (CSCS), vol. 1, pp. 29-36, Bucharest, Romania, 26-29 May, 2009.\n  (ISSN: 2066-4451)", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing is increasingly being viewed as the next phase of Large\nScale Distributed Systems (LSDSs). However, the vision of large scale resource\nsharing is not yet a reality in many areas - Grid computing is an evolving area\nof computing, where standards and technology are still being developed to\nenable this new paradigm. Hence, in this paper we analyze the current\ndevelopment of middleware tools for LSDS, from multiple perspectives:\narchitecture, applications and market research. For each perspective we are\ninterested in relevant technologies used in undergoing projects, existing\nproducts or services and useful design issues. In the end, based on this\napproach, we draw some conclusions regarding the future research directions in\nthis area.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2009 17:14:55 GMT"}], "update_date": "2009-10-16", "authors_parsed": [["Pop", "Florin", ""], ["Dobre", "Ciprian Mihai", ""], ["Costan", "Alexandru", ""], ["Andreica", "Mugurel Ionut", ""], ["Tirsa", "Eliana-Dina", ""], ["Stratan", "Corina", ""], ["Cristea", "Valentin", ""]]}, {"id": "0910.3883", "submitter": "Victor M. Preciado", "authors": "Victor M. Preciado, Alireza Tahbaz-Salehi, and Ali Jadbabaie", "title": "Variance Analysis of Randomized Consensus in Switching Directed Networks", "comments": "6 pages, 3 figures, submitted to American Control Conference 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the asymptotic properties of distributed consensus\nalgorithms over switching directed random networks. More specifically, we focus\non consensus algorithms over independent and identically distributed, directed\nErdos-Renyi random graphs, where each agent can communicate with any other\nagent with some exogenously specified probability $p$. While it is well-known\nthat consensus algorithms over Erdos-Renyi random networks result in an\nasymptotic agreement over the network, an analytical characterization of the\ndistribution of the asymptotic consensus value is still an open question. In\nthis paper, we provide closed-form expressions for the mean and variance of the\nasymptotic random consensus value, in terms of the size of the network and the\nprobability of communication $p$. We also provide numerical simulations that\nillustrate our results.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2009 16:05:58 GMT"}], "update_date": "2009-10-21", "authors_parsed": [["Preciado", "Victor M.", ""], ["Tahbaz-Salehi", "Alireza", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "0910.4024", "submitter": "Aubin Jarry", "authors": "Florian Huc and Aubin Jarry", "title": "VRAC: Simulation Results #1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to make full use of geographic routing techniques developed for\nlarge scale networks, nodes must be localized. However, localization and\nvirtual localization techniques in sensor networks are dependent either on\nexpensive and sometimes unavailable hardware (e.g. GPS) or on sophisticated\nlocalization calculus (e.g. triangulation) which are both error-prone and with\na costly overhead.\n  Instead of localizing nodes in a traditional 2-dimensional space, we use\ndirectly the raw distance to a set of anchors to route messages in a\nmulti-dimensional space. This should enable us to use any geographic routing\nprotocol in a robust and efficient manner in a very large range of scenarios.\nWe test this technique for two different geographic routing algorithms, namely\nGRIC and ROAM. The simulation results show that using the raw coordinates does\nnot decrease their efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2009 08:58:15 GMT"}], "update_date": "2009-10-22", "authors_parsed": [["Huc", "Florian", ""], ["Jarry", "Aubin", ""]]}, {"id": "0910.4507", "submitter": "Sam Skipsey", "authors": "Sam Skipsey (1), David Ambrose-Griffith (2), Greig Cowan (3), Mike\n  Kenyon (1), Orlando Richards (3), Phil Roffe (2), Graeme Stewart (1) ((1)\n  University of Glasgow, UK, (2) University of Durham, UK, (3) University of\n  Edinburgh, UK)", "title": "ScotGrid: Providing an Effective Distributed Tier-2 in the LHC Era", "comments": "Preprint for 17th International Conference on Computing in High\n  Energy and Nuclear Physics, 7 pages, 1 figure", "journal-ref": "J.Phys.Conf.Ser.219:052014,2010", "doi": "10.1088/1742-6596/219/5/052014", "report-no": "GLAS-PPE/2009-07", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ScotGrid is a distributed Tier-2 centre in the UK with sites in Durham,\nEdinburgh and Glasgow. ScotGrid has undergone a huge expansion in hardware in\nanticipation of the LHC and now provides more than 4MSI2K and 500TB to the LHC\nVOs. Scaling up to this level of provision has brought many challenges to the\nTier-2 and we show in this paper how we have adopted new methods of organising\nthe centres, from fabric management and monitoring to remote management of\nsites to management and operational procedures, to meet these challenges. We\ndescribe how we have coped with different operational models at the sites,\nwhere Glagsow and Durham sites are managed \"in house\" but resources at\nEdinburgh are managed as a central university resource. This required the\nadoption of a different fabric management model at Edinburgh and a special\nengagement with the cluster managers. Challenges arose from the different job\nmodels of local and grid submission that required special attention to resolve.\nWe show how ScotGrid has successfully provided an infrastructure for ATLAS and\nLHCb Monte Carlo production. Special attention has been paid to ensuring that\nuser analysis functions efficiently, which has required optimisation of local\nstorage and networking to cope with the demands of user analysis. Finally,\nalthough these Tier-2 resources are pledged to the whole VO, we have\nestablished close links with our local physics user communities as being the\nbest way to ensure that the Tier-2 functions effectively as a part of the LHC\ngrid computing framework..\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2009 13:02:19 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Skipsey", "Sam", ""], ["Ambrose-Griffith", "David", ""], ["Cowan", "Greig", ""], ["Kenyon", "Mike", ""], ["Richards", "Orlando", ""], ["Roffe", "Phil", ""], ["Stewart", "Graeme", ""]]}, {"id": "0910.4510", "submitter": "Sam Skipsey", "authors": "Sam Skipsey (1), Greig Cowan (2), Mike Kenyon (1), Stuart Purdie (1),\n  Graeme Stewart (1) ((1) University of Glasgow, UK, (2) University of\n  Edinburgh, UK)", "title": "Optimised access to user analysis data using the gLite DPM", "comments": "8 pages, 9 figures, preprint for 17th International Conference on\n  Computing in High Energy and Nuclear Physics", "journal-ref": "J.Phys.Conf.Ser.219:062066,2010", "doi": "10.1088/1742-6596/219/6/062066", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ScotGrid distributed Tier-2 now provides more that 4MSI2K and 500TB for\nLHC computing, which is spread across three sites at Durham, Edinburgh and\nGlasgow. Tier-2 sites have a dual role to play in the computing models of the\nLHC VOs. Firstly, their CPU resources are used for the generation of Monte\nCarlo event data. Secondly, the end user analysis data is distributed across\nthe grid to the site's storage system and held on disk ready for processing by\nphysicists' analysis jobs. In this paper we show how we have designed the\nScotGrid storage and data management resources in order to optimise access by\nphysicists to LHC data. Within ScotGrid, all sites use the gLite DPM storage\nmanager middleware. Using the EGEE grid to submit real ATLAS analysis code to\nprocess VO data stored on the ScotGrid sites, we present an analysis of the\nperformance of the architecture at one site, and procedures that may be\nundertaken to improve such. The results will be presented from the point of\nview of the end user (in terms of number of events processed/second) and from\nthe point of view of the site, which wishes to minimise load and the impact\nthat analysis activity has on other users of the system.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2009 13:29:44 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Skipsey", "Sam", ""], ["Cowan", "Greig", ""], ["Kenyon", "Mike", ""], ["Purdie", "Stuart", ""], ["Stewart", "Graeme", ""]]}, {"id": "0910.4568", "submitter": "Ilango Sriram", "authors": "Ilango Sriram", "title": "SPECI, a simulation tool exploring cloud-scale data centres", "comments": null, "journal-ref": "Ilango Sriram, SPECI, a Simulation Tool Exploring Cloud-Scale Data\n  Centres, In: CloudCom 2009, LNCS 5931, pp. 381-392, 2009, M.G. Jaatun, G.\n  Zhao, and C. Rong (Eds.), Springer-Verlag Berlin Heidelberg 2009", "doi": "10.1007/978-3-642-10665-1_35", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a rapid increase in the size of data centres (DCs) used to provide\ncloud computing services. It is commonly agreed that not all properties in the\nmiddleware that manages DCs will scale linearly with the number of components.\nFurther, \"normal failure\" complicates the assessment of the per-formance of a\nDC. However, unlike in other engineering domains, there are no well established\ntools that allow the prediction of the performance and behav-iour of future\ngenerations of DCs. SPECI, Simulation Program for Elastic Cloud\nInfrastructures, is a simulation tool which allows exploration of aspects of\nscaling as well as performance properties of future DCs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2009 19:05:29 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Sriram", "Ilango", ""]]}, {"id": "0910.4572", "submitter": "Paul Bunn", "authors": "Paul Bunn, Rafail Ostrovsky", "title": "Throughput in Asynchronous Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new, \"worst-case\" model for an asynchronous communication\nnetwork and investigate the simplest (yet central) task in this model, namely\nthe feasibility of end-to-end routing. Motivated by the question of how\nsuccessful a protocol can hope to perform in a network whose reliability is\nguaranteed by as few assumptions as possible, we combine the main\n\"unreliability\" features encountered in network models in the literature,\nallowing our model to exhibit all of these characteristics simultaneously. In\nparticular, our model captures networks that exhibit the following properties:\n1) On-line; 2) Dynamic Topology; 3)Distributed/Local Control 4) Asynchronous\nCommunication; 5) (Polynomially) Bounded Memory; 6) No Minimal Connectivity\nAssumptions. In the confines of this network, we evaluate throughput\nperformance and prove matching upper and lower bounds. In particular, using\ncompetitive analysis (perhaps somewhat surprisingly) we prove that the optimal\ncompetitive ratio of any on-line protocol is 1/n (where n is the number of\nnodes in the network), and then we describe a specific protocol and prove that\nit is n-competitive. The model we describe in the paper and for which we\nachieve the above matching upper and lower bounds for throughput represents the\n\"worst-case\" network, in that it makes no reliability assumptions. In many\npractical applications, the optimal competitive ratio of 1/n may be\nunacceptable, and consequently stronger assumptions must be imposed on the\nnetwork to improve performance. However, we believe that a fundamental starting\npoint to understanding which assumptions are necessary to impose on a network\nmodel, given some desired throughput performance, is to understand what is\nachievable in the worst case for the simplest task (namely end-to-end routing).\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2009 19:35:26 GMT"}], "update_date": "2009-10-26", "authors_parsed": [["Bunn", "Paul", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "0910.4711", "submitter": "Shrisha Rao", "authors": "Rajashekar Annaji, Shrisha Rao", "title": "Parallelization of the LBG Vector Quantization Algorithm for Shared\n  Memory Systems", "comments": "14 pages", "journal-ref": "International Journal of Image Processing, vol. 3, no. 4,\n  July/August 2009, pp. 170-183", "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a parallel approach for the Vector Quantization (VQ)\nproblem in image processing. VQ deals with codebook generation from the input\ntraining data set and replacement of any arbitrary data with the nearest\ncodevector. Most of the efforts in VQ have been directed towards designing\nparallel search algorithms for the codebook, and little has hitherto been done\nin evolving a parallelized procedure to obtain an optimum codebook. This\nparallel algorithm addresses the problem of designing an optimum codebook using\nthe traditional LBG type of vector quantization algorithm for shared memory\nsystems and for the efficient usage of parallel processors. Using the codebook\nformed from a training set, any arbitrary input data is replaced with the\nnearest codevector from the codebook. The effectiveness of the proposed\nalgorithm is indicated.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2009 04:46:56 GMT"}], "update_date": "2009-10-27", "authors_parsed": [["Annaji", "Rajashekar", ""], ["Rao", "Shrisha", ""]]}, {"id": "0910.5816", "submitter": "Giuseppe Notarstefano", "authors": "Giuseppe Notarstefano and Francesco Bullo", "title": "Distributed Abstract Optimization via Constraints Consensus: Theory and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed abstract programs are a novel class of distributed optimization\nproblems where (i) the number of variables is much smaller than the number of\nconstraints and (ii) each constraint is associated to a network node. Abstract\noptimization programs are a generalization of linear programs that captures\nnumerous geometric optimization problems. We propose novel constraints\nconsensus algorithms for distributed abstract programs: as each node\niteratively identifies locally active constraints and exchanges them with its\nneighbors, the network computes the active constraints determining the global\noptimum. The proposed algorithms are appropriate for networks with weak\ntime-dependent connectivity requirements and tight memory constraints. We show\nhow suitable target localization and formation control problems can be tackled\nvia constraints consensus.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2009 09:06:19 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2009 13:39:28 GMT"}], "update_date": "2009-11-02", "authors_parsed": [["Notarstefano", "Giuseppe", ""], ["Bullo", "Francesco", ""]]}, {"id": "0910.5920", "submitter": "David O'Callaghan", "authors": "David O'Callaghan, Louise Doran and Brian Coghlan", "title": "Evaluating Trust in Grid Certificates", "comments": "5 pages, 1 figure, accepted for ACM SAC 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital certificates are used to secure international computation and data\nstorage grids used for e-Science projects, like the Worldwide Large Hadron\nCollider Computing Grid. The International Grid Trust Federation has defined\nthe Grid Certificate Profile: a set of guidelines for digital certificates used\nfor grid authentication. We have designed and implemented a program and related\ntest suites for checking X.509 certificates against the certificate profiles\nand policies relevant for use on the Grid. The result is a practical tool that\nassists implementers and users of public key infrastructures to reach\nappropriate trust decisions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2009 17:30:10 GMT"}], "update_date": "2009-11-02", "authors_parsed": [["O'Callaghan", "David", ""], ["Doran", "Louise", ""], ["Coghlan", "Brian", ""]]}]