[{"id": "1308.0056", "submitter": "Kaiwen Zhang", "authors": "Kaiwen Zhang, Hans-Arno Jacobsen", "title": "SDN-like: The Next Generation of Pub/Sub", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-Defined Networking (SDN) has raised the boundaries of cloud\ncomputing by offering unparalleled levels of control and flexibility to system\nadministrators over their virtualized environments. To properly embrace this\nnew era of SDN-driven network architectures, the research community must not\nonly consider the impact of SDN over the protocol stack, but also on its\noverlying networked applications. In this big ideas paper, we study the impact\nof SDN on the design of future message-oriented middleware, specifically\npub/sub systems. We argue that key concepts put forth by SDN can be applied in\na meaningful fashion to the next generation of pub/sub systems. First, pub/sub\ncan adopt a logically centralized controller model for maintenance, monitoring,\nand control of the overlay network. We establish a parallel with existing work\non centralized pub/sub routing and discuss how the logically centralized\ncontroller model can be implemented in a distributed manner. Second, we\ninvestigate the separation of the control and data plane, which is integral to\nSDN, which can be adopted to raise the level of decoupling in pub/sub. We\nintroduce a new model of pub/sub which separates the traditional publisher and\nsubscriber roles into flow regulators and producer/consumers of data. We then\npresent use cases that benefit from this approach and study the impact of\ndecoupling for performance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 22:37:33 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Zhang", "Kaiwen", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1308.0083", "submitter": "Wei Wang", "authors": "Wei Wang, Baochun Li, Ben Liang", "title": "Dominant Resource Fairness in Cloud Computing Systems with Heterogeneous\n  Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-resource allocation problem in cloud computing systems\nwhere the resource pool is constructed from a large number of heterogeneous\nservers, representing different points in the configuration space of resources\nsuch as processing, memory, and storage. We design a multi-resource allocation\nmechanism, called DRFH, that generalizes the notion of Dominant Resource\nFairness (DRF) from a single server to multiple heterogeneous servers. DRFH\nprovides a number of highly desirable properties. With DRFH, no user prefers\nthe allocation of another user; no one can improve its allocation without\ndecreasing that of the others; and more importantly, no user has an incentive\nto lie about its resource demand. As a direct application, we design a simple\nheuristic that implements DRFH in real-world systems. Large-scale simulations\ndriven by Google cluster traces show that DRFH significantly outperforms the\ntraditional slot-based scheduler, leading to much higher resource utilization\nwith substantially shorter job completion times.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 03:08:22 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Wang", "Wei", ""], ["Li", "Baochun", ""], ["Liang", "Ben", ""]]}, {"id": "1308.0148", "submitter": "\\\"Omer Demirel", "authors": "Omer Demirel and Ivo F. Sbalzarini", "title": "Balancing indivisible real-valued loads in arbitrary networks", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parallel computing, a problem is divided into a set of smaller tasks that\nare distributed across multiple processing elements. Balancing the load of the\nprocessing elements is key to achieving good performance and scalability. If\nthe computational costs of the individual tasks vary over time in an\nunpredictable way, dynamic load balancing aims at migrating them between\nprocessing elements so as to maintain load balance. During dynamic load\nbalancing, the tasks amount to indivisible work packets with a real-valued\ncost. For this case of indivisible, real- valued loads, we analyze the\nbalancing circuit model, a local dynamic load-balancing scheme that does not\nrequire global communication. We extend previous analyses to the present case\nand provide a probabilistic bound for the achievable load balance. Based on an\nanalogy with the offline balls-into-bins problem, we further propose a novel\nalgorithm for dynamic balancing of indivisible, real-valued loads. We benchmark\nthe proposed algorithm in numerical experiments and compare it with the\nclassical greedy algorithm, both in terms of solution quality and communication\ncost. We find that the increased communication cost of the proposed algorithm\nis compensated by a higher solution quality, leading on average to about an\norder of magnitude gain in overall performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 10:32:33 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Demirel", "Omer", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1308.0390", "submitter": "EPTCS", "authors": "Ivan Lanese (Focus Team, University of Bologna/INRIA, Italy), Fabrizio\n  Montesi (IT University of Copenhagen, Denmark), Gianluigi Zavattaro (Focus\n  Team, University of Bologna/INRIA, Italy)", "title": "Amending Choreographies", "comments": "In Proceedings WWV 2013, arXiv:1308.0268", "journal-ref": "EPTCS 123, 2013, pp. 34-48", "doi": "10.4204/EPTCS.123.5", "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreographies are global descriptions of system behaviors, from which the\nlocal behavior of each endpoint entity can be obtained automatically through\nprojection. To guarantee that its projection is correct, i.e. it has the same\nbehaviors of the original choreography, a choreography usually has to respect\nsome coherency conditions. This restricts the set of choreographies that can be\nprojected.\n  In this paper, we present a transformation for amending choreographies that\ndo not respect common syntactic conditions for projection correctness.\nSpecifically, our transformation automatically reduces the amount of\nconcurrency, and it infers and adds hidden communications that make the\nresulting choreography respect the desired conditions, while preserving its\nbehavior.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 01:55:37 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Lanese", "Ivan", "", "Focus Team, University of Bologna/INRIA, Italy"], ["Montesi", "Fabrizio", "", "IT University of Copenhagen, Denmark"], ["Zavattaro", "Gianluigi", "", "Focus\n  Team, University of Bologna/INRIA, Italy"]]}, {"id": "1308.0568", "submitter": "Mohammed El-Dosuky", "authors": "M A Awad, M Z Rashad, M A Elsoud and M A El-dosuky", "title": "Visualization of Job Scheduling in Grid Computers", "comments": null, "journal-ref": "International Journal of Computer Applications 74(8):37-40, July\n  2013", "doi": "10.5120/12908-0036", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the hot problems in grid computing is job scheduling. It is known that\nthe job scheduling is NP-complete, and thus the use of heuristics is the de\nfacto approach to deal with this practice in its difficulty. The proposed is an\nimagination to fish swarm, job dispatcher and Visualization gridsim to execute\nsome jobs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 18:09:23 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Awad", "M A", ""], ["Rashad", "M Z", ""], ["Elsoud", "M A", ""], ["El-dosuky", "M A", ""]]}, {"id": "1308.0585", "submitter": "Cheng Wang", "authors": "Cheng Wang, Bhuvan Urgaonkar, Qian Wang, George Kesidis, and Anand\n  Sivasubramaniam", "title": "Data Center Cost Optimization Via Workload Modulation Under Real-World\n  Electricity Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate optimization problems to study how data centers might modulate\ntheir power demands for cost-effective operation taking into account three key\ncomplex features exhibited by real-world electricity pricing schemes: (i)\ntime-varying prices (e.g., time-of-day pricing, spot pricing, or higher energy\nprices during coincident peaks) and (ii) separate charge for peak power\nconsumption. Our focus is on demand modulation at the granularity of an entire\ndata center or a large part of it. For computational tractability reasons, we\nwork with a fluid model for power demands which we imagine can be modulated\nusing two abstract knobs of demand dropping and demand delaying (each with its\nassociated penalties or costs). Given many data center workloads and electric\nprices can be effectively predicted using statistical modeling techniques, we\ndevise a stochastic dynamic program (SDP) that can leverage such predictive\nmodels. Since the SDP can be computationally infeasible in many real platforms,\nwe devise approximations for it. We also devise fully online algorithms that\nmight be useful for scenarios with poor power demand or utility price\npredictability. For one of our online algorithms, we prove a competitive ratio\nof 2-1/n. Finally, using empirical evaluation with both real-world and\nsynthetic power demands and real-world prices, we demonstrate the efficacy of\nour techniques. As two salient empirically-gained insights: (i) demand delaying\nis more effective than demand dropping regarding to peak shaving (e.g., 10.74%\ncost saving with only delaying vs. 1.45% with only dropping for Google\nworkload) and (ii) workloads tend to have different cost saving potential under\nvarious electricity tariffs (e.g., 16.97% cost saving under peak-based tariff\nvs. 1.55% under time-varying pricing tariff for Facebook workload).\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 19:30:14 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2013 21:50:45 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2013 03:56:17 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2013 16:18:26 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Wang", "Cheng", ""], ["Urgaonkar", "Bhuvan", ""], ["Wang", "Qian", ""], ["Kesidis", "George", ""], ["Sivasubramaniam", "Anand", ""]]}, {"id": "1308.0761", "submitter": "Alexander Semenov", "authors": "Alexander Semenov, Oleg Zaikin", "title": "On estimating total time to solve SAT in distributed computing\n  environments: Application to the SAT@home project", "comments": "This paper was submitted to SAT-2013 conference. Its materials were\n  reported in a poster session (the paper in its full variant was not\n  accepted). 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to estimate the total time required to solve SAT\nin distributed environments via partitioning approach. It is based on the\nobservation that for some simple forms of problem partitioning one can use the\nMonte Carlo approach to estimate the time required to solve an original\nproblem. The method proposed is based on an algorithm for searching for\npartitioning with an optimal solving time estimation. We applied this method to\nestimate the time required to perform logical cryptanalysis of the widely known\nstream ciphers A5/1 and Bivium. The paper also describes a volunteer computing\nproject SAT@home aimed at solving hard combinatorial problems reduced to SAT.\nIn this project during several months there were solved 10 problems of logical\ncryptanalysis of the A5/1 cipher thatcould not be solved using known rainbow\ntables.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 00:30:09 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Semenov", "Alexander", ""], ["Zaikin", "Oleg", ""]]}, {"id": "1308.0843", "submitter": "Meenakshi Narain", "authors": "A. Avetisyan (1), S. Bhattacharya (2), M. Narain (2), S. Padhi (3), J.\n  Hirschauer (4), T. Levshina (4), P. McBride (4), C. Sehgal (4), M. Slyz (4),\n  M. Rynge (5), S. Malik (6), J. Stupak III (7), ((1) Boston University,\n  Boston, USA (2) Brown University, Providence, USA (3) University of\n  California, San Diego, USA (4) Fermi National Accelerator Lab, Batavia, USA\n  (5) Information Sciences Institute, Marina del Rey, USA (6) University of\n  Nebraska, Lincoln, USA (7) Purdue University Calumet, Hammond, USA)", "title": "Snowmass Energy Frontier Simulations using the Open Science Grid (A\n  Snowmass 2013 whitepaper)", "comments": null, "journal-ref": null, "doi": null, "report-no": "SNOW13-00168", "categories": "hep-ex cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snowmass is a US long-term planning study for the high-energy community by\nthe American Physical Society's Division of Particles and Fields. For its\nsimulation studies, opportunistic resources are harnessed using the Open\nScience Grid infrastructure. Late binding grid technology, GlideinWMS, was used\nfor distributed scheduling of the simulation jobs across many sites mainly in\nthe US. The pilot infrastructure also uses the Parrot mechanism to dynamically\naccess CvmFS in order to ascertain a homogeneous environment across the nodes.\nThis report presents the resource usage and the storage model used for\nsimulating large statistics Standard Model backgrounds needed for Snowmass\nEnergy Frontier studies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 19:30:19 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2013 00:17:49 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Avetisyan", "A.", ""], ["Bhattacharya", "S.", ""], ["Narain", "M.", ""], ["Padhi", "S.", ""], ["Hirschauer", "J.", ""], ["Levshina", "T.", ""], ["McBride", "P.", ""], ["Sehgal", "C.", ""], ["Slyz", "M.", ""], ["Rynge", "M.", ""], ["Malik", "S.", ""], ["Stupak", "J.", "III"]]}, {"id": "1308.1031", "submitter": "Bj\\\"orn Lohrmann", "authors": "Bj\\\"orn Lohrmann, Daniel Warneke, Odej Kao", "title": "Nephele Streaming: Stream Processing Under QoS Constraints At Scale", "comments": "Journal of Cluster Computing, 2013", "journal-ref": null, "doi": "10.1007/s10586-013-0281-8", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to process large numbers of continuous data streams in a\nnear-real-time fashion has become a crucial prerequisite for many scientific\nand industrial use cases in recent years. While the individual data streams are\nusually trivial to process, their aggregated data volumes easily exceed the\nscalability of traditional stream processing systems. At the same time,\nmassively-parallel data processing systems like MapReduce or Dryad currently\nenjoy a tremendous popularity for data-intensive applications and have proven\nto scale to large numbers of nodes. Many of these systems also provide\nstreaming capabilities. However, unlike traditional stream processors, these\nsystems have disregarded QoS requirements of prospective stream processing\napplications so far. In this paper we address this gap. First, we analyze\ncommon design principles of today's parallel data processing frameworks and\nidentify those principles that provide degrees of freedom in trading off the\nQoS goals latency and throughput. Second, we propose a highly distributed\nscheme which allows these frameworks to detect violations of user-defined QoS\nconstraints and optimize the job execution without manual interaction. As a\nproof of concept, we implemented our approach for our massively-parallel data\nprocessing framework Nephele and evaluated its effectiveness through a\ncomparison with Hadoop Online. For an example streaming application from the\nmultimedia domain running on a cluster of 200 nodes, our approach improves the\nprocessing latency by a factor of at least 13 while preserving high data\nthroughput when needed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 16:15:58 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Lohrmann", "Bj\u00f6rn", ""], ["Warneke", "Daniel", ""], ["Kao", "Odej", ""]]}, {"id": "1308.1247", "submitter": "Peter Elmer", "authors": "Peter Elmer, Salvatore Rappoccio, Kevin Stenson, Peter Wittich", "title": "The Need for an R&D and Upgrade Program for CMS Software and Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.DC physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the next ten years, the physics reach of the Large Hadron Collider (LHC)\nat the European Organization for Nuclear Research (CERN) will be greatly\nextended through increases in the instantaneous luminosity of the accelerator\nand large increases in the amount of collected data. Due to changes in the way\nMoore's Law computing performance gains have been realized in the past decade,\nan aggressive program of R&D is needed to ensure that the computing capability\nof CMS will be up to the task of collecting and analyzing this data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 11:47:22 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Elmer", "Peter", ""], ["Rappoccio", "Salvatore", ""], ["Stenson", "Kevin", ""], ["Wittich", "Peter", ""]]}, {"id": "1308.1303", "submitter": "Arokia Paul  Rajan", "authors": "Arokia Paul Rajan and Shanmugapriyaa", "title": "Evolution of Cloud Storage as Cloud Computing Infrastructure Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enterprises are driving towards less cost, more availability, agility,\nmanaged risk - all of which is accelerated towards Cloud Computing. Cloud is\nnot a particular product, but a way of delivering IT services that are\nconsumable on demand, elastic to scale up and down as needed, and follow a\npay-for-usage model. Out of the three common types of cloud computing service\nmodels, Infrastructure as a Service (IaaS) is a service model that provides\nservers, computing power, network bandwidth and Storage capacity, as a service\nto their subscribers. Cloud can relate to many things but without the\nfundamental storage pieces, which is provided as a service namely Cloud\nStorage, none of the other applications is possible. This paper introduces\nCloud Storage, which covers the key technologies in cloud computing and Cloud\nStorage, management insights about cloud computing, different types of cloud\nservices, driving forces of cloud computing and cloud storage, advantages and\nchallenges of cloud storage and concludes by pinpointing few challenges to be\naddressed by the cloud storage providers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 06:11:12 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Rajan", "Arokia Paul", ""], ["Shanmugapriyaa", "", ""]]}, {"id": "1308.1343", "submitter": "Erik Schnetter", "authors": "Erik Schnetter", "title": "Performance and Optimization Abstractions for Large Scale Heterogeneous\n  Systems in the Cactus/Chemora Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe a set of lower-level abstractions to improve performance on\nmodern large scale heterogeneous systems. These provide portable access to\nsystem- and hardware-dependent features, automatically apply dynamic\noptimizations at run time, and target stencil-based codes used in finite\ndifferencing, finite volume, or block-structured adaptive mesh refinement\ncodes.\n  These abstractions include a novel data structure to manage refinement\ninformation for block-structured adaptive mesh refinement, an iterator\nmechanism to efficiently traverse multi-dimensional arrays in stencil-based\ncodes, and a portable API and implementation for explicit SIMD vectorization.\n  These abstractions can either be employed manually, or be targeted by\nautomated code generation, or be used via support libraries by compilers during\ncode generation. The implementations described below are available in the\nCactus framework, and are used e.g. in the Einstein Toolkit for relativistic\nastrophysics simulations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 16:40:47 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Schnetter", "Erik", ""]]}, {"id": "1308.1358", "submitter": "Gustavo Maciel Dias Vieira", "authors": "Gustavo M. D. Vieira, Luiz E. Buzato", "title": "The Performance of Paxos and Fast Paxos", "comments": "14 pages, published in the Proc. of the 27th Brazilian Symposium on\n  Computer Networks, Recife, Brazil, May 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paxos and Fast Paxos are optimal consensus algorithms that are simple and\nelegant, while suitable for efficient implementation. In this paper, we compare\nthe performance of both algorithms in failure-free and failure-prone runs using\nTreplica, a general replication toolkit that implements these algorithms in a\nmodular and efficient manner. We have found that Paxos outperforms Fast Paxos\nfor small number of replicas and that collisions are not the cause of this\nperformance difference.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 17:44:13 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Vieira", "Gustavo M. D.", ""], ["Buzato", "Luiz E.", ""]]}, {"id": "1308.1419", "submitter": "Crist\\'obal A. Navarro", "authors": "Cristobal A. Navarro, Nancy Hitschfeld", "title": "Improving the GPU space of computation under triangular domain problems", "comments": "6 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a stage in the GPU computing pipeline where a grid of thread-blocks\nis mapped to the problem domain. Normally, this grid is a k-dimensional\nbounding box that covers a k-dimensional problem no matter its shape. Threads\nthat fall inside the problem domain perform computations, otherwise they are\ndiscarded at runtime. For problems with non-square geometry, this is not always\nthe best idea because part of the space of computation is executed without any\npractical use. Two- dimensional triangular domain problems, alias td-problems,\nare a particular case of interest. Problems such as the Euclidean distance map,\nLU decomposition, collision detection and simula- tions over triangular tiled\ndomains are all td-problems and they appear frequently in many areas of\nscience. In this work, we propose an improved GPU mapping function g(lambda),\nthat maps any lambda block to a unique location (i, j) in the triangular\ndomain. The mapping is based on the properties of the lower triangular matrix\nand it works at a block level, thus not compromising thread organization within\na block. The theoretical improvement from using g(lambda) is upper bounded as I\n< 2 and the number of wasted blocks is reduced from O(n^2) to O(n). We compare\nour strategy with other proposed methods; the upper-triangular mapping (UTM),\nthe rectangular box (RB) and the recursive partition (REC). Our experimental\nresults on Nvidias Kepler GPU architecture show that g(lambda) is between 12%\nand 15% faster than the bounding box (BB) strategy. When compared to the other\nstrategies, our mapping runs significantly faster than UTM and it is as fast as\nRB in practical use, with the advantage that thread organization is not\ncompromised, as in RB. This work also contributes at presenting, for the first\ntime, a fair comparison of all existing strategies running the same experiments\nunder the same hardware.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 20:44:35 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Navarro", "Cristobal A.", ""], ["Hitschfeld", "Nancy", ""]]}, {"id": "1308.1536", "submitter": "Gleb Beliakov", "authors": "Gleb Beliakov and Yuri Matiyasevich", "title": "A Parallel Algorithm for Calculation of Large Determinants with High\n  Accuracy for GPUs and MPI clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA math.NA math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel algorithm for calculating very large determinants with\narbitrary precision on computer clusters. This algorithm minimises data\nmovements between the nodes and computes not only the determinant but also all\nminors corresponding to a particular row or column at a little extra cost, and\nalso the determinants and minors of all submatrices in the top left corner at\nno extra cost. We implemented the algorithm in arbitrary precision arithmetic,\nsuitable for very ill conditioned matrices, and empirically estimated the loss\nof precision. The algorithm was applied to studies of Riemann's zeta function.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 11:24:07 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 11:06:59 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Beliakov", "Gleb", ""], ["Matiyasevich", "Yuri", ""]]}, {"id": "1308.1763", "submitter": "Nitin Rakesh", "authors": "Nitin Rakesh and Vipin Tyagi", "title": "Linear Network Coding on Multi-Mesh of Trees (MMT) using All to All\n  Broadcast (AAB)", "comments": "10 pages, 15 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 3, No. 1, May 2011", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce linear network coding on parallel architecture for multi-source\nfinite acyclic network. In this problem, different messages in diverse time\nperiods are broadcast and every nonsource node in the network decodes and\nencodes the message based on further communication.We wish to minimize the\ncommunication steps and time complexity involved in transfer of data from\nnode-to-node during parallel communication.We have used Multi-Mesh of Trees\n(MMT) topology for implementing network coding. To envisage our result, we use\nall-to-all broadcast as communication algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 06:21:21 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Rakesh", "Nitin", ""], ["Tyagi", "Vipin", ""]]}, {"id": "1308.1806", "submitter": "Om Pr", "authors": "Gaurav Mittal, Dr. Nishtha Kesswani, Kuldeep Goswami", "title": "A Survey of Current Trends in Distributed, Grid and Cloud Computing", "comments": "6 pages", "journal-ref": "International Journal of Advanced Studies in Computers, Science &\n  Engineering (IJASCSE Vol 2, Issue 3, 2013)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the 1990s to 2012 the internet changed the world of computing\ndrastically. It started its journey with parallel computing after it advanced\nto distributed computing and further to grid computing. And in present scenario\nit creates a new world which is pronounced as a Cloud Computing [1]. These all\nthree terms have different meanings. Cloud computing is based on backward\ncomputing schemes like cluster computing, distributed computing, grid computing\nand utility computing. The basic concept of cloud computing is virtualization.\nIt provides virtual hardware and software resources to various requesting\nprograms. This paper gives a detailed description about cluster computing, grid\ncomputing and cloud computing and gives an insight of some implementations of\nthe same. We try to list the inspirations for the advent of all these\ntechnologies. We also account for some present scenario faults of grid\ncomputing and also discuss new cloud computing projects which are being managed\nby the Government of India for learning. The paper also reviews the existing\nwork and covers (analytically), to some extent, some innovative ideas that can\nbe implemented.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 10:23:09 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Mittal", "Gaurav", ""], ["Kesswani", "Dr. Nishtha", ""], ["Goswami", "Kuldeep", ""]]}, {"id": "1308.2058", "submitter": "Blesson Varghese", "authors": "Ishan Patel, Blesson Varghese, Adam Barker", "title": "RBioCloud: A Light-weight Framework for Bioconductor and R-based Jobs on\n  the Cloud", "comments": "Webpage: http://www.rbiocloud.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale ad hoc analytics of genomic data is popular using the\nR-programming language supported by 671 software packages provided by\nBioconductor. More recently, analytical jobs are benefitting from on-demand\ncomputing and storage, their scalability and their low maintenance cost, all of\nwhich are offered by the cloud. While Biologists and Bioinformaticists can take\nan analytical job and execute it on their personal workstations, it remains\nchallenging to seamlessly execute the job on the cloud infrastructure without\nextensive knowledge of the cloud dashboard. How analytical jobs can not only\nwith minimum effort be executed on the cloud, but also how both the resources\nand data required by the job can be managed is explored in this paper. An\nopen-source light-weight framework for executing R-scripts using Bioconductor\npackages, referred to as `RBioCloud', is designed and developed. RBioCloud\noffers a set of simple command-line tools for managing the cloud resources, the\ndata and the execution of the job. Three biological test cases validate the\nfeasibility of RBioCloud. The framework is publicly available from\nhttp://www.rbiocloud.com.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 09:20:02 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Patel", "Ishan", ""], ["Varghese", "Blesson", ""], ["Barker", "Adam", ""]]}, {"id": "1308.2066", "submitter": "Blesson Varghese", "authors": "Aman Bahl, Oliver Baltzer, Andrew Rau-Chaplin, Blesson Varghese", "title": "Parallel Simulations for Analysing Portfolios of Catastrophic Event Risk", "comments": "Proceedings of the Workshop at the International Conference for High\n  Performance Computing, Networking, Storage and Analysis (SC), 2012, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of the analytical pipeline of a modern quantitative\ninsurance/reinsurance company is a stochastic simulation technique for\nportfolio risk analysis and pricing process referred to as Aggregate Analysis.\nSupport for the computation of risk measures including Probable Maximum Loss\n(PML) and the Tail Value at Risk (TVAR) for a variety of types of complex\nproperty catastrophe insurance contracts including Cat eXcess of Loss (XL), or\nPer-Occurrence XL, and Aggregate XL, and contracts that combine these measures\nis obtained in Aggregate Analysis.\n  In this paper, we explore parallel methods for aggregate risk analysis. A\nparallel aggregate risk analysis algorithm and an engine based on the algorithm\nis proposed. This engine is implemented in C and OpenMP for multi-core CPUs and\nin C and CUDA for many-core GPUs. Performance analysis of the algorithm\nindicates that GPUs offer an alternative HPC solution for aggregate risk\nanalysis that is cost effective. The optimised algorithm on the GPU performs a\n1 million trial aggregate simulation with 1000 catastrophic events per trial on\na typical exposure set and contract structure in just over 20 seconds which is\napproximately 15x times faster than the sequential counterpart. This can\nsufficiently support the real-time pricing scenario in which an underwriter\nanalyses different contractual terms and pricing while discussing a deal with a\nclient over the phone.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 09:43:51 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Bahl", "Aman", ""], ["Baltzer", "Oliver", ""], ["Rau-Chaplin", "Andrew", ""], ["Varghese", "Blesson", ""]]}, {"id": "1308.2147", "submitter": "Paolo Romano", "authors": "Danny Hendler, Alex Naiman, Sebastiano Peluso, Francesco Quaglia,\n  Paolo Romano, Adi Suissa", "title": "Exploiting Locality in Lease-Based Replicated Transactional Memory via\n  Task Migration", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Lilac-TM, the first locality-aware Distributed Software\nTransactional Memory (DSTM) implementation. Lilac-TM is a fully decentralized\nlease-based replicated DSTM. It employs a novel self- optimizing lease\ncirculation scheme based on the idea of dynamically determining whether to\nmigrate transactions to the nodes that own the leases required for their\nvalidation, or to demand the acquisition of these leases by the node that\noriginated the transaction. Our experimental evaluation establishes that\nLilac-TM provides significant performance gains for distributed workloads\nexhibiting data locality, while typically incurring no overhead for non-data\nlocal workloads.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 15:04:09 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Hendler", "Danny", ""], ["Naiman", "Alex", ""], ["Peluso", "Sebastiano", ""], ["Quaglia", "Francesco", ""], ["Romano", "Paolo", ""], ["Suissa", "Adi", ""]]}, {"id": "1308.2166", "submitter": "Kanat Tangwongsan", "authors": "Kanat Tangwongsan, A. Pavan, and Srikanta Tirthapura", "title": "Parallel Triangle Counting in Massive Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of triangles in a graph is a fundamental metric, used in social\nnetwork analysis, link classification and recommendation, and more. Driven by\nthese applications and the trend that modern graph datasets are both large and\ndynamic, we present the design and implementation of a fast and cache-efficient\nparallel algorithm for estimating the number of triangles in a massive\nundirected graph whose edges arrive as a stream. It brings together the\nbenefits of streaming algorithms and parallel algorithms. By building on the\nstreaming algorithms framework, the algorithm has a small memory footprint. By\nleveraging the paralell cache-oblivious framework, it makes efficient use of\nthe memory hierarchy of modern multicore machines without needing to know its\nspecific parameters. We prove theoretical bounds on accuracy, memory access\ncost, and parallel runtime complexity, as well as showing empirically that the\nalgorithm yields accurate results and substantial speedups compared to an\noptimized sequential implementation.\n  (This is an expanded version of a CIKM'13 paper of the same title.)\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 15:54:22 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Tangwongsan", "Kanat", ""], ["Pavan", "A.", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1308.2473", "submitter": "Sriram Pemmaraju", "authors": "Andrew Berns, James Hegeman, Sriram V. Pemmaraju", "title": "Super-Fast Distributed Algorithms for Metric Facility Location", "comments": "15 pages, 2 figures. This is the full version of a paper that\n  appeared in ICALP 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a distributed O(1)-approximation algorithm, with\nexpected-$O(\\log \\log n)$ running time, in the $\\mathcal{CONGEST}$ model for\nthe metric facility location problem on a size-$n$ clique network. Though\nmetric facility location has been considered by a number of researchers in\nlow-diameter settings, this is the first sub-logarithmic-round algorithm for\nthe problem that yields an O(1)-approximation in the setting of non-uniform\nfacility opening costs. In order to obtain this result, our paper makes three\nmain technical contributions. First, we show a new lower bound for metric\nfacility location, extending the lower bound of B\\u{a}doiu et al. (ICALP 2005)\nthat applies only to the special case of uniform facility opening costs. Next,\nwe demonstrate a reduction of the distributed metric facility location problem\nto the problem of computing an O(1)-ruling set of an appropriate spanning\nsubgraph. Finally, we present a sub-logarithmic-round (in expectation)\nalgorithm for computing a 2-ruling set in a spanning subgraph of a clique. Our\nalgorithm accomplishes this by using a combination of randomized and\ndeterministic sparsification.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 06:30:09 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Berns", "Andrew", ""], ["Hegeman", "James", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1308.2480", "submitter": "Gerard Gorman", "authors": "Georgios Rokos, Gerard J. Gorman, James Southern, Paul H.J. Kelly", "title": "A thread-parallel algorithm for anisotropic mesh adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anisotropic mesh adaptation is a powerful way to directly minimise the\ncomputational cost of mesh based simulation. It is particularly important for\nmulti-scale problems where the required number of floating-point operations can\nbe reduced by orders of magnitude relative to more traditional static mesh\napproaches.\n  Increasingly, finite element and finite volume codes are being optimised for\nmodern multi-core architectures. Typically, decomposition methods implemented\nthrough the Message Passing Interface (MPI) are applied for inter-node\nparallelisation, while a threaded programming model, such as OpenMP, is used\nfor intra-node parallelisation. Inter-node parallelism for mesh adaptivity has\nbeen successfully implemented by a number of groups. However, thread-level\nparallelism is significantly more challenging because the underlying data\nstructures are extensively modified during mesh adaptation and a greater degree\nof parallelism must be realised.\n  In this paper we describe a new thread-parallel algorithm for anisotropic\nmesh adaptation algorithms. For each of the mesh optimisation phases\n(refinement, coarsening, swapping and smoothing) we describe how independent\nsets of tasks are defined. We show how a deferred updates strategy can be used\nto update the mesh data structures in parallel and without data contention. We\nshow that despite the complex nature of mesh adaptation and inherent load\nimbalances in the mesh adaptivity, a parallel efficiency of 60% is achieved on\nan 8 core Intel Xeon Sandybridge, and a 40% parallel efficiency is achieved\nusing 16 cores in a 2 socket Intel Xeon Sandybridge ccNUMA system.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 07:45:15 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Rokos", "Georgios", ""], ["Gorman", "Gerard J.", ""], ["Southern", "James", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "1308.2572", "submitter": "Blesson Varghese", "authors": "A. K. Bahl, O. Baltzer, A. Rau-Chaplin, B. Varghese and A. Whiteway", "title": "Achieving Speedup in Aggregate Risk Analysis using Multiple GPUs", "comments": "Workshop Proceedings of International Conference on Parallel\n  Processing, Lyon, France, 2013, 8 pages. arXiv admin note: text overlap with\n  arXiv:1308.2066", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.DS q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation techniques employed for the analysis of portfolios of\ninsurance/reinsurance risk, often referred to as `Aggregate Risk Analysis', can\nbenefit from exploiting state-of-the-art high-performance computing platforms.\nIn this paper, parallel methods to speed-up aggregate risk analysis for\nsupporting real-time pricing are explored. An algorithm for analysing aggregate\nrisk is proposed and implemented for multi-core CPUs and for many-core GPUs.\nExperimental studies indicate that GPUs offer a feasible alternative solution\nover traditional high-performance computing systems. A simulation of 1,000,000\ntrials with 1,000 catastrophic events per trial on a typical exposure set and\ncontract structure is performed in less than 5 seconds on a multiple GPU\nplatform. The key result is that the multiple GPU implementation can be used in\nreal-time pricing scenarios as it is approximately 77x times faster than the\nsequential counterpart implemented on a CPU.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 14:09:45 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Bahl", "A. K.", ""], ["Baltzer", "O.", ""], ["Rau-Chaplin", "A.", ""], ["Varghese", "B.", ""], ["Whiteway", "A.", ""]]}, {"id": "1308.2694", "submitter": "Sriram Pemmaraju", "authors": "James Hegeman and Sriram V. Pemmaraju", "title": "A Super-Fast Distributed Algorithm for Bipartite Metric Facility\n  Location", "comments": "22 pages. This is the full version of a paper that appeared in DISC\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\textit{facility location} problem consists of a set of\n\\textit{facilities} $\\mathcal{F}$, a set of \\textit{clients} $\\mathcal{C}$, an\n\\textit{opening cost} $f_i$ associated with each facility $x_i$, and a\n\\textit{connection cost} $D(x_i,y_j)$ between each facility $x_i$ and client\n$y_j$. The goal is to find a subset of facilities to \\textit{open}, and to\nconnect each client to an open facility, so as to minimize the total facility\nopening costs plus connection costs. This paper presents the first\nexpected-sub-logarithmic-round distributed O(1)-approximation algorithm in the\n$\\mathcal{CONGEST}$ model for the \\textit{metric} facility location problem on\nthe complete bipartite network with parts $\\mathcal{F}$ and $\\mathcal{C}$. Our\nalgorithm has an expected running time of $O((\\log \\log n)^3)$ rounds, where $n\n= |\\mathcal{F}| + |\\mathcal{C}|$. This result can be viewed as a continuation\nof our recent work (ICALP 2012) in which we presented the first\nsub-logarithmic-round distributed O(1)-approximation algorithm for metric\nfacility location on a \\textit{clique} network. The bipartite setting presents\nseveral new challenges not present in the problem on a clique network. We\npresent two new techniques to overcome these challenges. (i) In order to deal\nwith the problem of not being able to choose appropriate probabilities (due to\nlack of adequate knowledge), we design an algorithm that performs a random walk\nover a probability space and analyze the progress our algorithm makes as the\nrandom walk proceeds. (ii) In order to deal with a problem of quickly\ndisseminating a collection of messages, possibly containing many duplicates,\nover the bipartite network, we design a probabilistic hashing scheme that\ndelivers all of the messages in expected-$O(\\log \\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 20:45:17 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Hegeman", "James", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1308.2787", "submitter": "Blesson Varghese", "authors": "Ishan Patel, Andrew Rau-Chaplin, Blesson Varghese", "title": "Accelerating R-based Analytics on the Cloud", "comments": "Concurrency and Computation, 2013", "journal-ref": null, "doi": "10.1002/cpe.3026", "report-no": null, "categories": "cs.DC cs.CE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how the benefits of cloud-based infrastructure can be\nharnessed for analytical workloads. Often the software handling analytical\nworkloads is not developed by a professional programmer, but on an ad hoc basis\nby Analysts in high-level programming environments such as R or Matlab. The\ngoal of this research is to allow Analysts to take an analytical job that\nexecutes on their personal workstations, and with minimum effort execute it on\ncloud infrastructure and manage both the resources and the data required by the\njob. If this can be facilitated gracefully, then the Analyst benefits from\non-demand resources, low maintenance cost and scalability of computing\nresources, all of which are offered by the cloud. In this paper, a Platform for\nParallel R-based Analytics on the Cloud (P2RAC) that is placed between an\nAnalyst and a cloud infrastructure is proposed and implemented. P2RAC offers a\nset of command-line tools for managing the resources, such as instances and\nclusters, the data and the execution of the software on the Amazon Elastic\nComputing Cloud infrastructure. Experimental studies are pursued using two\nparallel problems and the results obtained confirm the feasibility of employing\nP2RAC for solving large-scale analytical problems on the cloud.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 08:58:24 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Patel", "Ishan", ""], ["Rau-Chaplin", "Andrew", ""], ["Varghese", "Blesson", ""]]}, {"id": "1308.2872", "submitter": "Blesson Varghese", "authors": "Blesson Varghese, Gerard McKee, Vassil Alexandrov", "title": "Can Agent Intelligence be used to Achieve Fault Tolerant Parallel\n  Computing Systems?", "comments": null, "journal-ref": "Parallel Processing Letters, Volume 21, Issue 04, December 2011", "doi": "10.1142/S012962641100028X", "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work reported in this paper is motivated towards validating an\nalternative approach for fault tolerance over traditional methods like\ncheckpointing that constrain efficacious fault tolerance. Can agent\nintelligence be used to achieve fault tolerant parallel computing systems? If\nso, \"What agent capabilities are required for fault tolerance?\", \"What parallel\ncomputational tasks can benefit from such agent capabilities?\" and \"How can\nagent capabilities be implemented for fault tolerance?\" need to be addressed.\nCognitive capabilities essential for achieving fault tolerance through agents\nare considered. Parallel reduction algorithms are identified as a class of\nalgorithms that can benefit from cognitive agent capabilities. The Message\nPassing Interface is utilized for implementing an intelligent agent based\napproach. Preliminary results obtained from the experiments validate the\nfeasibility of an agent based approach for achieving fault tolerance in\nparallel computing systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 14:09:03 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Varghese", "Blesson", ""], ["McKee", "Gerard", ""], ["Alexandrov", "Vassil", ""]]}, {"id": "1308.2881", "submitter": "Holger Machens", "authors": "Holger Machens and Volker Turau", "title": "Opacity of Memory Management in Software Transactional Memory", "comments": "Keywords: transactional memory, opacity, privatization, memory\n  reclamation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opacity of Transactional Memory is proposed to be established by incremental\nvalidation. Quiescence in terms of epoch-based memory reclamation is applied to\ndeal with doomed transactions causing memory access violations. This method\nunfortunately involves increased memory consumption and does not cover\nreclamations outside of transactions. This paper introduces a different method\nwhich combines incremental validation with elements of sandboxing to solve\nthese issues.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 14:37:51 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Machens", "Holger", ""], ["Turau", "Volker", ""]]}, {"id": "1308.2979", "submitter": "Marco Serafini", "authors": "Flavio P. Junqueira and Marco Serafini", "title": "On Barriers and the Gap between Active and Passive Replication (Full\n  Version)", "comments": "A shorter version of this work (without appendices) appears in the\n  proceedings of the 27th International Symposium on Distributed Computing\n  (DISC) 2013 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active replication is commonly built on top of the atomic broadcast\nprimitive. Passive replication, which has been recently used in the popular\nZooKeeper coordination system, can be naturally built on top of the\nprimary-order atomic broadcast primitive. Passive replication differs from\nactive replication in that it requires processes to cross a barrier before they\nbecome primaries and start broadcasting messages. In this paper, we propose a\nbarrier function tau that explains and encapsulates the differences between\nexisting primary-order atomic broadcast algorithms, namely semi-passive\nreplication and Zookeeper atomic broadcast (Zab), as well as the differences\nbetween Paxos and Zab. We also show that implementing primary-order atomic\nbroadcast on top of a generic consensus primitive and tau inherently results in\nhigher time complexity than atomic broadcast, as witnessed by existing\nalgorithms. We overcome this problem by presenting an alternative,\nprimary-order atomic broadcast implementation that builds on top of a generic\nconsensus primitive and uses consensus itself to form a barrier. This algorithm\nis modular and matches the time complexity of existing tau-based algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 21:09:27 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 12:11:23 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2013 04:48:05 GMT"}, {"version": "v4", "created": "Tue, 10 Dec 2013 09:29:40 GMT"}, {"version": "v5", "created": "Mon, 12 Oct 2015 07:42:03 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Junqueira", "Flavio P.", ""], ["Serafini", "Marco", ""]]}, {"id": "1308.3615", "submitter": "Blesson Varghese", "authors": "Andrew Rau-Chaplin, Blesson Varghese, Duane Wilson, Zhimin Yao, and\n  Norbert Zeh", "title": "QuPARA: Query-Driven Large-Scale Portfolio Aggregate Risk Analysis on\n  MapReduce", "comments": "9 pages, IEEE International Conference on Big Data (BigData), Santa\n  Clara, USA, 2013", "journal-ref": null, "doi": "10.1109/BigData.2013.6691640", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation techniques are used for portfolio risk analysis. Risk\nportfolios may consist of thousands of reinsurance contracts covering millions\nof insured locations. To quantify risk each portfolio must be evaluated in up\nto a million simulation trials, each capturing a different possible sequence of\ncatastrophic events over the course of a contractual year. In this paper, we\nexplore the design of a flexible framework for portfolio risk analysis that\nfacilitates answering a rich variety of catastrophic risk queries. Rather than\naggregating simulation data in order to produce a small set of high-level risk\nmetrics efficiently (as is often done in production risk management systems),\nthe focus here is on allowing the user to pose queries on unaggregated or\npartially aggregated data. The goal is to provide a flexible framework that can\nbe used by analysts to answer a wide variety of unanticipated but natural ad\nhoc queries. Such detailed queries can help actuaries or underwriters to better\nunderstand the multiple dimensions (e.g., spatial correlation, seasonality,\nperil features, construction features, and financial terms) that can impact\nportfolio risk. We implemented a prototype system, called QuPARA (Query-Driven\nLarge-Scale Portfolio Aggregate Risk Analysis), using Hadoop, which is Apache's\nimplementation of the MapReduce paradigm. This allows the user to take\nadvantage of large parallel compute servers in order to answer ad hoc risk\nanalysis queries efficiently even on very large data sets typically encountered\nin practice. We describe the design and implementation of QuPARA and present\nexperimental results that demonstrate its feasibility. A full portfolio risk\nanalysis run consisting of a 1,000,000 trial simulation, with 1,000 events per\ntrial, and 3,200 risk transfer contracts can be completed on a 16-node Hadoop\ncluster in just over 20 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 12:15:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Rau-Chaplin", "Andrew", ""], ["Varghese", "Blesson", ""], ["Wilson", "Duane", ""], ["Yao", "Zhimin", ""], ["Zeh", "Norbert", ""]]}, {"id": "1308.3648", "submitter": "Sascha Hunold", "authors": "Sascha Hunold and Jesper Larsson Tr\\\"aff", "title": "On the State and Importance of Reproducible Experimental Research in\n  Parallel Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer science is also an experimental science. This is particularly the\ncase for parallel computing, which is in a total state of flux, and where\nexperiments are necessary to substantiate, complement, and challenge\ntheoretical modeling and analysis. Here, experimental work is as important as\nare advances in theory, that are indeed often driven by the experimental\nfindings. In parallel computing, scientific contributions presented in research\narticles are therefore often based on experimental data, with a substantial\npart devoted to presenting and discussing the experimental findings. As in all\nof experimental science, experiments must be presented in a way that makes\nreproduction by other researchers possible, in principle. Despite appearance to\nthe contrary, we contend that reproducibility plays a small role, and is\ntypically not achieved. As can be found, articles often do not have a\nsufficiently detailed description of their experiments, and do not make\navailable the software used to obtain the claimed results. As a consequence,\nparallel computational results are most often impossible to reproduce, often\nquestionable, and therefore of little or no scientific value. We believe that\nthe description of how to reproduce findings should play an important part in\nevery serious, experiment-based parallel computing research article. We aim to\ninitiate a discussion of the reproducibility issue in parallel computing, and\nelaborate on the importance of reproducible research for (1) better and sounder\ntechnical/scientific papers, (2) a sounder and more efficient review process\nand (3) more effective collective work. This paper expresses our current view\non the subject and should be read as a position statement for discussion and\nfuture work. We do not consider the related (but no less important) issue of\nthe quality of the experimental design.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 15:11:35 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Hunold", "Sascha", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1308.4011", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "Using Modularity Metrics to assist Move Method Refactoring of Large\n  System", "comments": null, "journal-ref": "7th International Conference on Complex, Intelligent, and Software\n  Intensive Systems (CISIS), pp. 529-534, 2013", "doi": "10.1109/CISIS.2013.96", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large software systems, refactoring activities can be a challenging task,\nsince for keeping component complexity under control the overall architecture\nas well as many details of each component have to be considered. Product\nmetrics are therefore often used to quantify several parameters related to the\nmodularity of a software system. This paper devises an approach for\nautomatically suggesting refactoring opportunities on large software systems.\nWe show that by assessing metrics for all components, move methods refactoring\nan be suggested in such a way to improve modularity of several components at\nonce, without hindering any other. However, computing metrics for large\nsoftware systems, comprising thousands of classes or more, can be a time\nconsuming task when performed on a single CPU. For this, we propose a solution\nthat computes metrics by resorting to GPU, hence greatly shortening computation\ntime. Thanks to our approach precise knowledge on several properties of the\nsystem can be continuously gathered while the system evolves, hence assisting\ndevelopers to quickly assess several solutions for reducing modularity issues.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 13:09:30 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1308.4166", "submitter": "Marco  Netto", "authors": "Carlos Cardonha and Marcos D. Assun\\c{c}\\~ao and Marco A. S. Netto and\n  Renato L. F. Cunha and Carlos Queiroz", "title": "Patience-aware Scheduling for Cloud Services: Freeing Users from the\n  Chains of Boredom", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling of service requests in Cloud computing has traditionally focused\non the reduction of pre-service wait, generally termed as waiting time. Under\ncertain conditions such as peak load, however, it is not always possible to\ngive reasonable response times to all users. This work explores the fact that\ndifferent users may have their own levels of tolerance or patience with\nresponse delays. We introduce scheduling strategies that produce better\nassignment plans by prioritising requests from users who expect to receive the\nresults earlier and by postponing servicing jobs from those who are more\ntolerant to response delays. Our analytical results show that the behaviour of\nusers' patience plays a key role in the evaluation of scheduling techniques,\nand our computational evaluation demonstrates that, under peak load, the new\nalgorithms typically provide better user experience than the traditional FIFO\nstrategy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 20:30:38 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Cardonha", "Carlos", ""], ["Assun\u00e7\u00e3o", "Marcos D.", ""], ["Netto", "Marco A. S.", ""], ["Cunha", "Renato L. F.", ""], ["Queiroz", "Carlos", ""]]}, {"id": "1308.4208", "submitter": "Jos\\'e Fernando Santos Carvalho", "authors": "Jose Fernando S. Carvalho, Paulo Anselmo da Mota Silveira Neto,\n  Vincius Cardoso Garcia, Rodrigo Elia Assad, Frederico Durao", "title": "A Systematic Mapping Study on Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing emerges from the global economic crisis as an option to use\ncomputing resources from a more rational point of view. In other words, a\ncheaper way to have IT resources. However, issues as security and privacy, SLA\n(Service Layer Agreement), resource sharing, and billing has left open\nquestions about the real gains of that model. This study aims to investigate\nstate-of-the-art in Cloud Computing, identify gaps, challenges, synthesize\navailable evidences both its use and development, and provides relevant\ninformation, clarifying open questions and common discussed issues about that\nmodel through literature. The good practices of systematic map- ping study\nmethodology were adopted in order to reach those objectives. Al- though Cloud\nComputing is based on a business model with over 50 years of existence,\nevidences found in this study indicate that Cloud Computing still presents\nlimitations that prevent the full use of the proposal on-demand.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 02:17:27 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Carvalho", "Jose Fernando S.", ""], ["Neto", "Paulo Anselmo da Mota Silveira", ""], ["Garcia", "Vincius Cardoso", ""], ["Assad", "Rodrigo Elia", ""], ["Durao", "Frederico", ""]]}, {"id": "1308.4391", "submitter": "Reza Rahimi", "authors": "M. Reza Rahimi, Nalini Venkatasubramanian, Sharad Mehrotra, and\n  Athanasios V. Vasilakos", "title": "On Optimal and Fair Service Allocation in Mobile Cloud Computing", "comments": "21 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the optimal and fair service allocation for a variety of\nmobile applications (single or group and collaborative mobile applications) in\nmobile cloud computing. We exploit the observation that using tiered clouds,\ni.e. clouds at multiple levels (local and public) can increase the performance\nand scalability of mobile applications. We proposed a novel framework to model\nmobile applications as a location-time workflows (LTW) of tasks; here users\nmobility patterns are translated to mobile service usage patterns. We show that\nan optimal mapping of LTWs to tiered cloud resources considering multiple QoS\ngoals such application delay, device power consumption and user cost/price is\nan NP-hard problem for both single and group-based applications. We propose an\nefficient heuristic algorithm called MuSIC that is able to perform well (73% of\noptimal, 30% better than simple strategies), and scale well to a large number\nof users while ensuring high mobile application QoS. We evaluate MuSIC and the\n2-tier mobile cloud approach via implementation (on real world clouds) and\nextensive simulations using rich mobile applications like intensive signal\nprocessing, video streaming and multimedia file sharing applications. Our\nexperimental and simulation results indicate that MuSIC supports scalable\noperation (100+ concurrent users executing complex workflows) while improving\nQoS. We observe about 25% lower delays and power (under fixed price\nconstraints) and about 35% decrease in price (considering fixed delay) in\ncomparison to only using the public cloud. Our studies also show that MuSIC\nperforms quite well under different mobility patterns, e.g. random waypoint and\nManhattan models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 19:36:58 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Rahimi", "M. Reza", ""], ["Venkatasubramanian", "Nalini", ""], ["Mehrotra", "Sharad", ""], ["Vasilakos", "Athanasios V.", ""]]}, {"id": "1308.5177", "submitter": "sukhjit Singh Sehra Er.", "authors": "Gitanjali, Sukhjit Singh Sehra, Jaiteg Singh", "title": "Policy Specification in Role based Access Control on Clouds", "comments": null, "journal-ref": "International Journal of Computer Applications 75(1):39-43, August\n  2013", "doi": "10.5120/13078-0253", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing is a set of IT Services that are provided to a customer over\na network and these services are delivered by third party provider who owns the\ninfrastructure and reduce the burden at user's end. Nowadays researchers\ndevoted their work access control method to enhance the security on Cloud. RBAC\nis attractive access model because the number of roles is significantly less\nhence users can be easily classified according to their roles. The Role-based\nAccess Control (RBAC) model provides efficient way to manage access to\ninformation while reducing the cost of security administration and complexity\nin large networked applications. This paper specify various policies in RBAC on\nclouds such as migration policy which helps the user to migrate the database\nschema and roles easily to the Cloud using XML with more security. Restriction\npolicy provide the security enhancement in Role Based Access Model by\nrestricting the number of transaction per user and if the number of\ntransactions will increase the admin will come to know through its monitoring\nsystem that unauthorized access has been made and it would be easier to take\naction against such happening. This paper proposes backup and restoration\npolicy in Role Based Access Model in which if the main cloud is crashed or not\nworking properly then the backup and restoration facility will be available to\navoid the lost of important data. In this case chances of loss of data are very\nless so enhance more security on Cloud Computing.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 16:41:56 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Gitanjali", "", ""], ["Sehra", "Sukhjit Singh", ""], ["Singh", "Jaiteg", ""]]}, {"id": "1308.6058", "submitter": "Navaz Syed A S", "authors": "A.S.Syed Navaz, C.Prabhadevi and V.Sangeetha", "title": "Data Grid Concepts for Data Security in Distributed Computing", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data grid is a distributed computing architecture that integrates a large\nnumber of data and computing resources into a single virtual data management\nsystem. It enables the sharing and coordinated use of data from various\nresources and provides various services to fit the needs of high performance\ndistributed and data-intensive computing. Here data partitioning and dynamic\nreplication in data grid are considered. In which security and access\nperformance of a system are efficient. There are several important requirements\nfor data grids, including information survivability, security, and access\nperformance. More specifically, the investigation is the problem of optimal\nallocation of sensitive data objects that are partitioned by using secret\nsharing scheme or erasure coding scheme and replicated. DATA PARTITIONING is\nknown as the single data can be divided into multiple objects. REPLICATION is\nknown as process of sharing information. storing same data in multiple systems.\nReplication techniques are frequently used to improve data availability. Single\npoint failure does not affect this system. Where the data will be secured.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 04:51:08 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Navaz", "A. S. Syed", ""], ["Prabhadevi", "C.", ""], ["Sangeetha", "V.", ""]]}, {"id": "1308.6208", "submitter": "Rong Yu", "authors": "Rong Yu, Yan Zhang, Stein Gjessing, Wenlong Xia, Kun Yang", "title": "Toward Cloud-based Vehicular Networks with Efficient Resource Management", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Internet of Things, all components in intelligent\ntransportation systems will be connected to improve transport safety, relieve\ntraffic congestion, reduce air pollution and enhance the comfort of driving.\nThe vision of all vehicles connected poses a significant challenge to the\ncollection and storage of large amounts of traffic-related data. In this\narticle, we propose to integrate cloud computing into vehicular networks such\nthat the vehicles can share computation resources, storage resources and\nbandwidth resources. The proposed architecture includes a vehicular cloud, a\nroadside cloud, and a central cloud. Then, we study cloud resource allocation\nand virtual machine migration for effective resource management in this\ncloud-based vehicular network. A game-theoretical approach is presented to\noptimally allocate cloud resources. Virtual machine migration due to vehicle\nmobility is solved based on a resource reservation scheme.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 16:31:55 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Yu", "Rong", ""], ["Zhang", "Yan", ""], ["Gjessing", "Stein", ""], ["Xia", "Wenlong", ""], ["Yang", "Kun", ""]]}, {"id": "1308.6464", "submitter": "Buddhadeb Sau Dr.", "authors": "Buddhadeb Sau and Krishnendu Mukhopadhyaya", "title": "Localizability of Wireless Sensor Networks: Beyond Wheel Extension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network is called localizable if the positions of all the nodes of the\nnetwork can be computed uniquely. If a network is localizable and embedded in\nplane with generic configuration, the positions of the nodes may be computed\nuniquely in finite time. Therefore, identifying localizable networks is an\nimportant function. If the complete information about the network is available\nat a single place, localizability can be tested in polynomial time. In a\ndistributed environment, networks with trilateration orderings (popular in real\napplications) and wheel extensions (a specific class of localizable networks)\nembedded in plane can be identified by existing techniques. We propose a\ndistributed technique which efficiently identifies a larger class of\nlocalizable networks. This class covers both trilateration and wheel\nextensions. In reality, exact distance is almost impossible or costly. The\nproposed algorithm based only on connectivity information. It requires no\ndistance information.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 13:37:44 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Sau", "Buddhadeb", ""], ["Mukhopadhyaya", "Krishnendu", ""]]}, {"id": "1308.6475", "submitter": "Thomas Petig", "authors": "Thomas Petig, Elad M. Schiller, Philippas Tsigas", "title": "Self-stabilizing TDMA Algorithms for Wireless Ad-hoc Networks without\n  External Reference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time division multiple access (TDMA) is a method for sharing communication\nmedia. In wireless communications, TDMA algorithms often divide the radio time\ninto timeslots of uniform size, $\\xi$, and then combine them into frames of\nuniform size, $\\tau$. We consider TDMA algorithms that allocate at least one\ntimeslot in every frame to every node. Given a maximal node degree, $\\delta$,\nand no access to external references for collision detection, time or position,\nwe consider the problem of collision-free self-stabilizing TDMA algorithms that\nuse constant frame size.\n  We demonstrate that this problem has no solution when the frame size is $\\tau\n< \\max\\{2\\delta,\\chi_2\\}$, where $\\chi_2$ is the chromatic number for\ndistance-$2$ vertex coloring. As a complement to this lower bound, we focus on\nproving the existence of collision-free self-stabilizing TDMA algorithms that\nuse constant frame size of $\\tau$. We consider basic settings (no hardware\nsupport for collision detection and no prior clock synchronization), and the\ncollision of concurrent transmissions from transmitters that are at most two\nhops apart. In the context of self-stabilizing systems that have no external\nreference, we are the first to study this problem (to the best of our\nknowledge), and use simulations to show convergence even with computation time\nuncertainties.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 14:12:19 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 11:50:16 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Petig", "Thomas", ""], ["Schiller", "Elad M.", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1308.6526", "submitter": "Xavier Vila\\c{c}a", "authors": "Xavier Vila\\c{c}a and Lu\\'is Rodrigues", "title": "On the Effectiveness of Punishments in a Repeated Epidemic Dissemination\n  Game", "comments": "76 pages, extended technical report, original paper is expected to\n  appear on Proceedings of the 15th International Symposium on Stabilization,\n  Safety, and Security of Distributed Systems (SSS 2013), corrected typo in\n  abstract, corrected citation to Kreps:82, improved description of the stage\n  game to justify fixed stage strategy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work uses Game Theory to study the effectiveness of punishments as an\nincentive for rational nodes to follow an epidemic dissemination protocol. The\ndissemination process is modeled as an infinite repetition of a stage game. At\nthe end of each stage, a monitoring mechanism informs each player of the\nactions of other nodes. The effectiveness of a punishing strategy is measured\nas the range of values for the benefit-to-cost ratio that sustain cooperation.\nThis paper studies both public and private monitoring. Under public monitoring,\nwe show that direct reciprocity is not an effective incentive, whereas full\nindirect reciprocity provides a nearly optimal effectiveness. Under private\nmonitoring, we identify necessary conditions regarding the topology of the\ngraph in order for punishments to be effective. When punishments are\ncoordinated, full indirect reciprocity is also effective with private\nmonitoring.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 17:13:00 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2013 17:49:23 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Vila\u00e7a", "Xavier", ""], ["Rodrigues", "Lu\u00eds", ""]]}, {"id": "1308.6745", "submitter": "Navaz Syed A S", "authors": "A.S.Syed Navaz, V.Sangeetha and C.Prabhadevi", "title": "Entropy based Anomaly Detection System to Prevent DDoS Attacks in Cloud", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing is a recent computing model provides consistent access to\nwide area distributed resources. It revolutionized the IT world with its\nservices provision infrastructure, less maintenance cost, data and service\navailability assurance, rapid accessibility and scalability. Grid and Cloud\nComputing Intrusion Detection System detects encrypted node communication and\nfind the hidden attack trial which inspects and detects those attacks that\nnetwork based and host based cant identify. It incorporates Knowledge and\nbehavior analysis to identify specific intrusions. Signature based IDS monitor\nthe packets in the network and identifies those threats by matching with\ndatabase but It fails to detect those attacks that are not included in\ndatabase. Signature based IDS will perform poor capturing in large volume of\nanomalies. Another problem is that Cloud Service Provider hides the attack that\nis caused by intruder, due to distributed nature cloud environment has high\npossibility for vulnerable resources. By impersonating legitimate users, the\nintruders can use a services abundant resources maliciously. In Proposed System\nwe combine few concepts which are available with new intrusion detection\ntechniques. Here to merge Entropy based System with Anomaly detection System\nfor providing multilevel Distributed Denial of Service. This is done in two\nsteps: First, Users are allowed to pass through router in network site in that\nit incorporates Detection Algorithm and detects for legitimate user. Second,\nagain it pass through router placed in cloud site in that it incorporates\nconfirmation Algorithm and checks for threshold value, if its beyond the\nthreshold value it considered as legitimate user, else its an intruder found in\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 04:55:57 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Navaz", "A. S. Syed", ""], ["Sangeetha", "V.", ""], ["Prabhadevi", "C.", ""]]}, {"id": "1308.6774", "submitter": "Peter Richtarik", "authors": "Rachael Tappenden and Peter Richtarik and Burak Buke", "title": "Separable Approximations and Decomposition Methods for the Augmented\n  Lagrangian", "comments": "28 pages, 6 algorithms, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study decomposition methods based on separable\napproximations for minimizing the augmented Lagrangian. In particular, we study\nand compare the Diagonal Quadratic Approximation Method (DQAM) of Mulvey and\nRuszczy\\'{n}ski and the Parallel Coordinate Descent Method (PCDM) of\nRicht\\'arik and Tak\\'a\\v{c}. We show that the two methods are equivalent for\nfeasibility problems up to the selection of a single step-size parameter.\nFurthermore, we prove an improved complexity bound for PCDM under strong\nconvexity, and show that this bound is at least $8(L'/\\bar{L})(\\omega-1)^2$\ntimes better than the best known bound for DQAM, where $\\omega$ is the degree\nof partial separability and $L'$ and $\\bar{L}$ are the maximum and average of\nthe block Lipschitz constants of the gradient of the quadratic penalty\nappearing in the augmented Lagrangian.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 15:39:32 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Tappenden", "Rachael", ""], ["Richtarik", "Peter", ""], ["Buke", "Burak", ""]]}, {"id": "1308.6823", "submitter": "Hui Miao", "authors": "Hui Miao, Xiangyang Liu, Bert Huang, Lise Getoor", "title": "A Hypergraph-Partitioned Vertex Programming Approach for Large-scale\n  Consensus Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2013.6691623", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern data science problems, techniques for extracting value from big\ndata require performing large-scale optimization over heterogenous, irregularly\nstructured data. Much of this data is best represented as multi-relational\ngraphs, making vertex programming abstractions such as those of Pregel and\nGraphLab ideal fits for modern large-scale data analysis. In this paper, we\ndescribe a vertex-programming implementation of a popular consensus\noptimization technique known as the alternating direction of multipliers\n(ADMM). ADMM consensus optimization allows elegant solution of complex\nobjectives such as inference in rich probabilistic models. We also introduce a\nnovel hypergraph partitioning technique that improves over state-of-the-art\npartitioning techniques for vertex programming and significantly reduces the\ncommunication cost by reducing the number of replicated nodes up to an order of\nmagnitude. We implemented our algorithm in GraphLab and measure scaling\nperformance on a variety of realistic bipartite graph distributions and a large\nsynthetic voter-opinion analysis application. In our experiments, we are able\nto achieve a 50% improvement in runtime over the current state-of-the-art\nGraphLab partitioning scheme.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 19:30:44 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Miao", "Hui", ""], ["Liu", "Xiangyang", ""], ["Huang", "Bert", ""], ["Getoor", "Lise", ""]]}]