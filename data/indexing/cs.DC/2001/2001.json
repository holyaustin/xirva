[{"id": "2001.00090", "submitter": "Somali Chaterji", "authors": "Somali Chaterji, Parinaz Naghizadeh, Muhammad Ashraful Alam, Saurabh\n  Bagchi, Mung Chiang, David Corman, Brian Henz, Suman Jana, Na Li, Shaoshuai\n  Mou, Meeko Oishi, Chunyi Peng, Tiark Rompf, Ashutosh Sabharwal, Shreyas\n  Sundaram, James Weimer, Jennifer Weller", "title": "Resilient Cyberphysical Systems and their Application Drivers: A\n  Technology Roadmap", "comments": "36 pages, 2 figures, NSF-supported workshop on Grand Challenges in\n  Resilience, held at Purdue, March 20-21, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberphysical systems (CPS) are ubiquitous in our personal and professional\nlives, and they promise to dramatically improve micro-communities (e.g., urban\nfarms, hospitals), macro-communities (e.g., cities and metropolises), urban\nstructures (e.g., smart homes and cars), and living structures (e.g., human\nbodies, synthetic genomes). The question that we address in this article\npertains to designing these CPS systems to be resilient-from-the-ground-up, and\nthrough progressive learning, resilient-by-reaction. An optimally designed\nsystem is resilient to both unique attacks and recurrent attacks, the latter\nwith a lower overhead. Overall, the notion of resilience can be thought of in\nthe light of three main sources of lack of resilience, as follows: exogenous\nfactors, such as natural variations and attack scenarios; mismatch between\nengineered designs and exogenous factors ranging from DDoS (distributed\ndenial-of-service) attacks or other cybersecurity nightmares, so called \"black\nswan\" events, disabling critical services of the municipal electrical grids and\nother connected infrastructures, data breaches, and network failures; and the\nfragility of engineered designs themselves encompassing bugs, human-computer\ninteractions (HCI), and the overall complexity of real-world systems. In the\npaper, our focus is on design and deployment innovations that are broadly\napplicable across a range of CPS application areas.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 01:33:01 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chaterji", "Somali", ""], ["Naghizadeh", "Parinaz", ""], ["Alam", "Muhammad Ashraful", ""], ["Bagchi", "Saurabh", ""], ["Chiang", "Mung", ""], ["Corman", "David", ""], ["Henz", "Brian", ""], ["Jana", "Suman", ""], ["Li", "Na", ""], ["Mou", "Shaoshuai", ""], ["Oishi", "Meeko", ""], ["Peng", "Chunyi", ""], ["Rompf", "Tiark", ""], ["Sabharwal", "Ashutosh", ""], ["Sundaram", "Shreyas", ""], ["Weimer", "James", ""], ["Weller", "Jennifer", ""]]}, {"id": "2001.00138", "submitter": "Xiaolong Ma", "authors": "Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin,\n  Yanzhi Wang, Bin Ren", "title": "PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with\n  Pattern-based Weight Pruning", "comments": "To be published in the Proceedings of Twenty-Fifth International\n  Conference on Architectural Support for Programming Languages and Operating\n  Systems (ASPLOS 20)", "journal-ref": null, "doi": "10.1145/3373376.3378534", "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of a spectrum of high-end mobile devices, many\napplications that formerly required desktop-level computation capability are\nbeing transferred to these devices. However, executing the inference of Deep\nNeural Networks (DNNs) is still challenging considering high computation and\nstorage demands, specifically, if real-time performance with high accuracy is\nneeded. Weight pruning of DNNs is proposed, but existing schemes represent two\nextremes in the design space: non-structured pruning is fine-grained, accurate,\nbut not hardware friendly; structured pruning is coarse-grained,\nhardware-efficient, but with higher accuracy loss. In this paper, we introduce\na new dimension, fine-grained pruning patterns inside the coarse-grained\nstructures, revealing a previously unknown point in design space. With the\nhigher accuracy enabled by fine-grained pruning patterns, the unique insight is\nto use the compiler to re-gain and guarantee high hardware efficiency. In other\nwords, our method achieves the best of both worlds, and is desirable across\ntheory/algorithm, compiler, and hardware levels. The proposed PatDNN is an\nend-to-end framework to efficiently execute DNN on mobile devices with the help\nof a novel model compression technique (pattern-based pruning based on extended\nADMM solution framework) and a set of thorough architecture-aware compiler- and\ncode generation-based optimizations (filter kernel reordering, compressed\nweight storage, register load redundancy elimination, and parameter\nauto-tuning). Evaluation results demonstrate that PatDNN outperforms three\nstate-of-the-art end-to-end DNN frameworks, TensorFlow Lite, TVM, and Alibaba\nMobile Neural Network with speedup up to 44.5x, 11.4x, and 7.1x, respectively,\nwith no accuracy compromise. Real-time inference of representative large-scale\nDNNs (e.g., VGG-16, ResNet-50) can be achieved using mobile devices.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 04:52:07 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 00:27:57 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 04:32:38 GMT"}, {"version": "v4", "created": "Wed, 22 Jan 2020 04:13:06 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Niu", "Wei", ""], ["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Wang", "Shihao", ""], ["Qian", "Xuehai", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""], ["Ren", "Bin", ""]]}, {"id": "2001.00164", "submitter": "Vinu Ellampallil Venugopal", "authors": "Vinu E. Venugopal, Martin Theobald, Samira Chaychi and Amal Tawakuli", "title": "AIR: A Light-Weight Yet High-Performance Dataflow Engine based on\n  Asynchronous Iterative Routing", "comments": "16 pages, 6 figures, 15 plots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Stream Processing Systems (DSPSs) are among the currently most\nemerging topics in data management, with applications ranging from real-time\nevent monitoring to processing complex dataflow programs and big data\nanalytics. The major market players in this domain are clearly represented by\nApache Spark and Flink, which provide a variety of frontend APIs for SQL,\nstatistical inference, machine learning, stream processing, and many others.\nYet rather few details are reported on the integration of these engines into\nthe underlying High-Performance Computing (HPC) infrastructure and the\ncommunication protocols they use. Spark and Flink, for example, are implemented\nin Java and still rely on a dedicated master node for managing their control\nflow among the worker nodes in a compute cluster.\n  In this paper, we describe the architecture of our AIR engine, which is\ndesigned from scratch in C++ using the Message Passing Interface (MPI),\npthreads for multithreading, and is directly deployed on top of a common HPC\nworkload manager such as SLURM. AIR implements a light-weight, dynamic sharding\nprotocol (referred to as \"Asynchronous Iterative Routing\"), which facilitates a\ndirect and asynchronous communication among all client nodes and thereby\ncompletely avoids the overhead induced by the control flow with a master node\nthat may otherwise form a performance bottleneck. Our experiments over a\nvariety of benchmark settings confirm that AIR outperforms Spark and Flink in\nterms of latency and throughput by a factor of up to 15; moreover, we\ndemonstrate that AIR scales out much better than existing DSPSs to clusters\nconsisting of up to 8 nodes and 224 cores.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 08:45:28 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 10:59:31 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Venugopal", "Vinu E.", ""], ["Theobald", "Martin", ""], ["Chaychi", "Samira", ""], ["Tawakuli", "Amal", ""]]}, {"id": "2001.00222", "submitter": "Christina Delimitrou", "authors": "Shannon Joyner, Michael MacCoss, Christina Delimitrou, Hakim\n  Weatherspoon", "title": "Ripple: A Practical Declarative Programming Framework for Serverless\n  Compute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has emerged as a promising alternative to\ninfrastructure- (IaaS) and platform-as-a-service (PaaS)cloud platforms for\napplications with ample parallelism and intermittent activity. Serverless\npromises greater resource elasticity, significant cost savings, and simplified\napplication deployment. All major cloud providers, including Amazon, Google,\nand Microsoft, have introduced serverless to their public cloud offerings. For\nserverless to reach its potential, there is a pressing need for programming\nframeworks that abstract the deployment complexity away from the user. This\nincludes simplifying the process of writing applications for serverless\nenvironments, automating task and data partitioning, and handling scheduling\nand fault tolerance.\n  We present Ripple, a programming framework designed to specifically take\napplications written for single-machine execution and allow them to take\nadvantage of the task parallelism of serverless. Ripple exposes a simple\ninterface that users can leverage to express the high-level dataflow of a wide\nspectrum of applications, including machine learning (ML) analytics, genomics,\nand proteomics. Ripple also automates resource provisioning, meeting\nuser-defined QoS targets, and handles fault tolerance by eagerly detecting\nstraggler tasks. We port Ripple over AWS Lambda and show that, across a set of\ndiverse applications, it provides an expressive and generalizable programming\nframework that simplifies running data-parallel applications on serverless, and\ncan improve performance by up to 80x compared to IaaS/PaaS clouds for similar\ncosts.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 15:20:41 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Joyner", "Shannon", ""], ["MacCoss", "Michael", ""], ["Delimitrou", "Christina", ""], ["Weatherspoon", "Hakim", ""]]}, {"id": "2001.00363", "submitter": "Gal Assa", "authors": "Gal Assa, Hagar Meir, Guy Golan-Gueta, Idit Keidar, Alexander\n  Spiegelman", "title": "Using Nesting to Push the Limits of Transactional Data Structure\n  Libraries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional data structure libraries (TDSL) combine the ease-of-programming\nof transactions with the high performance and scalability of custom-tailored\nconcurrent data structures. They can be very efficient thanks to their ability\nto exploit data structure semantics in order to reduce overhead, aborts, and\nwasted work compared to general-purpose software transactional memory. However,\nTDSLs were not previously used for complex use-cases involving long\ntransactions and a variety of data structures.\n  In this paper, we boost the performance and usability of a TDSL, towards\nallowing it to support complex applications. A key idea is nesting. Nested\ntransactions create checkpoints within a longer transaction, so as to limit the\nscope of abort, without changing the semantics of the original transaction. We\nbuild a Java TDSL with built-in support for nested transactions over a number\nof data structures. We conduct a case study of a complex network intrusion\ndetection system that invests a significant amount of work to process each\npacket. Our study shows that our library outperforms publicly available STMs\ntwofold without nesting, and by up to 16x when nesting is used.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 09:00:37 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:26:27 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Assa", "Gal", ""], ["Meir", "Hagar", ""], ["Golan-Gueta", "Guy", ""], ["Keidar", "Idit", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "2001.00567", "submitter": "Faheem Zafari", "authors": "Faheem Zafari, Kin K. Leung, Don Towsley, Prithwish Basu, Ananthram\n  Swami and Jian Li", "title": "Let's Share: A Game-Theoretic Framework for Resource Sharing in Mobile\n  Edge Clouds", "comments": "The paper is currently under review in IEEE Transactions on Network\n  and Service Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing seeks to provide resources to different delay-sensitive\napplications. This is a challenging problem as an edge cloud-service provider\nmay not have sufficient resources to satisfy all resource requests.\nFurthermore, allocating available resources optimally to different applications\nis also challenging. Resource sharing among different edge cloud-service\nproviders can address the aforementioned limitation as certain service\nproviders may have resources available that can be ``rented'' by other service\nproviders. However, edge cloud service providers can have different objectives\nor \\emph{utilities}. Therefore, there is a need for an efficient and effective\nmechanism to share resources among service providers, while considering the\ndifferent objectives of various providers. We model resource sharing as a\nmulti-objective optimization problem and present a solution framework based on\n\\emph{Cooperative Game Theory} (CGT). We consider the strategy where each\nservice provider allocates resources to its native applications first and\nshares the remaining resources with applications from other service providers.\nWe prove that for a monotonic, non-decreasing utility function, the game is\ncanonical and convex. Hence, the \\emph{core} is not empty and the grand\ncoalition is stable. We propose two algorithms \\emph{Game-theoretic Pareto\noptimal allocation} (GPOA) and \\emph{Polyandrous-Polygamous Matching based\nPareto Optimal Allocation} (PPMPOA) that provide allocations from the core.\nHence the obtained allocations are \\emph{Pareto} optimal and the grand\ncoalition of all the service providers is stable. Experimental results confirm\nthat our proposed resource sharing framework improves utilities of edge\ncloud-service providers and application request satisfaction.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 18:58:26 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Zafari", "Faheem", ""], ["Leung", "Kin K.", ""], ["Towsley", "Don", ""], ["Basu", "Prithwish", ""], ["Swami", "Ananthram", ""], ["Li", "Jian", ""]]}, {"id": "2001.00660", "submitter": "Jiajia Li", "authors": "Jiajia Li and Mahesh Lakshminarasimhan and Xiaolong Wu and Ang Li and\n  Catherine Olschanowsky and Kevin Barker", "title": "A Parallel Sparse Tensor Benchmark Suite on CPUs and GPUs", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tensor computations present significant performance challenges that impact a\nwide spectrum of applications ranging from machine learning, healthcare\nanalytics, social network analysis, data mining to quantum chemistry and signal\nprocessing. Efforts to improve the performance of tensor computations include\nexploring data layout, execution scheduling, and parallelism in common tensor\nkernels. This work presents a benchmark suite for arbitrary-order sparse tensor\nkernels using state-of-the-art tensor formats: coordinate (COO) and\nhierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of\nreference tensor kernel implementations that are compatible with real-world\ntensors and power law tensors extended from synthetic graph generation\ntechniques. We also propose Roofline performance models for these kernels to\nprovide insights of computer platforms from sparse tensor view.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 22:56:15 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Li", "Jiajia", ""], ["Lakshminarasimhan", "Mahesh", ""], ["Wu", "Xiaolong", ""], ["Li", "Ang", ""], ["Olschanowsky", "Catherine", ""], ["Barker", "Kevin", ""]]}, {"id": "2001.00746", "submitter": "Faizan Safdar Ali", "authors": "Faizan Ali, Moayad Aloqaily, Omar Alfandi, and Oznur Ozkasap", "title": "Cyberphysical Blockchain-Enabled Peer-to-Peer Energy Trading", "comments": "6 pages, 3 Figures, IEEE Computer, Accepted with minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability and security problems of the centralized architecture models in\ncyberphysical systems have great potential to be solved by novel blockchain\nbased distributed models.A decentralized energy trading system takes advantage\nof various sources and effectively coordinates the energy to ensure optimal\nutilization of the available resources. It achieves that goal by managing\nphysical, social and business infrastructures using technologies such as\nInternet of Things (IoT), cloud computing and network systems. Addressing the\nimportance of blockchain-enabled energy trading in the context of cyberphysical\nsystems, this article provides a thorough overview of the P2P energy trading\nand the utilization of blockchain to enhance the efficiency and the overall\nperformance including the degree of decentralization, scalability and the\nsecurity of the systems. Three blockchain based energy trading models have been\nproposed to overcome the technical challenges and market barriers for better\nadoption of this disruptive technology.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 07:12:09 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 07:40:48 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 11:16:12 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ali", "Faizan", ""], ["Aloqaily", "Moayad", ""], ["Alfandi", "Omar", ""], ["Ozkasap", "Oznur", ""]]}, {"id": "2001.00747", "submitter": "Faizan Safdar Ali", "authors": "Faizan Safdar Ali, Alptekin Kupcu", "title": "Improving PKI, BGP, and DNS Using Blockchain: A Systematic Review", "comments": "6 Pages, 2 Figures, ISC Turkey", "journal-ref": "https://www.iscturkey.org/, 2019", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has many backbone components on top of which the whole world is\nconnected. It is important to make these components, like Border Gateway\nProtocol (BGP), Domain Name System (DNS), and Public Key Infrastructure (PKI),\nsecure and work without any interruption. All of the aforementioned components\nhave vulnerabilities, mainly because of their dependence on the centralized\nparties, that should be resolved.\n  Blockchain is revolutionizing the concept of today's Internet, primarily\nbecause of its degree of decentralization and security properties. In this\npaper, we discuss how blockchain provides nearly complete solutions to the open\nchallenges for these network backbone components.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 07:12:54 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Ali", "Faizan Safdar", ""], ["Kupcu", "Alptekin", ""]]}, {"id": "2001.00767", "submitter": "Yu Yang", "authors": "Yu Yang, Guoqiang Hu, Costas J. Spanos", "title": "A Proximal Linearization-based Decentralized Method for Nonconvex\n  Problems with Nonlinear Constraints", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized optimization for non-convex problems are now demanding by many\nemerging applications (e.g., smart grids, smart building, etc.). Though\ndramatic progress has been achieved in convex problems, the results for\nnon-convex cases, especially with non-linear constraints, are still largely\nunexplored. This is mainly due to the challenges imposed by the non-linearity\nand non-convexity, which makes establishing the convergence conditions\nbewildered. This paper investigates decentralized optimization for a class of\nstructured non-convex problems characterized by: (i) nonconvex global objective\nfunction (possibly nonsmooth) and (ii) coupled nonlinear constraints and local\nbounded convex constraints w.r.t. the agents. For such problems, a\ndecentralized approach called Proximal Linearizationbased Decentralized Method\n(PLDM) is proposed. Different from the traditional (augmented) Lagrangian-based\nmethods which usually require the exact (local) optima at each iteration, the\nproposed method leverages a proximal linearization-based technique to update\nthe decision variables iteratively, which makes it computationally efficient\nand viable for the non-linear cases. Under some standard conditions, the PLDM\nglobal convergence and local convergence rate to the epsilon-critical points\nare studied based on the Kurdyka-Lojasiewicz property which holds for most\nanalytical functions. Finally, the performance and efficacy of the method are\nillustrated through a numerical example and an application to multi-zone\nheating, ventilation and air-conditioning (HVAC) control.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 09:29:53 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Yang", "Yu", ""], ["Hu", "Guoqiang", ""], ["Spanos", "Costas J.", ""]]}, {"id": "2001.00884", "submitter": "Idris Abdulmumin", "authors": "Garba Aliyu, Kana A. F. D., Abdullahi Mohammed, Idris Abdulmumin,\n  Shehu Adamu, Fatsuma Jauro", "title": "Improving Grid Computing Performance by Optimally Reducing Checkpointing\n  Effect", "comments": "17 pages, 8 figures, 8 tables", "journal-ref": "International Journal of Information Processing and Communication\n  (IJIPC) 9 (2020) 52-64", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid computing is a collection of computer resources that are gathered\ntogether from various areas to give computational resources such as storage,\ndata or application services. This is to permit clients to access this huge\nmeasure of processing resources without the need to know where these might be\nfound and what technology such as, hardware equipment and operating system was\nused. Dependability and performance are among the key difficulties faced in a\ngrid computing environment. Various systems have been proposed in the\nliterature to handle recouping from resource failure in Grid computing\nenvironment. One case of such system is checkpointing. Checkpointing is a\nsystem that endures faults when resources failed. Checkpointing method has the\nupside of lessening the work lost because of resource faults. However,\ncheckpointing presents a huge runtime overhead. In this paper, we propose an\nimproved checkpointing system to bring down runtime overhead. A replica is\nadded to ensure the availability of resources. This replicates all\ncheckpointing files to other machines as opposed to having dedicated\ncheckpointing machine. The results of simulation employing GridSim noted that\nretaining the number of resources fixed and changing the number of gridlets,\ngains of up to 11%, 9%, and 11% on makespan, throughput, and turnaround time\nrespectively were realized while changing the number of resources and\npreserving the number of gridlets fixed, increases of up to 11%, 8%, and 9% on\nmakespan, throughput, and turnaround time respectively, were realized\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 16:35:10 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 12:31:28 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Aliyu", "Garba", ""], ["D.", "Kana A. F.", ""], ["Mohammed", "Abdullahi", ""], ["Abdulmumin", "Idris", ""], ["Adamu", "Shehu", ""], ["Jauro", "Fatsuma", ""]]}, {"id": "2001.00919", "submitter": "Tarun Chitra", "authors": "Tarun Chitra", "title": "Competitive equilibria between staking and on-chain lending", "comments": "25 pages, Accepted to Stanford Blockchain Conference and MIT\n  Cryptoeconomic Systems '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Proof of Stake (PoS) is a burgeoning Sybil resistance mechanism that aims to\nhave a digital asset (\"token\") serve as security collateral in crypto networks.\nHowever, PoS has so far eluded a comprehensive threat model that encompasses\nboth Byzantine attacks from distributed systems and financial attacks that\narise from the dual usage of the token as a means of payment and a Sybil\nresistance mechanism. In particular, the existence of derivatives markets makes\nmalicious coordination among validators easier to execute than in Proof of Work\nsystems. We demonstrate that it is also possible for on-chain lending smart\ncontracts to cannibalize network security in PoS systems. When the yield\nprovided by these contracts is more attractive than the inflation rate provided\nfrom staking, stakers will tend to remove their staked tokens and lend them\nout, thus reducing network security. In this paper, we provide a simple\nstochastic model that describes how rational validators with varying risk\npreferences react to changes in staking and lending returns. For a particular\nconfiguration of this model, we provide a formal proof of a phase transition\nbetween equilibria in which tokens are predominantly staked and those in which\nthey are predominantly lent. We further validate this emergent adversarial\nbehavior (e.g. reduced staked token supply) with agent-based simulations that\nsample transitions under more realistic conditions. Our results illustrate that\nrational, non-adversarial actors can dramatically reduce PoS network security\nif block rewards are not calibrated appropriately above the expected yields of\non-chain lending.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 01:22:35 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 18:22:52 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Chitra", "Tarun", ""]]}, {"id": "2001.00975", "submitter": "Mahmoud Barhamgi", "authors": "Mahmoud Barhamgi, Charith Perera, Chia-Mu Yu, Djamal Benslimane, David\n  Camacho and Christine Bonnet", "title": "Privacy in Data Service Composition", "comments": null, "journal-ref": null, "doi": "10.1109/TSC.2019.2963309", "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern information systems different information features, about the same\nindividual, are often collected and managed by autonomous data collection\nservices that may have different privacy policies. Answering many end-users'\nlegitimate queries requires the integration of data from multiple such\nservices. However, data integration is often hindered by the lack of a trusted\nentity, often called a mediator, with which the services can share their data\nand delegate the enforcement of their privacy policies. In this paper, we\npropose a flexible privacy-preserving data integration approach for answering\ndata integration queries without the need for a trusted mediator. In our\napproach, services are allowed to enforce their privacy policies locally. The\nmediator is considered to be untrusted, and only has access to encrypted\ninformation to allow it to link data subjects across the different services.\nServices, by virtue of a new privacy requirement, dubbed k-Protection, limiting\nprivacy leaks, cannot infer information about the data held by each other.\nEnd-users, in turn, have access to privacy-sanitized data only. We evaluated\nour approach using an example and a real dataset from the healthcare\napplication domain. The results are promising from both the privacy\npreservation and the performance perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 20:21:45 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Barhamgi", "Mahmoud", ""], ["Perera", "Charith", ""], ["Yu", "Chia-Mu", ""], ["Benslimane", "Djamal", ""], ["Camacho", "David", ""], ["Bonnet", "Christine", ""]]}, {"id": "2001.01043", "submitter": "Shibo Wang", "authors": "Shibo Wang, Shusen Yang, Cong Zhao", "title": "SurveilEdge: Real-time Video Query based on Collaborative Cloud-Edge\n  Deep Learning", "comments": "To appear at IEEE INFOCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-time query of massive surveillance video data plays a fundamental\nrole in various smart urban applications such as public safety and intelligent\ntransportation. Traditional cloud-based approaches are not applicable because\nof high transmission latency and prohibitive bandwidth cost, while edge devices\nare often incapable of executing complex vision algorithms with low latency and\nhigh accuracy due to restricted resources. Given the infeasibility of both\ncloud-only and edge-only solutions, we present SurveilEdge, a collaborative\ncloud-edge system for real-time queries of large-scale surveillance video\nstreams. Specifically, we design a convolutional neural network (CNN) training\nscheme to reduce the training time with high accuracy, and an intelligent task\nallocator to balance the load among different computing nodes and to achieve\nthe latency-accuracy tradeoff for real-time queries. We implement SurveilEdge\non a prototype with multiple edge devices and a public Cloud, and conduct\nextensive experiments using realworld surveillance video datasets. Evaluation\nresults demonstrate that SurveilEdge manages to achieve up to 7x less bandwidth\ncost and 5.4x faster query response time than the cloud-only solution; and can\nimprove query accuracy by up to 43.9% and achieve 15.8x speedup respectively,\nin comparison with edge-only approaches.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 06:05:23 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 03:26:24 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Shibo", ""], ["Yang", "Shusen", ""], ["Zhao", "Cong", ""]]}, {"id": "2001.01146", "submitter": "Li-Yang Tan", "authors": "Moses Charikar, Weiyun Ma, Li-Yang Tan", "title": "New lower bounds for Massively Parallel Computation from query\n  complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roughgarden, Vassilvitskii, and Wang (JACM 18) recently introduced a novel\nframework for proving lower bounds for Massively Parallel Computation using\ntechniques from boolean function complexity. We extend their framework in two\ndifferent ways, to capture two common features of Massively Parallel\nComputation:\n  $\\circ$ Adaptivity, where machines can write to and adaptively read from\nshared memory throughout the execution of the computation. Recent work of\nBehnezhad et al. (SPAA 19) showed that adaptivity enables significantly\nimproved round complexities for a number of central graph problems.\n  $\\circ$ Promise problems, where the algorithm only has to succeed on certain\ninputs. These inputs may have special structure that is of particular interest,\nor they may be representative of hard instances of the overall problem.\n  Using this extended framework, we give the first unconditional lower bounds\non the complexity of distinguishing whether an input graph is a cycle of length\n$n$ or two cycles of length $n/2$. This promise problem, 1v2-Cycle, has emerged\nas a central problem in the study of Massively Parallel Computation. We prove\nthat any adaptive algorithm for the 1v2-Cycle problem with I/O capacity\n$O(n^{\\varepsilon})$ per machine requires $\\Omega(1/\\varepsilon)$ rounds,\nmatching a recent upper bound of Behnezhad et al.\n  In addition to strengthening the connections between Massively Parallel\nComputation and boolean function complexity, we also develop new machinery to\nreason about the latter. At the heart of our proofs are optimal lower bounds on\nthe query complexity and approximate certificate complexity of the 1v2-Cycle\nproblem.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 00:43:50 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Charikar", "Moses", ""], ["Ma", "Weiyun", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2001.01174", "submitter": "Dongfang Zhao", "authors": "Xinying Wang, Olamide Timothy Tawose, Feng Yan, Dongfang Zhao", "title": "Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain\n  Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interoperability across multiple blockchains would play a critical role\nin future blockchain-based data management paradigm. Existing techniques either\nwork only for two blockchains or requires a centralized component to govern the\ncross-blockchain transaction execution, neither of which would meet the\nscalability requirement. This paper proposes a new distributed commit protocol,\nnamely \\textit{cross-blockchain transaction} (CBT), for conducting transactions\nacross an arbitrary number of blockchains without any centralized component.\nThe key idea of CBT is to extend the two-phase commit protocol with a heartbeat\nmechanism to ensure the liveness of CBT without introducing additional nodes or\nblockchains. We have implemented CBT and compared it to the state-of-the-art\nprotocols, demonstrating CBT's low overhead (3.6\\% between two blockchains,\nless than $1\\%$ among 32 or more blockchains) and high scalability (linear\nscalability on up to 64-blockchain transactions). In addition, we developed a\ngraphic user interface for users to virtually monitor the status of the\ncross-blockchain transactions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 05:58:41 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Wang", "Xinying", ""], ["Tawose", "Olamide Timothy", ""], ["Yan", "Feng", ""], ["Zhao", "Dongfang", ""]]}, {"id": "2001.01192", "submitter": "Martin Stufi", "authors": "Martin \\v{S}tufi, Boris Ba\\v{c}i\\'c, Leonid Stoimenov", "title": "Big Data Architecture in Czech Republic Healthcare Service:\n  Requirements, TPC-H Benchmarks and Vertica", "comments": "Manuscript prepared for Big Data journal\n  (https://home.liebertpub.com/publications/big-data/611/overview)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Big data in healthcare has made a positive difference in advancing analytical\ncapabilities and lowering the costs of medical care. In addition to providing\nanalytical capabilities on platforms supporting current and near-future AI with\nmachine-learning and data-mining algorithms, there is also a need for ethical\nconsiderations mandating new ways to preserve privacy, all of which are\npreconditioned by the growing body of regulations and expectations. The purpose\nof this study is to improve existing clinical care by implementing a big data\nplatform for the Czech Republic National Health Service. Based on the achieved\nperformance and its compliance with mandatory guidelines, the reported big-data\nplatform was selected as the winning solution from the Czech Republic national\ntender (Tender Id. VZ0036628, No. Z2017-035520). The platform, based on\nanalytical Vertica NoSQL database for massive data processing, complies with\nthe TPC-H1 for decision support benchmark, the European Union (EU) and the\nCzech Republic requirements, well-exceeding defined system performance\nthresholds. The reported artefacts and concepts are transferrable to healthcare\nsystems in other countries and are intended to provide personalised autonomous\nassessment from big data in a cost-effective, scalable and high-performance\nmanner. The implemented platform allows: (1) scalability; (2) further\nimplementations of newly-developed machine learning algorithms for\nclassification and predictive analytics; (3) security improvements related to\nElectronic Health Records (EHR) by using automated functions for data\nencryption and decryption; and (4) the use of big data to allow strategic\nplanning in healthcare.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 08:51:33 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["\u0160tufi", "Martin", ""], ["Ba\u010di\u0107", "Boris", ""], ["Stoimenov", "Leonid", ""]]}, {"id": "2001.01278", "submitter": "Vasilios Mavroudis Mr", "authors": "Vasilios Mavroudis, Karl W\\\"ust, Aritra Dhar, Kari Kostiainen, Srdjan\n  Capkun", "title": "Snappy: Fast On-chain Payments with Practical Collaterals", "comments": "Network and Distributed Systems Security (NDSS) Symposium 2020, 23-26\n  February 2020, San Diego, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissionless blockchains offer many advantages but also have significant\nlimitations including high latency. This prevents their use in important\nscenarios such as retail payments, where merchants should approve payments\nfast. Prior works have attempted to mitigate this problem by moving\ntransactions off the chain. However, such Layer-2 solutions have their own\nproblems: payment channels require a separate deposit towards each merchant and\nthus significant locked-in funds from customers; payment hubs require very\nlarge operator deposits that depend on the number of customers; and side-chains\nrequire trusted validators.\n  In this paper, we propose Snappy, a novel solution that enables recipients,\nlike merchants, to safely accept fast payments. In Snappy, all payments are on\nthe chain, while small customer collaterals and moderate merchant collaterals\nact as payment guarantees. Besides receiving payments, merchants also act as\nstatekeepers who collectively track and approve incoming payments using\nmajority voting. In case of a double-spending attack, the victim merchant can\nrecover lost funds either from the collateral of the malicious customer or a\ncolluding statekeeper (merchant). Snappy overcomes the main problems of\nprevious solutions: a single customer collateral can be used to shop with many\nmerchants; merchant collaterals are independent of the number of customers; and\nvalidators do not have to be trusted. Our Ethereum prototype shows that safe,\nfast (<2 seconds) and cheap payments are possible on existing blockchains.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 17:45:02 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Mavroudis", "Vasilios", ""], ["W\u00fcst", "Karl", ""], ["Dhar", "Aritra", ""], ["Kostiainen", "Kari", ""], ["Capkun", "Srdjan", ""]]}, {"id": "2001.01347", "submitter": "Xing Zhao", "authors": "Xing Zhao, Manos Papagelis, Aijun An, Bao Xin Chen, Junfeng Liu,\n  Yonggang Hu", "title": "Elastic Bulk Synchronous Parallel Model for Distributed Deep Learning", "comments": "The paper was accepted in the proceedings of the IEEE International\n  Conference on Data Mining 2019 (ICDM'19), 1504-1509", "journal-ref": "ICDM 2019, 1504-1509", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bulk synchronous parallel (BSP) is a celebrated synchronization model for\ngeneral-purpose parallel computing that has successfully been employed for\ndistributed training of machine learning models. A prevalent shortcoming of the\nBSP is that it requires workers to wait for the straggler at every iteration.\nTo ameliorate this shortcoming of classic BSP, we propose ELASTICBSP a model\nthat aims to relax its strict synchronization requirement. The proposed model\noffers more flexibility and adaptability during the training phase, without\nsacrificing on the accuracy of the trained model. We also propose an efficient\nmethod that materializes the model, named ZIPLINE. The algorithm is tunable and\ncan effectively balance the trade-off between quality of convergence and\niteration throughput, in order to accommodate different environments or\napplications. A thorough experimental evaluation demonstrates that our proposed\nELASTICBSP model converges faster and to a higher accuracy than the classic\nBSP. It also achieves comparable (if not higher) accuracy than the other\nsensible synchronization models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 01:05:50 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhao", "Xing", ""], ["Papagelis", "Manos", ""], ["An", "Aijun", ""], ["Chen", "Bao Xin", ""], ["Liu", "Junfeng", ""], ["Hu", "Yonggang", ""]]}, {"id": "2001.01358", "submitter": "Wei Feng", "authors": "Hongxin Wei, Wei Feng, Chi Zhang, Yunfei Chen, Yuguang Fang, and Ning\n  Ge", "title": "Creating Efficient Blockchains for the Internet of Things by Coordinated\n  Satellite-Terrestrial Networks", "comments": "8 pages, 5 figures", "journal-ref": "IEEE Wireless Communications ( Volume: 27, Issue: 3, June 2020)", "doi": "10.1109/MNET.001.1900326", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has emerged as a promising technology that can guarantee data\nconsistency and integrity among distributed participants. It has been used in\nmany applications of the Internet of Things (IoT). However, since IoT\napplications often introduce a massive number of devices into blockchain\nsystems, the efficiency of the blockchain becomes a serious problem. In this\narticle, we analyze the key factors affecting the efficiency of blockchain.\nUnlike most existing solutions that handle this from the computing perspective,\nwe consider the problem from the communication perspective. Particularly, we\npropose a coordinated satellite-terrestrial network to create efficient\nblockchains. We also derive a network scheduling strategy for the proposed\narchitecture. Simulation results demonstrate that the proposed system can\nsupport blockchains for higher efficiency. Moreover, several open research\nissues and design challenges will be discussed.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 01:54:57 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wei", "Hongxin", ""], ["Feng", "Wei", ""], ["Zhang", "Chi", ""], ["Chen", "Yunfei", ""], ["Fang", "Yuguang", ""], ["Ge", "Ning", ""]]}, {"id": "2001.01473", "submitter": "Kazuaki Matsumura", "authors": "Kazuaki Matsumura, Hamid Reza Zohouri, Mohamed Wahib, Toshio Endo,\n  Satoshi Matsuoka", "title": "AN5D: Automated Stencil Framework for High-Degree Temporal Blocking on\n  GPUs", "comments": null, "journal-ref": null, "doi": "10.1145/3368826.3377904", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil computation is one of the most widely-used compute patterns in high\nperformance computing applications. Spatial and temporal blocking have been\nproposed to overcome the memory-bound nature of this type of computation by\nmoving memory pressure from external memory to on-chip memory on GPUs. However,\ncorrectly implementing those optimizations while considering the complexity of\nthe architecture and memory hierarchy of GPUs to achieve high performance is\ndifficult. We propose AN5D, an automated stencil framework which is capable of\nautomatically transforming and optimizing stencil patterns in a given C source\ncode, and generating corresponding CUDA code. Parameter tuning in our framework\nis guided by our performance model. Our novel optimization strategy reduces\nshared memory and register pressure in comparison to existing implementations,\nallowing performance scaling up to a temporal blocking degree of 10. We achieve\nthe highest performance reported so far for all evaluated stencil benchmarks on\nthe state-of-the-art Tesla V100 GPU.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 10:35:46 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 02:27:14 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 12:44:54 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2020 16:22:52 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Matsumura", "Kazuaki", ""], ["Zohouri", "Hamid Reza", ""], ["Wahib", "Mohamed", ""], ["Endo", "Toshio", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "2001.01523", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B. Allen, Randy P.\n  Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency", "title": "Think Locally, Act Globally: Federated Learning with Local and Global\n  Representations", "comments": "NeurIPS 2019 Workshop on Federated Learning distinguished student\n  paper award. Code: https://github.com/pliang279/LG-FedAvg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a method of training models on private data distributed\nover multiple devices. To keep device data private, the global model is trained\nby only communicating parameters and updates which poses scalability challenges\nfor large models. To this end, we propose a new federated learning algorithm\nthat jointly learns compact local representations on each device and a global\nmodel across all devices. As a result, the global model can be smaller since it\nonly operates on local representations, reducing the number of communicated\nparameters. Theoretically, we provide a generalization analysis which shows\nthat a combination of local and global models reduces both variance in the data\nas well as variance across device distributions. Empirically, we demonstrate\nthat local models enable communication-efficient training while retaining\nperformance. We also evaluate on the task of personalized mood prediction from\nreal-world mobile data where privacy is key. Finally, local models handle\nheterogeneous data from new devices, and learn fair representations that\nobfuscate protected attributes such as race, age, and gender.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 12:40:21 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 07:23:45 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 08:12:35 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Liang", "Paul Pu", ""], ["Liu", "Terrance", ""], ["Ziyin", "Liu", ""], ["Allen", "Nicholas B.", ""], ["Auerbach", "Randy P.", ""], ["Brent", "David", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2001.01603", "submitter": "Jonathan Hasenburg", "authors": "Jonathan Hasenburg and David Bermbach", "title": "GeoBroker: Leveraging Geo-Contexts for IoT Data Distribution", "comments": "Accepted for publication in Elsevier Computer Communications", "journal-ref": "Computer Communications 151 (2020) 473-484", "doi": "10.1016/j.comcom.2020.01.015", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the Internet of Things, the relevance of data often depends on the\ngeographic context of data producers and consumers. Today's data distribution\nservices, however, mostly focus on data content and not on geo-context, which\ncould help to reduce the dissemination of excess data in many IoT scenarios. In\nthis paper, we propose to use the geo-context information associated with\ndevices to control data distribution. We define what geo-context dimensions\nexist and compare our definition with concepts from related work.\n  Furthermore, we designed GeoBroker, a data distribution service that uses the\nlocation of things, as well as geofences for messages and subscriptions, to\ncontrol data distribution. This way, we enable new IoT application scenarios\nwhile also increasing overall system efficiency for scenarios where\ngeo-contexts matter by delivering only relevant messages. We evaluate our\napproach based on a proof-of-concept prototype and several experiments.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 14:25:06 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 18:22:06 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 14:30:13 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Hasenburg", "Jonathan", ""], ["Bermbach", "David", ""]]}, {"id": "2001.01858", "submitter": "Alex Aizman", "authors": "Alex Aizman, Gavin Maltby, Thomas Breuel", "title": "High Performance I/O For Large Scale Deep Learning", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Training deep learning (DL) models on petascale datasets is essential for\nachieving competitive and state-of-the-art performance in applications such as\nspeech, video analytics, and object recognition. However, existing distributed\nfilesystems were not developed for the access patterns and usability\nrequirements of DL jobs. In this paper, we describe AIStore, a highly scalable,\neasy-to-deploy storage system, and WebDataset, a standards-based storage format\nand library that permits efficient access to very large datasets. We compare\nsystem performance experimentally using image classification workloads and\nstoring training data on a variety of backends, including local SSDs,\nsingle-node NFS, and two identical bare-metal clusters: HDFS and AIStore.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 02:35:42 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Aizman", "Alex", ""], ["Maltby", "Gavin", ""], ["Breuel", "Thomas", ""]]}, {"id": "2001.01861", "submitter": "Subru Krishnan", "authors": "Mohammad Hossein Namaki, Avrilia Floratou, Fotis Psallidas, Subru\n  Krishnan, Ashvin Agrawal, Yinghui Wu, Yiwen Zhu and Markus Weimer", "title": "Vamsa: Automated Provenance Tracking in Data Science Scripts", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403205", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There has recently been a lot of ongoing research in the areas of fairness,\nbias and explainability of machine learning (ML) models due to the self-evident\nor regulatory requirements of various ML applications. We make the following\nobservation: All of these approaches require a robust understanding of the\nrelationship between ML models and the data used to train them. In this work,\nwe introduce the ML provenance tracking problem: the fundamental idea is to\nautomatically track which columns in a dataset have been used to derive the\nfeatures/labels of an ML model. We discuss the challenges in capturing such\ninformation in the context of Python, the most common language used by data\nscientists. We then present Vamsa, a modular system that extracts provenance\nfrom Python scripts without requiring any changes to the users' code. Using 26K\nreal data science scripts, we verify the effectiveness of Vamsa in terms of\ncoverage, and performance. We also evaluate Vamsa's accuracy on a smaller\nsubset of manually labeled data. Our analysis shows that Vamsa's precision and\nrecall range from 90.4% to 99.1% and its latency is in the order of\nmilliseconds for average size scripts. Drawing from our experience in deploying\nML models in production, we also present an example in which Vamsa helps\nautomatically identify models that are affected by data corruption issues.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 02:39:02 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 16:58:22 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Namaki", "Mohammad Hossein", ""], ["Floratou", "Avrilia", ""], ["Psallidas", "Fotis", ""], ["Krishnan", "Subru", ""], ["Agrawal", "Ashvin", ""], ["Wu", "Yinghui", ""], ["Zhu", "Yiwen", ""], ["Weimer", "Markus", ""]]}, {"id": "2001.01865", "submitter": "Yinqiu Liu", "authors": "Yinqiu Liu, Kai Qian, Jianli Chen, Kun Wang, Lei He", "title": "Effective Scaling of Blockchain Beyond Consensus Innovations and Moore's\n  Law", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging technology, blockchain has achieved great success in numerous\napplication scenarios, from intelligent healthcare to smart cities. However, a\nlong-standing bottleneck hindering its further development is the massive\nresource consumption attributed to the distributed storage and computation\nmethods. This makes blockchain suffer from insufficient performance and poor\nscalability. Here, we analyze the recent blockchain techniques and demonstrate\nthat the potential of widely-adopted consensus-based scaling is seriously\nlimited, especially in the current era when Moore's law-based hardware scaling\nis about to end. We achieve this by developing an open-source benchmarking\ntool, called Prism, for investigating the key factors causing low resource\nefficiency and then discuss various topology and hardware innovations which\ncould help to scale up blockchain. To the best of our knowledge, this is the\nfirst in-depth study that explores the next-generation scaling strategies by\nconducting large-scale and comprehensive benchmarking.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 02:51:29 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 04:17:44 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Liu", "Yinqiu", ""], ["Qian", "Kai", ""], ["Chen", "Jianli", ""], ["Wang", "Kun", ""], ["He", "Lei", ""]]}, {"id": "2001.01999", "submitter": "Ruslan Nikolaev", "authors": "Ruslan Nikolaev, Binoy Ravindran", "title": "Universal Wait-Free Memory Reclamation", "comments": null, "journal-ref": "25th ACM SIGPLAN Annual Symposium on Principles and Practice of\n  Parallel Programming (PPoPP 2020)", "doi": "10.1145/3332466.3374540", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a universal memory reclamation scheme, Wait-Free\nEras (WFE), for deleted memory blocks in wait-free concurrent data structures.\nWFE's key innovation is that it is completely wait-free. Although some prior\ntechniques provide similar guarantees for certain data structures, they lack\nsupport for arbitrary wait-free data structures. Consequently, developers are\ntypically forced to marry their wait-free data structures with lock-free Hazard\nPointers or (potentially blocking) epoch-based memory reclamation. Since both\nthese schemes provide weaker progress guarantees, they essentially forfeit the\nstrong progress guarantee of wait-free data structures. Though making the\noriginal Hazard Pointers scheme or epoch-based reclamation completely wait-free\nseems infeasible, we achieved this goal with a more recent, (lock-free) Hazard\nEras scheme, which we extend to guarantee wait-freedom. As this extension is\nnon-trivial, we discuss all challenges pertaining to the construction of\nuniversal wait-free memory reclamation.\n  WFE is implementable on ubiquitous x86_64 and AArch64 (ARM) architectures.\nIts API is mostly compatible with Hazard Pointers, which allows easy\ntransitioning of existing data structures into WFE. Our experimental\nevaluations show that WFE's performance is close to epoch-based reclamation and\nalmost matches the original Hazard Eras scheme, while providing the stronger\nwait-free progress guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 12:23:23 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 02:54:02 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Nikolaev", "Ruslan", ""], ["Ravindran", "Binoy", ""]]}, {"id": "2001.02191", "submitter": "Michele Scquizzato", "authors": "Danupon Nanongkai and Michele Scquizzato", "title": "Equivalence Classes and Conditional Hardness in Massively Parallel\n  Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massively Parallel Computation (MPC) model serves as a common abstraction\nof many modern large-scale data processing frameworks, and has been receiving\nincreasingly more attention over the past few years, especially in the context\nof classical graph problems. So far, the only way to argue lower bounds for\nthis model is to condition on conjectures about the hardness of some specific\nproblems, such as graph connectivity on promise graphs that are either one\ncycle or two cycles, usually called the one cycle vs. two cycles problem. This\nis unlike the traditional arguments based on conjectures about complexity\nclasses (e.g., $\\textsf{P} \\neq \\textsf{NP}$), which are often more robust in\nthe sense that refuting them would lead to groundbreaking algorithms for a\nwhole bunch of problems.\n  In this paper we present connections between problems and classes of problems\nthat allow the latter type of arguments. These connections concern the class of\nproblems solvable in a sublogarithmic amount of rounds in the MPC model,\ndenoted by $\\textsf{MPC}(o(\\log N))$, and some standard classes concerning\nspace complexity, namely $\\textsf{L}$ and $\\textsf{NL}$, and suggest\nconjectures that are robust in the sense that refuting them would lead to many\nsurprisingly fast new algorithms in the MPC model. We also obtain new\nconditional lower bounds, and prove new reductions and equivalences between\nproblems in the MPC model.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 17:40:00 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Nanongkai", "Danupon", ""], ["Scquizzato", "Michele", ""]]}, {"id": "2001.02338", "submitter": "Richard Liaw", "authors": "Richard Liaw, Romil Bhardwaj, Lisa Dunlap, Yitian Zou, Joseph\n  Gonzalez, Ion Stoica, Alexey Tumanov", "title": "HyperSched: Dynamic Resource Reallocation for Model Development on a\n  Deadline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior research in resource scheduling for machine learning training workloads\nhas largely focused on minimizing job completion times. Commonly, these model\ntraining workloads collectively search over a large number of parameter values\nthat control the learning process in a hyperparameter search. It is preferable\nto identify and maximally provision the best-performing hyperparameter\nconfiguration (trial) to achieve the highest accuracy result as soon as\npossible.\n  To optimally trade-off evaluating multiple configurations and training the\nmost promising ones by a fixed deadline, we design and build HyperSched -- a\ndynamic application-level resource scheduler to track, identify, and\npreferentially allocate resources to the best performing trials to maximize\naccuracy by the deadline. HyperSched leverages three properties of a\nhyperparameter search workload over-looked in prior work - trial disposability,\nprogressively identifiable rankings among different configurations, and\nspace-time constraints - to outperform standard hyperparameter search\nalgorithms across a variety of benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 02:01:50 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Liaw", "Richard", ""], ["Bhardwaj", "Romil", ""], ["Dunlap", "Lisa", ""], ["Zou", "Yitian", ""], ["Gonzalez", "Joseph", ""], ["Stoica", "Ion", ""], ["Tumanov", "Alexey", ""]]}, {"id": "2001.02433", "submitter": "Bingqing Shen", "authors": "Bingqing Shen, Weiming Tan, Jingzhi Guo, Hongming Cai, Bin Wang, and\n  Shuaihe Zhuo", "title": "From Trend Analysis to Virtual World System Design Requirement\n  Satisfaction Study", "comments": "30 pages, 8 figures, 2 tables", "journal-ref": "Future Internet 2020, 12, 112", "doi": "10.3390/fi12070112", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual worlds have become global platforms connecting millions of people and\ncontaining various technologies. The development of technology, shift of market\nvalue, and change of user preference shape the features of virtual worlds. In\nthis paper, we first study the new features of virtual worlds and emergent\nrequirements of system development through trend analysis. Based on the trend\nanalysis, we constructed the new design requirement space. We then discuss the\nrequirement satisfaction of existing virtual world system architectures and\nhighlight their limitations through a literature survey. The comparison of\nexisting system architectures sheds some light on future virtual world system\ndevelopment to match the changing trends of the user market. At the end of this\nstudy, we briefly introduce our ongoing study, a new architecture, called\nVirtual Net, and discuss its possibility in requirement satisfaction and new\nresearch challenges.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 10:10:19 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 11:46:00 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 08:12:32 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 08:24:44 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Shen", "Bingqing", ""], ["Tan", "Weiming", ""], ["Guo", "Jingzhi", ""], ["Cai", "Hongming", ""], ["Wang", "Bin", ""], ["Zhuo", "Shuaihe", ""]]}, {"id": "2001.02498", "submitter": "Hanqing Zeng", "authors": "Hanqing Zeng, Viktor Prasanna", "title": "GraphACT: Accelerating GCN Training on CPU-FPGA Heterogeneous Platforms", "comments": "Published in ACM/SIGDA FPGA '20", "journal-ref": null, "doi": "10.1145/3373087.3375312", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep\nlearning model for representation learning on graphs. It is challenging to\naccelerate training of GCNs, due to (1) substantial and irregular data\ncommunication to propagate information within the graph, and (2) intensive\ncomputation to propagate information along the neural network layers. To\naddress these challenges, we design a novel accelerator for training GCNs on\nCPU-FPGA heterogeneous systems, by incorporating multiple\nalgorithm-architecture co-optimizations. We first analyze the computation and\ncommunication characteristics of various GCN training algorithms, and select a\nsubgraph-based algorithm that is well suited for hardware execution. To\noptimize the feature propagation within subgraphs, we propose a lightweight\npre-processing step based on a graph theoretic approach. Such pre-processing\nperformed on the CPU significantly reduces the memory access requirements and\nthe computation to be performed on the FPGA. To accelerate the weight update in\nGCN layers, we propose a systolic array based design for efficient\nparallelization. We integrate the above optimizations into a complete hardware\npipeline, and analyze its load-balance and resource utilization by accurate\nperformance modeling. We evaluate our design on a Xilinx Alveo U200 board\nhosted by a 40-core Xeon server. On three large graphs, we achieve an order of\nmagnitude training speedup with negligible accuracy loss, compared with\nstate-of-the-art implementation on a multi-core platform.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 21:19:01 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zeng", "Hanqing", ""], ["Prasanna", "Viktor", ""]]}, {"id": "2001.02504", "submitter": "Pengfei Zhang", "authors": "Pengfei Zhang, Eric Lo, Baotong Lu", "title": "High Performance Depthwise and Pointwise Convolutions on Mobile Devices", "comments": "8 pages, Thirty-Four AAAI conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lightweight convolutional neural networks (e.g., MobileNets) are specifically\ndesigned to carry out inference directly on mobile devices. Among the various\nlightweight models, depthwise convolution (DWConv) and pointwise convolution\n(PWConv) are their key operations. In this paper, we observe that the existing\nimplementations of DWConv and PWConv are not well utilizing the ARM processors\nin the mobile devices, and exhibit lots of cache misses under multi-core and\npoor data reuse at register level. We propose techniques to re-optimize the\nimplementations of DWConv and PWConv based on ARM architecture. Experimental\nresults show that our implementation can respectively achieve a speedup of up\nto 5.5x and 2.1x against TVM (Chen et al. 2018) on DWConv and PWConv.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 09:39:18 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhang", "Pengfei", ""], ["Lo", "Eric", ""], ["Lu", "Baotong", ""]]}, {"id": "2001.02505", "submitter": "Romain Rouvoy", "authors": "Guillaume Fieni, Romain Rouvoy, Lionel Seinturier", "title": "SmartWatts: Self-Calibrating Software-Defined Power Meter for Containers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained power monitoring of software activities becomes unavoidable to\nmaximize the power usage efficiency of data centers. In particular, achieving\nan optimal scheduling of containers requires the deployment of software-defined\npower~meters to go beyond the granularity of hardware power monitoring sensors,\nsuch as Power Distribution Units (PDU) or Intel's Running Average Power Limit\n(RAPL), to deliver power estimations of activities at the granularity of\nsoftware~containers. However, the definition of the underlying power models\nthat estimate the power consumption remains a long and fragile process that is\ntightly coupled to the host machine.\n  To overcome these limitations, this paper introduces SmartWatts: a\nlightweight power monitoring system that adopts online calibration to\nautomatically adjust the CPU and DRAM power models in order to maximize the\naccuracy of runtime power estimations of containers. Unlike state-of-the-art\ntechniques, SmartWatts does not require any a priori training phase or hardware\nequipment to configure the power models and can therefore be deployed on a wide\nrange of machines including the latest power optimizations, at no cost.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 16:18:08 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Fieni", "Guillaume", ""], ["Rouvoy", "Romain", ""], ["Seinturier", "Lionel", ""]]}, {"id": "2001.02514", "submitter": "Mingyu Yan", "authors": "Mingyu Yan, Lei Deng, Xing Hu, Ling Liang, Yujing Feng, Xiaochun Ye,\n  Zhimin Zhang, Dongrui Fan, Yuan Xie", "title": "HyGCN: A GCN Accelerator with Hybrid Architecture", "comments": "To Appear in 2020 IEEE International Symposium on High Performance\n  Computer Architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we first characterize the hybrid execution patterns of GCNs on\nIntel Xeon CPU. Guided by the characterization, we design a GCN accelerator,\nHyGCN, using a hybrid architecture to efficiently perform GCNs. Specifically,\nfirst, we build a new programming model to exploit the fine-grained parallelism\nfor our hardware design. Second, we propose a hardware design with two\nefficient processing engines to alleviate the irregularity of Aggregation phase\nand leverage the regularity of Combination phase. Besides, these engines can\nexploit various parallelism and reuse highly reusable data efficiently. Third,\nwe optimize the overall system via inter-engine pipeline for inter-phase fusion\nand priority-based off-chip memory access coordination to improve off-chip\nbandwidth utilization. Compared to the state-of-the-art software framework\nrunning on Intel Xeon CPU and NVIDIA V100 GPU, our work achieves on average\n1509$\\times$ speedup with 2500$\\times$ energy reduction and average 6.5$\\times$\nspeedup with 10$\\times$ energy reduction, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 03:44:46 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Yan", "Mingyu", ""], ["Deng", "Lei", ""], ["Hu", "Xing", ""], ["Liang", "Ling", ""], ["Feng", "Yujing", ""], ["Ye", "Xiaochun", ""], ["Zhang", "Zhimin", ""], ["Fan", "Dongrui", ""], ["Xie", "Yuan", ""]]}, {"id": "2001.02561", "submitter": "Fabio Lopez-Pires", "authors": "Fabio Lopez-Pires and Lino Chamorro and Benjamin Baran", "title": "A Multi-Objective Approach for Multi-Cloud Infrastructure Brokering in\n  Dynamic Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Service Brokers (CSBs) facilitate complex resource allocation\ndecisions, efficiently mapping dynamic tenant demands onto dynamic provider\noffers, where several objectives should ideally be considered. This work\nproposes for the first time a pure multi-objective formulation of a\nbroker-oriented Virtual Machine Placement (VMP) problem for dynamic\nenvironments, simultaneously optimizing the following objective functions: (i)\nTotal Infrastructure CPU (TICPU), (ii) Total Infrastructure Memory (TIMEM) and\n(iii) Total Infrastructure Price (TIP) while considering load balancing across\nproviders. To solve the formulated multi-objective problem, a Multi-Objective\nEvolutionary Algorithm (MOEA) is proposed. Considering that each time a demand\n(or offer) change occurs, a set of non-dominated solutions is found by\nPareto-based algorithms as the one proposed, different selection strategies\nwere evaluated in order to automatically select a convenient solution.\nAdditionally, the proposed algorithm, including the considered selection\nstrategies, was compared against mono-objective state-of-the-art alternatives\nin different scenarios with real data from providers in actual markets.\nExperimental results demonstrate that a pure multi-objective optimization\napproach considering the preferred solution selection strategy (S3)\noutperformed other mono-objective evaluated alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 15:14:06 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Lopez-Pires", "Fabio", ""], ["Chamorro", "Lino", ""], ["Baran", "Benjamin", ""]]}, {"id": "2001.02611", "submitter": "Newton Masinde", "authors": "Newton Masinde and Kalman Graffi", "title": "Peer-to-Peer based Social Networks: A Comprehensive Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks, such as Facebook and twitter, are a growing\nphenomenon in today's world, with various platforms providing capabilities for\nindividuals to collaborate through messaging and chatting as well as sharing of\ncontent such as videos and photos. Most, if not all, of these platforms are\nbased on centralized computing systems, meaning that the control and management\nof the systems lies in the hand of one provider, which must be trusted to treat\nthe data and communication traces securely. While users aim for privacy and\ndata sovereignty, often the providers aim to monetize the data they store.\nEven, federated privately run social networks require a few enthusiasts that\nserve the community and have, through that, access to the data they manage. As\na zero-trust alternative, peer-to-peer (P2P) technologies promise networks that\nare self organizing and secure-by-design, in which the final data sovereignty\nlies at the corresponding user. Such networks support end-to-end communication,\nuncompromising access control, anonymity and resilience against censorship and\nmassive data leaks through misused trust. The goals of this survey are\nthree-fold. Firstly, the survey elaborates the properties of P2P-based online\nsocial networks and defines the requirements for such (zero-trust) platforms.\nSecondly, it elaborates on the building blocks for P2P frameworks that allow\nthe creation of such sophisticated and demanding applications, such as\nuser/identity management, reliable data storage, secure communication, access\ncontrol and general-purpose extensibility, features that are not addressed in\nother P2P surveys. As a third point, it gives an overview of proposed P2P-based\nonline social network applications, frameworks and architectures. In specific,\nit explores the technical details, inter-dependencies and maturity of the\navailable solutions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:47:08 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 20:43:02 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Masinde", "Newton", ""], ["Graffi", "Kalman", ""]]}, {"id": "2001.02670", "submitter": "Giuseppe Antonio Di Luna", "authors": "Giuseppe Antonio Di Luna, Emmanuelle Anceaume, Silvia Bonomi, Leonardo\n  Querzoni", "title": "Synchronous Byzantine Lattice Agreement in ${\\cal O}(\\log (f))$ Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Lattice Agreement (LA) problem, originally proposed by Attiya et al.\n\\cite{Attiya:1995}, a set of processes has to decide on a chain of a lattice.\nMore precisely, each correct process proposes an element $e$ of a certain\njoin-semi lattice $L$ and it has to decide on a value that contains $e$.\nMoreover, any pair $p_i,p_j$ of correct processes has to decide two values\n$dec_i$ and $dec_j$ that are comparable (e.g., $dec_i \\leq dec_j$ or $dec_j <\ndec_i$). LA has been studied for its practical applications, as example it can\nbe used to implement a snapshot objects \\cite{Attiya:1995} or a replicated\nstate machine with commutative operations \\cite{Faleiro:2012}. Interestingly,\nthe study of the Byzantine Lattice Agreement started only recently, and it has\nbeen mainly devoted to asynchronous systems. The synchronous case has been\nobject of a recent pre-print \\cite{Zheng:aa} where Zheng et al. propose an\nalgorithm terminating in ${\\cal O}(\\sqrt f)$ rounds and tolerating $f < \\lceil\nn/3 \\rceil$ Byzantine processes.\n  In this paper we present new contributions for the synchronous case. We\ninvestigate the problem in the usual message passing model for a system of $n$\nprocesses with distinct unique IDs. We first prove that, when only\nauthenticated channels are available, the problem cannot be solved if $f=n/3$\nor more processes are Byzantine. We then propose a novel algorithm that works\nin a synchronous system model with signatures (i.e., the {\\em authenticated\nmessage} model), tolerates up to $f$ byzantine failures (where $f<n/3$) and\nthat terminates in ${\\cal O}(\\log f)$ rounds. We discuss how to remove\nauthenticated messages at the price of algorithm resiliency ($f < n/4$).\nFinally, we present a transformer that converts any synchronous LA algorithm to\nan algorithm for synchronous Generalised Lattice Agreement.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 18:56:00 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 18:27:17 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Di Luna", "Giuseppe Antonio", ""], ["Anceaume", "Emmanuelle", ""], ["Bonomi", "Silvia", ""], ["Querzoni", "Leonardo", ""]]}, {"id": "2001.02772", "submitter": "Udit Gupta", "authors": "Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen,\n  Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, Carole-Jean Wu", "title": "DeepRecSys: A System for Optimizing End-To-End At-scale Neural\n  Recommendation Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural personalized recommendation is the corner-stone of a wide collection\nof cloud services and products, constituting significant compute demand of the\ncloud infrastructure. Thus, improving the execution efficiency of neural\nrecommendation directly translates into infrastructure capacity saving. In this\npaper, we devise a novel end-to-end modeling infrastructure, DeepRecInfra, that\nadopts an algorithm and system co-design methodology to custom-design systems\nfor recommendation use cases. Leveraging the insights from the recommendation\ncharacterization, a new dynamic scheduler, DeepRecSched, is proposed to\nmaximize latency-bounded throughput by taking into account characteristics of\ninference query size and arrival patterns, recommendation model architectures,\nand underlying hardware systems. By doing so, system throughput is doubled\nacross the eight industry-representative recommendation models. Finally,\ndesign, deployment, and evaluation in at-scale production datacenter shows over\n30% latency reduction across a wide variety of recommendation models running on\nhundreds of machines.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 22:25:10 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Gupta", "Udit", ""], ["Hsia", "Samuel", ""], ["Saraph", "Vikram", ""], ["Wang", "Xiaodong", ""], ["Reagen", "Brandon", ""], ["Wei", "Gu-Yeon", ""], ["Lee", "Hsien-Hsin S.", ""], ["Brooks", "David", ""], ["Wu", "Carole-Jean", ""]]}, {"id": "2001.02925", "submitter": "Geeta Yadav", "authors": "Geeta Yadav, Kolin Paul", "title": "Architecture and Security of SCADA Systems: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipeline bursting, production lines shut down, frenzy traffic, trains\nconfrontation, nuclear reactor shut down, disrupted electric supply,\ninterrupted oxygen supply in ICU - these catastrophic events could result\nbecause of an erroneous SCADA system/ Industrial Control System(ICS). SCADA\nsystems have become an essential part of automated control and monitoring of\nmany of the Critical Infrastructures (CI). Modern SCADA systems have evolved\nfrom standalone systems into sophisticated complex, open systems, connected to\nthe Internet. This geographically distributed modern SCADA system is vulnerable\nto threats and cyber attacks. In this paper, we first review the SCADA system\narchitectures that have been proposed/implemented followed by attacks on such\nsystems to understand and highlight the evolving security needs for SCADA\nsystems. A short investigation of the current state of intrusion detection\ntechniques in SCADA systems is done , followed by a brief study of testbeds for\nSCADA systems. The cloud and Internet of things (IoT) based SCADA systems are\nstudied by analysing the architecture of modern SCADA systems. This review\npaper ends by highlighting the critical research problems that need to be\nresolved to close the gaps in the security of SCADA systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 11:30:50 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Yadav", "Geeta", ""], ["Paul", "Kolin", ""]]}, {"id": "2001.02962", "submitter": "Newton Masinde", "authors": "Kalman Graffi and Newton Masinde", "title": "LibreSocial: A Peer-to-Peer Framework for Online Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed online social networks (DOSNs) were first proposed to solve the\nproblem of privacy, security and scalability. A significant amount of research\nwas undertaken to offer viable DOSN solutions that were capable of competing\nwith the existing centralized OSN applications such as Facebook, LinkedIn and\nInstagram. This research led to the emergence of the use of peer-to-peer (P2P)\nnetworks as a possible solution, upon which several OSNs such as\nLifeSocial.KOM, Safebook, PeerSoN among others were based. In this paper, we\ndefine the basic requirements for a P2P OSN. We then revisit one of the first\nP2P-based OSNs, LifeSocial.KOM, that is now called LibreSocial, which evolved\nin the past years to address the challenges of running a completely\ndecentralized social network. Over the course of time, several essential new\ntechnologies have been incorporated within LibreSocial for better\nfunctionalities. We describe the architecture and each individual component of\nLibreSocial and point out how LibreSocial meets the basic requirements for a\nfully functional distributed OSN.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 13:15:37 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 20:38:03 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Graffi", "Kalman", ""], ["Masinde", "Newton", ""]]}, {"id": "2001.03147", "submitter": "Carlos Baquero", "authors": "Ariel Shtul and Carlos Baquero and Paulo S\\'ergio Almeida", "title": "Age-Partitioned Bloom Filters", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom filters (BF) are widely used for approximate membership queries over a\nset of elements. BF variants allow removals, sets of unbounded size or querying\na sliding window over an unbounded stream. However, for this last case the best\ncurrent approaches are dictionary based (e.g., based on Cuckoo Filters or\nTinyTable), and it may seem that BF-based approaches will never be competitive\nto dictionary-based ones. In this paper we present Age-Partitioned Bloom\nFilters, a BF-based approach for duplicate detection in sliding windows that\nnot only is competitive in time-complexity, but has better space usage than\ncurrent dictionary-based approaches (e.g., SWAMP), at the cost of some moderate\nslack. APBFs retain the BF simplicity, unlike dictionary-based approaches,\nimportant for hardware-based implementations, and can integrate known\nimprovements such as double hashing or blocking. We present an Age-Partitioned\nBlocked Bloom Filter variant which can operate with 2-3 cache-line accesses per\ninsertion and around 2-4 per query, even for high accuracy filters.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 18:23:11 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Shtul", "Ariel", ""], ["Baquero", "Carlos", ""], ["Almeida", "Paulo S\u00e9rgio", ""]]}, {"id": "2001.03166", "submitter": "Pranay Sharma", "authors": "Pranay Sharma, Prashant Khanduri, Lixin Shen, Donald J. Bucci Jr.,\n  Pramod K. Varshney", "title": "On Distributed Online Convex Optimization with Sublinear Dynamic Regret\n  and Fit", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider a distributed online convex optimization problem,\nwith time-varying (potentially adversarial) constraints. A set of nodes,\njointly aim to minimize a global objective function, which is the sum of local\nconvex functions. The objective and constraint functions are revealed locally\nto the nodes, at each time, after taking an action. Naturally, the constraints\ncannot be instantaneously satisfied. Therefore, we reformulate the problem to\nsatisfy these constraints in the long term. To this end, we propose a\ndistributed primal-dual mirror descent based approach, in which the primal and\ndual updates are carried out locally at all the nodes. This is followed by\nsharing and mixing of the primal variables by the local nodes via communication\nwith the immediate neighbors. To quantify the performance of the proposed\nalgorithm, we utilize the challenging, but more realistic metrics of dynamic\nregret and fit. Dynamic regret measures the cumulative loss incurred by the\nalgorithm, compared to the best dynamic strategy. On the other hand, fit\nmeasures the long term cumulative constraint violations. Without assuming the\nrestrictive Slater's conditions, we show that the proposed algorithm achieves\nsublinear regret and fit under mild, commonly used assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 18:53:58 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 17:49:28 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Sharma", "Pranay", ""], ["Khanduri", "Prashant", ""], ["Shen", "Lixin", ""], ["Bucci", "Donald J.", "Jr."], ["Varshney", "Pramod K.", ""]]}, {"id": "2001.03244", "submitter": "Elad Michael Schiller (PhD)", "authors": "Oskar Lundstr\\\"om and Michel Raynal and Elad M. Schiller", "title": "Self-stabilizing Uniform Reliable Broadcast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a well-known communication abstraction called Uniform Reliable\nBroadcast (URB). URB is central in the design and implementation of\nfault-tolerant distributed systems, as many non-trivial fault-tolerant\ndistributed applications require communication with provable guarantees on\nmessage deliveries. Our study focuses on fault-tolerant implementations for\ntime-free message-passing systems that are prone to node-failures. Moreover, we\naim at the design of an even more robust communication abstraction. We do so\nthrough the lenses of self-stabilization---a very strong notion of\nfault-tolerance. In addition to node and communication failures,\nself-stabilizing algorithms can recover after the occurrence of arbitrary\ntransient faults; these faults represent any violation of the assumptions\naccording to which the system was designed to operate (as long as the algorithm\ncode stays intact).\n  This work proposes the first self-stabilizing URB solution for time-free\nmessage-passing systems that are prone to node-failures. The proposed algorithm\nhas an O(bufferUnitSize) stabilization time (in terms of asynchronous cycles)\nfrom arbitrary transient faults, where bufferUnitSize is a predefined constant\nthat can be set according to the available memory. Moreover, the communication\ncosts of our algorithm are similar to the ones of the non-self-stabilizing\nstate-of-the-art. The main differences are that our proposal considers repeated\ngossiping of O(1) bits messages and deals with bounded space (which is a\nprerequisite for self-stabilization). Specifically, each node needs to store up\nto bufferUnitSize n records and each record is of size O(v + n log n) bits,\nwhere n is the number of nodes in the system and v is the number of bits needed\nto encode a single URB instance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 22:11:21 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Lundstr\u00f6m", "Oskar", ""], ["Raynal", "Michel", ""], ["Schiller", "Elad M.", ""]]}, {"id": "2001.03362", "submitter": "Jan Skrzypczak", "authors": "Jan Skrzypczak, Florian Schintke, Thorsten Sch\\\"utt", "title": "RMWPaxos: Fault-Tolerant In-Place Consensus Sequences", "comments": null, "journal-ref": null, "doi": "10.1109/TPDS.2020.2981891", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building consensus sequences based on distributed, fault-tolerant consensus,\nas used for replicated state machines, typically requires a separate\ndistributed state for every new consensus instance. Allocating and maintaining\nthis state causes significant overhead. In particular, freeing the distributed,\noutdated states in a fault-tolerant way is not trivial and adds further\ncomplexity and cost to the system.\n  In this paper, we propose an extension to the single-decree Paxos protocol\nthat can learn a sequence of consensus decisions 'in-place', i.e. with a single\nset of distributed states. Our protocol does not require dynamic log structures\nand hence has no need for distributed log pruning, snapshotting, compaction, or\ndynamic resource allocation.\n  The protocol builds a fault-tolerant atomic register that supports arbitrary\nread-modify-write operations. We use the concept of consistent quorums to\ndetect whether the previous consensus still needs to be consolidated or is\nalready finished so that the next consensus value can be safely proposed.\nReading a consolidated consensus is done without state modifications and is\nthereby free of concurrency control and demand for serialisation. A proposer\nthat is not interrupted reaches agreement on consecutive consensus decisions\nwithin a single message round-trip per decision by preparing the acceptors\neagerly with the previous request.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 09:31:50 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 13:45:45 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Skrzypczak", "Jan", ""], ["Schintke", "Florian", ""], ["Sch\u00fctt", "Thorsten", ""]]}, {"id": "2001.03365", "submitter": "Junhui Kim", "authors": "Junhui Kim and Joongheon Kim", "title": "Demo: Light-Weight Programming Language for Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This demo abstract introduces a new light-weight programming language koa\nwhich is suitable for blockchain system design and implementation. In this\nabstract, the basic features of koa are introduced including working system\n(with playground), architecture, and virtual machine operations. Rum-time\nexecution of software implemented by koa will be presented during the session.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 09:39:21 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Kim", "Junhui", ""], ["Kim", "Joongheon", ""]]}, {"id": "2001.03384", "submitter": "Evangelos Pournaras", "authors": "Brionna Davis, Grace Jennings, Taylor Pothast, Ilias Gerostathopoulos,\n  Evangelos Pournaras, Raphael E. Stern", "title": "Decentralized Optimization of Vehicle Route Planning -- A Cross-City\n  Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.DC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New mobility concepts are at the forefront of research and innovation in\nsmart cities. The introduction of connected and autonomous vehicles enables new\npossibilities in vehicle routing. Specifically, knowing the origin and\ndestination of each agent in the network can allow for real-time routing of the\nvehicles to optimize network performance. However, this relies on individual\nvehicles being \"altruistic\" i.e., being willing to accept an alternative\nnon-preferred route in order to achieve a network-level performance goal. In\nthis work, we conduct a study to compare different levels of agent altruism and\nthe resulting effect on the network-level traffic performance. Specifically,\nthis study compares the effects of different underlying urban structures on the\noverall network performance, and investigates which characteristics of the\nnetwork make it possible to realize routing improvements using a decentralized\noptimization router. The main finding is that, with increased vehicle altruism,\nit is possible to balance traffic flow among the links of the network. We show\nevidence that the decentralized optimization router is more effective with\nnetworks of high load while we study the influence of cities characteristics,\nin particular: networks with a higher number of nodes (intersections) or edges\n(roads) per unit area allow for more possible alternate routes, and thus higher\npotential to improve network performance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 11:02:51 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Davis", "Brionna", ""], ["Jennings", "Grace", ""], ["Pothast", "Taylor", ""], ["Gerostathopoulos", "Ilias", ""], ["Pournaras", "Evangelos", ""], ["Stern", "Raphael E.", ""]]}, {"id": "2001.03389", "submitter": "Dany Vohl", "authors": "Alessio Sclocco, Dany Vohl and Rob V. van Nieuwpoort", "title": "Real-Time RFI Mitigation for the Apertif Radio Transient System", "comments": "6 pages, 10 figures. To appear in Proceedings from the 2019 Radio\n  Frequency Interference workshop (RFI 2019), Toulouse, France (23-26\n  September)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current and upcoming radio telescopes are being designed with increasing\nsensitivity to detect new and mysterious radio sources of astrophysical origin.\nWhile this increased sensitivity improves the likelihood of discoveries, it\nalso makes these instruments more susceptible to the deleterious effects of\nRadio Frequency Interference (RFI). The challenge posed by RFI is exacerbated\nby the high data-rates achieved by modern radio telescopes, which require\nreal-time processing to keep up with the data. Furthermore, the high data-rates\ndo not allow for permanent storage of observations at high resolution. Offline\nRFI mitigation is therefore not possible anymore. The real-time requirement\nmakes RFI mitigation even more challenging because, on one side, the techniques\nused for mitigation need to be fast and simple, and on the other side they also\nneed to be robust enough to cope with just a partial view of the data.\n  The Apertif Radio Transient System (ARTS) is the real-time, time-domain,\ntransient detection instrument of the Westerbork Synthesis Radio Telescope\n(WSRT), processing 73 Gb of data per second. Even with a deep learning\nclassifier, the ARTS pipeline requires state-of-the-art real-time RFI\nmitigation to reduce the number of false-positive detections. Our solution to\nthis challenge is RFIm, a high-performance, open-source, tuned, and extensible\nRFI mitigation library. The goal of this library is to provide users with RFI\nmitigation routines that are designed to run in real-time on many-core\naccelerators, such as Graphics Processing Units, and that can be highly-tuned\nto achieve code and performance portability to different hardware platforms and\nscientific use-cases. Results on the ARTS show that we can achieve real-time\nRFI mitigation, with a minimal impact on the total execution time of the search\npipeline, and considerably reduce the number of false-positives.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 11:25:07 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 10:01:15 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Sclocco", "Alessio", ""], ["Vohl", "Dany", ""], ["van Nieuwpoort", "Rob V.", ""]]}, {"id": "2001.03457", "submitter": "Nikolaos Kallimanis", "authors": "Panagiota Fatourou, Nikolaos D. Kallimanis and Eleni Kanellou", "title": "An Efficient Universal Construction for Large Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents L-UC, a universal construction that efficiently\nimplements dynamic objects of large state in a wait-free manner. The step\ncomplexity of L-UC is O(n+kw), where n is the number of processes, k is the\ninterval contention (i.e., the maximum number of active processes during the\nexecution interval of an operation), and w is the worst-case time complexity to\nperform an operation on the sequential implementation of the simulated object.\nL-UC efficiently implements objects whose size can change dynamically. It\nimproves upon previous universal constructions either by efficiently handling\nobjects whose state is large and can change dynamically, or by achieving better\nstep complexity.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 14:10:10 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Fatourou", "Panagiota", ""], ["Kallimanis", "Nikolaos D.", ""], ["Kanellou", "Eleni", ""]]}, {"id": "2001.03535", "submitter": "Pengfei Xu", "authors": "Pengfei Xu, Xiaofan Zhang, Cong Hao, Yang Zhao, Yongan Zhang, Yue\n  Wang, Chaojian Li, Zetong Guan, Deming Chen, Yingyan Lin", "title": "AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs\n  and ASICs", "comments": "Accepted by 28th ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays (FPGA'2020)", "journal-ref": null, "doi": "10.1145/3373087.3375306", "report-no": null, "categories": "cs.DC cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a growing\ndemand for DNN chips. However, designing DNN chips is non-trivial because: (1)\nmainstream DNNs have millions of parameters and operations; (2) the large\ndesign space due to the numerous design choices of dataflows, processing\nelements, memory hierarchy, etc.; and (3) an algorithm/hardware co-design is\nneeded to allow the same DNN functionality to have a different decomposition,\nwhich would require different hardware IPs to meet the application\nspecifications. Therefore, DNN chips take a long time to design and require\ncross-disciplinary experts. To enable fast and effective DNN chip design, we\npropose AutoDNNchip - a DNN chip generator that can automatically generate both\nFPGA- and ASIC-based DNN chip implementation given DNNs from machine learning\nframeworks (e.g., PyTorch) for a designated application and dataset.\nSpecifically, AutoDNNchip consists of two integrated enablers: (1) a Chip\nPredictor, built on top of a graph-based accelerator representation, which can\naccurately and efficiently predict a DNN accelerator's energy, throughput, and\narea based on the DNN model parameters, hardware configuration,\ntechnology-based IPs, and platform constraints; and (2) a Chip Builder, which\ncan automatically explore the design space of DNN chips (including IP\nselection, block configuration, resource balancing, etc.), optimize chip design\nvia the Chip Predictor, and then generate optimized synthesizable RTL to\nachieve the target design metrics. Experimental results show that our Chip\nPredictor's predicted performance differs from real-measured ones by < 10% when\nvalidated using 15 DNN models and 4 platforms (edge-FPGA/TPU/GPU and ASIC).\nFurthermore, accelerators generated by our AutoDNNchip can achieve better (up\nto 3.86X improvement) performance than that of expert-crafted state-of-the-art\naccelerators.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 05:32:15 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 17:02:51 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2020 18:53:08 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2020 23:50:57 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Xu", "Pengfei", ""], ["Zhang", "Xiaofan", ""], ["Hao", "Cong", ""], ["Zhao", "Yang", ""], ["Zhang", "Yongan", ""], ["Wang", "Yue", ""], ["Li", "Chaojian", ""], ["Guan", "Zetong", ""], ["Chen", "Deming", ""], ["Lin", "Yingyan", ""]]}, {"id": "2001.03537", "submitter": "Chenhao Xie", "authors": "Chenhao Xie, Xin Fu, Mingsong Chen, Shuaiwen Leon Song", "title": "OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future\n  NUMA-Based Multi-GPU Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3307650.3322247", "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the strong computation capability, NUMA-based multi-GPU system is a\npromising candidate to provide sustainable and scalable performance for Virtual\nReality. However, the entire multi-GPU system is viewed as a single GPU which\nignores the data locality in VR rendering during the workload distribution,\nleading to tremendous remote memory accesses among GPU models. By conducting\ncomprehensive characterizations on different kinds of parallel rendering\nframeworks, we observe that distributing the rendering object along with its\nrequired data per GPM can reduce the inter-GPM memory accesses. However, this\nobject-level rendering still faces two major challenges in NUMA-based multi-GPU\nsystem: (1) the large data locality between the left and right views of the\nsame object and the data sharing among different objects and (2) the unbalanced\nworkloads induced by the software-level distribution and composition\nmechanisms. To tackle these challenges, we propose object-oriented VR rendering\nframework (OO-VR) that conducts the software and hardware co-optimization to\nprovide a NUMA friendly solution for VR multi-view rendering in NUMA-based\nmulti-GPU systems. We first propose an object-oriented VR programming model to\nexploit the data sharing between two views of the same object and group objects\ninto batches based on their texture sharing levels. Then, we design an object\naware runtime batch distribution engine and distributed hardware composition\nunit to achieve the balanced workloads among GPMs. Finally, evaluations on our\nVR featured simulator show that OO-VR provides 1.58x overall performance\nimprovement and 76% inter-GPM memory traffic reduction over the\nstate-of-the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly\nperformance scalability for the future larger multi-GPU scenarios with ever\nincreasing asymmetric bandwidth between local and remote memory.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 19:44:51 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Xie", "Chenhao", ""], ["Fu", "Xin", ""], ["Chen", "Mingsong", ""], ["Song", "Shuaiwen Leon", ""]]}, {"id": "2001.03717", "submitter": "Dushyant Behl", "authors": "Prabal Banerjee, Dushyant Behl, Palanivel Kodeswaran, Chaitanya Kumar,\n  Sushmita Ruj and Sayandeep Sen", "title": "Verifiable and Auditable Digital Interchange Framework", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of fairness and transparency in online marketplaces\nselling digital content, where all parties are not actively participating in\nthe trade. We present the design, implementation and evaluation of VADER, a\nhighly scalable solution for multi-party fair digital exchange that combines\nthe trusted execution of blockchains with intelligent protocol design and\nincentivization schemes. We prototype VADER on Hyperledger Fabric and\nextensively evaluate our system on a realistic testbed spanning five public\ncloud datacenters, spread across four continents. Our results demonstrate that\nVADER adds only minimal overhead of 16% in median case compared to a baseline\nsolution, while significantly outperforming a naive blockchain based solution\nthat adds an overhead of 764%.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 06:36:23 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 06:44:25 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Banerjee", "Prabal", ""], ["Behl", "Dushyant", ""], ["Kodeswaran", "Palanivel", ""], ["Kumar", "Chaitanya", ""], ["Ruj", "Sushmita", ""], ["Sen", "Sayandeep", ""]]}, {"id": "2001.03822", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao", "title": "Permissioned Blockchain Revisited: A Byzantine Game-Theoretical\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the popularity and practical applicability of blockchains, there is\nvery limited work on the theoretical foundation of blockchains: The lack of\nrigorous theory and analysis behind the curtain of blockchains has severely\nstaggered its broader applications. This paper attempts to lay out a\ntheoretical foundation for a specific type of blockchains---the ones requiring\nbasic authenticity from the participants, also called \\textit{permissioned\nblockchain}. We formulate permissioned blockchain systems and operations into a\ngame-theoretical problem by incorporating constraints implied by the wisdom\nfrom distributed computing and Byzantine systems. We show that in a\nnoncooperative blockchain game (NBG), a Nash equilibrium can be efficiently\nfound in a closed-form even though the game involves more than two players.\nSomewhat surprisingly, the simulation results of the Nash equilibrium implies\nthat the game can reach a stable status regardless of the number of Byzantine\nnodes and trustworthy players. We then study a harder problem where players are\nallowed to form coalitions: the coalitional blockchain game (CBG). We show that\nalthough the Shapley value for a CBG can be expressed in a more succinct form,\nits core is empty.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 01:53:18 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Zhao", "Dongfang", ""]]}, {"id": "2001.03836", "submitter": "Xin Zhang", "authors": "Xin Zhang, Minghong Fang, Jia Liu, and Zhengyuan Zhu", "title": "Private and Communication-Efficient Edge Learning: A Sparse Differential\n  Gaussian-Masking Distributed SGD Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rise of machine learning (ML) and the proliferation of smart mobile\ndevices, recent years have witnessed a surge of interest in performing ML in\nwireless edge networks. In this paper, we consider the problem of jointly\nimproving data privacy and communication efficiency of distributed edge\nlearning, both of which are critical performance metrics in wireless edge\nnetwork computing. Toward this end, we propose a new decentralized stochastic\ngradient method with sparse differential Gaussian-masked stochastic gradients\n(SDM-DSGD) for non-convex distributed edge learning. Our main contributions are\nthree-fold: i) We theoretically establish the privacy and communication\nefficiency performance guarantee of our SDM-DSGD method, which outperforms all\nexisting works; ii) We show that SDM-DSGD improves the fundamental\ntraining-privacy trade-off by {\\em two orders of magnitude} compared with the\nstate-of-the-art. iii) We reveal theoretical insights and offer practical\ndesign guidelines for the interactions between privacy preservation and\ncommunication efficiency, two conflicting performance goals. We conduct\nextensive experiments with a variety of learning models on MNIST and CIFAR-10\ndatasets to verify our theoretical findings. Collectively, our results\ncontribute to the theory and algorithm design for distributed edge learning.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 03:04:45 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 01:49:58 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2020 16:14:42 GMT"}, {"version": "v4", "created": "Sat, 28 Mar 2020 15:20:20 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zhang", "Xin", ""], ["Fang", "Minghong", ""], ["Liu", "Jia", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "2001.03929", "submitter": "Xiangqiang Gao", "authors": "Xiangqiang Gao, Rongke Liu (Senior Member, IEEE), and Aryan Kaushik", "title": "Hierarchical Multi-Agent Optimization for Resource Allocation in Cloud\n  Computing", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems Volume: 32,\n  Issue: 3, March 1 2021", "doi": "10.1109/TPDS.2020.3030920", "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud computing, an important concern is to allocate the available\nresources of service nodes to the requested tasks on demand and to make the\nobjective function optimum, i.e., maximizing resource utilization, payoffs and\navailable bandwidth. This paper proposes a hierarchical multi-agent\noptimization (HMAO) algorithm in order to maximize the resource utilization and\nmake the bandwidth cost minimum for cloud computing. The proposed HMAO\nalgorithm is a combination of the genetic algorithm (GA) and the multi-agent\noptimization (MAO) algorithm. With maximizing the resource utilization, an\nimproved GA is implemented to find a set of service nodes that are used to\ndeploy the requested tasks. A decentralized-based MAO algorithm is presented to\nminimize the bandwidth cost. We study the effect of key parameters of the HMAO\nalgorithm by the Taguchi method and evaluate the performance results. When\ncompared with genetic algorithm (GA) and fast elitist non-dominated sorting\ngenetic (NSGA-II) algorithm, the simulation results demonstrate that the HMAO\nalgorithm is more effective than the existing solutions to solve the problem of\nresource allocation with a large number of the requested tasks. Furthermore, we\nprovide the performance comparison of the HMAO algorithm with the first-fit\ngreedy approach in on-line resource allocation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 13:30:21 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Gao", "Xiangqiang", "", "Senior Member, IEEE"], ["Liu", "Rongke", "", "Senior Member, IEEE"], ["Kaushik", "Aryan", ""]]}, {"id": "2001.03936", "submitter": "Haimin Chen", "authors": "Haimin Chen, Chaodong Zheng", "title": "Broadcasting Competitively against Adaptive Adversary in Multi-channel\n  Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadcasting in wireless networks is vulnerable to adversarial jamming. To\nthwart such behavior, \\emph{resource competitive analysis} is proposed. In this\nframework, sending, listening, or jamming on one channel for one time slot\ncosts one unit of energy. The adversary can employ arbitrary strategy to\ndisrupt communication, but has a limited energy budget $T$. The honest nodes,\non the other hand, aim to accomplish broadcast while spending only $o(T)$.\nPrevious work has shown, in a $C$-channels network containing $n$ nodes, for\nlarge $T$ values, each node can receive the message in $\\tilde{O}(T/C)$ time,\nwhile spending only $\\tilde{O}(\\sqrt{T/n})$ energy. However, these\nmulti-channel algorithms only work for certain values of $n$ and $C$, and can\nonly tolerate an oblivious adversary.\n  In this work, we provide new upper and lower bounds for broadcasting in\nmulti-channel radio networks, from the perspective of resource competitiveness.\nOur algorithms work for arbitrary $n,C$ values, require minimal prior\nknowledge, and can tolerate a powerful adaptive adversary. More specifically,\nin our algorithms, for large $T$ values, each node's runtime is $O(T/C)$, and\neach node's energy cost is $\\tilde{O}(\\sqrt{T/n})$. We also complement\nalgorithmic results with lower bounds, proving both the time complexity and the\nenergy complexity of our algorithms are optimal or near-optimal (within a\npoly-log factor). Our technical contributions lie in using \"epidemic broadcast\"\nto achieve time efficiency and resource competitiveness, and employing coupling\ntechniques in the analysis to handle the adaptivity of the adversary. At the\nlower bound side, we first derive a new energy complexity lower bound for\n1-to-1 communication in the multi-channel setting, and then apply simulation\nand reduction arguments to obtain the desired result.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 14:10:05 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 14:47:04 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 08:55:47 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chen", "Haimin", ""], ["Zheng", "Chaodong", ""]]}, {"id": "2001.04005", "submitter": "Nicholas Woolsey", "authors": "Nicholas Woolsey, Rong-Rong Chen, Mingyue Ji", "title": "Heterogeneous Computation Assignments in Coded Elastic Computing", "comments": "Submitted to ISIT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal design of a heterogeneous coded elastic computing (CEC)\nnetwork where machines have varying relative computation speeds. CEC introduced\nby Yang {\\it et al.} is a framework which mitigates the impact of elastic\nevents, where machines join and leave the network. A set of data is distributed\namong storage constrained machines using a Maximum Distance Separable (MDS)\ncode such that any subset of machines of a specific size can perform the\ndesired computations. This design eliminates the need to re-distribute the data\nafter each elastic event. In this work, we develop a process for an arbitrary\nheterogeneous computing network to minimize the overall computation time by\ndefining an optimal computation load, or number of computations assigned to\neach machine. We then present an algorithm to define a specific computation\nassignment among the machines that makes use of the MDS code and meets the\noptimal computation load.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 22:27:03 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Woolsey", "Nicholas", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "2001.04175", "submitter": "Martin Thomas Horsch", "authors": "Martin Thomas Horsch, Silvia Chiacchiera, Youness Bami, Georg J.\n  Schmitz, Gabriele Mogni, Gerhard Goldbeck, and Emanuele Ghedini", "title": "Reliable and interoperable computational molecular engineering: 2.\n  Semantic interoperability based on the European Materials and Modelling\n  Ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European Materials and Modelling Ontology (EMMO) is a top-level ontology\ndesigned by the European Materials Modelling Council to facilitate semantic\ninteroperability between platforms, models, and tools in computational\nmolecular engineering, integrated computational materials engineering, and\nrelated applications of materials modelling and characterization. Additionally,\ndomain ontologies exist based on data technology developments from specific\nplatforms. The present work discusses the ongoing work on establishing a\nEuropean Virtual Marketplace Framework, into which diverse platforms can be\nintegrated. It addresses common challenges that arise when marketplace-level\ndomain ontologies are combined with a top-level ontology like the EMMO by\nontology alignment.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:02:37 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Horsch", "Martin Thomas", ""], ["Chiacchiera", "Silvia", ""], ["Bami", "Youness", ""], ["Schmitz", "Georg J.", ""], ["Mogni", "Gabriele", ""], ["Goldbeck", "Gerhard", ""], ["Ghedini", "Emanuele", ""]]}, {"id": "2001.04206", "submitter": "Athanasios Stratikopoulos", "authors": "Athanasios Stratikopoulos, Juan Fumero, Zoran Sevarac and Christos\n  Kotselidis", "title": "Towards High Performance Java-based Deep Learning Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of modern cloud services along with the huge volume of data\nproduced on a daily basis, have set the demand for fast and efficient data\nprocessing. This demand is common among numerous application domains, such as\ndeep learning, data mining, and computer vision. Prior research has focused on\nemploying hardware accelerators as a means to overcome this inefficiency. This\ntrend has driven software development to target heterogeneous execution, and\nseveral modern computing systems have incorporated a mixture of diverse\ncomputing components, including GPUs and FPGAs. However, the specialization of\nthe applications' code for heterogeneous execution is not a trivial task, as it\nrequires developers to have hardware expertise in order to obtain high\nperformance. The vast majority of the existing deep learning frameworks that\nsupport heterogeneous acceleration, rely on the implementation of wrapper calls\nfrom a high-level programming language to a low-level accelerator backend, such\nas OpenCL, CUDA or HLS.\n  In this paper we have employed TornadoVM, a state-of-the-art heterogeneous\nprogramming framework to transparently accelerate Deep Netts; a Java-based deep\nlearning framework. Our initial results demonstrate up to 8x performance\nspeedup when executing the back propagation process of the network's training\non AMD GPUs against the sequential execution of the original Deep Netts\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:03:13 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Stratikopoulos", "Athanasios", ""], ["Fumero", "Juan", ""], ["Sevarac", "Zoran", ""], ["Kotselidis", "Christos", ""]]}, {"id": "2001.04229", "submitter": "Faheem Zafari", "authors": "Faheem Zafari, Prithwish Basu, Kin K. Leung, Jian Li, Ananthram Swami\n  and Don Towsley", "title": "Resource Sharing in the Edge: A Distributed Bargaining-Theoretic\n  Approach", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing demand for edge computing resources, particularly due to\nincreasing popularity of Internet of Things (IoT), and distributed machine/deep\nlearning applications poses a significant challenge. On the one hand, certain\nedge service providers (ESPs) may not have sufficient resources to satisfy\ntheir applications according to the associated service-level agreements. On the\nother hand, some ESPs may have additional unused resources. In this paper, we\npropose a resource-sharing framework that allows different ESPs to optimally\nutilize their resources and improve the satisfaction level of applications\nsubject to constraints such as communication cost for sharing resources across\nESPs. Our framework considers that different ESPs have their own objectives for\nutilizing their resources, thus resulting in a multi-objective optimization\nproblem. We present an $N$-person \\emph{Nash Bargaining Solution} (NBS) for\nresource allocation and sharing among ESPs with \\emph{Pareto} optimality\nguarantee. Furthermore, we propose a \\emph{distributed}, primal-dual algorithm\nto obtain the NBS by proving that the strong-duality property holds for the\nresultant resource sharing optimization problem.\n  Using synthetic and real-world data traces, we show numerically that the\nproposed NBS based framework not only enhances the ability to satisfy\napplications' resource demands, but also improves utilities of different ESPs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:26:05 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 01:27:00 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 11:33:41 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Zafari", "Faheem", ""], ["Basu", "Prithwish", ""], ["Leung", "Kin K.", ""], ["Li", "Jian", ""], ["Swami", "Ananthram", ""], ["Towsley", "Don", ""]]}, {"id": "2001.04235", "submitter": "James Aspnes", "authors": "James Aspnes", "title": "Notes on Theory of Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Notes for the Yale course CPSC 465/565 Theory of Distributed Systems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 18:51:18 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 18:50:16 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Aspnes", "James", ""]]}, {"id": "2001.04281", "submitter": "Paul J. Pritz", "authors": "Paul J. Pritz and Daniel Perez and Kin K. Leung", "title": "Fast-Fourier-Forecasting Resource Utilisation in Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distributed computing systems often consist of hundreds of nodes, executing\ntasks with different resource requirements. Efficient resource provisioning and\ntask scheduling in such systems are non-trivial and require close monitoring\nand accurate forecasting of the state of the system, specifically resource\nutilisation at its constituent machines. Two challenges present themselves\ntowards these objectives. First, collecting monitoring data entails substantial\ncommunication overhead. This overhead can be prohibitively high, especially in\nnetworks where bandwidth is limited. Second, forecasting models to predict\nresource utilisation should be accurate and need to exhibit high inference\nspeed. Mission critical scheduling and resource allocation algorithms use these\npredictions and rely on their immediate availability. To address the first\nchallenge, we present a communication-efficient data collection mechanism.\nResource utilisation data is collected at the individual machines in the system\nand transmitted to a central controller in batches. Each batch is processed by\nan adaptive data-reduction algorithm based on Fourier transforms and truncation\nin the frequency domain. We show that the proposed mechanism leads to a\nsignificant reduction in communication overhead while incurring only minimal\nerror and adhering to accuracy guarantees. To address the second challenge, we\npropose a deep learning architecture using complex Gated Recurrent Units to\nforecast resource utilisation. This architecture is directly integrated with\nthe above data collection mechanism to improve inference speed of our\nforecasting model. Using two real-world datasets, we demonstrate the\neffectiveness of our approach, both in terms of forecasting accuracy and\ninference speed. Our approach resolves challenges encountered in resource\nprovisioning frameworks and can be applied to other forecasting problems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:31:33 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 15:58:24 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 14:51:19 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Pritz", "Paul J.", ""], ["Perez", "Daniel", ""], ["Leung", "Kin K.", ""]]}, {"id": "2001.04374", "submitter": "Payal Dabas", "authors": "Payal and Sangita Kansal", "title": "Domination in Signed Petri Net", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, domination in Signed Petri net(SPN) has been introduced.We\nidentify some of the Petri net structures where a dominating set can\nexist.Applications of producer consumer problem, searching of food by bees and\nfinding similarity in research papers are given to understand the areas where\nthe proposed theory can be used.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 16:13:50 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Payal", "", ""], ["Kansal", "Sangita", ""]]}, {"id": "2001.04525", "submitter": "Anisur Molla Rahaman", "authors": "Subhrangsu Mandal, Anisur Rahaman Molla, and William K. Moses Jr", "title": "Live Exploration with Mobile Robots in a Dynamic Ring, Revisited", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph exploration problem requires a group of mobile robots, initially\nplaced arbitrarily on the nodes of a graph, to work collaboratively to explore\nthe graph such that each node is eventually visited by at least one robot. One\nimportant requirement of exploration is the {\\em termination} condition, i.e.,\nthe robots must know that exploration is completed. The problem of live\nexploration of a dynamic ring using mobile robots was recently introduced in\n[Di Luna et al., ICDCS 2016]. In it, they proposed multiple algorithms to solve\nexploration in fully synchronous and semi-synchronous settings with various\nguarantees when $2$ robots were involved. They also provided guarantees that\nwith certain assumptions, exploration of the ring using two robots was\nimpossible. An important question left open was how the presence of $3$ robots\nwould affect the results. In this paper, we try to settle this question in a\nfully synchronous setting and also show how to extend our results to a\nsemi-synchronous setting.\n  In particular, we present algorithms for exploration with explicit\ntermination using $3$ robots in conjunction with either (i) unique IDs of the\nrobots and edge crossing detection capability (i.e., two robots moving in\nopposite directions through an edge in the same round can detect each other),\nor (ii) access to randomness. The time complexity of our deterministic\nalgorithm is asymptotically optimal. We also provide complementary\nimpossibility results showing that there does not exist any explicit\ntermination algorithm for $2$ robots. The theoretical analysis and\ncomprehensive simulations of our algorithm show the effectiveness and\nefficiency of the algorithm in dynamic rings. We also present an algorithm to\nachieve exploration with partial termination using $3$ robots in the\nsemi-synchronous setting.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 20:29:20 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Mandal", "Subhrangsu", ""], ["Molla", "Anisur Rahaman", ""], ["Moses", "William K.", "Jr"]]}, {"id": "2001.04592", "submitter": "Vikram Sreekanti", "authors": "Vikram Sreekanti, Chenggang Wu, Xiayue Charles Lin, Johann\n  Schleier-Smith, Jose M. Faleiro, Joseph E. Gonzalez, Joseph M. Hellerstein,\n  Alexey Tumanov", "title": "Cloudburst: Stateful Functions-as-a-Service", "comments": null, "journal-ref": "PVLDB, 13(11):2438-2452, 2020", "doi": "10.14778/3407790.3407836", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function-as-a-Service (FaaS) platforms and \"serverless\" cloud computing are\nbecoming increasingly popular. Current FaaS offerings are targeted at stateless\nfunctions that do minimal I/O and communication. We argue that the benefits of\nserverless computing can be extended to a broader range of applications and\nalgorithms. We present the design and implementation of Cloudburst, a stateful\nFaaS platform that provides familiar Python programming with low-latency\nmutable state and communication, while maintaining the autoscaling benefits of\nserverless computing. Cloudburst accomplishes this by leveraging Anna, an\nautoscaling key-value store, for state sharing and overlay routing combined\nwith mutable caches co-located with function executors for data locality.\nPerformant cache consistency emerges as a key challenge in this architecture.\nTo this end, Cloudburst provides a combination of lattice-encapsulated state\nand new definitions and protocols for distributed session consistency.\nEmpirical results on benchmarks and diverse applications show that Cloudburst\nmakes stateful functions practical, reducing the state-management overheads of\ncurrent FaaS platforms by orders of magnitude while also improving the state of\nthe art in serverless consistency.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 02:22:02 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 17:33:09 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 21:29:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Sreekanti", "Vikram", ""], ["Wu", "Chenggang", ""], ["Lin", "Xiayue Charles", ""], ["Schleier-Smith", "Johann", ""], ["Faleiro", "Jose M.", ""], ["Gonzalez", "Joseph E.", ""], ["Hellerstein", "Joseph M.", ""], ["Tumanov", "Alexey", ""]]}, {"id": "2001.04756", "submitter": "Shiqiang Wang", "authors": "Pengchao Han, Shiqiang Wang, Kin K. Leung", "title": "Adaptive Gradient Sparsification for Efficient Federated Learning: An\n  Online Learning Approach", "comments": "Accepted at IEEE ICDCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is an emerging technique for training machine\nlearning models using geographically dispersed data collected by local\nentities. It includes local computation and synchronization steps. To reduce\nthe communication overhead and improve the overall efficiency of FL, gradient\nsparsification (GS) can be applied, where instead of the full gradient, only a\nsmall subset of important elements of the gradient is communicated. Existing\nwork on GS uses a fixed degree of gradient sparsity for i.i.d.-distributed data\nwithin a datacenter. In this paper, we consider adaptive degree of sparsity and\nnon-i.i.d. local datasets. We first present a fairness-aware GS method which\nensures that different clients provide a similar amount of updates. Then, with\nthe goal of minimizing the overall training time, we propose a novel online\nlearning formulation and algorithm for automatically determining the\nnear-optimal communication and computation trade-off that is controlled by the\ndegree of gradient sparsity. The online learning algorithm uses an estimated\nsign of the derivative of the objective function, which gives a regret bound\nthat is asymptotically equal to the case where exact derivative is available.\nExperiments with real datasets confirm the benefits of our proposed approaches,\nshowing up to $40\\%$ improvement in model accuracy for a finite training time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:09:23 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 17:56:09 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 16:34:48 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Han", "Pengchao", ""], ["Wang", "Shiqiang", ""], ["Leung", "Kin K.", ""]]}, {"id": "2001.04787", "submitter": "Yanhong Annie Liu", "authors": "Saksham Chand and Yanhong A Liu", "title": "What's Live? Understanding Distributed Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed consensus algorithms such as Paxos have been studied extensively.\nThey all use the same definition of safety. Liveness is especially important in\npractice despite well-known theoretical impossibility results. However, many\ndifferent liveness properties and assumptions have been stated, and there are\nno systematic comparisons for better understanding of these properties.\n  This paper systematically studies and compares different liveness properties\nstated for over 30 prominent consensus algorithms and variants. We introduce a\nprecise high-level language and formally specify these properties in the\nlanguage. We then create a hierarchy of liveness properties combining two\nhierarchies of the assumptions used and a hierarchy of the assertions made, and\ncompare the strengths and weaknesses of algorithms that ensure these\nproperties. Our formal specifications and systematic comparisons led to the\ndiscovery of a range of problems in various stated liveness properties, from\ntoo weak assumptions for which no liveness assertions can hold, to too strong\nassumptions making it trivial to achieve the assertions. We also developed TLA+\nspecifications of these liveness properties, and we use model checking of\nexecution steps to illustrate liveness patterns for Paxos.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 14:12:44 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 04:02:53 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chand", "Saksham", ""], ["Liu", "Yanhong A", ""]]}, {"id": "2001.04886", "submitter": "Anthony Chronopoulos", "authors": "A.T. Chronopoulos and S.K. Kim", "title": "s-Step Orthomin and GMRES implemented on parallel computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Orthomin ( Omin ) and the Generalized Minimal Residual method ( GMRES )\nare commonly used iterative methods for approximating the solution of\nnon-symmetric linear systems. The s-step generalizations of these methods\nenhance their data locality parallel and properties by forming s simultaneous\nsearch direction vectors. Good data locality is the key in achieving near peak\nrates on memory hierarchical supercomputers. The theoretical derivation of the\ns-step Arnoldi and Omin has been published in the past. Here we derive the\ns-step GMRES method. We then implement s-step Omin and GMRES on a Cray-2\nhierarchical memory supercomputer.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:45:05 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 06:51:31 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chronopoulos", "A. T.", ""], ["Kim", "S. K.", ""]]}, {"id": "2001.04937", "submitter": "Jesus Rodriguez Sanchez", "authors": "Jesus Rodriguez Sanchez, Ove Edfors, Fredrik Rusek, Liang Liu", "title": "Processing Distribution and Architecture Tradeoff for Large Intelligent\n  Surface Implementation", "comments": "Presented at IEEE ICC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Large Intelligent Surface (LIS) concept has emerged recently as a new\nparadigm for wireless communication, remote sensing and positioning. It\nconsists of a continuous radiating surface placed relatively close to the\nusers, which is able to communicate with users by independent transmission and\nreception (replacing base stations). Despite of its potential, there are a lot\nof challenges from an implementation point of view, with the interconnection\ndata-rate and computational complexity being the most relevant. Distributed\nprocessing techniques and hierarchical architectures are expected to play a\nvital role addressing this while ensuring scalability. In this paper we perform\nalgorithm-architecture codesign and analyze the hardware requirements and\narchitecture trade-offs for a discrete LIS to perform uplink detection. By\ndoing this, we expect to give concrete case studies and guidelines for\nefficient implementation of LIS systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:54:08 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 09:38:42 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Sanchez", "Jesus Rodriguez", ""], ["Edfors", "Ove", ""], ["Rusek", "Fredrik", ""], ["Liu", "Liang", ""]]}, {"id": "2001.05082", "submitter": "Jianyu Niu", "authors": "Jianyu Niu, Ziyu Wang, Fangyu Gai, and Chen Feng", "title": "Incentive Analysis of Bitcoin-NG, Revisited", "comments": "12 pages, 11 figures", "journal-ref": "IFIP WG7.3 Performance 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin-NG is among the first blockchain protocols to approach the\n\\emph{near-optimal} throughput by decoupling blockchain operation into two\nplanes: leader election and transaction serialization. Its decoupling idea has\ninspired a new generation of high-performance blockchain protocols. However,\nthe existing incentive analysis of Bitcoin-NG has several limitations. First,\nthe impact of network capacity is ignored. Second, an integrated incentive\nanalysis that jointly considers both key blocks and microblocks is still\nmissing.\n  In this paper, we aim to address these two limitations. First, we propose a\nnew incentive analysis that takes the network capacity into account, showing\nthat Bitcoin-NG can still maintain incentive compatibility against the\nmicroblock mining attack even under limited network capacity. Second, we\nleverage a Markov decision process (MDP) to jointly analyze the incentive of\nboth key blocks and microblocks, showing that the selfish mining revenue of\nBitcoin-NG is a little higher than that in Bitcoin only when the selfish miner\ncontrols more than 35\\% of the mining power. We hope that our in-depth\nincentive analysis for Bitcoin-NG can shed some light on the mechanism design\nand incentive analysis of next-generation blockchain protocols.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 23:36:44 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 01:28:49 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 18:51:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Niu", "Jianyu", ""], ["Wang", "Ziyu", ""], ["Gai", "Fangyu", ""], ["Feng", "Chen", ""]]}, {"id": "2001.05101", "submitter": "Qian Yu", "authors": "Qian Yu, A. Salman Avestimehr", "title": "Entangled Polynomial Codes for Secure, Private, and Batch Distributed\n  Matrix Multiplication: Breaking the \"Cubic\" Barrier", "comments": "To appear in ISIT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed matrix multiplication, a common scenario is to assign each\nworker a fraction of the multiplication task, by partitioning the input\nmatrices into smaller submatrices. In particular, by dividing two input\nmatrices into $m$-by-$p$ and $p$-by-$n$ subblocks, a single multiplication task\ncan be viewed as computing linear combinations of $pmn$ submatrix products,\nwhich can be assigned to $pmn$ workers. Such block-partitioning based designs\nhave been widely studied under the topics of secure, private, and batch\ncomputation, where the state of the arts all require computing at least \"cubic\"\n($pmn$) number of submatrix multiplications. Entangled polynomial codes, first\npresented for straggler mitigation, provides a powerful method for breaking the\ncubic barrier. It achieves a subcubic recovery threshold, meaning that the\nfinal product can be recovered from \\emph{any} subset of multiplication results\nwith a size order-wise smaller than $pmn$. In this work, we show that entangled\npolynomial codes can be further extended to also include these three important\nsettings, and provide a unified framework that order-wise reduces the total\ncomputational costs upon the state of the arts by achieving subcubic recovery\nthresholds.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 01:52:22 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 16:50:01 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Yu", "Qian", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2001.05218", "submitter": "Grzegorz Korcyl", "authors": "G. Korcyl and P. Korcyl", "title": "Optimized implementation of the conjugate gradient algorithm for\n  FPGA-based platforms using the Dirac-Wilson operator as an example", "comments": "description of FPGA accelerated Conjugate Gradient open-source code\n  located at https://bitbucket.org/fpgafais/hpcg_fpga, 11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now a noticeable trend in High Performance Computing that the systems\nare becoming more and more heterogeneous. Compute nodes with a host CPU are\nbeing equipped with accelerators, the latter being a GPU or FPGA cards or both.\nIn many cases at the heart of scientific applications running on such systems\nare iterative linear solvers. In this work we present a software package which\nincludes an FPGA implementation of the Conjugate Gradient algorithm using a\nparticular problem of the Dirac-Wilson operator as encountered in numerical\nsimulations of Quantum Chromodynamics. The software is written in OpenCL and\nC++ and is optimized for maximal performance. Our framework allows for a simple\nimplementation of other linear operators, while keeping the data transport\nmechanisms unaltered. Hence, our software can serve as a backbone for many\napplications which are expected to gain a significant boost factor on FPGA\naccelerators. As such systems are expected to become more and more widespread,\nthe need for highly performant FPGA implementations of the Conjugate Gradient\nalgorithm and its variants will certainly increase and the porting investment\ncan be greatly facilitated by the attached code.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 10:30:47 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Korcyl", "G.", ""], ["Korcyl", "P.", ""]]}, {"id": "2001.05240", "submitter": "Yibin Xu", "authors": "Yibin Xu and Yangyu Huang", "title": "An n/2 Byzantine node tolerate Blockchain Sharding approach", "comments": null, "journal-ref": "In The 35th ACM/SIGAPP Symposium on Applied Computing (SAC '20),\n  March 30-April 3, 2020, Brno, Czech Republic. ACM, NewYork, NY, USA,", "doi": "10.1145/3341105.3374069", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Blockchain Sharding approaches can only tolerate up to n/3 of\nnodes being adversary because they rely on the hypergeometric distribution to\nmake a failure (an adversary does not have n/3 of nodes globally but can\nmanipulate the consensus of a Shard) hard to happen. The system must maintain a\nlarge Shard size (the number of nodes inside a Shard) to sustain the low\nfailure probability so that only a small number of Shards may exist. In this\npaper, we present a new approach of Blockchain Sharding that can withstand up\nto n/2 of nodes being bad. We categorise the nodes into different classes, and\nevery Shard has a fixed number of nodes from different classes. We prove that\nthis design is much more secure than the traditional models (only have one\nclass) and the Shard size can be reduced significantly. In this way, many more\nShards can exist, and the transaction throughput can be largely increased. The\nimproved Blockchain Sharding approach is promising to serve as the foundation\nfor decentralised autonomous organisations and decentralised database.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 11:14:14 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 07:14:57 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 14:39:45 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Xu", "Yibin", ""], ["Huang", "Yangyu", ""]]}, {"id": "2001.05293", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray", "title": "Lazy object copy as a platform for population-based probabilistic\n  programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers dynamic memory management for population-based\nprobabilistic programs, such as those using particle methods for inference.\nSuch programs exhibit a pattern of allocating, copying, potentially mutating,\nand deallocating collections of similar objects through successive generations.\nThese objects may assemble data structures such as stacks, queues, lists,\nragged arrays, and trees, which may be of random, and possibly unbounded, size.\nFor the simple case of $N$ particles, $T$ generations, $D$ objects, and\nresampling at each generation, dense representation requires $O(DNT)$ memory,\nwhile sparse representation requires only $O(DT+DN\\log DN)$ memory, based on\nexisting theoretical results. This work describes an object copy-on-write\nplatform to automate this saving for the programmer. The core idea is\nformalized using labeled directed multigraphs, where vertices represent\nobjects, edges the pointers between them, and labels the necessary bookkeeping.\nA specific labeling scheme is proposed for high performance under the\nmotivating pattern. The platform is implemented for the Birch probabilistic\nprogramming language, using smart pointers, hash tables, and reference-counting\ngarbage collection. It is tested empirically on a number of realistic\nprobabilistic programs, and shown to significantly reduce memory use and\nexecution time in a manner consistent with theoretical expectations. This\nenables copy-on-write for the imperative programmer, lazy deep copies for the\nobject-oriented programmer, and in-place write optimizations for the functional\nprogrammer.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 00:52:27 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Murray", "Lawrence M.", ""]]}, {"id": "2001.05299", "submitter": "Sushil Mahavir Varma", "authors": "Sushil Mahavir Varma, Siva Theja Maguluri", "title": "Throughput Optimal Routing in Blockchain Based Payment Systems", "comments": "17 pages, 6 Figures, Accepted to be published in IEEE TCNS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptocurrency networks such as Bitcoin have emerged as a distributed\nalternative to traditional centralized financial transaction networks. However,\nthere are major challenges in scaling up the throughput of such networks.\nLightning network and Spider network are alternates that build bidirectional\npayment channels on top of cryptocurrency networks using smart contracts, to\nenable fast transactions that bypass the Blockchain. In this paper, we study\nthe problem of routing transactions in such a payment processing network. We\nfirst propose a Stochastic model to study such a system, as opposed to a fluid\nmodel that is studied in the literature. Each link in such a model is a\ntwo-sided queue, and unlike classical queues, such queues are not stable unless\nthere is an external control. We propose a notion of stability for the payment\nprocessing network consisting of such two-sided queues using the notion of\non-chain rebalancing. We then characterize the capacity region and propose a\nthroughput optimal algorithm that stabilizes the system under any load within\nthe capacity region. The stochastic model enables us to study closed loop\npolicies, which typically have better queuing/delay performance than the open\nloop policies (or static split rules) studied in the literature. We investigate\nthis through simulations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 19:46:25 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 21:08:53 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 18:28:05 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Varma", "Sushil Mahavir", ""], ["Maguluri", "Siva Theja", ""]]}, {"id": "2001.05358", "submitter": "Somayyeh Firoozi", "authors": "Reza Fotohi, Somayyeh Firoozi Bari", "title": "A novel countermeasure technique to protect WSN against denial-of-sleep\n  attacks using firefly and Hopfield neural network (HNN) algorithms", "comments": "27 pages, 10 figures, Journal", "journal-ref": "The Journal of Supercomputing, 1-27 (2020)", "doi": "10.1007/s11227-019-03131-x", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless sensor networks (WSNs) contain numerous nodes that their main goals\nare to monitor and control environments. Also, sensor nodes distribute based on\nnetwork usage. One of the most significant issues in this type of network is\nthe energy consumption of sensor nodes. In fixed-sink networks, nodes which are\nnear the sink act as an interface to transfer data of other nodes to sink. This\ncauses the energy consumption of sensors reduces rapidly. Therefore, the\nlifetime of the network declines. Sensor nodes owing to their weaknesses are\nsusceptible to several threats, one of which is denial-of-sleep attack (DoSA)\nthreatening WSN. Hence, the DoSA refers to the energy loss in these nodes by\nmaintaining the nodes from entering energy-saving and sleep mode. In this\npaper, a hybrid approach is proposed based on mobile sink, firefly algorithm\nbased on leach, and Hopfield neural network (WSN-FAHN). Thus, mobile sink is\napplied to both improve energy consumption and increase network lifetime.\nFirefly algorithm is proposed to cluster nodes and authenticate in two levels\nto prevent from DoSA. In addition, Hopfield neural network detects the\ndirection route of the sink movement to send data of CH. Furthermore, here\nWSN-FAHN technique is assessed through wide simulations performed in the NS-2\nenvironment. The WSN-FAHN procedure superiority is demonstrated by simulation\noutcomes in comparison with contemporary schemes based on performance metrics\nlike packet delivery ratio (PDR), average throughput, detection ratio, and\nnetwork lifetime while decreasing the average residual energy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 14:59:14 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Fotohi", "Reza", ""], ["Bari", "Somayyeh Firoozi", ""]]}, {"id": "2001.05452", "submitter": "Abishek Sankararaman", "authors": "Ronshee Chawla, Abishek Sankararaman, Ayalvadi Ganesh, Sanjay\n  Shakkottai", "title": "The Gossiping Insert-Eliminate Algorithm for Multi-Agent Bandits", "comments": "To Appear in AISTATS 2020. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a decentralized multi-agent Multi Armed Bandit (MAB) setup\nconsisting of $N$ agents, solving the same MAB instance to minimize individual\ncumulative regret. In our model, agents collaborate by exchanging messages\nthrough pairwise gossip style communications on an arbitrary connected graph.\nWe develop two novel algorithms, where each agent only plays from a subset of\nall the arms. Agents use the communication medium to recommend only arm-IDs\n(not samples), and thus update the set of arms from which they play. We\nestablish that, if agents communicate $\\Omega(\\log(T))$ times through any\nconnected pairwise gossip mechanism, then every agent's regret is a factor of\norder $N$ smaller compared to the case of no collaborations. Furthermore, we\nshow that the communication constraints only have a second order effect on the\nregret of our algorithm. We then analyze this second order term of the regret\nto derive bounds on the regret-communication tradeoffs. Finally, we empirically\nevaluate our algorithm and conclude that the insights are fundamental and not\nartifacts of our bounds. We also show a lower bound which gives that the regret\nscaling obtained by our algorithm cannot be improved even in the absence of any\ncommunication constraints. Our results thus demonstrate that even a minimal\nlevel of collaboration among agents greatly reduces regret for all agents.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 17:49:29 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 00:09:46 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 21:11:46 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chawla", "Ronshee", ""], ["Sankararaman", "Abishek", ""], ["Ganesh", "Ayalvadi", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "2001.05585", "submitter": "Crist\\'obal A. Navarro", "authors": "Crist\\'obal A. Navarro, Roberto Carrasco, Ricardo J. Barrientos,\n  Javier A. Riquelme, Raimundo Vega", "title": "GPU Tensor Cores for fast Arithmetic Reductions", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a GPU tensor core approach that encodes the arithmetic\nreduction of $n$ numbers as a set of chained $m \\times m$ matrix multiply\naccumulate (MMA) operations executed in parallel by GPU tensor cores. The\nasymptotic running time of the proposed chained tensor core approach is $T(n)=5\nlog_{m^2}{n}$ and its speedup is $S=\\dfrac{4}{5} log_{2}{m^2}$ over the classic\n$O(n \\log n)$ parallel reduction algorithm. Experimental performance results\nshow that the proposed reduction method is $\\sim 3.2 \\times$ faster than a\nconventional GPU reduction implementation, and preserves the numerical\nprecision because the sub-results of each chain of $R$ MMAs is kept as a 32-bit\nfloating point value, before being all reduced into as a final 32-bit result.\nThe chained MMA design allows a flexible configuration of thread-blocks; small\nthread-blocks of 32 or 128 threads can still achieve maximum performance using\na chain of $R=4,5$ MMAs per block, while large thread-blocks work best with\n$R=1$. The results obtained in this work show that tensor cores can indeed\nprovide a significant performance improvement to non-Machine Learning\napplications such as the arithmetic reduction, which is an integration tool for\nstudying many scientific phenomena.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 22:44:30 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Navarro", "Crist\u00f3bal A.", ""], ["Carrasco", "Roberto", ""], ["Barrientos", "Ricardo J.", ""], ["Riquelme", "Javier A.", ""], ["Vega", "Raimundo", ""]]}, {"id": "2001.05713", "submitter": "Guangxu Zhu", "authors": "Guangxu Zhu, Yuqing Du, Deniz Gunduz, and Kaibin Huang", "title": "One-Bit Over-the-Air Aggregation for Communication-Efficient Federated\n  Edge Learning: Design and Convergence Analysis", "comments": "to appear in IEEE Transactions on Wireless Communications, the open\n  source codes are available at\n  https://github.com/BornHater/One-bit-over-the-air-computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated edge learning (FEEL) is a popular framework for model training at\nan edge server using data distributed at edge devices (e.g., smart-phones and\nsensors) without compromising their privacy. In the FEEL framework, edge\ndevices periodically transmit high-dimensional stochastic gradients to the edge\nserver, where these gradients are aggregated and used to update a global model.\nWhen the edge devices share the same communication medium, the multiple access\nchannel (MAC) from the devices to the edge server induces a communication\nbottleneck. To overcome this bottleneck, an efficient broadband analog\ntransmission scheme has been recently proposed, featuring the aggregation of\nanalog modulated gradients (or local models) via the waveform-superposition\nproperty of the wireless medium. However, the assumed linear analog modulation\nmakes it difficult to deploy this technique in modern wireless systems that\nexclusively use digital modulation. To address this issue, we propose in this\nwork a novel digital version of broadband over-the-air aggregation, called\none-bit broadband digital aggregation (OBDA). The new scheme features one-bit\ngradient quantization followed by digital quadrature amplitude modulation (QAM)\nat edge devices and over-the-air majority-voting based decoding at edge server.\nWe provide a comprehensive analysis of the effects of wireless channel\nhostilities (channel noise, fading, and channel estimation errors) on the\nconvergence rate of the proposed FEEL scheme. The analysis shows that the\nhostilities slow down the convergence of the learning process by introducing a\nscaling factor and a bias term into the gradient norm. However, we show that\nall the negative effects vanish as the number of participating devices grows,\nbut at a different rate for each type of channel hostility.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 09:42:04 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 15:24:32 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhu", "Guangxu", ""], ["Du", "Yuqing", ""], ["Gunduz", "Deniz", ""], ["Huang", "Kaibin", ""]]}, {"id": "2001.05759", "submitter": "Diego Garc\\'ia-Gil", "authors": "Diego Garc\\'ia-Gil, Salvador Garc\\'ia, Ning Xiong, Francisco Herrera", "title": "A Methodology guided by Decision Trees Ensemble and Smart Data for\n  Imbalanced Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differences in data size per class, also known as imbalanced data\ndistribution, have become a common problem affecting data quality. Big Data\nscenarios pose a new challenge to traditional imbalanced classification\nalgorithms, since they are not prepared to work with such amount of data. Split\ndata strategies and lack of data in the minority class due to the use of\nMapReduce paradigm have posed new challenges for tackling the imbalance between\nclasses in Big Data scenarios. Ensembles have shown to be able to successfully\naddress imbalanced data problems. Smart Data refers to data of enough quality\nto achieve high performance models. The combination of ensembles and Smart\nData, achieved through Big Data preprocessing, should be a great synergy. In\nthis paper, we propose a novel methodology based on Decision Trees Ensemble\nwith Smart Data for addressing the imbalanced classification problem in Big\nData domains, namely DeTE_SD methodology. This methodology is based on the\nlearning of different decision trees using distributed quality data for the\nensemble process. This quality data is achieved by fusing Random\nDiscretization, Principal Components Analysis and clustering-based Random\nOversampling for obtaining different Smart Data versions of the original data.\nExperiments carried out in 21 binary adapted datasets have shown that our\nmethodology outperforms Random Forest.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 12:25:59 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 07:29:03 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Garc\u00eda-Gil", "Diego", ""], ["Garc\u00eda", "Salvador", ""], ["Xiong", "Ning", ""], ["Herrera", "Francisco", ""]]}, {"id": "2001.05811", "submitter": "Petr T\\r{u}ma", "authors": "Lubom\\'ir Bulej, Vojt\\v{e}ch Hork\\'y, Petr T\\r{u}ma, Fran\\c{c}ois\n  Farquet, Aleksandar Prokopec", "title": "Duet Benchmarking: Improving Measurement Accuracy in the Cloud", "comments": null, "journal-ref": null, "doi": "10.1145/3358960.3379132", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the duet measurement procedure, which helps improve the\naccuracy of performance comparison experiments conducted on shared machines by\nexecuting the measured artifacts in parallel and evaluating their relative\nperformance together, rather than individually. Specifically, we analyze the\nbehavior of the procedure in multiple cloud environments and use experimental\nevidence to answer multiple research questions concerning the assumption\nunderlying the procedure. We demonstrate improvements in accuracy ranging from\n2.3x to 12.5x (5.03x on average) for the tested ScalaBench (and DaCapo)\nworkloads, and from 23.8x to 82.4x (37.4x on average) for the SPEC CPU 2017\nworkloads.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 14:17:28 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 08:37:19 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Bulej", "Lubom\u00edr", ""], ["Hork\u00fd", "Vojt\u011bch", ""], ["T\u016fma", "Petr", ""], ["Farquet", "Fran\u00e7ois", ""], ["Prokopec", "Aleksandar", ""]]}, {"id": "2001.05870", "submitter": "Amir Erfan Eshratifar", "authors": "Amir Erfan Eshratifar and Massoud Pedram", "title": "Runtime Deep Model Multiplexing for Reduced Latency and Energy\n  Consumption Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning algorithm to design a light-weight neural multiplexer\nthat given the input and computational resource requirements, calls the model\nthat will consume the minimum compute resources for a successful inference.\nMobile devices can use the proposed algorithm to offload the hard inputs to the\ncloud while inferring the easy ones locally. Besides, in the large scale\ncloud-based intelligent applications, instead of replicating the most-accurate\nmodel, a range of small and large models can be multiplexed from depending on\nthe input's complexity which will save the cloud's computational resources. The\ninput complexity or hardness is determined by the number of models that can\npredict the correct label. For example, if no model can predict the label\ncorrectly, then the input is considered as the hardest. The proposed algorithm\nallows the mobile device to detect the inputs that can be processed locally and\nthe ones that require a larger model and should be sent a cloud server.\nTherefore, the mobile user benefits from not only the local processing but also\nfrom an accurate model hosted on a cloud server. Our experimental results show\nthat the proposed algorithm improves mobile's model accuracy by 8.52% which is\nbecause of those inputs that are properly selected and offloaded to the cloud\nserver. In addition, it saves the cloud providers' compute resources by a\nfactor of 2.85x as small models are chosen for easier inputs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 23:49:51 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 17:07:31 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Eshratifar", "Amir Erfan", ""], ["Pedram", "Massoud", ""]]}, {"id": "2001.06139", "submitter": "Sheng Di", "authors": "Robert Underwood, Sheng Di, Jon C. Calhoun, and Franck Cappello", "title": "FRaZ: A Generic High-Fidelity Fixed-Ratio Lossy Compression Framework\n  for Scientific Floating-point Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ever-increasing volumes of scientific floating-point data being produced\nby high-performance computing applications, significantly reducing scientific\nfloating-point data size is critical, and error-controlled lossy compressors\nhave been developed for years. None of the existing scientific floating-point\nlossy data compressors, however, support effective fixed-ratio lossy\ncompression. Yet fixed-ratio lossy compression for scientific floating-point\ndata not only compresses to the requested ratio but also respects a\nuser-specified error bound with higher fidelity. In this paper, we present\nFRaZ: a generic fixed-ratio lossy compression framework respecting\nuser-specified error constraints. The contribution is twofold. (1) We develop\nan efficient iterative approach to accurately determine the appropriate error\nsettings for different lossy compressors based on target compression ratios.\n(2) We perform a thorough performance and accuracy evaluation for our proposed\nfixed-ratio compression framework with multiple state-of-the-art\nerror-controlled lossy compressors, using several real-world scientific\nfloating-point datasets from different domains. Experiments show that FRaZ\neffectively identifies the optimum error setting in the entire error setting\nspace of any given lossy compressor. While fixed-ratio lossy compression is\nslower than fixed-error compression, it provides an important new lossy\ncompression technique for users of very large scientific floating-point\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 02:53:56 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Underwood", "Robert", ""], ["Di", "Sheng", ""], ["Calhoun", "Jon C.", ""], ["Cappello", "Franck", ""]]}, {"id": "2001.06194", "submitter": "Zhen Yu", "authors": "Ping Zhou, Zhen Yu, Jingyi Ma, Maozai Tian, and Ye Fan", "title": "Communication-Efficient Distributed Estimator for Generalized Linear\n  Models with a Diverging Number of Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed statistical inference has recently attracted immense attention.\nThe asymptotic efficiency of the maximum likelihood estimator (MLE), the\none-step MLE, and the aggregated estimating equation estimator are established\nfor generalized linear models under the \"large $n$, diverging $p_n$\" framework,\nwhere the dimension of the covariates $p_n$ grows to infinity at a polynomial\nrate $o(n^\\alpha)$ for some $0<\\alpha<1$. Then a novel method is proposed to\nobtain an asymptotically efficient estimator for large-scale distributed data\nby two rounds of communication. In this novel method, the assumption on the\nnumber of servers is more relaxed and thus practical for real-world\napplications. Simulations and a case study demonstrate the satisfactory\nfinite-sample performance of the proposed estimators.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 08:51:11 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 17:25:05 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zhou", "Ping", ""], ["Yu", "Zhen", ""], ["Ma", "Jingyi", ""], ["Tian", "Maozai", ""], ["Fan", "Ye", ""]]}, {"id": "2001.06271", "submitter": "Dragos-Adrian (Adi) Seredinschi PhD", "authors": "Rachid Guerraoui and Jovan Komatovic and Petr Kuznetsov and\n  Yvonne-Anne Pignolet and Dragos-Adrian Seredinschi and Andrei Tonkikh", "title": "Dynamic Byzantine Reliable Broadcast [Technical Report]", "comments": "This is an extended version of a conference article, appearingin the\n  proceedings of the 24th Int. Conference on Principles of Distributed Systems\n  (OPODIS 2020). This work has been supported in part by a grant from\n  Interchain Foundation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable broadcast is a communication primitive guaranteeing, intuitively,\nthat all processes in a distributed system deliver the same set of messages.\nThe reason why this primitive is appealing is twofold: (i) we can implement it\ndeterministically in a completely asynchronous environment, unlike stronger\nprimitives like consensus and total-order broadcast, and yet (ii) reliable\nbroadcast is powerful enough to implement important applications like payment\nsystems.\n  The problem we tackle in this paper is that of dynamic reliable broadcast,\ni.e., enabling processes to join or leave the system. This property is\ndesirable for long-lived applications (aiming to be highly available), yet has\nbeen precluded in previous asynchronous reliable broadcast protocols. We study\nthis property in a general adversarial (i.e., Byzantine) environment.\n  We introduce the first specification of a dynamic Byzantine reliable\nbroadcast (DBRB) primitive that is amenable to an asynchronous implementation.\nWe then present an algorithm implementing this specification in an asynchronous\nnetwork. Our DBRB algorithm ensures that if any correct process in the system\nbroadcasts a message, then every correct process delivers that message unless\nit leaves the system. Moreover, if a correct process delivers a message, then\nevery correct process that has not expressed its will to leave the system\ndelivers that message. We assume that more than $2/3$ of processes in the\nsystem are correct at all times, which is tight in our context.\n  We also show that if only one process in the system can fail---and it can\nfail only by crashing---then it is impossible to implement a stronger\nprimitive, ensuring that if any correct process in the system broadcasts or\ndelivers a message, then every correct process in the system delivers that\nmessage---including those that leave.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 12:47:31 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 19:16:50 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Komatovic", "Jovan", ""], ["Kuznetsov", "Petr", ""], ["Pignolet", "Yvonne-Anne", ""], ["Seredinschi", "Dragos-Adrian", ""], ["Tonkikh", "Andrei", ""]]}, {"id": "2001.06403", "submitter": "Saad Quader", "authors": "Aggelos Kiayias, Saad Quader, and Alexander Russell", "title": "Consistency of Proof-of-Stake Blockchains with Concurrent Honest Slot\n  Leaders", "comments": "Includes new sections describing (1) an adaptive online adversary and\n  (2) an efficient algorithm to compute consistency error probabilities. arXiv\n  admin note: text overlap with arXiv:1911.10187", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the fundamental security threshold of eventual consensus\nProof-of-Stake (PoS) blockchain protocols under the longest-chain rule by\nshowing, for the first time, the positive effect of rounds with concurrent\nhonest leaders.\n  Current security analyses reduce consistency to the dynamics of an abstract,\nround-based block creation process that is determined by three events\nassociated with a round: (i) event $A$: at least one adversarial leader, (ii)\nevent $S$: a single honest leader, and (iii) event $M$: multiple, but honest,\nleaders. We present an asymptotically optimal consistency analysis assuming\nthat an honest round is more likely than an adversarial round (i.e., $\\Pr[S] +\n\\Pr[M] > \\Pr[A]$); this threshold is optimal. This is a first in the literature\nand can be applied to both the simple synchronous communication as well as\ncommunication with bounded delays.\n  In all existing consistency analyses, event $M$ is either penalized or\ntreated neutrally. Specifically, the consistency analyses in Ouroboros Praos\n(Eurocrypt 2018) and Genesis (CCS 2018) assume that $\\Pr[S] - \\Pr[M] > \\Pr[A]$;\nthe analyses in Sleepy Consensus (Asiacrypt 2017) and Snow White (Fin. Crypto\n2019) assume that $\\Pr[S] > \\Pr[A]$. Moreover, all existing analyses completely\nbreak down when $\\Pr[S] < \\Pr[A]$. These thresholds determine the critical\ntrade-off between the honest majority, network delays, and consistency error.\n  Our new results can be directly applied to improve the security guarantees of\nthe existing protocols. We also provide an efficient algorithm to explicitly\ncalculate these error probabilities in the synchronous setting. Furthermore, we\ncomplement these results by analyzing the setting where $S$ is rare, even\nallowing $\\Pr[S] = 0$, under the added assumption that honest players adopt a\nconsistent chain selection rule.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 02:50:47 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 01:17:19 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 19:47:18 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 20:27:25 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Kiayias", "Aggelos", ""], ["Quader", "Saad", ""], ["Russell", "Alexander", ""]]}, {"id": "2001.06778", "submitter": "Zhaohua Chen", "authors": "Mengqian Zhang, Jichen Li, Zhaohua Chen, Hongyin Chen, Xiaotie Deng", "title": "CycLedger: A Scalable and Secure Parallel Protocol for Distributed\n  Ledger via Sharding", "comments": "Full version of the paper in IPDPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional public distributed ledgers have not been able to scale-out well\nand work efficiently. Sharding is deemed as a promising way to solve this\nproblem. By partitioning all nodes into small committees and letting them work\nin parallel, we can significantly lower the amount of communication and\ncomputation, reduce the overhead on each node's storage, as well as enhance the\nthroughput of the distributed ledger. Existing sharding-based protocols still\nsuffer from several serious drawbacks. The first thing is that all non-faulty\nnodes must connect well with each other, which demands a huge number of\ncommunication channels in the network. Moreover, previous protocols have faced\ngreat loss in efficiency in the case where the honesty of each committee's\nleader is in question. At the same time, no explicit incentive is provided for\nnodes to actively participate in the protocol.\n  We present CycLedger, a scalable and secure parallel protocol for distributed\nledger via sharding. Our protocol selects a leader and a partial set for each\ncommittee, who are in charge of maintaining intra-shard consensus and\ncommunicating with other committees, to reduce the amortized complexity of\ncommunication, computation, and storage on all nodes. We introduce a novel\nsemi-commitment scheme between committees and a recovery procedure to prevent\nthe system from crashing even when leaders of committees are malicious. To add\nincentive for the network, we use the concept of reputation, which measures\neach node's trusty computing power. As nodes with a higher reputation receive\nmore rewards, there is an encouragement for nodes with strong computing ability\nto work honestly to gain reputation. In this way, we strike out a new path to\nestablish scalability, security, and incentive for the sharding-based\ndistributed ledger.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 05:28:58 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 03:48:06 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 05:28:00 GMT"}, {"version": "v4", "created": "Sun, 5 Apr 2020 12:38:11 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhang", "Mengqian", ""], ["Li", "Jichen", ""], ["Chen", "Zhaohua", ""], ["Chen", "Hongyin", ""], ["Deng", "Xiaotie", ""]]}, {"id": "2001.06935", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Tim Davis, Chansup Byun, William Arcand, David Bestor,\n  William Bergeron, Vijay Gadepally, Matthew Hubbell, Michael Houle, Michael\n  Jones, Anna Klein, Peter Michaleas, Lauren Milechin, Julie Mullen, Andrew\n  Prout, Antonio Rosa, Siddharth Samsi, Charles Yee, Albert Reuther", "title": "75,000,000,000 Streaming Inserts/Second Using Hierarchical Hypersparse\n  GraphBLAS Matrices", "comments": "4 pages, 4 figures, 28 references, accepted to IPDPS GrAPL 2020.\n  arXiv admin note: substantial text overlap with arXiv:1907.04217", "journal-ref": null, "doi": "10.1109/IPDPSW50202.2020.00046", "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SuiteSparse GraphBLAS C-library implements high performance hypersparse\nmatrices with bindings to a variety of languages (Python, Julia, and\nMatlab/Octave). GraphBLAS provides a lightweight in-memory database\nimplementation of hypersparse matrices that are ideal for analyzing many types\nof network data, while providing rigorous mathematical guarantees, such as\nlinearity. Streaming updates of hypersparse matrices put enormous pressure on\nthe memory hierarchy. This work benchmarks an implementation of hierarchical\nhypersparse matrices that reduces memory pressure and dramatically increases\nthe update rate into a hypersparse matrices. The parameters of hierarchical\nhypersparse matrices rely on controlling the number of entries in each level in\nthe hierarchy before an update is cascaded. The parameters are easily tunable\nto achieve optimal performance for a variety of applications. Hierarchical\nhypersparse matrices achieve over 1,000,000 updates per second in a single\ninstance. Scaling to 31,000 instances of hierarchical hypersparse matrices\narrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update\nrate of 75,000,000,000 updates per second. This capability allows the MIT\nSuperCloud to analyze extremely large streaming network data sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 01:41:17 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 20:50:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kepner", "Jeremy", ""], ["Davis", "Tim", ""], ["Byun", "Chansup", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "2001.06989", "submitter": "Aydin Buluc", "authors": "Katherine Yelick, Aydin Buluc, Muaaz Awan, Ariful Azad, Benjamin\n  Brock, Rob Egan, Saliya Ekanayake, Marquita Ellis, Evangelos Georganas,\n  Giulia Guidi, Steven Hofmeyr, Oguz Selvitopi, Cristina Teodoropol, Leonid\n  Oliker", "title": "The Parallelism Motifs of Genomic Data Analysis", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2019.0394", "report-no": null, "categories": "cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic data sets are growing dramatically as the cost of sequencing\ncontinues to decline and small sequencing devices become available. Enormous\ncommunity databases store and share this data with the research community, but\nsome of these genomic data analysis problems require large scale computational\nplatforms to meet both the memory and computational requirements. These\napplications differ from scientific simulations that dominate the workload on\nhigh end parallel systems today and place different requirements on programming\nsupport, software libraries, and parallel architectural design. For example,\nthey involve irregular communication patterns such as asynchronous updates to\nshared data structures. We consider several problems in high performance\ngenomics analysis, including alignment, profiling, clustering, and assembly for\nboth single genomes and metagenomes. We identify some of the common\ncomputational patterns or motifs that help inform parallelization strategies\nand compare our motifs to some of the established lists, arguing that at least\ntwo key patterns, sorting and hashing, are missing.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 06:03:46 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Yelick", "Katherine", ""], ["Buluc", "Aydin", ""], ["Awan", "Muaaz", ""], ["Azad", "Ariful", ""], ["Brock", "Benjamin", ""], ["Egan", "Rob", ""], ["Ekanayake", "Saliya", ""], ["Ellis", "Marquita", ""], ["Georganas", "Evangelos", ""], ["Guidi", "Giulia", ""], ["Hofmeyr", "Steven", ""], ["Selvitopi", "Oguz", ""], ["Teodoropol", "Cristina", ""], ["Oliker", "Leonid", ""]]}, {"id": "2001.07000", "submitter": "Yibin Xu", "authors": "Yibin Xu and Yangyu Huang", "title": "Contract-connection:An efficient communication protocol for Distributed\n  Ledger Technology", "comments": null, "journal-ref": "2019 IEEE 38th International Performance Computing and\n  Communications Conference (IPCCC)", "doi": "10.1109/IPCCC47392.2019.8958730", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Ledger Technology (DLT) is promising to become the foundation of\nmany decentralised systems. However, the unbalanced and unregulated network\nlayout contributes to the inefficiency of DLT especially in the Internet of\nThings (IoT) environments, where nodes connect to only a limited number of\npeers. The data communication speed globally is unbalanced and does not live up\nto the constraints of efficient real-time distributed systems. In this paper,\nwe introduce a new communication protocol, which enables nodes to calculate the\ntradeoff between connecting/disconnecting a peer in a completely decentralised\nmanner. The network layout globally is continuously re-balancing and optimising\nalong with nodes adjusting their peers. This communication protocol weakened\nthe inequality of the communication network. The experiment suggests this\ncommunication protocol is stable and efficient.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 07:23:02 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Xu", "Yibin", ""], ["Huang", "Yangyu", ""]]}, {"id": "2001.07016", "submitter": "Doriane Perard", "authors": "Doriane Perard, Lucas Gicquel, J\\'er\\^ome Lacan", "title": "BlockHouse: Blockchain-based Distributed Storehouse System", "comments": "Published in \"9TH Latin-American Symposium on Dependable Computing\",\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper BlockHouse, a decentralized/P2P storage system fully\nbased on private blockchains. Each participant can rent his unused storage in\norder to host data of other members. This system uses a dual Smart Contract and\nProof of Retrievability system to automatically check at a fixed frequency if\nthe file is still hosted. In addition to transparency, the blockchain allows a\nbetter integration with all payments associated to this type of system (\nregular payments, sequestration to ensure good behaviors of users, ...). Except\nthe data transferred between the client and the server, all the actions go\nthrough a smart contract in the blockchain in order to log, pay and secure the\nentire storage process.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 08:38:07 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Perard", "Doriane", ""], ["Gicquel", "Lucas", ""], ["Lacan", "J\u00e9r\u00f4me", ""]]}, {"id": "2001.07022", "submitter": "Dongfang Zhao", "authors": "Abdullah Al-Mamun and Dongfang Zhao", "title": "BAASH: Enabling Blockchain-as-a-Service on High-Performance Computing\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art approach to manage blockchains is to process blocks of\ntransactions in a shared-nothing environment. Although blockchains have the\npotential to provide various services for high-performance computing (HPC)\nsystems, HPC will not be able to embrace blockchains before the following two\nmissing pieces become available: (i) new consensus protocols being aware of the\nshared-storage architecture in HPC, and (ii) new fault-tolerant mechanisms\ncompensating for HPC's programming model---the message passing interface\n(MPI)---that is vulnerable for blockchain-like workloads. To this end, we\ndesign a new set of consensus protocols crafted for the HPC platforms and a new\nfault-tolerance subsystem compensating for the failures caused by faulty MPI\nprocesses. Built on top of the new protocols and fault-tolerance mechanism, a\nprototype system is implemented and evaluated with two million transactions on\na 500-core HPC cluster, showing $6\\times$, $12\\times$, and $75\\times$ higher\nthroughput than Hyperldeger, Ethereum, and Parity, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 08:49:41 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Al-Mamun", "Abdullah", ""], ["Zhao", "Dongfang", ""]]}, {"id": "2001.07023", "submitter": "Yibin Xu", "authors": "Yibin Xu and Yangyu Huang", "title": "Segment blockchain: A size reduced storage mechanism for blockchain", "comments": null, "journal-ref": "IEEE Access,2020. https://ieeexplore.ieee.org/document/8957450/", "doi": "10.1109/ACCESS.2020.2966464", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth of the blockchain size has become a major contributing\nfactor that hinders the decentralisation of blockchain and its potential\nimplementations in data-heavy applications. In this paper, we propose segment\nblockchain, an approach that segmentises blockchain and enables nodes to only\nstore a copy of one blockchain segment. We use \\emph{PoW} as a membership\nthreshold to limit the number of nodes taken by an Adversary---the Adversary\ncan only gain at most $n/2$ of nodes in a network of $n$ nodes when it has\n$50\\%$ of the calculation power in the system (the Nakamoto blockchain security\nthreshold). A segment blockchain system fails when an Adversary stores all\ncopies of a segment, because the Adversary can then leave the system, causing a\npermanent loss of the segment. We theoretically prove that segment blockchain\ncan sustain a $(AD/n)^m$ failure probability when the Adversary has no more\nthan $AD$ number of nodes and every segment is stored by $m$ number of nodes.\nThe storage requirement is mostly shrunken compared to the traditional design\nand therefore making the blockchain more suitable for data-heavy applications.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 08:54:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Xu", "Yibin", ""], ["Huang", "Yangyu", ""]]}, {"id": "2001.07077", "submitter": "Ahmad Alhilal", "authors": "Ahmad Alhilal, Tristan Braud, and Pan Hui", "title": "Distributed Vehicular Computing at the Dawn of 5G: a Survey", "comments": "34 pages, 10 figures, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in information technology have revolutionized the automotive\nindustry, paving the way for next-generation smart and connected vehicles.\nConnected vehicles can collaborate to deliver novel services and applications.\nThese services and applications require 1) massive volumes of data that\nperceive ambient environments, 2) ultra-reliable and low-latency communication\nnetworks, 3) real-time data processing which provides decision support under\napplication-specific constraints. Addressing such constraints introduces\nsignificant challenges with current communication and computation technologies.\nCoincidentally, the fifth generation of cellular networks (5G) was developed to\nrespond to communication challenges by providing an infrastructure for\nlow-latency, high-reliability, and high bandwidth communication. At the core of\nthis infrastructure, edge computing allows data offloading and computation at\nthe edge of the network, ensuring low-latency and context-awareness, and\npushing the utilization efficiency of 5G to its limit. In this paper, we aim at\nproviding a comprehensive overview of the state of research on vehicular\ncomputing in the emerging age of 5G. After reviewing the main vehicular\napplications requirements and challenges, we follow a bottom-up approach,\nstarting with the promising technologies for vehicular communications, all the\nway up to Artificial Intelligence (AI) solutions. We explore the various\narchitectures for vehicular computing, including centralized Cloud Computing,\nVehicular Cloud Computing, and Vehicular Edge computing, and investigate the\npotential data analytics technologies and their integration on top of the\nvehicular computing architectures. We finally discuss several future research\ndirections and applications for vehicular computation systems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 12:18:18 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Alhilal", "Ahmad", ""], ["Braud", "Tristan", ""], ["Hui", "Pan", ""]]}, {"id": "2001.07086", "submitter": "Ruben Mayer", "authors": "Ruben Mayer and Kamil Orujzade and Hans-Arno Jacobsen", "title": "2PS: High-Quality Edge Partitioning with Two-Phase Streaming", "comments": "in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph partitioning is an important preprocessing step to distributed graph\nprocessing. In edge partitioning, the edge set of a given graph is split into\n$k$ equally-sized partitions, such that the replication of vertices across\npartitions is minimized. Streaming is a viable approach to partition graphs\nthat exceed the memory capacities of a single server. The graph is ingested as\na stream of edges, and one edge at a time is immediately and irrevocably\nassigned to a partition based on a scoring function. However, streaming\npartitioning suffers from the uninformed assignment problem: At the time of\npartitioning early edges in the stream, there is no information available about\nthe rest of the edges. As a consequence, edge assignments are often driven by\nbalancing considerations, and the achieved replication factor is comparably\nhigh. In this paper, we propose 2PS, a novel two-phase streaming algorithm for\nhigh-quality edge partitioning. In the first phase, vertices are separated into\nclusters by a lightweight streaming clustering algorithm. In the second phase,\nthe graph is re-streamed and edge partitioning is performed while taking into\naccount the clustering of the vertices from the first phase. Our evaluations\nshow that 2PS can achieve a replication factor that is comparable to\nheavy-weight random access partitioners while inducing orders of magnitude\nlower memory overhead.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 12:52:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mayer", "Ruben", ""], ["Orujzade", "Kamil", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "2001.07091", "submitter": "Md Sadek Ferdous", "authors": "Md Sadek Ferdous, Mohammad Jabed Morshed Chowdhury, Mohammad A. Hoque\n  and Alan Colman", "title": "Blockchain Consensus Algorithms: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, blockchain technology has received unparalleled attention\nfrom academia, industry, and governments all around the world. It is considered\na technological breakthrough anticipated to disrupt several application\ndomains. This has resulted in a plethora of blockchain systems for various\npurposes. However, many of these blockchain systems suffer from serious\nshortcomings related to their performance and security, which need to be\naddressed before any wide-scale adoption can be achieved. A crucial component\nof any blockchain system is its underlying consensus algorithm, which in many\nways, determines its performance and security. Therefore, to address the\nlimitations of different blockchain systems, several existing as well novel\nconsensus algorithms have been introduced. A systematic analysis of these\nalgorithms will help to understand how and why any particular blockchain\nperforms the way it functions. However, the existing studies of consensus\nalgorithms are not comprehensive. Those studies have incomplete discussions on\nthe properties of the algorithms and fail to analyse several major blockchain\nconsensus algorithms in terms of their scopes. This article fills this gap by\nanalysing a wide range of consensus algorithms using a comprehensive taxonomy\nof properties and by examining the implications of different issues still\nprevalent in consensus algorithms in detail. The result of the analysis is\npresented in tabular formats, which provides a visual illustration of these\nalgorithms in a meaningful way. We have also analysed more than hundred top\ncrypto-currencies belonging to different categories of consensus algorithms to\nunderstand their properties and to implicate different trends in these\ncrypto-currencies. Finally, we have presented a decision tree of algorithms to\nbe used as a tool to test the suitability of consensus algorithms under\ndifferent criteria.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:00:07 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 14:41:27 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Ferdous", "Md Sadek", ""], ["Chowdhury", "Mohammad Jabed Morshed", ""], ["Hoque", "Mohammad A.", ""], ["Colman", "Alan", ""]]}, {"id": "2001.07103", "submitter": "Claude Tadonki Dr. HDR", "authors": "Claude Tadonki", "title": "OpenMP Parallelization of Dynamic Programming and Greedy Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicore has emerged as a typical architecture model since its advent and\nstands now as a standard. The trend is to increase the number of cores and\nimprove the performance of the memory system. Providing an efficient multicore\nimplementation for a important algorithmic kernel is a genuine contribution.\nFrom a methodology standpoint, this should be done at the level of the\nunderlying paradigm if any. In this paper, we study the cases of {\\em dynamic\nprogramming} and {\\em greedy algorithms}, which are two major algorithmic\nparadigms. We exclusively consider directives-based loop parallelization\nthrough OpenMP and investigate necessary pre-transformations to reach a regular\nparallel form. We evaluate our methodology with a selection of well-known\ncombinatorial optimization problems on an INTEL Broadwell processor. Key points\nfor scalability are discussed before and after experimental results. Our\nimmediate perspective is to extend our study to the manycore case, with a\nspecial focus on NUMA configurations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:37:11 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tadonki", "Claude", ""]]}, {"id": "2001.07104", "submitter": "Holger Fr\\\"oning", "authors": "Lorenz Braun, Sotirios Nikas, Chen Song, Vincent Heuveline, Holger\n  Fr\\\"oning", "title": "A Simple Model for Portable and Fast Prediction of Execution Time and\n  Power Consumption of GPU Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing compute kernel execution behavior on GPUs for efficient task\nscheduling is a non-trivial task. We address this with a simple model enabling\nportable and fast predictions among different GPUs using only\nhardware-independent features. This model is built based on random forests\nusing 189 individual compute kernels from benchmarks such as Parboil, Rodinia,\nPolybench-GPU and SHOC. Evaluation of the model performance using\ncross-validation yields a median Mean Average Percentage Error (MAPE) of\n8.86-52.00% and 1.84-2.94%, for time respectively power prediction across five\ndifferent GPUs, while latency for a single prediction varies between 15 and 108\nmilliseconds.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:40:54 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 12:15:51 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 12:47:57 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Braun", "Lorenz", ""], ["Nikas", "Sotirios", ""], ["Song", "Chen", ""], ["Heuveline", "Vincent", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2001.07134", "submitter": "Christian Schulz", "authors": "Marcelo Fonseca Faraj, Alexander van der Grinten, Henning Meyerhenke,\n  Jesper Larsson Tr\\\"aff, and Christian Schulz", "title": "High-Quality Hierarchical Process Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioning graphs into blocks of roughly equal size such that few edges run\nbetween blocks is a frequently needed operation when processing graphs on a\nparallel computer. When a topology of a distributed system is known an\nimportant task is then to map the blocks of the partition onto the processors\nsuch that the overall communication cost is reduced. We present novel\nmultilevel algorithms that integrate graph partitioning and process mapping.\nImportant ingredients of our algorithm include fast label propagation, more\nlocalized local search, initial partitioning, as well as a compressed data\nstructure to compute processor distances without storing a distance matrix.\nExperiments indicate that our algorithms speed up the overall mapping process\nand, due to the integrated multilevel approach, also find much better solutions\nin practice. For example, one configuration of our algorithm yields better\nsolutions than the previous state-of-the-art in terms of mapping quality while\nbeing a factor 62 faster. Compared to the currently fastest iterated multilevel\nmapping algorithm Scotch, we obtain 16% better solutions while investing\nslightly more running time.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 15:05:05 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 12:02:09 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Faraj", "Marcelo Fonseca", ""], ["van der Grinten", "Alexander", ""], ["Meyerhenke", "Henning", ""], ["Tr\u00e4ff", "Jesper Larsson", ""], ["Schulz", "Christian", ""]]}, {"id": "2001.07158", "submitter": "Suhas Thejaswi", "authors": "Suhas Thejaswi, Aristides Gionis, Juho Lauri", "title": "Finding path motifs in large temporal graphs using algebraic\n  fingerprints", "comments": "version prior to peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of pattern-detection problems in vertex-colored temporal\ngraphs. In particular, given a vertex-colored temporal graph and a multiset of\ncolors as a query, we search for temporal paths in the graph that contain the\ncolors specified in the query. These types of problems have several\napplications, for example in recommending tours for tourists or detecting\nabnormal behavior in a network of financial transactions. For the family of\npattern-detection problems we consider, we establish complexity results and\ndesign an algebraic-algorithmic framework based on constrained multilinear\nsieving. We demonstrate that our solution scales to massive graphs with up to a\nbillion edges for a multiset query with five colors and up to hundred million\nedges for a multiset query with ten colors, despite the problems being NP-hard.\nOur implementation, which is publicly available, exhibits practical edge-linear\nscalability and is highly optimized. For instance, in a real-world graph\ndataset with more than six million edges and a multiset query with ten colors,\nwe can extract an optimum solution in less than eight minutes on a Haswell\ndesktop with four cores.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 16:13:27 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 04:31:40 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 13:18:08 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 14:40:49 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Thejaswi", "Suhas", ""], ["Gionis", "Aristides", ""], ["Lauri", "Juho", ""]]}, {"id": "2001.07227", "submitter": "Burak Hasircioglu", "authors": "Burak Hasircioglu, Jesus Gomez-Vilardebo, and Deniz Gunduz", "title": "Bivariate Polynomial Coding for Efficient Distributed Matrix\n  Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded computing is an effective technique to mitigate \"stragglers\" in\nlarge-scale and distributed matrix multiplication. In particular, univariate\npolynomial codes have been shown to be effective in straggler mitigation by\nmaking the task completion time to depend only on the fastest workers. However,\nthese schemes completely ignore the work done by the straggling workers\nresulting in a waste of computational resources. To reduce the amount of work\nleft unfinished at workers, one can further decompose the matrix multiplication\ntask into smaller sub-tasks, and assign multiple sub-tasks to each worker,\npossibly heterogeneously, to better fit their particular storage and\ncomputation capacities. In this work, we propose bivariate polynomial codes to\nefficiently exploit the work carried out by straggling workers. We show that\nbivariate polynomial codes bring significant advantages in terms of upload\ncommunication costs and storage efficiency, measured in terms of number of\nsub-tasks that can be computed per worker. We propose two bivariate polynomial\ncoding schemes. The first one exploits the fact that bivariate interpolation is\nalways possible on a rectangular grid of evaluation points. We obtain such\npoints at the cost of adding some redundant computations. For the second\nscheme, we relax the decoding constraints, and require decodability for almost\nall choices of the evaluation points. We present interpolation sets satisfying\nthe such decodability conditions for certain storage configurations of workers.\nOur numerical results show that bivariate polynomial coding considerably\nreduces the completion time of distributed matrix multiplication. We believe\nthat this work opens up a new class of previously unexplored coding schemes for\nefficient coded distributed computation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 19:26:28 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 21:30:55 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Hasircioglu", "Burak", ""], ["Gomez-Vilardebo", "Jesus", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2001.07276", "submitter": "Yong Yang", "authors": "Yong Yang, Long Wang, Jing Gu, Ying Li", "title": "Transparently Capturing Request Execution Path for Anomaly Detection", "comments": "13pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing scale and complexity of cloud systems and big data\nanalytics platforms, it is becoming more and more challenging to understand and\ndiagnose the processing of a service request in such distributed platforms. One\nway that helps to deal with this problem is to capture the complete end-to-end\nexecution path of service requests among all involved components accurately.\nThis paper presents REPTrace, a generic methodology for capturing such\nexecution paths in a transparent fashion. We analyze a comprehensive list of\nexecution scenarios, and propose principles and algorithms for generating the\nend-to-end request execution path for all the scenarios. Moreover, this paper\npresents an anomaly detection approach exploiting request execution paths to\ndetect anomalies of the execution during request processing. The experiments on\nfour popular distributed platforms with different workloads show that REPTrace\ncan transparently capture the accurate request execution path with reasonable\nlatency and negligible network overhead. Fault injection experiments show that\nexecution anomalies are detected with high recall (96%).\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 23:01:53 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 03:29:05 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yang", "Yong", ""], ["Wang", "Long", ""], ["Gu", "Jing", ""], ["Li", "Ying", ""]]}, {"id": "2001.07297", "submitter": "Saraju Mohanty", "authors": "Deepak Puthal and Saraju P. Mohanty and Venkata P. Yanambaka and Elias\n  Kougianos", "title": "PoAh: A Novel Consensus Algorithm for Fast Scalable Private Blockchain\n  for Large-scale IoT Frameworks", "comments": "26 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's connected world, resource constrained devices are deployed for\nsensing and decision making applications, ranging from smart cities to\nenvironmental monitoring. Those recourse constrained devices are connected to\ncreate real-time distributed networks popularly known as the Internet of Things\n(IoT), fog computing and edge computing. The blockchain is gaining a lot of\ninterest in these domains to secure the system by ignoring centralized\ndependencies, where proof-of-work (PoW) plays a vital role to make the whole\nsecurity solution decentralized. Due to the resource limitations of the\ndevices, PoW is not suitable for blockchain-based security solutions. This\npaper presents a novel consensus algorithm called Proof-of-Authentication\n(PoAh), which introduces a cryptographic authentication mechanism to replace\nPoW for resource constrained devices, and to make the blockchain\napplication-specific. PoAh is thus suitable for private as well as permissioned\nblockchains. Further, PoAh not only secures the systems, but also maintains\nsystem sustainability and scalability. The proposed consensus algorithm is\nevaluated theoretically in simulation scenarios, and in real-time hardware\ntestbeds to validate its performance. Finally, PoAh and its integration with\nthe blockchain in the IoT and edge computing scenarios is discussed. The\nproposed PoAh, while running in limited computer resources (e.g. single-board\ncomputing devices like the Raspberry Pi) has a latency in the order of 3 secs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 00:36:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Puthal", "Deepak", ""], ["Mohanty", "Saraju P.", ""], ["Yanambaka", "Venkata P.", ""], ["Kougianos", "Elias", ""]]}, {"id": "2001.07463", "submitter": "Benedek Rozemberczki", "authors": "Benedek Rozemberczki and Rik Sarkar", "title": "Fast Sequence-Based Embedding with Diffusion Graphs", "comments": "Source code available at:\n  https://github.com/benedekrozemberczki/diff2vec", "journal-ref": "CompleNet 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A graph embedding is a representation of graph vertices in a low-dimensional\nspace, which approximately preserves properties such as distances between\nnodes. Vertex sequence-based embedding procedures use features extracted from\nlinear sequences of nodes to create embeddings using a neural network. In this\npaper, we propose diffusion graphs as a method to rapidly generate vertex\nsequences for network embedding. Its computational efficiency is superior to\nprevious methods due to simpler sequence generation, and it produces more\naccurate results. In experiments, we found that the performance relative to\nother methods improves with increasing edge density in the graph. In a\ncommunity detection task, clustering nodes in the embedding space produces\nbetter results compared to other sequence-based embedding methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:04:21 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Rozemberczki", "Benedek", ""], ["Sarkar", "Rik", ""]]}, {"id": "2001.07490", "submitter": "Vipul Gupta", "authors": "Vipul Gupta, Dominic Carrano, Yaoqing Yang, Vaishaal Shankar, Thomas\n  Courtade, Kannan Ramchandran", "title": "Serverless Straggler Mitigation using Local Error-Correcting Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inexpensive cloud services, such as serverless computing, are often\nvulnerable to straggling nodes that increase end-to-end latency for distributed\ncomputation. We propose and implement simple yet principled approaches for\nstraggler mitigation in serverless systems for matrix multiplication and\nevaluate them on several common applications from machine learning and\nhigh-performance computing. The proposed schemes are inspired by\nerror-correcting codes and employ parallel encoding and decoding over the data\nstored in the cloud using serverless workers. This creates a fully distributed\ncomputing framework without using a master node to conduct encoding or\ndecoding, which removes the computation, communication and storage bottleneck\nat the master. On the theory side, we establish that our proposed scheme is\nasymptotically optimal in terms of decoding time and provide a lower bound on\nthe number of stragglers it can tolerate with high probability. Through\nextensive experiments, we show that our scheme outperforms existing schemes\nsuch as speculative execution and other coding theoretic methods by at least\n25%.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:54:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Gupta", "Vipul", ""], ["Carrano", "Dominic", ""], ["Yang", "Yaoqing", ""], ["Shankar", "Vaishaal", ""], ["Courtade", "Thomas", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "2001.07496", "submitter": "Mahieddine Djoudi", "authors": "Kemchi Sofiane, Abdelhafid Zitouni (LIRE), Mahieddine Djoudi (TECHN\\'E\n  - EA 6316)", "title": "Self Organization Agent Oriented Dynamic Resource Allocation on Open\n  Federated Clouds Environment", "comments": null, "journal-ref": "International Conference on Cloud Computing Technologies and\n  Applications, CLOUDTECH 2016, May 2016, Marrakech, Morocco", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure uninterrupted services to the cloud clients from federated cloud\nproviders, it is important to guarantee an efficient allocation of the cloud\nresources to users to improve the rate of client satisfaction and the quality\nof the service provisions. It is better to get as more computing and storage\nresources as possible. In cloud domain several Multi Agent Resource Allocation\nmethods have been proposed to implement the problem of dynamic resource\nallocation. However the problem is still open and many works to do in this\nfield. In cloud computing robustness is important so in this paper we focus on\nauto-adaptive method to deal with changes of open federated cloud computing\nenvironment. Our approach is hybrid, we first adopt an existing organizations\noptimization approach for self organization in broker agent organization to\ncombine it with already existing Multi Agent Resource Allocation approach on\nFederated Clouds. We consider an open clouds federation environment which is\ndynamic and in constant evolution, new cloud operators can join the federation\nor leave this one. At the same time our approach is multi criterion which can\ntake in account various parameters (i.e. computing load balance of mediator\nagent, geographical distance (network delay) between costumer and provider...).\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 13:05:46 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Sofiane", "Kemchi", "", "LIRE"], ["Zitouni", "Abdelhafid", "", "LIRE"], ["Djoudi", "Mahieddine", "", "TECHN\u00c9\n  - EA 6316"]]}, {"id": "2001.07497", "submitter": "Carla Mouradian", "authors": "Carla Mouradian, Fereshteh Ebrahimnezhad, Yassine Jebbar, Jasmeen Kaur\n  Ahluwalia, Seyedeh Negar Afrasiabi, Roch H. Glitho, Ashok Moghe", "title": "An IoT Platform-as-a-service for NFV Based -- Hybrid Cloud / Fog Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing, despite its inherent advantages (e.g., resource efficiency)\nstill faces several challenges. the wide are network used to connect the cloud\nto end-users could cause high latency, which may not be tolerable for some\napplications, especially Internet of Things (IoT applications. Fog computing\ncan reduce this latency by extending the traditional cloud architecture to the\nedge of the network and by enabling the deployment of some application\ncomponents on fog nodes. Application providers use Platform-as-a-Service (PaaS)\nto provision (i.e., develop, deploy, manage, and orchestrate) applications in\ncloud. However, existing PaaS solutions (including IoT PaaS) usually focus on\ncloud and do not enable provisioning of applications with components spanning\ncloud and fog. provisioning such applications require novel functions, such as\napplication graph generation, that are absent from existing PaaS. Furthermore,\nseveral functions offered by existing PaaS (e.g., publication/discovery) need\nto be significantly extended in order to fit in a hybrid cloud/fog environment.\nIn this paper, we propose a novel architecture for PaaS for hybrid cloud/fog\nsystem. It is IoT use case-driven, and its applications' components are\nimplemented as Virtual Network Functions (VNFs) with execution sequences\nmodeled s graphs with sub-structures such as selection and loops. It automates\nthe provisioning of applications with components spanning cloud and fog. In\naddition, it enables the discovery of existing cloud and fog nodes and\ngenerates application graphs. A proof of concept is built based on Cloudify\nopen source. Feasibility is demonstrated by evaluating its performance when\nPaaS modules and application components are placed in clouds and fogs in\ndifferent geographical locations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:42:02 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mouradian", "Carla", ""], ["Ebrahimnezhad", "Fereshteh", ""], ["Jebbar", "Yassine", ""], ["Ahluwalia", "Jasmeen Kaur", ""], ["Afrasiabi", "Seyedeh Negar", ""], ["Glitho", "Roch H.", ""], ["Moghe", "Ashok", ""]]}, {"id": "2001.07504", "submitter": "Sophie Chabridon", "authors": "Nicolas Aussel (INF, ACMES-SAMOVAR, IP Paris), Sophie Chabridon (IP\n  Paris, INF, ACMES-SAMOVAR), Yohan Petetin (TIPIC-SAMOVAR, CITI, IP Paris)", "title": "Combining Federated and Active Learning for Communication-efficient\n  Distributed Failure Prediction in Aeronautics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning has proven useful in the recent years as a way to achieve\nfailure prediction for industrial systems. However, the high computational\nresources necessary to run learning algorithms are an obstacle to its\nwidespread application. The sub-field of Distributed Learning offers a solution\nto this problem by enabling the use of remote resources but at the expense of\nintroducing communication costs in the application that are not always\nacceptable. In this paper, we propose a distributed learning approach able to\noptimize the use of computational and communication resources to achieve\nexcellent learning model performances through a centralized architecture. To\nachieve this, we present a new centralized distributed learning algorithm that\nrelies on the learning paradigms of Active Learning and Federated Learning to\noffer a communication-efficient method that offers guarantees of model\nprecision on both the clients and the central server. We evaluate this method\non a public benchmark and show that its performances in terms of precision are\nvery close to state-of-the-art performance level of non-distributed learning\ndespite additional constraints.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 13:17:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Aussel", "Nicolas", "", "INF, ACMES-SAMOVAR, IP Paris"], ["Chabridon", "Sophie", "", "IP\n  Paris, INF, ACMES-SAMOVAR"], ["Petetin", "Yohan", "", "TIPIC-SAMOVAR, CITI, IP Paris"]]}, {"id": "2001.07557", "submitter": "Tilo Wettig", "authors": "Benjamin Huth, Nils Meyer, Tilo Wettig", "title": "Lattice QCD on a novel vector architecture", "comments": "Proceedings of Lattice 2019, 7 pages, 6 colorful figures (changed\n  Fig. 6d) and its description)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SX-Aurora TSUBASA PCIe accelerator card is the newest model of NEC's SX\narchitecture family. Its multi-core vector processor features a vector length\nof 16 kbits and interfaces with up to 48 GB of HBM2 memory in the current\nmodels, available since 2018. The compute performance is up to 2.45 TFlop/s\npeak in double precision, and the memory throughput is up to 1.2 TB/s peak. New\nmodels with improved performance characteristics are announced for the near\nfuture. In this contribution we discuss key aspects of the SX-Aurora and\ndescribe how we enabled the architecture in the Grid Lattice QCD framework.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 14:18:07 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 22:12:52 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Huth", "Benjamin", ""], ["Meyer", "Nils", ""], ["Wettig", "Tilo", ""]]}, {"id": "2001.07704", "submitter": "Maxim Zakharov", "authors": "Maxim Zakharov", "title": "Asynchronous Consensus Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes a new consensus algorithm which is asynchronous and\nuses gossip based message dissemination between nodes. The current version of\nthe algorithm does not cover the case of a node failure or significantly\ndelayed response. This is the subject of further research of the algorithm. An\noutline of a new design for trust-less payment system is given in appendices.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 03:01:26 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Zakharov", "Maxim", ""]]}, {"id": "2001.07706", "submitter": "EPTCS", "authors": "Jan Olaf Blech (Aalto University)", "title": "Towards Digital Twins for the Description of Automotive Software Systems", "comments": "In Proceedings QAPL 2019, arXiv:2001.06163", "journal-ref": "EPTCS 312, 2020, pp. 20-28", "doi": "10.4204/EPTCS.312.2", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present models for automotive software that capture quantitative and\nqualitative aspects of software systems and the underlying hardware\narchitecture. In particular, we consider different levels of computing power.\nThese range from controllers up to the cloud. We present a modeling approach\nfor software deployment taking different automotive requirements such as\ncriticality, latency, memory, computational resources, and communication into\naccount. Our models capture automotive software and hardware system\nconfigurations and can serve as digital twins that are digital counterparts of\n(usually) physical entities. Furthermore, we highlight connected research areas\nand challenges.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 02:17:27 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Blech", "Jan Olaf", "", "Aalto University"]]}, {"id": "2001.07709", "submitter": "Kun He Prof.", "authors": "Kun He, Kevin Tole, Fei Ni, Yong Yuan, Linyun Liao", "title": "Adaptive Large Neighborhood Search for Circle Bin Packing Problem", "comments": "13 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a new variant of packing problem called the circle bin packing\nproblem (CBPP), which is to find a dense packing of circle items to multiple\nsquare bins so as to minimize the number of used bins. To this end, we propose\nan adaptive large neighborhood search (ALNS) algorithm, which uses our Greedy\nAlgorithm with Corner Occupying Action (GACOA) to construct an initial layout.\nThe greedy solution is usually in a local optimum trap, and ALNS enables\nmultiple neighborhood search that depends on the stochastic annealing schedule\nto avoid getting stuck in local minimum traps. Specifically, ALNS perturbs the\ncurrent layout to jump out of a local optimum by iteratively reassigns some\ncircles and accepts the new layout with some probability during the search. The\nacceptance probability is adjusted adaptively using simulated annealing that\nfine-tunes the search direction in order to reach the global optimum. We\nbenchmark computational results against GACOA in heterogeneous instances. ALNS\nalways outperforms GACOA in improving the objective function, and in several\ncases, there is a significant reduction on the number of bins used in the\npacking.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 10:14:15 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["He", "Kun", ""], ["Tole", "Kevin", ""], ["Ni", "Fei", ""], ["Yuan", "Yong", ""], ["Liao", "Linyun", ""]]}, {"id": "2001.07734", "submitter": "Angelo Capossele", "authors": "Bartosz Kusmierz, William Sanders, Andreas Penzkofer, Angelo\n  Capossele, Alon Gal", "title": "Properties of the Tangle for Uniform Random and Random Walk Tip\n  Selection", "comments": "Published in: 2019 IEEE International Conference on Blockchain\n  (Blockchain)", "journal-ref": "2019 IEEE International Conference on Blockchain (Blockchain)", "doi": "10.1109/Blockchain.2019.00037", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing number of applications for distributed ledger technologies is\ndriving both industry and academia to solve the limitations of blockchain,\nparticularly its scalability issues. Recent distributed ledger technologies\nhave replaced the blockchain linear structure with a more flexible directed\nacyclic graph in an attempt to accommodate a higher throughput. Despite the\nfast-growing diffusion of directed acyclic graph based distributed ledger\ntechnologies, researchers lack a basic understanding of their behavior. In this\npaper we analyze the Tangle, a directed acyclic graph that is used (with\ncertain modifications) in various protocols such as IOTA, Byteball, Avalanche\nor SPECTRE. Our contribution is threefold. First, we run simulations in a\ncontinuous-time model to examine tip count stability and cumulative weight\nevolution while varying the rate of incoming transactions. In particular we\nconfirm analytical predictions on the number of tips with uniform random tip\nselection strategy. Second, we show how different tip selection algorithms\naffect the growth of the Tangle. Moreover, we explain these differences by\nanalyzing the spread of exit probabilities of random walks. Our findings\nconfirm analytically derived predictions and provide novel insights on the\ndifferent phases of growth of cumulative weight as well as on the average time\ndifference for a transaction to receive its first approval when using distinct\ntip selection algorithms. Lastly, we analyze simulation overhead and\nperformance as a function of Tangle size and compare results for different tip\nselection algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 19:01:09 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Kusmierz", "Bartosz", ""], ["Sanders", "William", ""], ["Penzkofer", "Andreas", ""], ["Capossele", "Angelo", ""], ["Gal", "Alon", ""]]}, {"id": "2001.07747", "submitter": "Maciej Besta", "authors": "Robert Gerstenberger, Maciej Besta, and Torsten Hoefler", "title": "Enabling Highly-Scalable Remote Memory Access Programming with MPI-3 One\n  Sided", "comments": "Best Paper Award at ACM/IEEE Supercomputing'13 (1/92), also Best\n  Student Paper finalist (8/92); source code of foMPI can be downloaded from\n  http://spcl.inf.ethz.ch/Research/Parallel_Programming/foMPI", "journal-ref": "Proceedings of the ACM/IEEE International Conference on High\n  Performance Computing, Networking, Storage and Analysis, pages 53:1--53:12,\n  November 2013", "doi": "10.1145/2503210.2503286", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern interconnects offer remote direct memory access (RDMA) features. Yet,\nmost applications rely on explicit message passing for communications albeit\ntheir unwanted overheads. The MPI-3.0 standard defines a programming interface\nfor exploiting RDMA networks directly, however, it's scalability and\npracticability has to be demonstrated in practice. In this work, we develop\nscalable bufferless protocols that implement the MPI-3.0 specification. Our\nprotocols support scaling to millions of cores with negligible memory\nconsumption while providing highest performance and minimal overheads. To arm\nprogrammers, we provide a spectrum of performance models for all critical\nfunctions and demonstrate the usability of our library and models with several\napplication studies with up to half a million processes. We show that our\ndesign is comparable to, or better than UPC and Fortran Coarrays in terms of\nlatency, bandwidth, and message rate. We also demonstrate application\nperformance improvements with comparable programming complexity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 19:20:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 13:20:08 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gerstenberger", "Robert", ""], ["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2001.07855", "submitter": "Yu Huang", "authors": "Kaile Huang, Yu Huang, Hengfeng Wei", "title": "Fine-grained Analysis on Fast Implementations of Distributed\n  Multi-writer Atomic Registers", "comments": "v0.6, title revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed multi-writer atomic registers are at the heart of a large number\nof distributed algorithms. While enjoying the benefits of atomicity,\nresearchers further explore fast implementations of atomic reigsters which are\noptimal in terms of data access latency. Though it is proved that multi-writer\natomic register implementations are impossible when both read and write are\nrequired to be fast, it is still open whether implementations are impossible\nwhen only write or read is required to be fast. This work proves the\nimpossibility of fast write implementations based on a series of chain\narguments among indistiguishable executions. We also show the necessary and\nsufficient condition for fast read implementations by extending the results in\nthe single-writer case. This work concludes a series of studies on fast\nimplementations of distributed atomic registers.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 02:48:51 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 12:45:40 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 09:10:54 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 15:07:02 GMT"}, {"version": "v5", "created": "Thu, 18 Jun 2020 08:20:11 GMT"}, {"version": "v6", "created": "Thu, 25 Jun 2020 07:44:45 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Huang", "Kaile", ""], ["Huang", "Yu", ""], ["Wei", "Hengfeng", ""]]}, {"id": "2001.07867", "submitter": "Tyler Crain", "authors": "Tyler Crain", "title": "A Simple and Efficient Binary Byzantine Consensus Algorithm using\n  Cryptography and Partial Synchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple and efficient Binary Byzantine faulty tolerant\nconsensus algorithm using a weak round coordinator and the partial synchrony\nassumption to ensure liveness. In the algorithm, non-faulty nodes perform an\ninitial broadcast followed by a executing a series of rounds consisting of a\nsingle message broadcast until termination. Each message is accompanied by a\ncryptographic proof of its validity. In odd rounds the binary value 1 can be\ndecided, in even round 0. Up to one third of the nodes can be faulty and\ntermination is ensured within a number of round of a constant factor of the\nnumber of faults. Experiments show termination can be reached in less than 200\nmilliseconds with 300 Amazon EC2 instances spread across 5 continents even with\npartial initial disagreement.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 03:47:46 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Crain", "Tyler", ""]]}, {"id": "2001.07897", "submitter": "William Buchanan Prof", "authors": "Will Major, William J Buchanan, Jawad Ahmad", "title": "An authentication protocol based on chaos and zero knowledge proof", "comments": "J. Nonlinear Dyn (2020)", "journal-ref": null, "doi": "10.1007/s11071-020-05463-3", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Port Knocking is a method for authenticating clients through a closed stance\nfirewall, and authorising their requested actions, enabling severs to offer\nservices to authenticated clients, without opening ports on the firewall.\nAdvances in port knocking have resulted in an increase in complexity in design,\npreventing port knocking solutions from realising their potential. This paper\nproposes a novel port knocking solution, named Crucible, which is a secure\nmethod of authentication, with high usability and features of stealth, allowing\nservers and services to remain hidden and protected. Crucible is a stateless\nsolution, only requiring the client memorise a command, the server's IP and a\nchosen password. The solution is forwarded as a method for protecting servers\nagainst attacks ranging from port scans, to zero-day exploitation. To act as a\nrandom oracle for both client and server, cryptographic hashes were generated\nthrough chaotic systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 07:18:28 GMT"}], "update_date": "2020-06-14", "authors_parsed": [["Major", "Will", ""], ["Buchanan", "William J", ""], ["Ahmad", "Jawad", ""]]}, {"id": "2001.08002", "submitter": "Ayat K Fekry", "authors": "Ayat Fekry, Lucian Carata, Thomas Pasquier, Andrew Rice, Andy Hopper", "title": "Tuneful: An Online Significance-Aware Configuration Tuner for Big Data\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed analytics engines such as Spark are a common choice for\nprocessing extremely large datasets. However, finding good configurations for\nthese systems remains challenging, with each workload potentially requiring a\ndifferent setup to run optimally. Using suboptimal configurations incurs\nsignificant extra runtime costs. %Furthermore, Spark and similar platforms are\ngaining traction within data-scientists communities where awareness of such\nissues is relatively low.\n  We propose Tuneful, an approach that efficiently tunes the configuration of\nin-memory cluster computing systems. Tuneful combines incremental Sensitivity\nAnalysis and Bayesian optimization to identify near-optimal configurations from\na high-dimensional search space, using a small number of executions. This setup\nallows the tuning to be done online, without any previous training. Our\nexperimental results show that Tuneful reduces the search time for finding\nclose-to-optimal configurations by 62\\% (at the median) when compared to\nexisting state-of-the-art techniques. This means that the amortization of the\ntuning cost happens significantly faster, enabling practical tuning for new\nclasses of workloads.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 13:24:41 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Fekry", "Ayat", ""], ["Carata", "Lucian", ""], ["Pasquier", "Thomas", ""], ["Rice", "Andrew", ""], ["Hopper", "Andy", ""]]}, {"id": "2001.08102", "submitter": "Ivars Dzabls Mr", "authors": "Ivars Dzalbs, Tatiana Kalganova", "title": "Accelerating supply chains with Ant Colony Optimization across range of\n  hardware solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ant Colony algorithm has been applied to various optimization problems,\nhowever most of the previous work on scaling and parallelism focuses on\nTravelling Salesman Problems (TSPs). Although, useful for benchmarks and new\nidea comparison, the algorithmic dynamics does not always transfer to complex\nreal-life problems, where additional meta-data is required during solution\nconstruction. This paper looks at real-life outbound supply chain problem using\nAnt Colony Optimization (ACO) and its scaling dynamics with two parallel ACO\narchitectures - Independent Ant Colonies (IAC) and Parallel Ants (PA). Results\nshowed that PA was able to reach a higher solution quality in fewer iterations\nas the number of parallel instances increased. Furthermore, speed performance\nwas measured across three different hardware solutions - 16 core CPU, 68 core\nXeon Phi and up to 4 Geforce GPUs. State of the art, ACO vectorization\ntechniques such as SS-Roulette were implemented using C++ and CUDA. Although\nexcellent for TSP, it was concluded that for the given supply chain problem\nGPUs are not suitable due to meta-data access footprint required. Furthermore,\ncompared to their sequential counterpart, vectorized CPU AVX2 implementation\nachieved 25.4x speedup on CPU while Xeon Phi with its AVX512 instruction set\nreached 148x on PA with Vectorized (PAwV). PAwV is therefore able to scale at\nleast up to 1024 parallel instances on the supply chain network problem solved.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:09:06 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Dzalbs", "Ivars", ""], ["Kalganova", "Tatiana", ""]]}, {"id": "2001.08108", "submitter": "Ami Paz", "authors": "Pierluigi Crescenzi, Pierre Fraigniaud, Ami Paz", "title": "Simple and Fast Distributed Computation of Betweenness Centrality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness centrality is a graph parameter that has been successfully\napplied to network analysis. In the context of computer networks, it was\nconsidered for various objectives, ranging from routing to service placement.\nHowever, as observed by Maccari et al. [INFOCOM 2018], research on betweenness\ncentrality for improving protocols was hampered by the lack of a usable, fully\ndistributed algorithm for computing this parameter. We resolve this issue by\ndesigning an efficient algorithm for computing betweenness centrality, which\ncan be implemented by minimal modifications to any distance-vector routing\nprotocol based on Bellman-Ford. The convergence time of our implementation is\nshown to be proportional to the diameter of the network\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:11:53 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Crescenzi", "Pierluigi", ""], ["Fraigniaud", "Pierre", ""], ["Paz", "Ami", ""]]}, {"id": "2001.08154", "submitter": "Yibin Xu", "authors": "Yibin Xu and Yangyu Huang and Jianhua Shao", "title": "Anchoring the value of Cryptocurrency", "comments": null, "journal-ref": "3rd International Workshop on Blockchain Oriented Software\n  Engineering. Western University. London, Canada, February 18, 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A decade long thrive of cryptocurrency has shown its potential as a source of\nalternative-finance and the security and the robustness of the underpinning\nblockchain technology.\n  However, most cryptocurrencies fail to show inimitability and their meanings\nin the real world. As a result, they usually start off as favourites but\nquickly become the outcasts of the digital asset market.\n  The blockchain society attempts to anchor the value of cryptocurrency with\nreal values by employing smart contracts and link it with computation resources\nand the digital-productivity that have value and demands in the real world. But\ntheir attempts have some undesirable effects due to a limited number of\npractical applications. This limitation is caused by the dilemma between high\nperformance and decentralisation (universal joinability). The emerging of\nblockchain sharding models, however, has offered a possible solution to address\nthis dilemma.\n  In this paper, we explore a financial model for blockchain sharding that will\nbuild an active link between the value of cryptocurrency and computation\nresources as well as the market and labour behaviours. Our model can adjust the\nprice of resources and the compensation for maintaining a system based on those\nbehaviours. We anchor the value of cryptocurrency by the amount of computation\nresources participated in and give the cryptocurrency a meaning as the exchange\nbetween computation resources globally. Finally, we present a working example\nwhich, through financial regularities, regulates the behaviour of anonymous\nparticipants, also incents/discourages participation dynamically.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 05:22:08 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 13:53:02 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Xu", "Yibin", ""], ["Huang", "Yangyu", ""], ["Shao", "Jianhua", ""]]}, {"id": "2001.08300", "submitter": "Shiqiang Wang", "authors": "Tiffany Tuor, Shiqiang Wang, Bong Jun Ko, Changchang Liu, Kin K. Leung", "title": "Overcoming Noisy and Irrelevant Data in Federated Learning", "comments": "Accepted version in the 25th International Conference on Pattern\n  Recognition (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image and vision applications require a large amount of data for model\ntraining. Collecting all such data at a central location can be challenging due\nto data privacy and communication bandwidth restrictions. Federated learning is\nan effective way of training a machine learning model in a distributed manner\nfrom local data collected by client devices, which does not require exchanging\nthe raw data among clients. A challenge is that among the large variety of data\ncollected at each client, it is likely that only a subset is relevant for a\nlearning task while the rest of data has a negative impact on model training.\nTherefore, before starting the learning process, it is important to select the\nsubset of data that is relevant to the given federated learning task. In this\npaper, we propose a method for distributedly selecting relevant data, where we\nuse a benchmark model trained on a small benchmark dataset that is\ntask-specific, to evaluate the relevance of individual data samples at each\nclient and select the data with sufficiently high relevance. Then, each client\nonly uses the selected subset of its data in the federated learning process.\nThe effectiveness of our proposed approach is evaluated on multiple real-world\nimage datasets in a simulated system with a large number of clients, showing up\nto $25\\%$ improvement in model accuracy compared to training with all data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 22:28:47 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 02:12:29 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tuor", "Tiffany", ""], ["Wang", "Shiqiang", ""], ["Ko", "Bong Jun", ""], ["Liu", "Changchang", ""], ["Leung", "Kin K.", ""]]}, {"id": "2001.08433", "submitter": "Asad Javed", "authors": "Asad Javed, Keijo Heljanko, Andrea Buda, Kary Fr\\\"amling", "title": "CEFIoT: A Fault-Tolerant IoT Architecture for Edge and Cloud", "comments": "6 pages, 6 figures", "journal-ref": "IEEE 4th World Forum on Internet of Things (WF-IoT), Singapore,\n  5-8 February, 2018", "doi": "10.1109/WF-IoT.2018.8355149", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT), the emerging computing infrastructure that refers\nto the networked interconnection of physical objects, incorporates a plethora\nof digital systems that are being developed by means of a large number of\napplications. Many of these applications administer data collection on the edge\nand offer data storage and analytics capabilities in the cloud. This raises the\nfollowing problems: (i) the processing stages in IoT applications need to have\nseparate implementations for both the edge and the cloud, (ii) the placement of\ncomputation is inflexible with separate software stacks, as the optimal\ndeployment decisions need to be made at runtime, and (iii) unified fault\ntolerance is essential in case of intermittent long-distance network\nconnectivity problems, malicious harming of edge devices, or harsh\nenvironments. This paper proposes a novel fault-tolerant architecture CEFIoT\nfor IoT applications by adopting state-of-the-art cloud technologies and\ndeploying them also for edge computing. We solve the data fault tolerance issue\nby exploiting the Apache Kafka publish/subscribe platform as the unified\nhigh-performance data replication solution offering a common software stack for\nboth the edge and the cloud. We also deploy Kubernetes for fault-tolerant\nmanagement and the advanced functionality allowing on-the-fly automatic\nreconfiguration of the processing pipeline to handle both hardware and network\nconnectivity based failures.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 10:27:37 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Javed", "Asad", ""], ["Heljanko", "Keijo", ""], ["Buda", "Andrea", ""], ["Fr\u00e4mling", "Kary", ""]]}, {"id": "2001.08448", "submitter": "Priyank Faldu", "authors": "Priyank Faldu and Jeff Diamond and Boris Grot", "title": "A Closer Look at Lightweight Graph Reordering", "comments": "Fixed ill-formatted page 6 from the earlier version. No content\n  changes", "journal-ref": "In Proceedings of the IEEE International Symposium on Workload\n  Characterization (IISWC), Orlando, Florida, USA, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph analytics power a range of applications in areas as diverse as finance,\nnetworking and business logistics. A common property of graphs used in the\ndomain of graph analytics is a power-law distribution of vertex connectivity,\nwherein a small number of vertices are responsible for a high fraction of all\nconnections in the graph. These richly-connected (hot) vertices inherently\nexhibit high reuse. However, their sparse distribution in memory leads to a\nsevere underutilization of on-chip cache capacity. Prior works have proposed\nlightweight skew-aware vertex reordering that places hot vertices adjacent to\neach other in memory, reducing the cache footprint of hot vertices. However, in\ndoing so, they may inadvertently destroy the inherent community structure\nwithin the graph, which may negate the performance gains achieved from the\nreduced footprint of hot vertices.\n  In this work, we study existing reordering techniques and demonstrate the\ninherent tension between reducing the cache footprint of hot vertices and\npreserving original graph structure. We quantify the potential performance loss\ndue to disruption in graph structure for different graph datasets. We further\nshow that reordering techniques that employ fine-grain reordering significantly\nincrease misses in the higher level caches, even when they reduce misses in the\nlast-level cache.\n  To overcome the limitations of existing reordering techniques, we propose\nDegree-Based Grouping (DBG), a novel lightweight reordering technique that\nemploys a coarse-grain reordering to largely preserve graph structure while\nreducing the cache footprint of hot vertices. Our evaluation on 40 combinations\nof various graph applications and datasets shows that, compared to a baseline\nwith no reordering, DBG yields an average application speed-up of 16.8% vs\n11.6% for the best-performing existing lightweight technique.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 11:11:01 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 11:20:50 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Faldu", "Priyank", ""], ["Diamond", "Jeff", ""], ["Grot", "Boris", ""]]}, {"id": "2001.08510", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley", "title": "Bibliography of distributed approximation beyond bounded degree", "comments": "An annotated bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is an informal bibliography of the papers dealing with\ndistributed approximation algorithms. A classic setting for such algorithms is\nbounded degree graphs, but there is a whole set of techniques that have been\ndeveloped for other classes. These later classes are the focus of the current\nwork. These classes have a geometric nature (planar, bounded genus and\nunit-disk graphs) and/or have bounded parameters (arboricity, expansion,\ngrowth, independence) or forbidden structures (forbidden minors).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:49:41 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 07:55:44 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Feuilloley", "Laurent", ""]]}, {"id": "2001.08516", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Peter Sanders, Matthias Schimek", "title": "Communication-Efficient String Sorting", "comments": "Full version to appear at IPDPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been surprisingly little work on algorithms for sorting strings on\ndistributed-memory parallel machines. We develop efficient algorithms for this\nproblem based on the multi-way merging principle. These algorithms inspect only\ncharacters that are needed to determine the sorting order. Moreover,\ncommunication volume is reduced by also communicating (roughly) only those\ncharacters and by communicating repetitions of the same prefixes only once.\nExperiments on up to 1280 cores reveal that these algorithm are often more than\nfive times faster than previous algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 13:57:47 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Bingmann", "Timo", ""], ["Sanders", "Peter", ""], ["Schimek", "Matthias", ""]]}, {"id": "2001.08529", "submitter": "Mustafa Ozdayi", "authors": "Mustafa Safa Ozdayi, Murat Kantarcioglu, and Bradley Malin", "title": "Leveraging Blockchain for Immutable Logging and Querying Across Multiple\n  Sites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has emerged as a decentralized and distributed framework that\nenables tamper-resilience and, thus, practical immutability for stored data.\nThis immutability property is important in scenarios where auditability is\ndesired, such as in maintaining access logs for sensitive healthcare and\nbiomedical data.However, the underlying data structure of blockchain, by\ndefault, does not provide capabilities to efficiently query the stored data. In\nthis investigation, we show that it is possible to efficiently run complex\naudit queries over the access log data stored on blockchains by using\nadditional key-value stores. This paper specifically reports on the approach we\ndesigned for the blockchain track of iDASH Privacy & Security Workshop 2018\ncompetition.Particularly, we implemented our solution and compared its loading\nand query-response performance with SQLite, a commonly used relational\ndatabase, using the data provided by the iDASH 2018 organizers. Depending on\nthe query type and the data size, the run time difference between blockchain\nbased query-response and SQLite based query-response ranged from 0.2 seconds to\n6 seconds. A deeper inspection revealed that range queries were the bottleneck\nof our solution which, nevertheless, scales up linearly. Concretely, this\ninvestigation demonstrates that blockchain-based systems can provide reasonable\nquery-response times to complex queries even if they only use simple key-value\nstores to manage their data. Consequently, we show that blockchains may be\nuseful for maintaining data with auditability and immutability requirements\nacross multiple sites.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 22:23:43 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 20:29:10 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Ozdayi", "Mustafa Safa", ""], ["Kantarcioglu", "Murat", ""], ["Malin", "Bradley", ""]]}, {"id": "2001.08557", "submitter": "Marco Zimmerling", "authors": "Marco Zimmerling, Luca Mottola, Silvia Santini", "title": "Synchronous Transmissions in Low-Power Wireless: A Survey of\n  Communication Protocols and Network Services", "comments": "Submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-power wireless communication is a central building block of\nCyber-physical Systems and the Internet of Things. Conventional low-power\nwireless protocols make avoiding packet collisions a cornerstone design choice.\nThe concept of synchronous transmissions challenges this view. As collisions\nare not necessarily destructive, under specific circumstances, commodity\nlow-power wireless radios are often able to receive useful information even in\nthe presence of superimposed signals from different transmitters. We survey the\ngrowing number of protocols that exploit synchronous transmissions for higher\nrobustness and efficiency as well as unprecedented functionality and\nversatility compared to conventional designs. The illustration of protocols\nbased on synchronous transmissions is cast in a conceptional framework we\nestablish, with the goal of highlighting differences and similarities among the\nproposed solutions. We conclude the paper with a discussion on open research\nquestions in this field.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 14:34:55 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 09:10:53 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Zimmerling", "Marco", ""], ["Mottola", "Luca", ""], ["Santini", "Silvia", ""]]}, {"id": "2001.08720", "submitter": "Chien-Sheng Yang", "authors": "Chien-Sheng Yang and A. Salman Avestimehr", "title": "Coded Computing for Secure Boolean Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing size of modern datasets necessitates splitting a large scale\ncomputation into smaller computations and operate in a distributed manner.\nAdversaries in a distributed system deliberately send erroneous data in order\nto affect the computation for their benefit. Boolean functions are the key\ncomponents of many applications, e.g., verification functions in blockchain\nsystems and design of cryptographic algorithms. We consider the problem of\ncomputing a Boolean function in a distributed computing system with particular\nfocus on \\emph{security against Byzantine workers}. Any Boolean function can be\nmodeled as a multivariate polynomial with high degree in general. However, the\nsecurity threshold (i.e., the maximum number of adversarial workers can be\ntolerated such that the correct results can be obtained) provided by the recent\nproposed Lagrange Coded Computing (LCC) can be extremely low if the degree of\nthe polynomial is high. We propose three different schemes called \\emph{coded\nAlgebraic normal form (ANF)}, \\emph{coded Disjunctive normal form (DNF)} and\n\\emph{coded polynomial threshold function (PTF)}. The key idea of the proposed\nschemes is to model it as the concatenation of some low-degree polynomials and\nthreshold functions. In terms of the security threshold, we show that the\nproposed coded ANF and coded DNF are optimal by providing a matching outer\nbound.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:28:08 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 14:40:20 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yang", "Chien-Sheng", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2001.08877", "submitter": "Hongji Wei", "authors": "T. Tony Cai, Hongji Wei", "title": "Distributed Gaussian Mean Estimation under Communication Constraints:\n  Optimal Rates and Communication-Efficient Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DC cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed estimation of a Gaussian mean under communication\nconstraints in a decision theoretical framework. Minimax rates of convergence,\nwhich characterize the tradeoff between the communication costs and statistical\naccuracy, are established in both the univariate and multivariate settings.\nCommunication-efficient and statistically optimal procedures are developed. In\nthe univariate case, the optimal rate depends only on the total communication\nbudget, so long as each local machine has at least one bit. However, in the\nmultivariate case, the minimax rate depends on the specific allocations of the\ncommunication budgets among the local machines.\n  Although optimal estimation of a Gaussian mean is relatively simple in the\nconventional setting, it is quite involved under the communication constraints,\nboth in terms of the optimal procedure design and lower bound argument. The\ntechniques developed in this paper can be of independent interest. An essential\nstep is the decomposition of the minimax estimation problem into two stages,\nlocalization and refinement. This critical decomposition provides a framework\nfor both the lower bound analysis and optimal procedure design.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 04:19:47 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Cai", "T. Tony", ""], ["Wei", "Hongji", ""]]}, {"id": "2001.08981", "submitter": "Amir Rastegarnia", "authors": "Vahid Vahidpour, Amir Rastegarnia, Azam Khalili, Wael M. Bazzi, Saeid\n  Sanei", "title": "Variants of Partial Update Augmented CLMS Algorithm and Their\n  Performance Analysis", "comments": "6", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturally complex-valued information or those presented in complex domain are\neffectively processed by an augmented complex least-mean-square (ACLMS)\nalgorithm. In some applications, the ACLMS algorithm may be too\ncomputationally- and memory-intensive to implement. In this paper, a new\nalgorithm, termed partial-update ACLMS (PU-ACLMS) algorithm is proposed, where\nonly a fraction of the coefficient set is selected to update at each iteration.\nDoing so, two types of partial-update schemes are presented referred to as the\nsequential and stochastic partial-updates, to reduce computational load and\npower consumption in the corresponding adaptive filter. The computational cost\nfor full-update PU-ACLMS and its partial-update implementations are discussed.\nNext, the steady-state mean and mean-square performance of PU-ACLMS for\nnon-circular complex signals are analyzed and closed-form expressions of the\nsteady-state excess mean-square error (EMSE) and mean-square deviation (MSD)\nare given. Then, employing the weighted energy-conservation relation, the EMSE\nand MSD learning curves are derived. The simulation results are verified and\ncompared with those of theoretical predictions through numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:06:29 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Vahidpour", "Vahid", ""], ["Rastegarnia", "Amir", ""], ["Khalili", "Azam", ""], ["Bazzi", "Wael M.", ""], ["Sanei", "Saeid", ""]]}, {"id": "2001.09011", "submitter": "Vijay Arya", "authors": "Nishant Baranwal Somy, Kalapriya Kannan, Vijay Arya, Sandeep Hans,\n  Abhishek Singh, Pranay Lohia, Sameep Mehta", "title": "Ownership preserving AI Market Places using Blockchain", "comments": null, "journal-ref": "IEEE International Conference on Blockchain, Blockchain 2019", "doi": "10.1109/Blockchain.2019.00029", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a blockchain based system that allows data owners, cloud vendors,\nand AI developers to collaboratively train machine learning models in a\ntrustless AI marketplace. Data is a highly valued digital asset and central to\nderiving business insights. Our system enables data owners to retain ownership\nand privacy of their data, while still allowing AI developers to leverage the\ndata for training. Similarly, AI developers can utilize compute resources from\ncloud vendors without loosing ownership or privacy of their trained models. Our\nsystem protocols are set up to incentivize all three entities - data owners,\ncloud vendors, and AI developers to truthfully record their actions on the\ndistributed ledger, so that the blockchain system provides verifiable evidence\nof wrongdoing and dispute resolution. Our system is implemented on the\nHyperledger Fabric and can provide a viable alternative to centralized AI\nsystems that do not guarantee data or model privacy. We present experimental\nperformance results that demonstrate the latency and throughput of its\ntransactions under different network configurations where peers on the\nblockchain may be spread across different datacenters and geographies. Our\nresults indicate that the proposed solution scales well to large number of data\nand model owners and can train up to 70 models per second on a 12-peer non\noptimized blockchain network and roughly 30 models per second in a 24 peer\nnetwork.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 14:47:28 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Somy", "Nishant Baranwal", ""], ["Kannan", "Kalapriya", ""], ["Arya", "Vijay", ""], ["Hans", "Sandeep", ""], ["Singh", "Abhishek", ""], ["Lohia", "Pranay", ""], ["Mehta", "Sameep", ""]]}, {"id": "2001.09018", "submitter": "Gabriele D'Angelo", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo", "title": "Are Distributed Ledger Technologies Ready for Smart Transportation\n  Systems?", "comments": "Proceedings of the 3rd Workshop on Cryptocurrencies and Blockchains\n  for Distributed Systems (CryBlock 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to understand whether Distributed Ledger\nTechnologies (DLTs) are ready to support complex services, such as those\nrelated to Intelligent Transportation Systems (ITS). In smart transportation\nservices, a huge amount of sensed data is generated by a multitude of vehicles.\nWhile DLTs provide very interesting features, such as immutability,\ntraceability and verifiability of data, some doubts on the scalability and\nresponsiveness of these technologies appear to be well-founded. We propose an\narchitecture for ITS that resorts to DLT features. Moreover, we provide\nexperimental results of a real test-bed over IOTA, a promising DLT for IoT.\nResults clearly show that, while the viability of the proposal cannot be\nrejected, further work is needed on the responsiveness of DLT infrastructures.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 10:32:07 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 12:14:48 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "2001.09070", "submitter": "Blesson Varghese", "authors": "Arkadiusz Madej and Nan Wang and Nikolaos Athanasopoulos and Rajiv\n  Ranjan and Blesson Varghese", "title": "Priority-based Fair Scheduling in Edge Computing", "comments": "10 pages; accepted to IEEE Int. Conf. on Fog and Edge Computing\n  (ICFEC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling is important in Edge computing. In contrast to the Cloud, Edge\nresources are hardware limited and cannot support workload-driven\ninfrastructure scaling. Hence, resource allocation and scheduling for the Edge\nrequires a fresh perspective. Existing Edge scheduling research assumes\navailability of all needed resources whenever a job request is made. This paper\nchallenges that assumption, since not all job requests from a Cloud server can\nbe scheduled on an Edge node. Thus, guaranteeing fairness among the clients\n(Cloud servers offloading jobs) while accounting for priorities of the jobs\nbecomes a critical task. This paper presents four scheduling techniques, the\nfirst is a naive first come first serve strategy and further proposes three\nstrategies, namely a client fair, priority fair, and hybrid that accounts for\nthe fairness of both clients and job priorities. An evaluation on a target\nplatform under three different scenarios, namely equal, random, and Gaussian\njob distributions is presented. The experimental studies highlight the low\noverheads and the distribution of scheduled jobs on the Edge node when compared\nto the naive strategy. The results confirm the superior performance of the\nhybrid strategy and showcase the feasibility of fair schedulers for Edge\ncomputing.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 16:13:10 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Madej", "Arkadiusz", ""], ["Wang", "Nan", ""], ["Athanasopoulos", "Nikolaos", ""], ["Ranjan", "Rajiv", ""], ["Varghese", "Blesson", ""]]}, {"id": "2001.09228", "submitter": "Blesson Varghese", "authors": "Nan Wang and Blesson Varghese", "title": "Context-aware Distribution of Fog Applications Using Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing is an emerging paradigm that aims to meet the increasing\ncomputation demands arising from the billions of devices connected to the\nInternet. Offloading services of an application from the Cloud to the edge of\nthe network can improve the overall Quality-of-Service (QoS) of the application\nsince it can process data closer to user devices. Diverse Fog nodes ranging\nfrom Wi-Fi routers to mini-clouds with varying resource capabilities makes it\nchallenging to determine which services of an application need to be offloaded.\nIn this paper, a context-aware mechanism for distributing applications across\nthe Cloud and the Fog is proposed. The mechanism dynamically generates\n(re)deployment plans for the application to maximise the performance efficiency\nof the application by taking the QoS and running costs into account. The\nmechanism relies on deep Q-networks to generate a distribution plan without\nprior knowledge of the available resources on the Fog node, the network\ncondition and the application. The feasibility of the proposed context-aware\ndistribution mechanism is demonstrated on two use-cases, namely a face\ndetection application and a location-based mobile game. The benefits are\nincreased utility of dynamic distribution in both use cases, when compared to a\nstatic distribution approach used in existing research.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 23:31:59 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Wang", "Nan", ""], ["Varghese", "Blesson", ""]]}, {"id": "2001.09244", "submitter": "Jincheng Du", "authors": "Andrei Lihu, Jincheng Du, Igor Barjaktarevic, Patrick Gerzanics and\n  Mark Harvilla", "title": "A Proof of Useful Work for Artificial Intelligence on the Blockchain", "comments": "24 pages, including 8 pages of supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin mining is a wasteful and resource-intensive process. To add a block\nof transactions to the blockchain, miners spend a considerable amount of\nenergy. The Bitcoin protocol, named 'proof of work' (PoW), resembles a lottery\nand the underlying computational work is not useful otherwise. In this paper,\nwe describe a novel 'proof of useful work' (PoUW) protocol based on training a\nmachine learning model on the blockchain. Miners get a chance to create new\ncoins after performing honest ML training work. Clients submit tasks and pay\nall training contributors. This is an extra incentive to participate in the\nnetwork because the system does not rely only on the lottery procedure. Using\nour consensus protocol, interested parties can order, complete, and verify\nuseful work in a distributed environment. We outline mechanisms to reward\nuseful work and punish malicious actors. We aim to build better AI systems\nusing the security of the blockchain.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 01:10:46 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Lihu", "Andrei", ""], ["Du", "Jincheng", ""], ["Barjaktarevic", "Igor", ""], ["Gerzanics", "Patrick", ""], ["Harvilla", "Mark", ""]]}, {"id": "2001.09258", "submitter": "Naoki Shibata", "authors": "Naoki Shibata and Francesco Petrogalli", "title": "SLEEF: A Portable Vectorized Library of C Standard Mathematical\n  Functions", "comments": "in IEEE Transactions on Parallel and Distributed Systems. This is a\n  version with all appendices included in a PDF. Accompanying software can be\n  accessed at https://sleef.org or https://codeocean.com/capsule/6861013", "journal-ref": null, "doi": "10.1109/TPDS.2019.2960333", "report-no": null, "categories": "cs.MS cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present techniques used to implement our portable\nvectorized library of C standard mathematical functions written entirely in C\nlanguage. In order to make the library portable while maintaining good\nperformance, intrinsic functions of vector extensions are abstracted by inline\nfunctions or preprocessor macros. We implemented the functions so that they can\nuse sub-features of vector extensions such as fused multiply-add, mask\nregisters and extraction of mantissa. In order to make computation with SIMD\ninstructions efficient, the library only uses a small number of conditional\nbranches, and all the computation paths are vectorized. We devised a variation\nof the Payne-Hanek argument reduction for trigonometric functions and a\nfloating point remainder, both of which are suitable for vector computation. We\ncompare the performance of our library to Intel SVML.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 03:05:52 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Shibata", "Naoki", ""], ["Petrogalli", "Francesco", ""]]}, {"id": "2001.09329", "submitter": "Michel Kr\\\"amer", "authors": "Michel Kr\\\"amer", "title": "GeoRocket: A scalable and cloud-based data store for big geospatial\n  files", "comments": "Accepted. To be published in SoftwareX", "journal-ref": "SoftwareX, Volume 11, 2020", "doi": "10.1016/j.softx.2020.100409", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GeoRocket, a software for the management of very large geospatial\ndatasets in the cloud. GeoRocket employs a novel way to handle arbitrarily\nlarge datasets by splitting them into chunks that are processed individually.\nThe software has a modern reactive architecture and makes use of existing\nservices including Elasticsearch and storage back ends such as MongoDB or\nAmazon S3. GeoRocket is schema-agnostic and supports a wide range of\nheterogeneous geospatial file formats. It is also format-preserving and does\nnot alter imported data in any way. The main benefits of GeoRocket are its\nperformance, scalability, and usability, which make it suitable for a number of\nscientific and commercial use cases dealing with very high data volumes,\ncomplex datasets, and high velocity (Big Data). GeoRocket also provides many\nopportunities for further research in the area of geospatial data management.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 15:06:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Kr\u00e4mer", "Michel", ""]]}, {"id": "2001.09352", "submitter": "Juuso Haavisto", "authors": "Juuso Haavisto and Jukka Riekki", "title": "Interoperable GPU Kernels as Latency Improver for MEC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed reality (MR) applications are expected to become common when 5G goes\nmainstream. However, the latency requirements are challenging to meet due to\nthe resources required by video-based remoting of graphics, that is, decoding\nvideo codecs. We propose an approach towards tackling this challenge: a\nclient-server implementation for transacting intermediate representation (IR)\nbetween a mobile UE and a MEC server instead of video codecs and this way\navoiding video decoding. We demonstrate the ability to address latency\nbottlenecks on edge computing workloads that transact graphics. We select\nSPIR-V compatible GPU kernels as the intermediate representation. Our approach\nrequires know-how in GPU architecture and GPU domain-specific languages (DSLs),\nbut compared to video-based edge graphics, it decreases UE device delay by\nsevenfold. Further, we find that due to low cold-start times on both UEs and\nMEC servers, application migration can happen in milliseconds. We imply that\ngraphics-based location-aware applications, such as MR, can benefit from this\nkind of approach.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 19:07:58 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Haavisto", "Juuso", ""], ["Riekki", "Jukka", ""]]}, {"id": "2001.09399", "submitter": "Suraj Padmanaban Kesavan", "authors": "Suraj P. Kesavan, Takanori Fujiwara, Jianping Kelvin Li, Caitlin Ross,\n  Misbah Mubarak, Christopher D. Carothers, Robert B. Ross, Kwan-Liu Ma", "title": "A Visual Analytics Framework for Reviewing Streaming Performance Data", "comments": "This is the author's preprint version that will be published in\n  Proceedings of IEEE Pacific Visualization Symposium, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and tuning the performance of extreme-scale parallel computing\nsystems demands a streaming approach due to the computational cost of applying\noffline algorithms to vast amounts of performance log data. Analyzing large\nstreaming data is challenging because the rate of receiving data and limited\ntime to comprehend data make it difficult for the analysts to sufficiently\nexamine the data without missing important changes or patterns. To support\nstreaming data analysis, we introduce a visual analytic framework comprising of\nthree modules: data management, analysis, and interactive visualization. The\ndata management module collects various computing and communication performance\nmetrics from the monitored system using streaming data processing techniques\nand feeds the data to the other two modules. The analysis module automatically\nidentifies important changes and patterns at the required latency. In\nparticular, we introduce a set of online and progressive analysis methods for\nnot only controlling the computational costs but also helping analysts better\nfollow the critical aspects of the analysis results. Finally, the interactive\nvisualization module provides the analysts with a coherent view of the changes\nand patterns in the continuously captured performance data. Through a\nmulti-faceted case study on performance analysis of parallel discrete-event\nsimulation, we demonstrate the effectiveness of our framework for identifying\nbottlenecks and locating outliers.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 04:34:22 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Kesavan", "Suraj P.", ""], ["Fujiwara", "Takanori", ""], ["Li", "Jianping Kelvin", ""], ["Ross", "Caitlin", ""], ["Mubarak", "Misbah", ""], ["Carothers", "Christopher D.", ""], ["Ross", "Robert B.", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2001.09645", "submitter": "Christian Schulz", "authors": "Johannes Langguth, Sebastian Schlag and Christian Schulz", "title": "Load-Balanced Bottleneck Objectives in Process Mapping", "comments": "Extended abstract accepted at CSC'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new problem formulation for graph partitioning that is tailored\nto the needs of time-critical simulations on modern heterogeneous\nsupercomputers.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 09:38:22 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Langguth", "Johannes", ""], ["Schlag", "Sebastian", ""], ["Schulz", "Christian", ""]]}, {"id": "2001.09670", "submitter": "Rafael Pereira Pires", "authors": "Rafael Pereira Pires", "title": "Distributed systems and trusted execution environments: Trade-offs and\n  challenges", "comments": "133 pages, PhD thesis, University of Neuch\\^atel, Defended on\n  December 3rd, 2019", "journal-ref": null, "doi": "10.35662/unine-thesis-2812", "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Security and privacy concerns in computer systems have grown in importance\nwith the ubiquity of connected devices. TEEs provide security guarantees based\non cryptographic constructs built in hardware. Intel software guard extensions\n(SGX), in particular, implements powerful mechanisms that can shield sensitive\ndata even from privileged users with full control of system software. In this\nwork, we essentially explore some of the challenges of designing secure\ndistributed systems by using Intel SGX as cornerstone. We do so by designing\nand experimentally evaluating several elementary systems ranging from\ncommunication and processing middleware to a peer-to-peer privacy-preserving\nsolution. We start with support systems that naturally fit cloud deployment\nscenarios, namely content-based routing, batching and stream processing\nframeworks. We implement prototypes and use them to analyse the manifested\nmemory usage issues intrinsic to SGX. Next, we aim at protecting very sensitive\ndata: cryptographic keys. By leveraging TEEs, we design protocols for group\ndata sharing that have lower computational complexity than legacy methods. As a\nbonus, our proposals allow large savings on metadata volume and processing time\nof cryptographic operations, all with equivalent security guarantees. Finally,\nwe propose privacy-preserving systems against established services like\nweb-search engines. Our evaluation shows that we propose the most robust system\nin comparison to existing solutions with regard to user re-identification rates\nand results accuracy in a scalable way. Overall, this thesis proposes new\nmechanisms that take advantage of TEEs for distributed system architectures. We\nshow through an empirical approach on top of Intel SGX what are the trade-offs\nof distinct designs applied to distributed communication and processing,\ncryptographic protocols and private web search.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 10:27:55 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 12:05:33 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 10:52:12 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Pires", "Rafael Pereira", ""]]}, {"id": "2001.09782", "submitter": "Tram Truong-Huu", "authors": "Tien-Dung Cao, Tram Truong-Huu, Hien Tran, and Khanh Tran", "title": "A Federated Learning Framework for Privacy-preserving and Parallel\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of such deep learning in practice has been hurdled by two\nissues: the computational cost of model training and the privacy issue of\ntraining data such as medical or healthcare records. The large size of both\nlearning models and datasets incurs a massive computational cost, requiring\nefficient approaches to speed up the training phase. While parallel and\ndistributed learning can address the issue of computational overhead,\npreserving the privacy of training data and intermediate results (e.g.,\ngradients) remains a hard problem. Enabling parallel training of deep learning\nmodels on distributed datasets while preserving data privacy is even more\ncomplex and challenging. In this paper, we develop and implement FEDF, a\ndistributed deep learning framework for privacy-preserving and parallel\ntraining. The framework allows a model to be learned on multiple\ngeographically-distributed training datasets (which may belong to different\nowners) while do not reveal any information of each dataset as well as the\nintermediate results. We formally prove the convergence of the learning model\nwhen training with the developed framework and its privacy-preserving property.\nWe carry out extensive experiments to evaluate the performance of the framework\nin terms of speedup ratio, the approximation to the upper-bound performance\n(when training centrally) and communication overhead between the master and\ntraining workers. The results show that the developed framework achieves a\nspeedup of up to 4.8x compared to the centralized training approach and\nmaintaining the performance approximation of the models within 4.5% of the\ncentrally-trained models. The proposed framework also significantly reduces the\namount of data exchanged between the master and training workers by up to 34%\ncompared to existing work.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 02:52:31 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 14:39:14 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Cao", "Tien-Dung", ""], ["Truong-Huu", "Tram", ""], ["Tran", "Hien", ""], ["Tran", "Khanh", ""]]}, {"id": "2001.09783", "submitter": "Priyank Faldu", "authors": "Priyank Faldu and Jeff Diamond and Boris Grot", "title": "Domain-Specialized Cache Management for Graph Analytics", "comments": "No content changes from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph analytics power a range of applications in areas as diverse as finance,\nnetworking and business logistics. A common property of graphs used in the\ndomain of graph analytics is a power-law distribution of vertex connectivity,\nwherein a small number of vertices are responsible for a high fraction of all\nconnections in the graph. These richly-connected, hot, vertices inherently\nexhibit high reuse. However, this work finds that state-of-the-art hardware\ncache management schemes struggle in capitalizing on their reuse due to highly\nirregular access patterns of graph analytics.\n  In response, we propose GRASP, domain-specialized cache management at the\nlast-level cache for graph analytics. GRASP augments existing cache policies to\nmaximize reuse of hot vertices by protecting them against cache thrashing,\nwhile maintaining sufficient flexibility to capture the reuse of other vertices\nas needed. GRASP keeps hardware cost negligible by leveraging lightweight\nsoftware support to pinpoint hot vertices, thus eliding the need for\nstorage-intensive prediction mechanisms employed by state-of-the-art cache\nmanagement schemes. On a set of diverse graph-analytic applications with large\nhigh-skew graph datasets, GRASP outperforms prior domain-agnostic schemes on\nall datapoints, yielding an average speed-up of 4.2% (max 9.4%) over the\nbest-performing prior scheme. GRASP remains robust on low-/no-skew datasets,\nwhereas prior schemes consistently cause a slowdown.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 18:46:26 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 11:40:31 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Faldu", "Priyank", ""], ["Diamond", "Jeff", ""], ["Grot", "Boris", ""]]}, {"id": "2001.09804", "submitter": "Antonios Katsarakis", "authors": "A. Katsarakis (1), V. Gavrielatos (1), M. Katebzadeh (1), A. Joshi\n  (2), A. Dragojevic (3), B. Grot (1), V. Nagarajan (1) ((1) University of\n  Edinburgh, (2) Intel, (3) Microsoft Research)", "title": "Hermes: a Fast, Fault-Tolerant and Linearizable Replication Protocol", "comments": "Accepted in ASPLOS 2020", "journal-ref": null, "doi": "10.1145/3373376.3378496", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's datacenter applications are underpinned by datastores that are\nresponsible for providing availability, consistency, and performance. For high\navailability in the presence of failures, these datastores replicate data\nacross several nodes. This is accomplished with the help of a reliable\nreplication protocol that is responsible for maintaining the replicas\nstrongly-consistent even when faults occur. Strong consistency is preferred to\nweaker consistency models that cannot guarantee an intuitive behavior for the\nclients. Furthermore, to accommodate high demand at real-time latencies,\ndatastores must deliver high throughput and low latency.\n  This work introduces Hermes, a broadcast-based reliable replication protocol\nfor in-memory datastores that provides both high throughput and low latency by\nenabling local reads and fully-concurrent fast writes at all replicas. Hermes\ncouples logical timestamps with cache-coherence-inspired invalidations to\nguarantee linearizability, avoid write serialization at a centralized ordering\npoint, resolve write conflicts locally at each replica (hence ensuring that\nwrites never abort) and provide fault-tolerance via replayable writes. Our\nimplementation of Hermes over an RDMA-enabled reliable datastore with five\nreplicas shows that Hermes consistently achieves higher throughput than\nstate-of-the-art RDMA-based reliable protocols (ZAB and CRAQ) across all write\nratios while also significantly reducing tail latency. At 5% writes, the tail\nlatency of Hermes is 3.6X lower than that of CRAQ and ZAB.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 14:10:03 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Katsarakis", "A.", ""], ["Gavrielatos", "V.", ""], ["Katebzadeh", "M.", ""], ["Joshi", "A.", ""], ["Dragojevic", "A.", ""], ["Grot", "B.", ""], ["Nagarajan", "V.", ""]]}, {"id": "2001.09957", "submitter": "David A. Monge Ph.D.", "authors": "Yisel Gar\\'i, David A. Monge, Elina Pacini, Cristian Mateos, and\n  Carlos Garc\\'ia Garino", "title": "Reinforcement Learning-based Application Autoscaling in the Cloud: A\n  Survey", "comments": "40 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) has demonstrated a great potential for\nautomatically solving decision-making problems in complex uncertain\nenvironments. RL proposes a computational approach that allows learning through\ninteraction in an environment with stochastic behavior, where agents take\nactions to maximize some cumulative short-term and long-term rewards. Some of\nthe most impressive results have been shown in Game Theory where agents\nexhibited superhuman performance in games like Go or Starcraft 2, which led to\nits gradual adoption in many other domains, including Cloud Computing.\nTherefore, RL appears as a promising approach for Autoscaling in Cloud since it\nis possible to learn transparent (with no human intervention), dynamic (no\nstatic plans), and adaptable (constantly updated) resource management policies\nto execute applications. These are three important distinctive aspects to\nconsider in comparison with other widely used autoscaling policies that are\ndefined in an ad-hoc way or statically computed as in solutions based on\nmeta-heuristics. Autoscaling exploits the Cloud elasticity to optimize the\nexecution of applications according to given optimization criteria, which\ndemands to decide when and how to scale-up/down computational resources, and\nhow to assign them to the upcoming processing workload. Such actions have to be\ntaken considering that the Cloud is a dynamic and uncertain environment.\nMotivated by this, many works apply RL to the autoscaling problem in the Cloud.\nIn this work, we survey exhaustively those proposals from major venues, and\nuniformly compare them based on a set of proposed taxonomies. We also discuss\nopen problems and prospective research in the area.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 18:23:43 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 14:10:54 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 14:14:31 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Gar\u00ed", "Yisel", ""], ["Monge", "David A.", ""], ["Pacini", "Elina", ""], ["Mateos", "Cristian", ""], ["Garino", "Carlos Garc\u00eda", ""]]}, {"id": "2001.09990", "submitter": "Anuj Vaishnav", "authors": "Anuj Vaishnav, Khoa Dang Pham, Joseph Powell and Dirk Koch", "title": "FOS: A Modular FPGA Operating System for Dynamic Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With FPGAs now being deployed in the cloud and at the edge, there is a need\nfor scalable design methods which can incorporate the heterogeneity present in\nthe hardware and software components of FPGA systems. Moreover, these FPGA\nsystems need to be maintainable and adaptable to changing workloads while\nimproving accessibility for the application developers. However, current FPGA\nsystems fail to achieve modularity and support for multi-tenancy due to\ndependencies between system components and lack of standardised abstraction\nlayers. To solve this, we introduce a modular FPGA operating system -- FOS,\nwhich adopts a modular FPGA development flow to allow each system component to\nbe changed and be agnostic to the heterogeneity of EDA tool versions, hardware\nand software layers. Further, to dynamically maximise the utilisation\ntransparently from the users, FOS employs resource-elastic scheduling to\narbitrate the FPGA resources in both time and spatial domain for any type of\naccelerators. Our evaluation on different FPGA boards shows that FOS can\nprovide performance improvements in both single-tenant and multi-tenant\nenvironments while substantially reducing the development time and, at the same\ntime, improving flexibility.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 14:20:37 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Vaishnav", "Anuj", ""], ["Pham", "Khoa Dang", ""], ["Powell", "Joseph", ""], ["Koch", "Dirk", ""]]}, {"id": "2001.09991", "submitter": "Stella Bitchebe", "authors": "Stella Bitchebe, Djob Mvondo, Alain Tchana, Laurent R\\'eveill\\`ere,\n  No\\\"el De Palma", "title": "Intel Page Modification Logging, a hardware virtualization feature:\n  study and improvement for virtual machine working set estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intel Page Modification Logging (PML) is a novel hardware feature for\ntracking virtual machine (VM) accessed memory pages. This task is essential in\ntoday's data centers since it allows, among others, checkpointing, live\nmigration and working set size (WSS) estimation. Relying on the Xen hypervisor,\nthis paper studies PML from three angles: power consumption, efficiency, and\nperformance impact on user applications. Our findings are as follows. First,\nPML does not incur any power consumption overhead. Second, PML reduces by up to\n10.18% both VM live migration and checkpointing time. Third, PML slightly\nreduces by up to 0.95% the performance degradation on applications incurred by\nlive migration and checkpointing. Fourth, PML however does not allow accurate\nWSS estimation because read accesses are not tracked and hot pages cannot be\nidentified. A naive extension of PML for addressing these limitations could\nlead to severe performance degradation (up to 34.8%) for the VM whose WSS is\ncomputed.\n  This paper presents Page Reference Logging (PRL), a smart extension of PML\nfor allowing both read and write accesses to be tracked. It does this without\nimpacting user VMs. The paper also presents a WSS estimation system which\nleverages PRL and shows how this algorithm can be integrated into a data center\nwhich implements memory overcommitment. We implement PRL and the WSS estimation\nsystem under Gem5, a very popular hardware simulator. The evaluation results\nvalidate the accuracy of PRL in the estimation of WSS. They also show that PRL\nincurs no performance degradation for user VMs.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 16:24:08 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Bitchebe", "Stella", ""], ["Mvondo", "Djob", ""], ["Tchana", "Alain", ""], ["R\u00e9veill\u00e8re", "Laurent", ""], ["De Palma", "No\u00ebl", ""]]}, {"id": "2001.09995", "submitter": "Richard Uhrie", "authors": "Richard Uhrie, Chaitali Chakrabarti, John Brunhaver", "title": "Automated Parallel Kernel Extraction from Dynamic Application Traces", "comments": "14 pages, 16 figures. Submitted to IEEE Transactions on Parallel and\n  Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern program runtime is dominated by segments of repeating code called\nkernels. Kernels are accelerated by increasing memory locality, increasing\ndata-parallelism, and exploiting producer-consumer parallelism among kernels -\nwhich requires hardware specialized for a particular class of kernels.\nProgramming this hardware can be difficult, requiring that the kernels be\nidentified and annotated in the code or translated to a domain-specific\nlanguage. This paper describes a technique to automatically localize parallel\nkernels from a dynamic application trace, facilitating further code\noptimization.\n  Dynamic trace collection is fast and compact. With optimization, it only\nincurs a time-dilation of a factor on nine and file-size of one megabyte per\nsecond, addressing a significant criticism of this approach. Kernel extraction\nis accurate and performed in linear time within logarithmic memory, detecting a\nwide range of kernels. This approach was validated across 16 libraries,\ncomprised of 10,507 kernels instances. To validate the accuracy of our detected\nkernels, five test programs were written that spans traditional kernel\ndefinitions and were certified to contain all the kernels that were expected.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 14:51:08 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Uhrie", "Richard", ""], ["Chakrabarti", "Chaitali", ""], ["Brunhaver", "John", ""]]}, {"id": "2001.10049", "submitter": "Marquita Ellis", "authors": "Marquita Ellis (1 and 2), Giulia Guidi (1 and 2), Ayd{\\i}n Bulu\\c{c}\n  (1 and 2), Leonid Oliker (2), Katherine Yelick (1 and 2) ((1) University of\n  California at Berkeley (2) Lawrence Berkeley National Lab)", "title": "diBELLA: Distributed Long Read to Long Read Alignment", "comments": "This is the authors' preprint of the article that appears in the\n  proceedings of ICPP 2019, the 48th International Conference on Parallel\n  Processing", "journal-ref": "In Proceedings of the 48th International Conference on Parallel\n  Processing (ICPP 2019). Association for Computing Machinery, New York, NY,\n  USA, Article 70, 1-11", "doi": "10.1145/3337821.3337919", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel algorithm and scalable implementation for genome\nanalysis, specifically the problem of finding overlaps and alignments for data\nfrom \"third generation\" long read sequencers. While long sequences of DNA offer\nenormous advantages for biological analysis and insight, current long read\nsequencing instruments have high error rates and therefore require different\napproaches to analysis than their short read counterparts. Our work focuses on\nan efficient distributed-memory parallelization of an accurate single-node\nalgorithm for overlapping and aligning long reads. We achieve scalability of\nthis irregular algorithm by addressing the competing issues of increasing\nparallelism, minimizing communication, constraining the memory footprint, and\nensuring good load balance. The resulting application, diBELLA, is the first\ndistributed memory overlapper and aligner specifically designed for long reads\nand parallel scalability. We describe and present analyses for high level\ndesign trade-offs and conduct an extensive empirical analysis that compares\nperformance characteristics across state-of-the-art HPC systems as well as a\ncommercial cloud architectures, highlighting the advantages of state-of-the-art\nnetwork technologies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 20:08:40 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Ellis", "Marquita", "", "1 and 2"], ["Guidi", "Giulia", "", "1 and 2"], ["Bulu\u00e7", "Ayd\u0131n", "", "1 and 2"], ["Oliker", "Leonid", "", "1 and 2"], ["Yelick", "Katherine", "", "1 and 2"]]}, {"id": "2001.10160", "submitter": "Mingyu Yan", "authors": "Mingyu Yan, Zhaodong Chen, Lei Deng, Xiaochun Ye, Zhimin Zhang,\n  Dongrui Fan, Yuan Xie", "title": "Characterizing and Understanding GCNs on GPU", "comments": "To Appear in IEEE Computer Architecture Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional neural networks (GCNs) have achieved state-of-the-art\nperformance on graph-structured data analysis. Like traditional neural\nnetworks, training and inference of GCNs are accelerated with GPUs. Therefore,\ncharacterizing and understanding the execution pattern of GCNs on GPU is\nimportant for both software and hardware optimization. Unfortunately, to the\nbest of our knowledge, there is no detailed characterization effort of GCN\nworkloads on GPU. In this paper, we characterize GCN workloads at inference\nstage and explore GCN models on NVIDIA V100 GPU. Given the characterization and\nexploration, we propose several useful guidelines for both software\noptimization and hardware optimization for the efficient execution of GCNs on\nGPU.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 04:10:36 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Yan", "Mingyu", ""], ["Chen", "Zhaodong", ""], ["Deng", "Lei", ""], ["Ye", "Xiaochun", ""], ["Zhang", "Zhimin", ""], ["Fan", "Dongrui", ""], ["Xie", "Yuan", ""]]}, {"id": "2001.10199", "submitter": "Yong Xiao", "authors": "Yong Xiao and Marwan Krunz", "title": "Distributed Optimization for Energy-efficient Fog Computing in the\n  Tactile Internet", "comments": null, "journal-ref": "Published at IEEE Journal on Selected Areas in Communications,\n  vol. 36, no. 11, pp. 2390 - 2400, November 2018", "doi": "10.1109/JSAC.2018.2872287", "report-no": null, "categories": "cs.NI cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile Internet is an emerging concept that focuses on supporting\nhigh-fidelity, ultra-responsive, and widely available human-to-machine\ninteractions. To reduce the transmission latency and alleviate Internet\ncongestion, fog computing has been advocated as an important component of the\nTactile Internet. In this paper, we focus on energy-efficient design of fog\ncomputing networks that support low-latency Tactile Internet applications. We\ninvestigate two performance metrics: Service response time of end-users and\npower usage efficiency of fog nodes. We quantify the fundamental tradeoff\nbetween these two metrics and then extend our analysis to fog computing\nnetworks involving cooperation between fog nodes. We introduce a novel\ncooperative fog computing concept, referred to as offload forwarding, in which\na set of fog nodes with different computing and energy resources can cooperate\nwith each other. The objective of this cooperation is to balance the workload\nprocessed by different fog nodes, further reduce the service response time, and\nimprove the efficiency of power usage. We develop a distributed optimization\nframework based on dual decomposition to achieve the optimal tradeoff. Our\nframework does not require fog nodes to disclose their private information nor\nconduct back-and-forth negotiations with each other. Two distributed\noptimization algorithms are proposed. One is based on the subgradient method\nwith dual decomposition and the other is based on distributed ADMM-VS. We prove\nthat both algorithms can achieve the optimal workload allocation that minimizes\nthe response time under the given power efficiency constraints of fog nodes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 07:44:10 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Xiao", "Yong", ""], ["Krunz", "Marwan", ""]]}, {"id": "2001.10300", "submitter": "Yong Xiao", "authors": "Yong Xiao and Marwan Krunz", "title": "Dynamic Network Slicing for Scalable Fog Computing Systems with Energy\n  Harvesting", "comments": null, "journal-ref": "Published at IEEE Journal on Selected Areas in Communications,\n  vol. 36, no. 12, pp. 2640 - 2654, December 2018", "doi": "10.1109/JSAC.2018.2871292", "report-no": null, "categories": "cs.NI cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies fog computing systems, in which cloud data centers can be\nsupplemented by a large number of fog nodes deployed in a wide geographical\narea. Each node relies on harvested energy from the surrounding environment to\nprovide computational services to local users. We propose the concept of\ndynamic network slicing in which a regional orchestrator coordinates workload\ndistribution among local fog nodes, providing partitions/slices of energy and\ncomputational resources to support a specific type of service with certain\nquality-of-service (QoS) guarantees. The resources allocated to each slice can\nbe dynamically adjusted according to service demands and energy availability. A\nstochastic overlapping coalition-formation game is developed to investigate\ndistributed cooperation and joint network slicing between fog nodes under\nrandomly fluctuating energy harvesting and workload arrival processes. We\nobserve that the overall processing capacity of the fog computing network can\nbe improved by allowing fog nodes to maintain a belief function about the\nunknown state and the private information of other nodes. An algorithm based on\na belief-state partially observable Markov decision process (B-POMDP) is\nproposed to achieve the optimal resource slicing structure among all fog nodes.\nWe describe how to implement our proposed dynamic network slicing within the\n3GPP network sharing architecture, and evaluate the performance of our proposed\nframework using the real BS location data of a real cellular system with over\n200 BSs deployed in the city of Dublin. Our numerical results show that our\nframework can significantly improve the workload processing capability of fog\ncomputing networks. In particular, even when each fog node can coordinate only\nwith its closest neighbor, the total amount of workload processed by fog nodes\ncan be almost doubled under certain scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 12:59:52 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Xiao", "Yong", ""], ["Krunz", "Marwan", ""]]}, {"id": "2001.10308", "submitter": "Hamid Nasiri", "authors": "Hamid Nasiri, Saeed Nasehi, Arman Divband, Maziar Goudarzi", "title": "A Scheduling Algorithm to Maximize Storm Throughput in Heterogeneous\n  Cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the most popular distributed stream processing frameworks (DSPFs),\nprograms are modeled as a directed acyclic graph. This model allows a DSPF to\nbenefit from the parallelism power of distributed clusters. However, choosing\nthe proper number of vertices for each operator and finding an appropriate\nmapping between these vertices and processing resources have a determinative\neffect on overall throughput and resource utilization; while the simplicity of\ncurrent DSPFs' schedulers leads these frameworks to perform poorly on\nlarge-scale clusters. In this paper, we present the design and implementation\nof a heterogeneity-aware scheduling algorithm that finds the proper number of\nthe vertices of an application graph and maps them to the most suitable cluster\nnode. We start to scale up the application graph over a given cluster\ngradually, by increasing the topology input rate and taking new instances from\nbottlenecked vertices. Our experimental results on Storm Micro-Benchmark show\nthat 1) the prediction model estimate CPU utilization with 92% accuracy. 2)\nCompared to default scheduler of Storm, our scheduler provides 7% to 44%\nthroughput enhancement. 3) The proposed method can find the solution within 4%\n(worst case) of the optimal scheduler which obtains the best scheduling\nscenario using an exhaustive search on problem design space.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 13:22:55 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Nasiri", "Hamid", ""], ["Nasehi", "Saeed", ""], ["Divband", "Arman", ""], ["Goudarzi", "Maziar", ""]]}, {"id": "2001.10402", "submitter": "Mohammad Mohammadi Amiri Dr.", "authors": "Mohammad Mohammadi Amiri, Deniz Gunduz, Sanjeev R. Kulkarni, H.\n  Vincent Poor", "title": "Convergence of Update Aware Device Scheduling for Federated Learning at\n  the Wireless Edge", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study federated learning (FL) at the wireless edge, where power-limited\ndevices with local datasets collaboratively train a joint model with the help\nof a remote parameter server (PS). We assume that the devices are connected to\nthe PS through a bandwidth-limited shared wireless channel. At each iteration\nof FL, a subset of the devices are scheduled to transmit their local model\nupdates to the PS over orthogonal channel resources, while each participating\ndevice must compress its model update to accommodate to its link capacity. We\ndesign novel scheduling and resource allocation policies that decide on the\nsubset of the devices to transmit at each round, and how the resources should\nbe allocated among the participating devices, not only based on their channel\nconditions, but also on the significance of their local model updates. We then\nestablish convergence of a wireless FL algorithm with device scheduling, where\ndevices have limited capacity to convey their messages. The results of\nnumerical experiments show that the proposed scheduling policy, based on both\nthe channel conditions and the significance of the local model updates,\nprovides a better long-term performance than scheduling policies based only on\neither of the two metrics individually. Furthermore, we observe that when the\ndata is independent and identically distributed (i.i.d.) across devices,\nselecting a single device at each round provides the best performance, while\nwhen the data distribution is non-i.i.d., scheduling multiple devices at each\nround improves the performance. This observation is verified by the convergence\nresult, which shows that the number of scheduled devices should increase for a\nless diverse and more biased data distribution.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 15:15:22 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 11:18:57 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Amiri", "Mohammad Mohammadi", ""], ["Gunduz", "Deniz", ""], ["Kulkarni", "Sanjeev R.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2001.10424", "submitter": "Carola Kruse", "authors": "Carola Kruse, Masha Sosonkina, Mario Arioli, Nicolas Tardieu and\n  Ulrich Ruede", "title": "Parallel solution of saddle point systems with nested iterative solvers\n  based on the Golub-Kahan Bidiagonalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalability study of Golub-Kahan bidiagonalization for the\nparallel iterative solution of symmetric indefinite linear systems with a 2x2\nblock structure. The algorithms have been implemented within the parallel\nnumerical library PETSc. Since a nested inner-outer iteration strategy may be\nnecessary, we investigate different choices for the inner solvers, including\nparallel sparse direct and multigrid accelerated iterative methods. We show the\nstrong and weak scalability of the Golub-Kahan bidiagonalization based\niterative method when applied to a two-dimensional Poiseuille flow and to two-\nand three-dimensional Stokes test problems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 15:54:41 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Kruse", "Carola", ""], ["Sosonkina", "Masha", ""], ["Arioli", "Mario", ""], ["Tardieu", "Nicolas", ""], ["Ruede", "Ulrich", ""]]}, {"id": "2001.10483", "submitter": "Ao Wang", "authors": "Ao Wang, Jingyuan Zhang, Xiaolong Ma, Ali Anwar, Lukas Rupprecht,\n  Dimitrios Skourtis, Vasily Tarasov, Feng Yan, Yue Cheng", "title": "InfiniCache: Exploiting Ephemeral Serverless Functions to Build a\n  Cost-Effective Memory Cache", "comments": "This is the preprint version of a paper published in USENIX FAST 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-scale web applications are becoming increasingly storage-intensive\nand rely heavily on in-memory object caching to attain required I/O\nperformance. We argue that the emerging serverless computing paradigm provides\na well-suited, cost-effective platform for object caching. We present\nInfiniCache, a first-of-its-kind in-memory object caching system that is\ncompletely built and deployed atop ephemeral serverless functions. InfiniCache\nexploits and orchestrates serverless functions' memory resources to enable\nelastic pay-per-use caching. InfiniCache's design combines erasure coding,\nintelligent billed duration control, and an efficient data backup mechanism to\nmaximize data availability and cost-effectiveness while balancing the risk of\nlosing cached state and performance. We implement InfiniCache on AWS Lambda and\nshow that it: (1) achieves 31 -- 96X tenant-side cost savings compared to AWS\nElastiCache for a large-object-only production workload, (2) can effectively\nprovide 95.4% data availability for each one hour window, and (3) enables\ncomparative performance seen in a typical in-memory cache.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 17:41:13 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Wang", "Ao", ""], ["Zhang", "Jingyuan", ""], ["Ma", "Xiaolong", ""], ["Anwar", "Ali", ""], ["Rupprecht", "Lukas", ""], ["Skourtis", "Dimitrios", ""], ["Tarasov", "Vasily", ""], ["Yan", "Feng", ""], ["Cheng", "Yue", ""]]}, {"id": "2001.10554", "submitter": "Gian Giacomo Guerreschi", "authors": "Gian Giacomo Guerreschi, Justin Hogaboam, Fabio Baruffa, Nicolas P. D.\n  Sawaya", "title": "Intel Quantum Simulator: A cloud-ready high-performance simulator of\n  quantum circuits", "comments": "Improved figures and updated link to the GitHub repository", "journal-ref": "Quantum Sci. Technol. 5, 034007 (2020)", "doi": "10.1088/2058-9565/ab8505", "report-no": null, "categories": "quant-ph cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical simulation of quantum computers will continue to play an essential\nrole in the progress of quantum information science, both for numerical studies\nof quantum algorithms and for modeling noise and errors. Here we introduce the\nlatest release of Intel Quantum Simulator (IQS), formerly known as qHiPSTER.\nThe high-performance computing (HPC) capability of the software allows users to\nleverage the available hardware resources provided by supercomputers, as well\nas available public cloud computing infrastructure. To take advantage of the\nlatter platform, together with the distributed simulation of each separate\nquantum state, IQS allows to subdivide the computational resources to simulate\na pool of related circuits in parallel. We highlight the technical\nimplementation of the distributed algorithm and details about the new pool\nfunctionality. We also include some basic benchmarks (up to 42 qubits) and\nperformance results obtained using HPC infrastructure. Finally, we use IQS to\nemulate a scenario in which many quantum devices are running in parallel to\nimplement the quantum approximate optimization algorithm, using particle swarm\noptimization as the classical subroutine. The results demonstrate that the\nhyperparameters of this classical optimization algorithm depends on the total\nnumber of quantum circuit simulations one has the bandwidth to perform. Intel\nQuantum Simulator has been released open-source with permissive licensing and\nis designed to simulate a large number of qubits, to emulate multiple quantum\ndevices running in parallel, and/or to study the effects of decoherence and\nother hardware errors on calculation results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 19:00:25 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 18:00:47 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Guerreschi", "Gian Giacomo", ""], ["Hogaboam", "Justin", ""], ["Baruffa", "Fabio", ""], ["Sawaya", "Nicolas P. D.", ""]]}, {"id": "2001.10641", "submitter": "Daniel N\\\"ust", "authors": "Daniel N\\\"ust, Dirk Eddelbuettel, Dom Bennett, Robrecht Cannoodt, Dav\n  Clark, Gergely Daroczi, Mark Edmondson, Colin Fay, Ellis Hughes, Lars\n  Kjeldgaard, Sean Lopp, Ben Marwick, Heather Nolis, Jacqueline Nolis, Hong\n  Ooi, Karthik Ram, Noam Ross, Lori Shepherd, P\\'eter S\\'olymos, Tyson Lee\n  Swetnam, Nitesh Turaga, Charlotte Van Petegem, Jason Williams, Craig Willis,\n  Nan Xiao", "title": "The Rockerverse: Packages and Applications for Containerization with R", "comments": "Source code for article available at\n  https://github.com/nuest/rockerverse-paper/ Updated version includes some new\n  paragraphs and corrections throughout the text; full diff available at\n  https://github.com/nuest/rockerverse-paper/compare/preprint.v2...preprint.v3", "journal-ref": "The R Journal (2020), 12:1, pages 437-461", "doi": "10.32614/RJ-2020-007", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Rocker Project provides widely used Docker images for R across different\napplication scenarios. This article surveys downstream projects that build upon\nthe Rocker Project images and presents the current state of R packages for\nmanaging Docker images and controlling containers. These use cases cover\ndiverse topics such as package development, reproducible research,\ncollaborative work, cloud-based data processing, and production deployment of\nservices. The variety of applications demonstrates the power of the Rocker\nProject specifically and containerisation in general. Across the diverse ways\nto use containers, we identified common themes: reproducible environments,\nscalability and efficiency, and portability across clouds. We conclude that the\ncurrent growth and diversification of use cases is likely to continue its\npositive impact, but see the need for consolidating the Rockerverse ecosystem\nof packages, developing common practices for applications, and exploring\nalternative containerisation software.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 00:00:40 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 11:12:57 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 12:48:44 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 13:54:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["N\u00fcst", "Daniel", ""], ["Eddelbuettel", "Dirk", ""], ["Bennett", "Dom", ""], ["Cannoodt", "Robrecht", ""], ["Clark", "Dav", ""], ["Daroczi", "Gergely", ""], ["Edmondson", "Mark", ""], ["Fay", "Colin", ""], ["Hughes", "Ellis", ""], ["Kjeldgaard", "Lars", ""], ["Lopp", "Sean", ""], ["Marwick", "Ben", ""], ["Nolis", "Heather", ""], ["Nolis", "Jacqueline", ""], ["Ooi", "Hong", ""], ["Ram", "Karthik", ""], ["Ross", "Noam", ""], ["Shepherd", "Lori", ""], ["S\u00f3lymos", "P\u00e9ter", ""], ["Swetnam", "Tyson Lee", ""], ["Turaga", "Nitesh", ""], ["Van Petegem", "Charlotte", ""], ["Williams", "Jason", ""], ["Willis", "Craig", ""], ["Xiao", "Nan", ""]]}, {"id": "2001.10815", "submitter": "Juraj Kardos", "authors": "Juraj Kardos and Drosos Kourounis and Olaf Schenk", "title": "Reduced-Space Interior Point Methods in Power Grid Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to critical environmental issues, the power systems have to accommodate a\nsignificant level of penetration of renewable generation which requires smart\napproaches to the power grid control. Associated optimal control problems are\nlarge-scale nonlinear optimization problems with up to hundreds of millions of\nvariables and constraints. The interior point methods become computationally\nintractable, mainly due to the solution of large linear systems.\n  This document addresses the computational bottlenecks of the interior point\nmethod during the solution of the security constrained optimal power flow\nproblems by applying reduced space quasi-Newton IPM, which could utilize\nhigh-performance computers due to the inherent parallelism in the adjoint\nmethod. Reduced space IPM approach and the adjoint method is a novel approach\nwhen it comes to solving the (security constrained) optimal power flow\nproblems. These were previously used in the PDE-constrained optimization. The\npresented methodology is suitable for high-performance architectures due to\ninherent parallelism in the adjoint method during the gradient evaluation,\nsince the individual contingency scenarios are modeled by independent set of\nthe constraints. Preliminary evaluation of the performance and convergence is\nperformed to study the reduced space approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 13:26:32 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Kardos", "Juraj", ""], ["Kourounis", "Drosos", ""], ["Schenk", "Olaf", ""]]}, {"id": "2001.10865", "submitter": "Ben Blamey", "authors": "Oliver Stein and Ben Blamey and Johan Karlsson and Alan Sabirsh and\n  Ola Spjuth and Andreas Hellander and Salman Toor", "title": "Smart Resource Management for Data Streaming using an Online Bin-packing\n  Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data stream processing frameworks provide reliable and efficient mechanisms\nfor executing complex workflows over large datasets. A common challenge for the\nmajority of currently available streaming frameworks is efficient utilization\nof resources. Most frameworks use static or semi-static settings for resource\nutilization that work well for established use cases but lead to marginal\nimprovements for unseen scenarios. Another pressing issue is the efficient\nprocessing of large individual objects such as images and matrices typical for\nscientific datasets. HarmonicIO has proven to be a good solution for streams of\nrelatively large individual objects, as demonstrated in a benchmark comparison\nwith the Spark and Kafka streaming frameworks. We here present an extension of\nthe HarmonicIO framework based on the online bin-packing algorithm, to allow\nfor efficient utilization of resources. Based on a real world use case from\nlarge-scale microscopy pipelines, we compare results of the new system to\nSpark's auto-scaling mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 14:32:39 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Stein", "Oliver", ""], ["Blamey", "Ben", ""], ["Karlsson", "Johan", ""], ["Sabirsh", "Alan", ""], ["Spjuth", "Ola", ""], ["Hellander", "Andreas", ""], ["Toor", "Salman", ""]]}, {"id": "2001.11017", "submitter": "Konstantin D Pandl", "authors": "Konstantin D. Pandl, Scott Thiebes, Manuel Schmidt-Kraepelin, Ali\n  Sunyaev", "title": "On the Convergence of Artificial Intelligence and Distributed Ledger\n  Technology: A Scoping Review and Future Research Agenda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in Artificial Intelligence (AI) and Distributed Ledger\nTechnology (DLT) currently lead to lively debates in academia and practice. AI\nprocesses data to perform tasks that were previously thought possible only for\nhumans. DLT has the potential to create consensus over data among a group of\nparticipants in uncertain environments. In recent research, both technologies\nare used in similar and even the same systems. Examples include the design of\nsecure distributed ledgers or the creation of allied learning systems\ndistributed across multiple nodes. This can lead to technological convergence,\nwhich in the past, has paved the way for major innovations in information\ntechnology. Previous work highlights several potential benefits of the\nconvergence of AI and DLT but only provides a limited theoretical framework to\ndescribe upcoming real-world integration cases of both technologies. We aim to\ncontribute by conducting a systematic literature review on previous work and\nproviding rigorously derived future research opportunities. This work helps\nresearchers active in AI or DLT to overcome current limitations in their field,\nand practitioners to develop systems along with the convergence of both\ntechnologies.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 18:57:27 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 13:36:25 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Pandl", "Konstantin D.", ""], ["Thiebes", "Scott", ""], ["Schmidt-Kraepelin", "Manuel", ""], ["Sunyaev", "Ali", ""]]}, {"id": "2001.11093", "submitter": "Yehia Elkhatib PhD", "authors": "Abdessalam Elhabbash, Assylbek Jumagaliyev, Gordon S. Blair, Yehia\n  Elkhatib", "title": "SLO-ML: A Language for Service Level Objective Modelling in Multi-cloud\n  Applications", "comments": null, "journal-ref": "In International Conference on Utility and Cloud Computing (UCC),\n  ACM, pages 241-250, December 2019", "doi": "10.1145/3344341.3368805", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud modelling languages (CMLs) are designed to assist customers in tackling\nthe diversity of services in the cloud market. While many CMLs have been\nproposed in the literature, they lack practical support for automating the\nselection of services based on the specific service level objectives of a\ncustomer's application. We put forward SLO-ML, a novel and generative CML to\ncapture service level requirements and, subsequently, to select the services to\nhonour customer requirements and generate the deployment code appropriate to\nthese services. We present the architectural design of SLO-ML and the\nassociated broker that realises the deployment operations. We rigorously\nevaluate SLO-ML using a mixed methods approach. First, we exploit an\nexperimental case study with a group of researchers and developers using a\nreal-world cloud application. We also assess overheads through an exhaustive\nset of empirical scalability tests. Through expressing the levels of gained\nproductivity and experienced usability, we highlight SLO-ML's profound\npotential in enabling user-centric cloud brokers. We also discuss limitations\nas application requirements grow.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 21:05:36 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Elhabbash", "Abdessalam", ""], ["Jumagaliyev", "Assylbek", ""], ["Blair", "Gordon S.", ""], ["Elkhatib", "Yehia", ""]]}, {"id": "2001.11433", "submitter": "Seyed-Vahid Sanei-Mehri", "authors": "Apurba Das, Seyed-Vahid Sanei-Mehri, and Srikanta Tirthapura", "title": "Shared-Memory Parallel Maximal Clique Enumeration from Static and\n  Dynamic Graphs", "comments": "This paper is accepted in ACM Transactions on Parallel Computing\n  (TOPC). A preliminary version [arXiv:1807.09417] of this work appeared in the\n  proceedings of the 25th IEEE International Conference on. High Performance\n  Computing, Data, and Analytics (HiPC), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maximal Clique Enumeration (MCE) is a fundamental graph mining problem, and\nis useful as a primitive in identifying dense structures in a graph. Due to the\nhigh computational cost of MCE, parallel methods are imperative for dealing\nwith large graphs. We present shared-memory parallel algorithms for MCE, with\nthe following properties: (1) the parallel algorithms are provably\nwork-efficient relative to a state-of-the-art sequential algorithm (2) the\nalgorithms have a provably small parallel depth, showing they can scale to a\nlarge number of processors, and (3) our implementations on a multicore machine\nshow good speedup and scaling behavior with increasing number of cores, and are\nsubstantially faster than prior shared-memory parallel algorithms for MCE; for\ninstance, on certain input graphs, while prior works either ran out of memory\nor did not complete in 5 hours, our implementation finished within a minute\nusing 32 cores. We also present work-efficient parallel algorithms for\nmaintaining the set of all maximal cliques in a dynamic graph that is changing\nthrough the addition of edges.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 16:25:17 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Das", "Apurba", ""], ["Sanei-Mehri", "Seyed-Vahid", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "2001.11806", "submitter": "Martin Bauer", "authors": "Martin Bauer, Harald K\\\"ostler, Ulrich R\\\"ude", "title": "lbmpy: Automatic code generation for efficient parallel lattice\n  Boltzmann methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lattice Boltzmann methods are a popular mesoscopic alternative to macroscopic\ncomputational fluid dynamics solvers. Many variants have been developed that\nvary in complexity, accuracy, and computational cost. Extensions are available\nto simulate multi-phase, multi-component, turbulent, or non-Newtonian flows. In\nthis work we present lbmpy, a code generation package that supports a wide\nvariety of different methods and provides a generic development environment for\nnew schemes as well. A high-level domain-specific language allows the user to\nformulate, extend and test various lattice Boltzmann schemes. The method\nspecification is represented in a symbolic intermediate representation.\nTransformations that operate on this intermediate representation optimize and\nparallelize the method, yielding highly efficient lattice Boltzmann compute\nkernels not only for single- and two-relaxation-time schemes but also for\nmulti-relaxation-time, cumulant, and entropically stabilized methods. An\nintegration into the HPC framework waLBerla makes massively parallel,\ndistributed simulations possible, which is demonstrated through scaling\nexperiments on the SuperMUC-NG supercomputing system\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 13:00:26 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 09:09:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Bauer", "Martin", ""], ["K\u00f6stler", "Harald", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "2001.11965", "submitter": "Eugen Z\\u{a}linescu", "authors": "L\\u{a}cr\\u{a}mioara A\\c{s}tefanoaei (1) and Pierre Chambart (1) and\n  Antonella Del Pozzo (2) and Thibault Rieutord (2,3) and Sara Tucci (2) and\n  Eugen Z\\u{a}linescu (1) ((1) Nomadic Labs, (2) CEA LIST, (3) Universit\\'e\n  Paris-Saclay)", "title": "Tenderbake -- A Solution to Dynamic Repeated Consensus for Blockchains", "comments": "This new version mainly brings the following improvements: (1)\n  endorsement messages do not need to contain a aQC, therefore the message size\n  of proposals and the size of message buffers decreases by a factor of `n`;\n  and (2) the implementation of 'betterChain' (now called 'betterHead') has\n  changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-generation blockchains provide probabilistic finality: a block can be\nrevoked, albeit the probability decreases as the block sinks deeper into the\nchain. Recent proposals revisited committee-based BFT consensus to provide\ndeterministic finality: as soon as a block is validated, it is never revoked. A\ndistinguishing characteristic of these second-generation blockchains over\nclassical BFT protocols is that committees change over time as the\nparticipation and the blockchain state evolve. In this paper, we push forward\nin this direction by proposing a formalization of the Dynamic Repeated\nConsensus problem and by providing generic procedures to solve it in the\ncontext of blockchains. Our approach is modular in that one can plug in\ndifferent synchronizers and single-shot consensus instances. To offer a\ncomplete solution, we provide a concrete instantiation, called Tenderbake, and\npresent a blockchain synchronizer and a single-shot consensus algorithm,\nworking in a Byzantine and partially synchronous system model with eventually\nsynchronous clocks. In contrast to recent proposals, our methodology is driven\nby the need to bound the message buffers. This is essential in preventing\nspamming and run-time memory errors. Moreover, Tenderbake processes can\nsynchronize with each other without exchanging messages, leveraging instead the\ninformation stored in the blockchain.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 17:28:00 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 18:56:55 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 11:21:16 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 09:51:55 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["A\u015ftefanoaei", "L\u0103cr\u0103mioara", ""], ["Chambart", "Pierre", ""], ["Del Pozzo", "Antonella", ""], ["Rieutord", "Thibault", ""], ["Tucci", "Sara", ""], ["Z\u0103linescu", "Eugen", ""]]}, {"id": "2001.11986", "submitter": "James Chin-Jen Pang", "authors": "James Chin-Jen Pang, Hessam Mahdavifar, and S. Sandeep Pradhan", "title": "Capacity-achieving Polar-based LDGM Codes with Crowdsourcing\n  Applications", "comments": "12 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study codes with sparse generator matrices. More\nspecifically, codes with a certain constraint on the weight of all the columns\nin the generator matrix are considered. The end result is the following. For\nany binary-input memoryless symmetric (BMS) channel and any epsilon > 2\nepsilon*, where epsilon^* = \\frac{1}{6}-\\frac{5}{3}\\log{\\frac{4}{3}} \\approx\n0.085, we show an explicit sequence of capacity-achieving codes with all the\ncolumn wights of the generator matrix upper bounded by (\\log N)^{1+epsilon},\nwhere N is the code block length. The constructions are based on polar codes.\nApplications to crowdsourcing are also shown.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 18:21:35 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Pang", "James Chin-Jen", ""], ["Mahdavifar", "Hessam", ""], ["Pradhan", "S. Sandeep", ""]]}]