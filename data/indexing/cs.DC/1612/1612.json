[{"id": "1612.00150", "submitter": "Tianyu Wu", "authors": "Tianyu Wu, Kun Yuan, Qing Ling, Wotao Yin, and Ali H. Sayed", "title": "Decentralized Consensus Optimization with Asynchrony and Delays", "comments": "added a table for comparison between related algorithms; added a\n  numerical comparison with asynchronous decentralized ADMM", "journal-ref": null, "doi": null, "report-no": "UCLA CAM Report 16-82", "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an asynchronous, decentralized algorithm for consensus\noptimization. The algorithm runs over a network in which the agents communicate\nwith their neighbors and perform local computation. In the proposed algorithm,\neach agent can compute and communicate independently at different times, for\ndifferent durations, with the information it has even if the latest information\nfrom its neighbors is not yet available. Such an asynchronous algorithm reduces\nthe time that agents would otherwise waste idle because of communication delays\nor because their neighbors are slower. It also eliminates the need for a global\nclock for synchronization. Mathematically, the algorithm involves both primal\nand dual variables, uses fixed step-size parameters, and provably converges to\nthe exact solution under a bounded delay assumption and a random agent\nassumption. When running synchronously, the algorithm performs just as well as\nexisting competitive synchronous algorithms such as PG-EXTRA, which diverges\nwithout synchronization. Numerical experiments confirm the theoretical findings\nand illustrate the performance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:17:29 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 23:21:46 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Wu", "Tianyu", ""], ["Yuan", "Kun", ""], ["Ling", "Qing", ""], ["Yin", "Wotao", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1612.00521", "submitter": "Sayed Hadi Hashemi", "authors": "Sayed Hadi Hashemi, Shadi A. Noghabi, William Gropp, Roy H Campbell", "title": "Performance Modeling of Distributed Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, machine learning has become extremely popular and can\nbe found in many aspects of our every day life. Nowayadays with explosion of\ndata while rapid growth of computation capacity, Distributed Deep Neural\nNetworks (DDNNs) which can improve their performance linearly with more\ncomputation resources, have become hot and trending. However, there has not\nbeen an in depth study of the performance of these systems, and how well they\nscale.\n  In this paper we analyze CNTK, one of the most commonly used DDNNs, by first\nbuilding a performance model and then evaluating the system two settings: a\nsmall cluster with all nodes in a single rack connected to a top of rack\nswitch, and in large scale using Blue Waters with arbitary placement of nodes.\nOur main focus was the scalability of the system with respect to adding more\nnodes. Based on our results, this system has an excessive initialization\noverhead because of poor I/O utilization which dominates the whole execution\ntime. Because of this, the system does not scale beyond a few nodes (4 in Blue\nWaters). Additionally, due to a single server-multiple worker design the server\nbecomes a bottleneck after 16 nodes limiting the scalability of the CNTK.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 00:07:24 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 23:36:40 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Hashemi", "Sayed Hadi", ""], ["Noghabi", "Shadi A.", ""], ["Gropp", "William", ""], ["Campbell", "Roy H", ""]]}, {"id": "1612.00903", "submitter": "Tianyu Wu", "authors": "Yat-Tin Chow, Wei Shi, Tianyu Wu, Wotao Yin", "title": "Expander Graph and Communication-Efficient Decentralized Optimization", "comments": "2016 IEEE Asilomar Conference on Signals, Systems, and Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss how to design the graph topology to reduce the\ncommunication complexity of certain algorithms for decentralized optimization.\nOur goal is to minimize the total communication needed to achieve a prescribed\naccuracy. We discover that the so-called expander graphs are near-optimal\nchoices. We propose three approaches to construct expander graphs for different\nnumbers of nodes and node degrees. Our numerical results show that the\nperformance of decentralized optimization is significantly better on expander\ngraphs than other regular graphs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 00:36:50 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chow", "Yat-Tin", ""], ["Shi", "Wei", ""], ["Wu", "Tianyu", ""], ["Yin", "Wotao", ""]]}, {"id": "1612.00989", "submitter": "Amanj Khorramian", "authors": "Amanj Khorramian and Akira Matsubayashi", "title": "Online Page Migration on Ring Networks in Uniform Model", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of page migration in ring networks. A ring\nnetwork is a connected graph, in which each node is connected with exactly two\nother nodes. In this problem, one of the nodes in a given network holds a page\nof size D. This node is called the server and the page is a non-duplicable data\nin the network. Requests are issued by nodes to access the page one after\nanother. Every time a new request is issued, the server must serve the request\nand may migrate to another node before the next request arrives. A service\ncosts the distance between the server and the requesting node, and the\nmigration costs the distance of the migration multiplied by D. The problem is\nto minimize the total costs of services and migrations. We study this problem\nin uniform model, for which the page has a unit size, i.e. D=1. A\n3.326-competitive algorithm improving the current best upper bound is designed.\nWe show that this ratio is tight for our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 17:20:17 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Khorramian", "Amanj", ""], ["Matsubayashi", "Akira", ""]]}, {"id": "1612.01061", "submitter": "Marek Klonowski", "authors": "Jacek Cicho\\'n, Zbigniew Go{\\l}\\eobbiewski, Marcin Kardas, Marek\n  Klonowski, Filip Zag\\'orski", "title": "On spreading rumor in heterogeneous systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a model of spreading information in heterogeneous\nsystems wherein we have two kinds of objects. Some of them are active and\nothers are passive. Active objects can, if they possess information, share it\nwith an encountered passive object. We focus on a particular case such that\nactive objects communicate independently with randomly chosen passive objects.\nSuch model is motivated by two real-life scenarios. The first one is a very\ndynamic system of mobile devices distributing information among stationary\ndevices. The second is an architecture wherein clients communicate with several\nservers and can leave some information learnt from other servers. The main\nquestion we investigate is how many rounds is needed to deliver the information\nto all objects under the assumption that at the beginning exactly one object\nhas the information?\n  In this paper we provide mathematical models of such process and show rigid\nand very precise mathematical analysis for some special cases important from\npractical point of view. Some mathematical results are quite surprising -- we\nfind relation of investigated process to both coupon collector's problem as\nwell as the birthday paradox. Additionally, we present simulations for showing\nbehaviour for general parameters\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 04:18:02 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Cicho\u0144", "Jacek", ""], ["Go\u0142\\eobbiewski", "Zbigniew", ""], ["Kardas", "Marcin", ""], ["Klonowski", "Marek", ""], ["Zag\u00f3rski", "Filip", ""]]}, {"id": "1612.01163", "submitter": "Evgeny Nikulchev", "authors": "Evgeny Nikulchev, Evgeniy Pluzhnik, Oleg Lukyanchikov, Dmitry\n  Biryukov, Elena Andrianova", "title": "QoS-based Computing Resources Partitioning between Virtual Machines in\n  the Cloud Architecture", "comments": "6 pages, International Journal of Advanced Computer Science and\n  Applications (2016) 7", "journal-ref": null, "doi": "10.14569/IJACSA.2016.071121", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud services have been used very widely, but configuration of the\nparameters, including the efficient allocation of resources, is an important\nobjective for the system architect. The article is devoted to solving the\nproblem of choosing the architecture of computers based on simulation and\ndeveloped program for monitoring computing resources. Techniques were developed\naimed at providing the required quality of service and efficient use of\nresources. The article describes the monitoring program of computing resources\nand time efficiency of the target application functions. On the basis of this\napplication the technique is shown and described in the experiment, designed to\nensure the requirements for quality of service, by isolating one process from\nthe others on different virtual machines inside the hypervisor.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 18:49:59 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Nikulchev", "Evgeny", ""], ["Pluzhnik", "Evgeniy", ""], ["Lukyanchikov", "Oleg", ""], ["Biryukov", "Dmitry", ""], ["Andrianova", "Elena", ""]]}, {"id": "1612.01178", "submitter": "Michael Sutton", "authors": "Michael Sutton, Tal Ben-Nun, Amnon Barak, Sreepathi Pai, Keshav\n  Pingali", "title": "Adaptive Work-Efficient Connected Components on the GPU", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents an adaptive work-efficient approach for implementing the\nConnected Components algorithm on GPUs. The results show a considerable\nincrease in performance (up to 6.8$\\times$) over current state-of-the-art\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 20:57:12 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sutton", "Michael", ""], ["Ben-Nun", "Tal", ""], ["Barak", "Amnon", ""], ["Pai", "Sreepathi", ""], ["Pingali", "Keshav", ""]]}, {"id": "1612.01395", "submitter": "Siegfried Cools", "authors": "Siegfried Cools and Wim Vanroose", "title": "The communication-hiding pipelined BiCGStab method for the parallel\n  solution of large unsymmetric linear systems", "comments": "24 pages, 5 figures, 4 tables, 45 references", "journal-ref": "Parallel Computing 65, pp. 1-20, July 2017", "doi": "10.1016/j.parco.2017.04.005", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A High Performance Computing alternative to traditional Krylov subspace\nmethods, pipelined Krylov subspace solvers offer better scalability in the\nstrong scaling limit compared to standard Krylov subspace methods for large and\nsparse linear systems. The typical synchronization bottleneck is mitigated by\noverlapping time-consuming global communication phases with local computations\nin the algorithm. This paper describes a general framework for deriving the\npipelined variant of any Krylov subspace algorithm. The proposed framework was\nimplicitly used to derive the pipelined Conjugate Gradient (p-CG) method in\n\"Hiding global synchronization latency in the preconditioned Conjugate Gradient\nalgorithm\" by P. Ghysels and W. Vanroose, Parallel Computing, 40(7):224--238,\n2014. The pipelining framework is subsequently illustrated by formulating a\npipelined version of the BiCGStab method for the solution of large unsymmetric\nlinear systems on parallel hardware. A residual replacement strategy is\nproposed to account for the possible loss of attainable accuracy and robustness\nby the pipelined BiCGStab method. It is shown that the pipelined algorithm\nimproves scalability on distributed memory machines, leading to significant\nspeedups compared to standard preconditioned BiCGStab.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 15:30:52 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 16:19:36 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 11:54:34 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Cools", "Siegfried", ""], ["Vanroose", "Wim", ""]]}, {"id": "1612.01437", "submitter": "Celestine D\\\"unner", "authors": "Celestine D\\\"unner, Thomas Parnell, Kubilay Atasu, Manolis Sifalakis,\n  Haralampos Pozidis", "title": "Understanding and Optimizing the Performance of Distributed Machine\n  Learning Applications on Apache Spark", "comments": "To appear in the 2017 IEEE International Conference on Big Data (Big\n  Data 2017), December 11-14, 2017, Boston, MA, USA", "journal-ref": null, "doi": "10.1109/BigData.2017.8257942", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the performance limits of Apache Spark for machine\nlearning applications. We begin by analyzing the characteristics of a\nstate-of-the-art distributed machine learning algorithm implemented in Spark\nand compare it to an equivalent reference implementation using the high\nperformance computing framework MPI. We identify critical bottlenecks of the\nSpark framework and carefully study their implications on the performance of\nthe algorithm. In order to improve Spark performance we then propose a number\nof practical techniques to alleviate some of its overheads. However, optimizing\ncomputational efficiency and framework related overheads is not the only key to\nperformance -- we demonstrate that in order to get the best performance out of\nany implementation it is necessary to carefully tune the algorithm to the\nrespective trade-off between computation time and communication latency. The\noptimal trade-off depends on both the properties of the distributed algorithm\nas well as infrastructure and framework-related characteristics. Finally, we\napply these technical and algorithmic optimizations to three different\ndistributed linear machine learning algorithms that have been implemented in\nSpark. We present results using five large datasets and demonstrate that by\nusing the proposed optimizations, we can achieve a reduction in the performance\ndifference between Spark and MPI from 20x to 2x.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 17:16:05 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 03:45:52 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["D\u00fcnner", "Celestine", ""], ["Parnell", "Thomas", ""], ["Atasu", "Kubilay", ""], ["Sifalakis", "Manolis", ""], ["Pozidis", "Haralampos", ""]]}, {"id": "1612.01458", "submitter": "Alessandro Maria Rizzi", "authors": "Alessandro Maria Rizzi", "title": "Support vector regression model for BigData systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays Big Data are becoming more and more important. Many sectors of our\neconomy are now guided by data-driven decision processes. Big Data and business\nintelligence applications are facilitated by the MapReduce programming model\nwhile, at infrastructural layer, cloud computing provides flexible and cost\neffective solutions for allocating on demand large clusters. In such systems,\ncapacity allocation, which is the ability to optimally size minimal resources\nfor achieve a certain level of performance, is a key challenge to enhance\nperformance for MapReduce jobs and minimize cloud resource costs. In order to\ndo so, one of the biggest challenge is to build an accurate performance model\nto estimate job execution time of MapReduce systems. Previous works applied\nsimulation based models for modeling such systems. Although this approach can\naccurately describe the behavior of Big Data clusters, it is too\ncomputationally expensive and does not scale to large system. We try to\novercome these issues by applying machine learning techniques. More precisely\nwe focus on Support Vector Regression (SVR) which is intrinsically more robust\nw.r.t other techniques, like, e.g., neural networks, and less sensitive to\noutliers in the training set. To better investigate these benefits, we compare\nSVR to linear regression.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 18:14:12 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Rizzi", "Alessandro Maria", ""]]}, {"id": "1612.01497", "submitter": "Junaid Khalid", "authors": "Junaid Khalid and Aditya Akella", "title": "Correctness and Performance for Stateful Chained Network Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network functions virtualization (NFV) allows operators to employ NF chains\nto realize custom policies, and dynamically add instances to meet demand or for\nfailover. NFs maintain detailed per- and cross-flow state which needs careful\nmanagement, especially during dynamic actions. Crucially, state management\nmust: (1) ensure NF chain-wide correctness and (2) have good performance. To\nthis end, we built \\name, an NFV framework that leverages an external state\nstore coupled with state management algorithms and metadata maintenance for\ncorrect operation even under a range of failures. Our evaluation shows that CHC\ncan support ~10Gbps per-NF throughput and <0.6mus increase in median per-NF\npacket processing latency, and chain-wide correctness at little additional\ncost.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:07:12 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 19:34:25 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Khalid", "Junaid", ""], ["Akella", "Aditya", ""]]}, {"id": "1612.01501", "submitter": "Georgios Smaragdos", "authors": "Georgios Smaragdos, Georgios Chatzikonstantis, Rahul Kukreja, Harry\n  Sidiropoulos, Dimitrios Rodopoulos, Ioannis Sourdis, Zaid Al-Ars,\n  Christoforos Kachris, Dimitrios Soudris, Chris I. De Zeeuw, Christos Strydis", "title": "BrainFrame: A node-level heterogeneous accelerator platform for neuron\n  simulations", "comments": "16 pages, 18 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The advent of High-Performance Computing (HPC) in recent years has\nled to its increasing use in brain study through computational models. The\nscale and complexity of such models are constantly increasing, leading to\nchallenging computational requirements. Even though modern HPC platforms can\noften deal with such challenges, the vast diversity of the modeling field does\nnot permit for a single acceleration (or homogeneous) platform to effectively\naddress the complete array of modeling requirements. Approach: In this paper we\npropose and build BrainFrame, a heterogeneous acceleration platform,\nincorporating three distinct acceleration technologies, a Dataflow Engine, a\nXeon Phi and a GP-GPU. The PyNN framework is also integrated into the platform.\nAs a challenging proof of concept, we analyze the performance of BrainFrame on\ndifferent instances of a state-of-the-art neuron model, modeling the Inferior-\nOlivary Nucleus using a biophysically-meaningful, extended Hodgkin-Huxley\nrepresentation. The model instances take into account not only the neuronal-\nnetwork dimensions but also different network-connectivity circumstances that\ncan drastically change application workload characteristics. Main results: The\nsynthetic approach of three HPC technologies demonstrated that BrainFrame is\nbetter able to cope with the modeling diversity encountered. Our performance\nanalysis shows clearly that the model directly affect performance and all three\ntechnologies are required to cope with all the model use cases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:18:30 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 13:54:49 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 13:48:46 GMT"}, {"version": "v4", "created": "Tue, 15 Aug 2017 16:26:43 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Smaragdos", "Georgios", ""], ["Chatzikonstantis", "Georgios", ""], ["Kukreja", "Rahul", ""], ["Sidiropoulos", "Harry", ""], ["Rodopoulos", "Dimitrios", ""], ["Sourdis", "Ioannis", ""], ["Al-Ars", "Zaid", ""], ["Kachris", "Christoforos", ""], ["Soudris", "Dimitrios", ""], ["De Zeeuw", "Chris I.", ""], ["Strydis", "Christos", ""]]}, {"id": "1612.01514", "submitter": "Siddhartha Jayanti", "authors": "Siddhartha V. Jayanti and Robert E. Tarjan", "title": "A Randomized Concurrent Algorithm for Disjoint Set Union", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disjoint set union problem is a basic problem in data structures with a\nwide variety of applications. We extend a known efficient sequential algorithm\nfor this problem to obtain a simple and efficient concurrent wait-free\nalgorithm running on an asynchronous parallel random access machine (APRAM).\nCrucial to our result is the use of randomization. Under a certain independence\nassumption, for a problem instance in which there are n elements, m operations,\nand p processes, our algorithm does Theta(m (alpha(n, m/(np)) + log(np/m + 1)))\nexpected work, where the expectation is over the random choices made by the\nalgorithm and alpha is a functional inverse of Ackermann's function. In\naddition, each operation takes O(log n) steps with high probability. Our\nalgorithm is significantly simpler and more efficient than previous algorithms\nproposed by Anderson and Woll. Under our independence assumption, our algorithm\nachieves almost-linear speed-up for applications in which all or most of the\nprocesses can be kept busy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:52:30 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Jayanti", "Siddhartha V.", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1612.01603", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Yoshifumi Fukumoto and Hiroki Kumazaki", "title": "Study of shoplifting prevention using image analysis and ERP check", "comments": "4 pages, in Japanese, 2 figures, IEICE Technical Report, SC2016-14,\n  Aug. 2016", "journal-ref": "IEICE Technical Report, SC2016-14, Aug. 2016. (c) 2016 IEICE", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a SaaS service which prevents shoplifting using\nimage analysis and ERP. In Japan, total damage of shoplifting reaches 450\nbillion yen and more than 1000 small shops gave up their businesses because of\nshoplifting. Based on recent cloud technology and data analysis technology, we\npropose a shoplifting prevention service with image analysis of security camera\nand ERP data check for small shops. We evaluated stream analysis of security\ncamera movie using online machine learining framework Jubatus.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:31:05 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Yamato", "Yoji", ""], ["Fukumoto", "Yoshifumi", ""], ["Kumazaki", "Hiroki", ""]]}, {"id": "1612.01842", "submitter": "Zaid Hussain", "authors": "Zaid Hussain", "title": "An Improved One-to-All Broadcasting in Higher Dimensional\n  Eisenstein-Jacobi Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a higher dimensional Eisenstein-Jacobi networks, has been proposed\nin [22], which is shown that they have better average distance with more number\nof nodes than a single dimensional EJ networks. Some communication algorithms\nsuch as one-to-all and all-to-all communications are well known and used in\ninterconnection networks. In one-to-all communication, a source node sends a\nmessage to every other node in the network. Whereas, in all-to-all\ncommunication, every node is considered as a source node and sends its message\nto every other node in the network. In this paper, an improved one-to-all\ncommunication algorithm in higher dimensional EJ networks is presented. The\npaper shows that the proposed algorithm achieves a lower average number of\nsteps to receiving the broadcasted message. In addition, since the links are\nassumed to be half-duplex, the all-to-all broadcasting algorithm is divided\ninto three phases. The simulation results are discussed and showed that the\nimproved one-to-all algorithm achieves better traffic performance than the\nwell-known one-to-all algorithm and has 2.7% less total number of senders\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 15:01:47 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 06:10:53 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Hussain", "Zaid", ""]]}, {"id": "1612.01845", "submitter": "Brendan Patch Mr", "authors": "Brendan Patch and Thomas Taimre", "title": "Transient Provisioning and Performance Evaluation for Cloud Computing\n  Platforms: A Capacity Value Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User demand on the computational resources of cloud computing platforms\nvaries over time. These variations in demand can be predictable or\nunpredictable, resulting in `bursty' fluctuations in demand. Furthermore,\ndemand can arrive in batches, and users whose demands are not met can be\nimpatient. We demonstrate how to compute the expected revenue loss over a\nfinite time horizon in the presence of all these model characteristics through\nthe use of matrix analytic methods. We then illustrate how to use this\nknowledge to make frequent short term provisioning decisions --- transient\nprovisioning. It is seen that taking each of the characteristics of fluctuating\nuser demand (predictable, unpredictable, batchy) into account can result in a\nsubstantial reduction of losses. Moreover, our transient provisioning framework\nallows for a wide variety of system behaviors to be modeled and gives simple\nexpressions for expected revenue loss which are straightforward to evaluate\nnumerically.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 15:09:52 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 08:35:20 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 13:23:08 GMT"}, {"version": "v4", "created": "Wed, 10 May 2017 09:53:53 GMT"}, {"version": "v5", "created": "Thu, 7 Sep 2017 09:33:06 GMT"}, {"version": "v6", "created": "Thu, 12 Oct 2017 14:36:14 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Patch", "Brendan", ""], ["Taimre", "Thomas", ""]]}, {"id": "1612.01855", "submitter": "Tobias Wicky", "authors": "Tobias Wicky, Edgar Solomonik, Torsten Hoefler", "title": "Communication-Avoiding Parallel Algorithms for Solving Triangular\n  Systems of Linear Equations", "comments": "10 pages, 1 figure, accepted at IPDPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new parallel algorithm for solving triangular systems with\nmultiple right hand sides (TRSM). TRSM is used extensively in numerical linear\nalgebra computations, both to solve triangular linear systems of equations as\nwell as to compute factorizations with triangular matrices, such as Cholesky,\nLU, and QR. Our algorithm achieves better theoretical scalability than known\nalternatives, while maintaining numerical stability, via selective use of\ntriangular matrix inversion. We leverage the fact that triangular inversion and\nmatrix multiplication are more parallelizable than the standard TRSM algorithm.\nBy only inverting triangular blocks along the diagonal of the initial matrix,\nwe generalize the usual way of TRSM computation and the full matrix inversion\napproach. This flexibility leads to an efficient algorithm for any ratio of the\nnumber of right hand sides to the triangular matrix dimension. We provide a\ndetailed communication cost analysis for our algorithm as well as for the\nrecursive triangular matrix inversion. This cost analysis makes it possible to\ndetermine optimal block sizes and processor grids a priori. Relative to the\nbest known algorithms for TRSM, our approach can require asymptotically fewer\nmessages, while performing optimal amounts of computation and communication in\nterms of words sent.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 15:20:38 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 10:57:58 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Wicky", "Tobias", ""], ["Solomonik", "Edgar", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1612.02168", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Marjorie Bournat, Yoann Dieudonn\\'e, Swan Dubois\n  and Franck Petit", "title": "Asynchronous approach in the plane: A deterministic polynomial algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the task of approach of two mobile agents having the\nsame limited range of vision and moving asynchronously in the plane. This task\nconsists in getting them in finite time within each other's range of vision.\nThe agents execute the same deterministic algorithm and are assumed to have a\ncompass showing the cardinal directions as well as a unit measure. On the other\nhand, they do not share any global coordinates system (like GPS), cannot\ncommunicate and have distinct labels. Each agent knows its label but does not\nknow the label of the other agent or the initial position of the other agent\nrelative to its own. The route of an agent is a sequence of segments that are\nsubsequently traversed in order to achieve approach. For each agent, the\ncomputation of its route depends only on its algorithm and its label. An\nadversary chooses the initial positions of both agents in the plane and\ncontrols the way each of them moves along every segment of the routes, in\nparticular by arbitrarily varying the speeds of the agents. A deterministic\napproach algorithm is a deterministic algorithm that always allows two agents\nwith any distinct labels to solve the task of approach regardless of the\nchoices and the behavior of the adversary. The cost of a complete execution of\nan approach algorithm is the length of both parts of route travelled by the\nagents until approach is completed. Let $\\Delta$ and $l$ be the initial\ndistance separating the agents and the length of the shortest label,\nrespectively. Assuming that $\\Delta$ and $l$ are unknown to both agents, does\nthere exist a deterministic approach algorithm always working at a cost that is\npolynomial in $\\Delta$ and $l$? In this paper, we provide a positive answer to\nthe above question by designing such an algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 09:50:02 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 15:46:52 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 09:32:03 GMT"}, {"version": "v4", "created": "Mon, 9 Jul 2018 12:48:22 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Bournat", "Marjorie", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Dubois", "Swan", ""], ["Petit", "Franck", ""]]}, {"id": "1612.02467", "submitter": "Alfons Hoekstra", "authors": "Saad Alowayyed, Derek Groen, Peter V. Coveney, Alfons G. Hoekstra", "title": "Multiscale Computing in the Exascale Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expect that multiscale simulations will be one of the main high\nperformance computing workloads in the exascale era. We propose multiscale\ncomputing patterns as a generic vehicle to realise load balanced, fault\ntolerant and energy aware high performance multiscale computing. Multiscale\ncomputing patterns should lead to a separation of concerns, whereby application\ndevelopers can compose multiscale models and execute multiscale simulations,\nwhile pattern software realises optimized, fault tolerant and energy aware\nmultiscale computing. We introduce three multiscale computing patterns, present\nan example of the extreme scaling pattern, and discuss our vision of how this\nmay shape multiscale computing in the exascale era.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 12:35:20 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Alowayyed", "Saad", ""], ["Groen", "Derek", ""], ["Coveney", "Peter V.", ""], ["Hoekstra", "Alfons G.", ""]]}, {"id": "1612.02468", "submitter": "Frederic Le Mouel", "authors": "Roya Golchay (CITI), Fr\\'ed\\'eric Le Mou\\\"el (CITI), Julien Ponge\n  (CITI), Nicolas Stouls (CITI)", "title": "Spontaneous Proximity Clouds: Making Mobile Devices to Collaborate for\n  Resource and Data Sharing", "comments": "in Proceedings of the 12th EAI International Conference on\n  Collaborative Computing: Networking, Applications and Worksharing\n  (CollaborateCom'2016), Nov 2016, Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The base motivation of Mobile Cloud Computing was empowering mobile devices\nby application offloading onto powerful cloud resources. However, this goal\ncan't entirely be reached because of the high offloading cost imposed by the\nlong physical distance between the mobile device and the cloud. To address this\nissue, we propose an application offloading onto a nearby mobile cloud composed\nof the mobile devices in the vicinity-a Spontaneous Proximity Cloud. We\nintroduce our proposed dynamic, ant-inspired, bi-objective offloading\nmiddleware-ACOMMA, and explain its extension to perform a close mobile\napplication offloading. With the learning-based offloading decision-making\nprocess of ACOMMA, combined to the collaborative resource sharing, the mobile\ndevices can cooperate for decision cache sharing. We evaluate the performance\nof ACOMMA in collaborative mode with real benchmarks Face Recognition and\nMonte-Carlo algorithms-and achieve 50% execution time gain.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:29:30 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Golchay", "Roya", "", "CITI"], ["Mou\u00ebl", "Fr\u00e9d\u00e9ric Le", "", "CITI"], ["Ponge", "Julien", "", "CITI"], ["Stouls", "Nicolas", "", "CITI"]]}, {"id": "1612.02495", "submitter": "Kyle Niemeyer", "authors": "Daniel Magee and Kyle E Niemeyer", "title": "An initial investigation of the performance of GPU-based swept\n  time-space decomposition", "comments": "14 pages; submitted to 2017 AIAA SciTech Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulations of physical phenomena are essential to the expedient design of\nprecision components in aerospace and other high-tech industries. These\nphenomena are often described by mathematical models involving partial\ndifferential equations (PDEs) without exact solutions. Modern design problems\nrequire simulations with a level of resolution that is difficult to achieve in\na reasonable amount of time even in effectively parallelized solvers. Though\nthe scale of the problem relative to available computing power is the greatest\nimpediment to accelerating these applications, significant performance gains\ncan be achieved through careful attention to the details of memory accesses.\nParallelized PDE solvers are subject to a trade-off in memory management: store\nthe solution for each timestep in abundant, global memory with high access\ncosts or in a limited, private memory with low access costs that must be passed\nbetween nodes. The GPU implementation of swept time-space decomposition\npresented here mitigates this dilemma by using private (shared) memory,\navoiding internode communication, and overwriting unnecessary values. It shows\nsignificant improvement in the execution time of the PDE solvers in one\ndimension achieving speedups of 6-2x for large and small problem sizes\nrespectively compared to naive GPU versions and 7-300x compared to parallel CPU\nversions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 00:19:54 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 21:27:02 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Magee", "Daniel", ""], ["Niemeyer", "Kyle E", ""]]}, {"id": "1612.02640", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Hiroki Kumazaki and Yoshifumi Fukumoto", "title": "Realtime Predictive Maintenance with Lambda Architecture", "comments": "4 pages, in Japanese, 3 figures, IEICE Technical Report, SC2016-28,\n  Nov. 2016", "journal-ref": "IEICE Technical Report, SC2016-28, Nov. 2016. (c) 2016 IEICE", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, IoT technologies have been progressed and applications of\nmaintenance area are expected. However, IoT maintenance applications are not\nspread in Japan yet because of insufficient analysis of real time situation,\nhigh cost to collect sensing data and to configure failure detection rules. In\nthis paper, using lambda architecture concept, we propose a maintenance\nplatform in which edge nodes analyze sensing data, detect anomaly, extract a\nnew detection rule in real time and a cloud orders maintenance automatically,\nalso analyzes whole data collected by batch process in detail, updates learning\nmodel of edge nodes to improve analysis accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 13:42:49 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Yamato", "Yoji", ""], ["Kumazaki", "Hiroki", ""], ["Fukumoto", "Yoshifumi", ""]]}, {"id": "1612.02916", "submitter": "Ling Ren", "authors": "Ittai Abraham and Dahlia Malkhi and Kartik Nayak and Ling Ren and\n  Alexander Spiegelman", "title": "Solida: A Blockchain Protocol Based on Reconfigurable Byzantine\n  Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decentralized cryptocurrency Bitcoin has experienced great success but\nalso encountered many challenges. One of the challenges has been the long\nconfirmation time. Another challenge is the lack of incentives at certain steps\nof the protocol, raising concerns for transaction withholding, selfish mining,\netc. To address these challenges, we propose Solida, a decentralized blockchain\nprotocol based on reconfigurable Byzantine consensus augmented by\nproof-of-work. Solida improves on Bitcoin in confirmation time, and provides\nsafety and liveness assuming the adversary control less than (roughly)\none-third of the total mining power.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 04:59:22 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 21:47:49 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Abraham", "Ittai", ""], ["Malkhi", "Dahlia", ""], ["Nayak", "Kartik", ""], ["Ren", "Ling", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "1612.02979", "submitter": "Claudio Mezzina", "authors": "Vitaly Buravlev, Rocco De Nicola, Claudio Antares Mezzina", "title": "Tuple spaces implementations and their efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the paradigms for parallel and distributed computing, the one\npopularized with Linda, and based on tuple spaces, is one of the least used,\ndespite the fact of being intuitive, easy to understand and to use. A tuple\nspace is a repository, where processes can add, withdraw or read tuples by\nmeans of atomic operations. Tuples may contain different values, and processes\ncan inspect their content via pattern matching. The lack of a reference\nimplementation for this paradigm has prevented its widespread. In this paper,\nfirst we perform an extensive analysis of a number of actual implementations of\nthe tuple space paradigm and summarise their main features. Then, we select\nfour such implementations and compare their performances on four different case\nstudies that aim at stressing different aspects of computing such as\ncommunication, data manipulation, and cpu usage. After reasoning on strengths\nand weaknesses of the four implementations, we conclude with some\nrecommendations for future work towards building an effective implementation of\nthe tuple space paradigm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 11:22:14 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Buravlev", "Vitaly", ""], ["De Nicola", "Rocco", ""], ["Mezzina", "Claudio Antares", ""]]}, {"id": "1612.03079", "submitter": "Daniel Crankshaw", "authors": "Daniel Crankshaw, Xin Wang, Giulio Zhou, Michael J. Franklin, Joseph\n  E. Gonzalez, Ion Stoica", "title": "Clipper: A Low-Latency Online Prediction Serving System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is being deployed in a growing number of applications which\ndemand real-time, accurate, and robust predictions under heavy query load.\nHowever, most machine learning frameworks and systems only address model\ntraining and not deployment.\n  In this paper, we introduce Clipper, a general-purpose low-latency prediction\nserving system. Interposing between end-user applications and a wide range of\nmachine learning frameworks, Clipper introduces a modular architecture to\nsimplify model deployment across frameworks and applications. Furthermore, by\nintroducing caching, batching, and adaptive model selection techniques, Clipper\nreduces prediction latency and improves prediction throughput, accuracy, and\nrobustness without modifying the underlying machine learning frameworks. We\nevaluate Clipper on four common machine learning benchmark datasets and\ndemonstrate its ability to meet the latency, accuracy, and throughput demands\nof online serving applications. Finally, we compare Clipper to the TensorFlow\nServing system and demonstrate that we are able to achieve comparable\nthroughput and latency while enabling model composition and online learning to\nimprove accuracy and render more robust predictions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 16:29:16 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 17:21:33 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Crankshaw", "Daniel", ""], ["Wang", "Xin", ""], ["Zhou", "Giulio", ""], ["Franklin", "Michael J.", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""]]}, {"id": "1612.03301", "submitter": "Qi Lei", "authors": "Rashish Tandon, Qi Lei, Alexandros G. Dimakis and Nikos Karampatziakis", "title": "Gradient Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel coding theoretic framework for mitigating stragglers in\ndistributed learning. We show how carefully replicating data blocks and coding\nacross gradients can provide tolerance to failures and stragglers for\nSynchronous Gradient Descent. We implement our schemes in python (using MPI) to\nrun on Amazon EC2, and show how we compare against baseline approaches in\nrunning time and generalization error.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 14:25:00 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 01:00:33 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Tandon", "Rashish", ""], ["Lei", "Qi", ""], ["Dimakis", "Alexandros G.", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1612.03413", "submitter": "George Teodoro", "authors": "George Teodoro, Tahsin Kurc, Luis F. R. Taveira, Alba C. M. A. Melo,\n  Jun Kong, and Joel Saltz", "title": "Efficient Methods and Parallel Execution for Algorithm Sensitivity\n  Analysis with Parameter Tuning on Microscopy Imaging Datasets", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: We describe an informatics framework for researchers and clinical\ninvestigators to efficiently perform parameter sensitivity analysis and\nauto-tuning for algorithms that segment and classify image features in a large\ndataset of high-resolution images. The computational cost of the sensitivity\nanalysis process can be very high, because the process requires processing the\ninput dataset several times to systematically evaluate how output varies when\ninput parameters are varied. Thus, high performance computing techniques are\nrequired to quickly execute the sensitivity analysis process.\n  Results: We carried out an empirical evaluation of the proposed method on\nhigh performance computing clusters with multi-core CPUs and co-processors\n(GPUs and Intel Xeon Phis). Our results show that (1) the framework achieves\nexcellent scalability and efficiency on a high performance computing cluster --\nexecution efficiency remained above 85% in all experiments; (2) the parameter\nauto-tuning methods are able to converge by visiting only a small fraction\n(0.0009%) of the search space with limited impact to the algorithm output\n(0.56% on average).\n  Conclusions: The sensitivity analysis framework provides a range of\nstrategies for the efficient exploration of the parameter space, as well as\nmultiple indexes to evaluate the effect of parameter modification to outputs or\neven correlation between parameters. Our work demonstrates the feasibility of\nperforming sensitivity analyses, parameter studies, and auto-tuning with large\ndatasets with the use of high-performance systems and techniques. The proposed\ntechnologies will enable the quantification of error estimations and output\nvariations in these pipelines, which may be used in application specific ways\nto assess uncertainty of conclusions extracted from data generated by these\nimage analysis pipelines.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 14:05:58 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Teodoro", "George", ""], ["Kurc", "Tahsin", ""], ["Taveira", "Luis F. R.", ""], ["Melo", "Alba C. M. A.", ""], ["Kong", "Jun", ""], ["Saltz", "Joel", ""]]}, {"id": "1612.03457", "submitter": "Robert Escriva", "authors": "Robert Escriva, Robbert van Renesse", "title": "Consus: Taming the Paxi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consus is a strictly serializable geo-replicated transactional key-value\nstore. The key contribution of Consus is a new commit protocol that reduces the\ncost of executing a transaction to three wide area message delays in the common\ncase. Augmenting the commit protocol are multiple Paxos implementations\noptimized for different purposes. Together the different implementations and\noptimizations comprise a cohesive system that provides low latency, high\navailability, and strong guarantees. This paper describes the techniques\nimplemented in the open source release of Consus, and lays the groundwork for\nevaluating Consus once the system implementation is sufficiently robust for a\nthorough evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 19:17:26 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Escriva", "Robert", ""], ["van Renesse", "Robbert", ""]]}, {"id": "1612.03852", "submitter": "Luis Veiga", "authors": "S\\'ergio Esteves, Helena Galhardas and Lu\\'is Veiga", "title": "Smart Scheduling of Continuous Data-Intensive Workflows with Machine\n  Learning Triggered Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": "INESC ID Technical Report 9/2016, Oct. 2016", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To extract value from evergrowing volumes of data, coming from a number of\ndifferent sources, and to drive decision making, organizations frequently\nresort to the composition of data processing workflows, since they are\nexpressive, flexible, and scalable. The typical workflow model enforces strict\ntemporal synchronization across processing steps without accounting the actual\neffect of intermediate computations on the final workflow output. However, this\nis not the most desirable behavior in a multitude of scenarios. We identify a\nclass of applications for continuous data processing where workflow output\nchanges slowly and without great significance in a short-to-medium time window,\nthus wasting compute resources and energy with current approaches.\n  To overcome such inefficiency, we introduce a novel workflow model, for\ncontinuous and data-intensive processing, capable of relaxing triggering\nsemantics according to the impact input data is assessed to have on changing\nthe workflow output. To assess this impact, learn the correlation between input\nand output variation, and guarantee correctness within a given tolerated error\nconstant, we rely on Machine Learning. The functionality of this workflow model\nis implemented in SmartFlux, a middleware framework which can be effortlessly\nintegrated with existing workflow managers. Experimental results indicate we\nare able to save a significant amount of resources while not deviating the\nworkflow output beyond a small error constant with high confidence level.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 19:04:24 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Esteves", "S\u00e9rgio", ""], ["Galhardas", "Helena", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1612.03937", "submitter": "Andrea Margheri", "authors": "Francesco Paolo Schiavo, Vladimiro Sassone, Luca Nicoletti, Andrea\n  Margheri", "title": "FaaS: Federation-as-a-Service", "comments": "Technical Report Edited by Francesco Paolo Schiavo, Vladimiro\n  Sassone, Luca Nicoletti and Andrea Margheri", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document is the main high-level architecture specification of the\nSUNFISH cloud federation solution. Its main objective is to introduce the\nconcept of Federation-as-a-Service (FaaS) and the SUNFISH platform. FaaS is the\nnew and innovative cloud federation service proposed by the SUNFISH project.\nThe document defines the functionalities of FaaS, its governance and precise\nobjectives. With respect to these objectives, the document proposes the\nhigh-level architecture of the SUNFISH platform: the software architecture that\npermits realising a FaaS federation. More specifically, the document describes\nall the components forming the platform, the offered functionalities and their\nhigh-level interactions underlying the main FaaS functionalities. The document\nconcludes by outlining the main implementation strategies towards the actual\nimplementation of the proposed cloud federation solution.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 21:31:55 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Schiavo", "Francesco Paolo", ""], ["Sassone", "Vladimiro", ""], ["Nicoletti", "Luca", ""], ["Margheri", "Andrea", ""]]}, {"id": "1612.04003", "submitter": "Aditya Devarakonda", "authors": "Aditya Devarakonda, Kimon Fountoulakis, James Demmel, Michael W.\n  Mahoney", "title": "Avoiding communication in primal and dual block coordinate descent\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primal and dual block coordinate descent methods are iterative methods for\nsolving regularized and unregularized optimization problems. Distributed-memory\nparallel implementations of these methods have become popular in analyzing\nlarge machine learning datasets. However, existing implementations communicate\nat every iteration which, on modern data center and supercomputing\narchitectures, often dominates the cost of floating-point computation. Recent\nresults on communication-avoiding Krylov subspace methods suggest that large\nspeedups are possible by re-organizing iterative algorithms to avoid\ncommunication. We show how applying similar algorithmic transformations can\nlead to primal and dual block coordinate descent methods that only communicate\nevery $s$ iterations--where $s$ is a tuning parameter--instead of every\niteration for the \\textit{regularized least-squares problem}. We show that the\ncommunication-avoiding variants reduce the number of synchronizations by a\nfactor of $s$ on distributed-memory parallel machines without altering the\nconvergence rate and attains strong scaling speedups of up to $6.1\\times$ on a\nCray XC30 supercomputer.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 02:59:33 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 01:57:40 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Devarakonda", "Aditya", ""], ["Fountoulakis", "Kimon", ""], ["Demmel", "James", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1612.04197", "submitter": "Sandeep Aswath Narayana", "authors": "Sandeep Aswath Narayana", "title": "An Artificial Neural Networks based Temperature Prediction Framework for\n  Network-on-Chip based Multicore Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous improvement in silicon process technologies has made possible the\nintegration of hundreds of cores on a single chip. However, power and heat have\nbecome dominant constraints in designing these massive multicore chips causing\nissues with reliability, timing variations and reduced lifetime of the chips.\nDynamic Thermal Management (DTM) is a solution to avoid high temperatures on\nthe die. Typical DTM schemes only address core level thermal issues. However,\nthe Network-on-chip (NoC) paradigm, which has emerged as an enabling\nmethodology for integrating hundreds to thousands of cores on the same die can\ncontribute significantly to the thermal issues. Moreover, the typical DTM is\ntriggered reactively based on temperature measurements from on-chip thermal\nsensor requiring long reaction times whereas predictive DTM method estimates\nfuture temperature in advance, eliminating the chance of temperature overshoot.\nArtificial Neural Networks (ANNs) have been used in various domains for\nmodeling and prediction with high accuracy due to its ability to learn and\nadapt. This thesis concentrates on designing an ANN prediction engine to\npredict the thermal profile of the cores and Network-on-Chip elements of the\nchip. This thermal profile of the chip is then used by the predictive DTM that\ncombines both core level and network level DTM techniques. On-chip wireless\ninterconnect which is recently envisioned to enable energy-efficient data\nexchange between cores in a multicore environment, will be used to provide a\nbroadcast-capable medium to efficiently distribute thermal control messages to\ntrigger and manage the DTM schemes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 09:11:13 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Narayana", "Sandeep Aswath", ""]]}, {"id": "1612.04251", "submitter": "Yuan Tang", "authors": "Yuan Tang", "title": "TF.Learn: TensorFlow's High-level Module for Distributed Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 16:00:51 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Tang", "Yuan", ""]]}, {"id": "1612.04425", "submitter": "Yangyang Xu", "authors": "Zhimin Peng, Yangyang Xu, Ming Yan, Wotao Yin", "title": "On the Convergence of Asynchronous Parallel Iteration with Unbounded\n  Delays", "comments": "accepted to JORSC", "journal-ref": "Journal of the Operations Research Society of China, 7 (2019),\n  5-42", "doi": "10.1007/s40305-017-0183-1", "report-no": null, "categories": "math.OC cs.DC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the surge of asynchronous parallel\n(async-parallel) iterative algorithms due to problems involving very\nlarge-scale data and a large number of decision variables. Because of\nasynchrony, the iterates are computed with outdated information, and the age of\nthe outdated information, which we call delay, is the number of times it has\nbeen updated since its creation. Almost all recent works prove convergence\nunder the assumption of a finite maximum delay and set their stepsize\nparameters accordingly. However, the maximum delay is practically unknown.\n  This paper presents convergence analysis of an async-parallel method from a\nprobabilistic viewpoint, and it allows for large unbounded delays. An explicit\nformula of stepsize that guarantees convergence is given depending on delays'\nstatistics. With $p+1$ identical processors, we empirically measured that\ndelays closely follow the Poisson distribution with parameter $p$, matching our\ntheoretical model, and thus the stepsize can be set accordingly. Simulations on\nboth convex and nonconvex optimization problems demonstrate the validness of\nour analysis and also show that the existing maximum-delay induced stepsize is\ntoo conservative, often slowing down the convergence of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 23:02:26 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 16:17:32 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Peng", "Zhimin", ""], ["Xu", "Yangyang", ""], ["Yan", "Ming", ""], ["Yin", "Wotao", ""]]}, {"id": "1612.04519", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Data allocation on disks with solution reconfiguration (problems,\n  heuristics)", "comments": "10 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses problem of data allocation in two-layer computer storage\nwhile taking into account dynamic digraph(s) over computing tasks. The basic\nversion of data file allocation on parallel hard magnetic disks is considered\nas special bin packing model. Two problems of the allocation solution\nreconfiguration (restructuring) are suggested: (i) one-stage restructuring\nmodel, (ii) multistage restructuring models. Solving schemes are based on\nsimplified heuristics. Numerical examples illustrate problems and solving\nschemes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 07:52:54 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2016 13:35:53 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1612.04898", "submitter": "Sunil Thulasidasan", "authors": "Sunil Thulasidasan, Jeffrey Bilmes, Garrett Kenyon", "title": "Efficient Distributed Semi-Supervised Learning using Stochastic\n  Regularization over Affinity Graphs", "comments": "NIPS 2016 Workshop on Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": "LA-UR-16-28681", "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a computationally efficient, stochastic graph-regularization\ntechnique that can be utilized for the semi-supervised training of deep neural\nnetworks in a parallel or distributed setting. We utilize a technique, first\ndescribed in [13] for the construction of mini-batches for stochastic gradient\ndescent (SGD) based on synthesized partitions of an affinity graph that are\nconsistent with the graph structure, but also preserve enough stochasticity for\nconvergence of SGD to good local minima. We show how our technique allows a\ngraph-based semi-supervised loss function to be decomposed into a sum over\nobjectives, facilitating data parallelism for scalable training of machine\nlearning models. Empirical results indicate that our method significantly\nimproves classification accuracy compared to the fully-supervised case when the\nfraction of labeled data is low, and in the parallel case, achieves significant\nspeed-up in terms of wall-clock time to convergence. We show the results for\nboth sequential and distributed-memory semi-supervised DNN training on a speech\ncorpus.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 01:00:23 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 17:23:25 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Thulasidasan", "Sunil", ""], ["Bilmes", "Jeffrey", ""], ["Kenyon", "Garrett", ""]]}, {"id": "1612.04997", "submitter": "Jian Liu Mr.", "authors": "Jian Liu and Wenting Li and Ghassan O. Karame and N. Asokan", "title": "Scalable Byzantine Consensus via Hardware-assisted Secret Sharing", "comments": "This paper will be published in IEEE Transactions on Computers", "journal-ref": null, "doi": "10.1109/TC.2018.2860009", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surging interest in blockchain technology has revitalized the search for\neffective Byzantine consensus schemes. In particular, the blockchain community\nhas been looking for ways to effectively integrate traditional Byzantine\nfault-tolerant (BFT) protocols into a blockchain consensus layer allowing\nvarious financial institutions to securely agree on the order of transactions.\nHowever, existing BFT protocols can only scale to tens of nodes due to their\n$O(n^2)$ message complexity.\n  In this paper, we propose FastBFT, a fast and scalable BFT protocol. At the\nheart of FastBFT is a novel message aggregation technique that combines\nhardware-based trusted execution environments (TEEs) with lightweight secret\nsharing primitives. Combining this technique with several other optimizations\n(i.e., optimistic execution, tree topology and failure detection), FastBFT\nachieves low latency and high throughput even for large scale networks. Via\nsystematic analysis and experiments, we demonstrate that FastBFT has better\nscalability and performance than previous BFT protocols.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 09:50:34 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 02:50:58 GMT"}, {"version": "v3", "created": "Wed, 25 Apr 2018 17:10:48 GMT"}, {"version": "v4", "created": "Fri, 27 Jul 2018 03:05:52 GMT"}, {"version": "v5", "created": "Wed, 14 Nov 2018 06:26:51 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Liu", "Jian", ""], ["Li", "Wenting", ""], ["Karame", "Ghassan O.", ""], ["Asokan", "N.", ""]]}, {"id": "1612.05205", "submitter": "Mohammad Roohitavaf", "authors": "Mohammad Roohitavaf and Sandeep Kulkarni", "title": "GentleRain+: Making GentleRain Robust on Clock Anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal consistency is in an intermediate consistency model that can be\nachieved together with high availability and high performance requirements even\nin presence of network partitions. There are several proposals in the\nliterature for causally consistent data stores. Thanks to the use of single\nscalar physical clocks, GentleRain has a throughput higher than other proposals\nsuch as COPS or Orbe. However, both of its correctness and performance relay on\nmonotonic synchronized physical clocks. Specifically, if physical clocks go\nbackward its correctness is violated. In addition, GentleRain is sensitive on\nthe clock synchronization, and clock skew may slow write operations in\nGenlteRain. In this paper, we want to solve this issue in GenlteRain by using\nHybrid Logical Clock (HLC) instead of physical clocks. Using HLC, GentleRain\nprotocl is not sensitive on the clock skew anymore. In addition, even if clocks\ngo backward, the correctness of the system is not violated. Furthermore, by\nHLC, we timestamp versions with a clock very close to the physical clocks.\nThus, we can take causally consistency snapshot of the system at any give\nphysical time. We call GentleRain protocol with HLCs GentleRain+. We have\nimplemented GentleRain+ protocol, and have evaluated it experimentally.\nGentleRain+ provides faster write operations compare to GentleRain that rely\nsolely on physical clocks to achieve causal consistency. We have also shown\nthat using HLC instead of physical clock does not have any overhead. Thus, it\nmakes GentleRain more robust on clock anomalies at no cost.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 05:10:09 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Roohitavaf", "Mohammad", ""], ["Kulkarni", "Sandeep", ""]]}, {"id": "1612.05236", "submitter": "Shripad Gade", "authors": "Shripad Gade and Nitin H. Vaidya", "title": "Private Learning on Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual data collection and widespread deployment of machine learning\nalgorithms, particularly the distributed variants, have raised new privacy\nchallenges. In a distributed machine learning scenario, the dataset is stored\namong several machines and they solve a distributed optimization problem to\ncollectively learn the underlying model. We present a secure multi-party\ncomputation inspired privacy preserving distributed algorithm for optimizing a\nconvex function consisting of several possibly non-convex functions. Each\nindividual objective function is privately stored with an agent while the\nagents communicate model parameters with neighbor machines connected in a\nnetwork. We show that our algorithm can correctly optimize the overall\nobjective function and learn the underlying model accurately. We further prove\nthat under a vertex connectivity condition on the topology, our algorithm\npreserves privacy of individual objective functions. We establish limits on the\nwhat a coalition of adversaries can learn by observing the messages and states\nshared over a network.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:44:50 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Gade", "Shripad", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1612.05317", "submitter": "Seiichiro Tani", "authors": "Seiichiro Tani", "title": "A Fast Exact Quantum Algorithm for Solitude Verification", "comments": "26 pages, accepted for publication in Quantum Information and\n  Computation (QIC)", "journal-ref": "Quantum Information & Computation, vol.17, no.1&2, pp.15--40, 2017", "doi": null, "report-no": null, "categories": "quant-ph cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solitude verification is arguably one of the simplest fundamental problems in\ndistributed computing, where the goal is to verify that there is a unique\ncontender in a network. This paper devises a quantum algorithm that exactly\nsolves the problem on an anonymous network, which is known as a network model\nwith minimal assumptions [Angluin, STOC'80]. The algorithm runs in $O(N)$\nrounds if every party initially has the common knowledge of an upper bound $N$\non the number of parties. This implies that all solvable problems can be solved\nin $O(N)$ rounds on average without error (i.e., with zero-sided error) on the\nnetwork. As a generalization, a quantum algorithm that works in $O(N\\log_2\n(\\max\\{k,2\\}))$ rounds is obtained for the problem of exactly computing any\nsymmetric Boolean function, over $n$ distributed input bits, which is constant\nover all the $n$ bits whose sum is larger than $k$ for $k\\in \\{0,1,\\dots,\nN-1\\}$. All these algorithms work with the bit complexities bounded by a\npolynomial in $N$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 00:08:48 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tani", "Seiichiro", ""]]}, {"id": "1612.05477", "submitter": "Karan Mitra Dr", "authors": "Karan Mitra, Saguna Saguna, Christer \\r{A}hlund, Rajiv Ranjan", "title": "ALPINE: A Bayesian System for Cloud Performance Diagnosis and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud performance diagnosis and prediction is a challenging problem due to\nthe stochastic nature of the cloud systems. Cloud performance is affected by a\nlarge set of factors including (but not limited to) virtual machine types,\nregions, workloads, wide area network delay and bandwidth. Therefore,\nnecessitating the determination of complex relationships between these factors.\nThe current research in this area does not address the challenge of building\nmodels that capture the uncertain and complex relationships between these\nfactors. Further, the challenge of cloud performance prediction under\nuncertainty has not garnered sufficient attention. This paper proposes develops\nand validates ALPINE, a Bayesian system for cloud performance diagnosis and\nprediction. ALPINE incorporates Bayesian networks to model uncertain and\ncomplex relationships between several factors mentioned above. It handles\nmissing, scarce and sparse data to diagnose and predict stochastic cloud\nperformance efficiently. We validate our proposed system using extensive real\ndata and trace-driven analysis and show that it predicts cloud performance with\nhigh accuracy of 91.93%.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 14:21:18 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Mitra", "Karan", ""], ["Saguna", "Saguna", ""], ["\u00c5hlund", "Christer", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1612.05633", "submitter": "Mansaf Alam Dr", "authors": "Syed Arshad Ali and Mansaf Alam", "title": "A Relative Study of Task Scheduling Algorithms in Cloud Computing\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing is a paradigm of both parallel processing and distributed\ncomputing. It offers computing facilities as a utility service in pay as par\nuse manner. Virtualization, self service provisioning, elasticity and pay per\nuse are the key features of Cloud Computing. It provides different types of\nresources over the Internet to perform user submitted tasks. In cloud\nenvironment, huge number of tasks are executed simultaneously, an effective\nTask Scheduling is required to gain better performance of the cloud system.\nVarious Cloud Based Task Scheduling algorithms are available that schedule the\ntask of user to resources for execution. Due to the novelty of Cloud Computing,\ntraditional scheduling algorithms cannot satisfy the needs of cloud , the\nresearchers are trying to modify traditional algorithms that can fulfill the\ncloud requirements like rapid elasticity, resource pooling and on demand self\nservice. In this paper the current state of Task Scheduling algorithms has been\ndiscussed and compared on the basis of various scheduling parameters like\nexecution time, throughput, make span, resource utilization, quality of\nservice, energy consumption, response time and cost.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 06:55:22 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ali", "Syed Arshad", ""], ["Alam", "Mansaf", ""]]}, {"id": "1612.05665", "submitter": "Yihan Sun", "authors": "Yihan Sun and Daniel Ferizovic and Guy E. Blelloch", "title": "PAM: Parallel Augmented Maps", "comments": null, "journal-ref": "Yihan Sun, Daniel Ferizovic, and Guy E. Belloch. 2018. PAM:\n  parallel augmented maps. In Proceedings of the 23rd ACM SIGPLAN Symposium on\n  Principles and Practice of Parallel Programming (PPoPP '18). ACM, New York,\n  NY, USA, 290-304", "doi": "10.1145/3178487.3178509", "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered (key-value) maps are an important and widely-used data type for\nlarge-scale data processing frameworks. Beyond simple search, insertion and\ndeletion, more advanced operations such as range extraction, filtering, and\nbulk updates form a critical part of these frameworks.\n  We describe an interface for ordered maps that is augmented to support fast\nrange queries and sums, and introduce a parallel and concurrent library called\nPAM (Parallel Augmented Maps) that implements the interface. The interface\nincludes a wide variety of functions on augmented maps ranging from basic\ninsertion and deletion to more interesting functions such as union,\nintersection, filtering, extracting ranges, splitting, and range-sums. We\ndescribe algorithms for these functions that are efficient both in theory and\npractice.\n  As examples of the use of the interface and the performance of PAM, we apply\nthe library to four applications: simple range sums, interval trees, 2D range\ntrees, and ranked word index searching. The interface greatly simplifies the\nimplementation of these data structures over direct implementations.\nSequentially the code achieves performance that matches or exceeds existing\nlibraries designed specially for a single application, and in parallel our\nimplementation gets speedups ranging from 40 to 90 on 72 cores with 2-way\nhyperthreading.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 22:02:49 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 22:36:15 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 18:31:41 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sun", "Yihan", ""], ["Ferizovic", "Daniel", ""], ["Blelloch", "Guy E.", ""]]}, {"id": "1612.05767", "submitter": "Marjorie Bournat", "authors": "Marjorie Bournat (Regal), Swan Dubois (Regal), Franck Petit (Regal)", "title": "Computability of Perpetual Exploration in Highly Dynamic Rings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider systems made of autonomous mobile robots evolving in highly\ndynamic discrete environment i.e., graphs where edges may appear and disappear\nunpredictably without any recurrence, stability, nor periodicity assumption.\nRobots are uniform (they execute the same algorithm), they are anonymous (they\nare devoid of any observable ID), they have no means allowing them to\ncommunicate together, they share no common sense of direction, and they have no\nglobal knowledge related to the size of the environment. However, each of them\nis endowed with persistent memory and is able to detect whether it stands alone\nat its current location. A highly dynamic environment is modeled by a graph\nsuch that its topology keeps continuously changing over time. In this paper, we\nconsider only dynamic graphs in which nodes are anonymous, each of them is\ninfinitely often reachable from any other one, and such that its underlying\ngraph (i.e., the static graph made of the same set of nodes and that includes\nall edges that are present at least once over time) forms a ring of arbitrary\nsize. In this context, we consider the fundamental problem of perpetual\nexploration: each node is required to be infinitely often visited by a robot.\nThis paper analyzes the computability of this problem in (fully) synchronous\nsettings, i.e., we study the deterministic solvability of the problem with\nrespect to the number of robots. We provide three algorithms and two\nimpossibility results that characterize, for any ring size, the necessary and\nsufficient number of robots to perform perpetual exploration of highly dynamic\nrings.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 13:52:55 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 11:35:04 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Bournat", "Marjorie", "", "Regal"], ["Dubois", "Swan", "", "Regal"], ["Petit", "Franck", "", "Regal"]]}, {"id": "1612.05858", "submitter": "Afsin Akdogan", "authors": "Afsin Akdogan", "title": "Partitioning, Indexing and Querying Spatial Data on Cloud", "comments": "PhD Dissertation - University of Southern California", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of mobile devices (e.g., smartphones, wearable technologies) is\nrapidly growing. In line with this trend, a massive amount of spatial data is\nbeing collected since these devices allow users to geo-tag user-generated\ncontent. Clearly, a scalable computing infrastructure is needed to manage such\nlarge datasets. Meanwhile, Cloud Computing service providers (e.g., Amazon,\nGoogle, and Microsoft) allow users to lease computing resources. However, most\nof the existing spatial indexing techniques are designed for the centralized\nparadigm which is limited to the capabilities of a single sever. To address the\nscalability shortcomings of existing approaches, we provide a study that focus\non generating a distributed spatial index structure that not only scales out to\nmultiple servers but also scales up since it fully exploits the multi-core CPUs\navailable on each server using Voronoi diagram as the partitioning and indexing\ntechnique which we also use to process spatial queries effectively. More\nspecifically, since the data objects continuously move and issue position\nupdates to the index structure, we collect the latest positions of objects and\nperiodically generate a read-only index to eliminate costly distributed\nupdates. Our approach scales near-linearly in index construction and query\nprocessing, and can efficiently construct an index for millions of objects\nwithin a few seconds. In addition to scalability and efficiency, we also aim to\nmaximize the server utilization that can support the same workload with less\nnumber of servers. Server utilization is a crucial point while using Cloud\nComputing because users are charged based on the total amount of time they\nreserve each server, with no consideration of utilization.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 06:24:06 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Akdogan", "Afsin", ""]]}, {"id": "1612.05859", "submitter": "Afsin Akdogan", "authors": "Afsin Akdogan, Hien To", "title": "Distributed Data Processing Frameworks for Big Graph Data", "comments": "Survey paper that covers data processing frameworks for big graph\n  data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we create so much data (2.5 quintillion bytes every day) that 90% of\nthe data in the world today has been created in the last two years alone [1].\nThis data comes from sensors used to gather traffic or climate information,\nposts to social media sites, photos, videos, emails, purchase transaction\nrecords, call logs of cellular networks, etc. This data is big data. In this\nreport, we first briefly discuss what programming models are used for big data\nprocessing, and focus on graph data and do a survey study about what\nprogramming models/frameworks are used to solve graph problems at very\nlarge-scale. In section 2, we introduce the programming models which are not\nspecifically designed to handle graph data but we include them in this survey\nbecause we believe these are important frameworks and/or there have been\nstudies to customize them for more efficient graph processing. In section 3, we\ndiscuss some techniques that yield up to 1340 times speedup for some certain\ngraph problems when applied to Hadoop. In section 4, we discuss vertex-based\nprogramming model which is simply designed to process large graphs and the\nframeworks adapting it. In section 5, we implement two of the fundamental graph\nalgorithms (Page Rank and Weight Bipartite Matching), and run them on a single\nnode as the baseline approach to see how fast they are for large datasets and\nwhether it is worth to partition them.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 06:32:31 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Akdogan", "Afsin", ""], ["To", "Hien", ""]]}, {"id": "1612.06090", "submitter": "Fabio Baruffa Dr.", "authors": "Fabio Baruffa, Luigi Iapichino, Nicolay J. Hammer, Vasileios Karakasis", "title": "Performance Optimisation of Smoothed Particle Hydrodynamics Algorithms\n  for Multi/Many-Core Architectures", "comments": "8 pages, 2 columns, 4 figures, accepted as paper at HPCS Proceedings\n  2017, IEEE XPLORE", "journal-ref": "proceedings of the 2017 International Conference on High\n  Performance Computing & Simulation (HPCS 2017), 381", "doi": "10.1109/HPCS.2017.64", "report-no": null, "categories": "cs.DC astro-ph.IM physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a strategy for code modernisation of Gadget, a widely used\ncommunity code for computational astrophysics. The focus of this work is on\nnode-level performance optimisation, targeting current multi/many-core IntelR\narchitectures. We identify and isolate a sample code kernel, which is\nrepresentative of a typical Smoothed Particle Hydrodynamics (SPH) algorithm.\nThe code modifications include threading parallelism optimisation, change of\nthe data layout into Structure of Arrays (SoA), auto-vectorisation and\nalgorithmic improvements in the particle sorting. We obtain shorter execution\ntime and improved threading scalability both on Intel XeonR ($2.6 \\times$ on\nIvy Bridge) and Xeon PhiTM ($13.7 \\times$ on Knights Corner) systems. First few\ntests of the optimised code result in $19.1 \\times$ faster execution on second\ngeneration Xeon Phi (Knights Landing), thus demonstrating the portability of\nthe devised optimisation solutions to upcoming architectures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 09:45:25 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 14:34:36 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Baruffa", "Fabio", ""], ["Iapichino", "Luigi", ""], ["Hammer", "Nicolay J.", ""], ["Karakasis", "Vasileios", ""]]}, {"id": "1612.06302", "submitter": "Tadeusz Kobus", "authors": "Tadeusz Kobus, Maciej Kokoci\\'nski, Pawe{\\l} T. Wojciechowski", "title": "Hybrid Transactional Replication: State-Machine and Deferred-Update\n  Replication Combined", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, 2018", "doi": "10.1109/TPDS.2018.2796079", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Hybrid Transactional Replication (HTR), a novel replication scheme\nfor highly dependable services. It combines two schemes: a transaction is\nexecuted either optimistically by only one service replica in the deferred\nupdate mode (DU), or deterministically by all replicas in the state machine\nmode (SM); the choice is made by an oracle. The DU mode allows for parallelism\nand thus takes advantage of multicore hardware. In contrast to DU, the SM mode\nguarantees abort-free execution, so it is suitable for irrevocable operations\nand transactions generating high contention. For expressiveness, transactions\ncan be discarded or retried on demand. We formally prove that the higher\nflexibility of the scheme does not come at the cost of weaker guarantees for\nclients: HTR satisfies strong consistency guarantees akin to those provided by\nother popular transactional replication schemes such as Deferred Update\nReplication. We developed HTR-enabled Paxos STM, an object-based distributed\ntransactional memory system, and evaluated it thoroughly under various\nworkloads. We show the benefits of using a novel oracle, which relies on\nmachine learning techniques for automatic adaptation to changing conditions. In\nour tests, the ML-based oracle provides up to 50% improvement in throughput\nwhen compared to the system running with DU-only or SM-only oracles. Our\napproach is inspired by a well known algorithm used in the context of the\nmulti-armed bandit problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 18:36:51 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Kobus", "Tadeusz", ""], ["Kokoci\u0144ski", "Maciej", ""], ["Wojciechowski", "Pawe\u0142 T.", ""]]}, {"id": "1612.06507", "submitter": "Ruozhou Yu", "authors": "Ruozhou Yu, Guoliang Xue, Xiang Zhang, Dan Li", "title": "Survivable and Bandwidth-Guaranteed Embedding of Virtual Clusters in\n  Cloud Data Centers (Extended Version)", "comments": "Added references [1, 2] to version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has emerged as a powerful and elastic platform for internet\nservice hosting, yet it also draws concerns of the unpredictable performance of\ncloud-based services due to network congestion. To offer predictable\nperformance, the virtual cluster abstraction of cloud services has been\nproposed, which enables allocation and performance isolation regarding both\ncomputing resources and network bandwidth in a simplified virtual network\nmodel. One issue arisen in virtual cluster allocation is the survivability of\ntenant services against physical failures. Existing works have studied virtual\ncluster backup provisioning with fixed primary embeddings, but have not\nconsidered the impact of primary embeddings on backup resource consumption. To\naddress this issue, in this paper we study how to embed virtual clusters\nsurvivably in the cloud data center, by jointly optimizing primary and backup\nembeddings of the virtual clusters. We formally define the survivable virtual\ncluster embedding problem. We then propose a novel algorithm, which computes\nthe most resource-efficient embedding given a tenant request. Since the optimal\nalgorithm has high time complexity, we further propose a faster heuristic\nalgorithm, which is several orders faster than the optimal solution, yet able\nto achieve similar performance. Besides theoretical analysis, we evaluate our\nalgorithms via extensive simulations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 04:55:44 GMT"}, {"version": "v2", "created": "Sun, 8 Jan 2017 19:15:21 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Yu", "Ruozhou", ""], ["Xue", "Guoliang", ""], ["Zhang", "Xiang", ""], ["Li", "Dan", ""]]}, {"id": "1612.06748", "submitter": "Oskar Schirmer", "authors": "Oskar Schirmer", "title": "NOP - A Simple Experimental Processor for Parallel Deployment", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of a parallel computing system using several thousands or even up\nto a million processors asks for processing units that are simple and thus\nsmall in space, to make as many processing units as possible fit on a single\ndie.\n  The design presented herewith is far from being optimised, it is not meant to\ncompete with industry performance devices. Its main purpose is to allow for a\nprototypical implementation of a dynamic software system as a proof of concept.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:49:43 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Schirmer", "Oskar", ""]]}, {"id": "1612.06749", "submitter": "Oskar Schirmer", "authors": "Oskar Schirmer", "title": "GuStL - An Experimental Guarded States Language", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming a parallel computing system that consists of several thousands or\neven up to a million message passing processing units may ask for a language\nthat supports waiting for and sending messages over hardware channels. As\nprograms are looked upon as state machines, the language provides syntax to\nimplement a main event driven loop. The language presented herewith surely will\nnot serve as a generic programming language for any arbitrary task. Its main\npurpose is to allow for a prototypical implementation of a dynamic software\nsystem as a proof of concept.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:50:21 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 09:31:29 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Schirmer", "Oskar", ""]]}, {"id": "1612.07128", "submitter": "Abdurrachman Mappuji", "authors": "Abdurrachman Mappuji, Nazrul Effendy, Muhamad Mustaghfirin, Fandy\n  Sondok, Rara Priska Yuniar, Sheptiani Putri Pangesti", "title": "Study of Raspberry Pi 2 Quad-core Cortex A7 CPU Cluster as a Mini\n  Supercomputer", "comments": "Pre-print of conference paper on International Conference on\n  Information Technology and Electrical Engineering", "journal-ref": null, "doi": "10.1109/ICITEED.2016.7863250", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance computing (HPC) devices is no longer exclusive for academic,\nR&D, or military purposes. The use of HPC device such as supercomputer now\ngrowing rapidly as some new area arise such as big data, and computer\nsimulation. It makes the use of supercomputer more inclusive. Todays\nsupercomputer has a huge computing power, but requires an enormous amount of\nenergy to operate. In contrast a single board computer (SBC) such as Raspberry\nPi has minimum computing power, but require a small amount of energy to\noperate, and as a bonus it is small and cheap. This paper covers the result of\nutilizing many Raspberry Pi 2 SBCs, a quad-core Cortex A7 900 MHz, as a cluster\nto compensate its computing power. The high performance linpack (HPL) is used\nto benchmark the computing power, and a power meter with resolution 10mV / 10mA\nis used to measure the power consumption. The experiment shows that the\nincrease of number of cores in every SBC member in a cluster is not giving\nsignificant increase in computing power. This experiment give a recommendation\nthat 4 nodes is a maximum number of nodes for SBC cluster based on the\ncharacteristic of computing performance and power consumption.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 18:23:44 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Mappuji", "Abdurrachman", ""], ["Effendy", "Nazrul", ""], ["Mustaghfirin", "Muhamad", ""], ["Sondok", "Fandy", ""], ["Yuniar", "Rara Priska", ""], ["Pangesti", "Sheptiani Putri", ""]]}, {"id": "1612.07404", "submitter": "Arijit Khan", "authors": "Arijit Khan", "title": "Vertex-Centric Graph Processing: The Good, the Bad, and the Ugly", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed graph algorithms that adopt an iterative vertex-centric\nframework for graph processing, popularized by the Google's Pregel system.\nSince then, there are several attempts to implement many graph algorithms in a\nvertex-centric framework, as well as efforts to design optimization techniques\nfor improving the efficiency. However, to the best of our knowledge, there has\nnot been any systematic study to compare these vertex-centric implementations\nwith their sequential counterparts. Our paper addresses this gap in two ways.\n(1) We analyze the computational complexity of such implementations with the\nnotion of time-processor product, and benchmark several vertex-centric graph\nalgorithms whether they perform more work with respect to their best-known\nsequential solutions. (2) Employing the concept of balanced practical Pregel\nalgorithms, we study if these implementations suffer from imbalanced workload\nand large number of iterations. Our findings illustrate that with the exception\nof Euler tour tree algorithm, all other algorithms either perform more work\nthan their best-known sequential approach, or suffer from imbalanced workload/\nlarge number of iterations, or even both. We also emphasize on graph algorithms\nthat are fundamentally difficult to be expressed in vertex-centric frameworks,\nand conclude by discussing the road ahead for distributed graph processing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 00:53:41 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Khan", "Arijit", ""]]}, {"id": "1612.07433", "submitter": "Harishchandra Dubey", "authors": "Rakesh K. Lenka, Rabindra K. Barik, Noopur Gupta, Syed Mohd Ali, Amiya\n  Rath, Harishchandra Dubey", "title": "Comparative Analysis of SpatialHadoop and GeoSpark for Geospatial Big\n  Data Analytics", "comments": "6 pages, 7 figures,table1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this digitalised world where every information is stored, the data a are\ngrowing exponentially. It is estimated that data are doubles itself every two\nyears. Geospatial data are one of the prime contributors to the big data\nscenario. There are numerous tools of the big data analytics. But not all the\nbig data analytics tools are capabilities to handle geospatial big data. In the\npresent paper, it has been discussed about the recent two popular open source\ngeospatial big data analytical tools i.e. Spatial- Hadoop and GeoSpark which\ncan be used for analysis and process the geospatial big data in efficient\nmanner. It has compared the architectural view of SpatialHadoop and GeoSpark.\nThrough the architectural comparison, it has also summarised the merits and\ndemerits of these tools according the execution times and volume of the data\nwhich has been used.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 03:46:11 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 21:48:57 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Lenka", "Rakesh K.", ""], ["Barik", "Rabindra K.", ""], ["Gupta", "Noopur", ""], ["Ali", "Syed Mohd", ""], ["Rath", "Amiya", ""], ["Dubey", "Harishchandra", ""]]}, {"id": "1612.07526", "submitter": "Shengguo Li", "authors": "Shengguo Li, Francois-Henry Rouet, Jie Liu, Chun Huang, Xingyu Gao and\n  Xuebin Chi", "title": "An efficient hybrid tridiagonal divide-and-conquer algorithm on\n  distributed memory architectures", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient divide-and-conquer (DC) algorithm is proposed for\nthe symmetric tridiagonal matrices based on ScaLAPACK and the hierarchically\nsemiseparable (HSS) matrices. HSS is an important type of rank-structured\nmatrices.Most time of the DC algorithm is cost by computing the eigenvectors\nvia the matrix-matrix multiplications (MMM). In our parallel hybrid DC (PHDC)\nalgorithm, MMM is accelerated by using the HSS matrix techniques when the\nintermediate matrix is large. All the HSS algorithms are done via the package\nSTRUMPACK. PHDC has been tested by using many different matrices. Compared with\nthe DC implementation in MKL, PHDC can be faster for some matrices with few\ndeflations when using hundreds of processes. However, the gains decrease as the\nnumber of processes increases. The comparisons of PHDC with ELPA (the\nEigenvalue soLvers for Petascale Applications library) are similar. PHDC is\nusually slower than MKL and ELPA when using 300 or more processes on Tianhe-2\nsupercomputer.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:19:09 GMT"}], "update_date": "2016-12-27", "authors_parsed": [["Li", "Shengguo", ""], ["Rouet", "Francois-Henry", ""], ["Liu", "Jie", ""], ["Huang", "Chun", ""], ["Gao", "Xingyu", ""], ["Chi", "Xuebin", ""]]}, {"id": "1612.07702", "submitter": "Tiago Vale", "authors": "Tiago M. Vale and Jo\\~ao A. Silva and Ricardo J. Dias and Jo\\~ao M.\n  Louren\\c{c}o", "title": "Pot: Deterministic transactional execution", "comments": "Published in ACM Transactions on Architecture and Code Optimization\n  (TACO) 13, 4", "journal-ref": "ACM Trans. Archit. Code Optim. 13, 4, Article 52 (December 2016)", "doi": "10.1145/3017993", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Pot, a system that leverages the concept of preordered\ntransactions to achieve deterministic multithreaded execution of programs that\nuse Transactional Memory. Preordered transactions eliminate the root cause of\nnondeterminism in transactional execution: they provide the illusion of\nexecuting in a deterministic serial order, unlike traditional transactions\nwhich appear to execute in a nondeterministic order that can change from\nexecution to execution. Pot uses a new concurrency control protocol that\nexploits the serialization order to distinguish between fast and speculative\ntransaction execution modes in order to mitigate the overhead of imposing a\ndeterministic order. We build two Pot prototypes: one using STM and another\nusing off-the-shelf HTM. To the best of our knowledge, Pot enables\ndeterministic execution of programs using off-the-shelf HTM for the first time.\nAn experimental evaluation shows that Pot achieves deterministic execution of\nTM programs with low overhead, sometimes even outperforming nondeterministic\nexecutions, and clearly outperforming the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 17:09:57 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Vale", "Tiago M.", ""], ["Silva", "Jo\u00e3o A.", ""], ["Dias", "Ricardo J.", ""], ["Louren\u00e7o", "Jo\u00e3o M.", ""]]}, {"id": "1612.08060", "submitter": "Luke Olson", "authors": "Amanda Bienz, William D. Gropp, Luke N. Olson", "title": "Node Aware Sparse Matrix-Vector Multiplication", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse matrix-vector multiply (SpMV) operation is a key computational\nkernel in many simulations and linear solvers. The large communication\nrequirements associated with a reference implementation of a parallel SpMV\nresult in poor parallel scalability. The cost of communication depends on the\nphysical locations of the send and receive processes: messages injected into\nthe network are more costly than messages sent between processes on the same\nnode. In this paper, a node aware parallel SpMV (NAPSpMV) is introduced to\nexploit knowledge of the system topology, specifically the node-processor\nlayout, to reduce costs associated with communication. The values of the input\nvector are redistributed to minimize both the number and the size of messages\nthat are injected into the network during a SpMV, leading to a reduction in\ncommunication costs. A variety of computational experiments that highlight the\nefficiency of this approach are presented.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 18:40:46 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 19:33:50 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 16:22:24 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Bienz", "Amanda", ""], ["Gropp", "William D.", ""], ["Olson", "Luke N.", ""]]}, {"id": "1612.08163", "submitter": "Mahdi Jelodari Mamaghani", "authors": "Ana Lava, Mahdi Jelodari Mamaghani, Siamak Mohammadi and Steve Furber", "title": "Application-aware Retiming of Accelerators: A High-level Data-driven\n  Approach", "comments": "7 pages, 6 figures, submitted to IEEE Design and Test Journal -\n  special issue on Accelerators in October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexibility at hardware level is the main driving force behind adaptive\nsystems whose aim is to realise microarhitecture deconfiguration 'online'. This\nfeature allows the software/hardware stack to tolerate drastic changes of the\nworkload in data centres. With emerge of FPGA reconfigurablity this technology\nis becoming a mainstream computing paradigm. Adaptivity is usually accompanied\nby the high-level tools to facilitate multi-dimensional space exploration. An\nessential aspect in this space is memory orchestration where on-chip and\noff-chip memory distribution significantly influences the architecture in\ncoping with the critical spatial and timing constraints, e.g. Place and Route.\nThis paper proposes a memory smart technique for a particular class of adaptive\nsystems: Elastic Circuits which enjoy slack elasticity at fine level of\ngranularity. We explore retiming of a set of popular benchmarks via\ninvestigating the memory distribution within and among accelerators. The area,\nperformance and power patterns are adopted by our high-level synthesis\nframework, with respect to the behaviour of the input descriptions, to improve\nthe quality of the synthesised elastic circuits.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 10:55:46 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Lava", "Ana", ""], ["Mamaghani", "Mahdi Jelodari", ""], ["Mohammadi", "Siamak", ""], ["Furber", "Steve", ""]]}, {"id": "1612.08463", "submitter": "Ji Liu", "authors": "Ji Liu, Shaoshuai Mou, A. Stephen Morse, Brian D. O. Anderson,\n  Changbin Yu", "title": "Request-Based Gossiping without Deadlocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the distributed averaging problem is meant the problem of computing the\naverage value of a set of numbers possessed by the agents in a distributed\nnetwork using only communication between neighboring agents. Gossiping is a\nwell-known approach to the problem which seeks to iteratively arrive at a\nsolution by allowing each agent to interchange information with at most one\nneighbor at each iterative step. Crafting a gossiping protocol which\naccomplishes this is challenging because gossiping is an inherently\ncollaborative process which can lead to deadlocks unless careful precautions\nare taken to ensure that it does not. Many gossiping protocols are\nrequest-based which means simply that a gossip between two agents will occur\nwhenever one of the two agents accepts a request to gossip placed by the other.\nIn this paper, we present three deterministic request-based protocols. We show\nby example that the first can deadlock. The second is guaranteed to avoid\ndeadlocks and requires fewer transmissions per iteration than standard\nbroadcast-based distributed averaging protocols by exploiting the idea of local\nordering together with the notion of an agent's neighbor queue; the protocol\nrequires the simplest queue updates, which provides an in-depth understanding\nof how local ordering and queue updates avoid deadlocks. It is shown that a\nthird protocol which uses a slightly more complicated queue update rule can\nlead to significantly faster convergence; a worst case bound on convergence\nrate is provided.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 00:24:51 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Liu", "Ji", ""], ["Mou", "Shaoshuai", ""], ["Morse", "A. Stephen", ""], ["Anderson", "Brian D. O.", ""], ["Yu", "Changbin", ""]]}, {"id": "1612.08543", "submitter": "Amir Hossein Akhavan Rahnama", "authors": "Amir Hossein Akhavan Rahnama", "title": "Distributed Real-Time Sentiment Analysis for Big Data Social Streams", "comments": null, "journal-ref": "IEEE 2014 International Conference on Control, Decision and\n  Information Technologies (CoDIT)", "doi": "10.1109/CoDIT.2014.6996998", "report-no": null, "categories": "stat.ML cs.CL cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data trend has enforced the data-centric systems to have continuous fast\ndata streams. In recent years, real-time analytics on stream data has formed\ninto a new research field, which aims to answer queries about\nwhat-is-happening-now with a negligible delay. The real challenge with\nreal-time stream data processing is that it is impossible to store instances of\ndata, and therefore online analytical algorithms are utilized. To perform\nreal-time analytics, pre-processing of data should be performed in a way that\nonly a short summary of stream is stored in main memory. In addition, due to\nhigh speed of arrival, average processing time for each instance of data should\nbe in such a way that incoming instances are not lost without being captured.\nLastly, the learner needs to provide high analytical accuracy measures.\nSentinel is a distributed system written in Java that aims to solve this\nchallenge by enforcing both the processing and learning process to be done in\ndistributed form. Sentinel is built on top of Apache Storm, a distributed\ncomputing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel\ndecision tree-learning algorithm based on the VFDT, with ability of enabling\nparallel classification in distributed environments. Sentinel also uses\nSpaceSaving to keep a summary of the data stream and stores its summary in a\nsynopsis data structure. Application of Sentinel on Twitter Public Stream API\nis shown and the results are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:10:18 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Rahnama", "Amir Hossein Akhavan", ""]]}, {"id": "1612.08608", "submitter": "Asim Kadav", "authors": "Asim Kadav, Erik Kruus", "title": "ASAP: Asynchronous Approximate Data-Parallel Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging workloads, such as graph processing and machine learning are\napproximate because of the scale of data involved and the stochastic nature of\nthe underlying algorithms. These algorithms are often distributed over multiple\nmachines using bulk-synchronous processing (BSP) or other synchronous\nprocessing paradigms such as map-reduce. However, data parallel processing\nprimitives such as repeated barrier and reduce operations introduce high\nsynchronization overheads. Hence, many existing data-processing platforms use\nasynchrony and staleness to improve data-parallel job performance. Often, these\nsystems simply change the synchronous communication to asynchronous between the\nworker nodes in the cluster. This improves the throughput of data processing\nbut results in poor accuracy of the final output since different workers may\nprogress at different speeds and process inconsistent intermediate outputs.\n  In this paper, we present ASAP, a model that provides asynchronous and\napproximate processing semantics for data-parallel computation. ASAP provides\nfine-grained worker synchronization using NOTIFY-ACK semantics that allows\nindependent workers to run asynchronously. ASAP also provides stochastic reduce\nthat provides approximate but guaranteed convergence to the same result as an\naggregated all-reduce. In our results, we show that ASAP can reduce\nsynchronization costs and provides 2-10X speedups in convergence and up to 10X\nsavings in network costs for distributed machine learning applications and\nprovides strong convergence guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 12:40:39 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Kadav", "Asim", ""], ["Kruus", "Erik", ""]]}, {"id": "1612.08701", "submitter": "Ahmed Mateen Mr.", "authors": "Ahmed Mateen and Lareab Chaudhary", "title": "A Peta-Scale Data Movement and Analysis in Data Warehouse (APSDMADW)", "comments": "5 pages", "journal-ref": "International Journal of Computer Applications Foundation of\n  Computer Science (FCS), NY, USA Volume 151 - Number 7 Year of Publication:\n  2016", "doi": "10.5120/ijca2016911702", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this research paper so as to handle Information warehousing as well as\nonline synthetic dispensation OLAP are necessary aspects of conclusion support\nwhich takes more and more turn into a focal point of the data source\nbusiness.This paper offers an outline of information warehousing also OLAP\nsystems with a highlighting on their latest necessities.All of us explain\nbackside end tackle for extract clean-up and load information into an Data\nwarehouse multi dimensional data model usual of OLAP frontend user tools for\nquery and facts evaluation server extension for useful query dispensation and\napparatus for metadata managing and for supervision the stockroom. Insights\ncentered on complete data on customer actions manufactured goods act and souk\nperformance are powerful advance and opposition in the internet gap .In this\nresearch conclude the company inspiration and the program and efficiency of\nservers working in a data warehouse through use of some new techniques and get\nbetter and efficient results. Data in petabyte scale. This test shows the data\ndropping rate in data warehouse. The locomotive is in creation at Yahoo! since\n2007 and presently manages more than half a dozen peta bytes of data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 18:37:46 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Mateen", "Ahmed", ""], ["Chaudhary", "Lareab", ""]]}, {"id": "1612.08702", "submitter": "Ahmed Mateen Mr.", "authors": "Ahmed Mateen and Lareab Chaudhary", "title": "Reduce The Wastage of Data During Movement in Data Warehouse", "comments": "5 pages", "journal-ref": "International Journal of Computer Applications Foundation of\n  Computer Science (FCS), NY, USA Volume 152 - Number 8 Year of Publication:\n  2016", "doi": "10.5120/ijca2016911907 10.5120/ijca2016911907", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this research paper so as to handle Data in warehousing as well as reduce\nthe wastage of data and provide a better results which takes more and more turn\ninto a focal point of the data source business. Data warehousing and on-line\nanalytical processing (OLAP) are vital fundamentals of resolution hold, which\nhas more and more become a focal point of the database manufacturing. Lots of\nmarketable yield and services be at the present accessible, and the entire\nprimary database management organization vendor nowadays have contributions in\nthe area assessment hold up spaces some quite dissimilar necessities on record\ntechnology compare to conventional on-line transaction giving out application.\nThis article gives a general idea of data warehousing and OLAP technologies,\nwith the highlighting on top of their latest necessities. So tools which is\nused for extract, clean-up and load information into back end of a information\nwarehouse; multidimensional data model usual of OLAP; front end client tools\nfor querying and data analysis; server extension for proficient query\nprocessing; and tools for data managing and for administration the warehouse.\nIn adding to survey the circumstances of the art, this article also identify a\nnumber of capable research issue, a few which are interrelated to data wastage\ntroubles. In this paper use some new techniques to reduce the wastage of data,\nprovide better results. In this paper take some values, put in anova table and\ngive results through graphs which shows performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 18:42:28 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Mateen", "Ahmed", ""], ["Chaudhary", "Lareab", ""]]}, {"id": "1612.08709", "submitter": "Mark Tygert", "authors": "Huamin Li, Yuval Kluger, and Mark Tygert", "title": "Randomized algorithms for distributed computation of principal component\n  analysis and singular value decomposition", "comments": "21 pages, 29 tables, 1 figure, 8 algorithms in pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms provide solutions to two ubiquitous problems: (1) the\ndistributed calculation of a principal component analysis or singular value\ndecomposition of a highly rectangular matrix, and (2) the distributed\ncalculation of a low-rank approximation (in the form of a singular value\ndecomposition) to an arbitrary matrix. Carefully honed algorithms yield results\nthat are uniformly superior to those of the stock, deterministic\nimplementations in Spark (the popular platform for distributed computation); in\nparticular, whereas the stock software will without warning return left\nsingular vectors that are far from numerically orthonormal, a significantly\nburnished randomized implementation generates left singular vectors that are\nnumerically orthonormal to nearly the machine precision.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 19:06:13 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 22:06:19 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 23:04:43 GMT"}, {"version": "v4", "created": "Mon, 1 Jan 2018 20:24:15 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Li", "Huamin", ""], ["Kluger", "Yuval", ""], ["Tygert", "Mark", ""]]}, {"id": "1612.09001", "submitter": "Kaipeng Li", "authors": "Kaipeng Li, Amanullah Ghazi, Chance Tarver, Jani Boutellier, Mahmoud\n  Abdelaziz, Lauri Anttila, Markku Juntti, Mikko Valkama, and Joseph R.\n  Cavallaro", "title": "Parallel Digital Predistortion Design on Mobile GPU and Embedded\n  Multicore CPU for Mobile Transmitters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital predistortion (DPD) is a widely adopted baseband processing technique\nin current radio transmitters. While DPD can effectively suppress unwanted\nspurious spectrum emissions stemming from imperfections of analog RF and\nbaseband electronics, it also introduces extra processing complexity and poses\nchallenges on efficient and flexible implementations, especially for mobile\ncellular transmitters, considering their limited computing power compared to\nbasestations. In this paper, we present high data rate implementations of\nbroadband DPD on modern embedded processors, such as mobile GPU and multicore\nCPU, by taking advantage of emerging parallel computing techniques for\nexploiting their computing resources. We further verify the suppression effect\nof DPD experimentally on real radio hardware platforms. Performance evaluation\nresults of our DPD design demonstrate the high efficacy of modern general\npurpose mobile processors on accelerating DPD processing for a mobile\ntransmitter.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 22:51:34 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Li", "Kaipeng", ""], ["Ghazi", "Amanullah", ""], ["Tarver", "Chance", ""], ["Boutellier", "Jani", ""], ["Abdelaziz", "Mahmoud", ""], ["Anttila", "Lauri", ""], ["Juntti", "Markku", ""], ["Valkama", "Mikko", ""], ["Cavallaro", "Joseph R.", ""]]}, {"id": "1612.09029", "submitter": "Xie Pei", "authors": "Pei Xie, Keyou You, Roberto Tempo, Shiji Song and Cheng Wu", "title": "Distributed Convex Optimization with Inequality Constraints over\n  Time-varying Unbalanced Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a distributed convex optimization problem with\ninequality constraints over time-varying unbalanced digraphs, where the cost\nfunction is a sum of local objectives, and each node of the graph only knows\nits local objective and inequality constraints. Although there is a vast\nliterature on distributed optimization, most of them require the graph to be\nbalanced, which is quite restrictive and not necessary. Very recently, the\nunbalanced problem has been resolved only for either time-invariant graphs or\nunconstrained optimization. This work addresses the unbalancedness by focusing\non an epigraph form of the constrained optimization. A striking feature is that\nthis novel idea can be easily used to study time-varying unbalanced digraphs.\nUnder local communications, a simple iterative algorithm is then designed for\neach node. We prove that if the graph is uniformly jointly strongly connected,\neach node asymptotically converges to some common optimal solution.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 03:13:03 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Xie", "Pei", ""], ["You", "Keyou", ""], ["Tempo", "Roberto", ""], ["Song", "Shiji", ""], ["Wu", "Cheng", ""]]}, {"id": "1612.09059", "submitter": "John Grundy", "authors": "Amani S. Ibrahim, James Hamlyn-Harris, John Grundy", "title": "Emerging Security Challenges of Cloud Virtual Infrastructure", "comments": "6 pages Published in APSEC 2010 Workshop on Cloud Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cloud computing model is rapidly transforming the IT landscape. Cloud\ncomputing is a new computing paradigm that delivers computing resources as a\nset of reliable and scalable internet-based services allowing customers to\nremotely run and manage these services. Infrastructure-as-a-service (IaaS) is\none of the popular cloud computing services. IaaS allows customers to increase\ntheir computing resources on the fly without investing in new hardware. IaaS\nadapts virtualization to enable on-demand access to a pool of virtual computing\nresources. Although there are great benefits to be gained from cloud computing,\ncloud computing also enables new categories of threats to be introduced. These\nthreats are a result of the cloud virtual infrastructure complexity created by\nthe adoption of the virtualization technology.\n  Breaching the security of any component in the cloud virtual infrastructure\nsignificantly impacts on the security of other components and consequently\naffects the overall system security. This paper explores the security problem\nof the cloud platform virtual infrastructure identifying the existing security\nthreats and the complexities of this virtual infrastructure. The paper also\ndiscusses the existing security approaches to secure the cloud virtual\ninfrastructure and their drawbacks. Finally, we propose and explore some key\nresearch challenges of implementing new virtualization-aware security solutions\nthat can provide the pre-emptive protection for complex and ever- dynamic cloud\nvirtual infrastructure.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 07:46:28 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ibrahim", "Amani S.", ""], ["Hamlyn-Harris", "James", ""], ["Grundy", "John", ""]]}, {"id": "1612.09145", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Adrian Kosowski and Przemys{\\l}aw Uzna\\'nski", "title": "Ergodic Effects in Token Circulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dynamical process in a network which distributes all particles\n(tokens) located at a node among its neighbors, in a round-robin manner.\n  We show that in the recurrent state of this dynamics (i.e., disregarding a\npolynomially long initialization phase of the system), the number of particles\nlocated on a given edge, averaged over an interval of time, is tightly\nconcentrated around the average particle density in the system. Formally, for a\nsystem of $k$ particles in a graph of $m$ edges, during any interval of length\n$T$, this time-averaged value is $k/m \\pm \\widetilde{O}(1/T)$, whenever\n$\\gcd(m,k) = \\widetilde{O}(1)$ (and so, e.g., whenever $m$ is a prime number).\nTo achieve these bounds, we link the behavior of the studied dynamics to\nergodic properties of traversals based on Eulerian circuits on a symmetric\ndirected graph. These results are proved through sum set methods and are likely\nto be of independent interest.\n  As a corollary, we also obtain bounds on the \\emph{idleness} of the studied\ndynamics, i.e., on the longest possible time between two consecutive\nappearances of a token on an edge, taken over all edges. Designing trajectories\nfor $k$ tokens in a way which minimizes idleness is fundamental to the study of\nthe patrolling problem in networks. Our results immediately imply a bound of\n$\\widetilde{O}(m/k)$ on the idleness of the studied process, showing that it is\na distributed $\\widetilde{O}(1)$-competitive solution to the patrolling task,\nfor all of the covered cases. Our work also provides some further insights that\nmay be interesting in load-balancing applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 13:54:35 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 12:30:20 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 20:29:52 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kosowski", "Adrian", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1612.09185", "submitter": "Muhammad Farooq-i-Azam", "authors": "Muhammad Farooq-i-Azam, Muhammad Naeem Ayyaz, Saleem Akhtar", "title": "Connectivity based technique for localization of nodes in wireless\n  sensor networks", "comments": "arXiv admin note: text overlap with arXiv:1611.03420", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a localization algorithm for wireless sensor networks, which is\nsimple in design, does not involve significant overhead and yet provides\nacceptable position estimates of sensor nodes. The algorithm uses settled nodes\nas beacon nodes so as to increase the number of beacon nodes. The algorithm is\nrange free and does not need any additional piece of hardware for ranging. It\nalso does not involve any significant communication overhead for localization.\nThe simulation and results show that good localization accuracy is achieved for\noutdoor environments.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 07:13:51 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Farooq-i-Azam", "Muhammad", ""], ["Ayyaz", "Muhammad Naeem", ""], ["Akhtar", "Saleem", ""]]}, {"id": "1612.09426", "submitter": "Vincent Gramoli", "authors": "Christopher Natoli, Vincent Gramoli", "title": "The Balance Attack Against Proof-Of-Work Blockchains: The R3 Testbed as\n  an Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we identify a new form of attack, called the Balance attack,\nagainst proof-of-work blockchain systems. The novelty of this attack consists\nof delaying network communications between multiple subgroups of nodes with\nbalanced mining power. Our theoretical analysis captures the precise tradeoff\nbetween the network delay and the mining power of the attacker needed to double\nspend in Ethereum with high probability.\n  We quantify our probabilistic analysis with statistics taken from the R3\nconsortium, and show that a single machine needs 20 minutes to attack the\nconsortium. Finally, we run an Ethereum private chain in a distributed system\nwith similar settings as R3 to demonstrate the feasibility of the approach, and\ndiscuss the application of the Balance attack to Bitcoin. Our results clearly\nconfirm that main proof-of-work blockchain protocols can be badly suited for\nconsortium blockchains.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 09:08:10 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Natoli", "Christopher", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1612.09576", "submitter": "Milind Chabbi", "authors": "Milind Chabbi, Abdelhalim Amer, Shasha Wen, Xu Liu", "title": "Correctness of Hierarchical MCS Locks with Timeout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript serves as a correctness proof of the Hierarchical MCS locks\nwith Timeout (HMCS-T) described in our paper titled \"An Efficient\nAbortable-locking Protocol for Multi-level NUMA Systems\" appearing in the\nproceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of\nParallel Programming.\n  HMCS-T is a very involved protocol. The system is stateful; the values of\nprior acquisition efforts affect the subsequent acquisition efforts. Also, the\nstatus of successors, predecessors, ancestors, and descendants affect steps\nfollowed by the protocol. The ability to make the protocol fully non-blocking\nleads to modifications to the \\texttt{next} field, which causes deviation from\nthe original MCS lock protocol both in acquisition and release. At several\nplaces, unconditional field updates are replaced with SWAP or CAS operations.\n  We follow a multi-step approach to prove the correctness of HMCS-T. To\ndemonstrate the correctness of the HMCS-T lock, we use the Spin model checking.\nModel checking causes a combinatorial explosion even to simulate a handful of\nthreads. First, we understand the minimal, sufficient configurations necessary\nto prove safety properties of a single level of lock in the tree. We construct\nHMCS-T locks that represent these configurations. We model check these\nconfigurations, which proves the correctness of components of an HMCS-T lock.\nFinally, building upon these facts, we argue logically for the correctness of\nHMCS-T<n>.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 19:57:58 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Chabbi", "Milind", ""], ["Amer", "Abdelhalim", ""], ["Wen", "Shasha", ""], ["Liu", "Xu", ""]]}]