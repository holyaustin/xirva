[{"id": "2101.00025", "submitter": "Yuval Peres", "authors": "Simina Branzei and Yuval Peres", "title": "Consensus with Bounded Space and Minimal Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a fundamental model in distributed computing, where\nmany nodes with bounded memory and computational power have random pairwise\ninteractions over time. This model has been studied in a rich body of\nliterature aiming to understand the tradeoffs between the memory and time\nneeded to perform computational tasks.\n  We study the population protocol model focusing on the communication\ncomplexity needed to achieve consensus with high probability. When the number\nof memory states is $s = O(\\log \\log{n})$, the best upper bound known was given\nby a protocol with $O(n \\log{n})$ communication, while the best lower bound was\n$\\Omega(n \\log(n)/s)$ communication.\n  We design a protocol that shows the lower bound is sharp. When each agent has\n$s=O(\\log{n}^{\\theta})$ states of memory, with $\\theta \\in (0,1/2)$, consensus\ncan be reached in time $O(\\log(n))$ with $O(n \\log{(n)}/s)$ communications with\nhigh probability.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:00:05 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Branzei", "Simina", ""], ["Peres", "Yuval", ""]]}, {"id": "2101.00172", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Chunk List: Concurrent Data Structures", "comments": "20 pages, 3 figures A full implementation can be found at\n  https://github.com/danielathome19/Chunk-List Update: Revised format to align\n  closer to IEEE standards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chunking data is obviously no new concept; however, I had never found any\ndata structures that used chunking as the basis of their implementation. I\nfigured that by using chunking alongside concurrency, I could create an\nextremely fast run-time in regards to particular methods as searching and/or\nsorting. By using chunking and concurrency to my advantage, I came up with the\nchunk list - a dynamic list-based data structure that would separate large\namounts of data into specifically sized chunks, each of which should be able to\nbe searched at the exact same time by searching each chunk on a separate\nthread. As a result of implementing this concept into its own class, I was able\nto create something that almost consistently gives around 20x-300x faster\nresults than a regular ArrayList. However, should speed be a particular issue\neven after implementation, users can modify the size of the chunks and\nbenchmark the speed of using smaller or larger chunks, depending on the amount\nof data being stored.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:45:56 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:32:25 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2101.00267", "submitter": "Christina Delimitrou", "authors": "Yu Gan, Mingyu Liang, Sundar Dev, David Lo, Christina Delimitrou", "title": "Sage: Using Unsupervised Learning for Scalable Performance Debugging in\n  Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud applications are increasingly shifting from large monolithic services\nto complex graphs of loosely-coupled microservices. Despite the advantages of\nmodularity and elasticity microservices offer, they also complicate cluster\nmanagement and performance debugging, as dependencies between tiers introduce\nbackpressure and cascading QoS violations.\n  We present Sage, a machine learning-driven root cause analysis system for\ninteractive cloud microservices. Sage leverages unsupervised ML models to\ncircumvent the overhead of trace labeling, captures the impact of dependencies\nbetween microservices to determine the root cause of unpredictable performance\nonline, and applies corrective actions to recover a cloud service's QoS. In\nexperiments on both dedicated local clusters and large clusters on Google\nCompute Engine we show that Sage consistently achieves over 93% accuracy in\ncorrectly identifying the root cause of QoS violations, and improves\nperformance predictability.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 16:44:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gan", "Yu", ""], ["Liang", "Mingyu", ""], ["Dev", "Sundar", ""], ["Lo", "David", ""], ["Delimitrou", "Christina", ""]]}, {"id": "2101.00330", "submitter": "Muhammad Saad", "authors": "Muhammad Saad, Zhan Qin, Kui Ren, DaeHun Nyang, David Mohaisen", "title": "e-PoS: Making Proof-of-Stake Decentralized and Fair", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain applications that rely on the Proof-of-Work (PoW) have\nincreasingly become energy inefficient with a staggering carbon footprint. In\ncontrast, energy-efficient alternative consensus protocols such as\nProof-of-Stake (PoS) may cause centralization and unfairness in the blockchain\nsystem. To address these challenges, we propose a modular version of PoS-based\nblockchain systems called epos that resists the centralization of network\nresources by extending mining opportunities to a wider set of stakeholders.\nMoreover, epos leverages the in-built system operations to promote fair mining\npractices by penalizing malicious entities. We validate epos's achievable\nobjectives through theoretical analysis and simulations. Our results show that\nepos ensures fairness and decentralization, and can be applied to existing\nblockchain applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 22:43:27 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Saad", "Muhammad", ""], ["Qin", "Zhan", ""], ["Ren", "Kui", ""], ["Nyang", "DaeHun", ""], ["Mohaisen", "David", ""]]}, {"id": "2101.00397", "submitter": "Naveen Tumkur Ramesh Babu", "authors": "Naveen T.R. Babu and Christopher Stewart", "title": "Early Work on Efficient Patching for Coordinating Edge Applications", "comments": "ICAC 2019 Submitted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple applications running on Edge computers can be orchestrated to\nachieve the desired goal. Orchestration of applications is prominent when\nworking with Internet of Things based applications, Autonomous driving and\nAutonomous Aerial vehicles. As the applications receive modified\nclassifiers/code, there will be multiple applications that need to be updated.\nIf all the classifiers are synchronously updated there would be increased\nthroughput and bandwidth degradation. On the other hand, delaying updates of\napplications which need immediate update hinders performance and delays\nprogress towards end goal. The updates of applications should be prioritized\nand updates should happen according to this priority. This paper explores the\nsetup and benchmarks to understand the impact of updates when multiple\napplications working to achieve same objective are orchestrated with\nprioritized updates. We discuss methods to build a distributed, reliable and\nscalable system called \"DSOC\"(Docker Swarm Orchestration Component).\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 07:47:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Babu", "Naveen T. R.", ""], ["Stewart", "Christopher", ""]]}, {"id": "2101.00775", "submitter": "Minghui Liwang", "authors": "Minghui Liwang, Ruitao Chen, Xianbin Wang", "title": "Resource Trading in Edge Computing-enabled IoV: An Efficient\n  Futures-based Approach", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) has become a promising solution to utilize\ndistributed computing resources for supporting computation-intensive vehicular\napplications in dynamic driving environments. To facilitate this paradigm, the\nonsite resource trading serves as a critical enabler. However, dynamic\ncommunications and resource conditions could lead unpredictable trading\nlatency, trading failure, and unfair pricing to the conventional resource\ntrading process. To overcome these challenges, we introduce a novel\nfutures-based resource trading approach in edge computing-enabled internet of\nvehicles (IoV), where a forward contract is used to facilitate resource trading\nrelated negotiations between an MEC server (seller) and a vehicle (buyer) in a\ngiven future term. Through estimating the historical statistics of future\nresource supply and network condition, we formulate the futures-based resource\ntrading as the optimization problem aiming to maximize the seller's and the\nbuyer's expected utility, while applying risk evaluations to relieve possible\nlosses incurred by the uncertainties in the system. To tackle this problem, we\npropose an efficient bilateral negotiation approach which facilitates the\nparticipants reaching a consensus. Extensive simulations demonstrate that the\nproposed futures-based resource trading brings considerable utilities to both\nparticipants, while significantly outperforming the baseline methods on\ncritical factors, e.g., trading failures and fairness, negotiation latency and\ncost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 04:53:13 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liwang", "Minghui", ""], ["Chen", "Ruitao", ""], ["Wang", "Xianbin", ""]]}, {"id": "2101.00778", "submitter": "Minghui Liwang", "authors": "Minghui Liwang, Zhibin Gao, Xianbin Wang", "title": "Let' s Trade in The Future! A Futures-Enabled Fast Resource Trading\n  Mechanism in Edge Computing-Assisted UAV Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) has emerged as one of the key technical aspects\nof the fifth-generation (5G) networks. The integration of MEC with\nresource-constrained unmanned aerial vehicles (UAVs) could enable flexible\nresource provisioning for supporting dynamic and computation-intensive UAV\napplications. Existing resource trading could facilitate this paradigm with\nproper incentives, which, however, may often incur unexpected negotiation\nlatency and energy consumption, trading failures and unfair pricing, due to the\nunpredictable nature of the resource trading process. Motivated by these\nchallenges, an efficient futures-based resource trading mechanism for edge\ncomputing-assisted UAV network is proposed, where a mutually beneficial and\nrisk-tolerable forward contract is devised to promote resource trading between\nan MEC server (seller) and a UAV (buyer). Two key problems i.e. futures\ncontract design before trading and power optimization during trading are\nstudied. By analyzing historical statistics associated with future resource\nsupply, demand, and air-to-ground communication quality, the contract design is\nformulated as a multi-objective optimization problem, aiming to maximize both\nthe seller's and the buyer's expected utilities, while estimating their\nacceptable risk tolerance. Accordingly, we propose an efficient bilateral\nnegotiation scheme to help players reach a trading consensus on the amount of\nresources and the relevant price. For the power optimization problem, we\ndevelop a practical algorithm that enables the buyer to determine its optimal\ntransmission power via convex optimization techniques. Comprehensive\nsimulations demonstrate that the proposed mechanism offers both players\nconsiderable utilities, while outperforming the onsite trading mechanism on\ntrading failures and fairness, negotiation latency, and cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:06:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liwang", "Minghui", ""], ["Gao", "Zhibin", ""], ["Wang", "Xianbin", ""]]}, {"id": "2101.00787", "submitter": "Su Wang", "authors": "Su Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito,\n  Mung Chiang, and Christopher G. Brinton", "title": "Device Sampling for Heterogeneous Federated Learning: Theory,\n  Algorithms, and Implementation", "comments": "This paper is accepted for publication in the proceedings of 2021\n  IEEE International Conference on Computer Communications (INFOCOM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional federated learning (FedL) architecture distributes machine\nlearning (ML) across worker devices by having them train local models that are\nperiodically aggregated by a server. FedL ignores two important characteristics\nof contemporary wireless networks, however: (i) the network may contain\nheterogeneous communication/computation resources, while (ii) there may be\nsignificant overlaps in devices' local data distributions. In this work, we\ndevelop a novel optimization methodology that jointly accounts for these\nfactors via intelligent device sampling complemented by device-to-device (D2D)\noffloading. Our optimization aims to select the best combination of sampled\nnodes and data offloading configuration to maximize FedL training accuracy\nsubject to realistic constraints on the network topology and device\ncapabilities. Theoretical analysis of the D2D offloading subproblem leads to\nnew FedL convergence bounds and an efficient sequential convex optimizer. Using\nthis result, we develop a sampling methodology based on graph convolutional\nnetworks (GCNs) which learns the relationship between network attributes,\nsampled nodes, and resulting offloading that maximizes FedL accuracy. Through\nevaluation on real-world datasets and network measurements from our IoT\ntestbed, we find that our methodology while sampling less than 5% of all\ndevices outperforms conventional FedL substantially both in terms of trained\nmodel accuracy and required resource utilization.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:59:50 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Su", ""], ["Lee", "Mengyuan", ""], ["Hosseinalipour", "Seyyedali", ""], ["Morabito", "Roberto", ""], ["Chiang", "Mung", ""], ["Brinton", "Christopher G.", ""]]}, {"id": "2101.00902", "submitter": "Chathura Sarathchandra Magurawalage", "authors": "Chathura Sarathchandra", "title": "REACT: Distributed Mobile Microservice Execution Enabled by Efficient\n  Inter-Process Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increased mobile connectivity, the range and number of services available\nin various computing environments in the network, demand mobile applications to\nbe highly dynamic to be able to efficiently incorporate those services into\napplications, along with other local capabilities on mobile devices. However,\nthe monolithic structure and mostly static configuration of mobile application\ncomponents today limit application's ability to dynamically manage internal\ncomponents, to be able to adapt to the user and the environment, and utilize\nvarious services in the network for improving the application experience.\n  In this paper, we present REACT, a new Android-based framework that enables\napps to be developed as a collection of loosely coupled microservices (MS). It\nallows individual distribution, dynamic management and offloading of MS to be\nexecuted by services in the network, based on contextual changes. REACT aims to\nprovide i) a framework as an Android Library for creating MS-based apps that\nadapt to contextual changes ii) a unified HTTP-based communication mechanism,\nusing Android Inter-Process Communication (IPC) for transporting requests\nbetween locally running MS, while allowing flexible and transparent switching\nbetween network and IPC requests, when offloading. We evaluate REACT by\nimplementing a video streaming app that dynamically offloads MS to web services\nin the network, adapting to contextual changes. The evaluation shows the\nadaptability to contextual changes and reductions in power consumption when\noffloading, while our communication mechanism overcomes performance limitations\nof Android IPC by enabling efficient transferring of large payloads between\nmobile MS.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 11:47:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sarathchandra", "Chathura", ""]]}, {"id": "2101.00941", "submitter": "Jan Novotny", "authors": "Jan Novotn\\'y, Karel Ad\\'amek, Wes Armour", "title": "Implementing CUDA Streams into AstroAccelerate -- A Case Study", "comments": "submitted to ADASS XXX, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be able to run tasks asynchronously on NVIDIA GPUs a programmer must\nexplicitly implement asynchronous execution in their code using the syntax of\nCUDA streams. Streams allow a programmer to launch independent concurrent\nexecution tasks, providing the ability to utilise different functional units on\nthe GPU asynchronously. For example, it is possible to transfer the results\nfrom a previous computation performed on input data n-1, over the PCIe bus\nwhilst computing the result for input data n, by placing different tasks in\ndifferent CUDA streams. The benefit of such an approach is that the time taken\nfor the data transfer between the host and device can be hidden with\ncomputation. This case study deals with the implementation of CUDA streams into\nAstroAccelerate. AstroAccelerate is a GPU accelerated real-time signal\nprocessing pipeline for time-domain radio astronomy.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:16:45 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 12:10:42 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 09:25:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Novotn\u00fd", "Jan", ""], ["Ad\u00e1mek", "Karel", ""], ["Armour", "Wes", ""]]}, {"id": "2101.00983", "submitter": "Ionut Anghel", "authors": "Claudia Daniela Antal (Pop), Tudor Cioara, Marcel Antal, Ionut Anghel", "title": "Blockchain platform for COVID-19 vaccine supply management", "comments": null, "journal-ref": "IEEE Open Journal of the Computer Society, Volume 2, 2021,\n  page(s): 164-178,ISSN: 2644-1268", "doi": "10.1109/OJCS.2021.3067450", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of the COVID-19 pandemic, the rapid roll-out of a vaccine and\nthe implementation of a worldwide immunization campaign is critical, but its\nsuccess will depend on the availability of an operational and transparent\ndistribution chain that can be audited by all relevant stakeholders. In this\npaper, we discuss how blockchain technology can be used for assuring the\ntransparent tracing of COVID-19 vaccine registration, storage and delivery, and\nside effects self-reporting. We present such system implementation in which\nblockchain technology is used for assuring data integrity and immutability in\ncase of beneficiary registration for vaccination, eliminating identity thefts\nand impersonations. Smart contracts are defined to monitor and track the proper\nvaccine distribution conditions against the safe handling rules defined by\nvaccine producers enabling the awareness of all network peers. For vaccine\nadministration, a transparent and tamper-proof side effects self-reporting\nsolution is provided considering person identification and administrated\nvaccine association. A prototype was implemented using the Ethereum test\nnetwork, Ropsten, considering the COVID-19 vaccine distribution tracking\nconditions. The results obtained for each on-chain operation can be checked and\nvalidated on the Etherscan, demonstrating various aspects of the proposed\nsystem such as immunization actors and safe rules registration, vaccine\ntracking, and administration. In terms of throughput and scalability, the\nproposed blockchain system shows promising results.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:56:34 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Antal", "Claudia Daniela", "", "Pop"], ["Cioara", "Tudor", ""], ["Antal", "Marcel", ""], ["Anghel", "Ionut", ""]]}, {"id": "2101.01159", "submitter": "Joseph Hellerstein", "authors": "Alvin Cheung, Natacha Crooks, Joseph M. Hellerstein and Matthew Milano", "title": "New Directions in Cloud Programming", "comments": null, "journal-ref": "CIDR 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.OS cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearly twenty years after the launch of AWS, it remains difficult for most\ndevelopers to harness the enormous potential of the cloud. In this paper we lay\nout an agenda for a new generation of cloud programming research aimed at\nbringing research ideas to programmers in an evolutionary fashion. Key to our\napproach is a separation of distributed programs into a PACT of four facets:\nProgram semantics, Availablity, Consistency and Targets of optimization. We\npropose to migrate developers gradually to PACT programming by lifting familiar\ncode into our more declarative level of abstraction. We then propose a\nmulti-stage compiler that emits human-readable code at each stage that can be\nhand-tuned by developers seeking more control. Our agenda raises numerous\nresearch challenges across multiple areas including language design, query\noptimization, transactions, distributed consistency, compilers and program\nsynthesis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:42:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cheung", "Alvin", ""], ["Crooks", "Natacha", ""], ["Hellerstein", "Joseph M.", ""], ["Milano", "Matthew", ""]]}, {"id": "2101.01177", "submitter": "Gihan  Ravideva Mudalige", "authors": "Kamalavasan Kamalakkannan, Gihan R. Mudalige, Istvan Z. Reguly, Suhaib\n  A. Fahmy", "title": "High-Level FPGA Accelerator Design for Structured-Mesh-Based Explicit\n  Numerical Solvers", "comments": "Preprint - Accepted to the 35th IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS 2021), May 2021, Portland, Oregon USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a workflow for synthesizing near-optimal FPGA\nimplementations for structured-mesh based stencil applications for explicit\nsolvers. It leverages key characteristics of the application class, its\ncomputation-communication pattern, and the architectural capabilities of the\nFPGA to accelerate solvers from the high-performance computing domain. Key new\nfeatures of the workflow are (1) the unification of standard state-of-the-art\ntechniques with a number of high-gain optimizations such as batching and\nspatial blocking/tiling, motivated by increasing throughput for real-world work\nloads and (2) the development and use of a predictive analytic model for\nexploring the design space, resource estimates and performance. Three\nrepresentative applications are implemented using the design workflow on a\nXilinx Alveo U280 FPGA, demonstrating near-optimal performance and over 85%\npredictive model accuracy. These are compared with equivalent highly-optimized\nimplementations of the same applications on modern HPC-grade GPUs (Nvidia V100)\nanalyzing time to solution, bandwidth and energy consumption. Performance\nresults indicate equivalent runtime performance of the FPGA implementations to\nthe V100 GPU, with over 2x energy savings, for the largest non-trivial\napplication synthesized on the FPGA compared to the best performing GPU-based\nsolution. Our investigation shows the considerable challenges in gaining high\nperformance on current generation FPGAs compared to traditional architectures.\nWe discuss determinants for a given stencil code to be amenable to FPGA\nimplementation, providing insights into the feasibility and profitability of a\ndesign and its resulting performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:27:15 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:57:16 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Kamalakkannan", "Kamalavasan", ""], ["Mudalige", "Gihan R.", ""], ["Reguly", "Istvan Z.", ""], ["Fahmy", "Suhaib A.", ""]]}, {"id": "2101.01300", "submitter": "Waheed Bajwa", "authors": "Arpita Gang and Waheed U. Bajwa", "title": "A Linearly Convergent Algorithm for Distributed Principal Component\n  Analysis", "comments": "33 pages; 15 figures; preprint of a journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is the workhorse tool for dimensionality\nreduction in this era of big data. While often overlooked, the purpose of PCA\nis not only to reduce data dimensionality, but also to yield features that are\nuncorrelated. Furthermore, the ever-increasing volume of data in the modern\nworld often requires storage of data samples across multiple machines, which\nprecludes the use of centralized PCA algorithms. This paper focuses on the dual\nobjective of PCA, namely, dimensionality reduction and decorrelation of\nfeatures, but in a distributed setting. This requires estimating the\neigenvectors of the data covariance matrix, as opposed to only estimating the\nsubspace spanned by the eigenvectors, when data is distributed across a network\nof machines. Although a few distributed solutions to the PCA problem have been\nproposed recently, convergence guarantees and/or communications overhead of\nthese solutions remain a concern. With an eye towards communications\nefficiency, this paper introduces a feedforward neural network-based one\ntime-scale distributed PCA algorithm termed Distributed Sanger's Algorithm\n(DSA) that estimates the eigenvectors of the data covariance matrix when data\nis distributed across an undirected and arbitrarily connected network of\nmachines. Furthermore, the proposed algorithm is shown to converge linearly to\na neighborhood of the true solution. Numerical results are also provided to\ndemonstrate the efficacy of the proposed solution.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:51:14 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:04:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gang", "Arpita", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2101.01332", "submitter": "Yichen Yang", "authors": "Yichen Yang, Phitchaya Mangpo Phothilimtha, Yisu Remy Wang, Max\n  Willsey, Sudip Roy, Jacques Pienaar", "title": "Equality Saturation for Tensor Graph Superoptimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major optimizations employed in deep learning frameworks is graph\nrewriting. Production frameworks rely on heuristics to decide if rewrite rules\nshould be applied and in which order. Prior research has shown that one can\ndiscover more optimal tensor computation graphs if we search for a better\nsequence of substitutions instead of relying on heuristics. However, we observe\nthat existing approaches for tensor graph superoptimization both in production\nand research frameworks apply substitutions in a sequential manner. Such\nsequential search methods are sensitive to the order in which the substitutions\nare applied and often only explore a small fragment of the exponential space of\nequivalent graphs. This paper presents a novel technique for tensor graph\nsuperoptimization that employs equality saturation to apply all possible\nsubstitutions at once. We show that our approach can find optimized graphs with\nup to 16% speedup over state-of-the-art, while spending on average 48x less\ntime optimizing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:30:35 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 15:05:06 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Yang", "Yichen", ""], ["Phothilimtha", "Phitchaya Mangpo", ""], ["Wang", "Yisu Remy", ""], ["Willsey", "Max", ""], ["Roy", "Sudip", ""], ["Pienaar", "Jacques", ""]]}, {"id": "2101.01335", "submitter": "Hoang-Dung Do", "authors": "Hoang-Dung Do, Valerie Hayot-Sasson, Rafael Ferreira da Silva,\n  Christopher Steele, Henri Casanova, Tristan Glatard", "title": "Modeling the Linux page cache for accurate simulation of data-intensive\n  applications", "comments": "10 pages, 8 figures, CCGrid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Big Data in recent years has resulted in a growing need for\nefficient data processing solutions. While infrastructures with sufficient\ncompute power are available, the I/O bottleneck remains. The Linux page cache\nis an efficient approach to reduce I/O overheads, but few experimental studies\nof its interactions with Big Data applications exist, partly due to limitations\nof real-world experiments. Simulation is a popular approach to address these\nissues, however, existing simulation frameworks do not simulate page caching\nfully, or even at all. As a result, simulation-based performance studies of\ndata-intensive applications lead to inaccurate results.\n  In this paper, we propose an I/O simulation model that includes the key\nfeatures of the Linux page cache. We have implemented this model as part of the\nWRENCH workflow simulation framework, which itself builds on the popular\nSimGrid distributed systems simulation framework. Our model and its\nimplementation enable the simulation of both single-threaded and multithreaded\napplications, and of both writeback and writethrough caches for local or\nnetwork-based filesystems. We evaluate the accuracy of our model in different\nconditions, including sequential and concurrent applications, as well as local\nand remote I/Os. We find that our page cache model reduces the simulation error\nby up to an order of magnitude when compared to state-of-the-art, cacheless\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:36:36 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Do", "Hoang-Dung", ""], ["Hayot-Sasson", "Valerie", ""], ["da Silva", "Rafael Ferreira", ""], ["Steele", "Christopher", ""], ["Casanova", "Henri", ""], ["Glatard", "Tristan", ""]]}, {"id": "2101.01376", "submitter": "Lei Wang", "authors": "Lei Wang, Yang Liu, Ian Manchester, and Guodong Shi", "title": "Differentially Private Distributed Computation via Public-Private\n  Communication Networks", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the problem of multi-agent computation under the\ndifferential privacy requirement of the agents' local datasets against\neavesdroppers having node-to-node communications. We first propose for the\nnetwork equipped with public-private networks. The private network is sparse\nand not even necessarily connected, over which communications are encrypted and\nsecure along with the intermediate node states; the public network is connected\nand may be dense, over which communications are allowed to be public. In this\nsetting, we propose a multi-gossip PPSC mechanism over the private network,\nwhere at each step, randomly selected node pairs update their states in such a\nway that they are shuffled with random noise while maintaining summation\nconsistency. We show that this mechanism can achieve any desired differential\nprivacy level with any prescribed probability. Next, we embed this mechanism in\ndistributed computing processes, and propose privacy-guarantee protocols for\nthree basic computation tasks, where an adaptive mechanism adjusts the amount\nof noise injected in PPSC steps for privacy protection, and the number of\nregular computation steps for accuracy guarantee. For average consensus, we\ndevelop a PPSC-Gossip averaging consensus algorithm by utilizing the\nmulti-gossip PPSC mechanism for privacy encryption before an averaging\nconsensus algorithm over the public network for local computations. For network\nlinear equations and distributed convex optimization, we develop two respective\ndistributed computing protocols by following the PPSC-Gossip averaging\nconsensus algorithm with an additional projection or gradient descent step\nwithin each step of computation. Given any privacy and accuracy requirements,\nit is shown that all three proposed protocols can compute their corresponding\nproblems with the desired computation accuracy, while achieving the desired\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 07:02:19 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Wang", "Lei", ""], ["Liu", "Yang", ""], ["Manchester", "Ian", ""], ["Shi", "Guodong", ""]]}, {"id": "2101.01409", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Yves M\\'etivier, John Michael Robson", "title": "Revisiting the Role of Coverings in Anonymous Networks: Spanning Tree\n  Construction and Topology Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper revisits two classical distributed problems in anonymous networks,\nnamely spanning tree construction and topology recognition, from the point of\nview of graph covering theory. For both problems, we characterize necessary and\nsufficient conditions on the communication graph in terms of directed symmetric\ncoverings. These characterizations answer along-standing open question posed by\nYamashita and Kameda [YK96], and shed new light on the connection between\ncoverings and the concepts of views and quotient graphs developed by the same\nauthors. Characterizing conditions in terms of coverings is significant because\nit connects the field with a vast body of classical literature in graph theory\nand algebraic topology. In particular, it gives access to powerful tools such\nas Reidemeister's theorem and Mazurkiewicz's algorithm. Combined together,\nthese tools allow us to present elegant proofs of otherwise intricate results,\nand their constructive nature makes them effectively usable in the algorithms.\nThis paper also gives us the opportunity to present the field of covering\ntheory in a pedagogical way, with a focus on the two aforementioned tools,\nwhose potential impact goes beyond the specific problems considered in this\nwork.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:39:46 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 20:50:10 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Casteigts", "Arnaud", ""], ["M\u00e9tivier", "Yves", ""], ["Robson", "John Michael", ""]]}, {"id": "2101.01901", "submitter": "Dimitris Chatzopoulos", "authors": "Christodoulos Pappas, Dimitris Chatzopoulos, Spyros Lalis, Manolis\n  Vavalis", "title": "IPLS : A Framework for Decentralized Federated Learning", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of resourceful mobile devices that store rich,\nmultidimensional and privacy-sensitive user data motivate the design of\nfederated learning (FL), a machine-learning (ML) paradigm that enables mobile\ndevices to produce an ML model without sharing their data. However, the\nmajority of the existing FL frameworks rely on centralized entities. In this\nwork, we introduce IPLS, a fully decentralized federated learning framework\nthat is partially based on the interplanetary file system (IPFS). By using IPLS\nand connecting into the corresponding private IPFS network, any party can\ninitiate the training process of an ML model or join an ongoing training\nprocess that has already been started by another party. IPLS scales with the\nnumber of participants, is robust against intermittent connectivity and dynamic\nparticipant departures/arrivals, requires minimal resources, and guarantees\nthat the accuracy of the trained model quickly converges to that of a\ncentralized FL framework with an accuracy drop of less than one per thousand.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:44:51 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Pappas", "Christodoulos", ""], ["Chatzopoulos", "Dimitris", ""], ["Lalis", "Spyros", ""], ["Vavalis", "Manolis", ""]]}, {"id": "2101.02074", "submitter": "Tobias Kronauer", "authors": "Tobias Kronauer, Joshwa Pohlmann, Maximilian Matthe, Till Smejkal,\n  Gerhard Fettweis", "title": "Latency Analysis of ROS2 Multi-Node Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.MA cs.PF cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Robot Operating System 2 (ROS2) targets distributed real-time systems and\nis widely used in the robotics community. Especially in these systems, latency\nin data processing and communication can lead to instabilities. Though being\nhighly configurable with respect to latency, ROS2 is often used with its\ndefault settings.\n  In this paper, we investigate the end-to-end latency of ROS2 for distributed\nsystems with default settings and different Data Distribution Service (DDS)\nmiddlewares. In addition, we profile the ROS2 stack and point out latency\nbottlenecks. Our findings indicate that end-to-end latency strongly depends on\nthe used DDS middleware. Moreover, we show that ROS2 can lead to 50% latency\noverhead compared to using low-level DDS communications. Our results imply\nguidelines for designing distributed ROS2 architectures and indicate\npossibilities for reducing the ROS2 overhead.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 14:50:09 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 09:15:21 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 06:10:04 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kronauer", "Tobias", ""], ["Pohlmann", "Joshwa", ""], ["Matthe", "Maximilian", ""], ["Smejkal", "Till", ""], ["Fettweis", "Gerhard", ""]]}, {"id": "2101.02159", "submitter": "Damian Straszak", "authors": "Daniel Kane and Andreas Fackler and Adam G\\k{a}gol and Damian Straszak", "title": "Highway: Efficient Consensus with Flexible Finality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been recently a lot of progress in designing efficient partially\nsynchronous BFT consensus protocols that are meant to serve as core consensus\nengines for Proof of Stake blockchain systems. While the state-of-the-art\nsolutions attain virtually optimal performance under this theoretical model,\nthere is still room for improvement, as several practical aspects of such\nsystems are not captured by this model. Most notably, during regular execution,\ndue to financial incentives in such systems, one expects an overwhelming\nfraction of nodes to honestly follow the protocol rules and only few of them to\nbe faulty, most likely due to temporary network issues. Intuitively, the fact\nthat almost all nodes behave honestly should result in stronger confidence in\nblocks finalized in such periods, however it is not the case under the\nclassical model, where finality is binary.\n  We propose Highway, a new consensus protocol that is safe and live in the\nclassical partially synchronous BFT model, while at the same time offering\npractical improvements over existing solutions. Specifically, block finality in\nHighway is not binary but is expressed by fraction of nodes that would need to\nbreak the protocol rules in order for a block to be reverted. During periods of\nhonest participation finality of blocks might reach well beyond 1/3 (as what\nwould be the maximum for classical protocols), up to even 1 (complete\ncertainty). Having finality defined this way, Highway offers flexibility with\nrespect to the configuration of security thresholds among nodes running the\nprotocol, allowing nodes with lower thresholds to reach finality faster than\nthe ones requiring higher levels of confidence.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:51:54 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 09:12:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kane", "Daniel", ""], ["Fackler", "Andreas", ""], ["G\u0105gol", "Adam", ""], ["Straszak", "Damian", ""]]}, {"id": "2101.02204", "submitter": "Samuel Thompson", "authors": "Steven H. VanderLeest (1), Samuel R. Thompson (2) ((1) Rapita Systems,\n  Inc, (2) Rapita Systems Ltd)", "title": "Measuring the Impact of Interference Channels on Multicore Avionics", "comments": "8 pages, 3 figures. Digital Avionics Systems Conference (DASC) 2020", "journal-ref": null, "doi": "10.1109/DASC50938.2020.9256647", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement-based analysis of software timing behavior provides important\ninsight and evidence for flight certification of modern avionics systems. For\nmulticore systems, however, this analysis is challenging due to interference\neffects from shared hardware resource usage. We present an approach to\nmulticore timing analysis that uses interference generators to stress\ninterference channels in multicore systems. The test methodology comprises two\nsteps. First, platform characterization measures the sensitivity of the\nhardware and RTOS to targeted interference channels using a combination of\ninterference generators. Second, software characterization measures the timing\nbehavior of instrumented applications while interference is generated on shared\nresources.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 10:51:40 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["VanderLeest", "Steven H.", ""], ["Thompson", "Samuel R.", ""]]}, {"id": "2101.02206", "submitter": "Xinwei Deng", "authors": "Xia Cai, Li Xu, C. Devon Lin, Yili Hong, Xinwei Deng", "title": "Sequential Design of Computer Experiments with Quantitative and\n  Qualitative Factors in Applications to HPC Performance Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer experiments with both qualitative and quantitative factors are\nwidely used in many applications. Motivated by the emerging need of optimal\nconfiguration in the high-performance computing (HPC) system, this work\nproposes a sequential design, denoted as adaptive composite exploitation and\nexploration (CEE), for optimization of computer experiments with qualitative\nand quantitative factors. The proposed adaptive CEE method combines the\npredictive mean and standard deviation based on the additive Gaussian process\nto achieve a meaningful balance between exploitation and exploration for\noptimization. Moreover, the adaptiveness of the proposed sequential procedure\nallows the selection of next design point from the adaptive design region.\nTheoretical justification of the adaptive design region is provided. The\nperformance of the proposed method is evaluated by several numerical examples\nin simulations. The case study of HPC performance optimization further\nelaborates the merits of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 11:46:19 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Cai", "Xia", ""], ["Xu", "Li", ""], ["Lin", "C. Devon", ""], ["Hong", "Yili", ""], ["Deng", "Xinwei", ""]]}, {"id": "2101.02286", "submitter": "Hang Song", "authors": "Hang Song, Kristen V. Matsuno, Jacob R. West, Akshay Subramaniam,\n  Aditya S. Ghate and Sanjiva K. Lele", "title": "Scalable Parallel Linear Solver for Compact Banded Systems on\n  Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A scalable algorithm for solving compact banded linear systems on distributed\nmemory architectures is presented. The proposed method factorizes the original\nsystem into two levels of memory hierarchies, and solves it using parallel\ncyclic reduction on both distributed and shared memory. This method has a lower\ncommunication footprint across distributed memory partitions compared to\nconventional algorithms involving data transpose or re-partitioning. The\nalgorithm developed in this work is generalized to cyclic compact banded\nsystems with flexible data decompositions. For cyclic compact banded systems,\nthe method is a direct solver with a deterministic operation and communication\ncounts depending on the matrix size, its bandwidth, and the partition strategy.\nThe implementation and runtime configuration details are discussed for\nperformance optimization. Scalability is demonstrated on the linear solver as\nwell as on a representative fluid mechanics application problem, in which the\ndominant computational cost is solving the cyclic tridiagonal linear systems of\ncompact numerical schemes on a 3D periodic domain. The algorithm is\nparticularly useful for solving the linear systems arising from the application\nof compact finite difference operators to a wide range of partial differential\nequation problems, such as but not limited to the numerical simulations of\ncompressible turbulent flows, aeroacoustics, elastic-plastic wave propagation,\nand electromagnetics. It alleviates obstacles to their use on modern high\nperformance computing hardware, where memory and computational power are\ndistributed across nodes with multi-threaded processing units.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 23:58:58 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 00:06:34 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Song", "Hang", ""], ["Matsuno", "Kristen V.", ""], ["West", "Jacob R.", ""], ["Subramaniam", "Akshay", ""], ["Ghate", "Aditya S.", ""], ["Lele", "Sanjiva K.", ""]]}, {"id": "2101.02373", "submitter": "Sin Kit Lo", "authors": "Sin Kit Lo, Qinghua Lu, Liming Zhu, Hye-young Paik, Xiwei Xu, Chen\n  Wang", "title": "Architectural Patterns for the Design of Federated Learning Systems", "comments": "Resubmitted after minor revision to Elsevier's Journal of Systems and\n  Software, Special issue on Software Architecture and Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has received fast-growing interests from academia and\nindustry to tackle the challenges of data hungriness and privacy in machine\nlearning. A federated learning system can be viewed as a large-scale\ndistributed system with different components and stakeholders as numerous\nclient devices participate in federated learning. Designing a federated\nlearning system requires software system design thinking apart from machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning technique aspects, the software architecture design\nconcerns in building federated learning systems have been largely ignored.\nTherefore, in this paper, we present a collection of architectural patterns to\ndeal with the design challenges of federated learning systems. Architectural\npatterns present reusable solutions to a commonly occurring problem within a\ngiven context during software architecture design. The presented patterns are\nbased on the results of a systematic literature review and include three client\nmanagement patterns, four model management patterns, three model training\npatterns, and four model aggregation patterns. The patterns are associated to\nthe particular state transitions in a federated learning model lifecycle,\nserving as a guidance for effective use of the patterns in the design of\nfederated learning systems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:11:09 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 05:10:21 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 05:09:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lo", "Sin Kit", ""], ["Lu", "Qinghua", ""], ["Zhu", "Liming", ""], ["Paik", "Hye-young", ""], ["Xu", "Xiwei", ""], ["Wang", "Chen", ""]]}, {"id": "2101.02814", "submitter": "Yukio Hayashi", "authors": "Yukio Hayashi, Atsushi Tanaka, and Jun Matsukubo", "title": "More Tolerant Reconstructed Networks by Self-Healing against Attacks in\n  Saving Resource", "comments": "23 pages, 6 figures", "journal-ref": "Entropy, Special Issue: Critical Phenomena and Optimization in\n  Complex Networks, Vol.23(Issue 1), No.102, pp.1-15, (2021)", "doi": "10.3390/e23010102", "report-no": null, "categories": "physics.soc-ph cs.DC cs.SI nlin.AO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex network infrastructure systems for power-supply, communication, and\ntransportation support our economical and social activities, however they are\nextremely vulnerable against the frequently increasing large disasters or\nattacks. Thus, a reconstructing from damaged network is rather advisable than\nempirically performed recovering to the original vulnerable one. In order to\nreconstruct a sustainable network, we focus on enhancing loops so as not to be\ntrees as possible by node removals. Although this optimization is corresponded\nto an intractable combinatorial problem, we propose self-healing methods based\non enhancing loops in applying an approximate calculation inspired from a\nstatistical physics approach. We show that both higher robustness and\nefficiency are obtained in our proposed methods with saving the resource of\nlinks and ports than ones in the conventional healing methods. Moreover, the\nreconstructed network by healing can become more tolerant than the original one\nbefore attacks, when some extent of damaged links are reusable or compensated\nas investment of resource. These results will be open up the potential of\nnetwork reconstruction by self-healing with adaptive capacity in the meaning of\nresilience.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 00:53:11 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Hayashi", "Yukio", ""], ["Tanaka", "Atsushi", ""], ["Matsukubo", "Jun", ""]]}, {"id": "2101.03201", "submitter": "Kai Zhao", "authors": "Kai Zhao, Sheng Di, Xin Liang, Sihuan Li, Dingwen Tao, Julie Bessac,\n  Zizhong Chen, Franck Cappello", "title": "SDRBench: Scientific Data Reduction Benchmark for Lossy Compressors", "comments": "Published in Proceedings of the 1st International Workshop on Big\n  Data Reduction @BigData'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient error-controlled lossy compressors are becoming critical to the\nsuccess of today's large-scale scientific applications because of the\never-increasing volume of data produced by the applications. In the past\ndecade, many lossless and lossy compressors have been developed with distinct\ndesign principles for different scientific datasets in largely diverse\nscientific domains. In order to support researchers and users assessing and\ncomparing compressors in a fair and convenient way, we establish a standard\ncompression assessment benchmark -- Scientific Data Reduction Benchmark\n(SDRBench). SDRBench contains a vast variety of real-world scientific datasets\nacross different domains, summarizes several critical compression quality\nevaluation metrics, and integrates many state-of-the-art lossy and lossless\ncompressors. We demonstrate evaluation results using SDRBench and summarize six\nvaluable takeaways that are helpful to the in-depth understanding of lossy\ncompressors.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 19:52:32 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhao", "Kai", ""], ["Di", "Sheng", ""], ["Liang", "Xin", ""], ["Li", "Sihuan", ""], ["Tao", "Dingwen", ""], ["Bessac", "Julie", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "2101.03300", "submitter": "Hang Chen", "authors": "Hang Chen, Syed Ali Asif, Jihong Park, Chien-Chung Shen, Mehdi Bennis", "title": "Robust Blockchained Federated Learning with Model Validation and\n  Proof-of-Stake Inspired Consensus", "comments": "8 pages, 7 figures, AAAI 2021 Workshop - Towards Robust, Secure and\n  Efficient Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a promising distributed learning solution that\nonly exchanges model parameters without revealing raw data. However, the\ncentralized architecture of FL is vulnerable to the single point of failure. In\naddition, FL does not examine the legitimacy of local models, so even a small\nfraction of malicious devices can disrupt global training. To resolve these\nrobustness issues of FL, in this paper, we propose a blockchain-based\ndecentralized FL framework, termed VBFL, by exploiting two mechanisms in a\nblockchained architecture. First, we introduced a novel decentralized\nvalidation mechanism such that the legitimacy of local model updates is\nexamined by individual validators. Second, we designed a dedicated\nproof-of-stake consensus mechanism where stake is more frequently rewarded to\nhonest devices, which protects the legitimate local model updates by increasing\ntheir chances of dictating the blocks appended to the blockchain. Together,\nthese solutions promote more federation within legitimate devices, enabling\nrobust FL. Our emulation results of the MNIST classification corroborate that\nwith 15% of malicious devices, VBFL achieves 87% accuracy, which is 7.4x higher\nthan Vanilla FL.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 06:30:38 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Hang", ""], ["Asif", "Syed Ali", ""], ["Park", "Jihong", ""], ["Shen", "Chien-Chung", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2101.03367", "submitter": "Stefano Savazzi", "authors": "Stefano Savazzi, Monica Nicoli, Mehdi Bennis, Sanaz Kianoush, Luca\n  Barbieri", "title": "Opportunities of Federated Learning in Connected, Cooperative and\n  Automated Industrial Systems", "comments": "The paper has been accepted for publication in the IEEE\n  Communications Magazine. The current arXiv contains an additional Appendix\n  that describes the dataset for the setup of Fig. 5", "journal-ref": "IEEE Communications Magazine, vol. 59, no. 2, pp. 16-21, February\n  2021", "doi": "10.1109/MCOM.001.2000200", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Next-generation autonomous and networked industrial systems (i.e., robots,\nvehicles, drones) have driven advances in ultra-reliable, low latency\ncommunications (URLLC) and computing. These networked multi-agent systems\nrequire fast, communication-efficient and distributed machine learning (ML) to\nprovide mission critical control functionalities. Distributed ML techniques,\nincluding federated learning (FL), represent a mushrooming multidisciplinary\nresearch area weaving in sensing, communication and learning. FL enables\ncontinual model training in distributed wireless systems: rather than fusing\nraw data samples at a centralized server, FL leverages a cooperative fusion\napproach where networked agents, connected via URLLC, act as distributed\nlearners that periodically exchange their locally trained model parameters.\nThis article explores emerging opportunities of FL for the next-generation\nnetworked industrial systems. Open problems are discussed, focusing on\ncooperative driving in connected automated vehicles and collaborative robotics\nin smart manufacturing.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 14:27:52 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 22:42:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Savazzi", "Stefano", ""], ["Nicoli", "Monica", ""], ["Bennis", "Mehdi", ""], ["Kianoush", "Sanaz", ""], ["Barbieri", "Luca", ""]]}, {"id": "2101.03524", "submitter": "Ahmad Banijamali", "authors": "Ahmad Banijamali, Pasi Kuvaja, Markku Oivo and Pooyan Jamshidi", "title": "Kuksa*: Self-Adaptive Microservices in Automotive Systems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-64148-1_23", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In pervasive dynamic environments, vehicles connect to other objects to send\noperational data and receive updates so that vehicular applications can provide\nservices to users on demand. Automotive systems should be self-adaptive,\nthereby they can make real-time decisions based on changing operating\nconditions. Emerging modern solutions, such as microservices could improve\nself-adaptation capabilities and ensure higher levels of quality performance in\nmany domains. We employed a real-world automotive platform called Eclipse Kuksa\nto propose a framework based on microservices architecture to enhance the\nself-adaptation capabilities of automotive systems for runtime data analysis.\nTo evaluate the designed solution, we conducted an experiment in an automotive\nlaboratory setting where our solution was implemented as a microservice-based\nadaptation engine and integrated with other Eclipse Kuksa components. The\nresults of our study indicate the importance of design trade-offs for quality\nrequirements' satisfaction levels of each microservices and the whole system\nfor the optimal performance of an adaptive system at runtime.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 11:11:11 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Banijamali", "Ahmad", ""], ["Kuvaja", "Pasi", ""], ["Oivo", "Markku", ""], ["Jamshidi", "Pooyan", ""]]}, {"id": "2101.03533", "submitter": "Md. Redowan Mahmud", "authors": "Redowan Mahmud, Adel N. Toosi", "title": "Con-Pi: A Distributed Container-based Edge and Fog Computing Framework\n  for Raspberry Pis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Edge and Fog computing paradigms overcome the limitations of Cloud-centric\nexecution for different latency-sensitive Internet of Things (IoT) applications\nby offering computing resources closer to the data sources. In both paradigms,\nsingle-board small computers like Raspberry Pis (RPis) are widely used as the\ncomputing nodes. RPis are usually equipped with processors having moderate\nspeed and provide supports for peripheral interfacing and networking. These\nfeatures make RPis well-suited to deal with IoT-driven operations such as data\nsensing, analysis and actuation. However, RPis are constrained in facilitating\nmulti-tenancy and resource sharing. The management of RPi-based computing and\nperipheral resources through centralized entities further degrades their\nperformance and service quality significantly. To address these issues, a\nframework, named \\textit{Con-Pi} is proposed in this work. It exploits the\nconcept of containerization and harnesses Docker containers to run IoT\napplications as microservices on RPis. Moreover, Con-Pi operates in a\ndistributed manner across multiple RPis and enables them to share resources.\nThe software system of the proposed framework also provides a scope to\nintegrate different application, resource and energy management policies for\nEdge and Fog computing. The performance of the proposed framework is compared\nwith the state-of-the-art frameworks by means of real-world experiments. The\nexperimental results evident that Con-Pi outperforms others in enhancing\nresponse time and managing energy usage and computing resources through\ndistributed offloading. Additionally, we have developed a pest bird deterrent\nsystem using Con-Pi to demonstrate its suitability in developing practical\nsolutions for various IoT-enabled use cases including smart agriculture.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 12:30:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mahmud", "Redowan", ""], ["Toosi", "Adel N.", ""]]}, {"id": "2101.03715", "submitter": "Zhuolun Xiang", "authors": "Zhuolun Xiang, Dahlia Malkhi, Kartik Nayak, Ling Ren", "title": "Strengthened Fault Tolerance in Byzantine Fault Tolerant Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine fault tolerant (BFT) state machine replication (SMR) is an\nimportant building block for constructing permissioned blockchain systems. In\ncontrast to Nakamoto Consensus where any block obtains higher assurance as\nburied deeper in the blockchain, in BFT SMR, any committed block is secure has\na fixed resilience threshold. In this paper, we investigate strengthened fault\ntolerance (SFT) in BFT SMR under partial synchrony, which provides gradually\nincreased resilience guarantees (like Nakamoto Consensus) during an optimistic\nperiod when the network is synchronous and the number of Byzantine faults is\nsmall. Moreover, the committed blocks can tolerate more than one-third (up to\ntwo-thirds) corruptions even after the optimistic period. Compared to the prior\nbest solution Flexible BFT which requires quadratic message complexity, our\nsolution maintains the linear message complexity of state-of-the-art BFT SMR\nprotocols and requires only marginal bookkeeping overhead. We implement our\nsolution over the open-source Diem project, and give experimental results that\ndemonstrate its efficiency under real-world scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:55:44 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Xiang", "Zhuolun", ""], ["Malkhi", "Dahlia", ""], ["Nayak", "Kartik", ""], ["Ren", "Ling", ""]]}, {"id": "2101.03722", "submitter": "Jagruti Sahoo", "authors": "Jagruti Sahoo, Kristin Barrett", "title": "Internet of Things (IoT) Application Model for Smart Farming", "comments": "Submitted to IEEE Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart Farming has brought a major transformation in the agriculture process\nby using the Internet of Things (IoT) devices, emerging technologies such as\ncloud computing, fog computing, and data analytics. It allows farmers to have\nreal-time awareness of the farm and help them make smart and informed\ndecisions. In this paper, we propose a distributed data flow (DDF) based model\nfor the smart farming application that is composed of interdependent modules.\nWe evaluate the proposed application model using two deployment strategies:\ncloud-based, and fog-based where the application modules are deployed on the\nfog and the cloud data center respectively. We compare the cloud-based and\nfog-based strategy in terms of end-to-end latency and network usage.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 06:23:00 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sahoo", "Jagruti", ""], ["Barrett", "Kristin", ""]]}, {"id": "2101.03733", "submitter": "Nevin Vunka Jungum", "authors": "Nevin Vunka Jungum, Nawaz Mohamudally and Nimal Nissanke", "title": "A Fault Tolerant Mechanism for Partitioning and Offloading Framework in\n  Pervasive Environments", "comments": null, "journal-ref": "International Journal of Computer Science Issues, Vol 17, Issue 6,\n  November 2020", "doi": "10.5281/zenodo.4431214", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application partitioning and code offloading are being researched extensively\nduring the past few years. Several frameworks for code offloading have been\nproposed. However, fewer works attempted to address issues occurred with its\nimplementation in pervasive environments such as frequent network disconnection\ndue to high mobility of users. Thus, in this paper, we proposed a fault\ntolerant algorithm that helps in consolidating the efficiency and robustness of\napplication partitioning and offloading frameworks. To permit the usage of\ndifferent fault tolerant policies such as replication and checkpointing, the\ndevices are grouped into high and low reliability clusters. Experimental\nresults shown that the fault tolerant algorithm can easily adapt to different\nexecution conditions while incurring minimum overhead.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 07:33:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Jungum", "Nevin Vunka", ""], ["Mohamudally", "Nawaz", ""], ["Nissanke", "Nimal", ""]]}, {"id": "2101.03780", "submitter": "Philipp Czerner", "authors": "Philipp Czerner, Stefan Jaax", "title": "Running Time Analysis of Broadcast Consensus Protocols", "comments": "To be published in the Proceedings of the 24th International\n  Conference on Foundations of Software Science and Computation Structures\n  (FoSSaCS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadcast consensus protocols (BCPs) are a model of computation, in which\nanonymous, identical, finite-state agents compute by sending/receiving global\nbroadcasts. BCPs are known to compute all number predicates in\n$\\mathsf{NL}=\\mathsf{NSPACE}(\\log n)$ where $n$ is the number of agents. They\ncan be considered an extension of the well-established model of population\nprotocols. This paper investigates execution time characteristics of BCPs. We\nshow that every predicate computable by population protocols is computable by a\nBCP with expected $\\mathcal{O}(n \\log n)$ interactions, which is asymptotically\noptimal. We further show that every log-space, randomized Turing machine can be\nsimulated by a BCP with $\\mathcal{O}(n \\log n \\cdot T)$ interactions in\nexpectation, where $T$ is the expected runtime of the Turing machine. This\nallows us to characterise polynomial-time BCPs as computing exactly the number\npredicates in $\\mathsf{ZPL}$, i.e. predicates decidable by log-space bounded\nrandomised Turing machine with zero-error in expected polynomial time where the\ninput is encoded as unary.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 09:14:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Czerner", "Philipp", ""], ["Jaax", "Stefan", ""]]}, {"id": "2101.03840", "submitter": "Hao Wang", "authors": "Qing Yang, Hao Wang", "title": "Privacy-Preserving Transactive Energy Management for IoT-aided Smart\n  Homes via Blockchain", "comments": null, "journal-ref": "IEEE Internet of Things Journal, 2021", "doi": "10.1109/JIOT.2021.3051323", "report-no": null, "categories": "eess.SY cs.CR cs.DC cs.SY math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the booming of smart grid, The ubiquitously deployed smart meters\nconstitutes an energy internet of things. This paper develops a novel\nblockchain-based transactive energy management system for IoT-aided smart\nhomes. We consider a holistic set of options for smart homes to participate in\ntransactive energy. Smart homes can interact with the grid to perform vertical\ntransactions, e.g., feeding in extra solar energy to the grid and providing\ndemand response service to alleviate the grid load. Smart homes can also\ninteract with peer users to perform horizontal transactions, e.g., peer-to-peer\nenergy trading. However, conventional transactive energy management method\nsuffers from the drawbacks of low efficiency, privacy leakage, and single-point\nfailure. To address these challenges, we develop a privacy-preserving\ndistributed algorithm that enables users to optimally manage their energy\nusages in parallel via the smart contract on the blockchain. Further, we design\nan efficient blockchain system tailored for IoT devices and develop the smart\ncontract to support the holistic transactive energy management system. Finally,\nwe evaluate the feasibility and performance of the blockchain-based transactive\nenergy management system through extensive simulations and experiments. The\nresults show that the blockchain-based transactive energy management system is\nfeasible on practical IoT devices and reduces the overall cost by 25%.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:21:55 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 13:18:12 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Qing", ""], ["Wang", "Hao", ""]]}, {"id": "2101.04025", "submitter": "Malte S. Kurz", "authors": "Malte S. Kurz", "title": "Distributed Double Machine Learning with a Serverless Architecture", "comments": null, "journal-ref": "In Companion of the ACM/SPEC International Conference on\n  Performance Engineering (ICPE '21), 2021, Association for Computing\n  Machinery, New York, NY, USA, 27-33", "doi": "10.1145/3447545.3451181", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores serverless cloud computing for double machine learning.\nBeing based on repeated cross-fitting, double machine learning is particularly\nwell suited to exploit the high level of parallelism achievable with serverless\ncomputing. It allows to get fast on-demand estimations without additional cloud\nmaintenance effort. We provide a prototype Python implementation\n\\texttt{DoubleML-Serverless} for the estimation of double machine learning\nmodels with the serverless computing platform AWS Lambda and demonstrate its\nutility with a case study analyzing estimation times and costs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:58:30 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 12:13:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kurz", "Malte S.", ""]]}, {"id": "2101.04163", "submitter": "Yipeng Zhou", "authors": "Yao Fu, Yipeng Zhou, Di Wu, Shui Yu, Yonggang Wen, Chao Li", "title": "On the Practicality of Differential Privacy in Federated Learning by\n  Tuning Iteration Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite that Federated Learning (FL) is well known for its privacy\nprotection when training machine learning models among distributed clients\ncollaboratively, recent studies have pointed out that the naive FL is\nsusceptible to gradient leakage attacks. In the meanwhile, Differential Privacy\n(DP) emerges as a promising countermeasure to defend against gradient leakage\nattacks. However, the adoption of DP by clients in FL may significantly\njeopardize the model accuracy. It is still an open problem to understand the\npracticality of DP from a theoretic perspective. In this paper, we make the\nfirst attempt to understand the practicality of DP in FL through tuning the\nnumber of conducted iterations. Based on the FedAvg algorithm, we formally\nderive the convergence rate with DP noises in FL. Then, we theoretically\nderive: 1) the conditions for the DP based FedAvg to converge as the number of\nglobal iterations (GI) approaches infinity; 2) the method to set the number of\nlocal iterations (LI) to minimize the negative influence of DP noises. By\nfurther substituting the Laplace and Gaussian mechanisms into the derived\nconvergence rate respectively, we show that: 3) The DP based FedAvg with the\nLaplace mechanism cannot converge, but the divergence rate can be effectively\nprohibited by setting the number of LIs with our method; 4) The learning error\nof the DP based FedAvg with the Gaussian mechanism can converge to a constant\nnumber finally if we use a fixed number of LIs per GI. To verify our\ntheoretical findings, we conduct extensive experiments using two real-world\ndatasets. The results not only validate our analysis results, but also provide\nuseful guidelines on how to optimize model accuracy when incorporating DP into\nFL\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:43:12 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Fu", "Yao", ""], ["Zhou", "Yipeng", ""], ["Wu", "Di", ""], ["Yu", "Shui", ""], ["Wen", "Yonggang", ""], ["Li", "Chao", ""]]}, {"id": "2101.04192", "submitter": "Marco Marcozzi", "authors": "Marco Marcozzi and Leonardo Mostarda", "title": "Quantum Consensus: an overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC cs.ET", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We review the literature about reaching agreement in quantum networks, also\ncalled quantum consensus. After a brief introduction to the key feature of\nquantum computing, allowing the reader with no quantum theory background to\nhave minimal tools to understand, we report a formal definition of quantum\nconsensus and the protocols proposed. Proposals are classified according to the\nquantum feature used to achieve agreement.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 21:10:06 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Marcozzi", "Marco", ""], ["Mostarda", "Leonardo", ""]]}, {"id": "2101.04335", "submitter": "Khaled Alanezi", "authors": "Khaled Alanezi and Xinyang Zhou and Lijun Chen and Shivakant Mishra", "title": "Panorama: A Framework to Support Collaborative Context Monitoring on\n  Co-Located Mobile Devices", "comments": "Published in MobiCase 2015", "journal-ref": null, "doi": "10.1007/978-3-319-29003-4_9", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A key challenge in wide adoption of sophisticated context-aware applications\nis the requirement of continuous sensing and context computing. This paper\npresents Panorama, a middleware that identifies collaboration opportunities to\noffload context computing tasks to nearby mobile devices as well as\ncloudlets/cloud. At the heart of Panorama is a multi-objective optimizer that\ntakes into account different constraints such as access cost, computation\ncapability, access latency, energy consumption and data privacy, and\nefficiently computes a collaboration plan optimized simultaneously for\ndifferent objectives such as minimizing cost, energy and/or execution time.\nPanorama provides support for discovering nearby devices and cloudlets/cloud,\ncomputing an optimal collaboration plan, distributing computation to\nparticipating devices, and getting the results back. The paper provides an\nextensive evaluation of Panorama via two representative context monitoring\napplications over a set of Android devices and a cloudlet/cloud under different\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 07:51:57 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Alanezi", "Khaled", ""], ["Zhou", "Xinyang", ""], ["Chen", "Lijun", ""], ["Mishra", "Shivakant", ""]]}, {"id": "2101.04395", "submitter": "Michael Witterauf", "authors": "Michael Witterauf and Dominik Walter and Frank Hannig and J\\\"urgen\n  Teich", "title": "Symbolic Loop Compilation for Tightly Coupled Processor Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop compilation for Tightly Coupled Processor Arrays (TCPAs), a class of\nmassively parallel loop accelerators, entails solving NP-hard problems, yet\ndepends on the loop bounds and number of available processing elements (PEs),\nparameters known only at runtime because of dynamic resource management and\ninput sizes. Therefore, this article proposes a two-phase approach called\nsymbolic loop compilation: At compile time, the necessary NP-complete problems\nare solved and the solutions compiled into a space-efficient symbolic\nconfiguration. At runtime, a concrete configuration is generated from the\nsymbolic configuration according to the parameters values. We show that the\nlatter phase, called instantiation, runs in polynomial time with its most\ncomplex step, program instantiation, not depending on the number of PEs. As\nvalidation, we performed symbolic loop compilation on real-world loops and\nmeasured time and space requirements. Our experiments confirm that a symbolic\nconfiguration is space-efficient and suited for systems with little memory --\noften, a symbolic configuration is smaller than a single concrete configuration\n-- and that program instantiation scales well with the number of PEs -- for\nexample, when instantiating a symbolic configuration of a matrix-matrix\nmultiplication, the execution time is similar for $4\\times 4$ and $32\\times 32$\nPEs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:38:39 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Witterauf", "Michael", ""], ["Walter", "Dominik", ""], ["Hannig", "Frank", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "2101.04400", "submitter": "Miguel A. Mosteiro", "authors": "Dariusz R. Kowalski and Miguel A. Mosteiro", "title": "Time and Communication Complexity of Leader Election in Anonymous\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of randomized Leader Election in synchronous distributed\nnetworks with indistinguishable nodes. We consider algorithms that work on\nnetworks of arbitrary topology in two settings, depending on whether the size\nof the network, i.e., the number of nodes, is known or not. In the former\nsetting, we present a new Leader Election protocol that improves over previous\nwork by lowering message complexity and making it close to a lower bound by a\nfactor of $\\tilde{O}(\\sqrt{t_{mix}\\sqrt{\\Phi}})$, where $\\Phi$ is the\nconductance and $t_{mix}$ is the mixing time of the network graph. We then show\nthat lacking the network size no Leader Election algorithm can guarantee that\nthe election is final with constant probability, even with unbounded\ncommunication. Hence, we further classify the problem as Irrevocable Leader\nElection (the classic one, requiring knowledge of n - as is our first protocol)\nor Revocable Leader Election, and present a new polynomial time and message\ncomplexity Revocable Leader Election algorithm in the setting without knowledge\nof network size. We analyze time and message complexity of our protocols in the\nCongest model of communication.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:48:44 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 21:46:20 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Kowalski", "Dariusz R.", ""], ["Mosteiro", "Miguel A.", ""]]}, {"id": "2101.04489", "submitter": "Thomas Loruenser", "authors": "Thomas Loruenser, Benjamin Rainer and Florian Wohner", "title": "Towards a Performance Model for Byzantine Fault Tolerant (Storage)\n  Services", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine fault-tolerant systems have been researched for more than four\ndecades, and although shown possible early, the solutions were impractical for\na long time. With PBFT the first practical solution was proposed in 1999 and\nspawned new research which culminated in novel applications using it today.\nAlthough the safety and liveness properties of PBFT-type protocols have been\nrigorously analyzed, when it comes to practical performance only empirical\nresults - often in artificial settings - are known and imperfections on the\ncommunication channels are not specifically considered. In this work we present\nthe first performance model for PBFT specifically considering the impact of\nunreliable channels and the use of different transport protocols over them. We\nalso did extensive simulations to verify the model and to gain more insight on\nthe impact of deployment parameters on the overall transaction time. We show\nthat the usage of UDP can lead to significant speedup for PBFT protocols\ncompared to TCP when tuned accordingly even over lossy channels. Finally, we\ncompared the simulation to a real implementation and measure the benefits of a\ndeveloped improvement directly. We found that the impact on the design of the\nnetwork layer has been overlooked in the past but offers some additional room\nfor improvement when it comes to practical performance. In this work we are\nfocusing on the optimistic case with no node failures, as this is hopefully the\nmost relevant situation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:10:25 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Loruenser", "Thomas", ""], ["Rainer", "Benjamin", ""], ["Wohner", "Florian", ""]]}, {"id": "2101.04677", "submitter": "Desiree Maldonado Carvalho", "authors": "Desiree M. Carvalho and Mari\\'a C. V. Nascimento", "title": "Hybrid matheuristics to solve the integrated lot sizing and scheduling\n  problem on parallel machines with sequence-dependent and non-triangular setup", "comments": "32 pages, 1 appendix, submitted to European Journal of Operational\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper approaches the integrated lot sizing and scheduling problem\n(ILSSP), in which non-identical machines work in parallel with non-triangular\nsequence-dependent setup costs and times, setup carry-over and capacity\nlimitation. The aim of the studied ILSSP, here called ILSSP-NT on parallel\nmachines, is to determine a production planning and tasks sequencing that meet\nperiod demands without delay and in such a way that the total costs of\nproduction, machine setup and inventory are minimized. The dearth of literature\non the ILSSP-NT, despite the increasing amount of applications in the\nindustrial sector, mainly in the food processing industry, motivated us to\nconduct this study. In this paper, we propose efficient methods to solve the\nILSSP-NT on parallel machines. The methods virtually consist in the\nhybridization of the relax-and-fix and fix-and-optimize methods with the\npath-relinking and kernel search heuristics. To assess how well the heuristics\nsolve the ILSSP-NT on parallel machines, we compared their results with those\nof the CPLEX solver with a fixed CPU time limit. The proposed matheuristics\nsignificantly outperformed CPLEX in most of the tested instances.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 00:33:57 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Carvalho", "Desiree M.", ""], ["Nascimento", "Mari\u00e1 C. V.", ""]]}, {"id": "2101.04766", "submitter": "Mahnush Movahedi", "authors": "Mahnush Movahedi, Benjamin M. Case, Andrew Knox, Li Li, Yiming Paul\n  Li, Sanjay Saravanan, Shubho Sengupta, Erik Taubeneck", "title": "Private Randomized Controlled Trials: A Protocol for Industry Scale\n  Deployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we outline a way to deploy a privacy-preserving protocol for\nmultiparty Randomized Controlled Trials on the scale of 500 million rows of\ndata and more than a billion gates. Randomized Controlled Trials (RCTs) are\nwidely used to improve business and policy decisions in various sectors such as\nhealthcare, education, criminology, and marketing. A Randomized Controlled\nTrial is a scientifically rigorous method to measure the effectiveness of a\ntreatment. This is accomplished by randomly allocating subjects to two or more\ngroups, treating them differently, and then comparing the outcomes across\ngroups. In many scenarios, multiple parties hold different parts of the data\nfor conducting and analyzing RCTs. Given privacy requirements and expectations\nof each of these parties, it is often challenging to have a centralized store\nof data to conduct and analyze RCTs.\n  We accomplish this by a three-stage solution. The first stage uses the\nPrivate Secret Share Set Intersection (PS$^3$I) solution to create a joined set\nand establish secret shares without revealing membership, while discarding\nindividuals who were placed into more than one group. The second stage runs\nmultiple instances of a general purpose MPC over a sharded database to\naggregate statistics about each experimental group while discarding individuals\nwho took an action before they received treatment. The third stage adds\ndistributed and calibrated Differential Privacy (DP) noise to the aggregate\nstatistics and uncertainty measures, providing formal two-sided privacy\nguarantees.\n  We also evaluate the performance of multiple open source general purpose MPC\nlibraries for this task. We additionally demonstrate how we have used this to\ncreate a working ads effectiveness measurement product capable of measuring\nhundreds of millions of individuals per experiment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:37:57 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Movahedi", "Mahnush", ""], ["Case", "Benjamin M.", ""], ["Knox", "Andrew", ""], ["Li", "Li", ""], ["Li", "Yiming Paul", ""], ["Saravanan", "Sanjay", ""], ["Sengupta", "Shubho", ""], ["Taubeneck", "Erik", ""]]}, {"id": "2101.04816", "submitter": "Agnieszka Miedlar", "authors": "Zachary R. Atkins, Christopher J. Vogl, Achintya Madduri, Nan Duan,\n  Agnieszka K. Miedlar, Daniel Merl", "title": "Distribution System Voltage Prediction from Smart Inverters using\n  Decentralized Regression", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As photovoltaic (PV) penetration continues to rise and smart inverter\nfunctionality continues to expand, smart inverters and other distributed energy\nresources (DERs) will play increasingly important roles in distribution system\npower management and security. In this paper, it is demonstrated that a\nconstellation of smart inverters in a simulated distribution circuit can enable\nprecise voltage predictions using an asynchronous and decentralized prediction\nalgorithm. Using simulated data and a constellation of 15 inverters in a ring\ncommunication topology, the COLA algorithm is shown to accomplish the learning\ntask required for voltage magnitude prediction with far less communication\noverhead than fully connected P2P learning protocols. Additionally, a dynamic\nstopping criterion is proposed that does not require a regularizer like the\noriginal COLA stopping criterion.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:22:36 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Atkins", "Zachary R.", ""], ["Vogl", "Christopher J.", ""], ["Madduri", "Achintya", ""], ["Duan", "Nan", ""], ["Miedlar", "Agnieszka K.", ""], ["Merl", "Daniel", ""]]}, {"id": "2101.04825", "submitter": "Dimitris Chatzopoulos", "authors": "Dimitris Chatzopoulos, Anurag Jain, Sujit Gujar, Boi Faltings, Pan Hui", "title": "Towards Mobile Distributed Ledgers", "comments": "Part of it was presented in IEEE Infocom 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in mobile computing have paved the way for new types of distributed\napplications that can be executed solely by mobile devices on device-to-device\n(D2D) ecosystems (e.g., crowdsensing). Sophisticated applications, like\ncryptocurrencies, need distributed ledgers to function. Distributed ledgers,\nsuch as blockchains and directed acyclic graphs (DAGs), employ consensus\nprotocols to add data in the form of blocks. However, such protocols are\ndesigned for resourceful devices that are interconnected via the Internet.\nMoreover, existing distributed ledgers are not deployable to D2D ecosystems\nsince their storage needs are continuously increasing. In this work, we\nintroduce and analyse Mneme, a DAG-based distributed ledger that can be\nmaintained solely by mobile devices. Mneme utilizes two novel consensus\nprotocols: Proof-of-Context (PoC) and Proof-of-Equivalence (PoE). PoC employs\nusers' context to add data on Mneme. PoE is executed periodically to summarize\ndata and produce equivalent blocks that require less storage. We analyze\nMneme's security and justify the ability of PoC and PoE to guarantee the\ncharacteristics of distributed ledgers: persistence and liveness. Furthermore,\nwe analyze potential attacks from malicious users and prove that the\nprobability of a successful attack is inversely proportional to the square of\nthe number of mobile users who maintain Mneme.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 01:26:36 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Chatzopoulos", "Dimitris", ""], ["Jain", "Anurag", ""], ["Gujar", "Sujit", ""], ["Faltings", "Boi", ""], ["Hui", "Pan", ""]]}, {"id": "2101.04977", "submitter": "Sasho Nedelkoski", "authors": "Jasmin Bogatinovski and Sasho Nedelkoski", "title": "Multi-Source Anomaly Detection in Distributed IT Systems", "comments": "12 pages. Presented at AIOPS 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-source data generated by distributed systems, provide a holistic\ndescription of the system. Harnessing the joint distribution of the different\nmodalities by a learning model can be beneficial for critical applications for\nmaintenance of the distributed systems. One such important task is the task of\nanomaly detection where we are interested in detecting the deviation of the\ncurrent behaviour of the system from the theoretically expected. In this work,\nwe utilize the joint representation from the distributed traces and system log\ndata for the task of anomaly detection in distributed systems. We demonstrate\nthat the joint utilization of traces and logs produced better results compared\nto the single modality anomaly detection methods. Furthermore, we formalize a\nlearning task - next template prediction NTP, that is used as a generalization\nfor anomaly detection for both logs and distributed trace. Finally, we\ndemonstrate that this formalization allows for the learning of template\nembedding for both the traces and logs. The joint embeddings can be reused in\nother applications as good initialization for spans and logs.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:11:32 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bogatinovski", "Jasmin", ""], ["Nedelkoski", "Sasho", ""]]}, {"id": "2101.05421", "submitter": "Sayaka Kamei", "authors": "Sayaka Kamei, Anissa Lamani, Fukuhito Ooshita, Sebastien Tixeuil,\n  Koichi Wada", "title": "Asynchronous Gathering in a Torus", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the gathering problem for asynchronous and oblivious robots that\ncannot communicate explicitly with each other, but are endowed with visibility\nsensors that allow them to see the positions of the other robots. Most of the\ninvestigations on the gathering problem on the discrete universe are done on\nring shaped networks due to the number of symmetric configuration. We extend in\nthis paper the study of the gathering problem on torus shaped networks assuming\nrobots endowed with local weak multiplicity detection. That is, robots cannot\nmake the difference between nodes occupied by only one robot from those\noccupied by more than one robots unless it is their current node. As a\nconsequence, solutions based on creating a single multiplicity node as a\nlandmark for the gathering cannot be used. We present in this paper a\ndeterministic algorithm that solves the gathering problem starting from any\nrigid configuration on an asymmetric unoriented torus shaped network.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 02:06:28 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kamei", "Sayaka", ""], ["Lamani", "Anissa", ""], ["Ooshita", "Fukuhito", ""], ["Tixeuil", "Sebastien", ""], ["Wada", "Koichi", ""]]}, {"id": "2101.05428", "submitter": "Priyanka Mary Mammen", "authors": "Priyanka Mary Mammen", "title": "Federated Learning: Opportunities and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) is a concept first introduced by Google in 2016, in\nwhich multiple devices collaboratively learn a machine learning model without\nsharing their private data under the supervision of a central server. This\noffers ample opportunities in critical domains such as healthcare, finance etc,\nwhere it is risky to share private user information to other organisations or\ndevices. While FL appears to be a promising Machine Learning (ML) technique to\nkeep the local data private, it is also vulnerable to attacks like other ML\nmodels. Given the growing interest in the FL domain, this report discusses the\nopportunities and challenges in federated learning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 02:44:28 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Mammen", "Priyanka Mary", ""]]}, {"id": "2101.05462", "submitter": "Haiwen Du Dr.", "authors": "Dongjie Zhu, Haiwen Du, Yundong Sun, Zhaoshuo Tian", "title": "Leader Confirmation Replication for Millisecond Consensus in\n  Geo-distributed Systems", "comments": "Prepared to submit to SRDS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-distributed private chain and database have created higher performance\nrequirements for consistency models. However, with millisecond network latency\nbetween nodes, the widely used leader-based SMR models cause frequent\nretransmission of logs since they cannot know the logs replication status in\ntime, which resulting in the leader costing high network and computing\nresource. To address the problem, we proposed a Leader Confirmation based\nReplication (LCR) model. First, we demonstrate the efficacy of the approach by\ndesigning the Future-Log Replication model, which the followers are responsible\nfor non-transactional log replication. It reduces the leader's network load\nusing the signal log. Secondly, we designed a Generation Re-replication\nstrategy, which can ensure the security and consistency of future-logs when the\nnumber of nodes changes. Finally, we implemented LCR-Raft and designed\nexperiments. The results show that in the single-ms network latency\nenvironments, LCR-Raft can provide 1.5X higher TPS, reduces transactional data\nresponse time 40%-60%, and network traffic by 20%-30% with acceptable network\ntraffic and CPU cost on followers. Besides, LCR can provide high portability\nsince it does not change the number of leader and election process.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 05:25:04 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 14:27:47 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhu", "Dongjie", ""], ["Du", "Haiwen", ""], ["Sun", "Yundong", ""], ["Tian", "Zhaoshuo", ""]]}, {"id": "2101.05471", "submitter": "Li Shen", "authors": "Congliang Chen, Li Shen, Fangyu Zou, Wei Liu", "title": "Towards Practical Adam: Non-Convexity, Convergence Theory, and\n  Mini-Batch Acceleration", "comments": "44 Pages. arXiv admin note: substantial text overlap with\n  arXiv:1811.09358", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adam is one of the most influential adaptive stochastic algorithms for\ntraining deep neural networks, which has been pointed out to be divergent even\nin the simple convex setting via a few simple counterexamples. Many attempts,\nsuch as decreasing an adaptive learning rate, adopting a big batch size,\nincorporating a temporal decorrelation technique, seeking an analogous\nsurrogate, \\textit{etc.}, have been tried to promote Adam-type algorithms to\nconverge. In contrast with existing approaches, we introduce an alternative\neasy-to-check sufficient condition, which merely depends on the parameters of\nthe base learning rate and combinations of historical second-order moments, to\nguarantee the global convergence of generic Adam for solving large-scale\nnon-convex stochastic optimization. This observation coupled with this\nsufficient condition gives much deeper interpretations on the divergence of\nAdam. On the other hand, in practice, mini-Adam and distributed-Adam are widely\nused without theoretical guarantee, we further give an analysis on how will the\nbatch size or the number of nodes in the distributed system will affect the\nconvergence of Adam, which theoretically shows that mini-batch and distributed\nAdam can be linearly accelerated by using a larger mini-batch size or more\nnumber of nodes. At last, we apply the generic Adam and mini-batch Adam with a\nsufficient condition for solving the counterexample and training several\ndifferent neural networks on various real-world datasets. Experimental results\nare exactly in accord with our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 06:42:29 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Chen", "Congliang", ""], ["Shen", "Li", ""], ["Zou", "Fangyu", ""], ["Liu", "Wei", ""]]}, {"id": "2101.05475", "submitter": "Mudabbir Kaleem", "authors": "Mudabbir Kaleem, Keshav Kasichainula, Rabimba Karanjai, Lei Xu, Zhimin\n  Gao, Lin Chen, Weidong Shi", "title": "EDSC: An Event-Driven Smart Contract Platform", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents EDSC, a novel smart contract platform design based on the\nevent-driven execution model as opposed to the traditionally employed\ntransaction-driven execution model. We reason that such a design is a better\nfit for many emerging smart contract applications and is better positioned to\naddress the scalability and performance challenges plaguing the smart contract\necosystem. We propose EDSC's design under the Ethereum framework, and the\ndesign can be easily adapted for other existing smart contract platforms. We\nhave conducted implementation using Ethereum client and experiments where\nperformance modeling results show on average 2.2 to 4.6 times reduced total\nlatency of event triggered smart contracts, which demonstrates its\neffectiveness for supporting contracts that demand timely execution based on\nevents. In addition, we discuss example use cases to demonstrate the design's\nutility and comment on its potential security dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:18:52 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kaleem", "Mudabbir", ""], ["Kasichainula", "Keshav", ""], ["Karanjai", "Rabimba", ""], ["Xu", "Lei", ""], ["Gao", "Zhimin", ""], ["Chen", "Lin", ""], ["Shi", "Weidong", ""]]}, {"id": "2101.05543", "submitter": "Giorgia Azzurra Marson", "authors": "Orestis Alpos, Christian Cachin, Giorgia Azzurra Marson, Luca Zanolini", "title": "On the Synchronization Power of Token Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern blockchains support a variety of distributed applications beyond\ncryptocurrencies, including smart contracts -- which let users execute\narbitrary code in a distributed and decentralized fashion. Regardless of their\nintended application, blockchain platforms implicitly assume consensus for the\ncorrect execution of a smart contract, thus requiring that all transactions are\ntotally ordered. It was only recently recognized that consensus is not\nnecessary to prevent double-spending in a cryptocurrency (Guerraoui et al.,\nPODC'19), contrary to common belief. This result suggests that current\nimplementations may be sacrificing efficiency and scalability because they\nsynchronize transactions much more tightly than actually needed. In this work,\nwe study the synchronization requirements of Ethereum's ERC20 token contract,\none of the most widely adopted smart contacts. Namely, we model a\nsmart-contract token as a concurrent object and analyze its consensus number as\na measure of synchronization power. We show that the richer set of methods\nsupported by ERC20 tokens, compared to standard cryptocurrencies, results in\nstrictly stronger synchronization requirements. More surprisingly, the\nsynchronization power of ERC20 tokens depends on the object's state and can\nthus be modified by method invocations. To prove this result, we develop a\ndedicated framework to express how the object's state affects the needed\nsynchronization level. Our findings indicate that ERC20 tokens, as well as\nother token standards, are more powerful and versatile than plain\ncryptocurrencies, and are subject to dynamic requirements. Developing specific\nsynchronization protocols that exploit these dynamic requirements will pave the\nway towards more robust and scalable blockchain platforms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:50:10 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Alpos", "Orestis", ""], ["Cachin", "Christian", ""], ["Marson", "Giorgia Azzurra", ""], ["Zanolini", "Luca", ""]]}, {"id": "2101.05782", "submitter": "Mario Juri\\'c", "authors": "Mario Juric, Steven Stetzler, Colin T. Slater", "title": "Checkpoint, Restore, and Live Migration for Science Platforms", "comments": "4 pages, 2 figures, to appear in the Proceedings of ADASS XXX", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a fully functional implementation of (per-user) checkpoint,\nrestore, and live migration capabilities for JupyterHub platforms.\nCheckpointing -- the ability to freeze and suspend to disk the running state\n(contents of memory, registers, open files, etc.) of a set of processes --\nenables the system to snapshot a user's Jupyter session to permanent storage.\nThe restore functionality brings a checkpointed session back to a running\nstate, to continue where it left off at a later time and potentially on a\ndifferent machine. Finally, live migration enables moving running Jupyter\nnotebook servers between different machines, transparent to the analysis code\nand w/o disconnecting the user. Our implementation of these capabilities works\nat the system level, with few limitations, and typical checkpoint/restore times\nof O(10s) with a pathway to O(1s) live migrations. It opens a myriad of\ninteresting use cases, especially for cloud-based deployments: from\ncheckpointing idle sessions w/o interruption of the user's work (achieving cost\nreductions of 4x or more), execution on spot instances w. transparent migration\non eviction (with additional cost reductions up to 3x), to automated migration\nof workloads to ideally suited instances (e.g. moving an analysis to a machine\nwith more or less RAM or cores based on observed resource utilization). The\ncapabilities we demonstrate can make science platforms fully elastic while\nretaining excellent user experience.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:34:10 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Juric", "Mario", ""], ["Stetzler", "Steven", ""], ["Slater", "Colin T.", ""]]}, {"id": "2101.05855", "submitter": "Akanksha Atrey", "authors": "Akanksha Atrey, Prashant Shenoy, David Jensen", "title": "Preserving Privacy in Personalized Models for Distributed Mobile\n  Services", "comments": "Published at ICDCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ubiquity of mobile devices has led to the proliferation of mobile\nservices that provide personalized and context-aware content to their users.\nModern mobile services are distributed between end-devices, such as\nsmartphones, and remote servers that reside in the cloud. Such services thrive\non their ability to predict future contexts to pre-fetch content or make\ncontext-specific recommendations. An increasingly common method to predict\nfuture contexts, such as location, is via machine learning (ML) models. Recent\nwork in context prediction has focused on ML model personalization where a\npersonalized model is learned for each individual user in order to tailor\npredictions or recommendations to a user's mobile behavior. While the use of\npersonalized models increases efficacy of the mobile service, we argue that it\nincreases privacy risk since a personalized model encodes contextual behavior\nunique to each user. To demonstrate these privacy risks, we present several\nattribute inference-based privacy attacks and show that such attacks can leak\nprivacy with up to 78% efficacy for top-3 predictions. We present Pelican, a\nprivacy-preserving personalization system for context-aware mobile services\nthat leverages both device and cloud resources to personalize ML models while\nminimizing the risk of privacy leakage for users. We evaluate Pelican using\nreal world traces for location-aware mobile services and show that Pelican can\nsubstantially reduce privacy leakage by up to 75%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 20:26:54 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 23:18:05 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Atrey", "Akanksha", ""], ["Shenoy", "Prashant", ""], ["Jensen", "David", ""]]}, {"id": "2101.05952", "submitter": "Beibei Zhang", "authors": "Beibei Zhang, Tian Xiang, Hongxuan Zhang, Te Li, Shiqiang Zhu, Jianjun\n  Gu", "title": "Dynamic DNN Decomposition for Lossless Synergistic Inference", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) sustain high performance in today's data\nprocessing applications. DNN inference is resource-intensive thus is difficult\nto fit into a mobile device. An alternative is to offload the DNN inference to\na cloud server. However, such an approach requires heavy raw data transmission\nbetween the mobile device and the cloud server, which is not suitable for\nmission-critical and privacy-sensitive applications such as autopilot. To solve\nthis problem, recent advances unleash DNN services using the edge computing\nparadigm. The existing approaches split a DNN into two parts and deploy the two\npartitions to computation nodes at two edge computing tiers. Nonetheless, these\nmethods overlook collaborative device-edge-cloud computation resources.\nBesides, previous algorithms demand the whole DNN re-partitioning to adapt to\ncomputation resource changes and network dynamics. Moreover, for\nresource-demanding convolutional layers, prior works do not give a parallel\nprocessing strategy without loss of accuracy at the edge side. To tackle these\nissues, we propose D3, a dynamic DNN decomposition system for synergistic\ninference without precision loss. The proposed system introduces a heuristic\nalgorithm named horizontal partition algorithm to split a DNN into three parts.\nThe algorithm can partially adjust the partitions at run time according to\nprocessing time and network conditions. At the edge side, a vertical separation\nmodule separates feature maps into tiles that can be independently run on\ndifferent edge nodes in parallel. Extensive quantitative evaluation of five\npopular DNNs illustrates that D3 outperforms the state-of-the-art counterparts\nup to 3.4 times in end-to-end DNN inference time and reduces backbone network\ncommunication overhead up to 3.68 times.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 03:18:53 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Zhang", "Beibei", ""], ["Xiang", "Tian", ""], ["Zhang", "Hongxuan", ""], ["Li", "Te", ""], ["Zhu", "Shiqiang", ""], ["Gu", "Jianjun", ""]]}, {"id": "2101.05961", "submitter": "Manish Shetty Molahalli", "authors": "Manish Shetty, Chetan Bansal, Sumit Kumar, Nikitha Rao, Nachiappan\n  Nagappan", "title": "SoftNER: Mining Knowledge Graphs From Cloud Incidents", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.05505", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The move from boxed products to services and the widespread adoption of cloud\ncomputing has had a huge impact on the software development life cycle and\nDevOps processes. Particularly, incident management has become critical for\ndeveloping and operating large-scale services. Prior work on incident\nmanagement has heavily focused on the challenges with incident triaging and\nde-duplication. In this work, we address the fundamental problem of structured\nknowledge extraction from service incidents. We have built SoftNER, a framework\nfor mining Knowledge Graphs from incident reports. First, we build a novel\nmulti-task learning based BiLSTM-CRF model which leverages not just the\nsemantic context but also the data-types for extracting factual information in\nthe form of named entities. Next, we present an approach to mine relations\nbetween the named entities for automatically constructing knowledge graphs. We\nhave deployed SoftNER at Microsoft, a major cloud service provider and have\nevaluated it on more than 2 months of cloud incidents. We show that the\nunsupervised machine learning pipeline has a high precision of 0.96. Our\nmulti-task learning based deep learning model also outperforms the\nstate-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER,\nwe are able to build accurate models for applications such as incident triaging\nand recommending entities based on their relevance to incident titles.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 04:15:26 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 08:35:29 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shetty", "Manish", ""], ["Bansal", "Chetan", ""], ["Kumar", "Sumit", ""], ["Rao", "Nikitha", ""], ["Nagappan", "Nachiappan", ""]]}, {"id": "2101.06000", "submitter": "Mahdi Zamani", "authors": "Rongjian Lan and Ganesha Upadhyaya and Stephen Tse and Mahdi Zamani", "title": "Horizon: A Gas-Efficient, Trustless Bridge for Cross-Chain Transactions", "comments": "14 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rise of digital currency systems that rely on blockchain to ensure\nledger security, the ability to perform cross-chain transactions is becoming a\ncrucial interoperability requirement. Such transactions allow not only funds to\nbe transferred from one blockchain to another (as done in atomic swaps), but\nalso a blockchain to verify the inclusion of any event on another blockchain.\nCross-chain bridges are protocols that allow on-chain exchange of\ncryptocurrencies, on-chain transfer of assets to sidechains, and cross-shard\nverification of events in sharded blockchains, many of which rely on Byzantine\nfault tolerance (BFT) for scalability. Unfortunately, existing bridge protocols\nthat can transfer funds from a BFT blockchain incur significant computation\noverhead on the destination blockchain, resulting in a high gas cost for smart\ncontract verification of events. In this paper, we propose Horizon, a\ngas-efficient, cross-chain bridge protocol to transfer assets from a BFT\nblockchain to another blockchain (e.g., Ethereum) that supports basic smart\ncontract execution.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:42:30 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Lan", "Rongjian", ""], ["Upadhyaya", "Ganesha", ""], ["Tse", "Stephen", ""], ["Zamani", "Mahdi", ""]]}, {"id": "2101.06056", "submitter": "Xu Chen", "authors": "Shuai Yu and Xiaowen Gong and Qian Shi and Xiaofei Wang and Xu Chen", "title": "EC-SAGINs: Edge Computing-enhanced Space-Air-Ground Integrated Networks\n  for Internet of Vehicles", "comments": "The paper is accepted by IEEE IoTJ, Jan. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing-enhanced Internet of Vehicles (EC-IoV) enables ubiquitous data\nprocessing and content sharing among vehicles and terrestrial edge computing\n(TEC) infrastructures (e.g., 5G base stations and roadside units) with little\nor no human intervention, plays a key role in the intelligent transportation\nsystems. However, EC-IoV is heavily dependent on the connections and\ninteractions between vehicles and TEC infrastructures, thus will break down in\nsome remote areas where TEC infrastructures are unavailable (e.g., desert,\nisolated islands and disaster-stricken areas). Driven by the ubiquitous\nconnections and global-area coverage, space-air-ground integrated networks\n(SAGINs) efficiently support seamless coverage and efficient resource\nmanagement, represent the next frontier for edge computing. In light of this,\nwe first review the state-of-the-art edge computing research for SAGINs in this\narticle. After discussing several existing orbital and aerial edge computing\narchitectures, we propose a framework of edge computing-enabled\nspace-air-ground integrated networks (EC-SAGINs) to support various IoV\nservices for the vehicles in remote areas. The main objective of the framework\nis to minimize the task completion time and satellite resource usage. To this\nend, a pre-classification scheme is presented to reduce the size of action\nspace, and a deep imitation learning (DIL) driven offloading and caching\nalgorithm is proposed to achieve real-time decision making. Simulation results\nshow the effectiveness of our proposed scheme. At last, we also discuss some\ntechnology challenges and future directions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:56:23 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yu", "Shuai", ""], ["Gong", "Xiaowen", ""], ["Shi", "Qian", ""], ["Wang", "Xiaofei", ""], ["Chen", "Xu", ""]]}, {"id": "2101.06139", "submitter": "Andreas Grammenos", "authors": "Andreas Grammenos, Themistoklis Charalambous, and Evangelia\n  Kalyvianaki", "title": "CPU Scheduling in Data Centers Using Asynchronous Finite-Time\n  Distributed Coordination Mechanisms", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an asynchronous iterative scheme which allows a set of\ninterconnected nodes to distributively reach an agreement to within a\npre-specified bound in a finite number of steps. While this scheme could be\nadopted in a wide variety of applications, we discuss it within the context of\ntask scheduling for data centers. In this context, the algorithm is guaranteed\nto approximately converge to the optimal scheduling plan, given the available\nresources, in a finite number of steps. Furthermore, being asynchronous, the\nproposed scheme is able to take in account the uncertainty that can be\nintroduced from straggler nodes or communication issues in the form of latency\nvariability while still converging to the target objective. In addition, by\nusing extensive empirical evaluation through simulations we show that the\nproposed method exhibits state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:34:25 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Grammenos", "Andreas", ""], ["Charalambous", "Themistoklis", ""], ["Kalyvianaki", "Evangelia", ""]]}, {"id": "2101.06171", "submitter": "Duc Thien Nguyen", "authors": "Duc Thien Nguyen, Shiau Hoong Lim, Laura Wynter and Desmond Cai", "title": "Probabilistic Inference for Learning from Untrusted Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning brings potential benefits of faster learning, better\nsolutions, and a greater propensity to transfer when heterogeneous data from\ndifferent parties increases diversity. However, because federated learning\ntasks tend to be large and complex, and training times non-negligible, it is\nimportant for the aggregation algorithm to be robust to non-IID data and\ncorrupted parties. This robustness relies on the ability to identify, and\nappropriately weight, incompatible parties. Recent work assumes that a\n\\textit{reference dataset} is available through which to perform the\nidentification. We consider settings where no such reference dataset is\navailable; rather, the quality and suitability of the parties needs to be\n\\textit{inferred}. We do so by bringing ideas from crowdsourced predictions and\ncollaborative filtering, where one must infer an unknown ground truth given\nproposals from participants with unknown quality. We propose novel federated\nlearning aggregation algorithms based on Bayesian inference that adapt to the\nquality of the parties. Empirically, we show that the algorithms outperform\nstandard and robust aggregation in federated learning on both synthetic and\nreal data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:30:06 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Nguyen", "Duc Thien", ""], ["Lim", "Shiau Hoong", ""], ["Wynter", "Laura", ""], ["Cai", "Desmond", ""]]}, {"id": "2101.06466", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Tam\\'as L\\'evai, Zhuojin Li, Marcos A. M. Vieira,\n  Ramesh Govindan, Barath Raghavan", "title": "Galleon: Reshaping the Square Peg of NFV", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software is often used for Network Functions (NFs) -- such as firewalls, NAT,\ndeep packet inspection, and encryption -- that are applied to traffic in the\nnetwork. The community has hoped that NFV would enable rapid development of new\nNFs and leverage commodity computing infrastructure. However, the challenge for\nresearchers and operators has been to align the square peg of high-speed packet\nprocessing with the round hole of cloud computing infrastructures and\nabstractions, all while delivering performance, scalability, and isolation.\nPast work has led to the belief that NFV is different enough that it requires\nnovel, custom approaches that deviate from today's norms. To the contrary, we\nshow that we can achieve performance, scalability, and isolation in NFV\njudiciously using mechanisms and abstractions of FaaS, the Linux kernel, NIC\nhardware, and OpenFlow switches. As such, with our system Galleon, NFV can be\npractically-deployable today in conventional cloud environments while\ndelivering up to double the performance per core compared to the state of the\nart.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:55:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Jianfeng", ""], ["L\u00e9vai", "Tam\u00e1s", ""], ["Li", "Zhuojin", ""], ["Vieira", "Marcos A. M.", ""], ["Govindan", "Ramesh", ""], ["Raghavan", "Barath", ""]]}, {"id": "2101.06485", "submitter": "Bohdan Trach", "authors": "Bohdan Trach (1), Rasha Faqeh (1), Oleksii Oleksenko (1), Wojciech\n  Ozga (1), Pramod Bhatotia (2), Christof Fetzer (1) ((1) TU Dresden, (2) TU\n  M\\\"unich)", "title": "T-Lease: A Trusted Lease Primitive for Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lease is an important primitive for building distributed protocols, and it\nis ubiquitously employed in distributed systems. However, the scope of the\nclassic lease abstraction is restricted to the trusted computing\ninfrastructure. Unfortunately, this important primitive cannot be employed in\nthe untrusted computing infrastructure because the trusted execution\nenvironments (TEEs) do not provide a trusted time source. In the untrusted\nenvironment, an adversary can easily manipulate the system clock to violate the\ncorrectness properties of lease-based systems. We tackle this problem by\nintroducing trusted lease -- a lease that maintains its correctness properties\neven in the presence of a clock-manipulating attacker. To achieve these\nproperties, we follow a \"trust but verify\" approach for an untrusted timer, and\ntransform it into a trusted timing primitive by leveraging two\nhardware-assisted ISA extensions (Intel TSX and SGX) available in commodity\nCPUs. We provide a design and implementation of trusted lease in a system\ncalled T-Lease -- the first trusted lease system that achieves high security,\nperformance, and precision. For the application developers, T-Lease exposes an\neasy-to-use generic APIs that facilitate its usage to build a wide range of\ndistributed protocols.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 17:23:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Trach", "Bohdan", ""], ["Faqeh", "Rasha", ""], ["Oleksenko", "Oleksii", ""], ["Ozga", "Wojciech", ""], ["Bhatotia", "Pramod", ""], ["Fetzer", "Christof", ""]]}, {"id": "2101.06524", "submitter": "Atousa Zarindast", "authors": "Atousa Zarindast, Anuj Sharma", "title": "Big Data application in congestion detection and classification using\n  Apache spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the era of big data, an explosive amount of information is now\navailable. This enormous increase of Big Data in both academia and industry\nrequires large-scale data processing systems. A large body of research is\nbehind optimizing Spark's performance to make it state of the art, a fast and\ngeneral data processing system. Many science and engineering fields have\nadvanced with Big Data analytics, such as Biology, finance, and transportation.\nIntelligent transportation systems (ITS) gain popularity and direct benefit\nfrom the richness of information. The objective is to improve the safety and\nmanagement of transportation networks by reducing congestion and incidents. The\nfirst step toward the goal is better understanding, modeling, and detecting\ncongestion across a network efficiently and effectively. In this study, we\nintroduce an efficient congestion detection model. The underlying network\nconsists of 3017 segments in I-35, I-80, I-29, and I-380 freeways with an\noverall length of 1570 miles and averaged (0.4-0.6) miles per segment. The\nresult of congestion detection shows the proposed method is 90% accurate while\nhas reduced computation time by 99.88%.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 21:26:11 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zarindast", "Atousa", ""], ["Sharma", "Anuj", ""]]}, {"id": "2101.06550", "submitter": "Andrew Gloster", "authors": "Andrew Gloster", "title": "GPU Methodologies for Numerical Partial Differential Equations", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we develop techniques to efficiently solve numerical Partial\nDifferential Equations (PDEs) using Graphical Processing Units (GPUs). Focus is\nput on both performance and re--usability of the methods developed, to this end\na library, cuSten, for applying finite--difference stencils to numerical grids\nis presented herein. On top of this various batched tridiagonal and\npentadiagonal matrix solvers are discussed. These have been benchmarked against\nthe current state of the art and shown to improve performance in the solution\nof numerical PDEs. A variety of other benchmarks and use cases for the GPU\nmethodologies are presented using the Cahn--Hilliard equation as a core\nexample, but it is emphasised the methods are completely general. Finally\nthrough the application of the GPU methodologies to the Cahn--Hilliard equation\nnew results are presented on the growth rates of the coarsened domains. In\nparticular a statistical model is built up using batches of simulations run on\nGPUs from which the growth rates are extracted, it is shown that in a finite\ndomain that the traditionally presented results of 1/3 scaling is in fact a\ndistribution around this value. This result is discussed in conjunction with\nmodelling via a stochastic PDE and sheds new light on the behaviour of the\nCahn--Hilliard equation in finite domains.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:24:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Gloster", "Andrew", ""]]}, {"id": "2101.06558", "submitter": "Rahul Paropkari", "authors": "Rahul Arun Paropkari, Anurag Thantharate, Cory Beard", "title": "Deep-Mobility: A Deep Learning Approach for an Efficient and Reliable 5G\n  Handover", "comments": "This paper was accepted at the 29th ICCCN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  5G cellular networks are being deployed all over the world and this\narchitecture supports ultra-dense network (UDN) deployment. Small cells have a\nvery important role in providing 5G connectivity to the end users. Exponential\nincreases in devices, data and network demands make it mandatory for the\nservice providers to manage handovers better, to cater to the services that a\nuser desire. In contrast to any traditional handover improvement scheme, we\ndevelop a 'Deep-Mobility' model by implementing a deep learning neural network\n(DLNN) to manage network mobility, utilizing in-network deep learning and\nprediction. We use network key performance indicators (KPIs) to train our model\nto analyze network traffic and handover requirements. In this method, RF signal\nconditions are continuously observed and tracked using deep learning neural\nnetworks such as the Recurrent neural network (RNN) or Long Short-Term Memory\nnetwork (LSTM) and system level inputs are also considered in conjunction, to\ntake a collective decision for a handover. We can study multiple parameters and\ninteractions between system events along with the user mobility, which would\nthen trigger a handoff in any given scenario. Here, we show the fundamental\nmodeling approach and demonstrate usefulness of our model while investigating\nimpacts and sensitivities of certain KPIs from the user equipment (UE) and\nnetwork side.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:31:37 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 01:19:11 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Paropkari", "Rahul Arun", ""], ["Thantharate", "Anurag", ""], ["Beard", "Cory", ""]]}, {"id": "2101.06582", "submitter": "Yiwen Han", "authors": "Yiwen Han and Shihao Shen and Xiaofei Wang and Shiqiang Wang and\n  Victor C.M. Leung", "title": "Tailored Learning-Based Scheduling for Kubernetes-Oriented Edge-Cloud\n  System", "comments": "IEEE INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kubernetes (k8s) has the potential to merge the distributed edge and the\ncloud but lacks a scheduling framework specifically for edge-cloud systems.\nBesides, the hierarchical distribution of heterogeneous resources and the\ncomplex dependencies among requests and resources make the modeling and\nscheduling of k8s-oriented edge-cloud systems particularly sophisticated. In\nthis paper, we introduce KaiS, a learning-based scheduling framework for such\nedge-cloud systems to improve the long-term throughput rate of request\nprocessing. First, we design a coordinated multi-agent actor-critic algorithm\nto cater to decentralized request dispatch and dynamic dispatch spaces within\nthe edge cluster. Second, for diverse system scales and structures, we use\ngraph neural networks to embed system state information, and combine the\nembedding results with multiple policy networks to reduce the orchestration\ndimensionality by stepwise scheduling. Finally, we adopt a two-time-scale\nscheduling mechanism to harmonize request dispatch and service orchestration,\nand present the implementation design of deploying the above algorithms\ncompatible with native k8s components. Experiments using real workload traces\nshow that KaiS can successfully learn appropriate scheduling policies,\nirrespective of request arrival patterns and system scales. Moreover, KaiS can\nenhance the average system throughput rate by 14.3% while reducing scheduling\ncost by 34.7% compared to baselines.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 03:45:25 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Han", "Yiwen", ""], ["Shen", "Shihao", ""], ["Wang", "Xiaofei", ""], ["Wang", "Shiqiang", ""], ["Leung", "Victor C. M.", ""]]}, {"id": "2101.06676", "submitter": "Mohammad Shojafar", "authors": "Ali Shahidinejad, Mostafa Ghobaei-Arani, Alireza Souri, Mohammad\n  Shojafar, Saru Kumari", "title": "A Technical Report for Light-Edge: A Lightweight Authentication Protocol\n  for IoT Devices in an Edge-Cloud Environment", "comments": "5 pages, 13 figures, 5 tables, technical report of IEEE Consumer\n  Electronics Magazine paper (Light-Edge)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Selected procedures in [1] and additional simulation results are presented in\ndetail in this report. We first present the IoT device registration in Section\nI, and we provide the details of fuzzy-based trust computation in Section II.\nIn the end, we show some additional simulation results for formal validation of\nthe Light-Edge under On-the-Fly Model Checker (OFMC) and Constraint-Logic-based\nATtack SEarcher (CLAtse) tools in Section III. See the original paper [1] for\nmore detail.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 14:00:08 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Shahidinejad", "Ali", ""], ["Ghobaei-Arani", "Mostafa", ""], ["Souri", "Alireza", ""], ["Shojafar", "Mohammad", ""], ["Kumari", "Saru", ""]]}, {"id": "2101.06682", "submitter": "Radoslava Hristova", "authors": "I. Hristov, R. Hristova, S. Dimova, P. Armyanov, N. Shegunov, I.\n  Puzynin, T. Puzynina, Z. Sharipov, Z. Tukhliev", "title": "On the efficient parallel computing of long term reliable trajectories\n  for the Lorenz system", "comments": "10 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2010.14993", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose an efficient parallelization of multiple-precision\nTaylor series method with variable stepsize and fixed order. For given level of\naccuracy the optimal variable stepsize determines higher order of the method\nthan in the case of optimal fixed stepsize. Although the used order of the\nmethod is greater then that in the case of fixed stepsize, and hence the\ncomputational work per step is greater, the reduced number of steps gives less\noverall work. Also the greater order of the method is beneficial in the sense\nthat it increases the parallel efficiency. As a model problem we use the\nparadigmatic Lorenz system. With 256 CPU cores in Nestum cluster, Sofia,\nBulgaria, we succeed to obtain a correct reference solution in the rather long\ntime interval - [0,11000]. To get this solution we performed two large\ncomputations: one computation with 4566 decimal digits of precision and 5240-th\norder method, and second computation for verification - with 4778 decimal\ndigits of precision and 5490-th order method.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 14:42:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hristov", "I.", ""], ["Hristova", "R.", ""], ["Dimova", "S.", ""], ["Armyanov", "P.", ""], ["Shegunov", "N.", ""], ["Puzynin", "I.", ""], ["Puzynina", "T.", ""], ["Sharipov", "Z.", ""], ["Tukhliev", "Z.", ""]]}, {"id": "2101.06737", "submitter": "Jamie Alnasir PhD", "authors": "Jamie J. Alnasir", "title": "Ten Simple Rules for Success with HPC, i.e. Responsibly BASHing that\n  Linux Cluster", "comments": "6 pages (5 excl. refs), 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  High-performance computing (HPC) clusters are widely used in-house at\nscientific and academic research institutions. For some users, the transition\nfrom running their analyses on a single workstation to running them on a\ncomplex, multi-tenanted cluster, usually employing some degree of parallelism,\ncan be challenging, if not bewildering, especially for users whose role is not\npredominantly computational in nature. On the other hand, there are more\nexperienced users, who can benefit from pointers on how to get the best from\ntheir use of HPC.\n  This Ten Simple Rules guide is aimed at helping you identify ways to improve\nyour utilisation of HPC, avoiding common pitfalls that can negatively impact\nother users and will also help ease the load (pun intended) on your HPC\nsysadmin. It is intended to provide technical advice common to the use of HPC\nplatforms such as LSF, Slurm, PBS/Torque, SGE, LoadLeveler and YARN, the\nscheduler used with Hadoop/Spark platform.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 18:12:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Alnasir", "Jamie J.", ""]]}, {"id": "2101.06758", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Catiuscia Melle, Italo Epicoco, Marco Pulimeno", "title": "Data stream fusion for accurate quantile tracking and analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UDDSKETCH is a recent algorithm for accurate tracking of quantiles in data\nstreams, derived from the DDSKETCH algorithm. UDDSKETCH provides accuracy\nguarantees covering the full range of quantiles independently of the input\ndistribution and greatly improves the accuracy with regard to DDSKETCH. In this\npaper we show how to compress and fuse data streams (or datasets) by using\nUDDSKETCH data summaries that are fused into a new summary related to the union\nof the streams (or datasets) processed by the input summaries whilst preserving\nboth the error and size guarantees provided by UDDSKETCH. This property of\nsketches, known as mergeability, enables parallel and distributed processing.\nWe prove that UDDSKETCH is fully mergeable and introduce a parallel version of\nUDDSKETCH suitable for message-passing based architectures. We formally prove\nits correctness and compare it to a parallel version of DDSKETCH, showing\nthrough extensive experimental results that our parallel algorithm almost\nalways outperforms the parallel DDSKETCH algorithm with regard to the overall\naccuracy in determining the quantiles.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:30:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Cafaro", "Massimo", ""], ["Melle", "Catiuscia", ""], ["Epicoco", "Italo", ""], ["Pulimeno", "Marco", ""]]}, {"id": "2101.06781", "submitter": "Mudabbir Kaleem", "authors": "Mudabbir Kaleem, Weidong Shi", "title": "Demystifying Pythia: A Survey of ChainLink Oracles Usage on Ethereum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts are dependent on oracle systems for their adoption and\nusability. We perform an empirical study of oracle systems' usage trends and\nadoption metrics to provide better insight into the health of the smart\ncontract ecosystem. We collect ChainLink usage data on the Ethereum network\nusing a modified Ethereum client and running a full node. We analyze the\ncollected data and present our findings and insights surrounding the usage\ntrends, adoption metrics, oracle pricing and service quality associated with\nChainLink on the Ethereum network.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:51:29 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 02:29:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kaleem", "Mudabbir", ""], ["Shi", "Weidong", ""]]}, {"id": "2101.06840", "submitter": "Shuangyan Yang", "authors": "Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,\n  Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale model training has been a playing ground for a limited few\nrequiring complex model refactoring and access to prohibitively expensive GPU\nclusters. ZeRO-Offload changes the large model training landscape by making\nlarge model training accessible to nearly everyone. It can train models with\nover 13 billion parameters on a single GPU, a 10x increase in size compared to\npopular framework such as PyTorch, and it does so without requiring any model\nchange from the data scientists or sacrificing computational efficiency.\nZeRO-Offload enables large model training by offloading data and compute to\nCPU. To preserve compute efficiency, it is designed to minimize the data\nmovement to/from GPU, and reduce CPU compute time while maximizing memory\nsavings on GPU. As a result, ZeRO-Offload can achieve 40 TFlops/GPU on a single\nNVIDIA V100 GPU for 10B parameter model compared to 30TF using PyTorch alone\nfor a 1.4B parameter model, the largest that can be trained without running out\nof memory. ZeRO-Offload is also designed to scale on multiple-GPUs when\navailable, offering near linear speedup on up to 128 GPUs. Additionally, it can\nwork together with model parallelism to train models with over 70 billion\nparameters on a single DGX-2 box, a 4.5x increase in model size compared to\nusing model parallelism alone. By combining compute and memory efficiency with\nease-of-use, ZeRO-Offload democratizes large-scale model training making it\naccessible to even data scientists with access to just a single GPU.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:11:25 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ren", "Jie", ""], ["Rajbhandari", "Samyam", ""], ["Aminabadi", "Reza Yazdani", ""], ["Ruwase", "Olatunji", ""], ["Yang", "Shuangyan", ""], ["Zhang", "Minjia", ""], ["Li", "Dong", ""], ["He", "Yuxiong", ""]]}, {"id": "2101.06911", "submitter": "Jiping Yu", "authors": "Jiping Yu, Wei Qin, Xiaowei Zhu, Zhenbo Sun, Jianqiang Huang, Xiaohan\n  Li, Wenguang Chen", "title": "DFOGraph: An I/O- and Communication-Efficient System for Distributed\n  Fully-out-of-Core Graph Processing", "comments": "12 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the magnitude of graph-structured data continually increasing, graph\nprocessing systems that can scale-out and scale-up are needed to handle\nextreme-scale datasets. While existing distributed out-of-core solutions have\nmade it possible, they suffer from limited performance due to excessive I/O and\ncommunication costs. We present DFOGraph, a distributed fully-out-of-core graph\nprocessing system that applies and assembles multiple techniques to enable I/O-\nand communication-efficient processing. DFOGraph builds upon two-level\ncolumn-oriented partition with adaptive compressed representations to allow\nfine-grained selective computation and communication, and it only issues\nnecessary disk and network requests. Our evaluation shows DFOGraph achieves\nperformance comparable to GridGraph and FlashGraph (>2.52x and 1.06x) on a\nsingle machine and outperforms Chaos and HybridGraph significantly (>12.94x and\n>10.82x) when scaling out.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 07:31:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yu", "Jiping", ""], ["Qin", "Wei", ""], ["Zhu", "Xiaowei", ""], ["Sun", "Zhenbo", ""], ["Huang", "Jianqiang", ""], ["Li", "Xiaohan", ""], ["Chen", "Wenguang", ""]]}, {"id": "2101.06966", "submitter": "Sebastien Tixeuil", "authors": "Thibaut Balabonski (VALS - LRI), Pierre Courtieu (CEDRIC - SYS), Robin\n  Pelle (VALS - LRI), Lionel Rieg (VERIMAG - IMAG), S\\'ebastien Tixeuil (NPA,\n  LINCS), Xavier Urbain (DRIM)", "title": "Computer Aided Formal Design of Swarm Robotics Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on formally studying mobile robotic swarms consider necessary\nand sufficient system hypotheses enabling to solve theoretical benchmark\nproblems (geometric pattern formation, gathering, scattering, etc.). We argue\nthat formal methods can also help in the early stage of mobile robotic swarms\nprotocol design, to obtain protocols that are correct-by-design, even for\nproblems arising from real-world use cases, not previously studied\ntheoretically. Our position is supported by a concrete case study. Starting\nfrom a real-world case scenario, we jointly design the formal problem\nspecification, a family of protocols that are able to solve the problem, and\ntheir corresponding proof of correctness, all expressed with the same formal\nframework. The concrete framework we use for our development is the PACTOLE\nlibrary based on the COQ proof assistant.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:12:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Balabonski", "Thibaut", "", "VALS - LRI"], ["Courtieu", "Pierre", "", "CEDRIC - SYS"], ["Pelle", "Robin", "", "VALS - LRI"], ["Rieg", "Lionel", "", "VERIMAG - IMAG"], ["Tixeuil", "S\u00e9bastien", "", "NPA,\n  LINCS"], ["Urbain", "Xavier", "", "DRIM"]]}, {"id": "2101.07026", "submitter": "Masatoshi Hanai", "authors": "Masatoshi Hanai, Nikos Tziritas, Toyotaro Suzumura, Wentong Cai,\n  Georgios Theodoropoulos", "title": "Time-Efficient and High-Quality Graph Partitioning for Graph Dynamic\n  Scaling", "comments": "21 pages, 15 figures. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic scaling of distributed computations plays an important role in\nthe utilization of elastic computational resources, such as the cloud. It\nenables the provisioning and de-provisioning of resources to match dynamic\nresource availability and demands. In the case of distributed graph processing,\nchanging the number of the graph partitions while maintaining high partitioning\nquality imposes serious computational overheads as typically a time-consuming\ngraph partitioning algorithm needs to execute each time repartitioning is\nrequired. In this paper, we propose a dynamic scaling method that can\nefficiently change the number of graph partitions while keeping its quality\nhigh. Our idea is based on two techniques: preprocessing and very fast edge\npartitioning, called graph edge ordering and chunk-based edge partitioning,\nrespectively. The former converts the graph data into an ordered edge list in\nsuch a way that edges with high locality are closer to each other. The latter\nimmediately divides the ordered edge list into an arbitrary number of\nhigh-quality partitions. The evaluation with the real-world billion-scale\ngraphs demonstrates that our proposed approach significantly reduces the\nrepartitioning time, while the partitioning quality it achieves is on par with\nthat of the best existing static method.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:06:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hanai", "Masatoshi", ""], ["Tziritas", "Nikos", ""], ["Suzumura", "Toyotaro", ""], ["Cai", "Wentong", ""], ["Theodoropoulos", "Georgios", ""]]}, {"id": "2101.07050", "submitter": "Ahmed Eleliemy", "authors": "Ahmed Eleliemy and Florina M. Ciorba", "title": "A Distributed Chunk Calculation Approach for Self-scheduling of Parallel\n  Applications on Distributed-memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop scheduling techniques aim to achieve load-balanced executions of\nscientific applications. Dynamic loop self-scheduling (DLS) libraries for\ndistributed-memory systems are typically MPI-based and employ a centralized\nchunk calculation approach (CCA) to assign variably-sized chunks of loop\niterations. We present a distributed chunk calculation approach (DCA) that\nsupports various types of DLS techniques. Using both CCA and DCA, twelve DLS\ntechniques are implemented and evaluated in different CPU slowdown scenarios.\nThe results show that the DLS techniques implemented using DCA outperform their\ncorresponding ones implemented with CCA, especially in extreme system slowdown\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:54:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Eleliemy", "Ahmed", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "2101.07095", "submitter": "Andrew Lewis-Pye", "authors": "Andrew Lewis-Pye, Tim Roughgarden", "title": "Byzantine Generals in the Permissionless Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consensus protocols have traditionally been studied in a setting where all\nparticipants are known to each other from the start of the protocol execution.\nIn the parlance of the 'blockchain' literature, this is referred to as the\npermissioned setting. What differentiates Bitcoin from these previously studied\nprotocols is that it operates in a permissionless setting, i.e. it is a\nprotocol for establishing consensus over an unknown network of participants\nthat anybody can join, with as many identities as they like in any role. The\narrival of this new form of protocol brings with it many questions. Beyond\nBitcoin, what can we prove about permissionless protocols in a general sense?\nHow does recent work on permissionless protocols in the blockchain literature\nrelate to the well-developed history of research on permissioned protocols in\ndistributed computing?\n  To answer these questions, we describe a formal framework for the analysis of\nboth permissioned and permissionless systems. Our framework allows for\n\"apples-to-apples\" comparisons between different categories of protocols and,\nin turn, the development of theory to formally discuss their relative merits. A\nmajor benefit of the framework is that it facilitates the application of a rich\nhistory of proofs and techniques in distributed computing to problems in\nblockchain and the study of permissionless systems. Within our framework, we\nthen address the questions above. We consider the Byzantine Generals Problem as\na formalisation of the problem of reaching consensus, and address a programme\nof research that asks, \"Under what adversarial conditions, and for what types\nof permissionless protocol, is consensus possible?\" We prove a number of\nresults for this programme, our main result being that deterministic consensus\nis not possible for decentralised permissionless protocols. To close, we give a\nlist of eight open questions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 14:36:36 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 12:49:22 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 14:54:43 GMT"}, {"version": "v4", "created": "Thu, 11 Feb 2021 09:09:51 GMT"}, {"version": "v5", "created": "Fri, 12 Feb 2021 09:35:41 GMT"}, {"version": "v6", "created": "Mon, 15 Feb 2021 09:06:37 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lewis-Pye", "Andrew", ""], ["Roughgarden", "Tim", ""]]}, {"id": "2101.07200", "submitter": "Thaleia Dimitra Doudali", "authors": "Thaleia Dimitra Doudali, Daniel Zahka and Ada Gavrilovska", "title": "Tuning the Frequency of Periodic Data Movements over Hybrid Memory\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging hybrid memory systems that comprise technologies such as Intel's\nOptane DC Persistent Memory, exhibit disparities in the access speeds and\ncapacity ratios of their heterogeneous memory components. This breaks many\nassumptions and heuristics designed for traditional DRAM-only platforms. High\napplication performance is feasible via dynamic data movement across memory\nunits, which maximizes the capacity use of DRAM while ensuring efficient use of\nthe aggregate system resources. Newly proposed solutions use performance models\nand machine intelligence to optimize which and how much data to move\ndynamically; however, the decision of when to move this data is based on\nempirical selection of time intervals, or left to the applications. Our\nexperimental evaluation shows that failure to properly configure the data\nmovement frequency can lead to 10%-100% slowdown for a given data movement\npolicy; yet, there is no established methodology on how to properly configure\nthis value for a given workload, platform and policy. We propose Cori, a\nsystem-level tuning solution that identifies and extracts the necessary\napplication-level data reuse information, and guides the selection of data\nmovement frequency to deliver gains in application performance and system\nresource efficiency. Experimental evaluation shows that Cori configures data\nmovement frequencies that provide application performance within 3% of the\noptimal one, and that it can achieve this up to 5x more quickly than random or\nbrute-force approaches. System-level validation of Cori on a platform with DRAM\nand Intel's Optane DC PMEM confirms its practicality and tuning efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 01:30:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Doudali", "Thaleia Dimitra", ""], ["Zahka", "Daniel", ""], ["Gavrilovska", "Ada", ""]]}, {"id": "2101.07235", "submitter": "Jean-Francois Rajotte", "authors": "Jean-Francois Rajotte, Sumit Mukherjee, Caleb Robinson, Anthony Ortiz,\n  Christopher West, Juan Lavista Ferres, Raymond T Ng", "title": "Reducing bias and increasing utility by federated generative modeling of\n  medical images using a centralized adversary", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce FELICIA (FEderated LearnIng with a CentralIzed Adversary) a\ngenerative mechanism enabling collaborative learning. In particular, we show\nhow a data owner with limited and biased data could benefit from other data\nowners while keeping data from all the sources private. This is a common\nscenario in medical image analysis where privacy legislation prevents data from\nbeing shared outside local premises. FELICIA works for a large family of\nGenerative Adversarial Networks (GAN) architectures including vanilla and\nconditional GANs as demonstrated in this work. We show that by using the\nFELICIA mechanism, a data owner with limited image samples can generate\nhigh-quality synthetic images with high utility while neither data owners has\nto provide access to its data. The sharing happens solely through a central\ndiscriminator that has access limited to synthetic data. Here, utility is\ndefined as classification performance on a real test set. We demonstrate these\nbenefits on several realistic healthcare scenarions using benchmark image\ndatasets (MNIST, CIFAR-10) as well as on medical images for the task of skin\nlesion classification. With multiple experiments, we show that even in the\nworst cases, combining FELICIA with real data gracefully achieves performance\non par with real data while most results significantly improves the utility.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:40:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rajotte", "Jean-Francois", ""], ["Mukherjee", "Sumit", ""], ["Robinson", "Caleb", ""], ["Ortiz", "Anthony", ""], ["West", "Christopher", ""], ["Ferres", "Juan Lavista", ""], ["Ng", "Raymond T", ""]]}, {"id": "2101.07344", "submitter": "Arjun Balasubramanian", "authors": "Arjun Balasubramanian, Adarsh Kumar, Yuhan Liu, Han Cao, Shivaram\n  Venkataraman, Aditya Akella", "title": "Accelerating Deep Learning Inference via Learned Caches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are witnessing increased adoption in multiple\ndomains owing to their high accuracy in solving real-world problems. However,\nthis high accuracy has been achieved by building deeper networks, posing a\nfundamental challenge to the low latency inference desired by user-facing\napplications. Current low latency solutions trade-off on accuracy or fail to\nexploit the inherent temporal locality in prediction serving workloads.\n  We observe that caching hidden layer outputs of the DNN can introduce a form\nof late-binding where inference requests only consume the amount of computation\nneeded. This enables a mechanism for achieving low latencies, coupled with an\nability to exploit temporal locality. However, traditional caching approaches\nincur high memory overheads and lookup latencies, leading us to design learned\ncaches - caches that consist of simple ML models that are continuously updated.\nWe present the design of GATI, an end-to-end prediction serving system that\nincorporates learned caches for low-latency DNN inference. Results show that\nGATI can reduce inference latency by up to 7.69X on realistic workloads.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:13:08 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Balasubramanian", "Arjun", ""], ["Kumar", "Adarsh", ""], ["Liu", "Yuhan", ""], ["Cao", "Han", ""], ["Venkataraman", "Shivaram", ""], ["Akella", "Aditya", ""]]}, {"id": "2101.07511", "submitter": "Adnan Qayyum", "authors": "Adnan Qayyum, Kashif Ahmad, Muhammad Ahtazaz Ahsan, Ala Al-Fuqaha, and\n  Junaid Qadir", "title": "Collaborative Federated Learning For Healthcare: Multi-Modal COVID-19\n  Diagnosis at the Edge", "comments": "preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite significant improvements over the last few years, cloud-based\nhealthcare applications continue to suffer from poor adoption due to their\nlimitations in meeting stringent security, privacy, and quality of service\nrequirements (such as low latency). The edge computing trend, along with\ntechniques for distributed machine learning such as federated learning, have\ngained popularity as a viable solution in such settings. In this paper, we\nleverage the capabilities of edge computing in medicine by analyzing and\nevaluating the potential of intelligent processing of clinical visual data at\nthe edge allowing the remote healthcare centers, lacking advanced diagnostic\nfacilities, to benefit from the multi-modal data securely. To this aim, we\nutilize the emerging concept of clustered federated learning (CFL) for an\nautomatic diagnosis of COVID-19. Such an automated system can help reduce the\nburden on healthcare systems across the world that has been under a lot of\nstress since the COVID-19 pandemic emerged in late 2019. We evaluate the\nperformance of the proposed framework under different experimental setups on\ntwo benchmark datasets. Promising results are obtained on both datasets\nresulting in comparable results against the central baseline where the\nspecialized models (i.e., each on a specific type of COVID-19 imagery) are\ntrained with central data, and improvements of 16\\% and 11\\% in overall\nF1-Scores have been achieved over the multi-modal model trained in the\nconventional Federated Learning setup on X-ray and Ultrasound datasets,\nrespectively. We also discuss in detail the associated challenges,\ntechnologies, tools, and techniques available for deploying ML at the edge in\nsuch privacy and delay-sensitive applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 08:40:59 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Qayyum", "Adnan", ""], ["Ahmad", "Kashif", ""], ["Ahsan", "Muhammad Ahtazaz", ""], ["Al-Fuqaha", "Ala", ""], ["Qadir", "Junaid", ""]]}, {"id": "2101.07557", "submitter": "Christina Giannoula", "authors": "Christina Giannoula, Nandita Vijaykumar, Nikela Papadopoulou,\n  Vasileios Karakostas, Ivan Fernandez, Juan G\\'omez-Luna, Lois Orosa,\n  Nectarios Koziris, Georgios Goumas, Onur Mutlu", "title": "SynCron: Efficient Synchronization Support for Near-Data-Processing\n  Architectures", "comments": "To appear in the 27th IEEE International Symposium on\n  High-Performance Computer Architecture (HPCA-27)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-Data-Processing (NDP) architectures present a promising way to alleviate\ndata movement costs and can provide significant performance and energy benefits\nto parallel applications. Typically, NDP architectures support several NDP\nunits, each including multiple simple cores placed close to memory. To fully\nleverage the benefits of NDP and achieve high performance for parallel\nworkloads, efficient synchronization among the NDP cores of a system is\nnecessary. However, supporting synchronization in many NDP systems is\nchallenging because they lack shared caches and hardware cache coherence\nsupport, which are commonly used for synchronization in multicore systems, and\ncommunication across different NDP units can be expensive.\n  This paper comprehensively examines the synchronization problem in NDP\nsystems, and proposes SynCron, an end-to-end synchronization solution for NDP\nsystems. SynCron adds low-cost hardware support near memory for synchronization\nacceleration, and avoids the need for hardware cache coherence support. SynCron\nhas three components: 1) a specialized cache memory structure to avoid memory\naccesses for synchronization and minimize latency overheads, 2) a hierarchical\nmessage-passing communication protocol to minimize expensive communication\nacross NDP units of the system, and 3) a hardware-only overflow management\nscheme to avoid performance degradation when hardware resources for\nsynchronization tracking are exceeded.\n  We evaluate SynCron using a variety of parallel workloads, covering various\ncontention scenarios. SynCron improves performance by 1.27$\\times$ on average\n(up to 1.78$\\times$) under high-contention scenarios, and by 1.35$\\times$ on\naverage (up to 2.29$\\times$) under low-contention real applications, compared\nto state-of-the-art approaches. SynCron reduces system energy consumption by\n2.08$\\times$ on average (up to 4.25$\\times$).\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:48:58 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 00:09:20 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 11:47:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Giannoula", "Christina", ""], ["Vijaykumar", "Nandita", ""], ["Papadopoulou", "Nikela", ""], ["Karakostas", "Vasileios", ""], ["Fernandez", "Ivan", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Orosa", "Lois", ""], ["Koziris", "Nectarios", ""], ["Goumas", "Georgios", ""], ["Mutlu", "Onur", ""]]}, {"id": "2101.07590", "submitter": "Orr Fischer", "authors": "Keren Censor-Hillel, Orr Fischer, Tzlil Gonen, Fran\\c{c}ois Le Gall,\n  Dean Leitersdorf, Rotem Oshman", "title": "Fast Distributed Algorithms for Girth, Cycles and Small Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we give fast distributed graph algorithms for detecting and\nlisting small subgraphs, and for computing or approximating the girth. Our\nalgorithms improve upon the state of the art by polynomial factors, and for\ngirth, we obtain an constant-time algorithm for additive +1 approximation in\nthe Congested Clique, and the first parametrized algorithm for exact\ncomputation in CONGEST.\n  In the Congested Clique, we develop a technique for learning small\nneighborhoods, and apply it to obtain an $O(1)$-round algorithm that computes\nthe girth with only an additive +1 error. Next, we introduce a new technique\n(the partition tree technique) allowing for efficiently and deterministically\nlisting all copies of any subgraph, improving upon the state-of the-art for\nnon-dense graphs. We give two applications of this technique: First we show\nthat for constant $k$, $C_{2k}$-detection can be solved in $O(1)$ rounds in the\nCongested Clique, improving on prior work which used matrix multiplication and\nhad polynomial round complexity. Second, we show that in triangle-free graphs,\nthe girth can be exactly computed in time polynomially faster than the best\nknown bounds for general graphs.\n  In CONGEST, we describe a new approach for finding cycles, and apply it in\ntwo ways: first we show a fast parametrized algorithm for girth with round\ncomplexity $\\tilde{O}(\\min(g\\cdot n^{1-1/\\Theta(g)},n))$ for any girth $g$; and\nsecond, we show how to find small even-length cycles $C_{2k}$ for $k = 3,4,5$\nin $O(n^{1-1/k})$ rounds, which is a polynomial improvement upon the previous\nrunning times.\n  Finally, using our improved $C_6$-freeness algorithm and the barrier on\nproving lower bounds on triangle-freeness of Eden et al., we show that\nimproving the current $\\tilde\\Omega(\\sqrt{n})$ lower bound for $C_6$-freeness\nof Korhonen et al. by any polynomial factor would imply strong circuit\ncomplexity lower bounds.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:29:24 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Fischer", "Orr", ""], ["Gonen", "Tzlil", ""], ["Gall", "Fran\u00e7ois Le", ""], ["Leitersdorf", "Dean", ""], ["Oshman", "Rotem", ""]]}, {"id": "2101.07968", "submitter": "Shangming Cai", "authors": "Shangming Cai, Dongsheng Wang, Haixia Wang, Yongqiang Lyu, Guangquan\n  Xu, Xi Zheng and Athanasios V. Vasilakos", "title": "DynaComm: Accelerating Distributed CNN Training between Edges and Clouds\n  through Dynamic Communication Scheduling", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce uploading bandwidth and address privacy concerns, deep learning at\nthe network edge has been an emerging topic. Typically, edge devices\ncollaboratively train a shared model using real-time generated data through the\nParameter Server framework. Although all the edge devices can share the\ncomputing workloads, the distributed training processes over edge networks are\nstill time-consuming due to the parameters and gradients transmission\nprocedures between parameter servers and edge devices. Focusing on accelerating\ndistributed Convolutional Neural Networks (CNNs) training at the network edge,\nwe present DynaComm, a novel scheduler that dynamically decomposes each\ntransmission procedure into several segments to achieve optimal communications\nand computations overlapping during run-time. Through experiments, we verify\nthat DynaComm manages to achieve optimal scheduling for all cases compared to\ncompeting strategies while the model accuracy remains untouched.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:09:41 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Cai", "Shangming", ""], ["Wang", "Dongsheng", ""], ["Wang", "Haixia", ""], ["Lyu", "Yongqiang", ""], ["Xu", "Guangquan", ""], ["Zheng", "Xi", ""], ["Vasilakos", "Athanasios V.", ""]]}, {"id": "2101.08062", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Donghyun Kang, and Young Ik Eom", "title": "Thread Evolution Kit for Optimizing Thread Operations on CE/IoT Devices", "comments": null, "journal-ref": null, "doi": "10.1109/TCE.2020.3033328", "report-no": null, "categories": "cs.OS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern operating systems have adopted the one-to-one thread model to\nsupport fast execution of threads in both multi-core and single-core systems.\nThis thread model, which maps the kernel-space and user-space threads in a\none-to-one manner, supports quick thread creation and termination in\nhigh-performance server environments. However, the performance of time-critical\nthreads is degraded when multiple threads are being run in low-end CE devices\nwith limited system resources. When a CE device runs many threads to support\ndiverse application functionalities, low-level hardware specifications often\nlead to significant resource contention among the threads trying to obtain\nsystem resources. As a result, the operating system encounters challenges, such\nas excessive thread context switching overhead, execution delay of\ntime-critical threads, and a lack of virtual memory for thread stacks. This\npaper proposes a state-of-the-art Thread Evolution Kit (TEK) that consists of\nthree primary components: a CPU Mediator, Stack Tuner, and Enhanced Thread\nIdentifier. From the experiment, we can see that the proposed scheme\nsignificantly improves user responsiveness (7x faster) under high CPU\ncontention compared to the traditional thread model. Also, TEK solves the\nsegmentation fault problem that frequently occurs when a CE application\nincreases the number of threads during its execution.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:54:59 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Lim", "Geunsik", ""], ["Kang", "Donghyun", ""], ["Eom", "Young Ik", ""]]}, {"id": "2101.08167", "submitter": "Khaled Zaouk", "authors": "Khaled Zaouk, Fei Song, Chenghao Lyu and Yanlei Diao", "title": "Neural-based Modeling for Performance Tuning of Spark Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud data analytics has become an integral part of enterprise business\noperations for data-driven insight discovery. Performance modeling of cloud\ndata analytics is crucial for performance tuning and other critical operations\nin the cloud. Traditional modeling techniques fail to adapt to the high degree\nof diversity in workloads and system behaviors in this domain. In this paper,\nwe bring recent Deep Learning techniques to bear on the process of automated\nperformance modeling of cloud data analytics, with a focus on Spark data\nanalytics as representative workloads. At the core of our work is the notion of\nlearning workload embeddings (with a set of desired properties) to represent\nfundamental computational characteristics of different jobs, which enable\nperformance prediction when used together with job configurations that control\nresource allocation and other system knobs. Our work provides an in-depth study\nof different modeling choices that suit our requirements. Results of extensive\nexperiments reveal the strengths and limitations of different modeling methods,\nas well as superior performance of our best performing method over a\nstate-of-the-art modeling tool for cloud analytics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:58:55 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zaouk", "Khaled", ""], ["Song", "Fei", ""], ["Lyu", "Chenghao", ""], ["Diao", "Yanlei", ""]]}, {"id": "2101.08204", "submitter": "Do Le Quoc", "authors": "Do Le Quoc, Franz Gregor, Sergei Arnautov, Roland Kunkel, Pramod\n  Bhatotia, Christof Fetzer", "title": "secureTF: A Secure TensorFlow Framework", "comments": "arXiv admin note: text overlap with arXiv:1902.04413", "journal-ref": "Pages 44-59, 2020", "doi": "10.1145/3423211.3425687", "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven intelligent applications in modern online services have become\nubiquitous. These applications are usually hosted in the untrusted cloud\ncomputing infrastructure. This poses significant security risks since these\napplications rely on applying machine learning algorithms on large datasets\nwhich may contain private and sensitive information.\n  To tackle this challenge, we designed secureTF, a distributed secure machine\nlearning framework based on Tensorflow for the untrusted cloud infrastructure.\nsecureTF is a generic platform to support unmodified TensorFlow applications,\nwhile providing end-to-end security for the input data, ML model, and\napplication code. secureTF is built from ground-up based on the security\nproperties provided by Trusted Execution Environments (TEEs). However, it\nextends the trust of a volatile memory region (or secure enclave) provided by\nthe single node TEE to secure a distributed infrastructure required for\nsupporting unmodified stateful machine learning applications running in the\ncloud.\n  The paper reports on our experiences about the system design choices and the\nsystem deployment in production use-cases. We conclude with the lessons learned\nbased on the limitations of our commercially available platform, and discuss\nopen research problems for the future work.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:36:53 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Quoc", "Do Le", ""], ["Gregor", "Franz", ""], ["Arnautov", "Sergei", ""], ["Kunkel", "Roland", ""], ["Bhatotia", "Pramod", ""], ["Fetzer", "Christof", ""]]}, {"id": "2101.08212", "submitter": "Kaushik Ayinala", "authors": "Kaushik Ayinala, Baek-Young Choi, Sejun Song", "title": "PiChu: Accelerating Block Broadcasting in Blockchain Networks with\n  Pipelining and Chunking", "comments": "2020 IEEE International Conference on Blockchain", "journal-ref": null, "doi": "10.1109/Blockchain50366.2020.00035", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Blockchain technologies have been rapidly enhanced in recent years. However,\nits scalability still has limitations in terms of throughput and broadcast\ndelay as the network and the amount of transaction data increase. To improve\nscalability of blockchain networks, we propose a novel approach named PiChu\nthat accelerates block propagation in blockchain networks by pipelining and\nverifying chunks of a block in parallel. Accelerating block propagation reduces\nthe mining interval and chance of fork occurring, which in turn increases\nthroughput. Our approach can be applied to the blockchain networks either\ndirectly or with a minor modification to the consensus. Through an extensive\nand large scale simulations, we validate that the proposed PiChu scheme\nsignificantly enhances the scalability of blockchain networks. For instance, a\n64 MB block can be broadcasted in just 80 seconds in a blockchain network with\na million nodes. The efficiency of PiChu broadcasting increases with bigger\nblock sizes and a larger number of nodes in the network.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:47:43 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ayinala", "Kaushik", ""], ["Choi", "Baek-Young", ""], ["Song", "Sejun", ""]]}, {"id": "2101.08358", "submitter": "Jason Mohoney", "authors": "Jason Mohoney, Roger Waleffe, Yiheng Xu, Theodoros Rekatsinas,\n  Shivaram Venkataraman", "title": "Marius: Learning Massive Graph Embeddings on a Single Machine", "comments": "Accepted into OSDI '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for computing the embeddings of large-scale graphs\non a single machine. A graph embedding is a fixed length vector representation\nfor each node (and/or edge-type) in a graph and has emerged as the de-facto\napproach to apply modern machine learning on graphs. We identify that current\nsystems for learning the embeddings of large-scale graphs are bottlenecked by\ndata movement, which results in poor resource utilization and inefficient\ntraining. These limitations require state-of-the-art systems to distribute\ntraining across multiple machines. We propose Marius, a system for efficient\ntraining of graph embeddings that leverages partition caching and buffer-aware\ndata orderings to minimize disk access and interleaves data movement with\ncomputation to maximize utilization. We compare Marius against two\nstate-of-the-art industrial systems on a diverse array of benchmarks. We\ndemonstrate that Marius achieves the same level of accuracy but is up to one\norder of magnitude faster. We also show that Marius can scale training to\ndatasets an order of magnitude beyond a single machine's GPU and CPU memory\ncapacity, enabling training of configurations with more than a billion edges\nand 550 GB of total parameters on a single machine with 16 GB of GPU memory and\n64 GB of CPU memory. Marius is open-sourced at www.marius-project.org.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:17:31 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 00:22:46 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Mohoney", "Jason", ""], ["Waleffe", "Roger", ""], ["Xu", "Yiheng", ""], ["Rekatsinas", "Theodoros", ""], ["Venkataraman", "Shivaram", ""]]}, {"id": "2101.08391", "submitter": "Xu Chen", "authors": "Qiong Wu and Xu Chen and Zhi Zhou and Liang Chen and Junshan Zhang", "title": "Deep Reinforcement Learning with Spatio-temporal Traffic Forecasting for\n  Data-Driven Base Station Sleep Control", "comments": "Accepted by IEEE/ACM Transactions on Networking, Jan. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the ever increasing mobile traffic demand in 5G era, base stations\n(BSs) have been densely deployed in radio access networks (RANs) to increase\nthe network coverage and capacity. However, as the high density of BSs is\ndesigned to accommodate peak traffic, it would consume an unnecessarily large\namount of energy if BSs are on during off-peak time. To save the energy\nconsumption of cellular networks, an effective way is to deactivate some idle\nbase stations that do not serve any traffic demand. In this paper, we develop a\ntraffic-aware dynamic BS sleep control framework, named DeepBSC, which presents\na novel data-driven learning approach to determine the BS active/sleep modes\nwhile meeting lower energy consumption and satisfactory Quality of Service\n(QoS) requirements. Specifically, the traffic demands are predicted by the\nproposed GS-STN model, which leverages the geographical and semantic\nspatial-temporal correlations of mobile traffic. With accurate mobile traffic\nforecasting, the BS sleep control problem is cast as a Markov Decision Process\nthat is solved by Actor-Critic reinforcement learning methods. To reduce the\nvariance of cost estimation in the dynamic environment, we propose a benchmark\ntransformation method that provides robust performance indicator for policy\nupdate. To expedite the training process, we adopt a Deep Deterministic Policy\nGradient (DDPG) approach, together with an explorer network, which can\nstrengthen the exploration further. Extensive experiments with a real-world\ndataset corroborate that our proposed framework significantly outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:39:42 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Wu", "Qiong", ""], ["Chen", "Xu", ""], ["Zhou", "Zhi", ""], ["Chen", "Liang", ""], ["Zhang", "Junshan", ""]]}, {"id": "2101.08428", "submitter": "Joshua Tobkin", "authors": "Joshua D. Tobkin", "title": "Introducing the Unitychain Structure: A novel blockchain-like structure\n  that enables greater parallel processing, security, and performance for\n  networks that leverage distributed key generation and classical consensus\n  protocols", "comments": "16 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A Unitychain is a novel blockchain-like structure that drastically improves\ntransaction scalability and security while maintaining ongoing network\nperformance, even if participating nodes are required to perform a new\nDistributed Key Generation procedure for security purposes. The Unitychain\nstructure, furthermore, enables greater parallel processing by the assignment\nof different network node configurations for various database and compute\nranges into multiple strands of blockchains that intersect, creating a\nmulti-helix structure, which we call a Unitychain. This thereby enables the\nnetwork to further bifurcate the roles of nodes into arbitrary yet\ndeterministic network responsibilities in order to maximize the global compute\npotential.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:57:41 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Tobkin", "Joshua D.", ""]]}, {"id": "2101.08715", "submitter": "Edward Stow", "authors": "Edward Stow, Riku Murai, Sajad Saeedi, Paul H. J. Kelly", "title": "Cain: Automatic Code Generation for Simultaneous Convolutional Kernels\n  on Focal-plane Sensor-processors", "comments": "17 pages, 4 figures, Accepted at LCPC 2020 to be published by\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Focal-plane Sensor-processors (FPSPs) are a camera technology that enable low\npower, high frame rate computation, making them suitable for edge computation.\nUnfortunately, these devices' limited instruction sets and registers make\ndeveloping complex algorithms difficult. In this work, we present Cain - a\ncompiler that targets SCAMP-5, a general-purpose FPSP - which generates code\nfrom multiple convolutional kernels. As an example, given the convolutional\nkernels for an MNIST digit recognition neural network, Cain produces code that\nis half as long, when compared to the other available compilers for SCAMP-5.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:48:28 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Stow", "Edward", ""], ["Murai", "Riku", ""], ["Saeedi", "Sajad", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "2101.08734", "submitter": "Nikoli Dryden", "authors": "Nikoli Dryden, Roman B\\\"ohringer, Tal Ben-Nun, Torsten Hoefler", "title": "Clairvoyant Prefetching for Distributed Machine Learning I/O", "comments": "13 pages, 16 figures; major revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I/O is emerging as a major bottleneck for machine learning training,\nespecially in distributed environments. Indeed, at large scale, I/O takes as\nmuch as 85% of training time. Addressing this I/O bottleneck necessitates\ncareful optimization, as optimal data ingestion pipelines differ between\nsystems, and require a delicate balance between access to local storage,\nexternal filesystems, and remote nodes. We introduce NoPFS, a machine learning\nI/O middleware, which provides a scalable, flexible, and easy-to-use solution\nto the I/O bottleneck. NoPFS uses clairvoyance: Given the seed generating the\nrandom access pattern for training with SGD, it can exactly predict when and\nwhere a sample will be accessed. We combine this with an analysis of access\npatterns and a performance model to provide distributed caching policies that\nadapt to different datasets and storage hierarchies. NoPFS reduces I/O times\nand improves end-to-end training by up to 5.4x on the ImageNet-1k,\nImageNet-22k, and CosmoFlow datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:21:42 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 12:28:20 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Dryden", "Nikoli", ""], ["B\u00f6hringer", "Roman", ""], ["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2101.08763", "submitter": "Philipp-Jan Honysz", "authors": "Philipp-Jan Honysz, Sebastian Buschj\\\"ager, Katharina Morik", "title": "GPU-Accelerated Optimizer-Aware Evaluation of Submodular Exemplar\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of submodular functions constitutes a viable way to perform\nclustering. Strong approximation guarantees and feasible optimization w.r.t.\nstreaming data make this clustering approach favorable. Technically, submodular\nfunctions map subsets of data to real values, which indicate how\n\"representative\" a specific subset is. Optimal sets might then be used to\npartition the data space and to infer clusters. Exemplar-based clustering is\none of the possible submodular functions, but suffers from high computational\ncomplexity. However, for practical applications, the particular real-time or\nwall-clock run-time is decisive. In this work, we present a novel way to\nevaluate this particular function on GPUs, which keeps the necessities of\noptimizers in mind and reduces wall-clock run-time. To discuss our GPU\nalgorithm, we investigated both the impact of different run-time critical\nproblem properties, like data dimensionality and the number of data points in a\nsubset, and the influence of required floating-point precision. In reproducible\nexperiments, our GPU algorithm was able to achieve competitive speedups of up\nto 72x depending on whether multi-threaded computation on CPUs was used for\ncomparison and the type of floating-point precision required. Half-precision\nGPU computation led to large speedups of up to 452x compared to\nsingle-precision, single-thread CPU computations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:23:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Honysz", "Philipp-Jan", ""], ["Buschj\u00e4ger", "Sebastian", ""], ["Morik", "Katharina", ""]]}, {"id": "2101.08837", "submitter": "Kerem \\\"Ozfatura", "authors": "Emre Ozfatura and Kerem Ozfatura and Deniz Gunduz", "title": "Time-Correlated Sparsification for Communication-Efficient Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) enables multiple clients to collaboratively train a\nshared model without disclosing their local datasets. This is achieved by\nexchanging local model updates with the help of a parameter server (PS).\nHowever, due to the increasing size of the trained models, the communication\nload due to the iterative exchanges between the clients and the PS often\nbecomes a bottleneck in the performance. Sparse communication is often employed\nto reduce the communication load, where only a small subset of the model\nupdates are communicated from the clients to the PS. In this paper, we\nintroduce a novel time-correlated sparsification (TCS) scheme, which builds\nupon the notion that sparse communication framework can be considered as\nidentifying the most significant elements of the underlying model. Hence, TCS\nseeks a certain correlation between the sparse representations used at\nconsecutive iterations in FL, so that the overhead due to encoding and\ntransmission of the sparse representation can be significantly reduced without\ncompromising the test accuracy. Through extensive simulations on the CIFAR-10\ndataset, we show that TCS can achieve centralized training accuracy with 100\ntimes sparsification, and up to 2000 times reduction in the communication load\nwhen employed together with quantization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 20:15:55 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ozfatura", "Emre", ""], ["Ozfatura", "Kerem", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2101.08878", "submitter": "Aamir Shafi", "authors": "Aamir Shafi, Jahanzeb Maqbool Hashmi, Hari Subramoni and Dhabaleswar\n  K. Panda", "title": "Efficient MPI-based Communication for GPU-Accelerated Dask Applications", "comments": "10 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dask is a popular parallel and distributed computing framework, which rivals\nApache Spark to enable task-based scalable processing of big data. The Dask\nDistributed library forms the basis of this computing engine and provides\nsupport for adding new communication devices. It currently has two\ncommunication devices: one for TCP and the other for high-speed networks using\nUCX-Py -- a Cython wrapper to UCX. This paper presents the design and\nimplementation of a new communication backend for Dask -- called MPI4Dask --\nthat is targeted for modern HPC clusters built with GPUs. MPI4Dask exploits\nmpi4py over MVAPICH2-GDR, which is a GPU-aware implementation of the Message\nPassing Interface (MPI) standard. MPI4Dask provides point-to-point asynchronous\nI/O communication coroutines, which are non-blocking concurrent operations\ndefined using the async/await keywords from the Python's asyncio framework. Our\nlatency and throughput comparisons suggest that MPI4Dask outperforms UCX by 6x\nfor 1 Byte message and 4x for large messages (2 MBytes and beyond)\nrespectively. We also conduct comparative performance evaluation of MPI4Dask\nwith UCX using two benchmark applications: 1) sum of cuPy array with its\ntranspose, and 2) cuDF merge. MPI4Dask speeds up the overall execution time of\nthe two applications by an average of 3.47x and 3.11x respectively on an\nin-house cluster built with NVIDIA Tesla V100 GPUs for 1-6 Dask workers. We\nalso perform scalability analysis of MPI4Dask against UCX for these\napplications on TACC's Frontera (GPU) system with upto 32 Dask workers on 32\nNVIDIA Quadro RTX 5000 GPUs and 256 CPU cores. MPI4Dask speeds up the execution\ntime for cuPy and cuDF applications by an average of 1.71x and 2.91x\nrespectively for 1-32 Dask workers on the Frontera (GPU) system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:59:08 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Shafi", "Aamir", ""], ["Hashmi", "Jahanzeb Maqbool", ""], ["Subramoni", "Hari", ""], ["Panda", "Dhabaleswar K.", ""]]}, {"id": "2101.08887", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Minho Lee, R.J.W.E. Lahaye, and Young Ik Eom", "title": "Distributed Compilation System for High-Speed Software Build Processes", "comments": null, "journal-ref": null, "doi": "10.1109/BIGCOMP.2014.6741419", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idle time of personal computers has increased steadily due to the\ngeneralization of computer usage and cloud computing. Clustering research aims\nat utilizing idle computer resources for processing a variable workload on a\nlarge number of computers. The workload is processed continually despite the\nvolatile status of the individual computer resources. This paper proposes a\ndistributed compilation system for improving the processing speed of\nCPU-intensive software compilations. This significantly reduces the compilation\ntime of mass sources by using the idle resources. We expect gains of up to 65%\ncompared to non-distributed compilation systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:24:39 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lim", "Geunsik", ""], ["Lee", "Minho", ""], ["Lahaye", "R. J. W. E.", ""], ["Eom", "Young Ik", ""]]}, {"id": "2101.08891", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Donghwa Lee, and Sang-Bum Suh", "title": "Cloud-Based Content Cooperation System to Assist Collaborative Learning\n  Environment", "comments": null, "journal-ref": null, "doi": "10.1109/TALE.2014.7062593", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online educational systems running on smart devices have the advantage of\nallowing users to learn online regardless of the location of the users. In\nparticular, data synchronization enables users to cooperate on contents in real\ntime anywhere by sharing their files via cloud storage. However, users cannot\ncollaborate by simultaneously modifying files that are shared with each other.\nIn this paper, we propose a content collaboration method and a history\nmanagement technique that are based on distributed system structure and can\nsynchronize data shared in the cloud for multiple users and multiple devices.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:46:53 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lim", "Geunsik", ""], ["Lee", "Donghwa", ""], ["Suh", "Sang-Bum", ""]]}, {"id": "2101.09222", "submitter": "Keehang Kwon", "authors": "Keehang Kwon", "title": "Computability-logic web: an alternative to deep learning", "comments": "9 pages. We discuss an approach to reaching general AI. arXiv admin\n  note: text overlap with arXiv:2010.08925, arXiv:1909.07036; substantial text\n  overlap with arXiv:0712.1345 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  {\\em Computability logic} (CoL) is a powerful, mathematically rigorous\ncomputational model. In this paper, we show that CoL-web, a web extension to\nCoL, naturally supports web programming where database updates are involved. To\nbe specific, we discuss an implementation of the AI ATM based on CoL (CL9 to be\nexact). More importantly, we argue that CoL-web supports a general AI and,\ntherefore, is a good alternative to neural nets and deep learning. We also\ndiscuss how to integrate neural nets into CoL-web.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:46:05 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kwon", "Keehang", ""]]}, {"id": "2101.09284", "submitter": "Geunsik Lim", "authors": "Geunsik Lim and Sang-Bum Suh", "title": "User-Level Memory Scheduler for Optimizing Application Performance in\n  NUMA-Based Multicore Systems", "comments": null, "journal-ref": null, "doi": "10.1109/ICSESS.2014.6933553", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicore CPU architectures have been established as a structure for\ngeneral-purpose systems for high-performance processing of applications. Recent\nmulticore CPU has evolved as a system architecture based on non-uniform memory\narchitecture. For the technique of using the kernel space that shifts the tasks\nto the ideal memory node, the characteristics of the applications of the\nuser-space cannot be considered. Therefore, kernel level approaches cannot\nexecute memory scheduling to recognize the importance of user applications.\nMoreover, users need to run applications after sufficiently understanding the\nmulticore CPU based on non-uniform memory architecture to ensure the high\nperformance of the user's applications. This paper presents a user-space memory\nscheduler that allocates the ideal memory node for tasks by monitoring the\ncharacteristics of non-uniform memory architecture. From our experiment, the\nproposed system improved the performance of the application by up to 25%\ncompared to the existing system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:28:55 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lim", "Geunsik", ""], ["Suh", "Sang-Bum", ""]]}, {"id": "2101.09337", "submitter": "Shuo Liu", "authors": "Shuo Liu, Nirupam Gupta, Nitin H. Vaidya", "title": "Approximate Byzantine Fault-Tolerance in Distributed Optimization", "comments": "40 pages, 5 figures, and 1 table. The report is an important\n  extension to prior work https://dl.acm.org/doi/abs/10.1145/3382734.3405748,\n  and arXiv:2003.09675; Added an extra experiment section in machine learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of Byzantine fault-tolerance in distributed\nmulti-agent optimization. In this problem, each agent has a local cost\nfunction, and in the fault-free case, the goal is to design a distributed\nalgorithm that allows all the agents to find a minimum point of all the agents'\naggregate cost function. We consider a scenario where some agents might be\nByzantine faulty that renders the original goal of computing a minimum point of\nall the agents' aggregate cost vacuous. A more reasonable objective for an\nalgorithm in this scenario is to allow all the non-faulty agents to compute the\nminimum point of only the non-faulty agents' aggregate cost. Prior work shows\nthat if there are up to $f$ (out of $n$) Byzantine agents then a minimum point\nof the non-faulty agents' aggregate cost can be computed exactly if and only if\nthe non-faulty agents' costs satisfy a certain redundancy property called\n$2f$-redundancy. However, $2f$-redundancy is an ideal property that can be\nsatisfied only in systems free from noise or uncertainties, which can make the\ngoal of exact fault-tolerance unachievable in some applications. Thus, we\nintroduce the notion of $(f,\\epsilon)$-resilience, a generalization of exact\nfault-tolerance wherein the objective is to find an approximate minimum point\nof the non-faulty aggregate cost, with $\\epsilon$ accuracy. This approximate\nfault-tolerance can be achieved under a weaker condition that is easier to\nsatisfy in practice, compared to $2f$-redundancy. We obtain necessary and\nsufficient conditions for achieving $(f,\\epsilon)$-resilience characterizing\nthe correlation between relaxation in redundancy and approximation in\nresilience. In case when the agents' cost functions are differentiable, we\nobtain conditions for $(f,\\epsilon)$-resilience of the distributed\ngradient-descent method when equipped with robust gradient aggregation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 21:14:25 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 22:40:32 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 05:57:41 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 22:39:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Shuo", ""], ["Gupta", "Nirupam", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2101.09355", "submitter": "Dmitrii Ustiugov", "authors": "Dmitrii Ustiugov, Plamen Petrov, Marios Kogias, Edouard Bugnion, Boris\n  Grot", "title": "Benchmarking, Analysis, and Optimization of Serverless Function\n  Snapshots", "comments": "To appear in ASPLOS 2021", "journal-ref": null, "doi": "10.1145/3445814.3446714", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has seen rapid adoption due to its high scalability and\nflexible, pay-as-you-go billing model. In serverless, developers structure\ntheir services as a collection of functions, sporadically invoked by various\nevents like clicks. High inter-arrival time variability of function invocations\nmotivates the providers to start new function instances upon each invocation,\nleading to significant cold-start delays that degrade user experience. To\nreduce cold-start latency, the industry has turned to snapshotting, whereby an\nimage of a fully-booted function is stored on disk, enabling a faster\ninvocation compared to booting a function from scratch.\n  This work introduces vHive, an open-source framework for serverless\nexperimentation with the goal of enabling researchers to study and innovate\nacross the entire serverless stack. Using vHive, we characterize a\nstate-of-the-art snapshot-based serverless infrastructure, based on\nindustry-leading Containerd orchestration framework and Firecracker hypervisor\ntechnologies. We find that the execution time of a function started from a\nsnapshot is 95% higher, on average, than when the same function is\nmemory-resident. We show that the high latency is attributable to frequent page\nfaults as the function's state is brought from disk into guest memory one page\nat a time. Our analysis further reveals that functions access the same stable\nworking set of pages across different invocations of the same function. By\nleveraging this insight, we build REAP, a light-weight software mechanism for\nserverless hosts that records functions' stable working set of guest memory\npages and proactively prefetches it from disk into memory. Compared to baseline\nsnapshotting, REAP slashes the cold-start delays by 3.7x, on average.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 00:03:28 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 17:57:59 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 21:26:44 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ustiugov", "Dmitrii", ""], ["Petrov", "Plamen", ""], ["Kogias", "Marios", ""], ["Bugnion", "Edouard", ""], ["Grot", "Boris", ""]]}, {"id": "2101.09359", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Changwoo Min, and YoungIk Eom", "title": "Load-Balancing for Improving User Responsiveness on Multicore Embedded\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most commercial embedded devices have been deployed with a single processor\narchitecture. The code size and complexity of applications running on embedded\ndevices are rapidly increasing due to the emergence of application business\nmodels such as Google Play Store and Apple App Store. As a result, a\nhigh-performance multicore CPUs have become a major trend in the embedded\nmarket as well as in the personal computer market. Due to this trend, many\ndevice manufacturers have been able to adopt more attractive user interfaces\nand high-performance applications for better user experiences on the multicore\nsystems. In this paper, we describe how to improve the real-time performance by\nreducing the user waiting time on multicore systems that use a partitioned\nper-CPU run queue scheduling technique. Rather than focusing on naive\nload-balancing scheme for equally balanced CPU usage, our approach tries to\nminimize the cost of task migration by considering the importance level of\nrunning tasks and to optimize per-CPU utilization on multicore embedded\nsystems. Consequently, our approach improves the real-time characteristics such\nas cache efficiency, user responsiveness, and latency. Experimental results\nunder heavy background stress show that our approach reduces the average\nscheduling latency of an urgent task by 2.3 times.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:30:13 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lim", "Geunsik", ""], ["Min", "Changwoo", ""], ["Eom", "YoungIk", ""]]}, {"id": "2101.09360", "submitter": "Geunsik Lim", "authors": "Geunsik Lim and MyungJoo Ham", "title": "BB: Booting Booster for Consumer Electronics with Modern OS", "comments": null, "journal-ref": null, "doi": "10.1145/2901318.2901320", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconventional computing platforms have spread widely and rapidly following\nsmart phones and tablets: consumer electronics such as smart TVs and digital\ncameras. For such devices, fast booting is a critical requirement; waiting tens\nof seconds for a TV or a camera to boot up is not acceptable, unlike a PC or\nsmart phone. Moreover, the software platforms of these devices have become as\nrich as conventional computing devices to provide comparable services. As a\nresult, the booting procedure to start every required OS service, hardware\ncomponent, and application, the quantity of which is ever increasing, may take\nunbearable time for most consumers. To accelerate booting, this paper\nintroduces \\textit{Booting Booster} (BB), which is used in all 2015 Samsung\nSmart TV models, and which runs the Linux-based Tizen OS. BB addresses the init\nscheme of Linux, which launches initial user-space OS services and applications\nand manages the life cycles of all user processes, by identifying and isolating\nbooting-critical tasks, deferring non-critical tasks, and enabling execution of\nmore tasks in parallel. BB has been successfully deployed in Samsung Smart TV\n2015 models achieving a cold boot in 3.5 s (compared to 8.1 s with full\ncommercial-grade optimizations without BB) without the need for suspend-to-RAM\nor hibernation. After this successful deployment, we have released the source\ncode via http://opensource.samsung.com, and BB will be included in the\nopen-source OS, Tizen (http://tizen.org).\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:32:13 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lim", "Geunsik", ""], ["Ham", "MyungJoo", ""]]}, {"id": "2101.09378", "submitter": "Bianca Trov\\`o", "authors": "Bianca Trov\\`o and Nazzareno Massari", "title": "Ants-Review: a Protocol for Incentivized Open Peer-Reviews on Ethereum", "comments": "8 pages, 1 figure, to appear as \"B. Trov\\`o, N. Massari\n  (forthcoming). Ants-Review: a Protocol for Incentivized Open Peer-Reviews on\n  Ethereum. In: Bartosz Balis, Dora B. Heras et al. (eds) Euro-Par 2020:\n  Parallel Processing Workshops. Euro-Par 2020. Lecture Notes in Computer\n  Science, Springer, Cham.\"", "journal-ref": null, "doi": "10.1007/978-3-030-71593-9_2", "report-no": null, "categories": "cs.DL cs.CR cs.CY cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer-review is a necessary and essential quality control step for scientific\npublications but lacks proper incentives. Indeed, the process, which is very\ncostly in terms of time and intellectual investment, not only is not\nremunerated by the journals but is also not openly recognized by the academic\ncommunity as a relevant scientific output for a researcher. Therefore,\nscientific dissemination is affected in timeliness, quality, and fairness.\nHere, to solve this issue, we propose a blockchain-based incentive system that\nrewards scientists for peer-reviewing other scientists' work and that builds up\ntrust and reputation. We designed a privacy-oriented protocol of smart\ncontracts called Ants-Review that allows authors to issue a bounty for open\nanonymous peer-reviews on Ethereum. If requirements are met, peer-reviews will\nbe accepted and paid by the approver proportionally to their assessed quality.\nTo promote ethical behavior and inclusiveness the system implements a gamified\nmechanism that allows the whole community to evaluate the peer-reviews and vote\nfor the best ones.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 23:32:41 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Trov\u00f2", "Bianca", ""], ["Massari", "Nazzareno", ""]]}, {"id": "2101.09428", "submitter": "WenJie Song", "authors": "Song WenJie, Shen Xuan", "title": "Vertical federated learning based on DFP and BFGS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As data privacy is gradually valued by people, federated learning(FL) has\nemerged because of its potential to protect data. FL uses homomorphic\nencryption and differential privacy encryption on the promise of ensuring data\nsecurity to realize distributed machine learning by exchanging encrypted\ninformation between different data providers. However, there are still many\nproblems in FL, such as the communication efficiency between the client and the\nserver and the data is non-iid. In order to solve the two problems mentioned\nabove, we propose a novel vertical federated learning framework based on the\nDFP and the BFGS(denoted as BDFL), then apply it to logistic regression.\nFinally, we perform experiments using real datasets to test efficiency of BDFL\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 06:15:04 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["WenJie", "Song", ""], ["Xuan", "Shen", ""]]}, {"id": "2101.09477", "submitter": "Dushyant Behl", "authors": "Dushyant Behl, Palanivel Kodeswaran, Venkatraman Ramakrishna,\n  Sayandeep Sen, Dhinakaran Vinayagamurthy", "title": "Trusted Data Notifications from Private Blockchains", "comments": "9 pages", "journal-ref": "IEEE International Conference on Blockchain, Blockchain 2020,\n  Rhodes Island, Greece, November 2-6, 2020, pages {53--61}", "doi": "10.1109/Blockchain50366.2020.00015", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private blockchain networks are used by enterprises to manage decentralized\nprocesses without trusted mediators and without exposing their assets publicly\non an open network like Ethereum. Yet external parties that cannot join such\nnetworks may have a compelling need to be informed about certain data items on\ntheir shared ledgers along with certifications of data authenticity; e.g., a\nmortgage bank may need to know about the sale of a mortgaged property from a\nnetwork managing property deeds. These parties are willing to compensate the\nnetworks in exchange for privately sharing information with proof of\nauthenticity and authorization for external use. We have devised a novel and\ncryptographically secure protocol to effect a fair exchange between rational\nnetwork members and information recipients using a public blockchain and atomic\nswap techniques. Using our protocol, any member of a private blockchain can\natomically reveal private blockchain data with proofs in exchange for a\nmonetary reward to an external party if and only if the external party is a\nvalid recipient. The protocol preserves confidentiality of data for the\nrecipient, and in addition, allows it to mount a challenge if the data turns\nout to be inauthentic. We also formally analyze the security and privacy of\nthis protocol, which can be used in a wide array of practical scenarios\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 10:45:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Behl", "Dushyant", ""], ["Kodeswaran", "Palanivel", ""], ["Ramakrishna", "Venkatraman", ""], ["Sen", "Sayandeep", ""], ["Vinayagamurthy", "Dhinakaran", ""]]}, {"id": "2101.09527", "submitter": "Jordi Bataller Mascarell", "authors": "Jordi Bataller Mascarell", "title": "Formal Definitions of Memory Consistency Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Shared Memory is a mechanism that allows several processes to communicate\nwith each other by accessing -- writing or reading -- a set of variables that\nthey have in common. A Consistency Model defines how each process observes the\nstate of the Memory, according to the accesses performed by it and by the rest\nof the processes in the system. Therefore, it determines what value a read\nreturns when a given process issues it. This implies that there must be an\nagreement among all, or among processes in different subsets, on the order in\nwhich all or a subset of the accesses happened. It is clear that a higher\nquantity of accesses or proceses taking part in the agreement makes it possibly\nharder or slower to be achieved. This is the main reason for which a number of\nConsistency Models for Shared Memory have been introduced. This paper is a\nhandy summary of [2] and [3] where consistency models (Sequential, Causal,\nPRAM, Cache, Processors, Slow), including synchronized ones (Weak, Release,\nEntry), were formally defined. This provides a better understanding of those\nmodels and a way to reason and compare them through a concise notation. There\nare many papers on this subject in the literature such as [11] with which this\nwork shares some concepts.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 15:57:54 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mascarell", "Jordi Bataller", ""]]}, {"id": "2101.09584", "submitter": "Yuval Tamir", "authors": "Diyu Zhou and Yuval Tamir", "title": "HyCoR: Fault-Tolerant Replicated Containers Based on Checkpoint and\n  Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HyCoR is a fully-operational fault tolerance mechanism for multiprocessor\nworkloads, based on container replication, using a hybrid of checkpointing and\nreplay. HyCoR derives from two insights regarding replication mechanisms: 1)\ndeterministic replay can overcome a key disadvantage of checkpointing alone --\nunacceptably long delays of outputs to clients, and 2) checkpointing can\novercome a key disadvantage of active replication with deterministic replay\nalone -- vulnerability to even rare replay failures due to an untracked\nnondeterministic events. With HyCoR, the primary sends periodic checkpoints to\nthe backup and logs the outcomes of sources of nondeterminism. Outputs to\nclients are delayed only by the short time it takes to send the corresponding\nlog to the backup. Upon primary failure, the backup replays only the short\ninterval since the last checkpoint, thus minimizing the window of\nvulnerability. HyCoR includes a \"best effort\" mechanism that results in a high\nrecovery rate even in the presence of data races, as long as their rate is low.\nThe evaluation includes measurement of the recovery rate and recovery latency\nbased on fault injection. On average, HyCoR delays responses to clients by less\nthan 1ms and recovers in less than 1s. For a set of eight real-world\nbenchmarks, if data races are eliminated, the performance overhead of HyCoR is\nunder 59%.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 21:08:25 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhou", "Diyu", ""], ["Tamir", "Yuval", ""]]}, {"id": "2101.09715", "submitter": "Tobias Latta", "authors": "Tobias Michel Latta", "title": "Combining SimTrust and Weighted Simple Exponential Smoothing", "comments": "2020 Conference on Self-Organising Systems at Christian-Albrechts\n  University in Kiel; 7 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the domain of Autonomic and Organic Computing, the entities of a\ndistributed system are variable as well as the efficiency and the intention of\ntheir work. Therefore, a scalable mechanism to incentivise/sanction entities\nwhich contribute towards/against the system goal is needed. Trust is a suited\nmetric to find benevolent entities. In this paper, we focus for one on the\nSimTrust model which introduces trust on entities when they share interest and\nopinions using tagging information. The second model is the Weighted Simple\nExponential Smoothing Trust metric (WSES) which functions on explicitly rated\nitems. WSES follows two basic rules which ensure a logic rating mechanism. When\nputting these two models in context, SimTrust has advantages on items that have\nnot been rated yet or can not easily be rated. WSES is a trust metric which\nreturns good results on explicit rank values. We propose concepts on combining\nboth approaches and state in which cases they are incompatible.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 13:14:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Latta", "Tobias Michel", ""]]}, {"id": "2101.09716", "submitter": "TianZhang He", "authors": "TianZhang He, Adel N. Toosi, Rajkumar Buyya", "title": "SLA-Aware Multiple Migration Planning and Scheduling in SDN-NFV-enabled\n  Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Software-Defined Networking (SDN)-enabled cloud data centers, live\nmigration is a key approach used for the reallocation of Virtual Machines (VMs)\nin cloud services and Virtual Network Functions (VNFs) in Service Function\nChaining (SFC). Using live migration methods, cloud providers can address their\ndynamic resource management and fault tolerance objectives without interrupting\nthe service of users. However, in cloud data centers, performing multiple live\nmigrations in arbitrary order can lead to service degradation. Therefore,\nefficient migration planning is essential to reduce the impact of live\nmigration overheads. In addition, to prevent Quality of Service (QoS)\ndegradations and Service Level Agreement (SLA) violations, it is necessary to\nset priorities for different live migration requests with various urgency. In\nthis paper, we propose SLAMIG, a set of algorithms that composes the\ndeadline-aware multiple migration grouping algorithm and on-line migration\nscheduling to determine the sequence of VM/VNF migrations. The experimental\nresults show that our approach with reasonable algorithm runtime can\nefficiently reduce the number of deadline misses and has a good migration\nperformance compared with the one-by-one scheduling and two state-of-the-art\nalgorithms in terms of total migration time, average execution time, downtime,\nand transferred data. We also evaluate and analyze the impact of multiple\nmigration planning and scheduling on QoS and energy consumption.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 13:15:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["He", "TianZhang", ""], ["Toosi", "Adel N.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2101.09796", "submitter": "Anshul Jindal", "authors": "Saitel Daniela Agudelo-Sanabria and Anshul Jindal", "title": "The Ifs and Buts of the Development Approaches for IoT Applications", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent growth of the Internet of Things (IoT) devices has lead to the\nrise of various complex applications where these applications involve\ninteractions among large numbers of heterogeneous devices. An important\nchallenge that needs to be addressed is to facilitate the agile development of\nIoT applications with minimal effort by the various parties involved in the\nprocess. However, IoT application development is challenging due to the wide\nvariety of hardware and software technologies that interact in an IoT system.\nMoreover, it involves dealing with issues that are attributed to different\nsoftware life-cycle phases: development, deployment, and progression.\n  In this paper, we examine three IoT application development approaches:\nMashup-based development, Model-based development, and Function-as-a-Service\nbased development. The advantages and disadvantages of each approach are\ndiscussed from different perspectives, including reliability, deployment\nexpeditiousness, ease of use, and targeted audience. Finally, we propose a\nsimple solution where these techniques are combined to deliver reliable\napplications while reducing costs and time to release.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 20:39:35 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Agudelo-Sanabria", "Saitel Daniela", ""], ["Jindal", "Anshul", ""]]}, {"id": "2101.09797", "submitter": "Zaid Hussain", "authors": "Zaid Hussain, Hosam AboElFotoh, and Bader AlBdaiwi", "title": "Independent Spanning Trees in Eisenstein-Jacobi Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanning trees are widely used in networks for broadcasting, fault-tolerance,\nand securely delivering messages. Hexagonal interconnection networks have a\nnumber of real life applications. Examples are cellular networks, computer\ngraphics, and image processing. Eisenstein-Jacobi (EJ) networks are a\ngeneralization of hexagonal mesh topology. They have a wide range of potential\napplications, and thus they have received researchers' attention in different\nareas among which interconnection networks and coding theory. In this paper, we\npresent two spanning trees' constructions for Eisenstein-Jacobi (EJ). The first\nconstructs three edge-disjoint node-independent spanning trees, while the\nsecond constructs six node-independent spanning trees but not edge disjoint.\nBased on the constructed trees, we develop routing algorithms that can securely\ndeliver a message and tolerate a number of faults in point-to-point or in\nbroadcast communications. The proposed work is also applied on higher\ndimensional EJ networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 20:45:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hussain", "Zaid", ""], ["AboElFotoh", "Hosam", ""], ["AlBdaiwi", "Bader", ""]]}, {"id": "2101.09799", "submitter": "Anshul Jindal", "authors": "Anshul Jindal, Paul Staab, Jorge Cardoso, Michael Gerndt and Vladimir\n  Podolskiy", "title": "Online Memory Leak Detection in the Cloud-based Infrastructures", "comments": "12 pages", "journal-ref": "International Workshop on Artificial Intelligence for IT\n  Operations (AIOPS) 2020", "doi": "10.1007/978-3-030-76352-7_21", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A memory leak in an application deployed on the cloud can affect the\navailability and reliability of the application. Therefore, to identify and\nultimately resolve it quickly is highly important. However, in the production\nenvironment running on the cloud, memory leak detection is a challenge without\nthe knowledge of the application or its internal object allocation details.\n  This paper addresses this challenge of online detection of memory leaks in\ncloud-based infrastructure without having any internal application knowledge by\nintroducing a novel machine learning based algorithm Precog. This algorithm\nsolely uses one metric i.e the system's memory utilization on which the\napplication is deployed for the detection of a memory leak. The developed\nalgorithm's accuracy was tested on 60 virtual machines manually labeled memory\nutilization data provided by our industry partner Huawei Munich Research Center\nand it was found that the proposed algorithm achieves the accuracy score of\n85\\% with less than half a second prediction time per virtual machine.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 20:48:45 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Jindal", "Anshul", ""], ["Staab", "Paul", ""], ["Cardoso", "Jorge", ""], ["Gerndt", "Michael", ""], ["Podolskiy", "Vladimir", ""]]}, {"id": "2101.09878", "submitter": "Ajesh Koyatan Chathoth", "authors": "Ajesh Koyatan Chathoth (1), Abhyuday Jagannatha (2), Stephen Lee (1)\n  ((1) University of Pittsburgh, (2) University of Massachusetts Amherst)", "title": "Federated Intrusion Detection for IoT with Heterogeneous Cohort Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) devices are becoming increasingly popular and are\ninfluencing many application domains such as healthcare and transportation.\nThese devices are used for real-world applications such as sensor monitoring,\nreal-time control. In this work, we look at differentially private (DP) neural\nnetwork (NN) based network intrusion detection systems (NIDS) to detect\nintrusion attacks on networks of such IoT devices. Existing NN training\nsolutions in this domain either ignore privacy considerations or assume that\nthe privacy requirements are homogeneous across all users. We show that the\nperformance of existing differentially private stochastic methods degrade for\nclients with non-identical data distributions when clients' privacy\nrequirements are heterogeneous. We define a cohort-based $(\\epsilon,\\delta)$-DP\nframework that models the more practical setting of IoT device cohorts with\nnon-identical clients and heterogeneous privacy requirements. We propose two\nnovel continual-learning based DP training methods that are designed to improve\nmodel performance in the aforementioned setting. To the best of our knowledge,\nours is the first system that employs a continual learning-based approach to\nhandle heterogeneity in client privacy requirements. We evaluate our approach\non real datasets and show that our techniques outperform the baselines. We also\nshow that our methods are robust to hyperparameter changes. Lastly, we show\nthat one of our proposed methods can easily adapt to post-hoc relaxations of\nclient privacy requirements.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:33:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chathoth", "Ajesh Koyatan", "", "University of Pittsburgh"], ["Jagannatha", "Abhyuday", "", "University of Massachusetts Amherst"], ["Lee", "Stephen", "", "University of Pittsburgh"]]}, {"id": "2101.10209", "submitter": "Arman Aghdashi", "authors": "Arman Aghdashi, Seyedeh Leili Mirtaheri", "title": "Novel Dynamic Load Balancing Algorithm for Cloud-Based Big Data\n  Analytics", "comments": "My supervisor has requested that i withdraw the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics in cloud environments introduces challenges such as\nreal-time load balancing besides security, privacy, and energy efficiency. In\nthis paper, we propose a novel load balancing algorithm in cloud environments\nthat performs resource allocation and task scheduling efficiently. The proposed\nload balancer reduces the execution response time in big data applications\nperformed on clouds. Scheduling, in general, is an NP-hard problem. In our\nproposed algorithm, we provide solutions to reduce the search area that leads\nto reduced complexity of the load balancing. We recommend two mathematical\noptimization models to perform dynamic resource allocation to virtual machines\nand task scheduling. The provided solution is based on the hill-climbing\nalgorithm to minimize response time. We evaluate the performance of proposed\nalgorithms in terms of response time, turnaround time, throughput metrics, and\nrequest distribution with some of the existing algorithms that show significant\nimprovements\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:18:59 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 21:12:49 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Aghdashi", "Arman", ""], ["Mirtaheri", "Seyedeh Leili", ""]]}, {"id": "2101.10417", "submitter": "Dragi Kimovski", "authors": "Dragi Kimovski, Roland Math\\'a, Josef Hammer, Narges Mehran, Hermann\n  Hellwagner and Radu Prodan", "title": "Cloud, Fog or Edge: Where to Compute?", "comments": null, "journal-ref": "IEEE Internet Computing 2021", "doi": "10.1109/MIC.2021.3050613", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computing continuum extends the high-performance cloud data centers with\nenergy-efficient and low-latency devices close to the data sources located at\nthe edge of the network. However, the heterogeneity of the computing continuum\nraises multiple challenges related to application management. These include\nwhere to offload an application - from the cloud to the edge - to meet its\ncomputation and communication requirements. To support these decisions, we\nprovide in this article a detailed performance and carbon footprint analysis of\na selection of use case applications with complementary resource requirements\nacross the computing continuum over a real-life evaluation testbed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:05:00 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kimovski", "Dragi", ""], ["Math\u00e1", "Roland", ""], ["Hammer", "Josef", ""], ["Mehran", "Narges", ""], ["Hellwagner", "Hermann", ""], ["Prodan", "Radu", ""]]}, {"id": "2101.10463", "submitter": "An Zou", "authors": "An Zou, Jing Li, Christopher D. Gill, and Xuan Zhang", "title": "RTGPU: Real-Time GPU Scheduling of Hard Deadline Parallel Tasks with\n  Fine-Grain Utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.AR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many emerging cyber-physical systems, such as autonomous vehicles and robots,\nrely heavily on artificial intelligence and machine learning algorithms to\nperform important system operations. Since these highly parallel applications\nare computationally intensive, they need to be accelerated by graphics\nprocessing units (GPUs) to meet stringent timing constraints. However, despite\nthe wide adoption of GPUs, efficiently scheduling multiple GPU applications\nwhile providing rigorous real-time guarantees remains a challenge. In this\npaper, we propose RTGPU, which can schedule the execution of multiple GPU\napplications in real-time to meet hard deadlines. Each GPU application can have\nmultiple CPU execution and memory copy segments, as well as GPU kernels. We\nstart with a model to explicitly account for the CPU and memory copy segments\nof these applications. We then consider the GPU architecture in the development\nof a precise timing model for the GPU kernels and leverage a technique known as\npersistent threads to implement fine-grained kernel scheduling with improved\nperformance through interleaved execution. Next, we propose a general method\nfor scheduling parallel GPU applications in real time. Finally, to schedule\nmultiple parallel GPU applications, we propose a practical real-time scheduling\nalgorithm based on federated scheduling and grid search (for GPU kernel\nsegments) with uniprocessor fixed priority scheduling (for multiple CPU and\nmemory copy segments). Our approach provides superior schedulability compared\nwith previous work, and gives real-time guarantees to meet hard deadlines for\nmultiple GPU applications according to comprehensive validation and evaluation\non a real NVIDIA GTX1080Ti GPU system.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:34:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:22:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zou", "An", ""], ["Li", "Jing", ""], ["Gill", "Christopher D.", ""], ["Zhang", "Xuan", ""]]}, {"id": "2101.10464", "submitter": "Mirko Zichichi", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo, V\\'ictor\n  Rodr\\'iguez-Doncel", "title": "Personal Data Access Control Through Distributed Authorization", "comments": null, "journal-ref": "2020 IEEE 19th International Symposium on Network Computing and\n  Applications (NCA)", "doi": "10.1109/NCA51143.2020.9306721", "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an architecture of a Personal Information Management\nSystem, in which individuals can define the access to their personal data by\nmeans of smart contracts. These smart contracts, running on the Ethereum\nblockchain, implement access control lists and grant immutability, traceability\nand verifiability of the references to personal data, which is stored itself in\na (possibly distributed) file system. A distributed authorization mechanism is\ndevised, where trust from multiple network nodes is necessary to grant the\naccess to the data. To this aim, two possible alternatives are described: a\nSecret Sharing scheme and Threshold Proxy Re-Encryption scheme. The performance\nof these alternatives is experimentally compared in terms of execution time.\nThreshold Proxy Re-Encryption appears to be faster in different scenarios, in\nparticular when increasing message size, number of nodes and the threshold\nvalue, i.e. number of nodes needed to grant the data disclosure.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:34:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""], ["Rodr\u00edguez-Doncel", "V\u00edctor", ""]]}, {"id": "2101.10697", "submitter": "Jossekin Beilharz", "authors": "Jossekin Beilharz, Philipp Wiesner, Arne Boockmeyer, Florian\n  Brokhausen, Ilja Behnke, Robert Schmid, Lukas Pirl, and Lauritz Thamsen", "title": "Towards a Staging Environment for the Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) applications promise to make many aspects of our\nlives more efficient and adaptive through the use of distributed sensing and\ncomputing nodes. A central aspect of such applications is their complex\ncommunication behavior that is heavily influenced by the physical environment\nof the system. To continuously improve IoT applications, a staging environment\nis needed that can provide operating conditions representative of deployments\nin the actual production environments -- similar to what is common practice in\ncloud application development today. Towards such a staging environment, we\npresent Marvis, a framework that orchestrates hybrid testbeds, co-simulated\ndomain environments, and a central network simulation for testing distributed\nIoT applications. Our preliminary results include an open source prototype and\na demonstration of a Vehicle-to-everything (V2X) communication scenario.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:40:20 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Beilharz", "Jossekin", ""], ["Wiesner", "Philipp", ""], ["Boockmeyer", "Arne", ""], ["Brokhausen", "Florian", ""], ["Behnke", "Ilja", ""], ["Schmid", "Robert", ""], ["Pirl", "Lukas", ""], ["Thamsen", "Lauritz", ""]]}, {"id": "2101.10761", "submitter": "Ahmed Elzanaty Dr.", "authors": "Ahmed M. Abdelmoniem and Ahmed Elzanaty and Mohamed-Slim Alouini and\n  Marco Canini", "title": "An Efficient Statistical-based Gradient Compression Technique for\n  Distributed Training Systems", "comments": "Accepted at the 2021 Machine Learning and Systems (MLSys) Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent many-fold increase in the size of deep neural networks makes\nefficient distributed training challenging. Many proposals exploit the\ncompressibility of the gradients and propose lossy compression techniques to\nspeed up the communication stage of distributed training. Nevertheless,\ncompression comes at the cost of reduced model quality and extra computation\noverhead. In this work, we design an efficient compressor with minimal\noverhead. Noting the sparsity of the gradients, we propose to model the\ngradients as random variables distributed according to some sparsity-inducing\ndistributions (SIDs). We empirically validate our assumption by studying the\nstatistical characteristics of the evolution of gradient vectors over the\ntraining process. We then propose Sparsity-Inducing Distribution-based\nCompression (SIDCo), a threshold-based sparsification scheme that enjoys\nsimilar threshold estimation quality to deep gradient compression (DGC) while\nbeing faster by imposing lower compression overhead. Our extensive evaluation\nof popular machine learning benchmarks involving both recurrent neural network\n(RNN) and convolution neural network (CNN) models shows that SIDCo speeds up\ntraining by up to 41:7%, 7:6%, and 1:9% compared to the no-compression\nbaseline, Topk, and DGC compressors, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:06:00 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 18:18:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Abdelmoniem", "Ahmed M.", ""], ["Elzanaty", "Ahmed", ""], ["Alouini", "Mohamed-Slim", ""], ["Canini", "Marco", ""]]}, {"id": "2101.10852", "submitter": "Hao Xu", "authors": "Lei Zhang, Hao Xu, Oluwakayode Onireti, Muhammad Ali Imran and Bin Cao", "title": "How Much Communication Resource is Needed to Run a Wireless Blockchain\n  Network?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is built on a peer-to-peer network that relies on frequent\ncommunications among the distributively located nodes. In particular, the\nconsensus mechanisms (CMs), which play a pivotal role in blockchain, are\ncommunication resource-demanding and largely determines blockchain security\nbound and other key performance metrics such as transaction throughput, latency\nand scalability. Most blockchain systems are designed in a stable wired\ncommunication network running in advanced devices under the assumption of\nsufficient communication resource provision. However, it is envisioned that the\nmajority of the blockchain node peers will be connected through the wireless\nnetwork in the future. Constrained by the highly dynamic wireless channel and\nscarce frequency spectrum, communication can significantly affect blockchain's\nkey performance metrics. Hence, in this paper, we present wireless blockchain\nnetworks (WBN) under various commonly used CMs and we answer the question of\nhow much communication resource is needed to run such a network. We first\npresent the role of communication in the four stages of the blockchain\nprocedure. We then discuss the relationship between the communication resource\nprovision and the WBNs performance, for three of the most used blockchain CMs\nnamely, Proof-of-Work (PoW), practical Byzantine Fault Tolerant (PBFT) and\nRaft. Finally, we provide analytical and simulated results to show the impact\nof the communication resource provision on blockchain performance.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 13:03:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhang", "Lei", ""], ["Xu", "Hao", ""], ["Onireti", "Oluwakayode", ""], ["Imran", "Muhammad Ali", ""], ["Cao", "Bin", ""]]}, {"id": "2101.10856", "submitter": "Hao Xu", "authors": "Hao Xu, Lei Zhang, Yunqing Sun, and Chih-Lin I", "title": "BE-RAN: Blockchain-enabled Open RAN with Decentralized Identity\n  Management and Privacy-Preserving Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio Access Networks (RAN) tends to be more distributed in the 5G and\nbeyond, in order to provide low latency and flexible on-demanding services. In\nthis paper, Blockchain-enabled Radio Access Networks (BE-RAN) is proposed as a\nnovel decentralized RAN architecture to facilitate enhanced security and\nprivacy on identification and authentication. It can offer user-centric\nidentity management for User Equipment (UE) and RAN elements, and enable mutual\nauthentication to all entities while enabling on-demand point-to-point\ncommunication with accountable billing service add-on on public network. Also,\na potential operating model with thorough decentralization of RAN is\nenvisioned. The paper also proposed a distributed privacy-preserving P2P\ncommunication approach, as one of the core use cases for future mobile\nnetworks, is presented as an essential complement to the existing core\nnetwork-based security and privacy management. The results show that BE-RAN\nsignificantly improves communication and computation overheads compared to the\nexisting communication authentication protocols.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:24:22 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 19:43:45 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 12:24:26 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Hao", ""], ["Zhang", "Lei", ""], ["Sun", "Yunqing", ""], ["I", "Chih-Lin", ""]]}, {"id": "2101.10881", "submitter": "Jan Verschelde", "authors": "Jan Verschelde", "title": "Accelerated Polynomial Evaluation and Differentiation at Power Series in\n  Multiple Double Precision", "comments": "Improved the introduction, adding two citations to related work;\n  fixed error, added table on the fluctuations of wall clock times. To appear\n  in the Proceedings of the 2021 IEEE International Parallel and Distributed\n  Processing Symposium Workshops (IPDPSW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA cs.SC math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem is to evaluate a polynomial in several variables and its gradient\nat a power series truncated to some finite degree with multiple double\nprecision arithmetic. To compensate for the cost overhead of multiple double\nprecision and power series arithmetic, data parallel algorithms for general\npurpose graphics processing units are presented. The reverse mode of\nalgorithmic differentiation is organized into a massively parallel computation\nof many convolutions and additions of truncated power series. Experimental\nresults demonstrate that teraflop performance is obtained in deca double\nprecision with power series truncated at degree 152. The algorithms scale well\nfor increasing precision and increasing degrees.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:42:43 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 16:14:34 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 00:22:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Verschelde", "Jan", ""]]}, {"id": "2101.10920", "submitter": "Nguyen Truong", "authors": "Nguyen Truong, Gyu Myoung Lee, Kai Sun, Florian Guitton, Yike Guo", "title": "A Blockchain-based Trust System for Decentralised Applications: When\n  trustless needs trust", "comments": "14 pages, 8 figures, submitted to Elsevier Future Generation Computer\n  Systems jornal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain technology has been envisaged to commence an era of decentralised\napplications and services (DApps) without the need for a trusted intermediary.\nSuch DApps open a marketplace in which services are delivered to end-users by\ncontributors which are then incentivised by cryptocurrencies in an automated,\npeer-to-peer, and trustless fashion. However, blockchain, consolidated by smart\ncontracts, only ensures on-chain data security, autonomy and integrity of the\nbusiness logic execution defined in smart contracts. It cannot guarantee the\nquality of service of DApps, which entirely depends on the services'\nperformance. Thus, there is a critical need for a trust system to reduce the\nrisk of dealing with fraudulent counterparts in a blockchain network. These\nreasons motivate us to develop a fully decentralised trust framework deployed\non top of a blockchain platform, operating along with DApps in the marketplace\nto demoralise deceptive entities while encouraging trustworthy ones. The trust\nsystem works as an underlying decentralised service providing a feedback\nmechanism for end-users and maintaining trust relationships among them in the\necosystem accordingly. We believe this research fortifies the DApps ecosystem\nby introducing an universal trust middleware for DApps as well as shedding\nlight on the implementation of a decentralised trust system.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:41:01 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Truong", "Nguyen", ""], ["Lee", "Gyu Myoung", ""], ["Sun", "Kai", ""], ["Guitton", "Florian", ""], ["Guo", "Yike", ""]]}, {"id": "2101.11049", "submitter": "Joel Fuentes", "authors": "Guei-Yuan Lueh, Kaiyu Chen, Gang Chen, Joel Fuentes, Wei-Yu Chen,\n  Fangwen Fu, Hong Jiang, Hongzheng Li, Daniel Rhee", "title": "C-for-Metal: High Performance SIMD Programming on Intel GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The SIMT execution model is commonly used for general GPU development. CUDA\nand OpenCL developers write scalar code that is implicitly parallelized by\ncompiler and hardware. On Intel GPUs, however, this abstraction has profound\nperformance implications as the underlying ISA is SIMD and important hardware\ncapabilities cannot be fully utilized. To close this performance gap we\nintroduce C-For-Metal (CM), an explicit SIMD programming framework designed to\ndeliver close-to-the-metal performance on Intel GPUs. The CM programming\nlanguage and its vector/matrix types provide an intuitive interface to exploit\nthe underlying hardware features, allowing fine-grained register management,\nSIMD size control and cross-lane data sharing. Experimental results show that\nCM applications from different domains outperform the best-known SIMT-based\nOpenCL implementations, achieving up to 2.7x speedup on the latest Intel GPU.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:43:50 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lueh", "Guei-Yuan", ""], ["Chen", "Kaiyu", ""], ["Chen", "Gang", ""], ["Fuentes", "Joel", ""], ["Chen", "Wei-Yu", ""], ["Fu", "Fangwen", ""], ["Jiang", "Hong", ""], ["Li", "Hongzheng", ""], ["Rhee", "Daniel", ""]]}, {"id": "2101.11053", "submitter": "Jian-Jia Chen", "authors": "Niklas Ueter and Mario G\\\"unzel and Jian-Jia Chen", "title": "Response-Time Analysis and Optimization for Probabilistic Conditional\n  Parallel DAG Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-time systems increasingly use multicore processors in order to satisfy\nthermal, power, and computational requirements. To exploit the architectural\nparallelism offered by the multicore processors, parallel task models,\nscheduling algorithms and response-time analyses with respect to real-time\nconstraints have to be provided. In this paper, we propose a reservation-based\nscheduling algorithm for sporadic constrained-deadline parallel conditional DAG\ntasks with probabilistic execution behaviour for applications that can tolerate\nbounded number of deadline misses and bounded tardiness. We devise design rules\nand analyses to guarantee bounded tardiness for a specified bounded probability\nfor $k$-consecutive deadline misses without enforcing late jobs to be\nimmediately aborted.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:52:58 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ueter", "Niklas", ""], ["G\u00fcnzel", "Mario", ""], ["Chen", "Jian-Jia", ""]]}, {"id": "2101.11126", "submitter": "Hamouma Moumen", "authors": "Badreddine Benreguia, Hamouma Moumen, Soheila Bouam, Chafik Arar", "title": "Self-stabilizing Algorithm for Maximal Distance-2 Independent Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In graph theory, an independent set is a subset of nodes where there are no\ntwo adjacent nodes. The independent set is maximal if no node outside the\nindependent set can join it. In network applications, maximal independent sets\ncan be used as cluster heads in ad hoc and wireless sensor networks. In order\nto deal with any failure in networks, self-stabilizing algorithms have been\nproposed in the literature to calculate the maximal independent set under\ndifferent hypotheses. In this paper, we propose a self-stabilizing algorithm to\ncompute a maximal independent set where nodes of the independent set are far\nfrom each other at least with distance 3. We prove the correctness and the\nconvergence of the proposed algorithm. Simulation tests show the ability of our\nalgorithm to find a reduced number of nodes in large scale networks which\nallows strong control of networks\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:57:06 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 21:10:28 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Benreguia", "Badreddine", ""], ["Moumen", "Hamouma", ""], ["Bouam", "Soheila", ""], ["Arar", "Chafik", ""]]}, {"id": "2101.11147", "submitter": "Mohammad Mukhtaruzzaman", "authors": "Mohammad Mukhtaruzzaman and Mohammed Atiquzzaman", "title": "Cloud based VANET Simulator (CVANETSIM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vehicular ad hoc network (VANET) is an integral part of vehicular\ncommunication. VANET suffers many problems such as scalability. To solve\nscalability and other problems of VANET, clustering is proposed. VANET\nclustering is different than any other kind of clustering due to the high\nmobility of the vehicles. Likewise, VANET and VANET clustering, VANET simulator\nrequires some unique features such as internet based real-time data processing,\nhuge data analysis, the complex calculation to maintain hierarchy among the\nvehicles, etc.; however, neither web based VANET simulator nor clustering\nmodule available in the existing simulators. Therefore, a simulator that will\nbe able to simulate any feature of VANET equipped with a clustering module and\naccessible via the internet is a growing need in vehicular communication\nresearch. At the Telecom and Network Research Lab (TNRL), University of\nOklahoma, we have developed a fully functional discrete-event VANET simulator\nthat includes all the features of VANET clustering. Moreover, the cloud based\nVANET simulator (CVANETSIM) is coming with an easy and interactive web\ninterface. To our best of our knowledge, CVANETSIM is the first of its kind\nwhich integrates features of the VANET simulator, built-in VANET clustering\nmodule, and accessible through the internet.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 00:49:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mukhtaruzzaman", "Mohammad", ""], ["Atiquzzaman", "Mohammed", ""]]}, {"id": "2101.11203", "submitter": "Haibo Yang Mr", "authors": "Haibo Yang, Minghong Fang, Jia Liu", "title": "Achieving Linear Speedup with Partial Worker Participation in Non-IID\n  Federated Learning", "comments": "Published as a conference paper at ICLR 2021, fixed errors in theorem\n  2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a distributed machine learning architecture that\nleverages a large number of workers to jointly learn a model with decentralized\ndata. FL has received increasing attention in recent years thanks to its data\nprivacy protection, communication efficiency and a linear speedup for\nconvergence in training (i.e., convergence performance increases linearly with\nrespect to the number of workers). However, existing studies on linear speedup\nfor convergence are only limited to the assumptions of i.i.d. datasets across\nworkers and/or full worker participation, both of which rarely hold in\npractice. So far, it remains an open question whether or not the linear speedup\nfor convergence is achievable under non-i.i.d. datasets with partial worker\nparticipation in FL. In this paper, we show that the answer is affirmative.\nSpecifically, we show that the federated averaging (FedAvg) algorithm (with\ntwo-sided learning rates) on non-i.i.d. datasets in non-convex settings\nachieves a convergence rate $\\mathcal{O}(\\frac{1}{\\sqrt{mKT}} + \\frac{1}{T})$\nfor full worker participation and a convergence rate\n$\\mathcal{O}(\\frac{\\sqrt{K}}{\\sqrt{nT}} + \\frac{1}{T})$ for partial worker\nparticipation, where $K$ is the number of local steps, $T$ is the number of\ntotal communication rounds, $m$ is the total worker number and $n$ is the\nworker number in one communication round if for partial worker participation.\nOur results also reveal that the local steps in FL could help the convergence\nand show that the maximum number of local steps can be improved to $T/m$ in\nfull worker participation. We conduct extensive experiments on MNIST and\nCIFAR-10 to verify our theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:38:27 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 23:15:23 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 03:30:18 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Yang", "Haibo", ""], ["Fang", "Minghong", ""], ["Liu", "Jia", ""]]}, {"id": "2101.11489", "submitter": "Bei  Wang", "authors": "Giuseppe Cerati, Peter Elmer, Brian Gravelle, Matti Kortelainen,\n  Vyacheslav Krutelyov, Steven Lantz, Mario Masciovecchio, Kevin McDermott,\n  Boyana Norris, Allison Reinsvold Hall, Micheal Reid, Daniel Riley, Matev\\v{z}\n  Tadel, Peter Wittich, Bei Wang, Frank W\\\"urthwein, and Avraham Yagil", "title": "Parallelizing the Unpacking and Clustering of Detector Data for\n  Reconstruction of Charged Particle Tracks on Multi-core CPUs and Many-core\n  GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results from parallelizing the unpacking and clustering steps of\nthe raw data from the silicon strip modules for reconstruction of charged\nparticle tracks. Throughput is further improved by concurrently processing\nmultiple events using nested OpenMP parallelism on CPU or CUDA streams on GPU.\nThe new implementation along with earlier work in developing a parallelized and\nvectorized implementation of the combinatoric Kalman filter algorithm has\nenabled efficient global reconstruction of the entire event on modern computer\narchitectures. We demonstrate the performance of the new implementation on\nIntel Xeon and NVIDIA GPU architectures.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:39:23 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Cerati", "Giuseppe", ""], ["Elmer", "Peter", ""], ["Gravelle", "Brian", ""], ["Kortelainen", "Matti", ""], ["Krutelyov", "Vyacheslav", ""], ["Lantz", "Steven", ""], ["Masciovecchio", "Mario", ""], ["McDermott", "Kevin", ""], ["Norris", "Boyana", ""], ["Hall", "Allison Reinsvold", ""], ["Reid", "Micheal", ""], ["Riley", "Daniel", ""], ["Tadel", "Matev\u017e", ""], ["Wittich", "Peter", ""], ["Wang", "Bei", ""], ["W\u00fcrthwein", "Frank", ""], ["Yagil", "Avraham", ""]]}, {"id": "2101.11653", "submitter": "Mahdi Soleymani", "authors": "Mahdi Soleymani, Ramy E. Ali, Hessam Mahdavifar, A. Salman Avestimehr", "title": "List-Decodable Coded Computing: Breaking the Adversarial Toleration\n  Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of coded computing where a computational task is\nperformed in a distributed fashion in the presence of adversarial workers. We\npropose techniques to break the adversarial toleration threshold barrier\npreviously known in coded computing. More specifically, we leverage\nlist-decoding techniques for folded Reed-Solomon (FRS) codes and propose novel\nalgorithms to recover the correct codeword using side information. In the coded\ncomputing setting, we show how the master node can perform certain carefully\ndesigned extra computations in order to obtain the side information. This side\ninformation will be then utilized to prune the output of list decoder in order\nto uniquely recover the true outcome. We further propose folded Lagrange coded\ncomputing, referred to as folded LCC or FLCC, to incorporate the developed\ntechniques into a specific coded computing setting. Our results show that FLCC\noutperforms LCC by breaking the barrier on the number of adversaries that can\nbe tolerated. In particular, the corresponding threshold in FLCC is improved by\na factor of two compared to that of LCC.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 19:17:33 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Soleymani", "Mahdi", ""], ["Ali", "Ramy E.", ""], ["Mahdavifar", "Hessam", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2101.11693", "submitter": "Mohammad Malekzadeh", "authors": "Mohammad Malekzadeh, Burak Hasircioglu, Nitish Mital, Kunal Katarya,\n  Mehmet Emre Ozfatura, Deniz G\\\"und\\\"uz", "title": "Dopamine: Differentially Private Federated Learning on Medical Data", "comments": "The Second AAAI Workshop on Privacy-Preserving Artificial\n  Intelligence (PPAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While rich medical datasets are hosted in hospitals distributed across the\nworld, concerns on patients' privacy is a barrier against using such data to\ntrain deep neural networks (DNNs) for medical diagnostics. We propose Dopamine,\na system to train DNNs on distributed datasets, which employs federated\nlearning (FL) with differentially-private stochastic gradient descent (DPSGD),\nand, in combination with secure aggregation, can establish a better trade-off\nbetween differential privacy (DP) guarantee and DNN's accuracy than other\napproaches. Results on a diabetic retinopathy~(DR) task show that Dopamine\nprovides a DP guarantee close to the centralized training counterpart, while\nachieving a better classification accuracy than FL with parallel DP where DPSGD\nis applied without coordination. Code is available at\nhttps://github.com/ipc-lab/private-ml-for-health.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 21:27:23 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 16:40:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Malekzadeh", "Mohammad", ""], ["Hasircioglu", "Burak", ""], ["Mital", "Nitish", ""], ["Katarya", "Kunal", ""], ["Ozfatura", "Mehmet Emre", ""], ["G\u00fcnd\u00fcz", "Deniz", ""]]}, {"id": "2101.11799", "submitter": "Kang Wei", "authors": "Kang Wei, Jun Li, Ming Ding, Chuan Ma, Yo-Seb Jeon and H. Vincent Poor", "title": "Covert Model Poisoning Against Federated Learning: Algorithm Design and\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL), as a type of distributed machine learning\nframeworks, is vulnerable to external attacks on FL models during parameters\ntransmissions. An attacker in FL may control a number of participant clients,\nand purposely craft the uploaded model parameters to manipulate system outputs,\nnamely, model poisoning (MP). In this paper, we aim to propose effective MP\nalgorithms to combat state-of-the-art defensive aggregation mechanisms (e.g.,\nKrum and Trimmed mean) implemented at the server without being noticed, i.e.,\ncovert MP (CMP). Specifically, we first formulate the MP as an optimization\nproblem by minimizing the Euclidean distance between the manipulated model and\ndesignated one, constrained by a defensive aggregation rule. Then, we develop\nCMP algorithms against different defensive mechanisms based on the solutions of\ntheir corresponding optimization problems. Furthermore, to reduce the\noptimization complexity, we propose low complexity CMP algorithms with a slight\nperformance degradation. In the case that the attacker does not know the\ndefensive aggregation mechanism, we design a blind CMP algorithm, in which the\nmanipulated model will be adjusted properly according to the aggregated model\ngenerated by the unknown defensive aggregation. Our experimental results\ndemonstrate that the proposed CMP algorithms are effective and substantially\noutperform existing attack mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:28:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wei", "Kang", ""], ["Li", "Jun", ""], ["Ding", "Ming", ""], ["Ma", "Chuan", ""], ["Jeon", "Yo-Seb", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2101.11856", "submitter": "Xiaopei Liu", "authors": "Yixin Chen, Wei Li, Rui Fan and Xiaopei Liu", "title": "GPU Optimization for High-Quality Kinetic Fluid Simulation", "comments": "16 pages, 25 figures, accepted by IEEE Transactions on Visualization\n  and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluid simulations are often performed using the incompressible Navier-Stokes\nequations (INSE), leading to sparse linear systems which are difficult to solve\nefficiently in parallel. Recently, kinetic methods based on the\nadaptive-central-moment multiple-relaxation-time (ACM-MRT) model have\ndemonstrated impressive capabilities to simulate both laminar and turbulent\nflows, with quality matching or surpassing that of state-of-the-art INSE\nsolvers. Furthermore, due to its local formulation, this method presents the\nopportunity for highly scalable implementations on parallel systems such as\nGPUs. However, an efficient ACM-MRT-based kinetic solver needs to overcome a\nnumber of computational challenges, especially when dealing with complex solids\ninside the fluid domain. In this paper, we present multiple novel GPU\noptimization techniques to efficiently implement high-quality ACM-MRT-based\nkinetic fluid simulations in domains containing complex solids. Our techniques\ninclude a new communication-efficient data layout, a load-balanced\nimmersed-boundary method, a multi-kernel launch method using a simplified\nformulation of ACM-MRT calculations to enable greater parallelism, and the\nintegration of these techniques into a parametric cost model to enable\nautomated parameter search to achieve optimal execution performance. We also\nextended our method to multi-GPU systems to enable large-scale simulations. To\ndemonstrate the state-of-the-art performance and high visual quality of our\nsolver, we present extensive experimental results and comparisons to other\nsolvers.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:02:15 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Chen", "Yixin", ""], ["Li", "Wei", ""], ["Fan", "Rui", ""], ["Liu", "Xiaopei", ""]]}, {"id": "2101.11896", "submitter": "Jiahuan Luo", "authors": "Xinle Liang, Yang Liu, Jiahuan Luo, Yuanqin He, Tianjian Chen, Qiang\n  Yang", "title": "Self-supervised Cross-silo Federated Neural Architecture Search", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) provides both model performance and data privacy for\nmachine learning tasks where samples or features are distributed among\ndifferent parties. In the training process of FL, no party has a global view of\ndata distributions or model architectures of other parties. Thus the\nmanually-designed architectures may not be optimal. In the past, Neural\nArchitecture Search (NAS) has been applied to FL to address this critical\nissue. However, existing Federated NAS approaches require prohibitive\ncommunication and computation effort, as well as the availability of\nhigh-quality labels. In this work, we present Self-supervised Vertical\nFederated Neural Architecture Search (SS-VFNAS) for automating FL where\nparticipants hold feature-partitioned data, a common cross-silo scenario called\nVertical Federated Learning (VFL). In the proposed framework, each party first\nconducts NAS using self-supervised approach to find a local optimal\narchitecture with its own data. Then, parties collaboratively improve the local\noptimal architecture in a VFL framework with supervision. We demonstrate\nexperimentally that our approach has superior performance, communication\nefficiency and privacy compared to Federated NAS and is capable of generating\nhigh-performance and highly-transferable heterogeneous architectures even with\ninsufficient overlapping samples, providing automation for those parties\nwithout deep learning expertise.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:57:30 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 02:23:50 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Liang", "Xinle", ""], ["Liu", "Yang", ""], ["Luo", "Jiahuan", ""], ["He", "Yuanqin", ""], ["Chen", "Tianjian", ""], ["Yang", "Qiang", ""]]}, {"id": "2101.12143", "submitter": "Donald Beaver", "authors": "Donald Rozinak Beaver", "title": "Security, Fault Tolerance, and Communication Complexity in Distributed\n  Systems", "comments": "PhD thesis, Harvard University, Cambridge, Massachusetts, USA, May\n  1990. Some chapters report joint work", "journal-ref": null, "doi": null, "report-no": "Harvard University Technical Report TR-24-90", "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present efficient and practical algorithms for a large, distributed system\nof processors to achieve reliable computations in a secure manner.\nSpecifically, we address the problem of computing a general function of several\nprivate inputs distributed among the processors of a network, while ensuring\nthe correctness of the results and the privacy of the inputs, despite\naccidental or malicious faults in the system. [...] Our algorithms maintain a\nlow cost in local processing time, are the first to achieve optimal levels of\nfault-tolerance, and most importantly, have low communication complexity. In\ncontrast to the best known previous methods, which require large numbers of\nrounds even for fairly simple computations, we devise protocols that use small\nmessages and a constant number of rounds regardless of the complexity of the\nfunction to be computed. Through direct algebraic approaches, we separate the\ncommunication complexity of secure computing from the computational complexity\nof the function to be computed. We examine security under both the modern\napproach of computational complexity-based cryptography and the classical\napproach of unconditional, information-theoretic security. We [...] support\nformal proofs of claims to security, addressing an important deficiency in the\nliterature. Our protocols are provably secure. In the realm of\ninformation-theoretic security, we characterize those functions which two\nparties can compute jointly with absolute privacy. We also characterize those\nfunctions which a weak processor can compute using the aid of powerful\nprocessors without having to reveal the instances of the problem it would like\nto solve. Our methods include a promising new technique called a locally random\nreduction, which has given rise not only to efficient solutions for many of the\nproblems considered in this work but to several powerful new results in\ncomplexity theory.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 17:51:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Beaver", "Donald Rozinak", ""]]}, {"id": "2101.12149", "submitter": "Andrew Myers", "authors": "A. Myers, A. Almgren, L. D. Amorim, J. Bell, L. Fedeli, L. Ge, K.\n  Gott, D. P. Grote, M. Hogan, A. Huebl, R. Jambunathan, R. Lehe, C. Ng, M.\n  Rowan, O. Shapoval, M. Th\\'evenet, J.-L. Vay, H. Vincenti, E. Yang, N.\n  Za\\\"im, W. Zhang, Y. Zhao, E. Zoni", "title": "Porting WarpX to GPU-accelerated platforms", "comments": "10 pages, 5 figures, submitted to Parallel Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC physics.acc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WarpX is a general purpose electromagnetic particle-in-cell code that was\noriginally designed to run on many-core CPU architectures. We describe the\nstrategy followed to allow WarpX to use the GPU-accelerated nodes on OLCF's\nSummit supercomputer, a strategy we believe will extend to the upcoming\nmachines Frontier and Aurora. We summarize the challenges encountered, lessons\nlearned, and give current performance results on a series of relevant benchmark\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:01:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Myers", "A.", ""], ["Almgren", "A.", ""], ["Amorim", "L. D.", ""], ["Bell", "J.", ""], ["Fedeli", "L.", ""], ["Ge", "L.", ""], ["Gott", "K.", ""], ["Grote", "D. P.", ""], ["Hogan", "M.", ""], ["Huebl", "A.", ""], ["Jambunathan", "R.", ""], ["Lehe", "R.", ""], ["Ng", "C.", ""], ["Rowan", "M.", ""], ["Shapoval", "O.", ""], ["Th\u00e9venet", "M.", ""], ["Vay", "J. -L.", ""], ["Vincenti", "H.", ""], ["Yang", "E.", ""], ["Za\u00efm", "N.", ""], ["Zhang", "W.", ""], ["Zhao", "Y.", ""], ["Zoni", "E.", ""]]}, {"id": "2101.12240", "submitter": "Nima Mohammadi", "authors": "Nima Mohammadi, Jianan Bai, Qiang Fan, Yifei Song, Yang Yi, Lingjia\n  Liu", "title": "Differential Privacy Meets Federated Learning under Communication\n  Constraints", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of federated learning systems is bottlenecked by\ncommunication costs and training variance. The communication overhead problem\nis usually addressed by three communication-reduction techniques, namely, model\ncompression, partial device participation, and periodic aggregation, at the\ncost of increased training variance. Different from traditional distributed\nlearning systems, federated learning suffers from data heterogeneity (since the\ndevices sample their data from possibly different distributions), which induces\nadditional variance among devices during training. Various variance-reduced\ntraining algorithms have been introduced to combat the effects of data\nheterogeneity, while they usually cost additional communication resources to\ndeliver necessary control information. Additionally, data privacy remains a\ncritical issue in FL, and thus there have been attempts at bringing\nDifferential Privacy to this framework as a mediator between utility and\nprivacy requirements. This paper investigates the trade-offs between\ncommunication costs and training variance under a resource-constrained\nfederated system theoretically and experimentally, and how communication\nreduction techniques interplay in a differentially private setting. The results\nprovide important insights into designing practical privacy-aware federated\nlearning systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:20:56 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Mohammadi", "Nima", ""], ["Bai", "Jianan", ""], ["Fan", "Qiang", ""], ["Song", "Yifei", ""], ["Yi", "Yang", ""], ["Liu", "Lingjia", ""]]}, {"id": "2101.12316", "submitter": "Nirupam Gupta", "authors": "Nirupam Gupta and Nitin H. Vaidya", "title": "Byzantine Fault-Tolerance in Peer-to-Peer Distributed Gradient-Descent", "comments": "34 pages, 1 figure. Closely related prior work: arXiv:2009.14763", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Byzantine fault-tolerance in the peer-to-peer\n(P2P) distributed gradient-descent method -- a prominent algorithm for\ndistributed optimization in a P2P system. In this problem, the system comprises\nof multiple agents, and each agent has a local cost function. In the fault-free\ncase, when all the agents are honest, the P2P distributed gradient-descent\nmethod allows all the agents to reach a consensus on a solution that minimizes\ntheir aggregate cost. However, we consider a scenario where a certain number of\nagents may be Byzantine faulty. Such faulty agents may not follow an algorithm\ncorrectly, and may share arbitrary incorrect information to prevent other\nnon-faulty agents from solving the optimization problem. In the presence of\nByzantine faulty agents, a more reasonable goal is to allow all the non-faulty\nagents to reach a consensus on a solution that minimizes the aggregate cost of\nall the non-faulty agents. We refer to this fault-tolerance goal as\n$f$-resilience where $f$ is the maximum number of Byzantine faulty agents in a\nsystem of $n$ agents, with $f < n$. Most prior work on fault-tolerance in P2P\ndistributed optimization only consider approximate fault-tolerance wherein,\nunlike $f$-resilience, all the non-faulty agents' compute a minimum point of a\nnon-uniformly weighted aggregate of their cost functions. We propose a\nfault-tolerance mechanism that confers provable $f$-resilience to the P2P\ndistributed gradient-descent method, provided the non-faulty agents satisfy the\nnecessary condition of $2f$-redundancy, defined later in the paper. Moreover,\ncompared to prior work, our algorithm is applicable to a larger class of\nhigh-dimensional convex distributed optimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 23:04:30 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Gupta", "Nirupam", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2101.12331", "submitter": "Sara Ghaemi", "authors": "Sara Ghaemi, Sara Rouhani, Rafael Belchior, Rui S. Cruz, Hamzeh\n  Khazaei, Petr Musilek", "title": "A Pub-Sub Architecture to Promote Blockchain Interoperability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The maturing of blockchain technology leads to heterogeneity, where multiple\nsolutions specialize in a particular use case. While the development of\ndifferent blockchain networks shows great potential for blockchains, the\nisolated networks have led to data and asset silos, limiting the applications\nof this technology. Blockchain interoperability solutions are essential to\nenable distributed ledgers to reach their full potential. Such solutions allow\nblockchains to support asset and data transfer, resulting in the development of\ninnovative applications.\n  This paper proposes a novel blockchain interoperability solution for\npermissioned blockchains based on the publish/subscribe architecture. We\nimplemented a prototype of this platform to show the feasibility of our design.\nWe evaluate our solution by implementing examples of the different publisher\nand subscriber networks, such as Hyperledger Besu, which is an Ethereum client,\nand two different versions of Hyperledger Fabric. We present a performance\nanalysis of the whole network that indicates its limits and bottlenecks.\nFinally, we discuss the extensibility and scalability of the platform in\ndifferent scenarios. Our evaluation shows that our system can handle a\nthroughput in the order of the hundreds of transactions per second.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 00:35:51 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Ghaemi", "Sara", ""], ["Rouhani", "Sara", ""], ["Belchior", "Rafael", ""], ["Cruz", "Rui S.", ""], ["Khazaei", "Hamzeh", ""], ["Musilek", "Petr", ""]]}, {"id": "2101.12682", "submitter": "Nazim Fat\\`es", "authors": "Nazim Fat\\`es and Ir\\`ene Marcovici and Siamak Taati", "title": "Self-stabilisation of cellular automata on tilings", "comments": "extended version of a paper published in the proceedings of\n  Reachability 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CG cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a finite set of local constraints, we seek a cellular automaton (i.e.,\na local and parallel algorithm) that self-stabilises on the configurations that\nsatisfy these constraints. More precisely, starting from a finite perturbation\nof a valid configuration, the cellular automaton must eventually fall back to\nthe space of valid configurations where it remains still. We allow the cellular\nautomaton to use extra symbols, but in that case, the extra symbols can also\nappear in the initial finite perturbation. For several classes of local\nconstraints (e.g., $k$-colourings with $k\\neq 3$, and North-East deterministic\nconstraints), we provide efficient self-stabilising cellular automata with or\nwithout additional symbols that wash out finite perturbations in linear or\nquadratic time, but also show that there are examples of local constraints for\nwhich the self-stabilisation problem is inherently hard. We also consider\nprobabilistic cellular automata rules and show that in some cases, the use of\nrandomness simplifies the problem. In the deterministic case, we show that if\nfinite perturbations are corrected in linear time, then the cellular automaton\nself-stabilises even starting from a random perturbation of a valid\nconfiguration, that is, when errors in the initial configuration occur\nindependently with a sufficiently low density.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 16:53:01 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Fat\u00e8s", "Nazim", ""], ["Marcovici", "Ir\u00e8ne", ""], ["Taati", "Siamak", ""]]}]