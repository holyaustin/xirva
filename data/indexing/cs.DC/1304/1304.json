[{"id": "1304.0012", "submitter": "Russell Power", "authors": "Russell Power", "title": "Using Memory-Protection to Simplify Zero-copy Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance networks (e.g. Infiniband) rely on zero-copy operations for\nperformance. Zero-copy operations, as the name implies, avoid copying buffers\nfor sending and receiving data. Instead, hardware devices directly read and\nwrite to application specified areas of memory. Since these networks can send\nand receive at nearly the same speed as the memory bus inside machines,\nzero-copy operations are necessary to achieve peak performance for many\napplications.\n  Unfortunately, programming with zero-copy APIs is a *giant pain*. Users must\ncarefully avoid using buffers that may be accessed by a device. Typically this\neither results in spaghetti code (where every access to a buffer is checked\nbefore usage), or blocking operations (which pretty much defeat the whole point\nof zero-copy).\n  We show that by abusing memory protection hardware, we can offer the best of\nboth worlds: a simple zero-copy mechanism which allows for non-blocking send\nand receives while protecting against incorrect accesses.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 20:03:47 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Power", "Russell", ""]]}, {"id": "1304.0267", "submitter": "Alexandre Goncalves", "authors": "Alexandre Domingues Goncalves, Lucia Maria Drummond, Artur Alves\n  Pessoa and Peter Hahn", "title": "Improving Lower Bounds for the Quadratic Assignment Problem by applying\n  a Distributed Dual Ascent Algorithm", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of the Reformulation Linearization Technique (RLT) to the\nQuadratic Assignment Problem (QAP) leads to a tight linear relaxation with huge\ndimensions that is hard to solve. Previous works found in the literature show\nthat these relaxations combined with branch-and-bound algorithms belong to the\nstate-of-the-art of exact methods for the QAP. For the level 3 RLT (RLT3),\nusing this relaxation is prohibitive in conventional machines for instances\nwith more than 22 locations due to memory limitations. This paper presents a\ndistributed version of a dual ascent algorithm for the RLT3 QAP relaxation that\napproximately solves it for instances with up to 30 locations for the first\ntime. Although, basically, the distributed algorithm has been implemented on\ntop of its sequential conterpart, some changes, which improved not only the\nparallel performance but also the quality of solutions, were proposed here.\nWhen compared to other lower bounding methods found in the literature, our\nalgorithm generates the best known lower bounds for 26 out of the 28 tested\ninstances, reaching the optimal solution in 18 of them.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 00:14:45 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Goncalves", "Alexandre Domingues", ""], ["Drummond", "Lucia Maria", ""], ["Pessoa", "Artur Alves", ""], ["Hahn", "Peter", ""]]}, {"id": "1304.0640", "submitter": "Louis-Charles Caron", "authors": "Louis-Charles Caron, \\and Michiel D'Haene, \\and Fr\\'ed\\'eric Mailhot,\n  \\and Benjamin Schrauwen, \\and Jean Rouat", "title": "Event management for large scale event-driven digital hardware spiking\n  neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2013.02.005", "report-no": null, "categories": "cs.NE cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in brain-like computation has led to the design of a plethora of\ninnovative neuromorphic systems. Individually, spiking neural networks (SNNs),\nevent-driven simulation and digital hardware neuromorphic systems get a lot of\nattention. Despite the popularity of event-driven SNNs in software, very few\ndigital hardware architectures are found. This is because existing hardware\nsolutions for event management scale badly with the number of events. This\npaper introduces the structured heap queue, a pipelined digital hardware data\nstructure, and demonstrates its suitability for event management. The\nstructured heap queue scales gracefully with the number of events, allowing the\nefficient implementation of large scale digital hardware event-driven SNNs. The\nscaling is linear for memory, logarithmic for logic resources and constant for\nprocessing time. The use of the structured heap queue is demonstrated on\nfield-programmable gate array (FPGA) with an image segmentation experiment and\na SNN of 65~536 neurons and 513~184 synapses. Events can be processed at the\nrate of 1 every 7 clock cycles and a 406$\\times$158 pixel image is segmented in\n200 ms.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 14:18:02 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Caron", "Louis-Charles", ""], ["D'Haene", "\\and Michiel", ""], ["Mailhot", "\\and Fr\u00e9d\u00e9ric", ""], ["Schrauwen", "\\and Benjamin", ""], ["Rouat", "\\and Jean", ""]]}, {"id": "1304.0681", "submitter": "Sergio Barrachina Mir", "authors": "H\\'ector Mart\\'inez (1), Joaqu\\'in T\\'arraga (2), Ignacio Medina (2),\n  Sergio Barrachina (1), Maribel Castillo (1), Joaqu\\'in Dopazo (2), Enrique S.\n  Quintana-Ort\\'i (1) ((1) Dpto. de Ingenier\\'ia y Ciencia de los Computadores,\n  Universidad Jaume I, Castell\\'on, Spain, (2) Computational Genomics\n  Institute, Centro de Investigaci\\'on Pr\\'incipe Felipe, Valencia, Spain)", "title": "Concurrent and Accurate RNA Sequencing on Multicore Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": "UJI ICC 2013-03-01", "categories": "q-bio.GN cs.DC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we introduce a novel parallel pipeline for fast and accurate\nmapping of RNA sequences on servers equipped with multicore processors. Our\nsoftware, named HPG-Aligner, leverages the speed of the Burrows-Wheeler\nTransform to map a large number of RNA fragments (reads) rapidly, as well as\nthe accuracy of the Smith-Waterman algorithm, that is employed to deal with\nconflictive reads. The aligner is complemented with a careful strategy to\ndetect splice junctions based on the division of RNA reads into short segments\n(or seeds), which are then mapped onto a number of candidate alignment\nlocations, providing useful information for the successful alignment of the\ncomplete reads.\n  Experimental results on platforms with AMD and Intel multicore processors\nreport the remarkable parallel performance of HPG-Aligner, on short and long\nRNA reads, which excels in both execution time and sensitivity to an\nstate-of-the-art aligner such as TopHat 2 built on top of Bowtie and Bowtie 2.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 16:34:36 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Mart\u00ednez", "H\u00e9ctor", ""], ["T\u00e1rraga", "Joaqu\u00edn", ""], ["Medina", "Ignacio", ""], ["Barrachina", "Sergio", ""], ["Castillo", "Maribel", ""], ["Dopazo", "Joaqu\u00edn", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""]]}, {"id": "1304.0872", "submitter": "David Doty", "authors": "David Doty", "title": "Timing in chemical reaction networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC cs.DS q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical reaction networks (CRNs) formally model chemistry in a well-mixed\nsolution. CRNs are widely used to describe information processing occurring in\nnatural cellular regulatory networks, and with upcoming advances in synthetic\nbiology, CRNs are a promising programming language for the design of artificial\nmolecular control circuitry. Due to a formal equivalence between CRNs and a\nmodel of distributed computing known as population protocols, results transfer\nreadily between the two models.\n  We show that if a CRN respects finite density (at most O(n) additional\nmolecules can be produced from n initial molecules), then starting from any\ndense initial configuration (all molecular species initially present have\ninitial count Omega(n), where n is the initial molecular count and volume),\nthen every producible species is produced in constant time with high\nprobability.\n  This implies that no CRN obeying the stated constraints can function as a\ntimer, able to produce a molecule, but doing so only after a time that is an\nunbounded function of the input size. This has consequences regarding an open\nquestion of Angluin, Aspnes, and Eisenstat concerning the ability of population\nprotocols to perform fast, reliable leader election and to simulate arbitrary\nalgorithms from a uniform initial state.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 08:50:30 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Doty", "David", ""]]}, {"id": "1304.0878", "submitter": "Ludovic Courtes", "authors": "Ludovic Court\\`es (INRIA Bordeaux - Sud-Ouest)", "title": "C Language Extensions for Hybrid CPU/GPU Programming with StarPU", "comments": null, "journal-ref": "N&deg; RR-8278 (2013)", "doi": null, "report-no": "RR-8278", "categories": "cs.MS cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern platforms used for high-performance computing (HPC) include machines\nwith both general-purpose CPUs, and \"accelerators\", often in the form of\ngraphical processing units (GPUs). StarPU is a C library to exploit such\nplatforms. It provides users with ways to define \"tasks\" to be executed on CPUs\nor GPUs, along with the dependencies among them, and by automatically\nscheduling them over all the available processing units. In doing so, it also\nrelieves programmers from the need to know the underlying architecture details:\nit adapts to the available CPUs and GPUs, and automatically transfers data\nbetween main memory and GPUs as needed. While StarPU's approach is successful\nat addressing run-time scheduling issues, being a C library makes for a poor\nand error-prone programming interface. This paper presents an effort started in\n2011 to promote some of the concepts exported by the library as C language\nconstructs, by means of an extension of the GCC compiler suite. Our main\ncontribution is the design and implementation of language extensions that map\nto StarPU's task programming paradigm. We argue that the proposed extensions\nmake it easier to get started with StarPU,eliminate errors that can occur when\nusing the C library, and help diagnose possible mistakes. We conclude on future\nwork.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 09:11:25 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2013 13:29:43 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Court\u00e8s", "Ludovic", "", "INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1304.1007", "submitter": "Jukka Suomela", "authors": "Mika G\\\"o\\\"os, Juho Hirvonen, Jukka Suomela", "title": "Linear-in-$\\Delta$ Lower Bounds in the LOCAL Model", "comments": "1 + 21 pages, 10 figures", "journal-ref": null, "doi": "10.1007/s00446-015-0245-8", "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By prior work, there is a distributed algorithm that finds a maximal\nfractional matching (maximal edge packing) in $O(\\Delta)$ rounds, where\n$\\Delta$ is the maximum degree of the graph. We show that this is optimal:\nthere is no distributed algorithm that finds a maximal fractional matching in\n$o(\\Delta)$ rounds.\n  Our work gives the first linear-in-$\\Delta$ lower bound for a natural graph\nproblem in the standard model of distributed computing---prior lower bounds for\na wide range of graph problems have been at best logarithmic in $\\Delta$.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 16:46:42 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["G\u00f6\u00f6s", "Mika", ""], ["Hirvonen", "Juho", ""], ["Suomela", "Jukka", ""]]}, {"id": "1304.1220", "submitter": "Petr  Kuznetsov", "authors": "Eli Gafni, Petr Kuznetsov, Ciprian Manolescu", "title": "A generalized asynchronous computability theorem", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the models of distributed computation defined as subsets of the\nruns of the iterated immediate snapshot model. Given a task $T$ and a model\n$M$, we provide topological conditions for $T$ to be solvable in $M$. When\napplied to the wait-free model, our conditions result in the celebrated\nAsynchronous Computability Theorem (ACT) of Herlihy and Shavit. To demonstrate\nthe utility of our characterization, we consider a task that has been shown\nearlier to admit only a very complex $t$-resilient solution. In contrast, our\ngeneralized computability theorem confirms its $t$-resilient solvability in a\nstraightforward manner.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 01:09:03 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2014 21:06:32 GMT"}, {"version": "v3", "created": "Tue, 20 May 2014 16:16:53 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Gafni", "Eli", ""], ["Kuznetsov", "Petr", ""], ["Manolescu", "Ciprian", ""]]}, {"id": "1304.1467", "submitter": "Reza Bosagh Zadeh", "authors": "Reza Bosagh Zadeh, Gunnar Carlsson", "title": "Dimension Independent Matrix Square using MapReduce", "comments": "arXiv admin note: text overlap with arXiv:1206.2082", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute the singular values of an $m \\times n$ sparse matrix $A$ in a\ndistributed setting, without communication dependence on $m$, which is useful\nfor very large $m$. In particular, we give a simple nonadaptive sampling scheme\nwhere the singular values of $A$ are estimated within relative error with\nconstant probability. Our proven bounds focus on the MapReduce framework, which\nhas become the de facto tool for handling such large matrices that cannot be\nstored or even streamed through a single machine.\n  On the way, we give a general method to compute $A^TA$. We preserve singular\nvalues of $A^TA$ with $\\epsilon$ relative error with shuffle size\n$O(n^2/\\epsilon^2)$ and reduce-key complexity $O(n/\\epsilon^2)$. We further\nshow that if only specific entries of $A^TA$ are required and $A$ has\nnonnegative entries, then we can reduce the shuffle size to $O(n \\log(n) / s)$\nand reduce-key complexity to $O(\\log(n)/s)$, where $s$ is the minimum cosine\nsimilarity for the entries being estimated. All of our bounds are independent\nof $m$, the larger dimension. We provide open-source implementations in Spark\nand Scalding, along with experiments in an industrial setting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 18:59:46 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 04:49:14 GMT"}, {"version": "v3", "created": "Wed, 22 Oct 2014 23:58:58 GMT"}, {"version": "v4", "created": "Thu, 24 Mar 2016 22:43:09 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Zadeh", "Reza Bosagh", ""], ["Carlsson", "Gunnar", ""]]}, {"id": "1304.1676", "submitter": "Sowmya Kamath S", "authors": "A Anji Reddy and S Sowmya Kamath", "title": "Research on Potential Semantic Web Service Discovery Mechanisms", "comments": "6 pages, International Conference on Recent Trends in Computer\n  Science and Engineering (ICRTCSE' 2012) May 3 - 4, 2012 Chennai, INDIA ISBN:\n  978-81-9089-807-2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Web services is an important paradigm in distributed application\ndevelopment. Currently, many businesses are seeking to convert their\napplications into web services because of its ability to promote\ninter-operability among applications. As a number of web services increase, the\nprocess of discovering appropriate web services for consumption from user's\nperspective gains importance. In this paper, we present a study of potential\nways of discovering web services and issues related to each of them. In\naddition, we discuss ontology concepts and related technologies, which\nincorporate semantic meaning and hence give domain knowledge about a web\nservice to improve the discovery mechanism. The paper also presents an overview\nof related research work, identifying metrics useful in filtering web service\nsearch mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 11:04:26 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Reddy", "A Anji", ""], ["Kamath", "S Sowmya", ""]]}, {"id": "1304.1831", "submitter": "David Gamarnik", "authors": "David Gamarnik and Madhu Sudan", "title": "Limits of local algorithms over sparse random graphs", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local algorithms on graphs are algorithms that run in parallel on the nodes\nof a graph to compute some global structural feature of the graph. Such\nalgorithms use only local information available at nodes to determine local\naspects of the global structure, while also potentially using some randomness.\nRecent research has shown that such algorithms show significant promise in\ncomputing structures like large independent sets in graphs locally. Indeed the\npromise led to a conjecture by Hatami, \\Lovasz and Szegedy\n\\cite{HatamiLovaszSzegedy} that local algorithms may be able to compute maximum\nindependent sets in (sparse) random $d$-regular graphs. In this paper we refute\nthis conjecture and show that every independent set produced by local\nalgorithms is multiplicative factor $1/2+1/(2\\sqrt{2})$ smaller than the\nlargest, asymptotically as $d\\rightarrow\\infty$.\n  Our result is based on an important clustering phenomena predicted first in\nthe literature on spin glasses, and recently proved rigorously for a variety of\nconstraint satisfaction problems on random graphs. Such properties suggest that\nthe geometry of the solution space can be quite intricate. The specific\nclustering property, that we prove and apply in this paper shows that typically\nevery two large independent sets in a random graph either have a significant\nintersection, or have a nearly empty intersection. As a result, large\nindependent sets are clustered according to the proximity to each other. While\nthe clustering property was postulated earlier as an obstruction for the\nsuccess of local algorithms, such as for example, the Belief Propagation\nalgorithm, our result is the first one where the clustering property is used to\nformally prove limits on local algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 22:36:46 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Gamarnik", "David", ""], ["Sudan", "Madhu", ""]]}, {"id": "1304.1838", "submitter": "Jagan Sankaranarayanan", "authors": "Jeff LeFevre, Jagan Sankaranarayanan, Hakan Hacigumus, Junichi\n  Tatemura, Neoklis Polyzotis", "title": "Towards a Workload for Evolutionary Analytics", "comments": "10 pages", "journal-ref": "DanaC: Workshop on Data analytics in the Cloud, June 2013, New\n  York, NY", "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Emerging data analysis involves the ingestion and exploration of new data\nsets, application of complex functions, and frequent query revisions based on\nobserving prior query answers. We call this new type of analysis evolutionary\nanalytics and identify its properties. This type of analysis is not well\nrepresented by current benchmark workloads. In this paper, we present a\nworkload and identify several metrics to test system support for evolutionary\nanalytics. Along with our metrics, we present methodologies for running the\nworkload that capture this analytical scenario.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 00:26:41 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2013 07:00:10 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2013 19:19:22 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["LeFevre", "Jeff", ""], ["Sankaranarayanan", "Jagan", ""], ["Hacigumus", "Hakan", ""], ["Tatemura", "Junichi", ""], ["Polyzotis", "Neoklis", ""]]}, {"id": "1304.1863", "submitter": "Yongkun Li", "authors": "Yongkun Li, Patrick P.C. Lee, John C.S. Lui", "title": "Stochastic Analysis on RAID Reliability for Solid-State Drives", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solid-state drives (SSDs) have been widely deployed in desktops and data\ncenters. However, SSDs suffer from bit errors, and the bit error rate is time\ndependent since it increases as an SSD wears down. Traditional storage systems\nmainly use parity-based RAID to provide reliability guarantees by striping\nredundancy across multiple devices, but the effectiveness of RAID in SSDs\nremains debatable as parity updates aggravate the wearing and bit error rates\nof SSDs. In particular, an open problem is that how different parity\ndistributions over multiple devices, such as the even distribution suggested by\nconventional wisdom, or uneven distributions proposed in recent RAID schemes\nfor SSDs, may influence the reliability of an SSD RAID array. To address this\nfundamental problem, we propose the first analytical model to quantify the\nreliability dynamics of an SSD RAID array. Specifically, we develop a\n\"non-homogeneous\" continuous time Markov chain model, and derive the transient\nreliability solution. We validate our model via trace-driven simulations and\nconduct numerical analysis to provide insights into the reliability dynamics of\nSSD RAID arrays under different parity distributions and subject to different\nbit error rates and array configurations. Designers can use our model to decide\nthe appropriate parity distribution based on their reliability requirements.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 07:48:02 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Li", "Yongkun", ""], ["Lee", "Patrick P. C.", ""], ["Lui", "John C. S.", ""]]}, {"id": "1304.1902", "submitter": "Pierre-Malo Deni\\'elou", "authors": "Pierre-Malo Deni\\'elou and Nobuko Yoshida", "title": "Multiparty Compatibility in Communicating Automata: Characterisation and\n  Synthesis of Global Session Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiparty session types are a type system that can ensure the safety and\nliveness of distributed peers via the global specification of their\ninteractions. To construct a global specification from a set of distributed\nuncontrolled behaviours, this paper explores the problem of fully\ncharacterising multiparty session types in terms of communicating automata. We\nequip global and local session types with labelled transition systems (LTSs)\nthat faithfully represent asynchronous communications through unbounded\nbuffered channels. Using the equivalence between the two LTSs, we identify a\nclass of communicating automata that exactly correspond to the projected local\ntypes. We exhibit an algorithm to synthesise a global type from a collection of\ncommunicating automata. The key property of our findings is the notion of\nmultiparty compatibility which non-trivially extends the duality condition for\nbinary session types.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 15:22:12 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Deni\u00e9lou", "Pierre-Malo", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "1304.1913", "submitter": "Carlo Spaccasassi Mr", "authors": "Carlo Spaccasassi and Vasileios Koutavas", "title": "Towards Efficient Abstractions for Concurrent Consensus", "comments": "15 pages, 5 figures, symposium: TFP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus is an often occurring problem in concurrent and distributed\nprogramming. We present a programming language with simple semantics and\nbuild-in support for consensus in the form of communicating transactions. We\nmotivate the need for such a construct with a characteristic example of\ngeneralized consensus which can be naturally encoded in our language. We then\nfocus on the challenges in achieving an implementation that can efficiently run\nsuch programs. We setup an architecture to evaluate different implementation\nalternatives and use it to experimentally evaluate runtime heuristics. This is\nthe basis for a research project on realistic programming language support for\nconsensus.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 17:00:00 GMT"}, {"version": "v2", "created": "Tue, 7 May 2013 16:47:24 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Spaccasassi", "Carlo", ""], ["Koutavas", "Vasileios", ""]]}, {"id": "1304.1966", "submitter": "Roberto Capuzzo-Dolcetta", "authors": "Roberto Capuzzo-Dolcetta and Mario Spera (Dep. of Physics, Sapienza,\n  Universit\\`a di Roma, Italy)", "title": "A Performance Comparison of Different Graphics Processing Units Running\n  Direct N-Body Simulations", "comments": "This paper has been submitted for publication to Computer Physics\n  Communications It consists of 26 pages, with 6 tables and 10 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2013.07.005", "report-no": null, "categories": "astro-ph.IM cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid computational architectures based on the joint power of Central\nProcessing Units and Graphic Processing Units (GPUs) are becoming popular and\npowerful hardware tools for a wide range of simulations in biology, chemistry,\nengineering, physics, etc..\n  In this paper we present a comparison of performance of various GPUs\navailable on market when applied to the numerical integration of the classic,\ngravitational, N-body problem. To do this, we developed an OpenCL version of\nthe parallel code (HiGPUs) to use for these tests, because this version is the\nonly apt to work on GPUs of different makes.\n  The main general result is that we confirm the reliability, speed and\ncheapness of GPUs when applied to the examined kind of problems (i.e. when the\nforces to evaluate are dependent on the mutual distances, as it happens in\ngravitational physics and molecular dynamics). More specifically, we find that\nalso the cheap GPUs built to be employed just for gaming applications are very\nperformant in terms of computing speed also in scientific applications and,\nalthough with some limitations in central memory and in bandwidth, can be a\ngood choice to implement a machine for scientific use at a very good\nperformance to cost ratio.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 08:08:51 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Capuzzo-Dolcetta", "Roberto", "", "Dep. of Physics, Sapienza,\n  Universit\u00e0 di Roma, Italy"], ["Spera", "Mario", "", "Dep. of Physics, Sapienza,\n  Universit\u00e0 di Roma, Italy"]]}, {"id": "1304.2302", "submitter": "Jonathan Malmaud", "authors": "Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka", "title": "ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process\n  Mixtures", "comments": "12 pages, 10 figures. Submitted to ICML 2013 during third submission\n  cycle", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian\nnonparametric modeling, and is widely used in tasks such as density estimation,\nnatural language processing, and time series modeling. Although MCMC inference\nmethods for the DP often provide a gold standard in terms asymptotic accuracy,\nthey can be computationally expensive and are not obviously parallelizable. We\npropose a reparameterization of the Dirichlet process that induces conditional\nindependencies between the atoms that form the random measure. This conditional\nindependence enables many of the Markov chain transition operators for DP\ninference to be simulated in parallel across multiple cores. Applied to mixture\nmodeling, our approach enables the Dirichlet process to simultaneously learn\nclusters that describe the data and superclusters that define the granularity\nof parallelization. Unlike previous approaches, our technique does not require\nalteration of the model and leaves the true posterior distribution invariant.\nIt also naturally lends itself to a distributed software implementation in\nterms of Map-Reduce, which we test in cluster configurations of over 50\nmachines and 100 cores. We present experiments exploring the parallel\nefficiency and convergence properties of our approach on both synthetic and\nreal-world data, including runs on 1MM data vectors in 256 dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 18:34:32 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Lovell", "Dan", ""], ["Malmaud", "Jonathan", ""], ["Adams", "Ryan P.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1304.2550", "submitter": "Daniel Merkle", "authors": "Felix P. Hargreaves and Daniel Merkle", "title": "FooPar: A Functional Object Oriented Parallel Framework in Scala", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FooPar, an extension for highly efficient Parallel Computing in\nthe multi-paradigm programming language Scala. Scala offers concise and clean\nsyntax and integrates functional programming features. Our framework FooPar\ncombines these features with parallel computing techniques. FooPar is designed\nmodular and supports easy access to different communication backends for\ndistributed memory architectures as well as high performance math libraries. In\nthis article we use it to parallelize matrix matrix multiplication and show its\nscalability by a isoefficiency analysis. In addition, results based on a\nempirical analysis on two supercomputers are given. We achieve close-to-optimal\nperformance wrt. theoretical peak performance. Based on this result we conclude\nthat FooPar allows to fully access Scala's design features without suffering\nfrom performance drops when compared to implementations purely based on C and\nMPI.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 12:23:47 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2013 14:17:20 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Hargreaves", "Felix P.", ""], ["Merkle", "Daniel", ""]]}, {"id": "1304.2617", "submitter": "Stefano Ferretti Stefano Ferretti", "authors": "Stefano Ferretti", "title": "Resilience of Dynamic Overlays through Local Interactions", "comments": "A version of this paper is published in the Proceedings of 5th\n  International Workshop on Simplifying Complex Networks for Pratictioners\n  (SIMPLEX 2013) - World Wide Web Conference (WWW 2013), ACM, Rio de Janeiro\n  (Brazil), May 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a self-organizing protocol for dynamic (unstructured P2P)\noverlay networks, which allows to react to the variability of node arrivals and\ndepartures. Through local interactions, the protocol avoids that the departure\nof nodes causes a partitioning of the overlay. We show that it is sufficient to\nhave knowledge about 1st and 2nd neighbours, plus a simple interaction P2P\nprotocol, to make unstructured networks resilient to node faults. A simulation\nassessment over different kinds of overlay networks demonstrates the viability\nof the proposal.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 14:43:02 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Ferretti", "Stefano", ""]]}, {"id": "1304.2840", "submitter": "Ivan Rodero", "authors": "Ivan Rodero and Manish Parashar", "title": "Cross-layer Application-aware Power/Energy Management for Extreme Scale\n  Science", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Performance Computing (HPC) has evolved over the past decades into\nincreasingly complex and powerful systems. Current HPC systems consume several\nMWs of power, enough to power small towns, and are in fact soon approaching the\nlimits of the power available to them. Estimates are with the given current\ntechnology, achieving exascale will require hundreds of MW, which is not\nfeasible from multiple perspectives. Architecture and technology researchers\nare aggressively addressing this; however as past history is shown, innovation\nat these levels are not sufficient and have to be accompanied with innovations\nat higher levels (algorithms, programming, runtime, OS) to achieve the multiple\norders of magnitude reduction - i.e., a comprehensive cross-layer and\napplication-aware strategy is required. Furthermore, energy/power-efficiency\nhas to be addressed in combination with quality of solutions, performance and\nreliability and other objectives and appropriate tradeoffs are required.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 04:06:30 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Rodero", "Ivan", ""], ["Parashar", "Manish", ""]]}, {"id": "1304.2936", "submitter": "Chinmay Narayan", "authors": "Chinmay Narayan, Shibashis Guha, S.Arun-Kumar", "title": "Inferring Fences in a Concurrent Program Using SC proof of Correctness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most proof systems for concurrent programs assume the underlying memory model\nto be sequentially consistent (SC), an assumption which does not hold for\nmodern multicore processors. These processors, for performance reasons,\nimplement relaxed memory models. As a result of this relaxation a program,\nproved correct on the SC memory model, might execute incorrectly. To ensure its\ncorrectness under relaxation, fence instructions are inserted in the code. In\nthis paper we show that the SC proof of correctness of an algorithm, carried\nout in the proof system of [Sou84], identifies per-thread instruction orderings\nsufficient for this SC proof. Further, to correctly execute this algorithm on\nan underlying relaxed memory model it is sufficient to respect only these\norderings by inserting fence instructions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 12:34:37 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Narayan", "Chinmay", ""], ["Guha", "Shibashis", ""], ["Arun-Kumar", "S.", ""]]}, {"id": "1304.2981", "submitter": "Sultan Ullah", "authors": "Sultan Ullah and Zheng Xuefeng", "title": "Cloud Computing: a Prologue", "comments": "04 Pages", "journal-ref": "International Journal of Advanced Research in Computer and\n  Communication Engineering Vol. 1, No. 1, March, 2012", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging internet based super computing model is represented by cloud\ncomputing. Cloud computing is the convergence and evolution of several concepts\nfrom virtualization, distributed storage, grid, and automation management to\nenable a more flexible approach for deploying and scaling applications.\nHowever, cloud computing moves the application software and databases to the\nlarge data centers, where the management of the data and services may not be\nfully trustworthy. The concept of cloud computing on the basis of the various\ndefinitions available in the industry and the characteristics of cloud\ncomputing are being analyzed in this paper. The paper also describes the main\ncloud service providers and their products followed by primary cloud computing\noperating systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 14:45:47 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2013 01:48:59 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Ullah", "Sultan", ""], ["Xuefeng", "Zheng", ""]]}, {"id": "1304.3134", "submitter": "Sultan Ullah", "authors": "Sultan Ullah, Zheng Xuefeng, Zhou Feng, Zhao Haichun", "title": "TCLOUD: Challenges and Best Practices for Cloud Computing", "comments": null, "journal-ref": "International Journal of Engineering Research and Technology Vol.\n  1 (09), 2012", "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has achieved an unbelievable adoption response rate but still\nits infancy stage is not over. It is an emerging paradigm and amazingly gaining\npopularity. The size of the market shared of the applications provided by cloud\ncomputing is still not much behind the expectations. It provides the\norganizations with great potential to minimize the cost and maximizes the\noverall operating effectiveness of computing required by an organization.\nDespite its growing popularity, still it is faced with security, privacy, and\nportability issues, which in one or the other way create hurdles in the fast\nacceptance of this new technology for the computing community. This paper\nprovides a concise all around analysis of the challenges faced by cloud\ncomputing community and also presents the solutions available to these\nchallenges.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 20:12:19 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Ullah", "Sultan", ""], ["Xuefeng", "Zheng", ""], ["Feng", "Zhou", ""], ["Haichun", "Zhao", ""]]}, {"id": "1304.3162", "submitter": "Santosh Vempala", "authors": "Ravindran Kannan and Santosh Vempala and David Woodruff", "title": "Principal Component Analysis and Higher Correlations for Distributed\n  Data", "comments": "rewritten with focus on two main results (distributed PCA,\n  higher-order moments and correlations) in the arbitrary partition model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider algorithmic problems in the setting in which the input data has\nbeen partitioned arbitrarily on many servers. The goal is to compute a function\nof all the data, and the bottleneck is the communication used by the algorithm.\nWe present algorithms for two illustrative problems on massive data sets: (1)\ncomputing a low-rank approximation of a matrix $A=A^1 + A^2 + \\ldots + A^s$,\nwith matrix $A^t$ stored on server $t$ and (2) computing a function of a vector\n$a_1 + a_2 + \\ldots + a_s$, where server $t$ has the vector $a_t$; this\nincludes the well-studied special case of computing frequency moments and\nseparable functions, as well as higher-order correlations such as the number of\nsubgraphs of a specified type occurring in a graph. For both problems we give\nalgorithms with nearly optimal communication, and in particular the only\ndependence on $n$, the size of the data, is in the number of bits needed to\nrepresent indices and words ($O(\\log n)$).\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 23:05:01 GMT"}, {"version": "v2", "created": "Mon, 20 May 2013 09:51:00 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2013 13:54:22 GMT"}, {"version": "v4", "created": "Sun, 29 Jun 2014 13:42:24 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Kannan", "Ravindran", ""], ["Vempala", "Santosh", ""], ["Woodruff", "David", ""]]}, {"id": "1304.3203", "submitter": "Sultan Ullah", "authors": "Sultan Ullah, Zheng Xuefeng", "title": "Cloud Computing Research Challenges", "comments": "IEEE 5th International Conference on BioMedical Engineering and\n  Informatics (BMEI 2012)", "journal-ref": "IEEE 5th International Conference on BioMedical Engineering and\n  Informatics (BMEI 2012), PP 1397 - 1401", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times cloud computing has appeared as a new model for hosting and\nconveying services over the Internet. This model is striking to business\nvendors as it eradicates the requirement for users to plan in advance, and it\npermits the organization to start from low level and then add more resources\nonly if there is an increase in the service demand. Even though cloud computing\npresents greater opportunities not only to information technology industry, but\nevery organization involved in utilizing the computing in one way or the other,\nit is still in infancy with many problems to be fixed. The paper discusses\nresearch challenges in cloud computing.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 06:10:12 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Ullah", "Sultan", ""], ["Xuefeng", "Zheng", ""]]}, {"id": "1304.3396", "submitter": "Ankit Mundra", "authors": "Ankit Mundra, Bhagvan K. Gupta, Geetanjali Rathee, Meenu Chawla, Nitin\n  Rakesh, Vipin Tyagi", "title": "Validated Real Time Middle Ware For Distributed Cyber Physical Systems\n  Using HMM", "comments": "11 Pages, 4 figures", "journal-ref": "International Journal of Distributed and Parallel Systems (IJDPS)\n  Vol.4, No.2, March 2013", "doi": "10.5121/ijdps.2013.4204", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Cyber Physical Systems designed for different scenario must be\ncapable enough to perform in an efficient manner in every situation. Earlier\napproaches, such as CORBA, has performed but with different time constraints.\nTherefore, there was the need to design reconfigurable, robust, validated and\nconsistent real time middle ware systems with end-to-end timing. In the\nDCPS-HMM we have proposed the processor efficiency and data validation which\nmay proof crucial in implementing various distributed systems such as credit\ncard systems or file transfer through network.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 19:09:42 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Mundra", "Ankit", ""], ["Gupta", "Bhagvan K.", ""], ["Rathee", "Geetanjali", ""], ["Chawla", "Meenu", ""], ["Rakesh", "Nitin", ""], ["Tyagi", "Vipin", ""]]}, {"id": "1304.3550", "submitter": "Gagan Dua", "authors": "Gagan Dua, Nitin Gautam, Dharmendar Sharma, Ankit Arora", "title": "Replay Attack Prevention in Kerberos Authentication Protocol Using\n  Triple Password", "comments": "12 pages, 2 Figures, 2 Tables", "journal-ref": "International Journal of Computer Networks & Communications\n  (IJCNC) Vol.5, No.2, March 2013", "doi": "10.5121/ijcnc.2013.5205", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replay attack and password attacks are serious issues in the Kerberos\nauthentication protocol. Many ideas have been proposed to prevent these attacks\nbut they increase complexity of the total Kerberos environment. In this paper\nwe present an improved method which prevents replay attacks and password\nattacks by using Triple password scheme. Three passwords are stored on\nAuthentication Server and Authentication Server sends two passwords to Ticket\nGranting Server (one for Application Server) by encrypting with the secret key\nshared between Authentication server and Ticket Granting server.\nSimilarly,Ticket Granting Server sends one password to Application Server by\nencrypting with the secret key shared between TGS and application server.\nMeanwhile, Service-Granting-Ticket is transferred to users by encrypting it\nwith the password that TGS just received from AS. It helps to prevent Replay\nattack.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 06:49:23 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Dua", "Gagan", ""], ["Gautam", "Nitin", ""], ["Sharma", "Dharmendar", ""], ["Arora", "Ankit", ""]]}, {"id": "1304.3978", "submitter": "Manan Shah", "authors": "Manan D. Shah, Harshad B. Prajapati", "title": "Reallocation and Allocation of Virtual Machines in Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has given the new face to the distributed field. Two main\nissues are discussed in this paper, (I) the process of finding the efficient\nvirtual machine by using the concept of load balancing algorithm. (II)\nReallocation of the Virtual Machines i.e. migration of the Virtual Machines\nwhen cloud provider is not available with the required Virtual Machines. We\nhave discussed about the different load balancing algorithms which are used for\ndeciding the efficient Virtual Machine for the allocation to the client on\ndemand. While in the second issue is concern we have discuss about different\nmodules available for the migration of Virtual Machines from one source machine\nto the other target machine. At last discussion about the different simulators\navailable for the cloud are carried out in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 05:02:16 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Shah", "Manan D.", ""], ["Prajapati", "Harshad B.", ""]]}, {"id": "1304.3980", "submitter": "Deepakbhai Chhaganbhai vegda", "authors": "Deepak.c.vegda, Harshad.B.Prajapati", "title": "Scheduling of Dependent Tasks Application using Random Search Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since beginning of Grid computing, scheduling of dependent tasks application\nhas attracted attention of researchers due to NP-Complete nature of the\nproblem. In Grid environment, scheduling is deciding about assignment of tasks\nto available resources. Scheduling in Grid is challenging when the tasks have\ndependencies and resources are heterogeneous. The main objective in scheduling\nof dependent tasks is minimizing make-span. Due to NP-complete nature of\nscheduling problem, exact solutions cannot generate schedule efficiently.\nTherefore, researchers apply heuristic or random search techniques to get\noptimal or near to optimal solution of such problems. In this paper, we show\nhow Genetic Algorithm can be used to solve dependent task scheduling problem.\nWe describe how initial population can be generated using random assignment and\nheight based approaches. We also present design of crossover and mutation\noperators to enable scheduling of dependent tasks application without violating\ndependency constraints. For implementation of GA based scheduling, we explore\nand analyze SimGrid and GridSim simulation toolkits. From results, we found\nthat SimGrid is suitable, as it has support of SimDag API for DAG applications.\nWe found that GA based approach can generate schedule for dependent tasks\napplication in reasonable time while trying to minimize make-span.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 05:04:31 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["vegda", "Deepak. c.", ""], ["Prajapati", "Harshad. B.", ""]]}, {"id": "1304.3992", "submitter": "Thara  Nair", "authors": "K. Phani Tejaswi, D. Shanmukha Rao, Thara Nair, A. V. V. Prasad", "title": "GPU Acclerated Automated Feature Extraction from Satellite Images", "comments": null, "journal-ref": "International Journal of Distributed and Parallel Systems (IJDPS)\n  Vol.4, No.2, March 2013", "doi": "10.5121/ijdps.2013.4201", "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large volumes of remote sensing data insists on higher\ndegree of automation in feature extraction, making it a need of the hour.The\nhuge quantum of data that needs to be processed entails accelerated processing\nto be enabled.GPUs, which were originally designed to provide efficient\nvisualization, are being massively employed for computation intensive parallel\nprocessing environments. Image processing in general and hence automated\nfeature extraction, is highly computation intensive, where performance\nimprovements have a direct impact on societal needs. In this context, an\nalgorithm has been formulated for automated feature extraction from a\npanchromatic or multispectral image based on image processing techniques. Two\nLaplacian of Guassian (LoG) masks were applied on the image individually\nfollowed by detection of zero crossing points and extracting the pixels based\non their standard deviation with the surrounding pixels. The two extracted\nimages with different LoG masks were combined together which resulted in an\nimage with the extracted features and edges. Finally the user is at liberty to\napply the image smoothing step depending on the noise content in the extracted\nimage. The image is passed through a hybrid median filter to remove the salt\nand pepper noise from the image. This paper discusses the aforesaid algorithm\nfor automated feature extraction, necessity of deployment of GPUs for the same;\nsystem-level challenges and quantifies the benefits of integrating GPUs in such\nenvironment. The results demonstrate that substantial enhancement in\nperformance margin can be achieved with the best utilization of GPU resources\nand an efficient parallelization strategy. Performance results in comparison\nwith the conventional computing scenario have provided a speedup of 20x, on\nrealization of this parallelizing strategy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 06:03:19 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Tejaswi", "K. Phani", ""], ["Rao", "D. Shanmukha", ""], ["Nair", "Thara", ""], ["Prasad", "A. V. V.", ""]]}, {"id": "1304.4002", "submitter": "Manik Lal Das", "authors": "Harsh N Thakker, Mayank Saha, Manik Lal Das", "title": "Reputation Algebra for Cloud-based Anonymous Data Storage Systems", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Given a cloud-based anonymous data storage system, there are two ways for\nmanaging the nodes involved in file transfers. One of them is using reputations\nand the other uses a micropayment system. In reputation-based approach, each\nnode has a reputation associated with it, which is used as a currency or\nfeedback collection for file exchange operations. There have been several\nattempts over the years to develop a strong and efficient reputation system\nthat provides credibility, fairness, and accountability. One such attempt was\nthe Free Haven Project that provides a strong foundation for cloud-based\nanonymous data storage systems. The work proposed in this paper is motivated by\nthe Free Haven Project aimed at developing a reputation system that facilitates\ndynamic operations such as adding servers, removing servers and changing role\nof authorities. The proposed system also provides algorithm for scoring and\nmaintaining reputations of the servers in order to achieve credibility,\naccountability and fairness.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 07:04:32 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Thakker", "Harsh N", ""], ["Saha", "Mayank", ""], ["Das", "Manik Lal", ""]]}, {"id": "1304.4326", "submitter": "Himanshu Chauhan", "authors": "Himanshu Chauhan, Vijay K. Garg, Aravind Natarajan, Neeraj Mittal", "title": "Distributed Abstraction Algorithm for Online Predicate Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing a distributed computation is a hard problem in general due to the\ncombinatorial explosion in the size of the state-space with the number of\nprocesses in the system. By abstracting the computation, unnecessary\nexplorations can be avoided. Computation slicing is an approach for abstracting\ndis- tributed computations with respect to a given predicate. We focus on\nregular predicates, a family of predicates that covers a large number of\ncommonly used predicates for runtime verification. The existing algorithms for\ncomputation slicing are centralized in nature in which a single process is\nresponsible for computing the slice in either offline or online manner. In this\npaper, we present a distributed online algorithm for computing the slice of a\ndistributed computation with respect to a regular predicate. Our algorithm\ndistributes the work and storage requirements across the system, thus reducing\nthe space and computation complexities per process. In addition, for\nconjunctive predicates, our algorithm also reduces the message load per\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 03:56:24 GMT"}, {"version": "v2", "created": "Fri, 31 May 2013 04:03:28 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2013 06:23:50 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Chauhan", "Himanshu", ""], ["Garg", "Vijay K.", ""], ["Natarajan", "Aravind", ""], ["Mittal", "Neeraj", ""]]}, {"id": "1304.4453", "submitter": "Christian Lorenz Staudt", "authors": "Christian L. Staudt and Henning Meyerhenke", "title": "Engineering Parallel Algorithms for Community Detection in Massive\n  Networks", "comments": "14 pages, first presented at the 2013 International Conference on\n  Parallel Processing, revised accepted for IEEE Transactions on Parallel and\n  Distributed Systems (TPDS)", "journal-ref": null, "doi": "10.1109/TPDS.2015.2390633", "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of graph-structured data has recently experienced an enormous\ngrowth in many applications. To transform such data into useful information,\nfast analytics algorithms and software tools are necessary. One common graph\nanalytics kernel is disjoint community detection (or graph clustering). Despite\nextensive research on heuristic solvers for this task, only few parallel codes\nexist, although parallelism will be necessary to scale to the data volume of\nreal-world applications. We address the deficit in computing capability by a\nflexible and extensible community detection framework with shared-memory\nparallelism. Within this framework we design and implement efficient parallel\ncommunity detection heuristics: A parallel label propagation scheme; the first\nlarge-scale parallelization of the well-known Louvain method, as well as an\nextension of the method adding refinement; and an ensemble scheme combining the\nabove. In extensive experiments driven by the algorithm engineering paradigm,\nwe identify the most successful parameters and combinations of these\nalgorithms. We also compare our implementations with state-of-the-art\ncompetitors. The processing rate of our fastest algorithm often reaches 50M\nedges/second. We recommend the parallel Louvain method and our variant with\nrefinement as both qualitatively strong and fast. Our methods are suitable for\nmassive data sets with billions of edges.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 14:03:38 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 11:55:40 GMT"}, {"version": "v3", "created": "Fri, 25 Jul 2014 13:47:47 GMT"}, {"version": "v4", "created": "Mon, 2 Feb 2015 13:27:38 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Staudt", "Christian L.", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1304.4519", "submitter": "David Doty", "authors": "David Doty and Monir Hajiaghayi", "title": "Leaderless deterministic chemical reaction networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1204.4176", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC cs.DS q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper answers an open question of Chen, Doty, and Soloveichik [1], who\nshowed that a function f:N^k --> N^l is deterministically computable by a\nstochastic chemical reaction network (CRN) if and only if the graph of f is a\nsemilinear subset of N^{k+l}. That construction crucially used \"leaders\": the\nability to start in an initial configuration with constant but non-zero counts\nof species other than the k species X_1,...,X_k representing the input to the\nfunction f. The authors asked whether deterministic CRNs without a leader\nretain the same power.\n  We answer this question affirmatively, showing that every semilinear function\nis deterministically computable by a CRN whose initial configuration contains\nonly the input species X_1,...,X_k, and zero counts of every other species. We\nshow that this CRN completes in expected time O(n), where n is the total number\nof input molecules. This time bound is slower than the O(log^5 n) achieved in\n[1], but faster than the O(n log n) achieved by the direct construction of [1]\n(Theorem 4.1 in the latest online version of [1]), since the fast construction\nof that paper (Theorem 4.4) relied heavily on the use of a fast, error-prone\nCRN that computes arbitrary computable functions, and which crucially uses a\nleader.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 17:04:10 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Doty", "David", ""], ["Hajiaghayi", "Monir", ""]]}, {"id": "1304.5045", "submitter": "Chii Chang", "authors": "Chii Chang, Sea Ling", "title": "Towards an Infrastructure-less SOA for Mobile Web Service Composition", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service composition enables customizable services to be provided to the\nservice consumers. Since the capabilities and the performances of mobile\ndevices (e.g., smart phone, PDA, handheld media player) have improved, a mobile\ndevice can be utilized to interact with external mobile service providers\ntowards providing composite Web service to remote clients. Existing approaches\non mobile-hosted service composition are usually platform dependent, and rely\non centralized infrastructure. Such approaches are not feasible in an open,\nmobile infrastructure-less environment, in which networked services are\nimplemented using different technologies, devices are capable of dynamically\njoining or leaving the network, and a centralized management entity is\nnonexistent. This paper proposes a solution to enable mobile Web service\ncomposition in an open, infrastructure-less environment based on loosely\ncoupled SOA techniques.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 08:10:19 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Chang", "Chii", ""], ["Ling", "Sea", ""]]}, {"id": "1304.5357", "submitter": "Toni Ernvall", "authors": "Toni Ernvall", "title": "Exact-Regenerating Codes between MBR and MSR Points", "comments": "5 pages, 2 figures, submitted to ITW 2013", "journal-ref": "2013 IEEE Information Theory Workshop, Sevilla, pages 1-5", "doi": "10.1109/ITW.2013.6691307", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study distributed storage systems with exact repair. We give\na construction for regenerating codes between the minimum storage regenerating\n(MSR) and the minimum bandwidth regenerating (MBR) points and show that in the\ncase that the parameters n, k, and d are close to each other our constructions\nare close to optimal when comparing to the known capacity when only functional\nrepair is required. We do this by showing that when the distances of the\nparameters n, k, and d are fixed but the actual values approach to infinity,\nthe fraction of the performance of our codes with exact repair and the known\ncapacity of codes with functional repair approaches to one.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 09:37:36 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Ernvall", "Toni", ""]]}, {"id": "1304.5583", "submitter": "Ameet Talwalkar", "authors": "Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I.\n  Jordan", "title": "Distributed Low-rank Subspace Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 03:54:48 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 02:55:18 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Talwalkar", "Ameet", ""], ["Mackey", "Lester", ""], ["Mu", "Yadong", ""], ["Chang", "Shih-Fu", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1304.5719", "submitter": "Jukka Suomela", "authors": "Danny Dolev, Keijo Heljanko, Matti J\\\"arvisalo, Janne H. Korhonen,\n  Christoph Lenzen, Joel Rybicki, Jukka Suomela, Siert Wieringa", "title": "Synchronous Counting and Computational Algorithm Design", "comments": "35 pages, extended and revised version", "journal-ref": null, "doi": "10.1016/j.jcss.2015.09.002", "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a complete communication network on $n$ nodes, each of which is a\nstate machine. In synchronous 2-counting, the nodes receive a common clock\npulse and they have to agree on which pulses are \"odd\" and which are \"even\". We\nrequire that the solution is self-stabilising (reaching the correct operation\nfrom any initial state) and it tolerates $f$ Byzantine failures (nodes that\nsend arbitrary misinformation). Prior algorithms are expensive to implement in\nhardware: they require a source of random bits or a large number of states.\n  This work consists of two parts. In the first part, we use computational\ntechniques (often known as synthesis) to construct very compact deterministic\nalgorithms for the first non-trivial case of $f = 1$. While no algorithm exists\nfor $n < 4$, we show that as few as 3 states per node are sufficient for all\nvalues $n \\ge 4$. Moreover, the problem cannot be solved with only 2 states per\nnode for $n = 4$, but there is a 2-state solution for all values $n \\ge 6$.\n  In the second part, we develop and compare two different approaches for\nsynthesising synchronous counting algorithms. Both approaches are based on\ncasting the synthesis problem as a propositional satisfiability (SAT) problem\nand employing modern SAT-solvers. The difference lies in how to solve the SAT\nproblem: either in a direct fashion, or incrementally within a counter-example\nguided abstraction refinement loop. Empirical results suggest that the former\ntechnique is more efficient if we want to synthesise time-optimal algorithms,\nwhile the latter technique discovers non-optimal algorithms more quickly.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 10:58:03 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 10:40:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dolev", "Danny", ""], ["Heljanko", "Keijo", ""], ["J\u00e4rvisalo", "Matti", ""], ["Korhonen", "Janne H.", ""], ["Lenzen", "Christoph", ""], ["Rybicki", "Joel", ""], ["Suomela", "Jukka", ""], ["Wieringa", "Siert", ""]]}, {"id": "1304.5966", "submitter": "Mile Sikic", "authors": "Matija Korpar and Mile Sikic", "title": "SW# - GPU enabled exact alignments on genome scale", "comments": "3 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Sequence alignment is one of the oldest and the most famous problems in\nbioinformatics. Even after 45 years, for one reason or another, this problem is\nstill actual; current solutions are trade-offs between execution time, memory\nconsumption and accuracy. We purpose SW#, a new CUDA GPU enabled and memory\nefficient implementation of dynamic programming algorithms for local alignment.\nIn this implementation indels are treated using the affine gap model. Although\nthere are other GPU implementations of the Smith-Waterman algorithm, SW# is the\nonly publicly available implementation that can produce sequence alignments on\ngenome-wide scale. For long sequences, our implementation is at least a few\nhundred times faster than a CPU version of the same algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 14:40:15 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Korpar", "Matija", ""], ["Sikic", "Mile", ""]]}, {"id": "1304.6176", "submitter": "Yang Zhang", "authors": "Yang Zhang, Dusit Niyato and Ping Wang", "title": "An Auction Mechanism for Resource Allocation in Mobile Cloud Computing\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile cloud computing system is composed of heterogeneous services and\nresources to be allocated by the cloud service provider to mobile cloud users.\nOn one hand, some of these resources are substitutable (e.g., users can use\nstorage from different places) that they have similar functions to the users.\nOn the other hand, some resources are complementary that the user will need\nthem as a bundle (e.g., users need both wireless connection and storage for\nonline photo posting). In this paper, we first model the resource allocation\nprocess of a mobile cloud computing system as an auction mechanism with premium\nand discount factors. The premium and discount factors indicate complementary\nand substitutable relations among cloud resources provided by the service\nprovider. Then, we analyze the individual rationality and incentive\ncompatibility (truthfulness) properties of the users in the proposed auction\nmechanism. The optimal solutions of the resource allocation and cost charging\nschemes in the auction mechanism is discussed afterwards.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 06:11:12 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Zhang", "Yang", ""], ["Niyato", "Dusit", ""], ["Wang", "Ping", ""]]}, {"id": "1304.6475", "submitter": "Haim Avron", "authors": "Haim Avron, Alex Druinsky, Anshul Gupta", "title": "Revisiting Asynchronous Linear Solvers: Provable Convergence Rate\n  Through Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous methods for solving systems of linear equations have been\nresearched since Chazan and Miranker's pioneering 1969 paper on chaotic\nrelaxation. The underlying idea of asynchronous methods is to avoid processor\nidle time by allowing the processors to continue to make progress even if not\nall progress made by other processors has been communicated to them.\n  Historically, the applicability of asynchronous methods for solving linear\nequations was limited to certain restricted classes of matrices, such as\ndiagonally dominant matrices. Furthermore, analysis of these methods focused on\nproving convergence in the limit. Comparison of the asynchronous convergence\nrate with its synchronous counterpart and its scaling with the number of\nprocessors were seldom studied, and are still not well understood.\n  In this paper, we propose a randomized shared-memory asynchronous method for\ngeneral symmetric positive definite matrices. We rigorously analyze the\nconvergence rate and prove that it is linear, and is close to that of the\nmethod's synchronous counterpart if the processor count is not excessive\nrelative to the size and sparsity of the matrix. We also present an algorithm\nfor unsymmetric systems and overdetermined least-squares. Our work presents a\nsignificant improvement in the applicability of asynchronous linear solvers as\nwell as in their convergence analysis, and suggests randomization as a key\nparadigm to serve as a foundation for asynchronous methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 03:18:53 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2013 20:19:45 GMT"}, {"version": "v3", "created": "Wed, 2 Jul 2014 20:43:04 GMT"}, {"version": "v4", "created": "Fri, 18 Jul 2014 20:08:45 GMT"}, {"version": "v5", "created": "Fri, 6 Mar 2015 21:17:17 GMT"}, {"version": "v6", "created": "Tue, 14 Jul 2015 20:35:14 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Avron", "Haim", ""], ["Druinsky", "Alex", ""], ["Gupta", "Anshul", ""]]}, {"id": "1304.6809", "submitter": "M.M.A. Hashem", "authors": "Kawser Wazed Nafi, Tonny Shekha Kar, Amjad Hossain and M.M.A Hashem", "title": "A New Trusted and E-Commerce Architecture for Cloud Computing", "comments": "Accepted for publication in the Procs. of the IEEE 2013 International\n  Conference on Informatics, Electronics and Vision (ICIEV 2013), pp.XX-XX,\n  Dhaka, Bangladesh, May 17-18, (2013). arXiv admin note: substantial text\n  overlap with arXiv:1304.4028", "journal-ref": "Procs. of the IEEE 2013 International Conference on Informatics,\n  Electronics and Vision (ICIEV 2013), pp.XX-XX, Dhaka, Bangladesh, May 17-18,\n  (2013)", "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing platform gives people the opportunity for sharing resources,\nservices and information among the people of the whole world. In private cloud\nsystem, information is shared among the persons who are in that cloud.\nPresently, different types of internet based systems are running in Cloud\nComputing environment. E-commerce is one of them. Present models are not\nsecured enough for executing e-transactions easily, especially in cloud\nplatform. Again, most of the time, clients fail to distinguish between the good\nonline business companies and the bad one, which discourages clients and\ncompanies to migrate in cloud. In this paper, we have proposed a newer\ne-commerce architecture depends on encryption based secured and fuzzy logic\nbased certain trust model which will be helpful to solve present e-commerce\nproblems. We had discussed about the whole working procedure of the model in\nthis paper. Finally, at the end of this paper, we have discussed some\nexperimental results about our proposed model which will help to show the\nvalidity of our model.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 05:54:00 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Nafi", "Kawser Wazed", ""], ["Kar", "Tonny Shekha", ""], ["Hossain", "Amjad", ""], ["Hashem", "M. M. A", ""]]}, {"id": "1304.6994", "submitter": "Swan Dubois", "authors": "Swan Dubois (LPD, EPFL), Rachid Guerraoui (LPD, EPFL)", "title": "Sp\\'eculation et auto-stabilisation", "comments": "in French", "journal-ref": "15\\`emes Rencontres Francophones sur les Aspects Algorithmiques\n  des T\\'el\\'ecommunications (AlgoTel), Pornic : France (2013)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-stabilization ensures that, after any transient fault, the system\nrecovers in a finite time and eventually exhibits a correct behaviour.\nSpeculation consists in guaranteeing that the system satisfies its requirements\nfor any execution but exhibits significantly better performances for a subset\nof executions that are more probable. A speculative protocol is in this sense\nsupposed to be both robust and efficient in practice. We introduce the notion\nof speculative stabilization which we illustrate through the mutual exclusion\nproblem. We then present a novel speculatively stabilizing mutual exclusion\nprotocol. Our protocol is self-stabilizing for any asynchronous execution. We\nprove that its stabilization time for synchronous executions is diam(g)/2 steps\n(where diam(g) denotes the diameter of the system). This complexity result is\nof independent interest. The celebrated mutual exclusion protocol of Dijkstra\nstabilizes in n steps (where n is the number of processes) in synchronous\nexecutions and the question whether the stabilization time could be strictly\nsmaller than the diameter has been open since then (almost 40 years). We show\nthat this is indeed possible for any underlying topology. We also provide a\nlower bound proof that shows that our new stabilization time of diam(g)/2 steps\nis optimal for synchronous executions, even if asynchronous stabilization is\nnot required.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 19:54:00 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Dubois", "Swan", "", "LPD, EPFL"], ["Guerraoui", "Rachid", "", "LPD, EPFL"]]}, {"id": "1304.7053", "submitter": "Chetan Jhurani", "authors": "Chetan Jhurani, Paul Mullowney", "title": "A GEMM interface and implementation on NVIDIA GPUs for multiple small\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interface and an implementation of the General Matrix Multiply\n(GEMM) routine for multiple small matrices processed simultaneously on NVIDIA\ngraphics processing units (GPUs). We focus on matrix sizes under 16. The\nimplementation can be easily extended to larger sizes. For single precision\nmatrices, our implementation is 30% to 600% faster than the batched cuBLAS\nimplementation distributed in the CUDA Toolkit 5.0 on NVIDIA Tesla K20c. For\nexample, we obtain 104 GFlop/s and 216 GFlop/s when multiplying 100,000\nindependent matrix pairs of size 10 and 16, respectively. Similar improvement\nin performance is obtained for other sizes, in single and double precision for\nreal and complex types, and when the number of matrices is smaller. Apart from\nour implementation, our different function interface also plays an important\nrole in the improved performance. Applications of this software include Finite\nElement computation on GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 02:22:14 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Jhurani", "Chetan", ""], ["Mullowney", "Paul", ""]]}, {"id": "1304.7054", "submitter": "Chetan Jhurani", "authors": "Chetan Jhurani", "title": "Batched Kronecker product for 2-D matrices and 3-D arrays on NVIDIA GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an interface and an implementation for performing Kronecker\nproduct actions on NVIDIA GPUs for multiple small 2-D matrices and 3-D arrays\nprocessed in parallel as a batch. This method is suited to cases where the\nKronecker product component matrices are identical but the operands in a\nmatrix-free application vary in the batch. Any batched GEMM (General Matrix\nMultiply) implementation, for example ours [1] or the one in cuBLAS, can also\nbe used for performing batched Kronecker products on GPUs. However, the\nspecialized implementation presented here is faster and uses less memory.\nPartly this is because a simple GEMM based approach would require extra copies\nto and from main memory. We focus on matrix sizes less than or equal to 16,\nsince these are the typical polynomial degrees in Finite Elements, but the\nimplementation can be easily extended for other sizes. We obtain 143 and 285\nGFlop/s for single precision real when processing matrices of size 10 and 16,\nrespectively on NVIDIA Tesla K20c using CUDA 5.0. The corresponding speeds for\n3-D array Kronecker products are 126 and 268 GFlop/s, respectively. Double\nprecision is easily supported using the C++ template mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 02:22:25 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Jhurani", "Chetan", ""]]}, {"id": "1304.7121", "submitter": "Jordi Arjona Aroca", "authors": "Jordi Arjona Aroca, Antonio Fernandez Anta, Miguel A. Mosteiro,\n  Christopher Thraves and Lin Wang", "title": "Power-efficient Assignment of Virtual Machines to Physical Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by current trends in cloud computing, we study a version of the\ngeneralized assignment problem where a set of virtual processors has to be\nimplemented by a set of identical processors. For literature consistency, we\nsay that a set of virtual machines (VMs) is assigned to a set of physical\nmachines (PMs). The optimization criteria is to minimize the power consumed by\nall the PMs. We term the problem Virtual Machine Assignment (VMA). Crucial\ndifferences with previous work include a variable number of PMs, that each VM\nmust be assigned to exactly one PM (i.e., VMs cannot be implemented\nfractionally), and a minimum power consumption for each active PM. Such\ninfrastructure may be strictly constrained in the number of PMs or in the PMs'\ncapacity, depending on how costly (in terms of power consumption) is to add a\nnew PM to the system or to heavily load some of the existing PMs. Low usage or\nample budget yields models where PM capacity and/or the number of PMs may be\nassumed unbounded for all practical purposes. We study 4 VMA problems depending\non whether the capacity or the number of PMs is bounded or not. Specifically,\nwe study hardness and online competitiveness for a variety of cases. To the\nbest of our knowledge, this is the first comprehensive study of the VMA problem\nfor this cost function.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 10:58:16 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2013 21:02:45 GMT"}, {"version": "v3", "created": "Tue, 10 Jun 2014 08:55:55 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Aroca", "Jordi Arjona", ""], ["Anta", "Antonio Fernandez", ""], ["Mosteiro", "Miguel A.", ""], ["Thraves", "Christopher", ""], ["Wang", "Lin", ""]]}, {"id": "1304.7544", "submitter": "Jimmy Lin", "authors": "Jimmy Lin", "title": "Monoidify! Monoids as a Design Principle for Efficient MapReduce\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that since the sort/shuffle stage in MapReduce is costly,\nlocal aggregation is one important principle to designing efficient algorithms.\nThis short paper represents an attempt to more clearly articulate this design\nprinciple in terms of monoids, which generalizes the use of combiners and the\nin-mapper combining pattern.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 00:30:36 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Lin", "Jimmy", ""]]}, {"id": "1304.7615", "submitter": "Adrian Jackson", "authors": "Adrian Jackson, Par Strand", "title": "MDMP: Managed Data Message Passing", "comments": "Submitted to SC13, 10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MDMP is a new parallel programming approach that aims to provide users with\nan easy way to add parallelism to programs, optimise the message passing costs\nof traditional scientific simulation algorithms, and enable existing MPI-based\nparallel programs to be optimised and extended without requiring the whole code\nto be re-written from scratch. MDMP utilises a directives based approach to\nenable users to specify what communications should take place in the code, and\nthen implements those communications for the user in an optimal manner using\nboth the information provided by the user and data collected from instrumenting\nthe code and gathering information on the data to be communicated. This work\nwill present the basic concepts and functionality of MDMP and discuss the\nperformance that can be achieved using our prototype implementation of MDMP on\nsome model scientific simulation applications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 10:35:48 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Jackson", "Adrian", ""], ["Strand", "Par", ""]]}, {"id": "1304.7654", "submitter": "Adrian Jackson", "authors": "Adrian Jackson and M. Sergio Campobasso", "title": "Optimised hybrid parallelisation of a CFD code on Many Core\n  architectures", "comments": "Submitted to the SC13 conference, 10 pages with 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COSA is a novel CFD system based on the compressible Navier-Stokes model for\nunsteady aerodynamics and aeroelasticity of fixed structures, rotary wings and\nturbomachinery blades. It includes a steady, time domain, and harmonic balance\nflow solver.\n  COSA has primarily been parallelised using MPI, but there is also a hybrid\nparallelisation that adds OpenMP functionality to the MPI parallelisation to\nenable larger number of cores to be utilised for a given simulation as the MPI\nparallelisation is limited to the number of geometric partitions (or blocks) in\nthe simulation, or to exploit multi-threaded hardware where appropriate. This\npaper outlines the work undertaken to optimise these two parallelisation\nstrategies, improving the efficiency of both and therefore reducing the\ncomputational time required to compute simulations. We also analyse the power\nconsumption of the code on a range of leading HPC systems to further understand\nthe performance of the code.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 13:22:38 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Jackson", "Adrian", ""], ["Campobasso", "M. Sergio", ""]]}, {"id": "1304.7664", "submitter": "Georg Hager", "authors": "Markus Wittmann, Georg Hager, Thomas Zeiser, Jan Treibig, Gerhard\n  Wellein", "title": "Chip-level and multi-node analysis of energy-optimized lattice-Boltzmann\n  CFD simulations", "comments": "23 pages, 13 figures; post-peer-review version", "journal-ref": null, "doi": "10.1002/cpe.3489", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory-bound algorithms show complex performance and energy consumption\nbehavior on multicore processors. We choose the lattice-Boltzmann method (LBM)\non an Intel Sandy Bridge cluster as a prototype scenario to investigate if and\nhow single-chip performance and power characteristics can be generalized to the\nhighly parallel case. First we perform an analysis of a sparse-lattice LBM\nimplementation for complex geometries. Using a single-core performance model,\nwe predict the intra-chip saturation characteristics and the optimal operating\npoint in terms of energy to solution as a function of implementation details,\nclock frequency, vectorization, and number of active cores per chip. We show\nthat high single-core performance and a correct choice of the number of active\ncores per chip are the essential optimizations for lowest energy to solution at\nminimal performance degradation. Then we extrapolate to the MPI-parallel level\nand quantify the energy-saving potential of various optimizations and execution\nmodes, where we find these guidelines to be even more important, especially\nwhen communication overhead is non-negligible. In our setup we could achieve\nenergy savings of 35% in this case, compared to a naive approach. We also\ndemonstrate that a simple non-reflective reduction of the clock speed leaves\nmost of the energy saving potential unused.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 13:59:21 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 13:44:25 GMT"}, {"version": "v3", "created": "Fri, 22 May 2015 13:34:40 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Wittmann", "Markus", ""], ["Hager", "Georg", ""], ["Zeiser", "Thomas", ""], ["Treibig", "Jan", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1304.7793", "submitter": "Guillaume Aupy", "authors": "Guillaume Aupy, Manu Shantharam, Anne Benoit, Yves Robert and Padma\n  Raghavan", "title": "Co-Scheduling Algorithms for High-Throughput Workload Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": "INRIA RR-8293", "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates co-scheduling algorithms for processing a set of\nparallel applications. Instead of executing each application one by one, using\na maximum degree of parallelism for each of them, we aim at scheduling several\napplications concurrently. We partition the original application set into a\nseries of packs, which are executed one by one. A pack comprises several\napplications, each of them with an assigned number of processors, with the\nconstraint that the total number of processors assigned within a pack does not\nexceed the maximum number of available processors. The objective is to\ndetermine a partition into packs, and an assignment of processors to\napplications, that minimize the sum of the execution times of the packs. We\nthoroughly study the complexity of this optimization problem, and propose\nseveral heuristics that exhibit very good performance on a variety of\nworkloads, whose application execution times model profiles of parallel\nscientific codes. We show that co-scheduling leads to to faster workload\ncompletion time and to faster response times on average (hence increasing\nsystem throughput and saving energy), for significant benefits over traditional\nscheduling from both the user and system perspectives.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 20:22:27 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Aupy", "Guillaume", ""], ["Shantharam", "Manu", ""], ["Benoit", "Anne", ""], ["Robert", "Yves", ""], ["Raghavan", "Padma", ""]]}, {"id": "1304.8029", "submitter": "Bernhard Etzlinger", "authors": "Bernhard Etzlinger, Henk Wymeersch, Andreas Springer", "title": "Cooperative Synchronization in Wireless Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2313531", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronization is a key functionality in wireless network, enabling a wide\nvariety of services. We consider a Bayesian inference framework whereby network\nnodes can achieve phase and skew synchronization in a fully distributed way. In\nparticular, under the assumption of Gaussian measurement noise, we derive two\nmessage passing methods (belief propagation and mean field), analyze their\nconvergence behavior, and perform a qualitative and quantitative comparison\nwith a number of competing algorithms. We also show that both methods can be\napplied in networks with and without master nodes. Our performance results are\ncomplemented by, and compared with, the relevant Bayesian Cram\\'er-Rao bounds.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 15:18:04 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 09:27:34 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Etzlinger", "Bernhard", ""], ["Wymeersch", "Henk", ""], ["Springer", "Andreas", ""]]}]