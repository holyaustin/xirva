[{"id": "1212.0017", "submitter": "Sabeur Aridhi", "authors": "Sabeur Aridhi, Laurent d'Orazio, Mondher Maddouri and Engelbert Mephu\n  Nguifo", "title": "A large-scale and fault-tolerant approach of subgraph mining using\n  density-based partitioning", "comments": "The paper is under reviewing and we want to cancel the submission.\n  Thank you for your understanding", "journal-ref": null, "doi": "10.1016/j.is.2013.08.005", "report-no": null, "categories": "cs.DB cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph mining approaches have become very popular, especially in\ndomains such as bioinformatics, chemoinformatics and social networks. In this\nscope, one of the most challenging tasks is frequent subgraph discovery. This\ntask has been motivated by the tremendously increasing size of existing graph\ndatabases. Since then, an important problem of designing efficient and scaling\napproaches for frequent subgraph discovery in large clusters, has taken place.\nHowever, failures are a norm rather than being an exception in large clusters.\nIn this context, the MapReduce framework was designed so that node failures are\nautomatically handled by the framework. In this paper, we propose a large-scale\nand fault-tolerant approach of subgraph mining by means of a density-based\npartitioning technique, using MapReduce. Our partitioning aims to balance\ncomputation load on a collection of machines. We experimentally show that our\napproach decreases significantly the execution time and scales the subgraph\ndiscovery process to large graph databases.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 21:17:59 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 13:37:24 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Aridhi", "Sabeur", ""], ["d'Orazio", "Laurent", ""], ["Maddouri", "Mondher", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1212.0085", "submitter": "Prateek Khandelwal", "authors": "Gaurav Somani, Prateek Khandelwal, Kapil Phatnani", "title": "VUPIC: Virtual Machine Usage Based Placement in IaaS Cloud", "comments": "9 Pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient resource allocation is one of the critical performance challenges\nin an Infrastructure as a Service (IaaS) cloud. Virtual machine (VM) placement\nand migration decision making methods are integral parts of these resource\nallocation mechanisms. We present a novel virtual machine placement algorithm\nwhich takes performance isolation amongst VMs and their continuous resource\nusage into account while taking placement decisions. Performance isolation is a\nform of resource contention between virtual machines interested in basic low\nlevel hardware resources (CPU, memory, storage, and networks bandwidth).\nResource contention amongst multiple co-hosted neighbouring VMs form the basis\nof the presented novel approach. Experiments are conducted to show the various\ncategories of applications and effect of performance isolation and resource\ncontention amongst them. A per-VM 3-dimensional Resource Utilization Vector\n(RUV) has been continuously calculated and used for placement decisions while\ntaking conflicting resource interests of VMs into account. Experiments using\nthe novel placement algorithm: VUPIC, show effective improvements in VM\nperformance as well as overall resource utilization of the cloud.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 08:51:48 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Somani", "Gaurav", ""], ["Khandelwal", "Prateek", ""], ["Phatnani", "Kapil", ""]]}, {"id": "1212.0156", "submitter": "Miranda Zhang", "authors": "Miranda Zhang, Rajiv Ranjan, Armin Haller, Dimitrios Georgakopoulos,\n  Michael Menzel, Surya Nepal", "title": "An Ontology based System for Cloud Infrastructure Services Discovery", "comments": "Accepted as an invited paper by Collaboratecom 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cloud infrastructure services landscape advances steadily leaving users\nin the agony of choice. As a result, Cloud service identification and discovery\nremains a hard problem due to different service descriptions, non standardised\nnaming conventions and heterogeneous types and features of Cloud services. In\nthis paper, we present an OWL based ontology, the Cloud Computing Ontology\n(CoCoOn) that defines functional and non functional concepts, attributes and\nrelations of infrastructure services. We also present a system...\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 20:39:22 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Zhang", "Miranda", ""], ["Ranjan", "Rajiv", ""], ["Haller", "Armin", ""], ["Georgakopoulos", "Dimitrios", ""], ["Menzel", "Michael", ""], ["Nepal", "Surya", ""]]}, {"id": "1212.0421", "submitter": "Piotr Skowron", "authors": "Piotr Skowron and Krzysztof Rzadca", "title": "Network delay-aware load balancing in selfish and cooperative\n  distributed systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a request processing system composed of organizations and their\nservers connected by the Internet.\n  The latency a user observes is a sum of communication delays and the time\nneeded to handle the request on a server. The handling time depends on the\nserver congestion, i.e. the total number of requests a server must handle. We\nanalyze the problem of balancing the load in a network of servers in order to\nminimize the total observed latency. We consider both cooperative and selfish\norganizations (each organization aiming to minimize the latency of the\nlocally-produced requests). The problem can be generalized to the task\nscheduling in a distributed cloud; or to content delivery in an\norganizationally-distributed CDNs.\n  In a cooperative network, we show that the problem is polynomially solvable.\nWe also present a distributed algorithm iteratively balancing the load. We show\nhow to estimate the distance between the current solution and the optimum based\non the amount of load exchanged by the algorithm. During the experimental\nevaluation, we show that the distributed algorithm is efficient, therefore it\ncan be used in networks with dynamically changing loads.\n  In a network of selfish organizations, we prove that the price of anarchy\n(the worst-case loss of performance due to selfishness) is low when the network\nis homogeneous and the servers are loaded (the request handling time is high\ncompared to the communication delay). After relaxing these assumptions, we\nassess the loss of performance caused by the selfishness experimentally,\nshowing that it remains low.\n  Our results indicate that a network of servers handling requests can be\nefficiently managed by a distributed algorithm. Additionally, even if the\nnetwork is organizationally distributed, with individual organizations\noptimizing performance of their requests, the network remains efficient.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 15:52:18 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Skowron", "Piotr", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "1212.0427", "submitter": "Piotr Skowron", "authors": "Piotr Skowron and Krzysztof Rzadca", "title": "Exploring heterogeneity of unreliable machines for p2p backup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P2P architecture is a viable option for enterprise backup. In contrast to\ndedicated backup servers, nowadays a standard solution, making backups directly\non organization's workstations should be cheaper (as existing hardware is\nused), more efficient (as there is no single bottleneck server) and more\nreliable (as the machines are geographically dispersed).\n  We present the architecture of a p2p backup system that uses pairwise\nreplication contracts between a data owner and a replicator. In contrast to\nstandard p2p storage systems using directly a DHT, the contracts allow our\nsystem to optimize replicas' placement depending on a specific optimization\nstrategy, and so to take advantage of the heterogeneity of the machines and the\nnetwork. Such optimization is particularly appealing in the context of backup:\nreplicas can be geographically dispersed, the load sent over the network can be\nminimized, or the optimization goal can be to minimize the backup/restore time.\nHowever, managing the contracts, keeping them consistent and adjusting them in\nresponse to dynamically changing environment is challenging.\n  We built a scientific prototype and ran the experiments on 150 workstations\nin the university's computer laboratories and, separately, on 50 PlanetLab\nnodes. We found out that the main factor affecting the quality of the system is\nthe availability of the machines. Yet, our main conclusion is that it is\npossible to build an efficient and reliable backup system on highly unreliable\nmachines (our computers had just 13% average availability).\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 16:09:02 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2013 09:00:16 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Skowron", "Piotr", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "1212.0785", "submitter": "Frank Winter", "authors": "Frank Winter", "title": "Gauge Field Generation on Large-Scale GPU-Enabled Systems", "comments": "The 30th International Symposium on Lattice Field Theory, June 24-29,\n  2012, Cairns, Australia (Acknowledgment and Citation added)", "journal-ref": "PoS(Lattice 2012)185", "doi": null, "report-no": null, "categories": "hep-lat cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years GPUs have been successfully applied to the task of\ninverting the fermion matrix in lattice QCD calculations. Even strong scaling\nto capability-level supercomputers, corresponding to O(100) GPUs or more has\nbeen achieved. However strong scaling a whole gauge field generation algorithm\nto this regim requires significantly more functionality than just having the\nmatrix inverter utilizing the GPUs and has not yet been accomplished. This\ncontribution extends QDP-JIT, the migration of SciDAC QDP++ to GPU-enabled\nparallel systems, to help to strong scale the whole Hybrid Monte-Carlo to this\nregime. Initial results are shown for gauge field generation with Chroma\nsimulating pure Wilson fermions on OLCF TitanDev.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 16:38:44 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2012 18:24:43 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Winter", "Frank", ""]]}, {"id": "1212.1046", "submitter": "Yuqing  Zhu", "authors": "Yuqing Zhu, Philip S. Yu, Jianmin Wang", "title": "Latency Bounding by Trading off Consistency in NoSQL Store: A Staging\n  and Stepwise Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latency is a key service factor for user satisfaction. Consistency is in a\ntrade-off relation with operation latency in the distributed and replicated\nscenario. Existing NoSQL stores guarantee either strong or weak consistencies\nbut none provides the best consistency based on the response latency. In this\npaper, we introduce dConssandra, a NoSQL store enabling users to specify\nlatency bounds for data access operations. dConssandra dynamically bounds data\naccess latency by trading off replica consistency. dConssandra is based on\nCassandra. In comparison to Cassandra's implementation, dConssandra has a\nstaged replication strategy enabling synchronous or asynchronous replication on\ndemand. The main idea to bound latency by trading off consistency is to\ndecompose the replication process into minute steps and bound latency by\nexecuting only a subset of these steps. dConssandra also implements a different\nin-memory storage architecture to support the above features. Experimental\nresults for dConssandra over an actual cluster demonstrate that (1) the actual\nresponse latency is bounded by the given latency constraint; (2) greater write\nlatency bounds lead to a lower latency in reading the latest value; and, (3)\ngreater read latency bounds lead to the return of more recently written values.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 15:16:30 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Zhu", "Yuqing", ""], ["Yu", "Philip S.", ""], ["Wang", "Jianmin", ""]]}, {"id": "1212.1095", "submitter": "Daniel Reem", "authors": "Daniel Reem", "title": "The projector algorithm: a simple parallel algorithm for computing\n  Voronoi diagrams and Delaunay graphs", "comments": "This is a major revision; re-organization and better presentation of\n  some parts; correction of several inaccuracies; improvement of some proofs\n  and figures; added references; modification of the title; the paper is long\n  but more than half of it is composed of proofs and references: it is\n  sufficient to look at pages 5, 7--11 in order to understand the algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Voronoi diagram is a certain geometric data structure which has numerous\napplications in various scientific and technological fields. The theory of\nalgorithms for computing 2D Euclidean Voronoi diagrams of point sites is rich\nand useful, with several different and important algorithms. However, this\ntheory has been quite steady during the last few decades in the sense that no\nessentially new algorithms have entered the game. In addition, most of the\nknown algorithms are serial in nature and hence cast inherent difficulties on\nthe possibility to compute the diagram in parallel. In this paper we present\nthe projector algorithm: a new and simple algorithm which enables the\n(combinatorial) computation of 2D Voronoi diagrams. The algorithm is\nsignificantly different from previous ones and some of the involved concepts in\nit are in the spirit of linear programming and optics. Parallel implementation\nis naturally supported since each Voronoi cell can be computed independently of\nthe other cells. A new combinatorial structure for representing the cells (and\nany convex polytope) is described along the way and the computation of the\ninduced Delaunay graph is obtained almost automatically.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 18:17:57 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2013 05:03:45 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2015 18:58:00 GMT"}, {"version": "v4", "created": "Sun, 12 Aug 2018 16:54:07 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Reem", "Daniel", ""]]}, {"id": "1212.1284", "submitter": "Mohammad Doomun", "authors": "M. N. Hulkury and M. R. Doomun", "title": "Integrated Green Cloud Computing Architecture", "comments": "6 pages, International Conference on Advanced Computer Science\n  Applications and Technologies, ACSAT 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Arbitrary usage of cloud computing, either private or public, can lead to\nuneconomical energy consumption in data processing, storage and communication.\nHence, green cloud computing solutions aim not only to save energy but also\nreduce operational costs and carbon footprints on the environment. In this\npaper, an Integrated Green Cloud Architecture (IGCA) is proposed that comprises\nof a client-oriented Green Cloud Middleware to assist managers in better\noverseeing and configuring their overall access to cloud services in the\ngreenest or most energy-efficient way. Decision making, whether to use local\nmachine processing, private or public clouds, is smartly handled by the\nmiddleware using predefined system specifications such as service level\nagreement (SLA), Quality of service (QoS), equipment specifications and job\ndescription provided by IT department. Analytical model is used to show the\nfeasibility to achieve efficient energy consumption while choosing between\nlocal, private and public Cloud service provider (CSP).\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 10:57:59 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Hulkury", "M. N.", ""], ["Doomun", "M. R.", ""]]}, {"id": "1212.1941", "submitter": "Vladimir Nikishkin", "authors": "Vladimir Nikishkin", "title": "Amortized communication complexity of an equality predicate", "comments": "12 pages, beta version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We study the communication complexity of a direct sum of independent copies\nof the equality predicate. We prove that the probabilistic communication\ncomplexity of this problem is equal to O(N); computational complexity of the\nproposed protocol is polynomial in size of inputs. Our protocol improves the\nresult achieved in 1995(Feder, Kushilevitz, Naor, Nisan). Our construction is\nbased on two techniques: Nisan's pseudorandom generator (1992) and Smith's\nstring synchronization algorithm (2007).\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 00:13:19 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2012 18:11:44 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Nikishkin", "Vladimir", ""]]}, {"id": "1212.2529", "submitter": "Francis Cabarle", "authors": "Francis George C. Cabarle, Kelvin C. Bu\\~no, Henry N. Adorna", "title": "On The Delays In Spiking Neural P Systems", "comments": "Presented at the 6th Symposium on the Mathematical Aspects of\n  Computer Science (SMACS2012), Boracay, Philippines. 6 figures, 6 pages, 2\n  columns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we extend and improve the results done in a previous work on\nsimulating Spiking Neural P systems (SNP systems in short) with delays using\nSNP systems without delays. We simulate the former with the latter over\nsequential, iteration, join, and split routing. Our results provide\nconstructions so that both systems halt at exactly the same time, start with\nonly one spike, and produce the same number of spikes to the environment after\nhalting.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 16:40:45 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Cabarle", "Francis George C.", ""], ["Bu\u00f1o", "Kelvin C.", ""], ["Adorna", "Henry N.", ""]]}, {"id": "1212.2720", "submitter": "Velumadhava Rao R", "authors": "R. Velumadhava Rao, K. Selvamani, R. Elakkiya", "title": "A secure key transfer protocol for group communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing security for messages in group communication is more essential and\ncritical nowadays. In group oriented applications such as Video conferencing\nand entertainment applications, it is necessary to secure the confidential data\nin such a way that intruders are not able to modify or transmit the data. Key\ntransfer protocols fully rely on trusted Key Generation Center (KGC) to compute\ngroup key and to transport the group keys to all communication parties in a\nsecured and secret manner. In this paper, an efficient key generation and key\ntransfer protocol has been proposed where KGC can broadcast group key\ninformation to all group members in a secure way. Hence, only authorized group\nmembers will be able to retrieve the secret key and unauthorized members cannot\nretrieve the secret key. Hence, inorder to maintain the forward and backward\nsecrecy, the group keys are updated whenever a new member joins or leaves the\ncommunication group. The proposed algorithm is more efficient and relies on NP\nclass. In addition, the keys are distributed to the group users in a safe and\nsecure way. Moreover, the key generated is also very strong since it uses\ncryptographic techniques which provide efficient computation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 07:23:12 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Rao", "R. Velumadhava", ""], ["Selvamani", "K.", ""], ["Elakkiya", "R.", ""]]}, {"id": "1212.2894", "submitter": "Chia-Mu Yu", "authors": "H. T. Kung and Chia-Mu Yu", "title": "Reducing Reconciliation Communication Cost with Compressed Sensing", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a reconciliation problem, where two hosts wish to synchronize\ntheir respective sets. Efficient solutions for minimizing the communication\ncost between the two hosts have been previously proposed in the literature.\nHowever, they rely on prior knowledge about the size of the set differences\nbetween the two sets to be reconciled. In this paper, we propose a method which\ncan achieve comparable efficiency without assuming this prior knowledge. Our\nmethod uses compressive sensing techniques which can leverage the expected\nsparsity in set differences. We study the performance of the method via\ntheoretical analysis and numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 04:12:46 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Kung", "H. T.", ""], ["Yu", "Chia-Mu", ""]]}, {"id": "1212.3074", "submitter": "Jigyasu Dubey Mr.", "authors": "Jigyasu Dubey, Vrinda Tokekar", "title": "Identification of efficient peers in P2P computing system for real time\n  applications", "comments": "12 Pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently the Peer to Peer computing paradigm rises as an economic solution\nfor the large scale computation problems. However due to the dynamic nature of\npeers it is very difficult to use this type of systems for the computations of\nreal time applications. Strict deadline of scientific and real time\napplications require predictable performance in such applications. We propose\nan algorithm to identify the group of reliable peers, from the available peers\non the Internet, for the processing of real time applications tasks. The\nalgorithm is based on joint evaluation of peer properties like peer\navailability, credibility, computation time and the turnaround time of the peer\nwith respect to the task distributor peer. Here we also define a method to\ncalculate turnaround time on task distributor peers at application level.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 07:29:01 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Dubey", "Jigyasu", ""], ["Tokekar", "Vrinda", ""]]}, {"id": "1212.3220", "submitter": "Mohammed El-Dosuky", "authors": "M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza and A.H. EL-Bassiouny", "title": "New SpiroPlanck Heuristics for High Energy Physics Networking and Future\n  Internet Testbeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for data intensive Grids, and advanced networks with high\nperformance that support our science has made the High Energy Physics community\na leading and a key co-developer of leading edge wide area networks. This paper\ngives an overview of the status for the world's research networks and major\ninternational links used by the high energy physics and other scientific\ncommunities, showing some Future Internet testbed architectures, scalability,\ngeographic scope, and extension between networks. The resemblance between\nwireless sensor network and future internet network, especially in scale\nconsideration as density and network coverage, inspires us to adopt the models\nof the former to the later. Then we test this assumption to see that this\nprovides a concise working model. This paper collects some heuristics that we\ncall them SpiroPlanck and employs them to model the coverage of dense networks.\nIn this paper, we propose a framework for the operation of FI testbeds\ncontaining a test scenario, new representation and visualization techniques,\nand possible performance measures. Investigations show that it is very\npromising and could be seen as a good optimization\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 17:55:25 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["El-Dosuky", "M. A.", ""], ["Rashad", "M. Z.", ""], ["Hamza", "T. T.", ""], ["EL-Bassiouny", "A. H.", ""]]}, {"id": "1212.3295", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash", "title": "Measures of Fault Tolerance in Distributed Simulated Annealing", "comments": "4 pages, 2 figures", "journal-ref": "International Conference on Perspective of Computer Confluence\n  with Sciences, pp 111-114, 2012, Excel Publisher", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we examine the different measures of Fault Tolerance in a\nDistributed Simulated Annealing process. Optimization by Simulated Annealing on\na distributed system is prone to various sources of failure. We analyse\nsimulated annealing algorithm, its architecture in distributed platform and\npotential sources of failures. We examine the behaviour of tolerant distributed\nsystem for optimization task. We present possible methods to overcome the\nfailures and achieve fault tolerance for the distributed simulated annealing\nprocess. We also examine the implementation of Simulated Annealing in MapReduce\nsystem and possible ways to prevent failures in reaching the global optima.\nThis paper will be beneficial to those who are interested in implementing a\nlarge scale distributed simulated annealing optimization problem of industrial\nor academic interest. We recommend hybrid tolerance technique to optimize the\ntrade-off between efficiency and availability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 20:00:40 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2012 06:07:50 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Prakash", "Aaditya", ""]]}, {"id": "1212.3418", "submitter": "Anissa Lamani", "authors": "Evangelos Bampas, Anissa Lamani (MIS), Franck Petit (LIP6), Mathieu\n  Valero (LIP6, INRIA Rocquencourt)", "title": "Self-Stabilizing Balancing Algorithm for Containment-Based Trees", "comments": "(2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containment-based trees encompass various handy structures such as B+-trees,\nR-trees and M-trees. They are widely used to build data indexes,\nrange-queryable overlays, publish/subscribe systems both in centralized and\ndistributed contexts. In addition to their versatility, their balanced shape\nensures an overall satisfactory performance. Re- cently, it has been shown that\ntheir distributed implementations can be fault-resilient. However, this\nrobustness is achieved at the cost of un-balancing the structure. While the\nstructure remains correct in terms of searchability, its performance can be\nsignificantly decreased. In this paper, we propose a distributed\nself-stabilizing algorithm to balance containment-based trees.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 09:17:50 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Bampas", "Evangelos", "", "MIS"], ["Lamani", "Anissa", "", "MIS"], ["Petit", "Franck", "", "LIP6"], ["Valero", "Mathieu", "", "LIP6, INRIA Rocquencourt"]]}, {"id": "1212.3480", "submitter": "Jens Dittrich", "authors": "Stefan Richter, Jorge-Arnulfo Quian\\'e-Ruiz, Stefan Schuh, Jens\n  Dittrich", "title": "Towards Zero-Overhead Adaptive Indexing in Hadoop", "comments": "Tech Report, Saarland University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several research works have focused on supporting index access in MapReduce\nsystems. These works have allowed users to significantly speed up selective\nMapReduce jobs by orders of magnitude. However, all these proposals require\nusers to create indexes upfront, which might be a difficult task in certain\napplications (such as in scientific and social applications) where workloads\nare evolving or hard to predict. To overcome this problem, we propose LIAH\n(Lazy Indexing and Adaptivity in Hadoop), a parallel, adaptive approach for\nindexing at minimal costs for MapReduce systems. The main idea of LIAH is to\nautomatically and incrementally adapt to users' workloads by creating clustered\nindexes on HDFS data blocks as a byproduct of executing MapReduce jobs. Besides\ndistributing indexing efforts over multiple computing nodes, LIAH also\nparallelises indexing with both map tasks computation and disk I/O. All this\nwithout any additional data copy in main memory and with minimal\nsynchronisation. The beauty of LIAH is that it piggybacks index creation on map\ntasks, which read relevant data from disk to main memory anyways. Hence, LIAH\ndoes not introduce any additional read I/O-costs and exploit free CPU cycles.\nAs a result and in contrast to existing adaptive indexing works, LIAH has a\nvery low (or invisible) indexing overhead, usually for the very first job.\nStill, LIAH can quickly converge to a complete index, i.e. all HDFS data blocks\nare indexed. Especially, LIAH can trade early job runtime improvements with\nfast complete index convergence. We compare LIAH with HAIL, a state-of-the-art\nindexing technique, as well as with standard Hadoop with respect to indexing\noverhead and workload performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 14:31:24 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Richter", "Stefan", ""], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", ""], ["Schuh", "Stefan", ""], ["Dittrich", "Jens", ""]]}, {"id": "1212.3496", "submitter": "Ilja Honkonen", "authors": "I. Honkonen (1,2), S. von Alfthan (1), A. Sandroos (1), P. Janhunen\n  (1), M. Palmroth (1) ((1) Finnish Meteorological Institute, Helsinki,\n  Finland, (2) Department of Physics, University of Helsinki, Helsinki,\n  Finland)", "title": "Parallel grid library for rapid and flexible simulation development", "comments": "Accepted to Computer Physics Communications, 36 pages, 13 figures", "journal-ref": "Computer Physics Communications, Volume 184, Issue 4, April 2013,\n  Pages 1297-1309", "doi": "10.1016/j.cpc.2012.12.017", "report-no": null, "categories": "cs.DC physics.comp-ph physics.space-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an easy to use and flexible grid library for developing highly\nscalable parallel simulations. The distributed cartesian cell-refinable grid\n(dccrg) supports adaptive mesh refinement and allows an arbitrary C++ class to\nbe used as cell data. The amount of data in grid cells can vary both in space\nand time allowing dccrg to be used in very different types of simulations, for\nexample in fluid and particle codes. Dccrg transfers the data between\nneighboring cells on different processes transparently and asynchronously\nallowing one to overlap computation and communication. This enables excellent\nscalability at least up to 32 k cores in magnetohydrodynamic tests depending on\nthe problem and hardware. In the version of dccrg presented here part of the\nmesh metadata is replicated between MPI processes reducing the scalability of\nadaptive mesh refinement (AMR) to between 200 and 600 processes. Dccrg is free\nsoftware that anyone can use, study and modify and is available at\n[https://gitorious.org/dccrg]. Users are also kindly requested to cite this\nwork when publishing results obtained with dccrg.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 15:03:16 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Honkonen", "I.", ""], ["von Alfthan", "S.", ""], ["Sandroos", "A.", ""], ["Janhunen", "P.", ""], ["Palmroth", "M.", ""]]}, {"id": "1212.3555", "submitter": "Dan Dobre Dan Dobre", "authors": "Dan Dobre, Ghassan Karame, Wenting Li, Matthias Majuntke, Neeraj Suri\n  and Marko Vukolic", "title": "Proofs of Writing for Efficient and Robust Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PoWerStore, the first efficient robust storage protocol that\nachieves optimal latency without using digital signatures. PoWerStore's\nrobustness comprises tolerating asynchrony, maximum number of Byzantine storage\nservers, any number of Byzantine readers and crash-faulty writers, and\nguaranteeing wait-freedom and linearizability of read/write operations.\nPoWerStore's efficiency stems from combining lightweight authentication,\nerasure coding and metadata write-backs where readers write-back only metadata\nto achieve linearizability. At the heart of PoWerStore are Proofs of Writing\n(PoW): a novel storage technique based on lightweight cryptography. PoW enable\nreads and writes in the single-writer variant of PoWerStore to have latency of\n2 rounds of communication between a client and storage servers in the\nworst-case (which we show optimal). We further present and implement a\nmulti-writer PoWerStore variant featuring 3-round writes/reads where the third\nread round is invoked only under active attacks, and show that it outperforms\nexisting robust storage protocols, including crash-tolerant ones.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 18:15:32 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2012 15:30:48 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Dobre", "Dan", ""], ["Karame", "Ghassan", ""], ["Li", "Wenting", ""], ["Majuntke", "Matthias", ""], ["Suri", "Neeraj", ""], ["Vukolic", "Marko", ""]]}, {"id": "1212.3876", "submitter": "EPTCS", "authors": "Gabriele Costa (Dipartimento di Informatica, Sistemistica e Telematica\n  Universita di Genova), Fabio Martinelli (Istituto di Informatica e\n  Telematica, Consiglio Nazionale delle Ricerche), Artsiom Yautsiukhin\n  (Istituto di Informatica e Telematica, Consiglio Nazionale delle Ricerche)", "title": "Metric-Aware Secure Service Orchestration", "comments": "In Proceedings ICE 2012, arXiv:1212.3458", "journal-ref": "EPTCS 104, 2012, pp. 32-46", "doi": "10.4204/EPTCS.104.4", "report-no": null, "categories": "cs.CR cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure orchestration is an important concern in the internet of service. Next\nto providing the required functionality the composite services must also\nprovide a reasonable level of security in order to protect sensitive data.\nThus, the orchestrator has a need to check whether the complex service is able\nto satisfy certain properties. Some properties are expressed with metrics for\nprecise definition of requirements. Thus, the problem is to analyse the values\nof metrics for a complex business process.\n  In this paper we extend our previous work on analysis of secure orchestration\nwith quantifiable properties. We show how to define, verify and enforce\nquantitative security requirements in one framework with other security\nproperties. The proposed approach should help to select the most suitable\nservice architecture and guarantee fulfilment of the declared security\nrequirements.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 03:42:15 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Costa", "Gabriele", "", "Dipartimento di Informatica, Sistemistica e Telematica\n  Universita di Genova"], ["Martinelli", "Fabio", "", "Istituto di Informatica e\n  Telematica, Consiglio Nazionale delle Ricerche"], ["Yautsiukhin", "Artsiom", "", "Istituto di Informatica e Telematica, Consiglio Nazionale delle Ricerche"]]}, {"id": "1212.4115", "submitter": "Stephan Zimmer", "authors": "Stephan Zimmer, Luisa Arrabito, Tom Glanzman, Tony Johnson, Claudia\n  Lavalley and Andrei Tsaregorodtsev", "title": "Extending the Fermi-LAT Data Processing Pipeline to the Grid", "comments": "This is an author-created, un-copyedited version of an article\n  accepted for publication in Journal of Physics: Conference Series. IOP\n  Publishing Ltd is not responsible for any errors or omissions in this version\n  of the manuscript or any version derived from it. The Version of Record is\n  available online at http://dx.doi.org/10.1088/1742-6596/396/3/032121", "journal-ref": "2012 J. Phys.: Conf. Ser. 396 032121", "doi": "10.1088/1742-6596/396/3/032121", "report-no": null, "categories": "astro-ph.IM cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Data Handling Pipeline (\"Pipeline\") has been developed for the Fermi\nGamma-Ray Space Telescope (Fermi) Large Area Telescope (LAT) which launched in\nJune 2008. Since then it has been in use to completely automate the production\nof data quality monitoring quantities, reconstruction and routine analysis of\nall data received from the satellite and to deliver science products to the\ncollaboration and the Fermi Science Support Center. Aside from the\nreconstruction of raw data from the satellite (Level 1), data reprocessing and\nvarious event-level analyses are also reasonably heavy loads on the pipeline\nand computing resources. These other loads, unlike Level 1, can run\ncontinuously for weeks or months at a time. In addition it receives heavy use\nin performing production Monte Carlo tasks.\n  The software comprises web-services that allow online monitoring and provides\ncharts summarizing work flow aspects and performance information. The server\nsupports communication with several batch systems such as LSF and BQS and\nrecently also Sun Grid Engine and Condor. This is accomplished through\ndedicated job control services that for Fermi are running at SLAC and the other\ncomputing site involved in this large scale framework, the Lyon computing\ncenter of IN2P3. While being different in the logic of a task, we evaluate a\nseparate interface to the Dirac system in order to communicate with EGI sites\nto utilize Grid resources, using dedicated Grid optimized systems rather than\ndeveloping our own. (abstract abridged)\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 19:39:23 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Zimmer", "Stephan", ""], ["Arrabito", "Luisa", ""], ["Glanzman", "Tom", ""], ["Johnson", "Tony", ""], ["Lavalley", "Claudia", ""], ["Tsaregorodtsev", "Andrei", ""]]}, {"id": "1212.4123", "submitter": "Sleiman Rabah", "authors": "Sleiman Rabah, Serguei A. Mokhov, Joey Paquet", "title": "An Interactive Graph-Based Automation Assistant: A Case Study to Manage\n  the GIPSY's Distributed Multi-tier Run-Time System", "comments": "Submitted for publication to RACS", "journal-ref": "In Proceedings of RACS '13. ACM, pp.387-394. 2013", "doi": "10.1145/2513228.2513286", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GIPSY system provides a framework for a distributed multi-tier\ndemand-driven evaluation of heterogeneous programs, in which certain tiers can\ngenerate demands, while others can respond to demands to work on them. They are\nconnected through a virtual network that can be flexibly reconfigured at\nrun-time. Although the demand generator components were originally designed\nspecifically for the eductive (demand-driven) evaluation of Lucid intensional\nprograms, the GIPSY's run-time's flexible framework design enables it to\nperform the execution of various kinds of programs that can be evaluated using\nthe demand-driven computational model. Management of the GISPY networks has\nbecome a tedious (although scripted) task that took manual command-line console\nto do, which does not scale for large experiments. Therefore a new component\nhas been designed and developed to allow users to represent, visualize, and\ninteractively create, configure and seamlessly manage such a network as a\ngraph. Consequently, this work presents a Graphical GMT Manager, an interactive\ngraph-based assistant component for the GIPSY network creation and\nconfiguration management. Besides allowing the management of the nodes and\ntiers (mapped to hosts where store, workers, and generators reside), it lets\nthe user to visually control the network parameters and the interconnection\nbetween computational nodes at run-time. In this paper we motivate and present\nthe key features of this newly implemented graph-based component. We give the\ngraph representation details, mapping of the graph nodes to tiers, tier groups,\nand specific commands. We provide the requirements and design specification of\nthe tool and its implementation. Then we detail and discuss some experimental\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 20:09:55 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 19:33:39 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2013 16:05:04 GMT"}, {"version": "v4", "created": "Fri, 21 Jun 2013 21:29:32 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Rabah", "Sleiman", ""], ["Mokhov", "Serguei A.", ""], ["Paquet", "Joey", ""]]}, {"id": "1212.4174", "submitter": "Mahantesh Halappanavar", "authors": "Chad Scherrer, Ambuj Tewari, Mahantesh Halappanavar, David Haglin", "title": "Feature Clustering for Accelerating Parallel Coordinate Descent", "comments": "Accepted for publication in the proceedings of NIPS (Neural\n  Information Processing Systems Foundations) 2012, Lake Tahoe, Nevada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale L1-regularized loss minimization problems arise in\nhigh-dimensional applications such as compressed sensing and high-dimensional\nsupervised learning, including classification and regression problems.\nHigh-performance algorithms and implementations are critical to efficiently\nsolving these problems. Building upon previous work on coordinate descent\nalgorithms for L1-regularized problems, we introduce a novel family of\nalgorithms called block-greedy coordinate descent that includes, as special\ncases, several existing algorithms such as SCD, Greedy CD, Shotgun, and\nThread-Greedy. We give a unified convergence analysis for the family of\nblock-greedy algorithms. The analysis suggests that block-greedy coordinate\ndescent can better exploit parallelism if features are clustered so that the\nmaximum inner product between features in different blocks is small. Our\ntheoretical convergence analysis is supported with experimental re- sults using\ndata from diverse real-world applications. We hope that algorithmic approaches\nand convergence analysis we provide will not only advance the field, but will\nalso encourage researchers to systematically explore the design space of\nalgorithms for solving large-scale L1-regularization problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 21:43:31 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Scherrer", "Chad", ""], ["Tewari", "Ambuj", ""], ["Halappanavar", "Mahantesh", ""], ["Haglin", "David", ""]]}, {"id": "1212.4287", "submitter": "Florian Richoux", "authors": "Charlotte Truchet, Florian Richoux and Philippe Codognet", "title": "Prediction of Parallel Speed-ups for Las Vegas Algorithms", "comments": "10 pages, 14 figures, 5 tables. Latex ACM Sigplan format", "journal-ref": "Proceedings of the IEEE 2013 42nd International Conference on\n  Parallel Processing", "doi": "10.1109/ICPP.2013.25", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic model for the parallel execution of Las Vegas\nalgorithms, i.e., randomized algorithms whose runtime might vary from one\nexecution to another, even with the same input. This model aims at predicting\nthe parallel performances (i.e., speedups) by analysis the runtime distribution\nof the sequential runs of the algorithm. Then, we study in practice the case of\na particular Las Vegas algorithm for combinatorial optimization, on three\nclassical problems, and compare with an actual parallel implementation up to\n256 cores. We show that the prediction can be quite accurate, matching the\nactual speedups very well up to 100 parallel cores and then with a deviation of\nabout 20% up to 256 cores.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 10:05:51 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Truchet", "Charlotte", ""], ["Richoux", "Florian", ""], ["Codognet", "Philippe", ""]]}, {"id": "1212.4444", "submitter": "EPTCS", "authors": "Kyriakos Poyias, Emilio Tuosto", "title": "Enforcing Architectural Styles in Presence of Unexpected Distributed\n  Reconfigurations", "comments": "In Proceedings ICE 2012, arXiv:1212.3458", "journal-ref": "EPTCS 104, 2012, pp. 67-82", "doi": "10.4204/EPTCS.104.7", "report-no": null, "categories": "cs.LO cs.DC cs.NI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architectural Design Rewriting (ADR, for short) is a rule-based formal\nframework for modelling the evolution of architectures of distributed systems.\nRules allow ADR graphs to be refined. After equipping ADR with a simple logic,\nwe equip rules with pre- and post-conditions; the former constraints the\napplicability of the rules while the later specifies properties of the\nresulting graphs. We give an algorithm to compute the weakest pre-condition out\nof a rule and its post-condition. On top of this algorithm, we design a simple\nmethodology that allows us to select which rules can be applied at the\narchitectural level to reconfigure a system so to regain its architectural\nstyle when it becomes compromised by unexpected run-time reconfigurations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 03:42:30 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Poyias", "Kyriakos", ""], ["Tuosto", "Emilio", ""]]}, {"id": "1212.4658", "submitter": "Stefano Stalio Mr", "authors": "Stefano Stalio, Giuseppe Di Carlo, Sandra Parlati, Piero Spinnato", "title": "Resource management on a VM based computer cluster for scientific\n  computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last ten years host virtualization has brought a revolution in the way\nalmost every activity related to information technology is thought of and\nperformed. The use of virtualization for HPC and HTC computing, while eagerly\ndesired, has probably been one of the last steps of this revolution, the\nperformance loss due to the hardware abstraction layer being the cause that\nslowed down a process that has been much faster in other fields. Nowadays the\nwidespread diffusion of virtualization and of new virtualization techniques\nseem to have helped breaking this last barrier and virtual host computing\ninfrastructures for HPC and HTC are found in many data centers. In this\ndocument the approach adopted at the INFN \"Laboratori Nazionali del Gran Sasso\"\nfor providing computational resources via a virtual host based computing\nfacility is described. Particular evidence is given to the storage layout, to\nthe middleware architecture and to resource allocation strategies, as these are\nissues for which a personalized solution was adopted. Other aspects may be\ncovered in the future within other documents.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 13:31:04 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Stalio", "Stefano", ""], ["Di Carlo", "Giuseppe", ""], ["Parlati", "Sandra", ""], ["Spinnato", "Piero", ""]]}, {"id": "1212.4692", "submitter": "Anjan Krishnamurthy", "authors": "Anjan K. Koundinya, Srinath N. K., K. A. K. Sharma, Kiran Kumar, Madhu\n  M. N., Kiran U. Shanbag", "title": "Map / Reduce Deisgn and Implementation of Apriori Alogirthm for handling\n  voluminous data-sets", "comments": "11 pages, 5 figures; Advanced Computing: An International Journal\n  (ACIJ), Vol.3, No.6, November 2012", "journal-ref": null, "doi": "10.5121/acij.2012.3604", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apriori is one of the key algorithms to generate frequent itemsets. Analyzing\nfrequent itemset is a crucial step in analysing structured data and in finding\nassociation relationship between items. This stands as an elementary foundation\nto supervised learning, which encompasses classifier and feature extraction\nmethods. Applying this algorithm is crucial to understand the behaviour of\nstructured data. Most of the structured data in scientific domain are\nvoluminous. Processing such kind of data requires state of the art computing\nmachines. Setting up such an infrastructure is expensive. Hence a distributed\nenvironment such as a clustered setup is employed for tackling such scenarios.\nApache Hadoop distribution is one of the cluster frameworks in distributed\nenvironment that helps by distributing voluminous data across a number of nodes\nin the framework. This paper focuses on map/reduce design and implementation of\nApriori algorithm for structured data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 15:04:12 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Koundinya", "Anjan K.", ""], ["K.", "Srinath N.", ""], ["Sharma", "K. A. K.", ""], ["Kumar", "Kiran", ""], ["N.", "Madhu M.", ""], ["Shanbag", "Kiran U.", ""]]}, {"id": "1212.4784", "submitter": "Sven Heinemeyer", "authors": "I. Campos, E. Fernandez del Castillo, S. Heinemeyer, A. Lopez-Garcia,\n  F. v.d. Pahlen", "title": "Phenomenology Tools on Cloud Infrastructures using OpenStack", "comments": "25 pages, 12 figures; information on memory usage included, as well\n  as minor modifications. Version to appear in EPJC", "journal-ref": null, "doi": "10.1140/epjc/s10052-013-2375-0", "report-no": null, "categories": "cs.DC hep-lat hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new environment for computations in particle physics\nphenomenology employing recent developments in cloud computing. On this\nenvironment users can create and manage \"virtual\" machines on which the\nphenomenology codes/tools can be deployed easily in an automated way. We\nanalyze the performance of this environment based on \"virtual\" machines versus\nthe utilization of \"real\" physical hardware. In this way we provide a\nqualitative result for the influence of the host operating system on the\nperformance of a representative set of applications for phenomenology\ncalculations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 18:27:44 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2013 15:02:27 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Campos", "I.", ""], ["del Castillo", "E. Fernandez", ""], ["Heinemeyer", "S.", ""], ["Lopez-Garcia", "A.", ""], ["Pahlen", "F. v. d.", ""]]}, {"id": "1212.5880", "submitter": "Ran Wolff", "authors": "Ran Wolff", "title": "Local Thresholding in General Network Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local thresholding algorithms were first presented more than a decade ago and\nhave since been applied to a variety of data mining tasks in peer-to-peer\nsystems, wireless sensor networks, and in grid systems. One critical assumption\nmade by those algorithms has always been cycle-free routing. The existence of\neven one cycle may lead all peers to the wrong outcome. Outside the lab,\nunfortunately, cycle freedom is not easy to achieve.\n  This work is the first to lift the requirement of cycle freedom by presenting\na local thresholding algorithm suitable for general network graphs. The\nalgorithm relies on a new repositioning of the problem in weighted vector\narithmetics, on a new stopping rule, whose proof does not require that the\nnetwork be cycle free, and on new methods for balance correction when the\nstopping rule fails.\n  The new stopping and update rules permit calculation of the very same\nfunctions that were calculable using previous algorithms, which do assume cycle\nfreedom. The algorithm is implemented on a standard peer-to-peer simulator and\nis validated for networks of up to 80,000 peers, organized in three different\ntopologies, which are representative of the topology of major current\ndistributed systems: the Internet, structured peer-to-peer systems, and\nwireless sensor networks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2012 09:22:28 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2013 08:20:12 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Wolff", "Ran", ""]]}, {"id": "1212.5956", "submitter": "Kinghsin Wang", "authors": "Jingxin K. Wang, Jianrui Ding, Tian Niu", "title": "Interoperability and Standardization of Intercloud Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is getting mature, and the interoperability and\nstandardization of the clouds is still waiting to be solved. This paper\ndiscussed the interoperability among clouds about message transmission, data\ntransmission and virtual machine transfer. Starting from IEEE Pioneering Cloud\nComputing Initiative, this paper discussed about standardization of the cloud\ncomputing, especially intercloud cloud computing. This paper also discussed the\nstandardization from the market-oriented view.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2012 19:24:35 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Wang", "Jingxin K.", ""], ["Ding", "Jianrui", ""], ["Niu", "Tian", ""]]}, {"id": "1212.6053", "submitter": "Nikolai Krivulin", "authors": "Nikolai K. Krivulin, Dennis Guster, Charles Hall", "title": "On parallel implementation of a discrete optimization random search\n  algorithm", "comments": "15 pages, 2 figures 1, table. ISSN 1109-2750", "journal-ref": "WSEAS Transactions on Computers, 2005, Vol. 4, No 9, pp. 1122-1129", "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random search algorithm intended to solve discrete optimization problems is\nconsidered. We outline the main components of the algorithm, and then describe\nit in more detail. We show how the algorithm can be implemented on parallel\ncomputer systems. A performance analysis of both serial and parallel versions\nof the algorithm is given, and related results of solving test problems are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 14:13:10 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 20:32:23 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Krivulin", "Nikolai K.", ""], ["Guster", "Dennis", ""], ["Hall", "Charles", ""]]}, {"id": "1212.6326", "submitter": "Karsten Ahnert", "authors": "Denis Demidov, Karsten Ahnert, Karl Rupp, Peter Gottschling", "title": "Programming CUDA and OpenCL: A Case Study Using Modern C++ Libraries", "comments": "21 pages, 4 figures, submitted to SIAM Journal of Scientific\n  Computing and accepted", "journal-ref": null, "doi": "10.1137/120903683", "report-no": null, "categories": "cs.MS cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comparison of several modern C++ libraries providing high-level\ninterfaces for programming multi- and many-core architectures on top of CUDA or\nOpenCL. The comparison focuses on the solution of ordinary differential\nequations and is based on odeint, a framework for the solution of systems of\nordinary differential equations. Odeint is designed in a very flexible way and\nmay be easily adapted for effective use of libraries such as Thrust, MTL4,\nVexCL, or ViennaCL, using CUDA or OpenCL technologies. We found that CUDA and\nOpenCL work equally well for problems of large sizes, while OpenCL has higher\noverhead for smaller problems. Furthermore, we show that modern high-level\nlibraries allow to effectively use the computational resources of many-core\nGPUs or multi-core CPUs without much knowledge of the underlying technologies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2012 08:56:00 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 07:50:28 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Demidov", "Denis", ""], ["Ahnert", "Karsten", ""], ["Rupp", "Karl", ""], ["Gottschling", "Peter", ""]]}, {"id": "1212.6640", "submitter": "Andrey Nikolaev", "authors": "Andrey Nikolaev", "title": "Exploring mutexes, the Oracle RDBMS retrial spinlocks", "comments": "Proceedings of International Conference on Informatics MEDIAS2012.\n  Cyprus, Limassol, May 7--14, 2012. ISBN 978-5-88835-023-2. 12 pages, 15\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spinlocks are widely used in database engines for processes synchronization.\nKGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions\nfor submicrosecond synchronization. The mutex contention is frequently observed\nin highly concurrent OLTP environments.\n  This work explores how Oracle mutexes operate, spin, and sleep. It develops\npredictive mathematical model and discusses parameters and statistics related\nto mutex performance tuning, as well as results of contention experiments.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2012 15:29:26 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Nikolaev", "Andrey", ""]]}]