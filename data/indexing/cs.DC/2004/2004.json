[{"id": "2004.00113", "submitter": "Antoine Paris", "authors": "Antoine Paris, Hamed Mirghasemi, Ivan Stupia, Luc Vandendorpe", "title": "Leveraging User-Diversity in Energy-Efficient Edge-Facilitated\n  Collaborative Fog Computing", "comments": "To be submitted to Transactions on Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications such as on-device collaborative neural network\ninference, this work investigates edge-facilitated collaborative fog computing\n- in which edge-devices collaborate with each other and with the edge of the\nnetwork to complete a processing task - to augment the computing capabilities\nof individual edge-devices while optimizing the collaboration for\nenergy-efficiency. Collaborative computing is modeled using the Map-Reduce\ndistributed computing framework, consisting in two rounds of computations\nseparated by a communication phase. The computing load is optimally distributed\namong the edge-devices, taking into account their diversity in term of\ncomputing and communications capabilities. In addition, edge-devices local\nparameters such as CPU clock frequency and RF transmit power are also optimized\nfor energy-efficiency. The corresponding optimization problem can be shown to\nbe convex and optimality conditions can be obtained through Lagrange duality\ntheory. A waterfilling-like interpretation for the size of the computing load\nassigned to each edge-device is given. Numerical experiments demonstrate the\nbenefits of the proposed optimal collaborative-computing scheme over various\nother schemes in several respects. Most notably, the proposed scheme exhibits\nincreased probability of successfully dealing with heavier computations and/or\nsmaller latency along with energy-efficiency gains of up to two orders of\nmagnitude. Both improvements come from the scheme ability to optimally leverage\nedge-devices diversity.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 21:07:34 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 10:12:23 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Paris", "Antoine", ""], ["Mirghasemi", "Hamed", ""], ["Stupia", "Ivan", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "2004.00138", "submitter": "Pierre Varlez", "authors": "Olivier Bal-P\\'etr\\'e, Pierre Varlez, Fernando Perez-Tellez", "title": "Pacloud: Towards a Universal Cloud-based Linux Package Manager", "comments": "Presented at CECCC 2019. 8 pages, 13 figures", "journal-ref": "Proceedings of the 2019 International Communication Engineering\n  and Cloud Computing Conference, October 2019, Pages 6-13", "doi": "10.1145/3380678.3380685", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Package managers are a very important part of Linux distributions but we have\nnoticed two weaknesses in them: They use pre-built packages that are not\noptimised for specific hardware and often they are too heavy for a specific\nneed, or packagesmay require plenty of time and resources to be compiled. In\nthis paper, we present a novel Linux package manager which uses cloud computing\nfeatures to compile and distribute Linux packages without impacting the end\nuser's performance. We also show how Portage, Gentoo's package manager can be\noptimised for customisation and performance, along with the cloud computing\nfeatures to compile Linux packages more efficiently. All of this resulting in a\nnew cloud-based Linux package manager that is built for better computing\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:07:52 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Bal-P\u00e9tr\u00e9", "Olivier", ""], ["Varlez", "Pierre", ""], ["Perez-Tellez", "Fernando", ""]]}, {"id": "2004.00215", "submitter": "Eric Goodman", "authors": "Eric L. Goodman, Dirk Grunwald", "title": "Streaming Temporal Graphs: Subgraph Matching", "comments": "Big Data 2019", "journal-ref": "Big Data 2019, pp. 4977-4986", "doi": "10.1109/BigData47090.2019.9006429", "report-no": null, "categories": "cs.PL cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate solutions to subgraph matching within a temporal stream of\ndata. We present a high-level language for describing temporal subgraphs of\ninterest, the Streaming Analytics Language (SAL). SAL programs are translated\ninto C++ code that is run in parallel on a cluster. We call this implementation\nof SAL the Streaming Analytics Machine (SAM). SAL programs are succinct,\nrequiring about 20 times fewer lines of code than using the SAM library\ndirectly, or writing an implementation using Apache Flink. To benchmark SAM we\ncalculate finding temporal triangles within streaming netflow data. Also, we\ncompare SAM to an implementation written for Flink. We find that SAM is able to\nscale to 128 nodes or 2560 cores, while Apache Flink has max throughput with 32\nnodes and degrades thereafter. Apache Flink has an advantage when triangles are\nrare, with max aggregate throughput for Flink at 32 nodes greater than the max\nachievable rate of SAM. In our experiments, when triangle occurrence was faster\nthan five per second per node, SAM performed better. Both frameworks may miss\nresults due to latencies in network communication. SAM consistently reported an\naverage of 93.7% of expected results while Flink decreases from 83.7% to 52.1%\nas we increase to the maximum size of the cluster. Overall, SAM can obtain\nrates of 91.8 billion netflows per day.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 03:42:00 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Goodman", "Eric L.", ""], ["Grunwald", "Dirk", ""]]}, {"id": "2004.00224", "submitter": "Dingwen Tao", "authors": "Sian Jin, Pascal Grosset, Christopher M. Biwer, Jesus Pulido, Jiannan\n  Tian, Dingwen Tao, James Ahrens", "title": "Understanding GPU-Based Lossy Compression for Extreme-Scale Cosmological\n  Simulations", "comments": "11 pages, 10 figures, published by IEEE IPDPS '20", "journal-ref": null, "doi": "10.1109/IPDPS47924.2020.00021", "report-no": null, "categories": "cs.DC astro-ph.IM physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help understand our universe better, researchers and scientists currently\nrun extreme-scale cosmology simulations on leadership supercomputers. However,\nsuch simulations can generate large amounts of scientific data, which often\nresult in expensive costs in data associated with data movement and storage.\nLossy compression techniques have become attractive because they significantly\nreduce data size and can maintain high data fidelity for post-analysis. In this\npaper, we propose to use GPU-based lossy compression for extreme-scale\ncosmological simulations. Our contributions are threefold: (1) we implement\nmultiple GPU-based lossy compressors to our opensource compression benchmark\nand analysis framework named Foresight; (2) we use Foresight to comprehensively\nevaluate the practicality of using GPU-based lossy compression on two\nreal-world extreme-scale cosmology simulations, namely HACC and Nyx, based on a\nseries of assessment metrics; and (3) we develop a general optimization\nguideline on how to determine the best-fit configurations for different lossy\ncompressors and cosmological simulations. Experiments show that GPU-based lossy\ncompression can provide necessary accuracy on post-analysis for cosmological\nsimulations and high compression ratio of 5~15x on the tested datasets, as well\nas much higher compression and decompression throughput than CPU-based\ncompressors.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 04:23:16 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 18:18:38 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Jin", "Sian", ""], ["Grosset", "Pascal", ""], ["Biwer", "Christopher M.", ""], ["Pulido", "Jesus", ""], ["Tian", "Jiannan", ""], ["Tao", "Dingwen", ""], ["Ahrens", "James", ""]]}, {"id": "2004.00372", "submitter": "Lars Larsson", "authors": "Lars Larsson, William T\\\"arneberg, Cristian Klein, Erik Elmroth, Maria\n  Kihl", "title": "Impact of etcd Deployment on Kubernetes, Istio, and Application\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By intrinsic necessity, Kubernetes is a complex platform. Its complexity\nmakes conducting performance analysis in that environment fraught with\ndifficulties and emergent behavior. Applications leveraging more \"moving parts\"\nsuch as the Istio service mesh makes the platform strictly more complex, not\nless. In this paper we study how underlying platform constitution and\ndeployment affects application performance, specifically in Kubernetes-based\nenvironments. We alter platform constitution via use of native Kubernetes\nnetworking or Istio. Platform deployment is altered via etcd data storage\nlocation at two extremes on the performance spectrum: network disk and RAM\ndisk. Our results show that etcd performance has a large impact on that of\nKubernetes and its ability to perform orchestration actions, and thereby\nindirectly on the performance of the application. The implication is that\nsystems researchers conducting performance evaluations cannot just consider\ntheir specific application as being under test, but must also take the\nunderlying Kubernetes platform into account. To conduct experiments of\nscientific rigor, we developed an experiment framework for conducting\nrepeatable and reproducible experiments. Our framework and resulting data set\nare openly available for the research community to build upon and reason about.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 08:50:58 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Larsson", "Lars", ""], ["T\u00e4rneberg", "William", ""], ["Klein", "Cristian", ""], ["Elmroth", "Erik", ""], ["Kihl", "Maria", ""]]}, {"id": "2004.00518", "submitter": "Mehrnoosh Shafiee", "authors": "Mehrnoosh Shafiee and Javad Ghaderi", "title": "Scheduling Parallel-Task Jobs Subject to Packing and Placement\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by modern parallel computing applications, we consider the problem\nof scheduling parallel-task jobs with heterogeneous resource requirements in a\ncluster of machines. Each job consists of a set of tasks that can be processed\nin parallel, however, the job is considered completed only when all its tasks\nfinish their processing, which we refer to as \"synchronization\" constraint.\nFurther, assignment of tasks to machines is subject to \"placement\" constraints,\ni.e., each task can be processed only on a subset of machines, and processing\ntimes can also be machine dependent. Once a task is scheduled on a machine, it\nrequires a certain amount of resource from that machine for the duration of its\nprocessing. A machine can process (\"pack\") multiple tasks at the same time,\nhowever the cumulative resource requirement of the tasks should not exceed the\nmachine's capacity.\n  Our objective is to minimize the weighted average of the jobs' completion\ntimes. The problem, subject to synchronization, packing and placement\nconstraints, is NP-hard, and prior theoretical results only concern much\nsimpler models. For the case that migration of tasks among the\nplacement-feasible machines is allowed, we propose a preemptive algorithm with\nan approximation ratio of $(6+\\epsilon)$. In the special case that only one\nmachine can process each task, we design an algorithm with improved\napproximation ratio of $4$. Finally, in the case that migrations (and\npreemptions) are not allowed, we design an algorithm with an approximation\nratio of $24$. Our algorithms use a combination of linear program relaxation\nand greedy packing techniques. We present extensive simulation results, using a\nreal traffic trace, that demonstrate that our algorithms yield significant\ngains over the prior approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 15:42:30 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 01:34:46 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Shafiee", "Mehrnoosh", ""], ["Ghaderi", "Javad", ""]]}, {"id": "2004.00773", "submitter": "Chuan Chen", "authors": "Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng and Qiang\n  Yan", "title": "A Blockchain-based Decentralized Federated Learning Framework with\n  Committee Consensus", "comments": "7 pages, 4 figures and 1 table", "journal-ref": null, "doi": "10.1109/MNET.011.2000263", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has been widely studied and applied to various scenarios.\nIn mobile computing scenarios, federated learning protects users from exposing\ntheir private data, while cooperatively training the global model for a variety\nof real-world applications. However, the security of federated learning is\nincreasingly being questioned, due to the malicious clients or central servers'\nconstant attack to the global model or user privacy data. To address these\nsecurity issues, we proposed a decentralized federated learning framework based\non blockchain, i.e., a Blockchain-based Federated Learning framework with\nCommittee consensus (BFLC). The framework uses blockchain for the global model\nstorage and the local model update exchange. To enable the proposed BFLC, we\nalso devised an innovative committee consensus mechanism, which can effectively\nreduce the amount of consensus computing and reduce malicious attacks. We then\ndiscussed the scalability of BFLC, including theoretical security, storage\noptimization, and incentives. Finally, we performed experiments using\nreal-world datasets to verify the effectiveness of the BFLC framework.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 02:04:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Li", "Yuzheng", ""], ["Chen", "Chuan", ""], ["Liu", "Nan", ""], ["Huang", "Huawei", ""], ["Zheng", "Zibin", ""], ["Yan", "Qiang", ""]]}, {"id": "2004.00969", "submitter": "Jacopo Soldani", "authors": "Vladimir Yussupov, Jacopo Soldani, Uwe Breitenb\\\"ucher, Antonio Brogi,\n  Frank Leymann", "title": "FaaSten Your Decisions: Classification Framework and Technology Review\n  of Function-as-a-Service Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function-as-a-Service (FaaS) is a cloud service model enabling developers to\noffload event-driven executable snippets of code. The execution and management\nof such functions becomes a FaaS provider's responsibility, hereby included\ntheir on-demand provisioning and automatic scaling. Key enablers for this cloud\nservice model are FaaS platforms, e.g., AWS Lambda, Microsoft Azure Functions\nor OpenFaaS. At the same time, the choice of the most appropriate FaaS platform\nfor deploying and running a serverless application is not trivial, as various\norganizational and technical aspects have to be taken into account. In this\nwork, we present (i) a FaaS platform classification framework derived using a\nmixed method study and (ii) a systematic technology review of the ten most\nprominent FaaS platforms, based on the proposed classification framework.\nMoreover, we present (iii) a FaaS platform selection support system, called\n\\faastener, which helps researchers and practitioners to choose the FaaS\nplatform most suited for their requirements.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 11:51:14 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Yussupov", "Vladimir", ""], ["Soldani", "Jacopo", ""], ["Breitenb\u00fccher", "Uwe", ""], ["Brogi", "Antonio", ""], ["Leymann", "Frank", ""]]}, {"id": "2004.01045", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao", "title": "Topological Properties of Multi-Party Blockchain Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-blockchain transaction remains one of the most challenging problems\nin blockchains. The root cause of the challenge lies in the nondeterministic\nnature of blockchains: A $n$-party transaction across multiple blockchains\nmight be partially rolled back due to the potential forks in any of the\nparticipating blockchains---eventually, only one fork will survive in the\ncompetition among miners. While some effort has recently been made to\ndeveloping hierarchically distributed commit protocols to make multi-party\ntransactions progress, there is no systematic method to reason about the\ntransaction outcome. This paper tackles this problem from a perspective of\npoint-set topology. We construct multiple topological spaces for the\ntransactions and blockchain forks, and show that these spaces are internally\nrelated through either homeomorphism or continuous functions. Combined\ntogether, these tools allow us to reason about the cross-blockchain\ntransactions through the growing-fork topology, an intuitive representation of\nblockchains.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 03:56:42 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 19:07:06 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 00:46:57 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zhao", "Dongfang", ""]]}, {"id": "2004.01062", "submitter": "EPTCS", "authors": "Stephanie Balzer (Carnegie Mellon University), Luca Padovani\n  (Universit\\`a di Torino)", "title": "Proceedings of the 12th International Workshop on Programming Language\n  Approaches to Concurrency- and Communication-cEntric Software", "comments": null, "journal-ref": "EPTCS 314, 2020", "doi": "10.4204/EPTCS.314", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern hardware platforms, from the very small to the very large,\nincreasingly provide parallel and distributed computing resources for\napplications to maximise performance. Many applications therefore need to make\neffective use of tens, hundreds, and even thousands of compute nodes.\nComputation in such systems is thus inherently concurrent and communication\ncentric. Effectively programming such applications is challenging; performance,\ncorrectness, and scalability are difficult to achieve. The development of\neffective programming methodologies for this increasingly parallel landscape\ntherefore demands exploration and understanding of a wide variety of\nfoundational and practical ideas. The International Workshop on Programming\nLanguage Approaches to Concurrency- and Communication-cEntric Software (PLACES)\nis dedicated to work in this area. The workshop offers a forum for researchers\nfrom different fields to exchange new ideas about these challenges to modern\nand future programming, where concurrency and distribution are the norm rather\nthan a marginal concern. This volume contains the proceedings of the 12th\nedition of PLACES, which was co-located with ETAPS 2020 in Dublin, Ireland.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:44:26 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Balzer", "Stephanie", "", "Carnegie Mellon University"], ["Padovani", "Luca", "", "Universit\u00e0 di Torino"]]}, {"id": "2004.01312", "submitter": "Nirupam Gupta", "authors": "Nirupam Gupta, Shripad Gade, Nikhil Chopra and Nitin H. Vaidya", "title": "Preserving Statistical Privacy in Distributed Optimization", "comments": "The updated version has simpler proofs. The paper has been\n  peer-reviewed, and accepted for the IEEE Control Systems Letters (L-CSS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed optimization protocol that preserves statistical\nprivacy of agents' local cost functions against a passive adversary that\ncorrupts some agents in the network. The protocol is a composition of a\ndistributed ``{\\em zero-sum}\" obfuscation protocol that obfuscates the agents'\nlocal cost functions, and a standard non-private distributed optimization\nmethod. We show that our protocol protects the statistical privacy of the\nagents' local cost functions against a passive adversary that corrupts up to\n$t$ arbitrary agents as long as the communication network has $(t+1)$-vertex\nconnectivity. The ``{\\em zero-sum}\" obfuscation protocol preserves the sum of\nthe agents' local cost functions and therefore ensures accuracy of the computed\nsolution.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 00:15:16 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 22:48:21 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gupta", "Nirupam", ""], ["Gade", "Shripad", ""], ["Chopra", "Nikhil", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2004.01477", "submitter": "Venkata Krishna P", "authors": "J. Mahalakshmi and P. Venkata Krishna", "title": "Localized Mobile Agent Framework for data processing on Internet of\n  Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) is the major research filed in the recent trends.\nIoT has the ability to create communication with any object. IoT produces big\namount of raw data at the time of data gathering. Therefore, there is a need of\nefficient mechanism to address the issue of IoT. This paper presents the\nlocalized MapReduce Framework for IoT. This frame work processes the data at\nthe local nodes without transferring the data to the cloud or high end servers.\nThe mobile agents have the ability to migrate from one node to another node for\ndata processing, Mapper and Reducer are used for agent duplication and result\naggregation. The performance of the proposed framework is evaluated using the\ncost function and the results proved the efficiency the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 11:35:43 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Mahalakshmi", "J.", ""], ["Krishna", "P. Venkata", ""]]}, {"id": "2004.01521", "submitter": "Zvezdin Besarabov", "authors": "Zvezdin Besarabov, Todor Kolev", "title": "Trustless parallel local search for effective distributed algorithm\n  discovery", "comments": "Submitted to IEEE Blockchain: From Technology to Marketplaces. arXiv\n  admin note: text overlap with arXiv:1909.03848", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metaheuristic search strategies have proven their effectiveness against\nman-made solutions in various contexts. They are generally effective in local\nsearch area exploitation, and their overall performance is largely impacted by\nthe balance between exploration and exploitation.\n  Recent developments in parallel local search explore methods to take\nadvantage of the efficient local exploitation of searches and reach impressive\nresults. This however restricts the scaling potential to nodes within a\nprivate, trusted computer cluster.\n  In this research we propose a novel blockchain protocol that allows parallel\nlocal search to scale to untrusted and anonymous computational nodes. The\nprotocol introduces publicly verifiable performance evaluation of the local\noptima reported by each node, creating a competitive environment between the\nlocal searches. That is strengthened with economical stimuli for producing good\nsolutions, that provide coordination between the nodes, as every node tries to\nexplore different sections of the search space to beat their competition.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 12:03:38 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Besarabov", "Zvezdin", ""], ["Kolev", "Todor", ""]]}, {"id": "2004.01531", "submitter": "Peter Hillmann", "authors": "Peter Hillmann, Lars Stiemert, Gabi Dreo, Oliver Rose", "title": "On the Path to High Precise IP Geolocation: A Self-Optimizing Model", "comments": "12 pages, 15 figures", "journal-ref": "International Journal of Intelligent Computing Research (IJICR),\n  Volume 7, Issue 1, March 2016", "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IP Geolocation is a key enabler for the Future Internet to provide\ngeographical location information for application services. For example, this\ndata is used by Content Delivery Networks to assign users to mirror servers,\nwhich are close by, hence providing enhanced traffic management. It is still a\nchallenging task to obtain precise and stable location information, whereas\nproper results are only achieved by the use of active latency measurements.\nThis paper presents an advanced approach for an accurate and self-optimizing\nmodel for location determination, including identification of optimized\nLandmark positions, which are used for probing. Moreover, the selection of\ncorrelated data and the estimated target location requires a sophisticated\nstrategy to identify the correct position. We present an improved approximation\nof network distances of usually unknown TIER infrastructures using the road\nnetwork. Our concept is evaluated under real-world conditions focusing Europe.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 12:45:27 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Hillmann", "Peter", ""], ["Stiemert", "Lars", ""], ["Dreo", "Gabi", ""], ["Rose", "Oliver", ""]]}, {"id": "2004.01562", "submitter": "Francesco d'Amore", "authors": "Andrea Clementi, Francesco d'Amore (COATI), George Giakkoupis (WIDE),\n  Emanuele Natale (COATI)", "title": "Search via Parallel L{\\'e}vy Walks on ${\\mathbb Z}^2$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the \\emph{L{\\'e}vy foraging hypothesis} -- the premise that\nvarious animal species have adapted to follow \\emph{L{\\'e}vy walks} to optimize\ntheir search efficiency -- we study the parallel hitting time of L{\\'e}vy walks\non the infinite two-dimensional grid.We consider $k$ independent discrete-time\nL{\\'e}vy walks, with the same exponent $\\alpha \\in(1,\\infty)$, that start from\nthe same node, and analyze the number of steps until the first walk visits a\ngiven target at distance $\\ell$.We show that for any choice of $k$ and $\\ell$\nfrom a large range, there is a unique optimal exponent $\\alpha\\_{k,\\ell} \\in\n(2,3)$, for which the hitting time is $\\tilde O(\\ell^2/k)$ w.h.p., while\nmodifying the exponent by an $\\epsilon$ term increases the hitting time by a\npolynomial factor, or the walks fail to hit the target almost surely.Based on\nthat, we propose a surprisingly simple and effective parallel search strategy,\nfor the setting where $k$ and $\\ell$ are unknown:The exponent of each L{\\'e}vy\nwalk is just chosen independently and uniformly at random from the interval\n$(2,3)$.This strategy achieves optimal search time (modulo polylogarithmic\nfactors) among all possible algorithms (even centralized ones that know\n$k$).Our results should be contrasted with a line of previous work showing that\nthe exponent $\\alpha = 2$ is optimal for various search problems.In our setting\nof $k$ parallel walks, we show that the optimal exponent depends on $k$ and\n$\\ell$, and that randomizing the choice of the exponents works simultaneously\nfor all $k$ and $\\ell$.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:39:36 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 15:05:17 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 09:11:04 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 07:49:47 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Clementi", "Andrea", "", "COATI"], ["d'Amore", "Francesco", "", "COATI"], ["Giakkoupis", "George", "", "WIDE"], ["Natale", "Emanuele", "", "COATI"]]}, {"id": "2004.01635", "submitter": "Kaan Kara", "authors": "Kaan Kara, Christoph Hagleitner, Dionysios Diamantopoulos, Dimitris\n  Syrivelis, Gustavo Alonso", "title": "High Bandwidth Memory on FPGAs: A Data Analytics Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA-based data processing in datacenters is increasing in popularity due to\nthe demands of modern workloads and the ensuing necessity for specialization in\nhardware. Driven by this trend, vendors are rapidly adapting reconfigurable\ndevices to suit data and compute intensive workloads. Inclusion of High\nBandwidth Memory (HBM) in FPGA devices is a recent example. HBM promises\novercoming the bandwidth bottleneck, faced often by FPGA-based accelerators due\nto their throughput oriented design. In this paper, we study the usage and\nbenefits of HBM on FPGAs from a data analytics perspective. We consider three\nworkloads that are often performed in analytics oriented databases and\nimplement them on FPGA showing in which cases they benefit from HBM: range\nselection, hash join, and stochastic gradient descent for linear model\ntraining. We integrate our designs into a columnar database (MonetDB) and show\nthe trade-offs arising from the integration related to data movement and\npartitioning. In certain cases, FPGA+HBM based solutions are able to surpass\nthe highest performance provided by either a 2-socket POWER9 system or a\n14-core XeonE5 by up to 1.8x (selection), 12.9x (join), and 3.2x (SGD).\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 11:29:31 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Kara", "Kaan", ""], ["Hagleitner", "Christoph", ""], ["Diamantopoulos", "Dionysios", ""], ["Syrivelis", "Dimitris", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2004.01636", "submitter": "Joshua Mack", "authors": "Joshua Mack, Nirmal Kumbhare, Anish NK, Umit Y. Ogras, Ali Akoglu", "title": "User-Space Emulation Framework for Domain-Specific SoC Design", "comments": "11 pages, 11 figures. To be published in proceedings of 2020\n  Heterogeneity in Computing Workshop http://hcw.oucreate.com/ held in\n  conjunction with IPDPS 2020 http://www.ipdps.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a portable, Linux-based emulation framework to\nprovide an ecosystem for hardware-software co-design of Domain-specific SoCs\n(DSSoCs) and enable their rapid evaluation during the pre-silicon design phase.\nThis framework holistically targets three key challenges of DSSoC design:\naccelerator integration, resource management, and application development. We\naddress these challenges via a flexible and lightweight user-space runtime\nenvironment that enables easy integration of new accelerators, scheduling\nheuristics, and user applications, and we illustrate the utility of each\nthrough various case studies. With signal processing (WiFi and RADAR) as the\ntarget domain, we use our framework to evaluate the performance of various\ndynamic workloads on hypothetical DSSoC hardware configurations composed of\nmixtures of CPU cores and FFT accelerators using a Zynq UltraScale+TM MPSoC. We\nshow the portability of this framework by conducting a similar study on an\nOdroid platform composed of big.LITTLE ARM clusters. Finally, we introduce a\nprototype compilation toolchain that enables automatic mapping of unlabeled C\ncode to DSSoC platforms. Taken together, this environment offers a unique\necosystem to rapidly perform functional verification and obtain performance and\nutilization estimates that help accelerate convergence towards a final DSSoC\ndesign.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 06:33:25 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 00:50:27 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Mack", "Joshua", ""], ["Kumbhare", "Nirmal", ""], ["NK", "Anish", ""], ["Ogras", "Umit Y.", ""], ["Akoglu", "Ali", ""]]}, {"id": "2004.01637", "submitter": "Soramichi Akiyama", "authors": "Soramichi Akiyama", "title": "Assessing Impact of Data Partitioning for Approximate Memory in C/C++\n  Code", "comments": "Presented in the 10th Workshop on Systems for Post-Moore\n  Architectures (SPMA), April 27, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate memory is a technique to mitigate the performance gap between\nmemory subsystems and CPUs with its reduced access latency at a cost of data\nintegrity. To gain benefit from approximate memory for realistic applications,\nit is crucial to partition applications' data to approximate data and critical\ndata and apply different error rates. However, error rates cannot be controlled\nin a fine-grained manner (e.g., per byte) due to fundamental limitations of how\napproximate memory can be realized. Due to this, if approximate data and\ncritical data are interleaved in a data structure (e.g., a C struct that has a\npointer and an approximatable number as its members), data partitioning may\ndegrade the application's performance because the data structure must be split\nto separate memory regions that have different error rates. This paper is the\nfirst to conduct an analysis of realistic C/C++ code to assess the impact of\nthis problem. First, we find the type of data (e.g., \"int\", \"struct point\")\nthat is assessed by the instruction that incurs the largest number of cache\nmisses in a benchmark, which we refer to as the target data type. Second, we\nqualitatively estimate if the target data type of an application has\napproximate data and critical data interleaved. To this end, we set up three\ncriteria to analyze it because definitively distinguishing a piece of data as\napproximate data or critical data is infeasible since it depends on each\nuse-case. We analyze 11 memory intensive benchmarks from SPEC CPU 2006 and 2\ngraph analytics frameworks, and show that the target data types of 9 benchmarks\nare either a C struct or a C++ class (criterion 1). Among them, two have a\npointer and a non-pointer member together (criterion 2) and three have a\nfloating point number and other members together (criterion 3).\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 02:36:32 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Akiyama", "Soramichi", ""]]}, {"id": "2004.01639", "submitter": "Shiming Li", "authors": "Shiming Li, Shasha Guo, Limeng Zhang, Ziyang Kang, Shiying Wang, Wei\n  Shi, Lei Wang, Weixia Xu", "title": "SNEAP: A Fast and Efficient Toolchain for Mapping Large-Scale Spiking\n  Neural Network onto NoC-based Neuromorphic Platform", "comments": "Accepted by GLSVLSI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural network (SNN), as the third generation of artificial neural\nnetworks, has been widely adopted in vision and audio tasks. Nowadays, many\nneuromorphic platforms support SNN simulation and adopt Network-on-Chips (NoC)\narchitecture for multi-cores interconnection.\n  However, interconnection brings huge area overhead to the platform. Moreover,\nrun-time communication on the interconnection has a significant effect on the\ntotal power consumption and performance of the platform. In this paper, we\npropose a toolchain called SNEAP for mapping SNNs to neuromorphic platforms\nwith multi-cores, which aims to reduce the energy and latency brought by spike\ncommunication on the interconnection.\n  SNEAP includes two key steps: partitioning the SNN to reduce the spikes\ncommunicated between partitions, and mapping the partitions of SNN to the NoC\nto reduce average hop of spikes under the constraint of hardware resources.\nSNEAP can reduce more spikes communicated on the interconnection of NoC and\nspend less time than other toolchains in the partitioning phase. Moreover, the\naverage hop of spikes is reduced more by SNEAP within a time period, which\neffectively reduces the energy and latency on the NoC-based neuromorphic\nplatform.\n  The experimental results show that SNEAP can achieve 418x reduction in\nend-to-end execution time, and reduce energy consumption and spike latency, on\naverage, by 23% and 51% respectively, compared with SpiNeMap.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:09:08 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Li", "Shiming", ""], ["Guo", "Shasha", ""], ["Zhang", "Limeng", ""], ["Kang", "Ziyang", ""], ["Wang", "Shiying", ""], ["Shi", "Wei", ""], ["Wang", "Lei", ""], ["Xu", "Weixia", ""]]}, {"id": "2004.01640", "submitter": "Imane Hilal Ms", "authors": "Fatima Ezzahra Mdarbi, Nadia Afifi, Imane Hilal, Hicham Belhadaoui", "title": "An Approach for Selecting Cloud Service Adequate to Big Data Case Study:\n  E-health Context", "comments": "13 pages, 4 figures, 16 tables", "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS), 2018, vol. 16, no 8", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expanding Cloud computing's services offers great opportunities for\nconsumers to find the best service and best cost. It offers a computing power\nand a storage space adapted especially for Big Data processing. However, it\nraises new challenges on how to select the best service out of the huge pool.\nIt is time-consuming for consumers to collect the necessary information and\nanalyze all service providers to make the right decision. Moreover, it'is a\nhighly demanding task from a computational perspective, because the same\ncomputations may be conducted repeatedly by multiple consumers who have similar\nrequirements. Therefore, in this paper, we propose an approach based on\nAnalytic Hierarchy Process (AHP) method, which manages the selection of the\nCloud Service adequate to Big Data based on its parameters and criteria. We\napplied this approach on a case study in order to validate its efficity. The\nstudied case is about the selection of the adequate Cloud Service for Big Data\nin the context of National Health Service (NHS) of United Kingdom (UK).\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:26:01 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Mdarbi", "Fatima Ezzahra", ""], ["Afifi", "Nadia", ""], ["Hilal", "Imane", ""], ["Belhadaoui", "Hicham", ""]]}, {"id": "2004.01743", "submitter": "Zitao Chen", "authors": "Zitao Chen, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik\n  Pattabiraman and Nathan DeBardeleben", "title": "TensorFI: A Flexible Fault Injection Framework for TensorFlow\n  Applications", "comments": "A preliminary version of this work was published in a workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning (ML) has seen increasing adoption in safety-critical\ndomains (e.g., autonomous vehicles), the reliability of ML systems has also\ngrown in importance. While prior studies have proposed techniques to enable\nefficient error-resilience techniques (e.g., selective instruction\nduplication), a fundamental requirement for realizing these techniques is a\ndetailed understanding of the application's resilience.\n  In this work, we present TensorFI, a high-level fault injection (FI)\nframework for TensorFlow-based applications. TensorFI is able to inject both\nhardware and software faults in general TensorFlow programs. TensorFI is a\nconfigurable FI tool that is flexible, easy to use, and portable. It can be\nintegrated into existing TensorFlow programs to assess their resilience for\ndifferent fault types (e.g., faults in particular operators). We use TensorFI\nto evaluate the resilience of 12 ML programs, including DNNs used in the\nautonomous vehicle domain. Our tool is publicly available at\nhttps://github.com/DependableSystemsLab/TensorFI.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 19:26:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chen", "Zitao", ""], ["Narayanan", "Niranjhana", ""], ["Fang", "Bo", ""], ["Li", "Guanpeng", ""], ["Pattabiraman", "Karthik", ""], ["DeBardeleben", "Nathan", ""]]}, {"id": "2004.01908", "submitter": "Ingo M\\\"uller", "authors": "Ingo M\\\"uller (1) and Renato Marroqu\\'in (2) and Dimitrios Koutsoukos\n  (1) and Mike Wawrzoniak (1) and Sabir Akhadov (3) and Gustavo Alonso (1) ((1)\n  Systems Group, Department of Computer Science, ETH Zurich, (2) Oracle Labs,\n  (3) Databricks)", "title": "The Collection Virtual Machine: An Abstraction for Multi-Frontend\n  Multi-Backend Data Analysis", "comments": "This paper is currently under review at DaMoN'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Getting the best performance from the ever-increasing number of hardware\nplatforms has been a recurring challenge for data processing systems. In recent\nyears, the advent of data science with its increasingly numerous and complex\ntypes of analytics has made this challenge even more difficult. In practice,\nsystem designers are overwhelmed by the number of combinations and typically\nimplement only one analysis/platform combination, leading to repeated\nimplementation effort -- and a plethora of semi-compatible tools for data\nscientists.\n  In this paper, we propose the \"Collection Virtual Machine\" (or CVM) -- an\nextensible compiler framework designed to keep the specialization process of\ndata analytics systems tractable. It can capture at the same time the essence\nof a large span of low-level, hardware-specific implementation techniques as\nwell as high-level operations of different types of analyses. At its core lies\na language for defining nested, collection-oriented intermediate\nrepresentations (IRs). Frontends produce programs in their IR flavors defined\nin that language, which get optimized through a series of rewritings (possibly\nchanging the IR flavor multiple times) until the program is finally expressed\nin an IR of platform-specific operators. While reducing the overall\nimplementation effort, this also improves the interoperability of both analyses\nand hardware platforms. We have used CVM successfully to build specialized\nbackends for platforms as diverse as multi-core CPUs, RDMA clusters, and\nserverless computing infrastructure in the cloud and expect similar results for\nmany more frontends and hardware platforms in the near future.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 11:02:36 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 19:48:05 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["M\u00fcller", "Ingo", ""], ["Marroqu\u00edn", "Renato", ""], ["Koutsoukos", "Dimitrios", ""], ["Wawrzoniak", "Mike", ""], ["Akhadov", "Sabir", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2004.01939", "submitter": "Quanquan C. Liu", "authors": "Thaddeus Dryja, Quanquan C. Liu, Neha Narula", "title": "A Lower Bound for Byzantine Agreement and Consensus for Adaptive\n  Adversaries using VDFs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale cryptocurrencies require the participation of millions of\nparticipants and support economic activity of billions of dollars, which has\nled to new lines of work in binary Byzantine Agreement (BBA) and consensus. The\nnew work aims to achieve communication-efficiency---given such a large $n$, not\neveryone can speak during the protocol. Several protocols have achieved\nconsensus with communication-efficiency, even under an adaptive adversary, but\nthey require additional strong assumptions---proof-of-work, memory-erasure,\netc. All of these protocols use multicast: every honest replica multicasts\nmessages to all other replicas. Under this model, we provide a new\ncommunication-efficient consensus protocol using Verifiable Delay Functions\n(VDFs) that is secure against adaptive adversaries and does not require the\nsame strong assumptions present in other protocols.\n  A natural question is whether we can extend the synchronous protocols to the\npartially synchronous setting---in this work, we show that using multicast, we\ncannot. Furthermore, we cannot achieve always safe communication-efficient\nprotocols (that maintain safety with probability 1) even in the synchronous\nsetting against a static adversary when honest replicas only choose to\nmulticast its messages. Considering these impossibility results, we describe a\nnew communication-efficient BBA protocol in a modified partially synchronous\nnetwork model which is secure against adaptive adversaries with high\nprobability.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 14:14:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Dryja", "Thaddeus", ""], ["Liu", "Quanquan C.", ""], ["Narula", "Neha", ""]]}, {"id": "2004.01953", "submitter": "Minghui LiWang", "authors": "Minghui LiWang, Zhibin Gao, Seyyedali Hosseinalipour, Huaiyu Dai,\n  Xianbin Wang", "title": "Energy-aware Allocation of Graph Jobs in Vehicular Cloud\n  Computing-enabled Software-defined IoV", "comments": "6 pages, 4 figures, INFOCOM WORKSHOP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-defined internet of vehicles (SDIoV) has emerged as a promising\nparadigm to realize flexible and comprehensive resource management, for next\ngeneration automobile transportation systems. In this paper, a vehicular cloud\ncomputing-based SDIoV framework is studied wherein the joint allocation of\ntransmission power and graph job is formulated as a nonlinear integer\nprogramming problem. To effectively address the problem, a\nstructure-preservation-based two-stage allocation scheme is proposed that\ndecouples template searching from power allocation. Specifically, a\nhierarchical tree-based random subgraph isomorphism mechanism is applied in the\nfirst stage by identifying potential mappings (templates) between the\ncomponents of graph jobs and service providers. A structure-preserving\nsimulated annealing-based power allocation algorithm is adopted in the second\nstage to achieve the trade-off between the job completion time and energy\nconsumption. Extensive simulations are conducted to verify the performance of\nthe proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 15:06:33 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 14:03:34 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["LiWang", "Minghui", ""], ["Gao", "Zhibin", ""], ["Hosseinalipour", "Seyyedali", ""], ["Dai", "Huaiyu", ""], ["Wang", "Xianbin", ""]]}, {"id": "2004.02003", "submitter": "Sudhanshu Sane", "authors": "Sudhanshu Sane, Abhishek Yenpure, Roxana Bujack, Matthew Larsen,\n  Kenneth Moreland, Christoph Garth and Hank Childs", "title": "Scalable In Situ Lagrangian Flow Map Extraction: Demonstrating the\n  Viability of a Communication-Free Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and evaluate a new algorithm for the in situ extraction of\nLagrangian flow maps, which we call Boundary Termination Optimization (BTO).\nOur approach is a communication-free model, requiring no message passing or\nsynchronization between processes, improving scalability, thereby reducing\noverall execution time and alleviating the encumbrance placed on simulation\ncodes from in situ processing. We terminate particle integration at node\nboundaries and store only a subset of the flow map that would have been\nextracted by communicating particles across nodes, thus introducing an\naccuracy-performance tradeoff. We run experiments with as many as 2048 GPUs and\nwith multiple simulation data sets. For the experiment configurations we\nconsider, our findings demonstrate that our communication-free technique saves\nas much as 2x to 4x in execution time in situ, while staying nearly as accurate\nquantitatively and qualitatively as previous work. Most significantly, this\nstudy establishes the viability of approaching in situ Lagrangian flow map\nextraction using communication-free models in the future.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 19:21:28 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sane", "Sudhanshu", ""], ["Yenpure", "Abhishek", ""], ["Bujack", "Roxana", ""], ["Larsen", "Matthew", ""], ["Moreland", "Kenneth", ""], ["Garth", "Christoph", ""], ["Childs", "Hank", ""]]}, {"id": "2004.02020", "submitter": "Haofan Zheng", "authors": "Haofan Zheng, Owen Arden", "title": "Building secure distributed applications the DECENT way", "comments": "15 pages, 18 figures. Added formal verification result for the\n  protocols; clarified verifiers and revokers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote attestation (RA) authenticates code running in trusted execution\nenvironments (TEEs), allow trusted code to be deployed even on untrusted hosts.\nHowever, trust relationships established by one component in a distributed\napplication may impact the security of other components, making it difficult to\nreason about the security of the application as a whole. Furthermore,\ntraditional RA approaches interact badly with modern web service design, which\ntends to employ small interacting microservices, short session lifetimes, and\nlittle or no state.\n  This paper presents the Decent Application Platform, a framework for building\nsecure decentralized applications. Decent applications authenticate and\nauthorize distributed enclave components using a protocol based on\nself-attestation certificates, a reusable credential based on RA and verifiable\nby a third party. Components mutually authenticate each other not only based on\ntheir code, but also based on the other components they trust, ensuring that no\ntransitively-connected components receive unauthorized information. While some\nother TEE frameworks support mutual authentication in some form, Decent is the\nonly system that supports mutual authentication without requiring an additional\ntrusted third party besides the trusted hardware's manufacturer. We have\nverified the secrecy and authenticity of Decent application data in ProVerif,\nand implemented two applications to evaluate Decent's expressiveness and\nperformance: DecentRide, a ride-sharing service, and DecentHT, a distributed\nhash table. On the YCSB benchmark, we show that DecentHT achieves 7.5x higher\nthroughput and 3.67x lower latency compared to a non-Decent implementation.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 21:20:38 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 04:25:16 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zheng", "Haofan", ""], ["Arden", "Owen", ""]]}, {"id": "2004.02089", "submitter": "Conrad M Albrecht", "authors": "Conrad M Albrecht, Marcus Freitag, Theodore G van Kessel, Siyuan Lu,\n  Hendrik F Hamann", "title": "Event Clustering & Event Series Characterization on Expected Frequency", "comments": null, "journal-ref": "2017 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData.2017.8258495", "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient clustering algorithm applicable to one-dimensional\ndata such as e.g. a series of timestamps. Given an expected frequency $\\Delta\nT^{-1}$, we introduce an $\\mathcal{O}(N)$-efficient method of characterizing\n$N$ events represented by an ordered series of timestamps $t_1,t_2,\\dots,t_N$.\nIn practice, the method proves useful to e.g. identify time intervals of\n\"missing\" data or to locate \"isolated events\". Moreover, we define measures to\nquantify a series of events by varying $\\Delta T$ to e.g. determine the quality\nof an Internet of Things service.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 04:06:59 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Albrecht", "Conrad M", ""], ["Freitag", "Marcus", ""], ["van Kessel", "Theodore G", ""], ["Lu", "Siyuan", ""], ["Hamann", "Hendrik F", ""]]}, {"id": "2004.02109", "submitter": "Shahin Nazarian", "authors": "Shahin Nazarian and Paul Bogdan", "title": "S4oC: A Self-optimizing, Self-adapting Secure System-on-Chip Design\n  Framework to Tackle Unknown Threats -- A Network Theoretic, Learning Approach", "comments": "This is an invited paper to ISCAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for the design and optimization of a secure\nself-optimizing, self-adapting system-on-chip (S4oC) architecture. The goal is\nto minimize the impact of attacks such as hardware Trojan and side-channel, by\nmaking real-time adjustments. S4oC learns to reconfigure itself, subject to\nvarious security measures and attacks, some of which possibly unknown at design\ntime. Furthermore, the data types and patterns of the target applications,\nenvironmental conditions, and sources of variations are incorporated. S4oC is a\nmanycore system, modeled as a four-layer graph, representing the model of\ncomputation (MoCp), model of connection (MoCn), model of memory (MoM) and model\nof storage (MoS), with a large number of elements including heterogeneous\nreconfigurable processing elements in MoCp, and memory elements in the MoM\nlayer. Security driven community detection, and neural networks are utilized\nfor application task clustering, and distributed reinforcement learning (RL)\nfor task mapping.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 06:55:05 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Nazarian", "Shahin", ""], ["Bogdan", "Paul", ""]]}, {"id": "2004.02163", "submitter": "Aritra Dutta", "authors": "Atal Narayan Sahu and Aritra Dutta and Aashutosh Tiwari and Peter\n  Richt\\'arik", "title": "On the Convergence Analysis of Asynchronous SGD for Solving Consistent\n  Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the realm of big data and machine learning, data-parallel, distributed\nstochastic algorithms have drawn significant attention in the present\ndays.~While the synchronous versions of these algorithms are well understood in\nterms of their convergence, the convergence analyses of their asynchronous\ncounterparts are not widely studied. In this paper, we propose and analyze a\n{\\it distributed, asynchronous parallel} SGD in light of solving an arbitrary\nconsistent linear system by reformulating the system into a stochastic\noptimization problem as studied by Richt\\'{a}rik and Tak\\'{a}\\~{c} in [35]. We\ncompare the convergence rates of our asynchronous SGD algorithm with the\nsynchronous parallel algorithm proposed by Richt\\'{a}rik and Tak\\'{a}\\v{c} in\n[35] under different choices of the hyperparameters---the stepsize, the damping\nfactor, the number of processors, and the delay factor. We show that our\nasynchronous parallel SGD algorithm also enjoys a global linear convergence\nrate, similar to the {\\em basic} method and the synchronous parallel method in\n[35] for solving any arbitrary consistent linear system via stochastic\nreformulation. We also show that our asynchronous parallel SGD improves upon\nthe {\\em basic} method with a better convergence rate when the number of\nprocessors is larger than four. We further show that this asynchronous approach\nperforms asymptotically better than its synchronous counterpart for certain\nlinear systems. Moreover, for certain linear systems, we compute the minimum\nnumber of processors required for which our asynchronous parallel SGD is\nbetter, and find that this number can be as low as two for some ill-conditioned\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 11:28:29 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sahu", "Atal Narayan", ""], ["Dutta", "Aritra", ""], ["Tiwari", "Aashutosh", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2004.02253", "submitter": "Carlos Segarra", "authors": "Paulo Gouveia, Jo\\~ao Neves, Carlos Segarra, Luca Liechti, Shady Issa,\n  Valerio Schiavoni, Miguel Matos", "title": "Kollaps: Decentralized and Dynamic Topology Emulation", "comments": "16 pages, EuroSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance and behavior of large-scale distributed applications is\nhighly influenced by network properties such as latency, bandwidth, packet\nloss, and jitter. For instance, an engineer might need to answer questions such\nas: What is the impact of an increase in network latency in application\nresponse time? How does moving a cluster between geographical regions affect\napplication throughput? How network dynamics affects application stability?\nAnswering these questions in a systematic and reproducible way is very hard,\ngiven the variability and lack of control over the underlying network.\nUnfortunately, state-of-the-art network emulation or testbeds scale poorly\n(i.e., MiniNet), focus exclusively on the control-plane (i.e., CrystalNet) or\nignore network dynamics (i.e., EmuLab). Kollaps is a fully distributed network\nemulator that address these limitations. Kollaps hinges on two key\nobservations. First, from an application's perspective, what matters are the\nemergent end-to-end properties (e.g., latency, bandwidth, packet loss, and\njitter) rather than the internal state of the routers and switches leading to\nthose properties. This premise allows us to build a simpler, dynamically\nadaptable, emulation model that circumvent maintaining the full network state.\nSecond, this simplified model is maintainable in a fully decentralized way,\nallowing the emulation to scale with the number of machines for the\napplication. Kollaps is fully decentralized, agnostic of the application\nlanguage and transport protocol, scales to thousands of processes and is\naccurate when compared against a bare-metal deployment or state-of-the-art\napproaches that emulate the full state of the network. We showcase how Kollaps\ncan accurately reproduce results from the literature and predict the behaviour\nof a complex unmodified distributed key-value store (i.e., Cassandra) under\ndifferent deployments.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 16:57:33 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Gouveia", "Paulo", ""], ["Neves", "Jo\u00e3o", ""], ["Segarra", "Carlos", ""], ["Liechti", "Luca", ""], ["Issa", "Shady", ""], ["Schiavoni", "Valerio", ""], ["Matos", "Miguel", ""]]}, {"id": "2004.02297", "submitter": "Sicong Zhuang", "authors": "Sicong Zhuang, Cristiano Malossi and Marc Casas", "title": "Reducing Data Motion to Accelerate the Training of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reduces the cost of DNNs training by decreasing the amount of data\nmovement across heterogeneous architectures composed of several GPUs and\nmulticore CPU devices. In particular, this paper proposes an algorithm to\ndynamically adapt the data representation format of network weights during\ntraining. This algorithm drives a compression procedure that reduces data size\nbefore sending them over the parallel system. We run an extensive evaluation\ncampaign considering several up-to-date deep neural network models and two\nhigh-end parallel architectures composed of multiple GPUs and CPU multicore\nchips. Our solution achieves average performance improvements from 6.18\\% up to\n11.91\\%.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 19:59:49 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhuang", "Sicong", ""], ["Malossi", "Cristiano", ""], ["Casas", "Marc", ""]]}, {"id": "2004.02336", "submitter": "He Li", "authors": "Xi Chen and Jason D. Lee and He Li and Yun Yang", "title": "Distributed Estimation for Principal Component Analysis: an Enlarged\n  Eigenspace Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing size of modern data sets brings many challenges to the existing\nstatistical estimation approaches, which calls for new distributed\nmethodologies. This paper studies distributed estimation for a fundamental\nstatistical machine learning problem, principal component analysis (PCA).\nDespite the massive literature on top eigenvector estimation, much less is\npresented for the top-$L$-dim ($L>1$) eigenspace estimation, especially in a\ndistributed manner. We propose a novel multi-round algorithm for constructing\ntop-$L$-dim eigenspace for distributed data. Our algorithm takes advantage of\nshift-and-invert preconditioning and convex optimization. Our estimator is\ncommunication-efficient and achieves a fast convergence rate. In contrast to\nthe existing divide-and-conquer algorithm, our approach has no restriction on\nthe number of machines. Theoretically, the traditional Davis-Kahan theorem\nrequires the explicit eigengap assumption to estimate the top-$L$-dim\neigenspace. To abandon this eigengap assumption, we consider a new route in our\nanalysis: instead of exactly identifying the top-$L$-dim eigenspace, we show\nthat our estimator is able to cover the targeted top-$L$-dim population\neigenspace. Our distributed algorithm can be applied to a wide range of\nstatistical problems based on PCA, such as principal component regression and\nsingle index model. Finally, We provide simulation studies to demonstrate the\nperformance of the proposed distributed estimator.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:28:08 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 19:43:28 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 02:24:02 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Chen", "Xi", ""], ["Lee", "Jason D.", ""], ["Li", "He", ""], ["Yang", "Yun", ""]]}, {"id": "2004.02369", "submitter": "Keval Vora", "authors": "Kasra Jamshidi, Rakesh Mahadasa, Keval Vora", "title": "Peregrine: A Pattern-Aware Graph Mining System", "comments": "This is the full version of the paper appearing in the European\n  Conference on Computer Systems (EuroSys), 2020", "journal-ref": null, "doi": "10.1145/3342195.3387548", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph mining workloads aim to extract structural properties of a graph by\nexploring its subgraph structures. General purpose graph mining systems provide\na generic runtime to explore subgraph structures of interest with the help of\nuser-defined functions that guide the overall exploration process. However, the\nstate-of-the-art graph mining systems remain largely oblivious to the shape (or\npattern) of the subgraphs that they mine. This causes them to: (a) explore\nunnecessary subgraphs; (b) perform expensive computations on the explored\nsubgraphs; and, (c) hold intermediate partial subgraphs in memory; all of which\naffect their overall performance. Furthermore, their programming models are\noften tied to their underlying exploration strategies, which makes it difficult\nfor domain users to express complex mining tasks.\n  In this paper, we develop Peregrine, a pattern-aware graph mining system that\ndirectly explores the subgraphs of interest while avoiding exploration of\nunnecessary subgraphs, and simultaneously bypassing expensive computations\nthroughout the mining process. We design a pattern-based programming model that\ntreats \"graph patterns\" as first class constructs and enables Peregrine to\nextract the semantics of patterns, which it uses to guide its exploration. Our\nevaluation shows that Peregrine outperforms state-of-the-art distributed and\nsingle machine graph mining systems, and scales to complex mining tasks on\nlarger graphs, while retaining simplicity and expressivity with its\n\"pattern-first\" programming approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 01:33:55 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Jamshidi", "Kasra", ""], ["Mahadasa", "Rakesh", ""], ["Vora", "Keval", ""]]}, {"id": "2004.02372", "submitter": "Yu Chen", "authors": "Qian Qu, Ronghua Xu, Seyed Yahya Nikouei, Yu Chen", "title": "An Experimental Study on Microservices based Edge Computing Platforms", "comments": "Accepted by the 2020 IEEE INFOCOM WKSHPS: CNERT: Computer and\n  Networking Experimental Research using Testbeds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid technological advances in the Internet of Things (IoT) allows the\nblueprint of Smart Cities to become feasible by integrating heterogeneous\ncloud/fog/edge computing paradigms to collaboratively provide variant smart\nservices in our cities and communities. Thanks to attractive features like fine\ngranularity and loose coupling, the microservices architecture has been\nproposed to provide scalable and extensible services in large scale distributed\nIoT systems. Recent studies have evaluated and analyzed the performance\ninterference between microservices based on scenarios on the cloud computing\nenvironment. However, they are not holistic for IoT applications given the\nrestriction of the edge device like computation consumption and network\ncapacity. This paper investigates multiple microservice deployment policies on\nthe edge computing platform. The microservices are developed as docker\ncontainers, and comprehensive experimental results demonstrate the performance\nand interference of microservices running on benchmark scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 02:03:19 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Qu", "Qian", ""], ["Xu", "Ronghua", ""], ["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""]]}, {"id": "2004.02439", "submitter": "Arvind Easwaran", "authors": "Arvind Easwaran, Insik Shin, Insup Lee", "title": "Optimal Virtual Cluster-based Multiprocessor Scheduling", "comments": "This is a post-peer-review, pre-copyedit version of an article\n  published in Springer Real-Time Systems journal. The final authenticated\n  version is available online at: https://doi.org/10.1007/s11241-009-9073-x", "journal-ref": "Springer Real-Time Systems, Volume 43, Pages 25-59, July 2009", "doi": "10.1007/s11241-009-9073-x", "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling of constrained deadline sporadic task systems on multiprocessor\nplatforms is an area which has received much attention in the recent past. It\nis widely believed that finding an optimal scheduler is hard, and therefore\nmost studies have focused on developing algorithms with good processor\nutilization bounds. These algorithms can be broadly classified into two\ncategories: partitioned scheduling in which tasks are statically assigned to\nindividual processors, and global scheduling in which each task is allowed to\nexecute on any processor in the platform. In this paper we consider a third,\nmore general, approach called cluster-based scheduling. In this approach each\ntask is statically assigned to a processor cluster, tasks in each cluster are\nglobally scheduled among themselves, and clusters in turn are scheduled on the\nmultiprocessor platform. We develop techniques to support such cluster-based\nscheduling algorithms, and also consider properties that minimize total\nprocessor utilization of individual clusters. In the last part of this paper,\nwe develop new virtual cluster-based scheduling algorithms. For implicit\ndeadline sporadic task systems, we develop an optimal scheduling algorithm that\nis neither Pfair nor ERfair. We also show that the processor utilization bound\nof US-EDF{m/(2m-1)} can be improved by using virtual clustering. Since neither\npartitioned nor global strategies dominate over the other, cluster-based\nscheduling is a natural direction for research towards achieving improved\nprocessor utilization bounds.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:24:40 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Easwaran", "Arvind", ""], ["Shin", "Insik", ""], ["Lee", "Insup", ""]]}, {"id": "2004.02543", "submitter": "Muhammed Siraj", "authors": "Muhammed Siraj, Mohd. Izuan Hafez Hj. Ninggal, Nur Izura Udzir,\n  Muhammad Daniel Hafiz Abdullah, Aziah Asmawi", "title": "SmartCoAuth: Smart-Contract privacy preservation mechanism on querying\n  sensitive records in the cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sensitive records stored in the cloud such as healthcare records, private\nconversation and credit card information are targets of hackers and privacy\nabuse. Current information and record management systems have difficulties\nachieving privacy protection of such sensitive records in a secure,\ntransparent, decentralized and trustless environment. The Blockchain technology\nis a nascent and a promising technology that facilitates data sharing and\naccess in a secure, decentralized and trustless environment. The technology\nenables the use of smart contracts that can be leveraged to complement existing\ntraditional systems to achieve security objectives that were never possible\nbefore. In this paper, we propose a framework based on Blockchain technology to\nenable privacy-preservation in a secured, decentralized, transparent and\ntrustless environment. We name our framework SmartCoAuth. It is based on\nEthereum Smart Contract functions as the secure, decentralized, transparent\nauthentication and authorization mechanism in the framework. It also enables\ntamper-proof auditing of access to the protected records. We analysed how\nSmartCoAuth could be integrated into a cloud application to provide reliable\nprivacy-preservation among stakeholders of healthcare records stored in the\ncloud. The proposed framework provides a satisfactory level of data utility and\nprivacy preservation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:24:48 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Siraj", "Muhammed", ""], ["Ninggal", "Mohd. Izuan Hafez Hj.", ""], ["Udzir", "Nur Izura", ""], ["Abdullah", "Muhammad Daniel Hafiz", ""], ["Asmawi", "Aziah", ""]]}, {"id": "2004.02738", "submitter": "Muhammad Asad", "authors": "Muhammad Asad, Ahmed Moustafa, Takayuki Ito and Muhammad Aslam", "title": "Evaluating the Communication Efficiency in Federated Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of advanced technologies, mobile devices are equipped with\ncomputing and sensing capabilities that gather excessive amounts of data. These\namounts of data are suitable for training different learning models. Cooperated\nwith advancements in Deep Learning (DL), these learning models empower numerous\nuseful applications, e.g., image processing, speech recognition, healthcare,\nvehicular network and many more. Traditionally, Machine Learning (ML)\napproaches require data to be centralised in cloud-based data-centres. However,\nthis data is often large in quantity and privacy-sensitive which prevents\nlogging into these data-centres for training the learning models. In turn, this\nresults in critical issues of high latency and communication inefficiency.\nRecently, in light of new privacy legislations in many countries, the concept\nof Federated Learning (FL) has been introduced. In FL, mobile users are\nempowered to learn a global model by aggregating their local models, without\nsharing the privacy-sensitive data. Usually, these mobile users have slow\nnetwork connections to the data-centre where the global model is maintained.\nMoreover, in a complex and large scale network, heterogeneous devices that have\nvarious energy constraints are involved. This raises the challenge of\ncommunication cost when implementing FL at large scale. To this end, in this\nresearch, we begin with the fundamentals of FL, and then, we highlight the\nrecent FL algorithms and evaluate their communication efficiency with detailed\ncomparisons. Furthermore, we propose a set of solutions to alleviate the\nexisting FL problems both from communication perspective and privacy\nperspective.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 15:31:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Asad", "Muhammad", ""], ["Moustafa", "Ahmed", ""], ["Ito", "Takayuki", ""], ["Aslam", "Muhammad", ""]]}, {"id": "2004.02833", "submitter": "Marco Berghoff", "authors": "Marco Berghoff, Martin Frank, Benjamin Seibold", "title": "Massively Parallel Stencil Strategies for Radiation Transport Moment\n  Model Simulations", "comments": "ICCS 2020 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The radiation transport equation is a mesoscopic equation in high dimensional\nphase space. Moment methods approximate it via a system of partial differential\nequations in traditional space-time. One challenge is the high computational\nintensity due to large vector sizes (1600 components for P39) in each spatial\ngrid point. In this work, we extend the calculable domain size in 3D\nsimulations considerably, by implementing the StaRMAP methodology within the\nmassively parallel HPC framework NAStJA, which is designed to use current\nsupercomputers efficiently. We apply several optimization techniques, including\na new memory layout and explicit SIMD vectorization. We showcase a simulation\nwith 200 billion degrees of freedom, and argue how the implementations can be\nextended and used in many scientific domains.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:25:55 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Berghoff", "Marco", ""], ["Frank", "Martin", ""], ["Seibold", "Benjamin", ""]]}, {"id": "2004.02841", "submitter": "Yuanhao Wei", "authors": "Michal Friedman, Naama Ben-David, Yuanhao Wei, Guy E. Blelloch, Erez\n  Petrank", "title": "NVTraverse: In NVRAM Data Structures, the Destination is More Important\n  than the Journey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent availability of fast, dense, byte-addressable non-volatile memory\nhas led to increasing interest in the problem of designing and specifying\ndurable data structures that can recover from system crashes. However,\ndesigning durable concurrent data structures that are efficient and also\nsatisfy a correctness criterion has proven to be very difficult, leading many\nalgorithms to be inefficient or incorrect in a concurrent setting. In this\npaper, we present a general transformation that takes a lock-free data\nstructure from a general class called traversal data structure (that we\nformally define) and automatically transforms it into an implementation of the\ndata structure for the NVRAM setting that is provably durably linearizable and\nhighly efficient. The transformation hinges on the observation that many data\nstructure operations begin with a traversal phase that does not need to be\npersisted, and thus we only begin persisting when the traversal reaches its\ndestination. We demonstrate the transformation's efficiency through extensive\nmeasurements on a system with Intel's recently released Optane DC persistent\nmemory, showing that it can outperform competitors on many workloads.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:36:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Friedman", "Michal", ""], ["Ben-David", "Naama", ""], ["Wei", "Yuanhao", ""], ["Blelloch", "Guy E.", ""], ["Petrank", "Erez", ""]]}, {"id": "2004.03054", "submitter": "Peng Xu", "authors": "Peng Xu, Jiguang Wan, Ping Huang, Xiaogang Yang, Chenlei Tang, Fei Wu,\n  Changsheng Xie", "title": "LUDA: Boost LSM Key Value Store Compactions with GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-Structured-Merge (LSM) tree-based key value stores are facing critical\nchallenges of fully leveraging the dramatic performance improvements of the\nunderlying storage devices, which makes the compaction operations of LSM key\nvalue stores become CPU-bound, and slow compactions significantly degrade key\nvalue store performance. To address this issue, we propose LUDA, an LSM key\nvalue store with CUDA, which uses a GPU to accelerate compaction operations of\nLSM key value stores. How to efficiently parallelize compaction procedures as\nwell as accommodate the optimal performance contract of the GPU architecture\nchallenge LUDA. Specifically, LUDA overcomes these challenges by exploiting the\ndata independence between compaction procedures and using cooperative sort\nmechanism and judicious data movements. Running on a commodity GPU under\ndifferent levels of CPU overhead, evaluation results show that LUDA provides up\nto 2x higher throughput and 2x data processing speed, and achieves more stable\n99th percentile latencies than LevelDB and RocksDB.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 00:37:01 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Xu", "Peng", ""], ["Wan", "Jiguang", ""], ["Huang", "Ping", ""], ["Yang", "Xiaogang", ""], ["Tang", "Chenlei", ""], ["Wu", "Fei", ""], ["Xie", "Changsheng", ""]]}, {"id": "2004.03072", "submitter": "Shijian Li", "authors": "Shijian Li and Robert J. Walls and Tian Guo", "title": "Characterizing and Modeling Distributed Training with Transient Cloud\n  GPU Servers", "comments": "11 pages, 12 figures, 5 tables, in proceedings of 40th IEEE\n  International Conference on Distributed Computing Systems (ICDCS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud GPU servers have become the de facto way for deep learning\npractitioners to train complex models on large-scale datasets. However, it is\nchallenging to determine the appropriate cluster configuration---e.g., server\ntype and number---for different training workloads while balancing the\ntrade-offs in training time, cost, and model accuracy. Adding to the complexity\nis the potential to reduce the monetary cost by using cheaper, but revocable,\ntransient GPU servers.\n  In this work, we analyze distributed training performance under diverse\ncluster configurations using CM-DARE, a cloud-based measurement and training\nframework. Our empirical datasets include measurements from three GPU types,\nsix geographic regions, twenty convolutional neural networks, and thousands of\nGoogle Cloud servers. We also demonstrate the feasibility of predicting\ntraining speed and overhead using regression-based models. Finally, we discuss\npotential use cases of our performance modeling such as detecting and\nmitigating performance bottlenecks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 01:49:58 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Li", "Shijian", ""], ["Walls", "Robert J.", ""], ["Guo", "Tian", ""]]}, {"id": "2004.03258", "submitter": "Marcos Maro\\~nas", "authors": "M. Maronas, K. Sala, S. Mateo, E. Ayguad\\'e, V. Beltran Barcelona\n  Supercomputing Center", "title": "Worksharing Tasks: An Efficient Way to Exploit Irregular and\n  Fine-Grained Loop Parallelism", "comments": null, "journal-ref": "2019 IEEE 26th International Conference on High Performance\n  Computing, Data, and Analytics (HiPC), Hyderabad, India, 2019, pp. 383-394", "doi": "10.1109/HiPC.2019.00053", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared memory programming models usually provide worksharing and task\nconstructs. The former relies on the efficient fork-join execution model to\nexploit structured parallelism; while the latter relies on fine-grained\nsynchronization among tasks and a flexible data-flow execution model to exploit\ndynamic, irregular, and nested parallelism. On applications that show both\nstructured and unstructured parallelism, both worksharing and task constructs\ncan be combined. However, it is difficult to mix both execution models without\npenalizing the data-flow execution model. Hence, on many applications\nstructured parallelism is also exploited using tasks to leverage the full\nbenefits of a pure data-flow execution model. However, task creation and\nmanagement might introduce a non-negligible overhead that prevents the\nefficient exploitation of fine-grained structured parallelism, especially on\nmany-core processors. In this work, we propose worksharing tasks. These are\ntasks that internally leverage worksharing techniques to exploit fine-grained\nstructured loop-based parallelism. The evaluation shows promising results on\nseveral benchmarks and platforms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 10:52:02 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Maronas", "M.", ""], ["Sala", "K.", ""], ["Mateo", "S.", ""], ["Ayguad\u00e9", "E.", ""], ["Center", "V. Beltran Barcelona Supercomputing", ""]]}, {"id": "2004.03276", "submitter": "Joel Scheuner", "authors": "Joel Scheuner, Philipp Leitner", "title": "Function-as-a-Service Performance Evaluation: A Multivocal Literature\n  Review", "comments": "improvements including postprint updates", "journal-ref": null, "doi": "10.1016/j.jss.2020.110708", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Function-as-a-Service (FaaS) is one form of the serverless cloud computing\nparadigm and is defined through FaaS platforms (e.g., AWS Lambda) executing\nevent-triggered code snippets (i.e., functions). Many studies that empirically\nevaluate the performance of such FaaS platforms have started to appear but we\nare currently lacking a comprehensive understanding of the overall domain. To\naddress this gap, we conducted a multivocal literature review (MLR) covering\n112 studies from academic (51) and grey (61) literature. We find that existing\nwork mainly studies the AWS Lambda platform and focuses on micro-benchmarks\nusing simple functions to measure CPU speed and FaaS platform overhead (i.e.,\ncontainer cold starts). Further, we discover a mismatch between academic and\nindustrial sources on tested platform configurations, find that function\ntriggers remain insufficiently studied, and identify HTTP API gateways and\ncloud storages as the most used external service integrations. Following\nexisting guidelines on experimentation in cloud systems, we discover many flaws\nthreatening the reproducibility of experiments presented in the surveyed\nstudies. We conclude with a discussion of gaps in literature and highlight\nmethodological suggestions that may serve to improve future FaaS performance\nevaluation studies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:21:47 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 15:31:53 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 16:35:24 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Scheuner", "Joel", ""], ["Leitner", "Philipp", ""]]}, {"id": "2004.03352", "submitter": "Salman Shaikh Dr", "authors": "Salman Ahmed Shaikh, Komal Mariam, Hiroyuki Kitagawa, Kyoung-Sook Kim", "title": "GeoFlink: A Distributed and Scalable Framework for the Real-time\n  Processing of Spatial Streams", "comments": "CIKM 2020 Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Flink is an open-source system for scalable processing of batch and\nstreaming data. Flink does not natively support efficient processing of spatial\ndata streams, which is a requirement of many applications dealing with spatial\ndata. Besides Flink, other scalable spatial data processing platforms including\nGeoSpark, Spatial Hadoop, etc. do not support streaming workloads and can only\nhandle static/batch workloads. To fill this gap, we present GeoFlink, which\nextends Apache Flink to support spatial data types, indexes and continuous\nqueries over spatial data streams. To enable the efficient processing of\nspatial continuous queries and for the effective data distribution across Flink\ncluster nodes, a gird-based index is introduced. GeoFlink currently supports\nspatial range, spatial $k$NN and spatial join queries on point data type. An\nextensive experimental study on real spatial data streams shows that GeoFlink\nachieves significantly higher query throughput than ordinary Flink processing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:27:02 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 05:06:13 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 01:25:46 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Shaikh", "Salman Ahmed", ""], ["Mariam", "Komal", ""], ["Kitagawa", "Hiroyuki", ""], ["Kim", "Kyoung-Sook", ""]]}, {"id": "2004.03462", "submitter": "Shahin Nazarian", "authors": "Xiqian Wang, Jiajin Xi, Yinghao Wang, Paul Bogdan, Shahin Nazarian", "title": "Efficient Task Mapping for Manycore Systems", "comments": "This paper is accepted to appear in ISCAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System-on-chip (SoC) has migrated from single core to manycore architectures\nto cope with the increasing complexity of real-life applications. Application\ntask mapping has a significant impact on the efficiency of manycore system\n(MCS) computation and communication. We present WAANSO, a scalable framework\nthat incorporates a Wavelet Clustering based approach to cluster application\ntasks. We also introduce Ant Swarm Optimization (ASO) based on iterative\nexecution of Ant Colony Optimization (ACO) and Particle Swarm Optimization\n(PSO) for task clustering and mapping to the MCS processing elements. We have\nshown that WAANSO can significantly increase the MCS energy and performance\nefficiencies. Based on our experiments on a 64-core system, WAANSO improves\nenergy efficiency by 19%, compared to baseline approaches, namely DPSO, ACO and\nbranch and bound (B&B). Additionally, the performance improves by 65.86%\ncompared to Density-Based Spatial Clustering of Applications with Noise\n(DBSCAN) baseline.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 06:50:50 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Wang", "Xiqian", ""], ["Xi", "Jiajin", ""], ["Wang", "Yinghao", ""], ["Bogdan", "Paul", ""], ["Nazarian", "Shahin", ""]]}, {"id": "2004.03717", "submitter": "Anup Das", "authors": "Shihao Song, Adarsha Balaji, Anup Das, Nagarajan Kandasamy, and James\n  Shackleford", "title": "Compiling Spiking Neural Networks to Neuromorphic Hardware", "comments": "10 pages, 17 figures, accepted at 21st ACM SIGPLAN/SIGBED\n  International Conference on Languages, Compilers, and Tools for Embedded\n  Systems (LCTES 2020)", "journal-ref": null, "doi": "10.1145/3372799.3394364", "report-no": null, "categories": "cs.DC cs.AR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning applications that are implemented with spike-based\ncomputation model, e.g., Spiking Neural Network (SNN), have a great potential\nto lower the energy consumption when they are executed on a neuromorphic\nhardware. However, compiling and mapping an SNN to the hardware is challenging,\nespecially when compute and storage resources of the hardware (viz. crossbar)\nneed to be shared among the neurons and synapses of the SNN. We propose an\napproach to analyze and compile SNNs on a resource-constrained neuromorphic\nhardware, providing guarantee on key performance metrics such as execution time\nand throughput. Our approach makes the following three key contributions.\nFirst, we propose a greedy technique to partition an SNN into clusters of\nneurons and synapses such that each cluster can fit on to the resources of a\ncrossbar. Second, we exploit the rich semantics and expressiveness of\nSynchronous Dataflow Graphs (SDFGs) to represent a clustered SNN and analyze\nits performance using Max-Plus Algebra, considering the available compute and\nstorage capacities, buffer sizes, and communication bandwidth. Third, we\npropose a self-timed execution-based fast technique to compile and admit\nSNN-based applications to a neuromorphic hardware at run-time, adapting\ndynamically to the available resources on the hardware. We evaluate our\napproach with standard SNN-based applications and demonstrate a significant\nperformance improvement compared to current practices.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 21:13:27 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 14:02:31 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Song", "Shihao", ""], ["Balaji", "Adarsha", ""], ["Das", "Anup", ""], ["Kandasamy", "Nagarajan", ""], ["Shackleford", "James", ""]]}, {"id": "2004.03749", "submitter": "Pengzhan Guo", "authors": "Pengzhan Guo, Zeyang Ye, Keli Xiao, Wei Zhu", "title": "Weighted Aggregating Stochastic Gradient Descent for Parallel Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the stochastic optimization problem with a focus on\ndeveloping scalable parallel algorithms for deep learning tasks. Our solution\ninvolves a reformation of the objective function for stochastic optimization in\nneural network models, along with a novel parallel strategy, coined weighted\naggregating stochastic gradient descent (WASGD). Following a theoretical\nanalysis on the characteristics of the new objective function, WASGD introduces\na decentralized weighted aggregating scheme based on the performance of local\nworkers. Without any center variable, the new method automatically assesses the\nimportance of local workers and accepts them according to their contributions.\nFurthermore, we have developed an enhanced version of the method, WASGD+, by\n(1) considering a designed sample order and (2) applying a more advanced weight\nevaluating function. To validate the new method, we benchmark our schemes\nagainst several popular algorithms including the state-of-the-art techniques\n(e.g., elastic averaging SGD) in training deep neural networks for\nclassification tasks. Comprehensive experiments have been conducted on four\nclassic datasets, including the CIFAR-100, CIFAR-10, Fashion-MNIST, and MNIST.\nThe subsequent results suggest the superiority of the WASGD scheme in\naccelerating the training of deep architecture. Better still, the enhanced\nversion, WASGD+, has been shown to be a significant improvement over its basic\nversion.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 23:38:29 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Guo", "Pengzhan", ""], ["Ye", "Zeyang", ""], ["Xiao", "Keli", ""], ["Zhu", "Wei", ""]]}, {"id": "2004.03841", "submitter": "Shantanu Sharma", "authors": "Nisha Panwar, Shantanu Sharma, Guoxi Wang, Sharad Mehrotra, and Nalini\n  Venkatasubramanian", "title": "Canopy: A Verifiable Privacy-Preserving Token Ring based Communication\n  Protocol for Smart Homes", "comments": "This version has been accepted in ACM Transactions on Cyber-Physical\n  Systems (TCPS). A preliminary version of this paper was accepted in ACM\n  Conference on Data and Application Security and Privacy (CODASPY) 2019. arXiv\n  admin note: substantial text overlap with arXiv:1901.08618", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the new privacy challenges that arise in smart homes.\nSpecifically, the paper focuses on inferring the user's activities -- which\nmay, in turn, lead to the user's privacy -- via inferences through device\nactivities and network traffic analysis. We develop techniques that are based\non a cryptographically secure token circulation in a ring network consisting of\nsmart home devices to prevent inferences from device activities, via device\nworkflow, i.e., inferences from a coordinated sequence of devices' actuation.\nThe solution hides the device activity and corresponding channel activities,\nand thus, preserve the individual's activities. We also extend our solution to\ndeal with a large number of devices and devices that produce large-sized data\nby implementing parallel rings. Our experiments also evaluate the performance\nin terms of communication overheads of the proposed approach and the obtained\nprivacy.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 06:57:01 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Wang", "Guoxi", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""]]}, {"id": "2004.03998", "submitter": "Liangliang Xu", "authors": "Liangliang Xu, Min Lyu, Zhipeng Li, Yongkun Li, Yinlong Xu", "title": "Deterministic Data Distribution for Efficient Recovery in Erasure-Coded\n  Storage Systems", "comments": "Technical Report. Journal version accepted to IEEE Transactions on\n  Parallel and Distributed Systems(TPDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to individual unreliable commodity components, failures are common in\nlarge-scale distributed storage systems. Erasure codes are widely deployed in\npractical storage systems to provide fault tolerance with low storage overhead.\nHowever, random data distribution (RDD), commonly used in erasure-coded storage\nsystems, induces heavy cross-rack traffic, load imbalance, and random access,\nwhich adversely affects failure recovery. In this paper, with orthogonal\narrays, we define a Deterministic Data Distribution ($D^3$) to uniformly\ndistribute data/parity blocks among nodes, and propose an efficient failure\nrecovery approach based on $D^3$, which minimizes the cross-rack repair traffic\nagainst a single node failure. Thanks to the uniformity of $D^3$, the proposed\nrecovery approach balances the repair traffic not only among nodes within a\nrack but also among racks. We implement $D^3$ over Reed-Solomon codes and\nLocally Repairable Codes in Hadoop Distributed File System (HDFS) with a\ncluster of 28 machines. Compared with RDD, our experiments show that $D^3$\nsignificantly speeds up the failure recovery up to 2.49 times for RS codes and\n1.38 times for LRCs. Moreover, $D^3$ supports front-end applications better\nthan RDD in both of normal and recovery states.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 14:05:00 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 01:37:50 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Xu", "Liangliang", ""], ["Lyu", "Min", ""], ["Li", "Zhipeng", ""], ["Li", "Yongkun", ""], ["Xu", "Yinlong", ""]]}, {"id": "2004.04104", "submitter": "Hieu Nguyen", "authors": "Nguyen Quang Hieu, Tran The Anh, Nguyen Cong Luong, Dusit Niyato, Dong\n  In Kim, Erik Elmroth", "title": "Resource Management for Blockchain-enabled Federated Learning: A Deep\n  Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain-enabled Federated Learning (BFL) enables mobile devices to\ncollaboratively train neural network models required by a Machine Learning\nModel Owner (MLMO) while keeping data on the mobile devices. Then, the model\nupdates are stored in the blockchain in a decentralized and reliable manner.\nHowever, the issue of BFL is that the mobile devices have energy and CPU\nconstraints that may reduce the system lifetime and training efficiency. The\nother issue is that the training latency may increase due to the blockchain\nmining process. To address these issues, the MLMO needs to (i) decide how much\ndata and energy that the mobile devices use for the training and (ii) determine\nthe block generation rate to minimize the system latency, energy consumption,\nand incentive cost while achieving the target accuracy for the model. Under the\nuncertainty of the BFL environment, it is challenging for the MLMO to determine\nthe optimal decisions. We propose to use the Deep Reinforcement Learning (DRL)\nto derive the optimal decisions for the MLMO.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:29:19 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 05:51:28 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Hieu", "Nguyen Quang", ""], ["Anh", "Tran The", ""], ["Luong", "Nguyen Cong", ""], ["Niyato", "Dusit", ""], ["Kim", "Dong In", ""], ["Elmroth", "Erik", ""]]}, {"id": "2004.04289", "submitter": "Benjamin Priest", "authors": "Benjamin W. Priest", "title": "DegreeSketch: Distributed Cardinality Sketches on Massive Graphs with\n  Applications", "comments": "22 pages, 8 figures, submitted to VLDB 2020, comments welcome", "journal-ref": null, "doi": null, "report-no": "LLNL-CONF-806542", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DegreeSketch, a semi-streaming distributed sketch data structure\nand demonstrate its utility for estimating local neighborhood sizes and local\ntriangle count heavy hitters on massive graphs. DegreeSketch consists of\nvertex-centric cardinality sketches distributed across a set of processors that\nare accumulated in a single pass, and then behaves as a persistent query engine\ncapable of approximately answering graph queries pertaining to the sizes of\nadjacency set unions and intersections. The $t$th local neighborhood of a\nvertex is the number of vertices reachable in $G$ from $v$ by traversing at\nmost $t$ edges, whereas the local triangle count is the number of 3-cycles in\nwhich it is included. Both metrics are useful in graph analysis applications,\nbut exact computations scale poorly as graph sizes grow. We present efficient\nalgorithms for estimating both local neighborhood sizes and local triangle\ncount heavy hitters using DegreeSketch. In our experiments we implement\nDegreeSketch using the celebrated hyperloglog cardinality sketch and utilize\nthe distributed communication tool YGM to achieve state-of-the-art performance\nin distributed memory.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 22:45:28 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Priest", "Benjamin W.", ""]]}, {"id": "2004.04294", "submitter": "Yujin Kwon", "authors": "Geunwoo Lim, Yujin Kwon, Yongdae Kim", "title": "Analysis of LFT2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a decentralized and transparent society, blockchain technology has been\ndeveloped. Along with this, quite a few consensus algorithms that are one of\ncore technologies in blockchain have been proposed. Among them, we analyze a\nconsensus algorithm called LFT2, which is used by a blockchain system, ICON. We\nfirst formulate the LFT2 consensus algorithm and then analyze safety and\nliveness, which can be considered as the most important properties in\ndistributed consensus system. We prove that LFT2 satisfies safety and liveness,\nwhere a certain assumption is required to prove liveness. In addition, we\ncompare LFT2 with two similar consensus algorithms, and from the comparison, we\nshow that a trade-off exist among the three consensus algorithms. Finally, we\nsimulate LFT2 to measure a liveness quality.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 23:15:49 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Lim", "Geunwoo", ""], ["Kwon", "Yujin", ""], ["Kim", "Yongdae", ""]]}, {"id": "2004.04302", "submitter": "Prashant Shenoy", "authors": "Pradeep Ambati, Noman Bashir, David Irwin, Mohammad Hajiesmaili, and\n  Prashant Shenoy", "title": "Hedge Your Bets: Optimizing Long-term Cloud Costs by Mixing VM\n  Purchasing Options", "comments": "11 pages, 10 figures. This paper will appear in the Proceedings of\n  the IEEE International Conference on Cloud Engineering, April 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud platforms offer the same VMs under many purchasing options that specify\ndifferent costs and time commitments, such as on-demand, reserved,\nsustained-use, scheduled reserve, transient, and spot block. In general, the\nstronger the commitment, i.e., longer and less flexible, the lower the price.\nHowever, longer and less flexible time commitments can increase cloud costs for\nusers if future workloads cannot utilize the VMs they committed to buying.\nLarge cloud customers often find it challenging to choose the right mix of\npurchasing options to reduce their long-term costs, while retaining the ability\nto adjust capacity up and down in response to workload variations.\n  To address the problem, we design policies to optimize long-term cloud costs\nby selecting a mix of VM purchasing options based on short- and long-term\nexpectations of workload utilization. We consider a batch trace spanning 4\nyears from a large shared cluster for a major state University system that\nincludes 14k cores and 60 million job submissions, and evaluate how these jobs\ncould be judiciously executed using cloud servers using our approach. Our\nresults show that our policies incur a cost within 41% of an optimistic optimal\noffline approach, and 50% less than solely using on-demand VMs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 23:51:54 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Ambati", "Pradeep", ""], ["Bashir", "Noman", ""], ["Irwin", "David", ""], ["Hajiesmaili", "Mohammad", ""], ["Shenoy", "Prashant", ""]]}, {"id": "2004.04303", "submitter": "Matthew Weidner", "authors": "Matthew Weidner, Heather Miller, Christopher Meiklejohn", "title": "Composing and Decomposing Op-Based CRDTs with Semidirect Products", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operation-based Conflict-free Replicated Data Types (CRDTs) are eventually\nconsistent replicated data types that automatically resolve conflicts between\nconcurrent operations. Op-based CRDTs must be designed differently for each\ndata type, and current designs use ad-hoc techniques to handle concurrent\noperations that do not naturally commute. We present a new construction, the\nsemidirect product of op-based CRDTs, which combines the operations of two\nCRDTs into one while handling conflicts between their concurrent operations in\na uniform way. We demonstrate the construction's utility by using it to\nconstruct novel CRDTs, as well as decomposing several existing CRDTs as\nsemidirect products of simpler CRDTs. Although it reproduces common CRDT\nsemantics, the semidirect product can be viewed as a restricted kind of\noperational transformation, thus forming a bridge between these two opposing\ntechniques for constructing replicated data types.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 23:57:54 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Weidner", "Matthew", ""], ["Miller", "Heather", ""], ["Meiklejohn", "Christopher", ""]]}, {"id": "2004.04314", "submitter": "Jie Xu", "authors": "Jie Xu, Heqiang Wang", "title": "Client Selection and Bandwidth Allocation in Wireless Federated Learning\n  Networks: A Long-Term Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies federated learning (FL) in a classic wireless network,\nwhere learning clients share a common wireless link to a coordinating server to\nperform federated model training using their local data. In such wireless\nfederated learning networks (WFLNs), optimizing the learning performance\ndepends crucially on how clients are selected and how bandwidth is allocated\namong the selected clients in every learning round, as both radio and client\nenergy resources are limited. While existing works have made some attempts to\nallocate the limited wireless resources to optimize FL, they focus on the\nproblem in individual learning rounds, overlooking an inherent yet critical\nfeature of federated learning. This paper brings a new long-term perspective to\nresource allocation in WFLNs, realizing that learning rounds are not only\ntemporally interdependent but also have varying significance towards the final\nlearning outcome. To this end, we first design data-driven experiments to show\nthat different temporal client selection patterns lead to considerably\ndifferent learning performance. With the obtained insights, we formulate a\nstochastic optimization problem for joint client selection and bandwidth\nallocation under long-term client energy constraints, and develop a new\nalgorithm that utilizes only currently available wireless channel information\nbut can achieve long-term performance guarantee. Further experiments show that\nour algorithm results in the desired temporal client selection pattern, is\nadaptive to changing network environments and far outperforms benchmarks that\nignore the long-term effect of FL.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 01:06:41 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Xu", "Jie", ""], ["Wang", "Heqiang", ""]]}, {"id": "2004.04338", "submitter": "Quan Nguyen Hoang", "authors": "Quan Nguyen, Andre Cronje, Michael Kong", "title": "OV: Validity-based Optimistic Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contract (SC) platforms form blocks of transactions into a chain and\nexecute them via user-defined smart contracts. In conventional platforms like\nBitcoin and Ethereum, the transactions within a block are executed\n\\emph{sequentially} by the miner and are then validated \\emph{sequentially} by\nthe validators to reach consensus about the final state of the block.\n  In order to leverage the advances of multicores, this paper explores the next\ngeneration of smart contract platforms that enables concurrent execution of\nsuch contracts. Reasoning about the validity of the object states is\nchallenging in concurrent smart contracts. We examine a programming model to\nsupport \\emph{optimistic} execution of SCTs. We introduce a novel programming\nlanguage, so-called OV, and a Solidity API to ease programing of optimistic\nsmart contracts. OV language together with static checking will help reasoning\nabout a crucial property of optimistically executed smart contracts -- the\nvalidity of object states in trustless systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 02:37:25 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Nguyen", "Quan", ""], ["Cronje", "Andre", ""], ["Kong", "Michael", ""]]}, {"id": "2004.04359", "submitter": "Arnab Das", "authors": "Arnab Das, Sriram Krishnamoorthy, Ian Briggs, Ganesh Gopalakrishnan,\n  Ramakrishna Tipireddy", "title": "FPDetect: Efficient Reasoning About Stencil Programs Using Selective\n  Direct Evaluation", "comments": "Accepted in Journal of ACM Transactions on Architecture and Code\n  Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA cs.PF math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present FPDetect, a low overhead approach for detecting logical errors and\nsoft errors affecting stencil computations without generating false positives.\nWe develop an offline analysis that tightly estimates the number of\nfloating-point bits preserved across stencil applications. This estimate\nrigorously bounds the values expected in the data space of the computation.\nViolations of this bound can be attributed with certainty to errors. FPDetect\nhelps synthesize error detectors customized for user-specified levels of\naccuracy and coverage. FPDetect also enables overhead reduction techniques\nbased on deploying these detectors coarsely in space and time. Experimental\nevaluations demonstrate the practicality of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 04:09:16 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 20:18:45 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 02:36:13 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 21:51:00 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Das", "Arnab", ""], ["Krishnamoorthy", "Sriram", ""], ["Briggs", "Ian", ""], ["Gopalakrishnan", "Ganesh", ""], ["Tipireddy", "Ramakrishna", ""]]}, {"id": "2004.04366", "submitter": "Xu Chen", "authors": "Haowei Chen, Liekang Zeng, Shuai Yu, and Xu Chen", "title": "Knowledge Distillation for Mobile Edge Computation Offloading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computation offloading allows mobile end devices to put execution of\ncompute-intensive task on the edge servers. End devices can decide whether\noffload the tasks to edge servers, cloud servers or execute locally according\nto current network condition and devices' profile in an online manner. In this\narticle, we propose an edge computation offloading framework based on Deep\nImitation Learning (DIL) and Knowledge Distillation (KD), which assists end\ndevices to quickly make fine-grained decisions to optimize the delay of\ncomputation tasks online. We formalize computation offloading problem into a\nmulti-label classification problem. Training samples for our DIL model are\ngenerated in an offline manner. After model is trained, we leverage knowledge\ndistillation to obtain a lightweight DIL model, by which we further reduce the\nmodel's inference delay. Numerical experiment shows that the offloading\ndecisions made by our model outperforms those made by other related policies in\nlatency metric. Also, our model has the shortest inference delay among all\npolicies.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 04:58:46 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Chen", "Haowei", ""], ["Zeng", "Liekang", ""], ["Yu", "Shuai", ""], ["Chen", "Xu", ""]]}, {"id": "2004.04557", "submitter": "Xiaohan Zhang", "authors": "Xiaohan Zhang, Yong Xiao, Qiang Li and Walid Saad", "title": "Deep Reinforcement Learning for Fog Computing-based Vehicular System\n  with Multi-operator Support", "comments": "6 pages, 9 figures. Accepted at IEEE International Conference on\n  Communications (ICC), Dublin, Ireland, June 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the potential performance improvement that can be achieved\nby enabling multi-operator wireless connectivity for cloud/fog\ncomputing-connected vehicular systems. Mobile network operator (MNO) selection\nand switching problem is formulated by jointly considering switching cost,\nquality-of-service (QoS) variations between MNOs, and the different prices that\ncan be charged by different MNOs as well as cloud and fog servers. A double\ndeep Q network (DQN) based switching policy is proposed and proved to be able\nto minimize the long-term average cost of each vehicle with guaranteed latency\nand reliability performance. The performance of the proposed approach is\nevaluated using the dataset collected in a commercially available city-wide LTE\nnetwork. Simulation results show that our proposed policy can significantly\nreduce the cost paid by each fog/cloud-connected vehicle with guaranteed\nlatency services.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 09:15:08 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Zhang", "Xiaohan", ""], ["Xiao", "Yong", ""], ["Li", "Qiang", ""], ["Saad", "Walid", ""]]}, {"id": "2004.04613", "submitter": "Nouraldin Jaber", "authors": "Nouraldin Jaber (1), Christopher Wagner (1), Swen Jacobs (2), Milind\n  Kulkarni (1), Roopsha Samanta (1) ((1) Purdue University, (2) CISPA Helmholtz\n  Center for Information Security)", "title": "QuickSilver: A Modeling and Parameterized Verification Framework for\n  Systems with Distributed Agreement", "comments": "41 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-automated parameterized verification of distributed systems, i.e.,\nverification of systems instantiated with an arbitrary number of processes,\nsuffers from scalability challenges, even when it is decidable. This paper\nseeks to push the boundaries of parameterized verification in the types of\nsystems that can be verified automatically as well as practically, by\nincorporating abstractions into the verification pipeline. We develop a\nframework---QuickSilver---for modeling and automated parameterized reasoning\nabout systems that build on distributed agreement protocols, such as consensus\nor leader election. QuickSilver includes a modeling language, Mercury, with\nprimitives for abstracting distributed agreement, syntactic conditions for\ndecidable and practical parameterized verification of systems modeled in\nMercury, and an implementation that has been demonstrably used for efficient,\nautomated parameterized verification of several benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 15:48:28 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 05:42:23 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Jaber", "Nouraldin", ""], ["Wagner", "Christopher", ""], ["Jacobs", "Swen", ""], ["Kulkarni", "Milind", ""], ["Samanta", "Roopsha", ""]]}, {"id": "2004.04628", "submitter": "Roman Iakymchuk", "authors": "Roman Iakymchuk, Daichi Mukunoki, Artur Podobas, Fabienne\n  J\\'ez\\'equel, Toshiyuki Imamura, Norihisa Fujita, Jens Huthmann, Shuhei Kudo,\n  Yiyu Tan, Jens Domke, Kai Torben Ohlhus, Takeshi Fukaya, Takeo Hoshi, Yuki\n  Murakami, Maho Nakata, Takeshi Ogita, Kentaro Sano, Taisuke Boku", "title": "White Paper from Workshop on Large-scale Parallel Numerical Computing\n  Technology (LSPANC 2020): HPC and Computer Arithmetic toward\n  Minimal-Precision Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": "hal-02536316", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In numerical computations, precision of floating-point computations is a key\nfactor to determine the performance (speed and energy-efficiency) as well as\nthe reliability (accuracy and reproducibility). However, precision generally\nplays a contrary role for both. Therefore, the ultimate concept for maximizing\nboth at the same time is the minimal-precision computing through\nprecision-tuning, which adjusts the optimal precision for each operation and\ndata. Several studies have been already conducted for it so far (e.g.\nPrecimoniuos and Verrou), but the scope of those studies is limited to the\nprecision-tuning alone. Hence, we aim to propose a broader concept of the\nminimal-precision computing system with precision-tuning, involving both\nhardware and software stack.\n  In 2019, we have started the Minimal-Precision Computing project to propose a\nmore broad concept of the minimal-precision computing system with\nprecision-tuning, involving both hardware and software stack. Specifically, our\nsystem combines (1) a precision-tuning method based on Discrete Stochastic\nArithmetic (DSA), (2) arbitrary-precision arithmetic libraries, (3) fast and\naccurate numerical libraries, and (4) Field-Programmable Gate Array (FPGA) with\nHigh-Level Synthesis (HLS).\n  In this white paper, we aim to provide an overview of various technologies\nrelated to minimal- and mixed-precision, to outline the future direction of the\nproject, as well as to discuss current challenges together with our project\nmembers and guest speakers at the LSPANC 2020 workshop;\nhttps://www.r-ccs.riken.jp/labs/lpnctrt/lspanc2020jan/.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 16:16:59 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 08:26:57 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Iakymchuk", "Roman", ""], ["Mukunoki", "Daichi", ""], ["Podobas", "Artur", ""], ["J\u00e9z\u00e9quel", "Fabienne", ""], ["Imamura", "Toshiyuki", ""], ["Fujita", "Norihisa", ""], ["Huthmann", "Jens", ""], ["Kudo", "Shuhei", ""], ["Tan", "Yiyu", ""], ["Domke", "Jens", ""], ["Ohlhus", "Kai Torben", ""], ["Fukaya", "Takeshi", ""], ["Hoshi", "Takeo", ""], ["Murakami", "Yuki", ""], ["Nakata", "Maho", ""], ["Ogita", "Takeshi", ""], ["Sano", "Kentaro", ""], ["Boku", "Taisuke", ""]]}, {"id": "2004.04633", "submitter": "Jamal Toutouh", "authors": "Emiliano Perez, Sergio Nesmachnow, Jamal Toutouh, Erik Hemberg,\n  Una-May O'Reilly", "title": "Parallel/distributed implementation of cellular training for generative\n  adversarial neural networks", "comments": "This article has been accepted for publication in IEEE International\n  Parallel and Distributed Processing Symposium, Parallel and Distributed\n  Combinatorics and Optimization, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are widely used to learn generative\nmodels. GANs consist of two networks, a generator and a discriminator, that\napply adversarial learning to optimize their parameters. This article presents\na parallel/distributed implementation of a cellular competitive coevolutionary\nmethod to train two populations of GANs. A distributed memory parallel\nimplementation is proposed for execution in high performance/supercomputing\ncenters. Efficient results are reported on addressing the generation of\nhandwritten digits (MNIST dataset samples). Moreover, the proposed\nimplementation is able to reduce the training times and scale properly when\nconsidering different grid sizes for training.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 16:01:58 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 15:12:43 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 17:55:24 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Perez", "Emiliano", ""], ["Nesmachnow", "Sergio", ""], ["Toutouh", "Jamal", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "2004.04640", "submitter": "Maohong Chen", "authors": "Maohong Chen, Yong Xiao, Qiang Li and Kwang-cheng Chen", "title": "Minimizing Age-of-Information for Fog Computing-supported Vehicular\n  Networks with Deep Q-learning", "comments": "6 pages,9 figures. Accepted at IEEE International Conference on\n  Communications (ICC), Dublin, Ireland, June 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected vehicular network is one of the key enablers for next generation\ncloud/fog-supported autonomous driving vehicles. Most connected vehicular\napplications require frequent status updates and Age of Information (AoI) is a\nmore relevant metric to evaluate the performance of wireless links between\nvehicles and cloud/fog servers. This paper introduces a novel proactive and\ndata-driven approach to optimize the driving route with a main objective of\nguaranteeing the confidence of AoI. In particular, we report a study on three\nmonth measurements of a multi-vehicle campus shuttle system connected to\ncloud/fog servers via a commercial LTE network. We establish empirical models\nfor AoI in connected vehicles and investigate the impact of major factors on\nthe performance of AoI. We also propose a Deep Q-Learning Netwrok (DQN)-based\nalgorithm to decide the optimal driving route for each connected vehicle with\nmaximized confidence level. Numerical results show that the proposed approach\ncan lead to a significant improvement on the AoI confidence for various types\nof services supported.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 05:19:25 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Chen", "Maohong", ""], ["Xiao", "Yong", ""], ["Li", "Qiang", ""], ["Chen", "Kwang-cheng", ""]]}, {"id": "2004.04643", "submitter": "Muhammad Huzaifa", "authors": "Muhammad Huzaifa, Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing,\n  Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Finn Sinclair, Boyuan\n  Tian, Hengzhi Yuan, Jeffrey Zhang, Sarita V. Adve", "title": "Exploring Extended Reality with ILLIXR: A New Playground for\n  Architecture Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we enter the era of domain-specific architectures, systems researchers\nmust understand the requirements of emerging application domains. Augmented and\nvirtual reality (AR/VR) or extended reality (XR) is one such important domain.\nThis paper presents ILLIXR, the first open source end-to-end XR system (1) with\nstate-of-the-art components, (2) integrated with a modular and extensible\nmultithreaded runtime, (3) providing an OpenXR compliant interface to XR\napplications (e.g., game engines), and (4) with the ability to report (and\ntrade off) several quality of experience (QoE) metrics. We analyze performance,\npower, and QoE metrics for the complete ILLIXR system and for its individual\ncomponents. Our analysis reveals several properties with implications for\narchitecture and systems research. These include demanding performance, power,\nand QoE requirements, a large diversity of critical tasks, inter-dependent\nexecution pipelines with challenges in scheduling and resource management, and\na large tradeoff space between performance/power and human perception related\nQoE metrics. ILLIXR and our analysis have the potential to propel new\ndirections in architecture and systems research in general, and impact XR in\nparticular. ILLIXR is open-source and available at https://illixr.github.io\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 01:17:29 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 01:23:21 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Huzaifa", "Muhammad", ""], ["Desai", "Rishi", ""], ["Grayson", "Samuel", ""], ["Jiang", "Xutao", ""], ["Jing", "Ying", ""], ["Lee", "Jae", ""], ["Lu", "Fang", ""], ["Pang", "Yihan", ""], ["Ravichandran", "Joseph", ""], ["Sinclair", "Finn", ""], ["Tian", "Boyuan", ""], ["Yuan", "Hengzhi", ""], ["Zhang", "Jeffrey", ""], ["Adve", "Sarita V.", ""]]}, {"id": "2004.04680", "submitter": "Shripad Gade", "authors": "Shripad Gade, Ji Liu, Nitin H. Vaidya", "title": "A Private and Finite-Time Algorithm for Solving a Distributed System of\n  Linear Equations", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CR cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a system of linear equations, denoted as $Ax = b$, which\nis horizontally partitioned (rows in $A$ and $b$) and stored over a network of\n$m$ devices connected in a fixed directed graph. We design a fast distributed\nalgorithm for solving such a partitioned system of linear equations, that\nadditionally, protects the privacy of local data against an honest-but-curious\nadversary that corrupts at most $\\tau$ nodes in the network. First, we present\nTITAN, privaTe fInite Time Average coNsensus algorithm, for solving a general\naverage consensus problem over directed graphs, while protecting statistical\nprivacy of private local data against an honest-but-curious adversary. Second,\nwe propose a distributed linear system solver that involves each agent/devices\ncomputing an update based on local private data, followed by private\naggregation using TITAN. Finally, we show convergence of our solver to the\nleast squares solution in finite rounds along with statistical privacy of local\nlinear equations against an honest-but-curious adversary provided the graph has\nweak vertex-connectivity of at least $\\tau+1$. We perform numerical experiments\nto validate our claims and compare our solution to the state-of-the-art methods\nby comparing computation, communication and memory costs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:04:03 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Gade", "Shripad", ""], ["Liu", "Ji", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2004.04798", "submitter": "Yibin Xu", "authors": "Yibin Xu, Jianhua Shao, Yangyu Huang, Tijs Slaats and Boris D\\\"udder", "title": "A $p/2$ Adversary Power Resistant Blockchain Sharding Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain Sharding is a blockchain performance enhancement approach. By\nsplitting a blockchain into several parallel-run committees (shards), it helps\nincrease transaction throughput, reduce computational resources required, and\nincrease reward expectation for participants. Recently, several flexible\nsharding methods that can tolerate up to $n/2$ Byzantine nodes ($n/2$ security\nlevel) have been proposed. However, these methods suffer from three main\ndrawbacks. First, in a non-sharding blockchain, nodes can have different weight\n(power or stake) to create a consensus, and as such an adversary needs to\ncontrol half of the overall weight in order to manipulate the system ($p/2$\nsecurity level). In blockchain sharding, all nodes carry the same weight. Thus,\nit is only under the assumption that honest participants create as many nodes\nas they should that a $n/2$ security level blockchain sharding reaches the\n$p/2$ security level. Second, when some nodes leave the system, other nodes\nneed to be reassigned, frequently, from shard to shard in order to maintain the\nsecurity level. This has an adverse effect on system performance. Third, while\nsome $n/2$ approaches can maintain data integrity with up to $n/2$ Byzantine\nnodes, their systems can halt with a smaller number of Byzantine nodes. In this\npaper, we present a $p/2$ security level blockchain sharding approach that does\nnot require honest participants to create multiple nodes, requires less node\nreassignment when some nodes leave the system, and can prevent the system from\nhalting. Our experiments show that our new approach outperforms existing\nblockchain sharding approaches in terms of security, transaction throughput and\nflexibility.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 20:31:03 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 06:42:05 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 22:59:44 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 19:30:43 GMT"}, {"version": "v5", "created": "Sun, 9 Aug 2020 13:00:13 GMT"}, {"version": "v6", "created": "Tue, 22 Jun 2021 09:30:37 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Xu", "Yibin", ""], ["Shao", "Jianhua", ""], ["Huang", "Yangyu", ""], ["Slaats", "Tijs", ""], ["D\u00fcdder", "Boris", ""]]}, {"id": "2004.04896", "submitter": "Nouraldin Jaber", "authors": "Nouraldin Jaber (1), Swen Jacobs (2), Christopher Wagner (1), Milind\n  Kulkarni (1), Roopsha Samanta (1) ((1) Purdue University, (2) CISPA Helmholtz\n  Center for Information Security)", "title": "Parameterized Verification of Systems with Global Synchronization and\n  Guards", "comments": "Conference version published at CAV 2020; this version contains a\n  correction of guard-compatibility conditions C2.1 and C2.2", "journal-ref": "Lecture Notes in Computer Science, vol 12224. Springer (2020)", "doi": "10.1007/978-3-030-53288-8_15", "report-no": null, "categories": "cs.FL cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by distributed applications that use consensus or other agreement\nprotocols for global coordination, we define a new computational model for\nparameterized systems that is based on a general global synchronization\nprimitive and allows for global transition guards. Our model generalizes many\nexisting models in the literature, including broadcast protocols and guarded\nprotocols. We show that reachability properties are decidable for systems\nwithout guards, and give sufficient conditions under which they remain\ndecidable in the presence of guards. Furthermore, we investigate cutoffs for\nreachability properties and provide sufficient conditions for small cutoffs in\na number of cases that are inspired by our target applications.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 03:59:22 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:23:55 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 02:47:55 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Jaber", "Nouraldin", ""], ["Jacobs", "Swen", ""], ["Wagner", "Christopher", ""], ["Kulkarni", "Milind", ""], ["Samanta", "Roopsha", ""]]}, {"id": "2004.04948", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura, Sennur Ulukus, Deniz Gunduz", "title": "Straggler-aware Distributed Learning: Communication Computation Latency\n  Trade-off", "comments": "This paper was presented in part at the 2019 IEEE International\n  Symposium on Information Theory (ISIT) in Paris, France, and at the 2019 IEEE\n  Data Science Workshop in Minneapolis, USA", "journal-ref": null, "doi": "10.3390/e22050544", "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When gradient descent (GD) is scaled to many parallel workers for large scale\nmachine learning problems, its per-iteration computation time is limited by the\nstraggling workers. Straggling workers can be tolerated by assigning redundant\ncomputations and coding across data and computations, but in most existing\nschemes, each non-straggling worker transmits one message per iteration to the\nparameter server (PS) after completing all its computations. Imposing such a\nlimitation results in two main drawbacks; over-computation due to inaccurate\nprediction of the straggling behaviour, and under-utilization due to treating\nworkers as straggler/non-straggler and discarding partial computations carried\nout by stragglers. In this paper, to overcome these drawbacks, we consider\nmulti-message communication (MMC) by allowing multiple computations to be\nconveyed from each worker per iteration, and design straggler avoidance\ntechniques accordingly. Then, we analyze how the proposed designs can be\nemployed efficiently to seek a balance between the computation and\ncommunication latency to minimize the overall latency. Furthermore, through\nextensive simulations, both model-based and real implementation on Amazon EC2\nservers, we identify the advantages and disadvantages of these designs in\ndifferent settings, and demonstrate that MMC can help improve upon existing\nstraggler avoidance schemes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 08:39:36 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Ozfatura", "Emre", ""], ["Ulukus", "Sennur", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2004.05046", "submitter": "Martijn De Vos", "authors": "Martijn de Vos, Can Umut Ileri, Johan Pouwelse", "title": "XChange: A Blockchain-based Mechanism for Generic Asset Trading In\n  Resource-constrained Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of industries rely on Internet-of-Things devices to\ntrack physical resources. Blockchain technology provides primitives to\nrepresent these resources as digital assets on a secure distributed ledger. Due\nto the proliferation of blockchain-based assets, there is an increasing need\nfor a generic mechanism to trade assets between isolated platforms. To date,\nthere is no such mechanism without reliance on a trusted third party.\n  In this work, we address this shortcoming and present XChange. Unlike\nexisting approaches for decentralized asset trading, we decouple trade\nmanagement and the actual exchange of assets. XChange mediates trade of any\ndigital asset between isolated blockchain platforms while limiting the fraud\nconducted by adversarial parties. We first describe a generic, five-phase\ntrading protocol that establishes and executes trade between individuals. This\nprotocol accounts full trade specifications on a separate blockchain. We then\ndevise a lightweight system architecture, composed of all required components\nfor a generic asset marketplace.\n  We implement XChange and conduct real-world experimentation. We leverage an\nexisting, lightweight blockchain, TrustChain, to account all orders and full\ntrade specifications. By deploying XChange on multiple low-resource devices, we\nshow that a full trade completes within half a second. To quantify the\nscalability of our mechanism, we conduct further experiments on our compute\ncluster. We conclude that the throughput of XChange, in terms of trades per\nsecond, scales linearly with the system load. Furthermore, we find that XChange\nexhibits superior throughput and order fulfil latency compared to related\ndecentralized exchanges, BitShares and Waves.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 14:19:37 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["de Vos", "Martijn", ""], ["Ileri", "Can Umut", ""], ["Pouwelse", "Johan", ""]]}, {"id": "2004.05074", "submitter": "Heidi Howard", "authors": "Heidi Howard, Richard Mortier", "title": "Paxos vs Raft: Have we reached consensus on distributed consensus?", "comments": "To be published in the 7th Workshop on Principles and Practice of\n  Consistency for Distributed Data (PaPoC)", "journal-ref": null, "doi": "10.1145/3380787.3393681", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed consensus is a fundamental primitive for constructing\nfault-tolerant, strongly-consistent distributed systems. Though many\ndistributed consensus algorithms have been proposed, just two dominate\nproduction systems: Paxos, the traditional, famously subtle, algorithm; and\nRaft, a more recent algorithm positioned as a more understandable alternative\nto Paxos.\n  In this paper, we consider the question of which algorithm, Paxos or Raft, is\nthe better solution to distributed consensus? We analyse both to determine\nexactly how they differ by describing a simplified Paxos algorithm using Raft's\nterminology and pragmatic abstractions.\n  We find that both Paxos and Raft take a very similar approach to distributed\nconsensus, differing only in their approach to leader election. Most notably,\nRaft only allows servers with up-to-date logs to become leaders, whereas Paxos\nallows any server to be leader provided it then updates its log to ensure it is\nup-to-date. Raft's approach is surprisingly efficient given its simplicity as,\nunlike Paxos, it does not require log entries to be exchanged during leader\nelection. We surmise that much of the understandability of Raft comes from the\npaper's clear presentation rather than being fundamental to the underlying\nalgorithm being presented.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 15:30:31 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 21:29:45 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Howard", "Heidi", ""], ["Mortier", "Richard", ""]]}, {"id": "2004.05188", "submitter": "Markus K\\\"uhbach", "authors": "Markus K\\\"uhbach, Priyanshu Bajaj, Murat Han Celik, Eric Aimo J\\\"agle,\n  Baptiste Gault", "title": "On Strong Scaling and Open Source Tools for Analyzing Atom Probe\n  Tomography Data", "comments": "38 pages, 6 main paper figures, 4 supplementary figures, settings\n  files, source code, and many results on Zenodo as supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atom probe tomography (APT) has matured to a versatile nanoanalytical\ncharacterization tool with applications that range from materials science to\ngeology and possibly beyond. Already, well over 100 APT microscopes exist\nworldwide. Information from the APT data requires a post-processing of the\nreconstructed point cloud which is realized via basic implementations of data\nscience methods, mostly executed with proprietary software. Limitations of the\nsoftware have motivated the APT community to develop supplementary\npost-processing tools to cope with increasing method complexity and higher\nquality demands: examples are how to improve method transparency, how to\nsupport batch processing capabilities, and how to document more completely the\nmethods and computational workflows to better align with the FAIR data\nstewardship principles.\n  One gap in the APT software tool landscape has been a collection of open\ntools which support scientific computing hardware. Here, we introduce\nPARAPROBE, an open source, efficient tool for the scientific computing of APT\ndata. We show how to process several computational geometry, spatial\nstatistics, and clustering tasks performantly for datasets as large as two\nbillion ions. Our parallelization efforts yield orders of magnitude performance\ngains and deliver batch processing capabilities. We contribute these tools in\nan effort to open up APT data mining and simplify it to make tools for rigorous\nquantification, sensitivity analyses, and cross-method benchmarking available\nto practitioners.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 18:09:26 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["K\u00fchbach", "Markus", ""], ["Bajaj", "Priyanshu", ""], ["Celik", "Murat Han", ""], ["J\u00e4gle", "Eric Aimo", ""], ["Gault", "Baptiste", ""]]}, {"id": "2004.05371", "submitter": "Mohamed Wahib", "authors": "Lingqi Zhang, Mohamed Wahib, Haoyu Zhang, Satoshi Matsuoka", "title": "A Study of Single and Multi-device Synchronization Methods in Nvidia\n  GPUs", "comments": "IPDPS20", "journal-ref": "IEEE International Parallel & Distributed Processing Symposium\n  2020", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are playing an increasingly important role in general-purpose computing.\nMany algorithms require synchronizations at different levels of granularity in\na single GPU. Additionally, the emergence of dense GPU nodes also calls for\nmulti-GPU synchronization. Nvidia's latest CUDA provides a variety of\nsynchronization methods. Until now, there is no full understanding of the\ncharacteristics of those synchronization methods. This work explores important\nundocumented features and provides an in-depth analysis of the performance\nconsiderations and pitfalls of the state-of-art synchronization methods for\nNvidia GPUs. The provided analysis would be useful when making design choices\nfor applications, libraries, and frameworks running on single and/or multi-GPU\nenvironments. We provide a case study of the commonly used reduction operator\nto illustrate how the knowledge gained in our analysis can be useful. We also\ndescribe our micro-benchmarks and measurement methods.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 11:21:48 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Lingqi", ""], ["Wahib", "Mohamed", ""], ["Zhang", "Haoyu", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "2004.05628", "submitter": "Reena Nair", "authors": "Reena Nair and Tony Field", "title": "GAPP: A Fast Profiler for Detecting Serialization Bottlenecks in\n  Parallel Linux Applications", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/3358960.3379136", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel profiling tool, GAPP, that identifies serialization\nbottlenecks in parallel Linux applications arising from load imbalance or\ncontention for shared resources . It works by tracing kernel context switch\nevents using kernel probes managed by the extended Berkeley Packet Filter\n(eBPF) framework. The overhead is thus extremely low (an average 4% run time\noverhead for the applications explored), the tool requires no program\ninstrumentation and works for a variety of serialization bottlenecks. We\nevaluate GAPP using the Parsec3.0 benchmark suite and two large open-source\nprojects: MySQL and Nektar++ (a spectral/hp element framework). We show that\nGAPP is able to reveal a wide range of bottleneck-related performance issues,\nfor example arising from synchronization primitives, busy-wait loops, memory\noperations, thread imbalance and resource contention.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 15:01:04 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Nair", "Reena", ""], ["Field", "Tony", ""]]}, {"id": "2004.05695", "submitter": "Husam Suleiman", "authors": "Husam Suleiman, Otman Basir", "title": "QoS-Driven Job Scheduling: Multi-Tier Dependency Considerations", "comments": null, "journal-ref": "pp.133-155, Volume 9, Number 9, July 2019", "doi": "10.5121/csit.2019.90912", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a cloud service provider, delivering optimal system performance while\nfulfilling Quality of Service (QoS) obligations is critical for maintaining a\nviably profitable business. This goal is often hard to attain given the\nirregular nature of cloud computing jobs. These jobs expect high QoS on an\non-demand fashion, that is on random arrival. To optimize the response to such\nclient demands, cloud service providers organize the cloud computing\nenvironment as a multi-tier architecture. Each tier executes its designated\ntasks and passes the job to the next tier; in a fashion similar, but not\nidentical, to the traditional job-shop environments. An optimization process\nmust take place to schedule the appropriate tasks of the job on the resources\nof the tier, so as to meet the QoS expectations of the job. Existing approaches\nemploy scheduling strategies that consider the performance optimization at the\nindividual resource level and produce optimal single-tier driven schedules. Due\nto the sequential nature of the multi-tier environment, the impact of such\nschedules on the performance of other resources and tiers tend to be ignored,\nresulting in a less than optimal performance when measured at the multi-tier\nlevel. In this paper, we propose a multi-tier-oriented job scheduling and\nallocation technique. The scheduling and allocation process is formulated as a\nproblem of assigning jobs to the resource queues of the cloud computing\nenvironment, where each resource of the environment employs a queue to hold the\njobs assigned to it. The scheduling problem is NP-hard, as such a biologically\ninspired genetic algorithm is proposed. The computing resources across all\ntiers of the environment are virtualized in one resource by means of a single\nqueue virtualization. A chromosome that mimics the sequencing and allocation of\nthe tasks in the proposed virtual queue is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 21:01:58 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Suleiman", "Husam", ""], ["Basir", "Otman", ""]]}, {"id": "2004.05696", "submitter": "Husam Suleiman", "authors": "Husam Suleiman, Otman Basir", "title": "Service Level Driven Job Scheduling in Multi-Tier Cloud Computing: A\n  Biologically Inspired Approach", "comments": null, "journal-ref": "pp.99-118, Volume 9, Number 9, July 2019", "doi": "10.5121/csit.2019.90910", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing environments often have to deal with random-arrival\ncomputational workloads that vary in resource requirements and demand high\nQuality of Service (QoS) obligations. It is typical that a\nService-Level-Agreement (SLA) is employed to govern the QoS obligations of the\ncloud computing service provider to the client. A typical challenge\nservice-providers face every day is maintaining a balance between the limited\nresources available for computing and the high QoS requirements of varying\nrandom demands. Any imbalance in managing these conflicting objectives may\nresult in either dissatisfied clients and potentially significant commercial\npenalties, or an over-resourced cloud computing environment that can be\nsignificantly costly to acquire and operate. Thus, scheduling the clients'\nworkloads as they arrive at the environment to ensure their timely execution\nhas been a central issue in cloud computing. Various approaches have been\nreported in the literature to address this problem: Shortest-Queue,\nJoin-Idle-Queue, Round Robin, MinMin, MaxMin, and Least Connection, to name a\nfew. However, optimization strategies of such approaches fail to capture QoS\nobligations and their associated commercial penalties. This paper presents an\napproach for service-level driven load scheduling and balancing in multi-tier\nenvironments. Joint scheduling and balancing operations are employed to\ndistribute and schedule jobs among the resources, such that the total waiting\ntime of client jobs is minimized, and thus the potential of a penalty to be\nincurred by the service provider is mitigated. A penalty model is used to\nquantify the penalty the service provider incurs as a function of the jobs'\ntotal waiting time. A Virtual-Queue abstraction is proposed to facilitate\noptimal job scheduling at the tier level. This problem is NP-complete, a\ngenetic algorithm is proposed for computing job schedules.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 21:03:37 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Suleiman", "Husam", ""], ["Basir", "Otman", ""]]}, {"id": "2004.05712", "submitter": "Chiranjeeb Buragohain", "authors": "Chiranjeeb Buragohain, Knut Magne Risvik, Paul Brett, Miguel Castro,\n  Wonhee Cho, Joshua Cowhig, Nikolas Gloy, Karthik Kalyanaraman, Richendra\n  Khanna, John Pao, Matthew Renzelmann, Alex Shamis, Timothy Tan and Shuheng\n  Zheng", "title": "A1: A Distributed In-Memory Graph Database", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3386135", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A1 is an in-memory distributed database used by the Bing search engine to\nsupport complex queries over structured data. The key enablers for A1 are\navailability of cheap DRAM and high speed RDMA (Remote Direct Memory Access)\nnetworking in commodity hardware. A1 uses FaRM as its underlying storage layer\nand builds the graph abstraction and query engine on top. The combination of\nin-memory storage and RDMA access requires rethinking how data is allocated,\norganized and queried in a large distributed system. A single A1 cluster can\nstore tens of billions of vertices and edges and support a throughput of 350+\nmillion of vertex reads per second with end to end query latency in single\ndigit milliseconds. In this paper we describe the A1 data model, RDMA optimized\ndata structures and query execution.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 22:58:46 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Buragohain", "Chiranjeeb", ""], ["Risvik", "Knut Magne", ""], ["Brett", "Paul", ""], ["Castro", "Miguel", ""], ["Cho", "Wonhee", ""], ["Cowhig", "Joshua", ""], ["Gloy", "Nikolas", ""], ["Kalyanaraman", "Karthik", ""], ["Khanna", "Richendra", ""], ["Pao", "John", ""], ["Renzelmann", "Matthew", ""], ["Shamis", "Alex", ""], ["Tan", "Timothy", ""], ["Zheng", "Shuheng", ""]]}, {"id": "2004.05723", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Brian Bockelman, Derek Weitzel, Xinkai Zhang, Hamid\n  Vakilzadian and David Swanson", "title": "Trua: Efficient Task Replication for Flexible User-defined Availability\n  in Scientific Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure is inevitable in scientific computing. As scientific applications and\nfacilities increase their scales over the last decades, finding the root cause\nof a failure can be very complex or at times nearly impossible. Different\nscientific computing customers have varying availability demands as well as a\ndiverse willingness to pay for availability. In contrast to existing solutions\nthat try to provide higher and higher availability in scientific grids, we\npropose a model called Task Replication for User-defined Availability (Trua).\nTrua provides flexible, user-defined, availability in scientific grids,\nallowing customers to express their desire for availability to computational\nproviders. Trua differs from existing task replication approaches in two folds.\nFirst, it relies on the historic failure information collected from the virtual\nlayer of the scientific grids. The reliability model for the failures can be\nrepresented with a bimodal Johnson distribution which is different from any\nexisting distributions. Second, it adopts an anomaly detector to filter out\nanomalous failures; it additionally adopts novel selection algorithms to\nmitigate the effects of temporary and spatial correlations of the failures\nwithout knowing the root cause of the failures. We apply the Trua on real-world\ntraces collected from the Open Science Grid (OSG). Our results show that the\nTrua can successfully meet user-defined availability demands.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 00:04:29 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Zhe", ""], ["Bockelman", "Brian", ""], ["Weitzel", "Derek", ""], ["Zhang", "Xinkai", ""], ["Vakilzadian", "Hamid", ""], ["Swanson", "David", ""]]}, {"id": "2004.05729", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Brian Bockelman, Derek Weitzel and David Swanson", "title": "Exploring Erasure Coding Techniques for High Availability of\n  Intermediate Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computing workflows generate enormous distributed data that is\nshort-lived, yet critical for job completion time. This class of data is called\nintermediate data. A common way to achieve high data availability is to\nreplicate data. However, an increasing scale of intermediate data generated in\nmodern scientific applications demands new storage techniques to improve\nstorage efficiency. Erasure Codes, as an alternative, can use less storage\nspace while maintaining similar data availability. In this paper, we adopt\nerasure codes for storing intermediate data and compare its performance with\nreplication. We also use the metric of Mean-Time-To-Data-Loss (MTTDL) to\nestimate the lifetime of intermediate data. We propose an algorithm to\nproactively relocate data redundancy from vulnerable machines to reliable ones\nto improve data availability with some extra network overhead. Furthermore, we\npropose an algorithm to assign redundancy units of data physically close to\neach other on the network to reduce the network bandwidth for reconstructing\ndata when it is being accessed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 00:13:01 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Zhe", ""], ["Bockelman", "Brian", ""], ["Weitzel", "Derek", ""], ["Swanson", "David", ""]]}, {"id": "2004.05740", "submitter": "Anirban Bhattacharjee", "authors": "Anirban Bhattacharjee, Ajay Dev Chhokra, Hongyang Sun, Shashank\n  Shekhar, Aniruddha Gokhale, Gabor Karsai, Abhishek Dubey", "title": "Deep-Edge: An Efficient Framework for Deep Learning Model Update on\n  Heterogeneous Edge", "comments": null, "journal-ref": "2020 IEEE 4th International Conference on Fog and Edge Computing\n  (ICFEC)", "doi": "10.1109/ICFEC50348.2020.00016", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) model-based AI services are increasingly offered in a\nvariety of predictive analytics services such as computer vision, natural\nlanguage processing, speech recognition. However, the quality of the DL models\ncan degrade over time due to changes in the input data distribution, thereby\nrequiring periodic model updates. Although cloud data-centers can meet the\ncomputational requirements of the resource-intensive and time-consuming model\nupdate task, transferring data from the edge devices to the cloud incurs a\nsignificant cost in terms of network bandwidth and are prone to data privacy\nissues. With the advent of GPU-enabled edge devices, the DL model update can be\nperformed at the edge in a distributed manner using multiple connected edge\ndevices. However, efficiently utilizing the edge resources for the model update\nis a hard problem due to the heterogeneity among the edge devices and the\nresource interference caused by the co-location of the DL model update task\nwith latency-critical tasks running in the background. To overcome these\nchallenges, we present Deep-Edge, a load- and interference-aware,\nfault-tolerant resource management framework for performing model update at the\nedge that uses distributed training. This paper makes the following\ncontributions. First, it provides a unified framework for monitoring,\nprofiling, and deploying the DL model update tasks on heterogeneous edge\ndevices. Second, it presents a scheduler that reduces the total re-training\ntime by appropriately selecting the edge devices and distributing data among\nthem such that no latency-critical applications experience deadline violations.\nFinally, we present empirical results to validate the efficacy of the framework\nusing a real-world DL model update case-study based on the Caltech dataset and\nan edge AI cluster testbed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 01:46:29 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 03:35:35 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Bhattacharjee", "Anirban", ""], ["Chhokra", "Ajay Dev", ""], ["Sun", "Hongyang", ""], ["Shekhar", "Shashank", ""], ["Gokhale", "Aniruddha", ""], ["Karsai", "Gabor", ""], ["Dubey", "Abhishek", ""]]}, {"id": "2004.05777", "submitter": "Anirban Ghose", "authors": "Anirban Ghose, Srijeeta Maity, Arijit Kar, Kaustubh Maloo, Soumyajit\n  Dey", "title": "Intelligent Orchestration of ADAS Pipelines on Next Generation\n  Automotive Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced Driver-Assistance Systems (ADAS) is one of the primary drivers\nbehind increasing levels of autonomy, driving comfort in this age of connected\nmobility. However, the performance of such systems is a function of execution\nrate which demands on-board platform-level support. With GPGPU platforms making\ntheir way into automobiles, there exists an opportunity to adaptively support\nhigh execution rates for ADAS tasks by exploiting architectural heterogeneity,\nkeeping in mind thermal reliability and long-term platform aging. We propose a\nfuture-proof, learning-based adaptive scheduling framework that leverages\nReinforcement Learning to discover suitable scenario based task-mapping\ndecisions for accommodating increased task-level throughput requirements.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 05:54:22 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ghose", "Anirban", ""], ["Maity", "Srijeeta", ""], ["Kar", "Arijit", ""], ["Maloo", "Kaustubh", ""], ["Dey", "Soumyajit", ""]]}, {"id": "2004.05868", "submitter": "Amir Javadpour", "authors": "Amir Javadpour, Guojun Wang, Samira Rezaei and Kuan Ching Li", "title": "Detecting Straggler MapReduce Tasks in Big Data Processing\n  Infrastructure by Neural Network", "comments": null, "journal-ref": null, "doi": "10.1007/s11227-019-03136-6", "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Straggler task detection is one of the main challenges in applying MapReduce\nfor parallelizing and distributing large-scale data processing. It is defined\nas detecting running tasks on weak nodes. Considering two stages in the Map\nphase copy, combine and three stages of Reduce shuffle, sort and reduce, the\ntotal execution time is the total sum of the execution time of these five\nstages. Estimating the correct execution time in each stage that results in\ncorrect total execution time is the primary purpose of this paper. The proposed\nmethod is based on the application of a backpropagation Neural Network NN on\nthe Hadoop for the detection of straggler tasks, to estimate the remaining\nexecution time of tasks that is very important in straggler task detection.\nResults achieved have been compared with popular algorithms in this domain such\nas LATE, ESAMR and the real remaining time for WordCount and Sort benchmarks,\nand shown able to detect straggler tasks and estimate execution time\naccurately. Besides, it supports to accelerate task execution time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 11:15:00 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Javadpour", "Amir", ""], ["Wang", "Guojun", ""], ["Rezaei", "Samira", ""], ["Li", "Kuan Ching", ""]]}, {"id": "2004.05933", "submitter": "Enrique Fynn", "authors": "Enrique Fynn, Alysson Bessani, Fernando Pedone", "title": "Smart Contracts on the Move", "comments": "Preprint to appear in the 50th IEEE/IFIP Dependable Systems and\n  Networks Conference (DSN'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain systems have received much attention and promise to revolutionize\nmany services. Yet, despite their popularity, current blockchain systems exist\nin isolation, that is, they cannot share information. While interoperability is\ncrucial for blockchain to reach widespread adoption, it is difficult to achieve\ndue to differences among existing blockchain technologies. This paper presents\na technique to allow blockchain interoperability. The core idea is to provide a\nprimitive operation to developers so that contracts and objects can switch from\none blockchain to another, without breaking consistency and violating key\nblockchain properties. To validate our ideas, we implemented our protocol in\ntwo popular blockchain clients that use the Ethereum virtual machine. We\ndiscuss how to build applications using the proposed protocol and show examples\nof applications based on real use cases that can move across blockchains. To\nanalyze the system performance we use a real trace from one of the most popular\nEthereum applications and replay it in a multi-blockchain environment.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 13:32:04 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 08:35:36 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 08:59:08 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Fynn", "Enrique", ""], ["Bessani", "Alysson", ""], ["Pedone", "Fernando", ""]]}, {"id": "2004.05962", "submitter": "Orestis Zachariadis", "authors": "Orestis Zachariadis, Andrea Teatini, Nitin Satpute, Juan G\\'omez-Luna,\n  Onur Mutlu, Ole Jakob Elle, Joaqu\\'in Olivares", "title": "Accelerating B-spline Interpolation on GPUs: Application to Medical\n  Image Registration", "comments": "Accepted in CMPB", "journal-ref": "Comput. Methods Programs Biomed. 193 (2020) 105431", "doi": "10.1016/j.cmpb.2020.105431", "report-no": null, "categories": "cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective. B-spline interpolation (BSI) is a popular technique\nin the context of medical imaging due to its adaptability and robustness in 3D\nobject modeling. A field that utilizes BSI is Image Guided Surgery (IGS). IGS\nprovides navigation using medical images, which can be segmented and\nreconstructed into 3D models, often through BSI. Image registration tasks also\nuse BSI to align pre-operative data to intra-operative data. However, such IGS\ntasks are computationally demanding, especially when applied to 3D medical\nimages, due to the complexity and amount of data involved. Therefore,\noptimization of IGS algorithms is greatly desirable, for example, to perform\nimage registration tasks intra-operatively and to enable real-time\napplications. A traditional CPU does not have sufficient computing power to\nachieve these goals. In this paper, we introduce a novel GPU implementation of\nBSI to accelerate the calculation of the deformation field in non-rigid image\nregistration algorithms.\n  Methods. Our BSI implementation on GPUs minimizes the data that needs to be\nmoved between memory and processing cores during loading of the input grid, and\nleverages the large on-chip GPU register file for reuse of input values.\nMoreover, we re-formulate our method as trilinear interpolations to reduce\ncomputational complexity and increase accuracy. To provide pre-clinical\nvalidation of our method and demonstrate its benefits in medical applications,\nwe integrate our improved BSI into a registration workflow for compensation of\nliver deformation (caused by pneumoperitoneum, i.e., inflation of the abdomen)\nand evaluate its performance.\n  Results. Our approach improves the performance of BSI by an average of 6.5x\nand interpolation accuracy by 2x compared to three state-of-the-art GPU\nimplementations. We observe up to 34% acceleration of non-rigid image\nregistration.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:26:36 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 11:33:02 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Zachariadis", "Orestis", ""], ["Teatini", "Andrea", ""], ["Satpute", "Nitin", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Mutlu", "Onur", ""], ["Elle", "Ole Jakob", ""], ["Olivares", "Joaqu\u00edn", ""]]}, {"id": "2004.06101", "submitter": "Mirek Riedewald", "authors": "Rundong Li, Wolfgang Gatterbauer, Mirek Riedewald", "title": "Near-Optimal Distributed Band-Joins through Recursive Partitioning", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3389750", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider running-time optimization for band-joins in a distributed system,\ne.g., the cloud. To balance load across worker machines, input has to be\npartitioned, which causes duplication. We explore how to resolve this tension\nbetween maximum load per worker and input duplication for band-joins between\ntwo relations. Previous work suffered from high optimization cost or considered\npartitionings that were too restricted (resulting in suboptimal join\nperformance). Our main insight is that recursive partitioning of the\njoin-attribute space with the appropriate split scoring measure can achieve\nboth low optimization cost and low join cost. It is the first approach that is\nnot only effective for one-dimensional band-joins but also for joins on\nmultiple attributes. Experiments indicate that our method is able to find\npartitionings that are within 10% of the lower bound for both maximum load per\nworker and input duplication for a broad range of settings, significantly\nimproving over previous work.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 17:59:27 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Li", "Rundong", ""], ["Gatterbauer", "Wolfgang", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2004.06215", "submitter": "Timothy Castiglia Mr.", "authors": "Timothy Castiglia, Colin Goldberg, Stacy Patterson", "title": "A Hierarchical Model for Fast Distributed Consensus in Dynamic Networks", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new consensus algorithms for dynamic networks. The first, Fast\nRaft, is a variation on the Raft consensus algorithm that reduces the number of\nmessage rounds in typical operation. Fast Raft is ideal for fast-paced\ndistributed systems where membership changes over time and where sites must\nreach consensus quickly. The second, C-Raft, is targeted for distributed\nsystems where sites are grouped into clusters, with fast communication within\nclusters and slower communication between clusters. C-Raft uses Fast Raft as a\nbuilding block and defines a hierarchical model of consensus to improve upon\nthroughput in globally distributed systems. We prove the safety and liveness\nproperties of each algorithm. Finally, we present an experimental evaluation of\nboth algorithms in AWS.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 22:00:49 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 19:28:23 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Castiglia", "Timothy", ""], ["Goldberg", "Colin", ""], ["Patterson", "Stacy", ""]]}, {"id": "2004.06254", "submitter": "Amirali Daghighi", "authors": "Amirali Daghighi and Jim Q. Chen", "title": "Comparisons of Algorithms in Big Data Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel computing is the fundamental base for MapReduce framework in Hadoop.\nEach data chunk is replicated over 3 servers for increasing availability of\ndata and decreasing probability of data loss. Hence, the 3 servers that have\nMap task stored on their disk are fastest servers to process them, which are\ncalled local servers. All servers in the same rack as local servers are called\nrack-local servers that are slower than local servers since data chunk\nassociated with Map task should be fetched through top of the rack switch. All\nother servers are called remote servers that are slowest servers since they\nneed to fetch data from a local server in another rack, so data should be\ntransmitted through at least 2 top of rack switches and a core switch. Note\nthat number of switches in path of data transfer depends on internal network\nstructure of data centers. The First-In-First-Out (FIFO) and Hadoop Fair\nScheduler (HFS) algorithms do not take rack structure of data centers into\naccount, so they are known to not be heavy-traffic delay optimal or even\nthroughput optimal. The recent advances on scheduling for data centers\nconsidering rack structure of them and heterogeneity of servers resulted in\nstate-of-the-art Balanced-PANDAS algorithm that outperforms classic MaxWeight\nalgorithm. In both Balanced-PANDAS and MaxWeight algorithms, processing rate of\nlocal, rack-local, and remote servers are assumed to be known. However, with\nthe change of traffic over time in addition to estimation errors of processing\nrates, it is not realistic to consider processing rates to be known. In this\nwork, we study robustness of Balanced-PANDAS and MaxWeight algorithms in terms\nof inaccurate estimations of processing rates. We observe that Balanced-PANDAS\nis not as sensitive as MaxWeight on the accuracy of processing rates, making it\nmore appealing to use in data centers.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 01:09:25 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 12:02:41 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Daghighi", "Amirali", ""], ["Chen", "Jim Q.", ""]]}, {"id": "2004.06263", "submitter": "Huang Lingxiao", "authors": "Lingxiao Huang and Nisheeth K. Vishnoi", "title": "Coresets for Clustering in Euclidean Spaces: Importance Sampling is\n  Nearly Optimal", "comments": "Full version of STOC 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of $n$ points in $\\mathbb{R}^d$, the goal of the\n$(k,z)$-clustering problem is to find a subset of $k$ \"centers\" that minimizes\nthe sum of the $z$-th powers of the Euclidean distance of each point to the\nclosest center. Special cases of the $(k,z)$-clustering problem include the\n$k$-median and $k$-means problems. Our main result is a unified two-stage\nimportance sampling framework that constructs an $\\varepsilon$-coreset for the\n$(k,z)$-clustering problem. Compared to the results for $(k,z)$-clustering in\n[Feldman and Langberg, STOC 2011], our framework saves a $\\varepsilon^2 d$\nfactor in the coreset size. Compared to the results for $(k,z)$-clustering in\n[Sohler and Woodruff, FOCS 2018], our framework saves a\n$\\operatorname{poly}(k)$ factor in the coreset size and avoids the\n$\\exp(k/\\varepsilon)$ term in the construction time. Specifically, our coreset\nfor $k$-median ($z=1$) has size $\\tilde{O}(\\varepsilon^{-4} k)$ which, when\ncompared to the result in [Sohler and Woodruff, STOC 2018], saves a $k$ factor\nin the coreset size. Our algorithmic results rely on a new dimensionality\nreduction technique that connects two well-known shape fitting problems:\nsubspace approximation and clustering, and may be of independent interest. We\nalso provide a size lower bound of $\\Omega\\left(k\\cdot \\min \\left\\{2^{z/20},d\n\\right\\}\\right)$ for a $0.01$-coreset for $(k,z)$-clustering, which has a\nlinear dependence of size on $k$ and an exponential dependence on $z$ that\nmatches our algorithmic results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 01:48:16 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 15:50:47 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 03:32:37 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Huang", "Lingxiao", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2004.06299", "submitter": "Lam Duc Nguyen", "authors": "Lam D. Nguyen, Anders E. Kal{\\o}r, Israel Leyva-Mayorga, and Petar\n  Popovski", "title": "Trusted Wireless Monitoring based on Blockchain over NB-IoT Connectivity", "comments": "7 pages, 6 figures, Accepted in IEEE Communication Magazine", "journal-ref": "IEEE Communication Magazine 2020", "doi": "10.1109/MCOM.001.2000116", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The data collected from Internet of Things (IoT) devices on various emissions\nor pollution, can have a significant economic value for the stakeholders. This\nmakes it prone to abuse or tampering and brings forward the need to integrate\nIoT with a Distributed Ledger Technology (DLT) to collect, store, and protect\nthe IoT data. However, DLT brings an additional overhead to the frugal IoT\nconnectivity and symmetrizes the IoT traffic, thus changing the usual\nassumption that IoT is uplink-oriented. We have implemented a platform that\nintegrates DLTs with a monitoring system based on narrowband IoT (NB-IoT). We\nevaluate the performance and discuss the tradeoffs in two use cases: data\nauthorization and real-time monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 04:50:02 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 23:56:32 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 20:11:43 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nguyen", "Lam D.", ""], ["Kal\u00f8r", "Anders E.", ""], ["Leyva-Mayorga", "Israel", ""], ["Popovski", "Petar", ""]]}, {"id": "2004.06354", "submitter": "Aleix Roca Nonell", "authors": "Aleix Roca, Samuel Rodr\\'iguez, Albert Segura, Kevin Marquet,\n  Vicen\\c{c} Beltran", "title": "A Linux Kernel Scheduler Extension for Multi-core Systems", "comments": "10 pages, 5 figures, conference", "journal-ref": null, "doi": "10.1109/HiPC.2019.00050", "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linux kernel is mostly designed for multi-programed environments, but\nhigh-performance applications have other requirements. Such applications are\nrun standalone, and usually rely on runtime systems to distribute the\napplication's workload on worker threads, one per core. However, due to current\nOSes limitations, it is not feasible to track whether workers are actually\nrunning or blocked due to, for instance, a requested resource. For I/O\nintensive applications, this leads to a significant performance degradation\ngiven that the core of a blocked thread becomes idle until it is able to run\nagain. In this paper, we present the proof-of-concept of a Linux kernel\nextension denoted User-Monitored Threads (UMT) which tackles this problem. Our\nextension allows a user-space process to be notified of when the selected\nthreads become blocked or unblocked, making it possible for a runtime to\nschedule additional work on the idle core. We implemented the extension on the\nLinux Kernel 5.1 and adapted the Nanos6 runtime of the OmpSs-2 programming\nmodel to take advantage of it. The whole prototype was tested on two\napplications which, on the tested hardware and the appropriate conditions,\nreported speedups of almost 2x.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 08:35:33 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Roca", "Aleix", ""], ["Rodr\u00edguez", "Samuel", ""], ["Segura", "Albert", ""], ["Marquet", "Kevin", ""], ["Beltran", "Vicen\u00e7", ""]]}, {"id": "2004.06436", "submitter": "Yael Hitron", "authors": "Yael Hitron, Merav Parter", "title": "Round-Efficient Distributed Byzantine Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first round efficient algorithms for several fundamental\ndistributed tasks in the presence of a Byzantine edge. Our algorithms work in\nthe CONGEST model of distributed computing. In the \\emph{Byzantine Broadcast}\nproblem, given is a network $G=(V,E)$ with an unknown Byzantine edge $e'$.\nThere is a source node $s$ holding an initial message $m_0$, and the goal is\nfor all the nodes in the network to receive a copy of $m_0$, while ignoring all\nother messages. Perhaps surprisingly, to the best of our knowledge, all\nexisting algorithms for the problem either assume that the Byzantine behavior\nis probabilistic, use polynomially large messages or else suffer from a large\nround complexity.\n  We give an $\\widetilde{O}(D^2)$-round \\footnote{The notion $\\widetilde{O}$\nhides poly-logarithmic terms, and the notion $\\widehat{O}$ hides a\nmultiplicative factor of an $2^{O(\\sqrt{\\log n})}$ term.} algorithm for the\nByzantine Broadcast problem, where $D$ is the diameter of the graph. The\ncommunication graph is required to be $3$-edge connected, which is known to be\na necessary condition. We also provide a Leader Election algorithm in the\npresence of a Byzantine edge with the same round complexity of\n$\\widetilde{O}(D^2)$ rounds. We use these algorithms to provide the efficient\nconstruction of \\emph{Byzantine cycle covers} which serve the basis for (i)\nByzantine BFS algorithms and (ii) a general compiler for algorithms in the\npresence of a Byzantine edge.\n  We hope that the tools provided in this paper will pave the way towards\nobtaining \\textbf{round-efficient algorithms} for many more distributed\nproblems in the presence of Byzantine edges and nodes.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 11:56:33 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Hitron", "Yael", ""], ["Parter", "Merav", ""]]}, {"id": "2004.06507", "submitter": "Oksana Shadura", "authors": "Vassil Vassilev (1), David Lange (1), Malik Shahzad Muzaffar (2),\n  Mircho Rodozov (3), Oksana Shadura (4) and Alexander Penev (5) ((1) Princeton\n  University, (2) CERN, (3) Bulgarian Academy of Sciences, (4) University of\n  Nebraska Lincoln, (5) University of Plovdiv)", "title": "C++ Modules in ROOT and Beyond", "comments": "Submitted as a proceedings to CHEP 2019", "journal-ref": null, "doi": "10.1051/epjconf/202024505011", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C++ Modules come in C++20 to fix the long-standing build scalability problems\nin the language. They provide an io-efficient, on-disk representation capable\nto reduce build times and peak memory usage. ROOT employs the C++ modules\ntechnology further in the ROOT dictionary system to improve its performance and\nreduce the memory footprint.\n  ROOT with C++ Modules was released as a technology preview in fall 2018,\nafter intensive development during the last few years. The current state is\nready for production, however, there is still room for performance\noptimizations. In this talk, we show the roadmap for making the technology\ndefault in ROOT. We demonstrate a global module indexing optimization which\nallows reducing the memory footprint dramatically for many workflows. We will\nreport user feedback on the migration to ROOT with C++ Modules.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 14:23:23 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 15:19:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Vassilev", "Vassil", ""], ["Lange", "David", ""], ["Muzaffar", "Malik Shahzad", ""], ["Rodozov", "Mircho", ""], ["Shadura", "Oksana", ""], ["Penev", "Alexander", ""]]}, {"id": "2004.06722", "submitter": "Misun Min Dr", "authors": "Paul Fischer, Misun Min, Thilina Rathnayake, Som Dutta, Tzanio Kolev,\n  Veselin Dobrev, Jean-Sylvain Camier, Martin Kronbichler, Tim Warburton, Kasia\n  Swirydowicz, Jed Brown", "title": "Scalability of High-Performance PDE Solvers", "comments": "25 pages, 54 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance tests and analyses are critical to effective HPC software\ndevelopment and are central components in the design and implementation of\ncomputational algorithms for achieving faster simulations on existing and\nfuture computing architectures for large-scale application problems. In this\npaper, we explore performance and space-time trade-offs for important\ncompute-intensive kernels of large-scale numerical solvers for PDEs that govern\na wide range of physical applications. We consider a sequence of PDE- motivated\nbake-off problems designed to establish best practices for efficient high-order\nsimulations across a variety of codes and platforms. We measure peak\nperformance (degrees of freedom per second) on a fixed number of nodes and\nidentify effective code optimization strategies for each architecture. In\naddition to peak performance, we identify the minimum time to solution at 80%\nparallel efficiency. The performance analysis is based on spectral and p-type\nfinite elements but is equally applicable to a broad spectrum of numerical PDE\ndiscretizations, including finite difference, finite volume, and h-type finite\nelements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:54:48 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Fischer", "Paul", ""], ["Min", "Misun", ""], ["Rathnayake", "Thilina", ""], ["Dutta", "Som", ""], ["Kolev", "Tzanio", ""], ["Dobrev", "Veselin", ""], ["Camier", "Jean-Sylvain", ""], ["Kronbichler", "Martin", ""], ["Warburton", "Tim", ""], ["Swirydowicz", "Kasia", ""], ["Brown", "Jed", ""]]}, {"id": "2004.06809", "submitter": "Anwar Ghani Dr.", "authors": "Anwar Ghani, Afzal Badshah, Saeedullah Jan, Abdulrahman A. Alshdadi,\n  Ali Daud", "title": "Issues and challenges in Cloud Storage Architecture: A Survey", "comments": "12 pages, 5 figures, The article may appear in a future issue of\n  Researchpedia.info Journal of Computing Surveys", "journal-ref": "Researchpedia Journal of Computing, Researchpedia.info, 2020,\n  1(1): 50-64", "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From home appliances to industrial enterprises, the Information and\nCommunication Technology (ICT) industry is revolutionizing the world. We are\nwitnessing the emergence of new technologies (e.g, Cloud computing, Fog\ncomputing, Internet of Things (IoT), Artificial Intelligence (AI) and\nBlock-chain) which proves the growing use of ICT (e,g. business, education,\nhealth, and home appliances), resulting in massive data generation. It is\nexpected that more than 175 ZB data will be processed annually by 75 billion\ndevices by 2025. The 5G technology (i.e. mobile communication technology)\ndramatically increases network speed, enabling users to upload ultra high\ndefinition videos in real-time, which will generate a massive stream of big\ndata. Furthermore, smart devices, having artificial intelligence, will act like\na human being (e.g, a self-driving vehicle, etc) on the network, will also\ngenerate big data. This sudden shift and massive data generation created\nserious challenges in storing and managing heterogeneous data at such a large\nscale. This article presents a state-of-the-art review of the issues and\nchallenges involved in storing heterogeneous big data, their countermeasures\n(i.e, from security and management perspectives), and future opportunities of\ncloud storage. These challenges are reviewed in detail and new dynamics for\nresearchers in the field of cloud storage are discovered.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 21:23:58 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 12:33:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ghani", "Anwar", ""], ["Badshah", "Afzal", ""], ["Jan", "Saeedullah", ""], ["Alshdadi", "Abdulrahman A.", ""], ["Daud", "Ali", ""]]}, {"id": "2004.06861", "submitter": "Yu Chen", "authors": "Joseph St. Cyr, Joshua Vanderpool, Yu Chen, Xiaohua Li", "title": "HODET: Hybrid Object DEtection and Tracking using mmWave Radar and\n  Visual Sensors", "comments": "2020 SPIE Defense + Commercial Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image sensors have been explored heavily in automotive applications for\ncollision avoidance and varying levels of autonomy. It requires a degree of\nbrightness, therefore, the use of an image sensor in nighttime operation or\ndark conditions can be problematic along with challenging weather such as fog.\nRadar sensors have been employed to help cover the various environmental\nchallenges with visible spectrum cameras. Edge computing technology has the\npotential to address a number of issues such as real-time processing\nrequirements, off-loading of processing from congested servers, and size,\nweight, power, and cost (SWaP-C) constraints. This paper proposes a novel\nHybrid Object DEtection and Tracking (HODET) using mmWave Radar and Visual\nSensors at the edge. The HODET is a computing application of low SWaP-C\nelectronics performing object detection, tracking and identification algorithms\nwith the simultaneous use of image and radar sensors. While the machine vision\ncamera alone could estimate the distance of an object, the radar sensor will\nprovide an accurate distance and vector of movement. This additional data\naccuracy can be leveraged to further discriminate a detected object to protect\nagainst spoofing attacks. A real-world smart community public safety monitoring\nscenario is selected to verify the effectiveness of HODET, which detects,\ntracks objects of interests and identify suspicious activities. The\nexperimental results demonstrate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 03:18:13 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Cyr", "Joseph St.", ""], ["Vanderpool", "Joshua", ""], ["Chen", "Yu", ""], ["Li", "Xiaohua", ""]]}, {"id": "2004.06896", "submitter": "Mao V. Ngo", "authors": "Mao V. Ngo, Tie Luo, Hakima Chaouchi, and Tony Q.S. Quek", "title": "Contextual-Bandit Anomaly Detection for IoT Data in Distributed\n  Hierarchical Edge Computing", "comments": "Accepted for presenting at IEEE International Conference on\n  Distributed Computing Systems (ICDCS), Demo Track, July 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep neural networks (DNN) greatly bolster real-time detection of\nanomalous IoT data. However, IoT devices can hardly afford complex DNN models,\nand offloading anomaly detection tasks to the cloud incurs long delay. In this\npaper, we propose and build a demo for an adaptive anomaly detection approach\nfor distributed hierarchical edge computing (HEC) systems to solve this\nproblem, for both univariate and multivariate IoT data. First, we construct\nmultiple anomaly detection DNN models with increasing complexity, and associate\neach model with a layer in HEC from bottom to top. Then, we design an adaptive\nscheme to select one of these models on the fly, based on the contextual\ninformation extracted from each input data. The model selection is formulated\nas a contextual bandit problem characterized by a single-step Markov decision\nprocess, and is solved using a reinforcement learning policy network. We build\nan HEC testbed, implement our proposed approach, and evaluate it using real IoT\ndatasets. The demo shows that our proposed approach significantly reduces\ndetection delay (e.g., by 71.4% for univariate dataset) without sacrificing\naccuracy, as compared to offloading detection tasks to the cloud. We also\ncompare it with other baseline schemes and demonstrate that it achieves the\nbest accuracy-delay tradeoff. Our demo is also available online:\nhttps://rebrand.ly/91a71\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 06:13:33 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Ngo", "Mao V.", ""], ["Luo", "Tie", ""], ["Chaouchi", "Hakima", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "2004.07033", "submitter": "Newton Masinde", "authors": "Newton Masinde, Moritz Kanzler and Kalman Graffi", "title": "Caching Structures for Distributed Data Management in P2P-based Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed applications require novel solutions to tackle problems that\narise due to the scarcity of resources such as bandwidth, memory and processing\npower. One of these challenges is seen in distributed data management. The\nchallenge is the two part problem of ensuring that the content is valid when\naccessed and updating it immediately when changed. This is especially difficult\nwhen considering p2p-based distributed online social networks, which aim to\nbuild reliable, secure social networking platforms on top of often unreliable\nand unsecure devices. In this paper, we propose three selection strategies,\nrandom, trend and social score, for a social caching mechanism. They consider\nthe social interaction patterns in the social network. We implement and\nevaluate them in a DHT-based distributed online social networks called\nLibreSocial and show that the social score is the best strategy. Further we\nimplement the social caching solution and also show that when used in\ncombination with the existing caching solution almost all requests can be\nserviced via cache while retaining the consistency of data during updates.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:12:54 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Masinde", "Newton", ""], ["Kanzler", "Moritz", ""], ["Graffi", "Kalman", ""]]}, {"id": "2004.07060", "submitter": "Etienne Rivi\\`ere", "authors": "Nicolae Berendea, Hugues Mercier, Emanuel Onica, and Etienne Rivi\\`ere", "title": "Fair and Efficient Gossip in Hyperledger Fabric", "comments": "To appear in IEEE ICDCS 2020, copyright is with IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned blockchains are supported by identified but individually\nuntrustworthy nodes, collectively maintaining a replicated ledger whose content\nis trusted. The Hyperledger Fabric permissioned blockchain system targets\nhigh-throughput transaction processing. Fabric uses a set of nodes tasked with\nthe ordering of transactions using consensus. Additional peers endorse and\nvalidate transactions, and maintain a copy of the ledger. The ability to\nquickly disseminate new transaction blocks from ordering nodes to all peers is\ncritical for both performance and consistency. Broadcast is handled by a gossip\nprotocol, using randomized exchanges of blocks between peers. We show that the\ncurrent implementation of gossip in Fabric leads to heavy tail distributions of\nblock propagation latencies, impacting performance, consistency, and fairness.\nWe contribute a novel design for gossip in Fabric that simultaneously optimizes\npropagation time, tail latency and bandwidth consumption. Using a 100-node\ncluster, we show that our enhanced gossip allows the dissemination of blocks to\nall peers more than 10 times faster than with the original implementation,\nwhile decreasing the overall network bandwidth consumption by more than 40%.\nWith a high throughput and concurrent application, this results in 17% to 36%\nfewer invalidated transactions for different block sizes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:45:48 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Berendea", "Nicolae", ""], ["Mercier", "Hugues", ""], ["Onica", "Emanuel", ""], ["Rivi\u00e8re", "Etienne", ""]]}, {"id": "2004.07087", "submitter": "Lum Ramabaja", "authors": "Lum Ramabaja", "title": "The Binary Vector Clock", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Binary Vector Clock is a simple, yet space-efficient algorithm for\ngenerating a partial order of transactions in account-based blockchain systems.\nThe Binary Vector Clock solves the problem of order dependency in systems such\nas Ethereum, caused by the total order of transactions that come from the same\naddress holder. The proposed algorithm has the same security as using regular\ntransaction nonces, requires very little overhead, and can potentially result\nin a significant increase in throughput for systems like Ethereum.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 13:38:58 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 14:53:25 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Ramabaja", "Lum", ""]]}, {"id": "2004.07140", "submitter": "Abdeljalil Beniiche", "authors": "Abdeljalil Beniiche", "title": "A Study of Blockchain Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limitation with smart contracts is that they cannot access external data\nwhich might be required to control the execution of business logic. Oracles can\nbe used to provide external data to smart contracts. An oracle is an interface\nthat delivers data from external data outside the blockchain to a smart\ncontract to consume. Oracle can deliver different types of data depending on\nthe industry and requirements. In this paper, we study and describe the widely\nused blockchain oracles. Then, we elaborate on his potential role, technical\narchitecture, and design patterns. Finally, we discuss the human oracle and his\nkey role in solving the truth problem by reaching a consensus about a certain\ninquiry and tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 05:05:35 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 19:07:49 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Beniiche", "Abdeljalil", ""]]}, {"id": "2004.07203", "submitter": "Hartmut Kaiser", "authors": "Nikunj Gupta, Jackson R. Mayo, Adrian S. Lemoine, Hartmut Kaiser", "title": "Implementing Software Resiliency in HPX for Extreme Scale Computing", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "SAND2020-3975 R", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exceptions and errors occurring within mission critical applications due to\nhardware failures have a high cost. With the emerging Next Generation Platforms\n(NGPs), the rate of hardware failures will invariably increase. Therefore,\ndesigning our applications to be resilient is a critical concern in order to\nretain the reliability of results while meeting the constraints on power\nbudgets. In this paper, we implement software resilience in HPX, an\nAsynchronous Many-Task Runtime system. We implement two resiliency APIs that we\nexpose to the application developers, namely task replication and task replay.\nTask replication repeats a task n-times and executes them asynchronously. Task\nreplay will reschedule a task up to n-times until a valid output is returned.\nFurthermore, we introduce an API that allows the application to verify the\nreturned result with a user provided predicate. We test the APIs with both\nartificial workloads and a dataflow based stencil application. We demonstrate\nthat only minor overheads are incurred when utilizing these resiliency features\nfor work loads where the task size is greater than 200 $\\mu$s. We also show\nthat most of the added execution time arises from the replay or replication of\nthe tasks themselves and not by the implementation of the APIs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:02:11 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Gupta", "Nikunj", ""], ["Mayo", "Jackson R.", ""], ["Lemoine", "Adrian S.", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "2004.08003", "submitter": "Poonam Yadav Dr", "authors": "Angelo Feraudo, Poonam Yadav, Richard Mortier, Paolo Bellavista, and\n  Jon Crowcroft", "title": "SoK: Beyond IoT MUD Deployments -- Challenges and Future Directions", "comments": "7 pages, 5 figures, WIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advancement of IoT devices in both domestic and industrial\nenvironments, the need to incorporate a mechanism to build accountability in\nthe IoT ecosystem is paramount. In the last few years, various initiatives have\nbeen started in this direction addressing many socio-technical concerns and\nchallenges to build an accountable system. The solution that has received a lot\nof attention in both industry and academia is the Manufacturer Usage\nDescription (MUD) specification. It gives the possibility to the IoT device\nmanufacturers to describe communications needed by each device to work\nproperly. MUD implementation is challenging not only due to the diversity of\nIoT devices and manufacturer/operator/regulators but also due to the\nincremental integration of MUD-based flow control in the already existing\nInternet infrastructure. To provide a better understanding of these challenges,\nin this work, we explore and investigate the prototypes of three\nimplementations proposed by different research teams and organisations, useful\nfor the community to understand which are the various features implemented by\nthe existing technologies. By considering that there exist some behaviours\nwhich can be only defined by local policy, we propose a MUD capable network\nintegrating our User Policy Server(UPS). The UPS provides network\nadministrators and endusers an opportunity to interact with MUD components\nthrough a user-friendly interface. Hence, we present a comprehensive survey of\nthe challenges.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 00:02:03 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 13:21:45 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Feraudo", "Angelo", ""], ["Yadav", "Poonam", ""], ["Mortier", "Richard", ""], ["Bellavista", "Paolo", ""], ["Crowcroft", "Jon", ""]]}, {"id": "2004.08039", "submitter": "William Kuszmaul", "authors": "Michael A. Bender, Tsvi Kopelowitz, William Kuszmaul, Seth Pettie", "title": "Contention Resolution Without Collision Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the contention resolution problem on a shared\ncommunication channel that does not support collision detection. A shared\ncommunication channel is a multiple access channel, which consists of a\nsequence of synchronized time slots. Players on the channel may attempt to\nbroadcast a packet (message) in any time slot. A player's broadcast succeeds if\nno other player broadcasts during that slot. If two or more players broadcast\nin the same time slot, then the broadcasts collide and both broadcasts fail.\nThe lack of collision detection means that a player monitoring the channel\ncannot differentiate between the case of two or more players broadcasting in\nthe same slot (a collision) and zero players broadcasting. In the\ncontention-resolution problem, players arrive on the channel over time, and\neach player has one packet to transmit. The goal is to coordinate the players\nso that each player is able to successfully transmit its packet within\nreasonable time. However, the players can only communicate via the shared\nchannel by choosing to either broadcast or not. A contention-resolution\nprotocol is measured in terms of its throughput (channel utilization). Previous\nwork on contention resolution that achieved constant throughput assumed that\neither players could detect collisions, or the players' arrival pattern is\ngenerated by a memoryless (non-adversarial) process. The foundational question\nanswered by this paper is whether collision detection is a luxury or necessity\nwhen the objective is to achieve constant throughput. We show that even without\ncollision detection, one can solve contention resolution, achieving constant\nthroughput, with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 02:33:19 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 12:57:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bender", "Michael A.", ""], ["Kopelowitz", "Tsvi", ""], ["Kuszmaul", "William", ""], ["Pettie", "Seth", ""]]}, {"id": "2004.08131", "submitter": "Shreshth Tuli", "authors": "Sukhpal Singh Gill, Shreshth Tuli, Adel Nadjaran Toosi, Felix\n  Cuadrado, Peter Garraghan, Rami Bahsoon, Hanan Lutfiyya, Rizos Sakellariou,\n  Omer Rana, Schahram Dustdar and Rajkumar Buyya", "title": "ThermoSim: Deep Learning based Framework for Modeling and Simulation of\n  Thermal-aware Resource Management for Cloud Computing Environments", "comments": "Accepted in Journal of Systems and Software", "journal-ref": null, "doi": "10.1016/j.jss.2020.110596", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current cloud computing frameworks host millions of physical servers that\nutilize cloud computing resources in the form of different virtual machines\n(VM). Cloud Data Center (CDC) infrastructures require significant amounts of\nenergy to deliver large scale computational services. Computing nodes generate\nlarge volumes of heat, requiring cooling units in turn to eliminate the effect\nof this heat. Thus, the overall energy consumption of the CDC increases\ntremendously for servers as well as for cooling units. However, current\nworkload allocation policies do not take into account the effect on temperature\nand it is challenging to simulate the thermal behavior of CDCs. There is a need\nfor a thermal-aware framework to simulate and model the behavior of nodes and\nmeasure the important performance parameters which can be affected by its\ntemperature. In this paper, we propose a lightweight framework, ThermoSim, for\nmodeling and simulation of thermal-aware resource management for cloud\ncomputing environments. This work presents a Recurrent Neural Network based\ndeep learning temperature predictor for CDCs which is utilized by ThermoSim for\nlightweight resource management in constrained cloud environments. ThermoSim\nextends the CloudSim toolkit helping to analyze the performance of various key\nparameters such as energy consumption, SLA violation rate, number of VM\nmigrations and temperature during the management of cloud resources for\nexecution of workloads. Further, different energy-aware and thermal-aware\nresource management techniques are tested using the proposed ThermoSim\nframework in order to validate it against the existing framework. The\nexperimental results demonstrate the proposed framework is capable of modeling\nand simulating the thermal behavior of a CDC and the ThermoSim framework is\nbetter than Thas in terms of energy consumption, cost, time, memory usage &\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 09:14:26 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 16:33:59 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Gill", "Sukhpal Singh", ""], ["Tuli", "Shreshth", ""], ["Toosi", "Adel Nadjaran", ""], ["Cuadrado", "Felix", ""], ["Garraghan", "Peter", ""], ["Bahsoon", "Rami", ""], ["Lutfiyya", "Hanan", ""], ["Sakellariou", "Rizos", ""], ["Rana", "Omer", ""], ["Dustdar", "Schahram", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2004.08177", "submitter": "Shashikant Ilager Mr", "authors": "Shashikant Ilager, Rajeev Muralidhar, Kotagiri Rammohanrao, Rajkumar\n  Buyya", "title": "A Data-Driven Frequency Scaling Approach for Deadline-aware Energy\n  Efficient Scheduling on Graphics Processing Units (GPUs)", "comments": "In the Proceedings of the 20th IEEE/ACM International Symposium on\n  Cluster, Cloud and Internet Computing (CCGRID 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computing paradigms, such as cloud computing, are increasingly\nadopting GPUs to boost their computing capabilities primarily due to the\nheterogeneous nature of AI/ML/deep learning workloads. However, the energy\nconsumption of GPUs is a critical problem. Dynamic Voltage Frequency Scaling\n(DVFS) is a widely used technique to reduce the dynamic power of GPUs. Yet,\nconfiguring the optimal clock frequency for essential performance requirements\nis a non-trivial task due to the complex nonlinear relationship between the\napplication's runtime performance characteristics, energy, and execution time.\nIt becomes more challenging when different applications behave distinctively\nwith similar clock settings. Simple analytical solutions and standard GPU\nfrequency scaling heuristics fail to capture these intricacies and scale the\nfrequencies appropriately. In this regard, we propose a data-driven frequency\nscaling technique by predicting the power and execution time of a given\napplication over different clock settings. We collect the data from application\nprofiling and train the models to predict the outcome accurately. The proposed\nsolution is generic and can be easily extended to different kinds of workloads\nand GPU architectures. Furthermore, using this frequency scaling by prediction\nmodels, we present a deadline-aware application scheduling algorithm to reduce\nenergy consumption while simultaneously meeting their deadlines. We conduct\nreal extensive experiments on NVIDIA GPUs using several benchmark applications.\nThe experiment results have shown that our prediction models have high accuracy\nwith the average RMSE values of 0.38 and 0.05 for energy and time prediction,\nrespectively. Also, the scheduling algorithm consumes 15.07% less energy as\ncompared to the baseline policies.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 11:21:31 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 01:47:27 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ilager", "Shashikant", ""], ["Muralidhar", "Rajeev", ""], ["Rammohanrao", "Kotagiri", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2004.08200", "submitter": "Brijesh Dongol", "authors": "Eleni Bila, Simon Doherty, Brijesh Dongol, John Derrick, Gerhard\n  Schellhorn, and Heike Wehrheim", "title": "Defining and Verifying Durable Opacity: Correctness for Persistent\n  Software Transactional Memory", "comments": "This is the full version of the paper that is to appear in FORTE 2020\n  (https://www.discotec.org/2020/forte)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM), aka persistent memory, is a new paradigm for\nmemory that preserves its contents even after power loss. The expected ubiquity\nof NVM has stimulated interest in the design of novel concepts ensuring\ncorrectness of concurrent programming abstractions in the face of persistency.\nSo far, this has lead to the design of a number of persistent concurrent data\nstructures, built to satisfy an associated notion of correctness: durable\nlinearizability.\n  In this paper, we transfer the principle of durable concurrent correctness to\nthe area of software transactional memory (STM). Software transactional memory\nalgorithms allow for concurrent access to shared state. Like linearizability\nfor concurrent data structures, opacity is the established notion of\ncorrectness for STMs. First, we provide a novel definition of durable opacity\nextending opacity to handle crashes and recovery in the context of NVM. Second,\nwe develop a durably opaque version of an existing STM algorithm, namely the\nTransactional Mutex Lock (TML). Third, we design a proof technique for durable\nopacity based on refinement between TML and an operational characterisation of\ndurable opacity by adapting the TMS2 specification. Finally, we apply this\nproof technique to show that the durable version of TML is indeed durably\nopaque. The correctness proof is mechanized within Isabelle.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 12:20:36 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Bila", "Eleni", ""], ["Doherty", "Simon", ""], ["Dongol", "Brijesh", ""], ["Derrick", "John", ""], ["Schellhorn", "Gerhard", ""], ["Wehrheim", "Heike", ""]]}, {"id": "2004.08282", "submitter": "Alkida Balliu", "authors": "Alkida Balliu, Sebastian Brandt, Dennis Olivetti", "title": "Distributed Lower Bounds for Ruling Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G = (V,E)$, an $(\\alpha, \\beta)$-ruling set is a subset $S\n\\subseteq V$ such that the distance between any two vertices in $S$ is at least\n$\\alpha$, and the distance between any vertex in $V$ and the closest vertex in\n$S$ is at most $\\beta$. We present lower bounds for distributedly computing\nruling sets.\n  More precisely, for the problem of computing a $(2, \\beta)$-ruling set in the\nLOCAL model, we show the following, where $n$ denotes the number of vertices,\n$\\Delta$ the maximum degree, and $c$ is some universal constant independent of\n$n$ and $\\Delta$.\n  $\\bullet$ Any deterministic algorithm requires $\\Omega\\left(\\min \\left\\{\n\\frac{\\log \\Delta}{\\beta \\log \\log \\Delta} , \\log_\\Delta n \\right\\} \\right)$\nrounds, for all $\\beta \\le c \\cdot \\min\\left\\{ \\sqrt{\\frac{\\log \\Delta}{\\log\n\\log \\Delta}} , \\log_\\Delta n \\right\\}$. By optimizing $\\Delta$, this implies a\ndeterministic lower bound of $\\Omega\\left(\\sqrt{\\frac{\\log n}{\\beta \\log \\log\nn}}\\right)$ for all $\\beta \\le c \\sqrt[3]{\\frac{\\log n}{\\log \\log n}}$.\n  $\\bullet$ Any randomized algorithm requires $\\Omega\\left(\\min \\left\\{\n\\frac{\\log \\Delta}{\\beta \\log \\log \\Delta} , \\log_\\Delta \\log n \\right\\}\n\\right)$ rounds, for all $\\beta \\le c \\cdot \\min\\left\\{ \\sqrt{\\frac{\\log\n\\Delta}{\\log \\log \\Delta}} , \\log_\\Delta \\log n \\right\\}$. By optimizing\n$\\Delta$, this implies a randomized lower bound of\n$\\Omega\\left(\\sqrt{\\frac{\\log \\log n}{\\beta \\log \\log \\log n}}\\right)$ for all\n$\\beta \\le c \\sqrt[3]{\\frac{\\log \\log n}{\\log \\log \\log n}}$.\n  For $\\beta > 1$, this improves on the previously best lower bound of\n$\\Omega(\\log^* n)$ rounds that follows from the 30-year-old bounds of Linial\n[FOCS'87] and Naor [J.Disc.Math.'91]. For $\\beta = 1$, i.e., for the problem of\ncomputing a maximal independent set, our results improve on the previously best\nlower bound of $\\Omega(\\log^* n)$ on trees, as our bounds already hold on\ntrees.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 14:53:25 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 18:18:06 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 19:24:49 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Olivetti", "Dennis", ""]]}, {"id": "2004.08348", "submitter": "Thibault Rieutord", "authors": "Petr Kuznetsov, Thibault Rieutord and Yuan He", "title": "An Asynchronous Computability Theorem for Fair Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple topological characterization of a large class of\nfair adversarial models via affine tasks: sub-complexes of the second iteration\nof the standard chromatic subdivision. We show that the task computability of a\nmodel in the class is precisely captured by iterations of the corresponding\naffine task. Fair adversaries include, but are not restricted to, the models of\nwait-freedom, t-resilience, and $k$-concurrency. Our results generalize and\nimprove all previously derived topological characterizations of the ability of\na model to solve distributed tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:09:35 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Rieutord", "Thibault", ""], ["He", "Yuan", ""]]}, {"id": "2004.08441", "submitter": "Anwar Ghani Dr.", "authors": "Haseeb Ur Rahman, Madjid Merabti, David Llewellyn-Jones, Sud Sudirman,\n  Anwar Ghani", "title": "Structuring Communities for Sharing Human Digital Memories in a Social\n  P2P Network", "comments": "13 Pages, 14 Figures, 39 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A community is sub-network inside P2P networks that partition the network\ninto groups of similar peers to improve performance by reducing network traffic\nand high search query success rate. Large communities are common in online\nsocial networks than traditional file-sharing P2P networks because many people\ncapture huge amounts of data through their lives. This increases the number of\nhosts bearing similar data in the network and hence increases the size of\ncommunities. This article presents a Memory Thread-based Communities for our\nentity-based social P2P network that partition the network into groups of peers\nsharing data belonging to an entity - person, place, object or interest, having\nits own digital memory or be a part another memory. These connected peers\nhaving further similarities by organizing the network using linear orderings. A\nMemory-Thread is the collection of digital memories having a common reference\nkey and organized according to some form of correlation. The simulation results\nshow an increase in network performance for the proposed scheme along with a\ndecrease in network overhead and higher query success rate compared to other\nsimilar schemes. The network maintains its performance even while the network\ntraffic and size increase.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 20:25:42 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Rahman", "Haseeb Ur", ""], ["Merabti", "Madjid", ""], ["Llewellyn-Jones", "David", ""], ["Sudirman", "Sud", ""], ["Ghani", "Anwar", ""]]}, {"id": "2004.08473", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao", "title": "Fork-Resilient Cross-Blockchain Transactions through Algebraic Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-blockchain transaction (CBT) serves as a cornerstone for the\nnext-generation, blockchain-based data management systems. However,\nstate-of-the-art CBT models do not address the effect of the possible local\nfork suspension that might invalidate the entire CBT. This paper takes an\nalgebraic-topological approach to abstract the blockchains and their\ntransactions into simplicial complexes and shows that CBTs cannot complete in\neither a \\textit{committed} or an \\textit{aborted} status by a $t$-resilient\nmessage-passing protocol. This result implies that a more sophisticated model\nis in need to support CBTs and, thus, sheds light on the future blockchain\ndesigns.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 22:30:52 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 19:47:35 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhao", "Dongfang", ""]]}, {"id": "2004.08488", "submitter": "Su Wang", "authors": "Su Wang, Yichen Ruan, Yuwei Tu, Satyavrat Wagle, Christopher G.\n  Brinton, Carlee Joe-Wong", "title": "Network-Aware Optimization of Distributed Learning for Fog Computing", "comments": "Accepted for publication in IEEE/ACM Transactions on Networking (16\n  pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing promises to enable machine learning tasks to scale to large\namounts of data by distributing processing across connected devices. Two key\nchallenges to achieving this goal are heterogeneity in devices compute\nresources and topology constraints on which devices can communicate with each\nother. We address these challenges by developing the first network-aware\ndistributed learning optimization methodology where devices optimally share\nlocal data processing and send their learnt parameters to a server for\naggregation at certain time intervals. Unlike traditional federated learning\nframeworks, our method enables devices to offload their data processing tasks\nto each other, with these decisions determined through a convex data transfer\noptimization problem that trades off costs associated with devices processing,\noffloading, and discarding data points. We analytically characterize the\noptimal data transfer solution for different fog network topologies, showing\nfor example that the value of offloading is approximately linear in the range\nof computing costs in the network. Our subsequent experiments on testbed\ndatasets we collect confirm that our algorithms are able to improve network\nresource utilization substantially without sacrificing the accuracy of the\nlearned model. In these experiments, we also study the effect of network\ndynamics, quantifying the impact of nodes entering or exiting the network on\nmodel learning and resource costs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 23:34:31 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 20:49:14 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 01:04:06 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 00:33:45 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wang", "Su", ""], ["Ruan", "Yichen", ""], ["Tu", "Yuwei", ""], ["Wagle", "Satyavrat", ""], ["Brinton", "Christopher G.", ""], ["Joe-Wong", "Carlee", ""]]}, {"id": "2004.08532", "submitter": "Da Zheng", "authors": "Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao\n  Xiong, Zheng Zhang, George Karypis", "title": "DGL-KE: Training Knowledge Graph Embeddings at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have emerged as a key abstraction for organizing information\nin diverse domains and their embeddings are increasingly used to harness their\ninformation in various information retrieval and machine learning tasks.\nHowever, the ever growing size of knowledge graphs requires computationally\nefficient algorithms capable of scaling to graphs with millions of nodes and\nbillions of edges. This paper presents DGL-KE, an open-source package to\nefficiently compute knowledge graph embeddings. DGL-KE introduces various novel\noptimizations that accelerate training on knowledge graphs with millions of\nnodes and billions of edges using multi-processing, multi-GPU, and distributed\nparallelism. These optimizations are designed to increase data locality, reduce\ncommunication overhead, overlap computations with memory accesses, and achieve\nhigh operation efficiency. Experiments on knowledge graphs consisting of over\n86M nodes and 338M edges show that DGL-KE can compute embeddings in 100 minutes\non an EC2 instance with 8 GPUs and 30 minutes on an EC2 cluster with 4 machines\nwith 48 cores/machine. These results represent a 2x~5x speedup over the best\ncompeting approaches. DGL-KE is available on https://github.com/awslabs/dgl-ke.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 05:50:52 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zheng", "Da", ""], ["Song", "Xiang", ""], ["Ma", "Chao", ""], ["Tan", "Zeyuan", ""], ["Ye", "Zihao", ""], ["Dong", "Jin", ""], ["Xiong", "Hao", ""], ["Zhang", "Zheng", ""], ["Karypis", "George", ""]]}, {"id": "2004.08546", "submitter": "Chaoyang He", "authors": "Chaoyang He, Murali Annavaram, Salman Avestimehr", "title": "Towards Non-I.I.D. and Invisible Data with FedNAS: Federated Deep\n  Learning via Neural Architecture Search", "comments": "accepted to CVPR 2020 workshop on neural architecture search and\n  beyond for representation learning. Code is released at https://fedml.ai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.MA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) has been proved to be an effective learning framework\nwhen data cannot be centralized due to privacy, communication costs, and\nregulatory restrictions. When training deep learning models under an FL\nsetting, people employ the predefined model architecture discovered in the\ncentralized environment. However, this predefined architecture may not be the\noptimal choice because it may not fit data with non-identical and independent\ndistribution (non-IID). Thus, we advocate automating federated learning\n(AutoFL) to improve model accuracy and reduce the manual design effort. We\nspecifically study AutoFL via Neural Architecture Search (NAS), which can\nautomate the design process. We propose a Federated NAS (FedNAS) algorithm to\nhelp scattered workers collaboratively searching for a better architecture with\nhigher accuracy. We also build a system based on FedNAS. Our experiments on\nnon-IID dataset show that the architecture searched by FedNAS can outperform\nthe manually predefined architecture.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:04:44 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 23:59:20 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 18:47:25 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 02:18:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["He", "Chaoyang", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2004.08548", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Proposal of Automatic FPGA Offloading for Applications Loop Statements", "comments": "13 pages, 4 figures, The 7th Annual Conference on Engineering and\n  Information Technology (ACEAIT 2020), pp.111-123, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the prediction of Moore's law slowing down, utilization\nof hardware other than CPU such as FPGA which is energy effective is\nincreasing. However, when using heterogeneous hardware other than CPUs,\nbarriers of technical skills such as OpenCL are high. Based on that, I have\nproposed environment adaptive software that enables automatic conversion,\nconfiguration, and high-performance operation of once written code, according\nto the hardware to be placed. Partly of the offloading to the GPU was automated\npreviously. In this paper, I propose and evaluate an automatic extraction\nmethod of appropriate offload target loop statements of source code as the\nfirst step of offloading to FPGA. I evaluate the effectiveness of the proposed\nmethod using existing applications.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:12:14 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2004.08680", "submitter": "Srinivasa Phani Chitti", "authors": "Phani Chitti", "title": "Data Model, Collection and Evaluation Framework for Local Energy Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed ledgers are a new type of database technology that allows open\naccess to data stored across distributed, decentralised, publicly maintained\ninfrastructures. Current implementations of the such ledgers expect competition\nbetween participants, are often energy hungry, poor in maintaining the natural\nstructure of data and suffer from scalability constraints. The aim of my\nresearch work is to develop a distributed ledger-based middleware for data\nmodelling and collection on household energy generation and use, while\naddressing scalability and energy inefficiency concerns of the ledger for this\nparticular application domain. The energy data collected and made available\nthrough this middleware will be used for digital energy service delivery (e.g.,\nautomated peer to peer energy trading, topological estimations, etc.). The\nmiddleware also provides a platform for a consumer focused digital energy\nservice delivery, as well as service model evaluation. The model evaluation\nwill enable the prospective service users to evaluate the suitability of the\ngiven service for their needs before making a decision of service subscription.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 18:14:25 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Chitti", "Phani", ""]]}, {"id": "2004.08698", "submitter": "Anwar Ghani Dr.", "authors": "Ghani ur Rehman, Anwar Ghani, Muhammad Zubair, Shahbaz Ahmad Khan\n  Ghayyure, Shad Muhammad", "title": "Honesty Based Democratic Scheme to Improve Community Cooperation for IoT\n  Based Vehicular Delay Tolerant Networks", "comments": "19 Pages, 5 Figures, 6 Tables, 55 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many Internet of things (IoT) applications have been developed and\nimplemented on unreliable wireless networks like the Delay tolerant network\n(DTN), however, efficient data transfer in DTN is still an important issue for\nthe IoT applications. One of the application areas of DTN is Vehicular Delay\nTolerant Network (VDTN) where the network faces communication disruption due to\nlack of end-to-end relay route. It is challenging as some of the nodes show\nselfish behavior to preserve their resources like memory, and energy level and\nbecome non-cooperative. In this article, an Honesty based Democratic Scheme\n(HBDS) is introduced where vehicles with higher honesty level are elected as\nheads -- during the process. Vehicles involved in the process would maximize\ntheir rewards (reputation) through active participation in the network\nactivities whereas nodes with non-cooperative selfish behavior are punished.\nThe honesty level of the heads is analyzed using Vickrey, Clarke, and Groves\n(VCG) model. The mathematical model and algorithms developed in the proposed\nHBDS technique are simulated using the VDTNSim framework to evaluate their\nefficiency. The performance results show that the proposed scheme dominates\ncurrent schemes in terms of packet delivery probability, packet delivery delay,\nnumber of packets drop, and overhead ratio.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 20:10:40 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Rehman", "Ghani ur", ""], ["Ghani", "Anwar", ""], ["Zubair", "Muhammad", ""], ["Ghayyure", "Shahbaz Ahmad Khan", ""], ["Muhammad", "Shad", ""]]}, {"id": "2004.08771", "submitter": "Florin Rusu", "authors": "Yujing Ma and Florin Rusu", "title": "Heterogeneous CPU+GPU Stochastic Gradient Descent Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely-adopted practice is to train deep learning models with specialized\nhardware accelerators, e.g., GPUs or TPUs, due to their superior performance on\nlinear algebra operations. However, this strategy does not employ effectively\nthe extensive CPU and memory resources -- which are used only for\npreprocessing, data transfer, and scheduling -- available by default on the\naccelerated servers. In this paper, we study training algorithms for deep\nlearning on heterogeneous CPU+GPU architectures. Our two-fold objective --\nmaximize convergence rate and resource utilization simultaneously -- makes the\nproblem challenging. In order to allow for a principled exploration of the\ndesign space, we first introduce a generic deep learning framework that\nexploits the difference in computational power and memory hierarchy between CPU\nand GPU through asynchronous message passing. Based on insights gained through\nexperimentation with the framework, we design two heterogeneous asynchronous\nstochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU\nHogbatch -- combines small batches on CPU with large batches on GPU in order to\nmaximize the utilization of both resources. However, this generates an\nunbalanced model update distribution which hinders the statistical convergence.\nThe second algorithm -- Adaptive Hogbatch -- assigns batches with continuously\nevolving size based on the relative speed of CPU and GPU. This balances the\nmodel updates ratio at the expense of a customizable decrease in utilization.\nWe show that the implementation of these algorithms in the proposed CPU+GPU\nframework achieves both faster convergence and higher resource utilization than\nTensorFlow on several real datasets and on two computing architectures -- an\non-premises server and a cloud instance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 05:21:20 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ma", "Yujing", ""], ["Rusu", "Florin", ""]]}, {"id": "2004.08776", "submitter": "Gerui Wang", "authors": "Gerui Wang, Shuo Wang, Vivek Bagaria, David Tse, and Pramod Viswanath", "title": "Prism Removes Consensus Bottleneck for Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of existing permissionless smart contract platforms such as\nEthereum is limited by the consensus layer. Prism is a new proof-of-work\nconsensus protocol that provably achieves throughput and latency up to physical\nlimits while retaining the strong guarantees of the longest chain protocol.\nThis paper reports experimental results from implementations of two smart\ncontract virtual machines, EVM and MoveVM, on top of Prism and demonstrates\nthat the consensus bottleneck has been removed. Code can be found at\nhttps://github.com/wgr523/prism-smart-contracts.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 06:13:34 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 03:50:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wang", "Gerui", ""], ["Wang", "Shuo", ""], ["Bagaria", "Vivek", ""], ["Tse", "David", ""], ["Viswanath", "Pramod", ""]]}, {"id": "2004.08893", "submitter": "Andreas Mang", "authors": "Malte Brunn and Naveen Himthani and George Biros and Miriam Mehl and\n  Andreas Mang", "title": "Fast GPU 3D Diffeomorphic Image Registration", "comments": "20 pages, 6 figures, 8 tables", "journal-ref": "Journal of Parallel and Distributed Computing 149:149-162, 2021", "doi": "10.1016/j.jpdc.2020.11.006", "report-no": null, "categories": "cs.DC eess.IV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D image registration is one of the most fundamental and computationally\nexpensive operations in medical image analysis. Here, we present a\nmixed-precision, Gauss--Newton--Krylov solver for diffeomorphic registration of\ntwo images. Our work extends the publicly available CLAIRE library to GPU\narchitectures. Despite the importance of image registration, only a few\nimplementations of large deformation diffeomorphic registration packages\nsupport GPUs. Our contributions are new algorithms to significantly reduce the\nrun time of the two main computational kernels in CLAIRE: calculation of\nderivatives and scattered-data interpolation. We deploy (i) highly-optimized,\nmixed-precision GPU-kernels for the evaluation of scattered-data interpolation,\n(ii) replace Fast-Fourier-Transform (FFT)-based first-order derivatives with\noptimized 8th-order finite differences, and (iii) compare with state-of-the-art\nCPU and GPU implementations. As a highlight, we demonstrate that we can\nregister $256^3$ clinical images in less than 6 seconds on a single NVIDIA\nTesla V100. This amounts to over 20$\\times$ speed-up over the current version\nof CLAIRE and over 30$\\times$ speed-up over existing GPU implementations.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 16:08:14 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Brunn", "Malte", ""], ["Himthani", "Naveen", ""], ["Biros", "George", ""], ["Mehl", "Miriam", ""], ["Mang", "Andreas", ""]]}, {"id": "2004.08948", "submitter": "Anwar Ghani Dr.", "authors": "Ghani ur Rehman, Anwar Ghani, Muhammad Zubair, Muhammad Imran Saeed,\n  Dhananjay Singh (Senior Member, IEEE)", "title": "SOS: Socially Omitting Selfishness in IoT for Smart and Connected\n  Communities", "comments": "25 Pages, 7 Figures, 7 Tables, 55 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart and Connected Communities (SCC) is an emerging field of Internet of\nThings (IoT), and it is having potential applications to improve human life.\nThe improvement may be in terms of preservation, revitalization, livability,\nand sustainability of a community. The resources of the nodes and devices in\nthe SCC have certain constraints that may not allow the devices and nodes to\ncooperate to save their resources such as memory, energy, and buffer, or simply\nmaximize their performance. Thus, to stimulate the nodes to avoid selfish\nbehavior, SSC needs a novel and well-organized solution to motivate nodes for\ncooperation. This article aims to resolve the issue of selfish behaviors in SCC\nand to encourage the nodes for cooperation. A novel mechanism Socially Omitting\nSelfishness (SOS) has been proposed to manage/eradicate selfishness using a\nsocially-oriented election process. The election process elects different heads\nbased on weight and cooperation (using VCG model). The election of heads and\nincentive mechanism encourages the nodes to show participation and behave as\nhighly cooperative members of the community. Furthermore, an extended version\nof the Dempster-Shafer model has been used to discourage the selfish behavior\nof the participating nodes in the SOS scheme. It uses different monitoring and\ngateway nodes to efficiently employ the proposed scheme. A mathematical model\nhas been developed for the aforementioned aspects and simulated through the NS2\nsimulation environment to analyze the performance of SOS. The results of the\nproposed scheme outperform the contemporary schemes in terms of average\ndelivery delay, packet delivery ratio, throughput, and average energy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 20:01:23 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Rehman", "Ghani ur", "", "Senior Member, IEEE"], ["Ghani", "Anwar", "", "Senior Member, IEEE"], ["Zubair", "Muhammad", "", "Senior Member, IEEE"], ["Saeed", "Muhammad Imran", "", "Senior Member, IEEE"], ["Singh", "Dhananjay", "", "Senior Member, IEEE"]]}, {"id": "2004.08991", "submitter": "James Flamino", "authors": "James Flamino, Christopher Abriola, Ben Zimmerman, Zhongheng Li, Joel\n  Douglas", "title": "Robust and Scalable Entity Alignment in Big Data", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Entity alignment has always had significant uses within a multitude of\ndiverse scientific fields. In particular, the concept of matching entities\nacross networks has grown in significance in the world of social science as\ncommunicative networks such as social media have expanded in scale and\npopularity. With the advent of big data, there is a growing need to provide\nanalysis on graphs of massive scale. However, with millions of nodes and\nbillions of edges, the idea of alignment between a myriad of graphs of similar\nscale using features extracted from potentially sparse or incomplete datasets\nbecomes daunting. In this paper we will propose a solution to the issue of\nlarge-scale alignments in the form of a multi-step pipeline. Within this\npipeline we introduce scalable feature extraction for robust temporal\nattributes, accompanied by novel and efficient clustering algorithms in order\nto find groupings of similar nodes across graphs. The features and their\nclusters are fed into a versatile alignment stage that accurately identifies\npartner nodes among millions of possible matches. Our results show that the\npipeline can process large data sets, achieving efficient runtimes within the\nmemory constraints.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 23:41:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Flamino", "James", ""], ["Abriola", "Christopher", ""], ["Zimmerman", "Ben", ""], ["Li", "Zhongheng", ""], ["Douglas", "Joel", ""]]}, {"id": "2004.09054", "submitter": "Dimitris Sakavalas", "authors": "Dimitris Sakavalas, Lewis Tseng, Nitin H. Vaidya", "title": "Asynchronous Byzantine Approximate Consensus in Directed Networks", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the approximate consensus problem in asynchronous\nmessage-passing networks where some nodes may become Byzantine faulty. We\nanswer an open problem raised by Tseng and Vaidya, 2012, proposing the first\nalgorithm of optimal resilience for directed networks. Interestingly, our\nresults show that the tight condition on the underlying communication networks\nfor asynchronous Byzantine approximate consensus coincides with the tight\ncondition for synchronous Byzantine exact consensus. Our results can be viewed\nas a non-trivial generalization of the algorithm by Abraham et al., 2004, which\napplies to the special case of complete networks. The tight condition and\ntechniques identified in the paper shed light on the fundamental properties for\nsolving approximate consensus in asynchronous directed networks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 05:06:31 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sakavalas", "Dimitris", ""], ["Tseng", "Lewis", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2004.09129", "submitter": "Sagnik Mukhopadhyay", "authors": "Michal Dory, Yuval Efron, Sagnik Mukhopadhyay, Danupon Nanongkai", "title": "Distributed Weighted Min-Cut in Nearly-Optimal Time", "comments": "Major changes: (i) The fragment decomposition technique is\n  simplified, (ii) Introduction and technical overview have been redone, and\n  (iii) The technical sections have been made simpler for better readability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum-weight cut (min-cut) is a basic measure of a network's connectivity\nstrength. While the min-cut can be computed efficiently in the sequential\nsetting [Karger STOC'96], there was no efficient way for a distributed network\nto compute its own min-cut without limiting the input structure or dropping the\noutput quality: In the standard CONGEST model, existing algorithms with\nnearly-optimal time (e.g. [Ghaffari, Kuhn, DISC'13; Nanongkai, Su, DISC'14])\ncan guarantee a solution that is $(1+\\epsilon)$-approximation at best while the\nexact $\\tilde O(n^{0.8}D^{0.2} + n^{0.9})$-time algorithm [Ghaffari, Nowicki,\nThorup, SODA'20] works only on *simple* networks (no weights and no parallel\nedges). Here $n$ and $D$ denote the network's number of vertices and\nhop-diameter, respectively. For the weighted case, the best bound was $\\tilde\nO(n)$ [Daga, Henzinger, Nanongkai, Saranurak, STOC'19].\n  In this paper, we provide an *exact* $\\tilde O(\\sqrt n + D)$-time algorithm\nfor computing min-cut on *weighted* networks. Our result improves even the\nprevious algorithm that works only on simple networks. Its time complexity\nmatches the known lower bound up to polylogarithmic factors. At the heart of\nour algorithm are a clever routing trick and two structural lemmas regarding\nthe structure of a minimum cut of a graph. These two structural lemmas\nconsiderably strengthen and generalize the framework of Mukhopadhyay-Nanongkai\n[STOC'20] and can be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 08:44:11 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 17:23:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dory", "Michal", ""], ["Efron", "Yuval", ""], ["Mukhopadhyay", "Sagnik", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "2004.09180", "submitter": "Evangelos Pournaras", "authors": "Thomas Asikis, Johannes Klinglmayr, Dirk Helbing, Evangelos Pournaras", "title": "How Value-Sensitive Design Can Empower Sustainable Consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a so-called overpopulated world, sustainable consumption is of existential\nimportance.However, the expanding spectrum of product choices and their\nproduction complexity challenge consumers to make informed and value-sensitive\ndecisions. Recent approaches based on (personalized) psychological manipulation\nare often intransparent, potentially privacy-invasive and inconsistent with\n(informational) self-determination. In contrast, responsible consumption based\non informed choices currently requires reasoning to an extent that tends to\noverwhelm human cognitive capacity. As a result, a collective shift towards\nsustainable consumption remains a grand challenge. Here we demonstrate a novel\npersonal shopping assistant implemented as a smart phone app that supports a\nvalue-sensitive design and leverages sustainability awareness, using experts'\nknowledge and \"wisdom of the crowd\" for transparent product information and\nexplainable product ratings. Real-world field experiments in two supermarkets\nconfirm higher sustainability awareness and a bottom-up behavioral shift\ntowards more sustainable consumption. These results encourage novel business\nmodels for retailers and producers, ethically aligned with consumer preferences\nand with higher sustainability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:11:20 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 07:16:28 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 07:43:13 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 14:56:03 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Asikis", "Thomas", ""], ["Klinglmayr", "Johannes", ""], ["Helbing", "Dirk", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "2004.09312", "submitter": "Peter Hillmann", "authors": "Peter Hillmann, Tobias Uhlig, Gabi Dreo Rodosek, Oliver Rose", "title": "A Novel Multi-Agent System for Complex Scheduling Problems", "comments": null, "journal-ref": "Winter Simulation Conference 2014", "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex scheduling problems require a large amount computation power and\ninnovative solution methods. The objective of this paper is the conception and\nimplementation of a multi-agent system that is applicable in various problem\ndomains. Independent specialized agents handle small tasks, to reach a\nsuperordinate target. Effective coordination is therefore required to achieve\nproductive cooperation. Role models and distributed artificial intelligence are\nemployed to tackle the resulting challenges. We simulate a NP-hard scheduling\nproblem to demonstrate the validity of our approach. In addition to the general\nagent based framework we propose new simulation-based optimization heuristics\nto given scheduling problems. Two of the described optimization algorithms are\nimplemented using agents. This paper highlights the advantages of the\nagent-based approach, like the reduction in layout complexity, improved control\nof complicated systems, and extendability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:04:58 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hillmann", "Peter", ""], ["Uhlig", "Tobias", ""], ["Rodosek", "Gabi Dreo", ""], ["Rose", "Oliver", ""]]}, {"id": "2004.09327", "submitter": "Peter Hillmann", "authors": "Peter Hillmann, Frank Tietze, Gabi Dreo Rodosek", "title": "Tracemax: A Novel Single Packet IP Traceback Strategy for Data-Flow\n  Analysis", "comments": "Keywords: Computer network management, IP networks, IP packet,\n  Traceback, Packet trace, Denial of Service", "journal-ref": "Local Computer Networks 2015", "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of the exact path that packets are routed on in the\nnetwork is quite a challenge. This paper presents a novel, efficient traceback\nstrategy named Tracemax in context of a defense system against distributed\ndenial of service (DDoS) attacks. A single packet can be directly traced over\nmany more hops than the current existing techniques allow. In combination with\na defense system it differentiates between multiple connections. It aims to\nletting non-malicious connections pass while bad ones get thwarted. The novel\nconcept allows detailed analyses of the traffic and the transmission path\nthrough the network. The strategy can effectively reduce the effect of common\nbandwidth and resource consumption attacks, foster early warning and prevention\nas well as higher the availability of the network services for the wanted\ncustomers.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:18:39 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hillmann", "Peter", ""], ["Tietze", "Frank", ""], ["Rodosek", "Gabi Dreo", ""]]}, {"id": "2004.09362", "submitter": "Dmitry Kolmakov", "authors": "Dmitry Kolmakov, Xuecang Zhang", "title": "A Generalization of the Allreduce Operation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allreduce is one of the most frequently used MPI collective operations, and\nthus its performance attracts much attention in the past decades. Many\nalgorithms were developed with different properties and purposes. We present a\nnovel approach to communication description based on the permutations inspired\nby the mathematics of a Rubik's cube where the moves form a mathematical\nstructure called group. Similarly, cyclic communication patterns between a set\nof $P$ processes may be described by a permutation group. This new approach\nallows constructing a generalization of the widely used Allreduce algorithms\nsuch as Ring, Recursive Doubling and Recursive Halving. Using the developed\napproach we build an algorithm that successfully solves the well-known problem\nof the non-power-of-two number of processes which breaks down the performance\nof many existing algorithms. The proposed algorithm provides a general solution\nfor any number of processes with the dynamically changing amount of\ncommunication steps between $\\lceil \\log{P} \\rceil$ for the latency-optimal\nversion and $2 \\cdot \\lceil \\log{P} \\rceil$ for the bandwidth-optimal case.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:08:56 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 10:58:08 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Kolmakov", "Dmitry", ""], ["Zhang", "Xuecang", ""]]}, {"id": "2004.09454", "submitter": "Yuan Zhou", "authors": "Nikolai Karpov, Qin Zhang, Yuan Zhou", "title": "Collaborative Top Distribution Identifications with Limited Interaction", "comments": "Accepted for presentation at FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem in this paper: given a set of $n$\ndistributions, find the top-$m$ ones with the largest means. This problem is\nalso called {\\em top-$m$ arm identifications} in the literature of\nreinforcement learning, and has numerous applications. We study the problem in\nthe collaborative learning model where we have multiple agents who can draw\nsamples from the $n$ distributions in parallel. Our goal is to characterize the\ntradeoffs between the running time of learning process and the number of rounds\nof interaction between agents, which is very expensive in various scenarios. We\ngive optimal time-round tradeoffs, as well as demonstrate complexity\nseparations between top-$1$ arm identification and top-$m$ arm identifications\nfor general $m$ and between fixed-time and fixed-confidence variants. As a\nbyproduct, we also give an algorithm for selecting the distribution with the\n$m$-th largest mean in the collaborative learning model.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:11:20 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 02:31:05 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Karpov", "Nikolai", ""], ["Zhang", "Qin", ""], ["Zhou", "Yuan", ""]]}, {"id": "2004.09492", "submitter": "Igor Sfiligoi", "authors": "I. Sfiligoi, D. Schultz, B. Riedel, F. Wuerthwein, S. Barnet and V.\n  Brik", "title": "Demonstrating a Pre-Exascale, Cost-Effective Multi-Cloud Environment for\n  Scientific Computing", "comments": "5 pages, 7 figures, to be published in proceedings of PEARC'20. arXiv\n  admin note: text overlap with arXiv:2002.06667", "journal-ref": null, "doi": "10.1145/3311790.3396625", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scientific computing needs are growing dramatically with time and are\nexpanding in science domains that were previously not compute intensive. When\ncompute workflows spike well in excess of the capacity of their local compute\nresource, capacity should be temporarily provisioned from somewhere else to\nboth meet deadlines and to increase scientific output. Public Clouds have\nbecome an attractive option due to their ability to be provisioned with minimal\nadvance notice. The available capacity of cost-effective instances is not well\nunderstood. This paper presents expanding the IceCube's production HTCondor\npool using cost-effective GPU instances in preemptible mode gathered from the\nthree major Cloud providers, namely Amazon Web Services, Microsoft Azure and\nthe Google Cloud Platform. Using this setup, we sustained for a whole workday\nabout 15k GPUs, corresponding to around 170 PFLOP32s, integrating over one\nEFLOP32 hour worth of science output for a price tag of about $60k. In this\npaper, we provide the reasoning behind Cloud instance selection, a description\nof the setup and an analysis of the provisioned resources, as well as a short\ndescription of the actual science output of the exercise.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 05:06:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Sfiligoi", "I.", ""], ["Schultz", "D.", ""], ["Riedel", "B.", ""], ["Wuerthwein", "F.", ""], ["Barnet", "S.", ""], ["Brik", "V.", ""]]}, {"id": "2004.09547", "submitter": "Tyler Crain", "authors": "Tyler Crain", "title": "Experimental Evaluation of Asynchronous Binary Byzantine Consensus\n  Algorithms with $t < n/3$ and $O(n^2)$ Messages and $O(1)$ Round Expected\n  Termination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work performs an experimental evaluation of four asynchronous binary\nByzantine consensus algorithms [11,16,18] in various configurations. In\naddition to being asynchronous these algorithms run in rounds, tolerate up to\none third of faulty nodes, use $O(n^2)$ messages, and use randomized common\ncoins to terminate in an expected constant number of rounds. Each of the four\nalgorithms have different requirements for the random coin, for the number of\nmessages needed per round, whether or not cryptographic signatures are needed,\namong other details. Two different non-interactive threshold common coin\nimplementations are tested, one using threshold signatures, and one based on\nthe Diffe-Hellman problem using validity proofs [11].\n  Experiments are run in single data center and geo-distributed configurations\nwith between $4$ and $48$ nodes. Various simple faulty behaviors are tested. As\nno algorithm performs best in all experimental conditions, two new algorithms\nintroduced that simply combine properties of the existing algorithms with the\ngoal of having good performance in the majority of experimental settings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 18:07:47 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 12:54:30 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Crain", "Tyler", ""]]}, {"id": "2004.09574", "submitter": "Xingyu Zhou", "authors": "Xingyu Zhou and Ness Shroff", "title": "A Note on Load Balancing in Many-Server Heavy-Traffic Regime", "comments": "arXiv admin note: text overlap with arXiv:2003.06454", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, we apply Stein's method to analyze the performance of general\nload balancing schemes in the many-server heavy-traffic regime. In particular,\nconsider a load balancing system of $N$ servers and the distance of arrival\nrate to the capacity region is given by $N^{1-\\alpha}$ with $\\alpha > 1$. We\nare interested in the performance as $N$ goes to infinity under a large class\nof policies. We establish different asymptotics under different scalings and\nconditions. Specifically, (i) If the second moments linearly increase with $N$\nwith coefficients $\\sigma_a^2$ and $\\nu_s^2$, then for any $\\alpha > 4$, the\ndistribution of the sum queue length scaled by $N^{-\\alpha}$ converges to an\nexponential random variable with mean $\\frac{\\sigma_a^2 + \\nu_s^2}{2}$. (3) If\nthe second moments quadratically increase with $N$ with coefficients\n$\\tilde{\\sigma}_a^2$ and $\\tilde{\\nu}_s^2$, then for any $\\alpha > 3$, the\ndistribution of the sum queue length scaled by $N^{-\\alpha-1}$ converges to an\nexponential random variable with mean $\\frac{\\tilde{\\sigma}_a^2 +\n\\tilde{\\nu}_s^2}{2}$. Both results are simple applications of our previously\ndeveloped framework of Stein's method for heavy-traffic analysis in\n\\cite{zhou2020note}.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 19:03:34 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 01:29:44 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 03:45:08 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhou", "Xingyu", ""], ["Shroff", "Ness", ""]]}, {"id": "2004.09883", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Proposal of Automatic Offloading for Function Blocks of Applications", "comments": "8 pages, 5 figures, The 8th IIAE International Conference on\n  Industrial Application Engineering 2020 (ICIAE 2020), pp.4-11, Mar. 2020", "journal-ref": null, "doi": null, "report-no": "The 8th IIAE International Conference on Industrial Application\n  Engineering 2020 (ICIAE 2020), pp.4-11, Mar. 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using heterogeneous hardware other than CPUs, barriers of technical\nskills such as OpenCL are high. Based on that, I have proposed environment\nadaptive software that enables automatic conversion, configuration, and\nhigh-performance operation of once written code, according to the hardware to\nbe placed. Partly of the offloading to the GPU was automated previously. In\nthis paper, I propose and evaluate an automatic extraction method of\nappropriate offload target loop statements of source code as the first step of\noffloading to FPGA. I evaluate the effectiveness of the proposed method in\nmultiple applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 10:23:26 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2004.09910", "submitter": "Chiheon Kim", "authors": "Chiheon Kim, Heungsub Lee, Myungryong Jeong, Woonhyuk Baek, Boogeon\n  Yoon, Ildoo Kim, Sungbin Lim, Sungwoong Kim", "title": "torchgpipe: On-the-fly Pipeline Parallelism for Training Giant Models", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and implement a ready-to-use library in PyTorch for performing\nmicro-batch pipeline parallelism with checkpointing proposed by GPipe (Huang et\nal., 2019). In particular, we develop a set of design components to enable\npipeline-parallel gradient computation in PyTorch's define-by-run and eager\nexecution environment. We show that each component is necessary to fully\nbenefit from pipeline parallelism in such environment, and demonstrate the\nefficiency of the library by applying it to various network architectures\nincluding AmoebaNet-D and U-Net. Our library is available at\nhttps://github.com/kakaobrain/torchgpipe .\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 11:27:00 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Kim", "Chiheon", ""], ["Lee", "Heungsub", ""], ["Jeong", "Myungryong", ""], ["Baek", "Woonhyuk", ""], ["Yoon", "Boogeon", ""], ["Kim", "Ildoo", ""], ["Lim", "Sungbin", ""], ["Kim", "Sungwoong", ""]]}, {"id": "2004.10033", "submitter": "Alessandro Pellegrini", "authors": "Alessandro Pellegrini, Francesco Quaglia", "title": "On the Relevance of Wait-free Coordination Algorithms in Shared-Memory\n  HPC:The Global Virtual Time Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-performance computing on shared-memory/multi-core architectures could\nsuffer from non-negligible performance bottlenecks due to coordination\nalgorithms, which are nevertheless necessary to ensure the overall correctness\nand/or to support the execution of housekeeping operations, e.g. to recover\ncomputing resources (e.g., memory). Although more complex in\ndesign/development, a paradigm switch from classical coordination algorithms to\nwait-free ones could significantly boost the performance of HPC applications.\n  In this paper we explore the relevance of this paradigm shift in\nshared-memory architectures, by focusing on the context of Parallel Discrete\nEvent Simulation, where the Global Virtual Time (GVT) represents a fundamental\ncoordination algorithm. It allows to compute the lower bound on the value of\nthe logical time passed through by all the entities participating in a\nparallel/distributed computation. Hence it can be used to discriminate what\nevents belong to the past history of the computation---thus being considered as\ncommitted---and allowing for memory recovery (e.g. of obsolete logs that were\ntaken in order to support state recoverability) and non-revokable operations\n(e.g. I/O).\n  We compare the reference (blocking) algorithm for shared memory, the one\nproposed by by Fujimoto and Hybinette \\cite{Fuj97}, with an innovative\nwait-free implementation, emphasizing on what design choices must be made to\nenforce this paradigm shift, and what are the performance implications of\nremoving critical sections in coordination algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:11:59 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Pellegrini", "Alessandro", ""], ["Quaglia", "Francesco", ""]]}, {"id": "2004.10077", "submitter": "Laurens Versluis", "authors": "Laurens Versluis, Alexandru Iosup", "title": "A Survey and Annotated Bibliography of Workflow Scheduling in Computing\n  Infrastructures: Community, Keyword, and Article Reviews -- Extended\n  Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workflows are prevalent in today's computing infrastructures. The workflow\nmodel support various different domains, from machine learning to finance and\nfrom astronomy to chemistry. Different Quality-of-Service (QoS) requirements\nand other desires of both users and providers makes workflow scheduling a tough\nproblem, especially since resource providers need to be as efficient as\npossible with their resources to be competitive. To a newcomer or even an\nexperienced researcher, sifting through the vast amount of articles can be a\ndaunting task. Questions regarding the difference techniques, policies,\nemerging areas, and opportunities arise. Surveys are an excellent way to cover\nthese questions, yet surveys rarely publish their tools and data on which it is\nbased. Moreover, the communities that are behind these articles are rarely\nstudied. We attempt to address these shortcomings in this work. We focus on\nfour areas within workflow scheduling: 1) the workflow formalism, 2) workflow\nallocation, 3) resource provisioning, and 4) applications and services. Each\npart features one or more taxonomies, a view of the community, important and\nemerging keywords, and directions for future work. We introduce and make\nopen-source an instrument we used to combine and store article meta-data. Using\nthis meta-data, we 1) obtain important keywords overall and per year, per\ncommunity, 2) identify keywords growing in importance, 3) get insight into the\nstructure and relations within each community, and 4) perform a systematic\nliterature survey per part to validate and complement our taxonomies.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:04:11 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Versluis", "Laurens", ""], ["Iosup", "Alexandru", ""]]}, {"id": "2004.10275", "submitter": "Michael Chang", "authors": "Michael Alan Chang, Domenic Bottini, Lisa Jian, Pranay Kumar, Aurojit\n  Panda, Scott Shenker", "title": "How to Train your DNN: The Network Operator Edition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Nets have hit quite a crest, But physical networks are where they\nmust rest, And here we put them all to the test, To see which network\noptimization is best.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:14:46 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Chang", "Michael Alan", ""], ["Bottini", "Domenic", ""], ["Jian", "Lisa", ""], ["Kumar", "Pranay", ""], ["Panda", "Aurojit", ""], ["Shenker", "Scott", ""]]}, {"id": "2004.10276", "submitter": "Dimitrios Kallergis", "authors": "Dimitrios Kallergis, Zacharenia Garofalaki, Georgios Katsikogiannis,\n  and Christos Douligeris", "title": "CAPODAZ: A Containerised Authorisation and Policy-driven Architecture\n  using Microservices", "comments": null, "journal-ref": null, "doi": "10.1016/j.adhoc.2020.102153", "report-no": null, "categories": "cs.NI cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The microservices architectural approach has important benefits regarding the\nagile applications' development and the delivery of complex solutions. However,\nto convey the information and share the data amongst services in a verifiable\nand stateless way, there is a need to enable appropriate access control methods\nand authorisations. In this paper, we study the use of policy-driven\nauthorisations with independent fine-grained microservices in the case of a\nreal-world machine-to-machine (M2M) scenario using a hybrid cloud-based\ninfrastructure and Internet of Things (IoT) services. We also model the\nauthentication flows which facilitate the message exchanges between the\ninvolved entities, and we propose a containerised authorisation and\npolicy-driven architecture (CAPODAZ) using the microservices paradigm. The\nproposed architecture implements a policy-based management framework and\nintegrates in an on-going work regarding a Cloud-IoT intelligent transportation\nservice. For the in-depth quantitative evaluation, we treat multiple\ndistributions of users' populations and assess the proposed architecture\nagainst other similar microservices. The numerical results based on the\nexperimental data show that there exists significant performance preponderance\nin terms of latency, throughput and successful requests.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:16:34 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 10:18:07 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Kallergis", "Dimitrios", ""], ["Garofalaki", "Zacharenia", ""], ["Katsikogiannis", "Georgios", ""], ["Douligeris", "Christos", ""]]}, {"id": "2004.10381", "submitter": "Zhe Wang", "authors": "Zhe Wang, Pradeep Subedi, Shaohua Duan, Yubo Qin, Philip Davis,\n  Anthony Simonet, Ivan Rodero, Manish Parashar", "title": "Exploring Trade-offs in Dynamic Task Triggering for Loosely Coupled\n  Scientific Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve near-time insights, scientific workflows tend to be\norganized in a flexible and dynamic way. Data-driven triggering of tasks has\nbeen explored as a way to support workflows that evolve based on the data.\nHowever, the overhead introduced by such dynamic triggering of tasks is an\nunder-studied topic. This paper discusses different facets of dynamic task\ntriggers. Particularly, we explore different ways of constructing a data-driven\ndynamic workflow and then evaluate the overheads introduced by such design\ndecisions. We evaluate workflows with varying data size, percentage of\ninteresting data, temporal data distribution, and number of tasks triggered.\nFinally, we provide advice based upon analysis of the evaluation results for\nusers looking to construct data-driven scientific workflows.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 03:17:13 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Wang", "Zhe", ""], ["Subedi", "Pradeep", ""], ["Duan", "Shaohua", ""], ["Qin", "Yubo", ""], ["Davis", "Philip", ""], ["Simonet", "Anthony", ""], ["Rodero", "Ivan", ""], ["Parashar", "Manish", ""]]}, {"id": "2004.10387", "submitter": "Qing Han", "authors": "Qing Han, Shusen Yang, Xuebin Ren, Cong Zhao, Jingqi Zhang, Xinyu Yang", "title": "OL4EL: Online Learning for Edge-cloud Collaborative Learning on\n  Heterogeneous Edges with Resource Constraints", "comments": "7 pages, 5 figures, to appear in IEEE Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning (ML) at network edge is a promising paradigm\nthat can preserve both network bandwidth and privacy of data providers.\nHowever, heterogeneous and limited computation and communication resources on\nedge servers (or edges) pose great challenges on distributed ML and formulate a\nnew paradigm of Edge Learning (i.e. edge-cloud collaborative machine learning).\nIn this article, we propose a novel framework of 'learning to learn' for\neffective Edge Learning (EL) on heterogeneous edges with resource constraints.\nWe first model the dynamic determination of collaboration strategy (i.e. the\nallocation of local iterations at edge servers and global aggregations on the\nCloud during collaborative learning process) as an online optimization problem\nto achieve the tradeoff between the performance of EL and the resource\nconsumption of edge servers. Then, we propose an Online Learning for EL (OL4EL)\nframework based on the budget-limited multi-armed bandit model. OL4EL supports\nboth synchronous and asynchronous learning patterns, and can be used for both\nsupervised and unsupervised learning tasks. To evaluate the performance of\nOL4EL, we conducted both real-world testbed experiments and extensive\nsimulations based on docker containers, where both Support Vector Machine and\nK-means were considered as use cases. Experimental results demonstrate that\nOL4EL significantly outperforms state-of-the-art EL and other collaborative ML\napproaches in terms of the trade-off between learning performance and resource\nconsumption.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 03:51:58 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 08:13:55 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Han", "Qing", ""], ["Yang", "Shusen", ""], ["Ren", "Xuebin", ""], ["Zhao", "Cong", ""], ["Zhang", "Jingqi", ""], ["Yang", "Xinyu", ""]]}, {"id": "2004.10488", "submitter": "Philipp Frauenthaler", "authors": "Marten Sigwart, Philipp Frauenthaler, Christof Spanring, Stefan\n  Schulte", "title": "Decentralized Cross-Blockchain Asset Transfers", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, solutions for cross-blockchain asset transfers are either tailored for\nspecific assets, require certain means of cross-blockchain communication, or\nneglect finality guarantees that prevent assets from getting lost in transit.\nIn this paper, we present a cross-blockchain asset transfer protocol that\nsupports arbitrary assets, is adaptable to different means of cross-blockchain\ncommunication, and adheres to requirements such as finality. The ability to\nfreely transfer assets between blockchains may increase transaction throughput\nand provide developers with more flexibility by allowing them to design digital\nassets that leverage the capacities and capabilities of multiple blockchains.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 10:52:08 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sigwart", "Marten", ""], ["Frauenthaler", "Philipp", ""], ["Spanring", "Christof", ""], ["Schulte", "Stefan", ""]]}, {"id": "2004.10619", "submitter": "Adam Shimi", "authors": "Adam Shimi, Aur\\'elie Hurault, and Philippe Queinnec", "title": "Derivation of Heard-Of Predicates From Elementary Behavioral Patterns", "comments": "Full version of a paper accepted to FORTE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There are many models of distributed computing, and no unifying mathematical\nframework for considering them all. One way to sidestep this issue is to start\nwith simple communication and fault models, and use them as building blocks to\nderive the complex models studied in the field. We thus define operations like\nunion, succession or repetition, which makes it easier to build complex models\nfrom simple ones while retaining expressivity.\n  To formalize this approach, we abstract away the complex models and\noperations in the Heard-Of model. This model relies on (possibly asynchronous)\nrounds; sequence of digraphs, one for each round, capture which messages sent\nat a given round are received before the receiver goes to the next round. A set\nof sequences, called a heard-of predicate,defines the legal communication\nbehaviors -- that is to say, a model of communication. Because the proposed\noperations behave well with this transformation of operational models into\nheard-of predicates, we can derive bounds, characterizations, and\nimplementations of the heard-of predicates for the constructions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:03:10 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Shimi", "Adam", ""], ["Hurault", "Aur\u00e9lie", ""], ["Queinnec", "Philippe", ""]]}, {"id": "2004.10670", "submitter": "Shulai Zhang", "authors": "Shulai Zhang and Xiaoli Ma", "title": "A General Difficulty Control Algorithm for Proof-of-Work Based\n  Blockchains", "comments": "5 pages, to be published in ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an efficient difficulty control algorithm is an essential problem\nin Proof-of-Work (PoW) based blockchains because the network hash rate is\nrandomly changing. This paper proposes a general difficulty control algorithm\nand provides insights for difficulty adjustment rules for PoW based\nblockchains. The proposed algorithm consists a two-layer neural network. It has\nlow memory cost, meanwhile satisfying the fast-updating and low volatility\nrequirements for difficulty adjustment. Real data from Ethereum are used in the\nsimulations to prove that the proposed algorithm has better performance for the\ncontrol of the block difficulty.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 16:08:46 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Zhang", "Shulai", ""], ["Ma", "Xiaoli", ""]]}, {"id": "2004.10674", "submitter": "Yu Chen", "authors": "Ronghua Xu, Yu Chen, Erik Blasch, Alexander Aved, Genshe Chen, Dan\n  Shen", "title": "Hybrid Blockchain-Enabled Secure Microservices Fabric for Decentralized\n  Multi-Domain Avionics Systems", "comments": "2020 SPIE Defense + Commercial Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancement in artificial intelligence (AI) and machine learning (ML),\ndynamic data driven application systems (DDDAS), and hierarchical\ncloud-fog-edge computing paradigm provide opportunities for enhancing\nmulti-domain systems performance. As one example that represents multi-domain\nscenario, a \"fly-by-feel\" system utilizes DDDAS framework to support autonomous\noperations and improve maneuverability, safety and fuel efficiency. The DDDAS\n\"fly-by-feel\" avionics system can enhance multi-domain coordination to support\ndomain specific operations. However, conventional enabling technologies rely on\na centralized manner for data aggregation, sharing and security policy\nenforcement, and it incurs critical issues related to bottleneck of\nperformance, data provenance and consistency. Inspired by the containerized\nmicroservices and blockchain technology, this paper introduces BLEM, a hybrid\nBLockchain-Enabled secure Microservices fabric to support decentralized, secure\nand efficient data fusion and multi-domain operations for avionics systems.\nLeveraging the fine-granularity and loose-coupling features of the\nmicroservices architecture, multidomain operations and security functionalities\nare decoupled into multiple containerized microservices. A hybrid blockchain\nfabric based on two-level committee consensus protocols is proposed to enable\ndecentralized security architecture and support immutability, auditability and\ntraceability for data provenience in existing multi-domain avionics system. Our\nevaluation results show the feasibility of the proposed BLEM mechanism to\nsupport decentralized security service and guarantee immutability, auditability\nand traceability for data provenience across domain boundaries.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 02:33:04 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Xu", "Ronghua", ""], ["Chen", "Yu", ""], ["Blasch", "Erik", ""], ["Aved", "Alexander", ""], ["Chen", "Genshe", ""], ["Shen", "Dan", ""]]}, {"id": "2004.10776", "submitter": "David Stalfa", "authors": "Biswaroop Maiti, Rajmohan Rajaraman, David Stalfa, Zoya Svitkina,\n  Aravindan Vijayaraghavan", "title": "Scheduling Precedence-Constrained Jobs on Related Machines with\n  Communication Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scheduling $n$ precedence-constrained jobs on $m$\nuniformly-related machines in the presence of an arbitrary, fixed communication\ndelay $\\rho$. We consider a model that allows job duplication, i.e. processing\nof the same job on multiple machines, which, as we show, can reduce the length\nof a schedule (i.e., its makespan) by a logarithmic factor. Our main result is\nan $O(\\log m \\log \\rho / \\log \\log \\rho)$-approximation algorithm for\nminimizing makespan, assuming the minimum makespan is at least $\\rho$. Our\nalgorithm is based on rounding a linear programming relaxation for the problem,\nwhich includes carefully designed constraints capturing the interaction among\ncommunication delay, precedence requirements, varying speeds, and job\nduplication. Our result builds on two previous lines of work, one with\ncommunication delay but identical machines (Lepere, Rapine 2002) and the other\nwith uniformly-related machines but no communication delay (Chudak, Shmoys\n1999). We next show that the integrality gap of our mathematical program is\n$\\Omega(\\sqrt{\\log \\rho})$. Our gap construction employs expander graphs and\nexploits a property of robust expansion and its generalization to paths of\nlonger length. Finally, we quantify the advantage of duplication in scheduling\nwith communication delay. We show that the best schedule without duplication\ncan have makespan $\\Omega(\\rho/\\log \\rho)$ or $\\Omega(\\log m/\\log\\log m)$ or\n$\\Omega(\\log n/\\log \\log n)$ times that of an optimal schedule allowing\nduplication. Nevertheless, we present a polynomial time algorithm to transform\nany schedule to a schedule without duplication at the cost of a $O(\\log^2 n\n\\log m)$ factor increase in makespan. Together with our makespan approximation\nalgorithm for schedules allowing duplication, this also yields a\npolylogarithmic-approximation algorithm for the setting where duplication is\nnot allowed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 18:23:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Maiti", "Biswaroop", ""], ["Rajaraman", "Rajmohan", ""], ["Stalfa", "David", ""], ["Svitkina", "Zoya", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2004.10811", "submitter": "Michael Lass", "authors": "Michael Lass, Robert Schade, Thomas D. K\\\"uhne, Christian Plessl", "title": "A Submatrix-Based Method for Approximate Matrix Function Evaluation in\n  the Quantum Chemistry Code CP2K", "comments": null, "journal-ref": null, "doi": "10.1109/SC41405.2020.00084", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic structure calculations based on density-functional theory (DFT)\nrepresent a significant part of today's HPC workloads and pose high demands on\nhigh-performance computing resources. To perform these quantum-mechanical DFT\ncalculations on complex large-scale systems, so-called linear scaling methods\ninstead of conventional cubic scaling methods are required. In this work, we\ntake up the idea of the submatrix method and apply it to the DFT computations\nin the software package CP2K. For that purpose, we transform the underlying\nnumeric operations on distributed, large, sparse matrices into computations on\nlocal, much smaller and nearly dense matrices. This allows us to exploit the\nfull floating-point performance of modern CPUs and to make use of dedicated\naccelerator hardware, where performance has been limited by memory bandwidth\nbefore. We demonstrate both functionality and performance of our implementation\nand show how it can be accelerated with GPUs and FPGAs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:28:13 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 12:00:50 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 12:58:26 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Lass", "Michael", ""], ["Schade", "Robert", ""], ["K\u00fchne", "Thomas D.", ""], ["Plessl", "Christian", ""]]}, {"id": "2004.10854", "submitter": "Dionysios Diamantopoulos", "authors": "Dionysios Diamantopoulos, Burkhard Ringlein, Mitra Purandare,\n  Gagandeep Singh, and Christoph Hagleitner", "title": "Agile Autotuning of a Transprecision Tensor Accelerator Overlay for TVM\n  Compiler Stack", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Specialized accelerators for tensor-operations, such as blocked-matrix\noperations and multi-dimensional convolutions, have been emerged as powerful\narchitecture choices for high-performance Deep-Learning computing. The rapid\ndevelopment of frameworks, models, and precision options challenges the\nadaptability of such tensor-accelerators since the adaptation to new\nrequirements incurs significant engineering costs. Programmable tensor\naccelerators offer a promising alternative by allowing reconfiguration of a\nvirtual architecture that overlays on top of the physical FPGA configurable\nfabric. We propose an overlay ({\\tau}-VTA) and an optimization method guided by\nagile-inspired auto-tuning techniques. We achieve higher performance and faster\nconvergence than state-of-art.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:12:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Diamantopoulos", "Dionysios", ""], ["Ringlein", "Burkhard", ""], ["Purandare", "Mitra", ""], ["Singh", "Gagandeep", ""], ["Hagleitner", "Christoph", ""]]}, {"id": "2004.10856", "submitter": "Zhenkun Cai", "authors": "Zhenkun Cai, Kaihao Ma, Xiao Yan, Yidi Wu, Yuzhen Huang, James Cheng,\n  Teng Su, Fan Yu", "title": "TensorOpt: Exploring the Tradeoffs in Distributed DNN Training with\n  Auto-Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good parallelization strategy can significantly improve the efficiency or\nreduce the cost for the distributed training of deep neural networks (DNNs).\nRecently, several methods have been proposed to find efficient parallelization\nstrategies but they all optimize a single objective (e.g., execution time,\nmemory consumption) and produce only one strategy. We propose FT, an efficient\nalgorithm that searches for an optimal set of parallelization strategies to\nallow the trade-off among different objectives. FT can adapt to different\nscenarios by minimizing the memory consumption when the number of devices is\nlimited and fully utilize additional resources to reduce the execution time.\nFor popular DNN models (e.g., vision, language), an in-depth analysis is\nconducted to understand the trade-offs among different objectives and their\ninfluence on the parallelization strategies. We also develop a user-friendly\nsystem, called TensorOpt, which allows users to run their distributed DNN\ntraining jobs without caring the details of parallelization strategies.\nExperimental results show that FT runs efficiently and provides accurate\nestimation of runtime costs, and TensorOpt is more flexible in adapting to\nresource availability compared with existing frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 02:57:35 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Cai", "Zhenkun", ""], ["Ma", "Kaihao", ""], ["Yan", "Xiao", ""], ["Wu", "Yidi", ""], ["Huang", "Yuzhen", ""], ["Cheng", "James", ""], ["Su", "Teng", ""], ["Yu", "Fan", ""]]}, {"id": "2004.10857", "submitter": "Mahdi Fahmideh", "authors": "Mahdi Fahmideha, Farhad Daneshgarb, Fethi Rabhic and Ghassan Beydound", "title": "A generic cloud migration process model", "comments": null, "journal-ref": "European Journal of Information Systems 28.3 (2019): 233-255", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cloud computing literature provides various ways to utilise cloud\nservices, each with a different viewpoint, focus, and mostly using\nheterogeneous technical-centric terms. This hinders efficient and consistent\nknowledge flow across the community. Little, if any, research has aimed on\ndeveloping an integrated process model which captures core domain concepts and\nties them together to provide an overarching view of migrating legacy systems\nto cloud platforms that is customisable for a given context. We adopt design\nscience research guidelines in which we use a metamodeling approach to develop\na generic process model and then evaluate and refine the model through three\ncase studies and domain expert reviews. This research benefits academics and\npractitioners alike by underpinning a substrate for constructing,\nstandardising, maintaining, and sharing bespoke cloud migration models that can\nbe applied to given cloud adoption scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:23:41 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Fahmideha", "Mahdi", ""], ["Daneshgarb", "Farhad", ""], ["Rabhic", "Fethi", ""], ["Beydound", "Ghassan", ""]]}, {"id": "2004.10858", "submitter": "Mahdi Fahmideh", "authors": "Mahdi Fahmideh, Ghassan Beydoun, Graham Low", "title": "Experiential probabilistic assessment of cloud services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial difficulties in adopting cloud services are often encountered\nduring upgrades of existing software systems. A reliable early stage analysis\ncan facilitate an informed decision process of moving systems to cloud\nplatforms. It can also mitigate risks against system quality goals. Towards\nthis, we propose an interactive goal reasoning approach which is supported by a\nprobabilistic layer for the precise analysis of cloud migration risks to\nimprove the reliability of risk control. The approach is illustrated using a\ncommercial scenario of integrating a digital document processing system to\nMicrosoft Azure cloud platform.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:46:58 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Fahmideh", "Mahdi", ""], ["Beydoun", "Ghassan", ""], ["Low", "Graham", ""]]}, {"id": "2004.10908", "submitter": "Tsung-Wei Huang", "authors": "Tsung-Wei Huang, Dian-Lun Lin, Yibo Lin, Chun-Xun Lin", "title": "Taskflow: A General-purpose Parallel and Heterogeneous Task Programming\n  System", "comments": "19 pages, 23 pages", "journal-ref": "2019 IEEE International Parallel and Distributed Processing\n  Symposium (IPDPS)", "doi": "10.1109/IPDPS.2019.00105", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Taskflow project addresses the long-standing question: How can we make it\neasier for developers to write parallel and heterogeneous programs with high\nperformance and simultaneous high productivity? Taskflow develops a simple and\npowerful task programming model to enable efficient implementations of\nheterogeneous decomposition strategies. Our programming model empowers users\nwith both static and dynamic task graph constructions to incorporate a broad\nrange of computational patterns including hybrid CPU-GPU computing, dynamic\ncontrol flow, and irregularity. We develop an efficient heterogeneous\nwork-stealing strategy that adapts worker threads to available task parallelism\nat any time during the graph execution. We have demonstrated promising\nperformance of Taskflow on both micro-benchmark and real-world applications. As\nan example, we solved a large machine learning workload by up to 1.5x faster,\n1.6x less memory, and 1.7x fewer lines of code than two industrial-strength\nsystems, oneTBB and StarPU, on a machine of 40 CPUs and 4 GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 00:21:05 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 18:50:03 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 03:49:24 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Huang", "Tsung-Wei", ""], ["Lin", "Dian-Lun", ""], ["Lin", "Yibo", ""], ["Lin", "Chun-Xun", ""]]}, {"id": "2004.10926", "submitter": "Rujia Wang", "authors": "Zhou Ni, Rujia Wang", "title": "Performance Evaluation of Secure Multi-party Computation on\n  Heterogeneous Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure multi-party computation (MPC) is a broad cryptographic concept that\ncan be adopted for privacy-preserving computation. With MPC, a number of\nparties can collaboratively compute a function, without revealing the actual\ninput or output of the plaintext to others. The applications of MPC range from\nprivacy-preserving voting, arithmetic calculation, and large-scale data\nanalysis. From the system perspective, each party in MPC can run on one compute\nnode. The compute nodes of multiple parties could be either homogeneous or\nheterogeneous; however, the distributed workloads from the MPC protocols tend\nto be always homogeneous (symmetric). In this work, we study a representative\nMPC framework and a set of MPC applications from the system performance\nperspective. We show the detailed online computation workflow of a\nstate-of-the-art MPC protocol and analyze the root cause of its stall time and\nperformance bottleneck on homogeneous and heterogeneous compute nodes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 01:42:47 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ni", "Zhou", ""], ["Wang", "Rujia", ""]]}, {"id": "2004.11059", "submitter": "Marius Meyer", "authors": "Marius Meyer, Tobias Kenter and Christian Plessl", "title": "Evaluating FPGA Accelerator Performance with a Parameterized OpenCL\n  Adaptation of the HPCChallenge Benchmark Suite", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGAs have found increasing adoption in data center applications since a new\ngeneration of high-level tools have become available which noticeably reduce\ndevelopment time for FPGA accelerators and still provide high quality of\nresults. There is however no high-level benchmark suite available which\nspecifically enables a comparison of FPGA architectures, programming tools and\nlibraries for HPC applications.\n  To fill this gap, we have developed an OpenCL-based open source\nimplementation of the HPCC benchmark suite for Xilinx and Intel FPGAs. This\nbenchmark can serve to analyze the current capabilities of FPGA devices, cards\nand development tool flows, track progress over time and point out specific\ndifficulties for FPGA acceleration in the HPC domain. Additionally, the\nbenchmark documents proven performance optimization patterns. We will continue\noptimizing and porting the benchmark for new generations of FPGAs and design\ntools and encourage active participation to create a valuable tool for the\ncommunity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 10:38:18 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 15:41:06 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 09:16:08 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Meyer", "Marius", ""], ["Kenter", "Tobias", ""], ["Plessl", "Christian", ""]]}, {"id": "2004.11062", "submitter": "Alexey Lastovetsky", "authors": "Emin Nuriyev, Alexey Lastovetsky", "title": "Accurate runtime selection of optimal MPI collective algorithms using\n  analytical performance modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of collective operations has been a critical issue since the\nadvent of MPI. Many algorithms have been proposed for each MPI collective\noperation but none of them proved optimal in all situations. Different\nalgorithms demonstrate superior performance depending on the platform, the\nmessage size, the number of processes, etc. MPI implementations perform the\nselection of the collective algorithm empirically, executing a simple runtime\ndecision function. While efficient, this approach does not guarantee the\noptimal selection. As a more accurate but equally efficient alternative, the\nuse of analytical performance models of collective algorithms for the selection\nprocess was proposed and studied. Unfortunately, the previous attempts in this\ndirection have not been successful. We revisit the analytical model-based\napproach and propose two innovations that significantly improve the selective\naccuracy of analytical models: (1) We derive analytical models from the code\nimplementing the algorithms rather than from their high-level mathematical\ndefinitions. This results in more detailed models. (2) We estimate model\nparameters separately for each collective algorithm and include the execution\nof this algorithm in the corresponding communication experiment. We\nexperimentally demonstrate the accuracy and efficiency of our approach using\nOpen MPI broadcast and gather algorithms and a Grid5000 cluster.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 10:42:36 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Nuriyev", "Emin", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "2004.11206", "submitter": "Sima Sinaei", "authors": "Najmeh Nazari, Seyed Ahmad Mirsalari, Sima Sinaei, Mostafa E. Salehi,\n  Masoud Daneshtalab", "title": "Multi-level Binarized LSTM in EEG Classification for Wearable Devices", "comments": "o appear in IEEE International Conference on Parallel, Distributed\n  and Network-based Processing in 2020. arXiv admin note: text overlap with\n  arXiv:1812.04818 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is widely used in various sequential\napplications. Complex LSTMs could be hardly deployed on wearable and\nresourced-limited devices due to the huge amount of computations and memory\nrequirements. Binary LSTMs are introduced to cope with this problem, however,\nthey lead to significant accuracy loss in some application such as EEG\nclassification which is essential to be deployed in wearable devices. In this\npaper, we propose an efficient multi-level binarized LSTM which has\nsignificantly reduced computations whereas ensuring an accuracy pretty close to\nfull precision LSTM. By deploying 5-level binarized weights and inputs, our\nmethod reduces area and delay of MAC operation about 31* and 27* in 65nm\ntechnology, respectively with less than 0.01% accuracy loss. In contrast to\nmany compute-intensive deep-learning approaches, the proposed algorithm is\nlightweight, and therefore, brings performance efficiency with accurate\nLSTM-based EEG classification to real-time wearable devices.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 17:48:55 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Nazari", "Najmeh", ""], ["Mirsalari", "Seyed Ahmad", ""], ["Sinaei", "Sima", ""], ["Salehi", "Mostafa E.", ""], ["Daneshtalab", "Masoud", ""]]}, {"id": "2004.11428", "submitter": "Martin Garriga", "authors": "Christos Tsigkanos, Martin Garriga, Luciano Baresi and Carlo Ghezzi", "title": "Cloud Deployment Tradeoffs for the Analysis of Spatially-Distributed\n  Systems of Internet-of-Things", "comments": "Accepted for publication in ACM Transactions on Internet of Things", "journal-ref": null, "doi": "10.1145/3381452", "report-no": null, "categories": "cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-enabled things and devices operating in the physical world are\nincreasingly integrated in modern distributed systems, supporting\nfunctionalities that require assurances that certain critical requirements are\nsatisfied by the overall system. We focus here on spatially-distributed\nInternet-of-Things systems such as smart environments, where the dynamics of\nspatial distribution of entities in the system is crucial to requirements\nsatisfaction. Analysis techniques need to be in place while systems operate to\nensure that requirements are fulfilled. This may be achieved by keeping a model\nof the system at runtime, monitoring events that lead to changes in the spatial\nenvironment, and performing analysis. This computationally-intensive runtime\nassurance method cannot be supported by resource-constrained devices that\npopulate the space and must be offloaded to the cloud. However, challenges\narise regarding resource allocation and cost, especially when the workload is\nunknown at the system's design time. As such, it may be difficult or even\nimpossible to guarantee application service level agreements, e.g., on response\ntimes. To this end, we instantiate spatial verification processes, integrating\nthem to the service layer of an IoT-cloud architecture based on microservices.\nWe propose several cloud deployments for such an architecture for assurance of\nspatial requirements -- based on virtual machines, containers, and the recent\nFunctions-as-a-Service paradigm. Then, we assess deployments' tradeoffs in\nterms of elasticity, performance and cost by using a workload scenario from a\nknown dataset of taxis roaming in Beijing. We argue that the approach can be\nreplicated in the design process of similar kinds of spatially distributed\nInternet-of-Things systems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 19:05:45 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Tsigkanos", "Christos", ""], ["Garriga", "Martin", ""], ["Baresi", "Luciano", ""], ["Ghezzi", "Carlo", ""]]}, {"id": "2004.11439", "submitter": "Kaushik Mondal", "authors": "Anisur Rahaman Molla, Kaushik Mondal, William K. Moses Jr", "title": "Efficient Dispersion on an Anonymous Ring in the Presence of Weak\n  Byzantine Robots", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of dispersion of mobile robots on a graph asks that $n$ robots\ninitially placed arbitrarily on the nodes of an $n$-node anonymous graph,\nautonomously move to reach a final configuration where exactly each node has at\nmost one robot on it. This problem is of significant interest due to its\nrelationship to other fundamental robot coordination problems, such as\nexploration, scattering, load balancing, relocation of self-driving electric\ncars to recharge stations, etc. The robots have unique IDs, typically in the\nrange $[1,poly(n)]$ and limited memory, whereas the graph is anonymous, i.e.,\nthe nodes do not have identifiers. The objective is to simultaneously minimize\ntwo performance metrics: (i) time to achieve dispersion and (ii) memory\nrequirement at each robot. This problem has been relatively well-studied when\nrobots are non-faulty.\n  In this paper, we introduce the notion of Byzantine faults to this problem,\ni.e., we formalize the problem of dispersion in the presence of up to $f$\nByzantine robots. We then study the problem on a ring while simultaneously\noptimizing the time complexity of algorithms and the memory requirement per\nrobot. Specifically, we design deterministic algorithms that attempt to match\nthe time lower bound ($\\Omega(n)$ rounds) and memory lower bound ($\\Omega(\\log\nn)$ bits per robot).\n  Our main result is a deterministic algorithm that is both time and memory\noptimal, i.e., $O(n)$ rounds and $O(\\log n)$ bits of memory required per robot,\nsubject to certain constraints. We subsequently provide results that require\nless assumptions but are either only time or memory optimal but not both. We\nalso provide a primitive, utilized often, that takes robots initially gathered\nat a node of the ring and disperses them in a time and memory optimal manner\nwithout additional assumptions required.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 19:51:50 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 09:43:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Molla", "Anisur Rahaman", ""], ["Mondal", "Kaushik", ""], ["Moses", "William K.", "Jr"]]}, {"id": "2004.11571", "submitter": "Pierre Fortin", "authors": "Pierre Fortin (LIP6), Ambroise Fleury, Fran\\c{c}ois Lemaire (LIFL),\n  Michael Monagan", "title": "High performance SIMD modular arithmetic for polynomial evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two essential problems in Computer Algebra, namely polynomial factorization\nand polynomial greatest common divisor computation, can be efficiently solved\nthanks to multiple polynomial evaluations in two variables using modular\narithmetic. In this article, we focus on the efficient computation of such\npolynomial evaluations on one single CPU core. We first show how to leverage\nSIMD computing for modular arithmetic on AVX2 and AVX-512 units, using both\nintrinsics and OpenMP compiler directives. Then we manage to increase the\noperational intensity and to exploit instruction-level parallelism in order to\nincrease the compute efficiency of these polynomial evaluations. All this\nresults in the end to performance gains up to about 5x on AVX2 and 10x on\nAVX-512.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 07:28:45 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Fortin", "Pierre", "", "LIP6"], ["Fleury", "Ambroise", "", "LIFL"], ["Lemaire", "Fran\u00e7ois", "", "LIFL"], ["Monagan", "Michael", ""]]}, {"id": "2004.11682", "submitter": "\\\"Ozg\\\"un Ak{\\i}n", "authors": "Ozgun Akin, Halil Faruk Deniz, Dogukan Nefis, Alp Kiziltan and Altan\n  Cakir", "title": "Enabling Big Data Analytics at Manufacturing Fields of Farplas\n  Automotive", "comments": "8 pages", "journal-ref": null, "doi": "10.1007/978-3-030-51156-2_94", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitization and data-driven manufacturing process is needed for today's\nindustry. The term Industry 4.0 stands for today industrial digitization which\nis defined as a new level of organization and control over the entire value\nchain of the life cycle of products; it is geared towards increasingly\nindividualized customer's high-quality expectations. However, due to the\nincrease in the number of connected devices and the variety of data, it has\nbecome difficult to store and analyze data with conventional systems. The\nmotivation of this paper is to provide an overview of the understanding of the\nbig data pipeline, providing a real-time on-premise data acquisition, data\ncompression, data storage and processing with Apache Kafka and Apache Spark\nimplementation on Apache Ha-doop cluster, and identifying the challenges and\nissues occurring with implementation the Farplas manufacturing company, which\nis one of the biggest Tier 1 automotive supplier in Turkey, to study the new\ntrends and streams related to topics via Industry 4.0.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:22:27 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Akin", "Ozgun", ""], ["Deniz", "Halil Faruk", ""], ["Nefis", "Dogukan", ""], ["Kiziltan", "Alp", ""], ["Cakir", "Altan", ""]]}, {"id": "2004.11707", "submitter": "Anwar Ghani Dr.", "authors": "Afzal Badshah, Anwar Ghani, Ali Daud, Anthony Theodore Chronopoulos,\n  Ateeqa Jalal", "title": "Revenue Maximization Approaches in IaaS Clouds: Research Challenges and\n  Opportunities", "comments": "28 Pages, 3 Figures, 5 Tables, 110 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Revenue generation is the main concern of any business, particularly in the\ncloud, where there is no direct interaction between the provider and the\nconsumer. Cloud computing is an emerging core for today's businesses, however,\nIts complications (e.g, installation, and migration) with traditional markets\nare the main challenges. It earns more but needs exemplary performance and\nmarketing skills. In recent years, cloud computing has become a successful\nparadigm for providing desktop services. It is expected that more than \\$ 331\nbillion will be invested by 2023, likewise, 51 billion devices are expected to\nbe connected to the cloud. Infrastructure as a Service (IaaS) provides physical\nresources (e.g, computing, memory, storage, and network) as VM instances. In\nthis article, the main revenue factors are categorized as SLA and penalty\nmanagement, resource scalability, customer satisfaction and management,\nresource utilization and provision, cost and price management, and advertising\nand auction. These parameters are investigated in detail and new dynamics for\nresearchers in the field of the cloud are discovered.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:42:36 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Badshah", "Afzal", ""], ["Ghani", "Anwar", ""], ["Daud", "Ali", ""], ["Chronopoulos", "Anthony Theodore", ""], ["Jalal", "Ateeqa", ""]]}, {"id": "2004.11725", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Nan Wang and David Bermbach and Cheol-Ho Hong and\n  Eyal de Lara and Weisong Shi and Christopher Stewart", "title": "A Survey on Edge Performance Benchmarking", "comments": "Accepted by ACM Computing Surveys, 16 December 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is the next Internet frontier that will leverage computing\nresources located near users, sensors, and data stores to provide more\nresponsive services. Therefore, it is envisioned that a large-scale,\ngeographically dispersed, and resource-rich distributed system will emerge and\nplay a key role in the future Internet. However, given the loosely coupled\nnature of such complex systems, their operational conditions are expected to\nchange significantly over time. In this context, the performance\ncharacteristics of such systems will need to be captured rapidly, which is\nreferred to as performance benchmarking, for application deployment, resource\norchestration, and adaptive decision-making. Edge performance benchmarking is a\nnascent research avenue that has started gaining momentum over the past five\nyears. This article first reviews articles published over the past three\ndecades to trace the history of performance benchmarking from tightly coupled\nto loosely coupled systems. It then systematically classifies previous research\nto identify the system under test, techniques analyzed, and benchmark runtime\nin edge performance benchmarking.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:05:59 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 14:26:40 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Varghese", "Blesson", ""], ["Wang", "Nan", ""], ["Bermbach", "David", ""], ["Hong", "Cheol-Ho", ""], ["de Lara", "Eyal", ""], ["Shi", "Weisong", ""], ["Stewart", "Christopher", ""]]}, {"id": "2004.12335", "submitter": "Leila Ismail Prof.", "authors": "Leila Ismail and Huned Materwala", "title": "Computing server power modeling in a data center: survey,taxonomy and\n  performance evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centers are large scale, energy-hungry infrastructure serving the\nincreasing computational demands as the world is becoming more connected in\nsmart cities. The emergence of advanced technologies such as cloud-based\nservices, internet of things (IoT) and big data analytics has augmented the\ngrowth of global data centers, leading to high energy consumption. This upsurge\nin energy consumption of the data centers not only incurs the issue of surging\nhigh cost (operational and maintenance) but also has an adverse effect on the\nenvironment. Dynamic power management in a data center environment requires the\ncognizance of the correlation between the system and hardware level performance\ncounters and the power consumption. Power consumption modeling exhibits this\ncorrelation and is crucial in designing energy-efficient optimization\nstrategies based on resource utilization. Several works in power modeling are\nproposed and used in the literature. However, these power models have been\nevaluated using different benchmarking applications, power measurement\ntechniques and error calculation formula on different machines. In this work,\nwe present a taxonomy and evaluation of 24 software-based power models using a\nunified environment, benchmarking applications, power measurement technique and\nerror formula, with the aim of achieving an objective comparison. We use\ndifferent servers architectures to assess the impact of heterogeneity on the\nmodels' comparison. The performance analysis of these models is elaborated in\nthe paper.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:52:27 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 13:03:05 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ismail", "Leila", ""], ["Materwala", "Huned", ""]]}, {"id": "2004.12378", "submitter": "Sheik Mohammad Mostakim Fattah", "authors": "Sheik Mohammad Mostakim Fattah, Athman Bouguettaya, and Sajib Mistry", "title": "Signature-based Selection of IaaS Cloud Services", "comments": "8 pages, Accepted and to appear in 2020 IEEE International Conference\n  on Web Services (ICWS). Content may change prior to final publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach to select IaaS cloud services for a long-term\nperiod where the service providers offer limited QoS information. The proposed\napproach leverages free short-term trials to obtain the previously undisclosed\nQoS information. A new significance-based trial scheme is proposed using\nfrequency distribution analysis to test a consumer's long-term workloads in a\nshort trial. We introduce a novel IaaS signature technique to uniquely identify\nthe variability of a provider's QoS performance. A Signature-based QoS\nPerformance Discovery (SPD) algorithm is proposed which leverages the\ncombination of free trials and IaaS signatures. A set of exhaustive experiments\nwith real-world datasets is conducted to evaluate the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 13:31:58 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 01:58:22 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Fattah", "Sheik Mohammad Mostakim", ""], ["Bouguettaya", "Athman", ""], ["Mistry", "Sajib", ""]]}, {"id": "2004.12661", "submitter": "Mark Burgess", "authors": "Mark Burgess", "title": "Information and Causality in Promise Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explicit link between Promise Theory and Information Theory, while\nperhaps obvious, is laid out explicitly here. It's shown how causally related\nobservations of promised behaviours relate to the probabilistic formulation of\ncausal information in Shannon's theory, and thus clarify the meaning of\nautonomy or causal independence, and further the connection between information\nand causal sets. Promise Theory helps to make clear a number of assumptions\nwhich are commonly taken for granted in causal descriptions. The concept of a\npromise is hard to escape. It serves as proxy for intent, whether a priori or\nby inference, and it is intrinsic to the interpretations of observations in the\nlatter.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 09:18:42 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Burgess", "Mark", ""]]}, {"id": "2004.12702", "submitter": "Dipankar Chaki", "authors": "Dipankar Chaki, Athman Bouguettaya and Sajib Mistry", "title": "A Conflict Detection Framework for IoT Services in Multi-resident Smart\n  Homes", "comments": "8 pages, 6 figures. This is an accepted paper and it is going to\n  appear in the Proceedings of the 2020 IEEE International Conference on Web\n  Services (IEEE ICWS 2020) affiliated with the 2020 IEEE World Congress on\n  Services (IEEE SERVICES 2020), Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework to detect conflicts among IoT services in a\nmulti-resident smart home. A novel IoT conflict model is proposed considering\nthe functional and non-functional properties of IoT services. We design a\nconflict ontology that formally represents different types of conflicts. A\nhybrid conflict detection algorithm is proposed by combining both\nknowledge-driven and data-driven approaches. Experimental results on real-world\ndatasets show the efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:50:52 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chaki", "Dipankar", ""], ["Bouguettaya", "Athman", ""], ["Mistry", "Sajib", ""]]}, {"id": "2004.12854", "submitter": "Lei Liu", "authors": "Lei Liu, Xinglei Dou", "title": "A New Qubits Mapping Mechanism for Multi-programming Quantum Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a specific quantum chip, multi-programming helps to improve overall\nthroughput and resource utilization. However, the previous solutions for\nmapping multiple programs onto a quantum chip often lead to resource\nunder-utilization, high error rate and low fidelity. In this paper, we propose\na new approach to map concurrent quantum programs. Our approach has three\ncritical components. The first one is the Community Detection Assisted\nPartition (CDAP) algorithm, which partitions physical qubits for concurrent\nquantum programs by considering both physical typology and the error rates,\navoiding the waste of robust resources. The second one is the X-SWAP scheme\nthat enables inter-program SWAP operations to reduce the SWAP overheads.\nFinally, we propose a compilation task scheduling framework, which dynamically\nselects concurrent quantum programs to be executed based on estimated fidelity,\nincreasing the throughput of the quantum computer. We evaluate our work on\npublicly available quantum computer IBMQ16 and a simulated quantum chip IBMQ20.\nOur work outperforms the previous solution on multi-programming in both\nfidelity and SWAP overheads by 12.0% and 11.1%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:08:47 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Liu", "Lei", ""], ["Dou", "Xinglei", ""]]}, {"id": "2004.12925", "submitter": "Marvin Xhemrishi", "authors": "Rawad Bitar, Marvin Xhemrishi and Antonia Wachter-Zeh", "title": "Rateless Codes for Private Distributed Matrix-Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing rateless coded private distributed\nmatrix-matrix multiplication. A master server owns two private matrices\n$\\mathbf{A}$ and $\\mathbf{B}$ and wants to hire worker nodes to help compute\nthe multiplication. The matrices should remain private from the workers, in an\ninformation-theoretic sense. This problem has been considered in the literature\nand codes with a predesigned threshold are constructed. More precisely, the\nmaster assigns tasks to the workers and waits for a predetermined number of\nworkers to finish their assigned tasks. The size of the tasks assigned to the\nworkers depends on the designed threshold. We are interested in settings where\nthe size of the task must be small and independent of the designed threshold.\nWe design a rateless private matrix-matrix multiplications scheme, called RPM3.\nOur scheme fixes the size of the tasks and allows the master to send multiple\ntasks to the workers. The master keeps receiving results until it can decode\nthe multiplication. Two main applications require this property: i) leverage\nthe possible heterogeneity in the system and assign more tasks to workers that\nare faster; and ii) assign tasks adaptively to account for a possibly\ntime-varying system.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:37:28 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bitar", "Rawad", ""], ["Xhemrishi", "Marvin", ""], ["Wachter-Zeh", "Antonia", ""]]}, {"id": "2004.13081", "submitter": "Matthew Leon", "authors": "Matthew Leon", "title": "The Dark Side of Unikernels for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the shortcomings of unikernels as a method of deployment\nfor machine learning inferencing applications as well as provides insights and\nanalysis on future work in this space. The findings of this paper advocate for\na tool to enable management of dependent libraries in a unikernel to enable a\nmore ergonomic build process as well as take advantage of the inherent security\nand performance benefits of unikernels.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:33:46 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Leon", "Matthew", ""]]}, {"id": "2004.13184", "submitter": "Dragos-Adrian (Adi) Seredinschi PhD", "authors": "Daniel Collins, Rachid Guerraoui, Jovan Komatovic, Matteo Monti,\n  Athanasios Xygkis, Matej Pavlovic, Petr Kuznetsov, Yvonne-Anne Pignolet,\n  Dragos-Adrian Seredinschi, Andrei Tonkikh", "title": "Online Payments by Merely Broadcasting Messages (Extended Version)", "comments": "This is an extended version of a conference article, appearing in the\n  proceedings of the 50th IEEE/IFIP Int. Conference on Dependable Systems and\n  Networks (DSN 2020). This work has been supported in part by the European\n  grant 862082, AT2 -- ERC-2019-PoC, and in part by a grant from Interchain\n  Foundation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of online payments, where users can transfer funds\namong themselves. We introduce Astro, a system solving this problem efficiently\nin a decentralized, deterministic, and completely asynchronous manner. Astro\nbuilds on the insight that consensus is unnecessary to prevent double-spending.\nInstead of consensus, Astro relies on a weaker primitive---Byzantine reliable\nbroadcast---enabling a simpler and more efficient implementation than\nconsensus-based payment systems.\n  In terms of efficiency, Astro executes a payment by merely broadcasting a\nmessage. The distinguishing feature of Astro is that it can maintain\nperformance robustly, i.e., remain unaffected by a fraction of replicas being\ncompromised or slowed down by an adversary. Our experiments on a public cloud\nnetwork show that Astro can achieve near-linear scalability in a sharded setup,\ngoing from $10K$ payments/sec (2 shards) to $20K$ payments/sec (4 shards). In a\nnutshell, Astro can match VISA-level average payment throughput, and achieves a\n$5\\times$ improvement over a state-of-the-art consensus-based solution, while\nexhibiting sub-second $95^{th}$ percentile latency.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 21:27:55 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Collins", "Daniel", ""], ["Guerraoui", "Rachid", ""], ["Komatovic", "Jovan", ""], ["Monti", "Matteo", ""], ["Xygkis", "Athanasios", ""], ["Pavlovic", "Matej", ""], ["Kuznetsov", "Petr", ""], ["Pignolet", "Yvonne-Anne", ""], ["Seredinschi", "Dragos-Adrian", ""], ["Tonkikh", "Andrei", ""]]}, {"id": "2004.13197", "submitter": "Mayank Goswami", "authors": "Michael A. Bender, Mayank Goswami, Dzejla Mededovic, Pablo Montes,\n  Kostas Tsichlas", "title": "Batched Predecessor and Sorting with Size-Priced Information in External\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the unit-cost comparison model, a black box takes an input two items and\noutputs the result of the comparison. Problems like sorting and searching have\nbeen studied in this model, and it has been generalized to include the concept\nof priced information, where different pairs of items (say database records)\nhave different comparison costs. These comparison costs can be arbitrary (in\nwhich case no algorithm can be close to optimal (Charikar et al. STOC 2000)),\nstructured (for example, the comparison cost may depend on the length of the\ndatabases (Gupta et al. FOCS 2001)), or stochastic (Angelov et al. LATIN 2008).\nMotivated by the database setting where the cost depends on the sizes of the\nitems, we consider the problems of sorting and batched predecessor where two\nnon-uniform sets of items $A$ and $B$ are given as input.\n  (1) In the RAM setting, we consider the scenario where both sets have $n$\nkeys each. The cost to compare two items in $A$ is $a$, to compare an item of\n$A$ to an item of $B$ is $b$, and to compare two items in $B$ is $c$. We give\nupper and lower bounds for the case $a \\le b \\le c$. Notice that the case $b=1,\na=c=\\infty$ is the famous ``nuts and bolts'' problem.\n  (2) In the Disk-Access Model (DAM), where transferring elements between disk\nand internal memory is the main bottleneck, we consider the scenario where\nelements in $B$ are larger than elements in $A$. The larger items take more\nI/Os to be brought into memory, consume more space in internal memory, and are\nrequired in their entirety for comparisons.\n  We first give output-sensitive lower and upper bounds on the batched\npredecessor problem, and use these to derive bounds on the complexity of\nsorting in the two models. Our bounds are tight in most cases, and require\nnovel generalizations of the classical lower bound techniques in external\nmemory to accommodate the non-uniformity of keys.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 22:17:19 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bender", "Michael A.", ""], ["Goswami", "Mayank", ""], ["Mededovic", "Dzejla", ""], ["Montes", "Pablo", ""], ["Tsichlas", "Kostas", ""]]}, {"id": "2004.13218", "submitter": "Tung Duong Nguyen", "authors": "Duong Tung Nguyen, Hieu Trung Nguyen, Ni Trieu, and Vijay K. Bhargava", "title": "Two-Stage Robust Edge Service Placement and Sizing under Demand\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing has emerged as a key technology to reduce network traffic,\nimprove user experience, and enable various Internet of Things applications.\nFrom the perspective of a service provider (SP), how to jointly optimize the\nservice placement, sizing, and workload allocation decisions is an important\nand challenging problem, which becomes even more complicated when considering\ndemand uncertainty. To this end, we propose a novel two-stage adaptive robust\noptimization framework to help the SP optimally determine the locations for\ninstalling their service (i.e., placement) and the amount of computing resource\nto purchase from each location (i.e., sizing). The service placement and sizing\nsolution of the proposed model can hedge against any possible realization\nwithin the uncertainty set of traffic demand. Given the first-stage robust\nsolution, the optimal resource and workload allocation decisions are computed\nin the second-stage after the uncertainty is revealed. To solve the two-stage\nmodel, in this paper, we present an iterative solution by employing the\ncolumn-and-constraint generation method that decomposes the underlying problem\ninto a master problem and a max-min subproblem associated with the second\nstage. Extensive numerical results are shown to illustrate the effectiveness of\nthe proposed two-stage robust optimization model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 00:22:14 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 13:22:27 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Nguyen", "Duong Tung", ""], ["Nguyen", "Hieu Trung", ""], ["Trieu", "Ni", ""], ["Bhargava", "Vijay K.", ""]]}, {"id": "2004.13284", "submitter": "EPTCS", "authors": "Hugues Evrard (Google)", "title": "Modeling the Raft Distributed Consensus Protocol in LNT", "comments": "In Proceedings MARS 2020, arXiv:2004.12403", "journal-ref": "EPTCS 316, 2020, pp. 15-39", "doi": "10.4204/EPTCS.316.2", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus protocols are crucial for reliable distributed systems as they let\nthem cope with network and server failures. For decades, most consensus\nprotocols have been designed as variations of the seminal Paxos, yet in 2014\nRaft was presented as a new, \"understandable\" protocol, meant to be easier to\nimplement than the notoriously subtle Paxos family. Raft has since been used in\nvarious industrial projects, e.g. Hashicorp's Consul or etcd (used by Google's\nKubernetes). The correctness of Raft is established via a manual proof, based\non a TLA+ specification of the protocol. This paper reports our experience in\nmodeling Raft in the LNT process algebra. We found a couple of issues with the\noriginal TLA+ specification of Raft, which has been corrected since. More\ngenerally, this exercise offers a great opportunity to discuss how to best use\nthe features of the LNT formal language and the associated CADP verification\ntoolbox to model distributed protocols, including network and server failures.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:21:37 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Evrard", "Hugues", "", "Google"]]}, {"id": "2004.13328", "submitter": "Manish Shukla", "authors": "Manish Shukla, Rajan M A, Sachin Lodha, Gautam Shroff, Ramesh Raskar", "title": "Privacy Guidelines for Contact Tracing Applications", "comments": "10 pages, 0 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contact tracing is a very powerful method to implement and enforce social\ndistancing to avoid spreading of infectious diseases. The traditional approach\nof contact tracing is time consuming, manpower intensive, dangerous and prone\nto error due to fatigue or lack of skill. Due to this there is an emergence of\nmobile based applications for contact tracing. These applications primarily\nutilize a combination of GPS based absolute location and Bluetooth based\nrelative location remitted from user's smartphone to infer various insights.\nThese applications have eased the task of contact tracing; however, they also\nhave severe implication on user's privacy, for example, mass surveillance,\npersonal information leakage and additionally revealing the behavioral patterns\nof the user. This impact on user's privacy leads to trust deficit in these\napplications, and hence defeats their purpose.\n  In this work we discuss the various scenarios which a contact tracing\napplication should be able to handle. We highlight the privacy handling of some\nof the prominent contact tracing applications. Additionally, we describe the\nvarious threat actors who can disrupt its working, or misuse end user's data,\nor hamper its mass adoption. Finally, we present privacy guidelines for contact\ntracing applications from different stakeholder's perspective. To best of our\nknowledge, this is the first generic work which provides privacy guidelines for\ncontact tracing applications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 06:44:14 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Shukla", "Manish", ""], ["A", "Rajan M", ""], ["Lodha", "Sachin", ""], ["Shroff", "Gautam", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2004.13336", "submitter": "Yuanzhong Xu", "authors": "Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake\n  Hechtman, Shibo Wang", "title": "Automatic Cross-Replica Sharding of Weight Update in Data-Parallel\n  Training", "comments": "12 pages, 23 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data-parallel synchronous training of deep neural networks, different\ndevices (replicas) run the same program with different partitions of the\ntraining batch, but weight update computation is repeated on all replicas,\nbecause the weights do not have a batch dimension to partition. This can be a\nbottleneck for performance and scalability in typical language models with\nlarge weights, and models with small per-replica batch size which is typical in\nlarge-scale training. This paper presents an approach to automatically shard\nthe weight update computation across replicas with efficient communication\nprimitives and data formatting, using static analysis and transformations on\nthe training computation graph. We show this technique achieves substantial\nspeedups on typical image and language models on Cloud TPUs, requiring no\nchange to model code. This technique helps close the gap between traditionally\nexpensive (ADAM) and cheap (SGD) optimizers, as they will only take a small\npart of training step time and have similar peak memory usage. It helped us to\nachieve state-of-the-art training performance in Google's MLPerf 0.6\nsubmission.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 07:13:50 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Xu", "Yuanzhong", ""], ["Lee", "HyoukJoong", ""], ["Chen", "Dehao", ""], ["Choi", "Hongjun", ""], ["Hechtman", "Blake", ""], ["Wang", "Shibo", ""]]}, {"id": "2004.13373", "submitter": "Maximilian H\\\"ob", "authors": "Maximilian H\\\"ob and Dieter Kranzlm\\\"uller", "title": "Enabling EASEY deployment of containerized applications for future HPC\n  systems", "comments": "International Conference on Computational Science ICCS2020, 13 pages", "journal-ref": "ICCS 2020: Computational Science 206-219", "doi": "10.1007/978-3-030-50371-0_15", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The upcoming exascale era will push the changes in computing architecture\nfrom classical CPU-based systems in hybrid GPU-heavy systems with much higher\nlevels of complexity. While such clusters are expected to improve the\nperformance of certain optimized HPC applications, it will also increase the\ndifficulties for those users who have yet to adapt their codes or are starting\nfrom scratch with new programming paradigms. Since there are still no\ncomprehensive automatic assistance mechanisms to enhance application\nperformance on such systems, we are proposing a support framework for future\nHPC architectures, called EASEY (Enable exASclae for EverYone). The solution\nbuilds on a layered software architecture, which offers different mechanisms on\neach layer for different tasks of tuning. This enables users to adjust the\nparameters on each of the layers, thereby enhancing specific characteristics of\ntheir codes. We introduce the framework with a Charliecloud-based solution,\nshowcasing the LULESH benchmark on the upper layers of our framework. Our\napproach can automatically deploy optimized container computations with\nnegligible overhead and at the same time reduce the time a scientist needs to\nspent on manual job submission configurations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:05:47 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 08:01:21 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["H\u00f6b", "Maximilian", ""], ["Kranzlm\u00fcller", "Dieter", ""]]}, {"id": "2004.13409", "submitter": "Andreas Penzkofer", "authors": "Andreas Penzkofer, Bartosz Kusmierz, Angelo Capossele, William\n  Sanders, Olivia Saa", "title": "Parasite Chain Detection in the IOTA Protocol", "comments": "Accepted for publication at Tokenomics 2020, Toulouse, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years several distributed ledger technologies based on directed\nacyclic graphs (DAGs) have appeared on the market. Similar to blockchain\ntechnologies, DAG-based systems aim to build an immutable ledger and are faced\nwith security concerns regarding the irreversibility of the ledger state.\nHowever, due to their more complex nature and recent popularity, the study of\nadversarial actions has received little attention so far. In this paper we are\nconcerned with a particular type of attack on the IOTA cryptocurrency, more\nspecifically a Parasite Chain attack that attempts to revert the history stored\nin the DAG structure, also called the Tangle.\n  In order to improve the security of the Tangle, we present a detection\nmechanism for this type of attack. In this mechanism, we embrace the complexity\nof the DAG structure by sampling certain aspects of it, more particularly the\ndistribution of the number of approvers. We initially describe models that\npredict the distribution that should be expected for a Tangle without any\nmalicious actors. We then introduce metrics that compare this reference\ndistribution with the measured distribution. Upon detection, measures can then\nbe taken to render the attack unsuccessful. We show that due to a form of the\nParasite Chain that is different from the main Tangle it is possible to detect\ncertain types of malicious chains. We also show that although the attacker may\nchange the structure of the Parasite Chain to avoid detection, this is done so\nat a significant cost since the attack is rendered less efficient.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 10:33:02 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Penzkofer", "Andreas", ""], ["Kusmierz", "Bartosz", ""], ["Capossele", "Angelo", ""], ["Sanders", "William", ""], ["Saa", "Olivia", ""]]}, {"id": "2004.13475", "submitter": "Crist\\'obal A. Navarro", "authors": "Crist\\'obal A. Navarro, Felipe A. Quezada, Nancy Hitschfeld, Raimundo\n  Vega, Benjamin Bustos", "title": "Efficient GPU Thread Mapping on Embedded 2D Fractals", "comments": "20 Pages. arXiv admin note: text overlap with arXiv:1706.04552", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new approach for mapping GPU threads onto a family of\ndiscrete embedded 2D fractals. A block-space map $\\lambda:\n\\mathbb{Z}_{\\mathbb{E}}^{2} \\mapsto \\mathbb{Z}_{\\mathbb{F}}^{2}$ is proposed,\nfrom Euclidean parallel space $\\mathbb{E}$ to embedded fractal space\n$\\mathbb{F}$, that maps in $\\mathcal{O}(\\log_2 \\log_2(n))$ time and uses no\nmore than $\\mathcal{O}(n^\\mathbb{H})$ threads with $\\mathbb{H}$ being the\nHausdorff dimension of the fractal, making it parallel space efficient. When\ncompared to a bounding-box (BB) approach, $\\lambda(\\omega)$ offers a\nsub-exponential improvement in parallel space and a monotonically increasing\nspeedup $n \\ge n_0$. The Sierpinski gasket fractal is used as a particular case\nstudy and the experimental performance results show that $\\lambda(\\omega)$\nreaches up to $9\\times$ of speedup over the bounding-box approach. A\ntensor-core based implementation of $\\lambda(\\omega)$ is also proposed for\nmodern GPUs, providing up to $\\sim40\\%$ of extra performance. The results\nobtained in this work show that doing efficient GPU thread mapping on fractal\ndomains can significantly improve the performance of several applications that\nwork with this type of geometry.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:33:25 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Navarro", "Crist\u00f3bal A.", ""], ["Quezada", "Felipe A.", ""], ["Hitschfeld", "Nancy", ""], ["Vega", "Raimundo", ""], ["Bustos", "Benjamin", ""]]}, {"id": "2004.13653", "submitter": "Wen Liu", "authors": "Yu Huang, Yan Li, Zhaofeng Zhang, Ryan Wen Liu", "title": "GPU-Accelerated Compression and Visualization of Large-Scale Vessel\n  Trajectories in Maritime IoT Industries", "comments": "19 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic identification system (AIS), an automatic vessel-tracking\nsystem, has been widely adopted to perform intelligent traffic management and\ncollision avoidance services in maritime Internet of Things (IoT) industries.\nWith the rapid development of maritime transportation, tremendous numbers of\nAIS-based vessel trajectory data have been collected, which make trajectory\ndata compression imperative and challenging. This paper mainly focuses on the\ncompression and visualization of large-scale vessel trajectories and their\nGraphics Processing Unit (GPU)-accelerated implementations. The visualization\nwas implemented to investigate the influence of compression on vessel\ntrajectory data quality. In particular, the Douglas-Peucker (DP) and Kernel\nDensity Estimation (KDE) algorithms, respectively utilized for trajectory\ncompression and visualization, were significantly accelerated through the\nmassively parallel computation capabilities of GPU architecture. Comprehensive\nexperiments on trajectory compression and visualization have been conducted on\nlarge-scale AIS data of recording ship movements collected from 3 different\nwater areas, i.e., the South Channel of Yangtze River Estuary, the Chengshan\nJiao Promontory, and the Zhoushan Islands. Experimental results illustrated\nthat (1) the proposed GPU-based parallel implementation frameworks could\nsignificantly reduce the computational time for both trajectory compression and\nvisualization; (2) the influence of compressed vessel trajectories on\ntrajectory visualization could be negligible if the compression threshold was\nselected suitably; (3) the Gaussian kernel was capable of generating more\nappropriate KDE-based visualization performance by comparing with other seven\nkernel functions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 16:34:49 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Huang", "Yu", ""], ["Li", "Yan", ""], ["Zhang", "Zhaofeng", ""], ["Liu", "Ryan Wen", ""]]}, {"id": "2004.13907", "submitter": "Santosh Nagarakatte", "authors": "Mohammadreza Soltaniyeh, Richard P. Martin, and Santosh Nagarakatte", "title": "Synergistic CPU-FPGA Acceleration of Sparse Linear Algebra", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": "Rutgers Computer Science Technical Report DCS-TR-750", "categories": "cs.DC cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes REAP, a software-hardware approach that enables high\nperformance sparse linear algebra computations on a cooperative CPU-FPGA\nplatform. REAP carefully separates the task of organizing the matrix elements\nfrom the computation phase. It uses the CPU to provide a first-pass\nre-organization of the matrix elements, allowing the FPGA to focus on the\ncomputation. We introduce a new intermediate representation that allows the CPU\nto communicate the sparse data and the scheduling decisions to the FPGA. The\ncomputation is optimized on the FPGA for effective resource utilization with\npipelining. REAP improves the performance of Sparse General Matrix\nMultiplication (SpGEMM) and Sparse Cholesky Factorization by 3.2X and 1.85X\ncompared to widely used sparse libraries for them on the CPU, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 01:06:52 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Soltaniyeh", "Mohammadreza", ""], ["Martin", "Richard P.", ""], ["Nagarakatte", "Santosh", ""]]}, {"id": "2004.13926", "submitter": "Shuo Wan", "authors": "Shuo Wan, Jiaxun Lu, Pingyi Fan and Khaled B. Letaief", "title": "Intelligent networking with Mobile Edge Computing: Vision and Challenges\n  for Dynamic Network Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) has been considered as a promising technique for\ninternet of things (IoT). By deploying edge servers at the proximity of\ndevices, it is expected to provide services and process data at a relatively\nlow delay by intelligent networking. However, the vast edge servers may face\ngreat challenges in terms of cooperation and resource allocation. Furthermore,\nintelligent networking requires online implementation in distributed mode. In\nsuch kinds of systems, the network scheduling can not follow any previously\nknown rule due to complicated application environment. Then statistical\nlearning rises up as a promising technique for network scheduling, where edges\ndynamically learn environmental elements with cooperations. It is expected such\nlearning based methods may relieve deficiency of model limitations, which\nenhance their practical use in dynamic network scheduling. In this paper, we\ninvestigate the vision and challenges of the intelligent IoT networking with\nmobile edge computing. From the systematic viewpoint, some major research\nopportunities are enumerated with respect to statistical learning.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 02:19:24 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wan", "Shuo", ""], ["Lu", "Jiaxun", ""], ["Fan", "Pingyi", ""], ["Letaief", "Khaled B.", ""]]}, {"id": "2004.13997", "submitter": "Jorge Pe\\~na Queralta", "authors": "Jorge Pe\\~na Queralta, Li Qingqing, Tuan Nguyen Gia, Hong-Linh Truong,\n  Tomi Westerlund", "title": "End-to-End Design for Self-Reconfigurable Heterogeneous Robotic Swarms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More widespread adoption requires swarms of robots to be more flexible for\nreal-world applications. Multiple challenges remain in complex scenarios where\na large amount of data needs to be processed in real-time and high degrees of\nsituational awareness are required. The options in this direction are limited\nin existing robotic swarms, mostly homogeneous robots with limited operational\nand reconfiguration flexibility. We address this by bringing elastic computing\ntechniques and dynamic resource management from the edge-cloud computing domain\nto the swarm robotics domain. This enables the dynamic provisioning of\ncollective capabilities in the swarm for different applications. Therefore, we\ntransform a swarm into a distributed sensing and computing platform capable of\ncomplex data processing tasks, which can then be offered as a service. In\nparticular, we discuss how this can be applied to adaptive resource management\nin a heterogeneous swarm of drones, and how we are implementing the dynamic\ndeployment of distributed data processing algorithms. With an elastic drone\nswarm built on reconfigurable hardware and containerized services, it will be\npossible to raise the self-awareness, degree of intelligence, and level of\nautonomy of heterogeneous swarms of robots. We describe novel directions for\ncollaborative perception, and new ways of interacting with a robotic swarm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 07:35:11 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Queralta", "Jorge Pe\u00f1a", ""], ["Qingqing", "Li", ""], ["Gia", "Tuan Nguyen", ""], ["Truong", "Hong-Linh", ""], ["Westerlund", "Tomi", ""]]}, {"id": "2004.14020", "submitter": "Sangeetha Abdu Jyothi", "authors": "Sayed Hadi Hashemi, Sangeetha Abdu Jyothi, Brighten Godfrey, Roy\n  Campbell", "title": "Caramel: Accelerating Decentralized Distributed Deep Learning with\n  Computation Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of choice for parameter aggregation in Deep Neural Network (DNN)\ntraining, a network-intensive task, is shifting from the Parameter Server model\nto decentralized aggregation schemes (AllReduce) inspired by theoretical\nguarantees of better performance. However, current implementations of AllReduce\noverlook the interdependence of communication and computation, resulting in\nsignificant performance degradation. In this paper, we develop Caramel, a\nsystem that accelerates decentralized distributed deep learning through\nmodel-aware computation scheduling and communication optimizations for\nAllReduce. Caramel achieves this goal through (a) computation DAG scheduling\nthat expands the feasible window of transfer for each parameter (transfer\nboundaries), and (b) network optimizations for smoothening of the load\nincluding adaptive batching and pipelining of parameter transfers. Caramel\nmaintains the correctness of the dataflow model, is hardware-independent, and\ndoes not require any user-level or framework-level changes. We implement\nCaramel over TensorFlow and show that the iteration time of DNN training can be\nimproved by up to 3.62x in a cloud environment.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:32:33 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Hashemi", "Sayed Hadi", ""], ["Jyothi", "Sangeetha Abdu", ""], ["Godfrey", "Brighten", ""], ["Campbell", "Roy", ""]]}, {"id": "2004.14072", "submitter": "Nitin Shivaraman", "authors": "Nitin Shivaraman, Saravanan Ramanathan, Shreejith Shanker, Arvind\n  Easwaran, Sebastian Steinhorst", "title": "DeCoRIC: Decentralized Connected Resilient IoT Clustering", "comments": "10 pages, 8 figures, 3 tables, accepted in ICCCN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining peer-to-peer connectivity with low energy overhead is a key\nrequirement for several emerging Internet of Things (IoT) applications. It is\nalso desirable to develop such connectivity solutions for non-static network\ntopologies, so that resilience to device failures can be fully realized.\nDecentralized clustering has emerged as a promising technique to address this\ncritical challenge. Clustering of nodes around cluster heads (CHs) provides an\nenergy-efficient two-tier framework for peer-to-peer communication. At the same\ntime, decentralization ensures that the framework can quickly adapt to a\ndynamically changing network topology. Although some decentralized clustering\nsolutions have been proposed in the literature, they either lack guarantees on\nconnectivity or incur significant energy overhead to maintain the clusters. In\nthis paper, we present Decentralized Connected Resilient IoT Clustering\n(DeCoRIC), an energy-efficient clustering scheme that is self-organizing and\nresilient to network changes while guaranteeing connectivity. Using experiments\nimplemented on the Contiki simulator, we show that our clustering scheme adapts\nitself to node faults in a time-bound manner. Our experiments show that DeCoRIC\nachieves 100% connectivity among all nodes while improving the power efficiency\nof nodes in the system compared to the state-of-the-art techniques BEEM and\nLEACH by up to 110% and 70%, respectively. The improved power efficiency also\ntranslates to longer lifetime before first node death with a best-case of 109%\nlonger than BEEM and 42% longer than LEACH.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:51:28 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Shivaraman", "Nitin", ""], ["Ramanathan", "Saravanan", ""], ["Shanker", "Shreejith", ""], ["Easwaran", "Arvind", ""], ["Steinhorst", "Sebastian", ""]]}, {"id": "2004.14093", "submitter": "Aznam Yacoub", "authors": "Aznam Yacoub", "title": "Virtual Communication Stack: Towards Building Integrated Simulator of\n  Mobile Ad Hoc Network-based Infrastructure for Disaster Response Scenarios", "comments": "Preprint. Unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Responses to disastrous events are a challenging problem, because of possible\ndamages on communication infrastructures. For instance, after a natural\ndisaster, infrastructures might be entirely destroyed. Different network\nparadigms were proposed in the literature in order to deploy adhoc network, and\nallow dealing with the lack of communications. However, all these solutions\nfocus only on the performance of the network itself, without taking into\naccount the specificities and heterogeneity of the components which use it.\nThis comes from the difficulty to integrate models with different levels of\nabstraction. Consequently, verification and validation of adhoc protocols\ncannot guarantee that the different systems will work as expected in\noperational conditions. However, the DEVS theory provides some mechanisms to\nallow integration of models with different natures. This paper proposes an\nintegrated simulation architecture based on DEVS which improves the accuracy of\nad hoc infrastructure simulators in the case of disaster response scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 11:33:11 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Yacoub", "Aznam", ""]]}, {"id": "2004.14180", "submitter": "Li Shen", "authors": "Congliang Chen, Li Shen, Haozhi Huang, and Wei Liu", "title": "Quantized Adam with Error Feedback", "comments": "Accepted to ACM Transactions on Intelligent Systems and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a distributed variant of adaptive stochastic\ngradient method for training deep neural networks in the parameter-server\nmodel. To reduce the communication cost among the workers and server, we\nincorporate two types of quantization schemes, i.e., gradient quantization and\nweight quantization, into the proposed distributed Adam. Besides, to reduce the\nbias introduced by quantization operations, we propose an error-feedback\ntechnique to compensate for the quantized gradient. Theoretically, in the\nstochastic nonconvex setting, we show that the distributed adaptive gradient\nmethod with gradient quantization and error-feedback converges to the\nfirst-order stationary point, and that the distributed adaptive gradient method\nwith weight quantization and error-feedback converges to the point related to\nthe quantized level under both the single-worker and multi-worker modes. At\nlast, we apply the proposed distributed adaptive gradient methods to train deep\nneural networks. Experimental results demonstrate the efficacy of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:21:54 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 04:41:03 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Congliang", ""], ["Shen", "Li", ""], ["Huang", "Haozhi", ""], ["Liu", "Wei", ""]]}, {"id": "2004.14527", "submitter": "Alysson Bessani", "authors": "Alysson Bessani, Eduardo Alchieri, Jo\\~ao Sousa, Andr\\'e Oliveira,\n  Fernando Pedone", "title": "From Byzantine Replication to Blockchain: Consensus is only the\n  Beginning", "comments": "This is a preprint of a paper to appear on the 50th IEEE/IFIP Int.\n  Conf. on Dependable Systems and Networks (DSN'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularization of blockchains leads to a resurgence of interest in\nByzantine Fault-Tolerant (BFT) state machine replication protocols. However,\nmuch of the work on this topic focuses on the underlying consensus protocols,\nwith emphasis on their lack of scalability, leaving other subtle limitations\nunaddressed. These limitations are related to the effects of maintaining a\ndurable blockchain instead of a write-ahead log and the requirement for\nreconfiguring the set of replicas in a decentralized way. We demonstrate these\nlimitations using a digital coin blockchain application and BFT-SMaRt, a\npopular BFT replication library. We show how they can be addressed both at a\nconceptual level, in a protocol-agnostic way, and by implementing SMaRtChain, a\nblockchain platform based on BFT-SMaRt. SMaRtChain improves the performance of\nour digital coin application by a factor of eight when compared with a naive\nimplementation on top of BFT-SMaRt. Moreover, SMaRtChain achieves a throughput\n$8\\times$ and $33\\times$ better than Tendermint and Hyperledger Fabric,\nrespectively, when ensuring strong durability on its blockchain.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:27:50 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Bessani", "Alysson", ""], ["Alchieri", "Eduardo", ""], ["Sousa", "Jo\u00e3o", ""], ["Oliveira", "Andr\u00e9", ""], ["Pedone", "Fernando", ""]]}, {"id": "2004.14559", "submitter": "Saravanan Ramanathan", "authors": "Saravanan Ramanathan (1), Nitin Shivaraman (1), Seima Suryasekaran\n  (1), Arvind Easwaran (2), Etienne Borde (3), Sebastian Steinhorst (4) ((1)\n  TUMCREATE Ltd., Singapore, (2) Nanyang Technological University, Singapore,\n  (3) Telecom Paris, France, (4) Technische Universitat Munchen, Germany)", "title": "A Survey on Time-Sensitive Resource Allocation in the Cloud Continuum", "comments": "15 pages. A version submitted to Information Technology | De Gruyter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) and Internet of Things (IoT) applications are\nrapidly growing in today's world where they are continuously connected to the\ninternet and process, store and exchange information among the devices and the\nenvironment. The cloud and edge platform is very crucial to these applications\ndue to their inherent compute-intensive and resource-constrained nature. One of\nthe foremost challenges in cloud and edge resource allocation is the efficient\nmanagement of computation and communication resources to meet the performance\nand latency guarantees of the applications. The heterogeneity of cloud\nresources (processors, memory, storage, bandwidth), variable cost structure and\nunpredictable workload patterns make the design of resource allocation\ntechniques complex. Numerous research studies have been carried out to address\nthis intricate problem. In this paper, the current state-of-the-art resource\nallocation techniques for the cloud continuum, in particular those that\nconsider time-sensitive applications, are reviewed. Furthermore, we present the\nkey challenges in the resource allocation problem for the cloud continuum, a\ntaxonomy to classify the existing literature and the potential research gaps.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:25:54 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ramanathan", "Saravanan", ""], ["Shivaraman", "Nitin", ""], ["Suryasekaran", "Seima", ""], ["Easwaran", "Arvind", ""], ["Borde", "Etienne", ""], ["Steinhorst", "Sebastian", ""]]}, {"id": "2004.14591", "submitter": "Xiao Fan Liu", "authors": "Ying-Hao Zhang and Xiao Fan Liu", "title": "Satellite Broadcasting Enabled Blockchain Protocol: A Preliminary Study", "comments": "Accepted by 2020 Information Communication Technologies Conference\n  (ICTC 2020)", "journal-ref": null, "doi": "10.1109/ICTC49638.2020.9123248", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low throughput has been the biggest obstacle of large-scale blockchain\napplications. During the past few years, researchers have proposed various\nschemes to improve the systems' throughput. However, due to the inherent\ninefficiency and defects of the Internet, especially in data broadcasting\ntasks, these efforts all rendered unsatisfactory. In this paper, we propose a\nnovel blockchain protocol which utilizes the satellite broadcasting network\ninstead of the traditional Internet for data broadcasting and consensus tasks.\nAn automatic resumption mechanism is also proposed to solve the unique\ncommunication problems of satellite broadcasting. Simulation results show that\nthe proposed algorithm has a lower communication cost and can greatly improve\nthe throughput of the blockchain system. Theoretical estimation of a satellite\nbroadcasting enabled blockchain system's throughput is 6,000,000 TPS with a 20\ngbps satellite bandwidth.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 05:46:45 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zhang", "Ying-Hao", ""], ["Liu", "Xiao Fan", ""]]}, {"id": "2004.14639", "submitter": "Yu Su", "authors": "Yu Su, Xiaoqi Ren, Shai Vardi, Adam Wierman", "title": "Communication-Aware Scheduling of Precedence-Constrained Tasks on\n  Related Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling precedence-constrained tasks is a classical problem that has been\nstudied for more than fifty years. However, little progress has been made in\nthe setting where there are communication delays between tasks. Results for the\ncase of identical machines were derived nearly thirty years ago, and yet no\nresults for related machines have followed. In this work, we propose a new\nscheduler, Generalized Earliest Time First (GETF), and provide the first\nprovable, worst-case approximation guarantees for the goals of minimizing both\nthe makespan and total weighted completion time of tasks with precedence\nconstraints on related machines with machine-dependent communication times.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 08:56:48 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Su", "Yu", ""], ["Ren", "Xiaoqi", ""], ["Vardi", "Shai", ""], ["Wierman", "Adam", ""]]}, {"id": "2004.14696", "submitter": "Chuan Xu", "authors": "Chuan Xu, Giovanni Neglia, Nicola Sebastianelli", "title": "Dynamic backup workers for parallel machine learning", "comments": "Journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular framework for distributed training of machine learning\nmodels is the (synchronous) parameter server (PS). This paradigm consists of\n$n$ workers, which iteratively compute updates of the model parameters, and a\nstateful PS, which waits and aggregates all updates to generate a new estimate\nof model parameters and sends it back to the workers for a new iteration.\nTransient computation slowdowns or transmission delays can intolerably lengthen\nthe time of each iteration. An efficient way to mitigate this problem is to let\nthe PS wait only for the fastest $n-b$ updates, before generating the new\nparameters. The slowest $b$ workers are called backup workers. The optimal\nnumber $b$ of backup workers depends on the cluster configuration and workload,\nbut also (as we show in this paper) on the hyper-parameters of the learning\nalgorithm and the current stage of the training. We propose DBW, an algorithm\nthat dynamically decides the number of backup workers during the training\nprocess to maximize the convergence speed at each iteration. Our experiments\nshow that DBW 1) removes the necessity to tune $b$ by preliminary\ntime-consuming experiments, and 2) makes the training up to a factor $3$ faster\nthan the optimal static configuration.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 11:25:00 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 01:35:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Xu", "Chuan", ""], ["Neglia", "Giovanni", ""], ["Sebastianelli", "Nicola", ""]]}, {"id": "2004.14746", "submitter": "Sarath Pathari", "authors": "Sarath Pathari", "title": "Cloud+: A safe and restrained data access control program for cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": "V5I4-1314", "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure distributed storage, which is a rising cloud administration, is\nplanned to guarantee the mystery of re-appropriated data yet also to give\nversatile data access to cloud customers whose data is out of physical control.\nCiphertext-Policy Attribute-Based Encryption (CP-ABE) is seen as a champion\namong the most reassuring frameworks that may be used to verify the\nconfirmation of the administration. Be that as it may, the use of CP-ABE may\nyield an unavoidable security burst which is known as the abuse of access\naccreditation (for example decoding right). In this paper, we look at the two\nessential occurrences of access accreditation misuse: one is on the\nsemi-believed specialist side, and the other is supportive of the cloud\ncustomer. To ease the abuse, we propose revocable CP-ABE based distributed\nstorage structure with express renouncing, planned information getting to and\nnumerous examining capacities alluded as Cloud+.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:05:49 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Pathari", "Sarath", ""]]}, {"id": "2004.14850", "submitter": "Ella Peltonen", "authors": "Ella Peltonen, Mehdi Bennis, Michele Capobianco, Merouane Debbah,\n  Aaron Ding, Felipe Gil-Casti\\~neira, Marko Jurmu, Teemu Karvonen, Markus\n  Kelanti, Adrian Kliks, Teemu Lepp\\\"anen, Lauri Lov\\'en, Tommi Mikkonen,\n  Ashwin Rao, Sumudu Samarakoon, Kari Sepp\\\"anen, Pawe{\\l} Sroka, Sasu Tarkoma,\n  Tingting Yang", "title": "6G White Paper on Edge Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this white paper we provide a vision for 6G Edge Intelligence. Moving\ntowards 5G and beyond the future 6G networks, intelligent solutions utilizing\ndata-driven machine learning and artificial intelligence become crucial for\nseveral real-world applications including but not limited to, more efficient\nmanufacturing, novel personal smart device environments and experiences, urban\ncomputing and autonomous traffic settings. We present edge computing along with\nother 6G enablers as a key component to establish the future 2030 intelligent\nInternet technologies as shown in this series of 6G White Papers.\n  In this white paper, we focus in the domains of edge computing infrastructure\nand platforms, data and edge network management, software development for edge,\nand real-time and distributed training of ML/AI algorithms, along with\nsecurity, privacy, pricing, and end-user aspects. We discuss the key enablers\nand challenges and identify the key research questions for the development of\nthe Intelligent Edge services. As a main outcome of this white paper, we\nenvision a transition from Internet of Things to Intelligent Internet of\nIntelligent Things and provide a roadmap for development of 6G Intelligent\nEdge.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:02:08 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Peltonen", "Ella", ""], ["Bennis", "Mehdi", ""], ["Capobianco", "Michele", ""], ["Debbah", "Merouane", ""], ["Ding", "Aaron", ""], ["Gil-Casti\u00f1eira", "Felipe", ""], ["Jurmu", "Marko", ""], ["Karvonen", "Teemu", ""], ["Kelanti", "Markus", ""], ["Kliks", "Adrian", ""], ["Lepp\u00e4nen", "Teemu", ""], ["Lov\u00e9n", "Lauri", ""], ["Mikkonen", "Tommi", ""], ["Rao", "Ashwin", ""], ["Samarakoon", "Sumudu", ""], ["Sepp\u00e4nen", "Kari", ""], ["Sroka", "Pawe\u0142", ""], ["Tarkoma", "Sasu", ""], ["Yang", "Tingting", ""]]}]