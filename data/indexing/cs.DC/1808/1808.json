[{"id": "1808.00079", "submitter": "Jianwei Feng", "authors": "Jianwei Feng and Dong Huang", "title": "Optimal Gradient Checkpoint Search for Arbitrary Computation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks(DNNs) require huge GPU memory when training on modern\nimage/video databases. Unfortunately, the GPU memory is physically finite,\nwhich limits the image resolutions and batch sizes that could be used in\ntraining for better DNN performance. Unlike solutions that require physically\nupgrade GPUs, the Gradient CheckPointing(GCP) training trades computation for\nmore memory beyond existing GPU hardware. GCP only stores a subset of\nintermediate tensors, called Gradient Checkpoints (GCs), during forward. Then\nduring backward, extra local forwards are conducted to compute the missing\ntensors. The total training memory cost becomes the sum of (1) the memory cost\nof the gradient checkpoints and (2) the maximum memory cost of local forwards.\nTo achieve maximal memory cut-offs, one needs optimal algorithms to select GCs.\nExisting GCP approaches rely on either manual input of GCs or heuristics-based\nGC search on Linear Computation Graphs (LCGs), and cannot apply to Arbitrary\nComputation Graphs(ACGs). In this paper, we present theories and optimal\nalgorithms on GC selection that, for the first time, are applicable to ACGs and\nachieve the maximal memory cut-offs. Extensive experiments show that our\napproach not only outperforms existing approaches (only applicable on LCGs),\nand is applicable to a vast family of LCG and ACG networks, such as Alexnet,\nVGG, ResNet, Densenet, Inception Net and highly complicated DNNs by Network\nArchitecture Search. Our work enables GCP training on ACGs, and cuts off up-to\n80% of training memory with a moderate time overhead (~30%-50%). Codes are\navailable\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 21:52:08 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 21:43:43 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 01:27:21 GMT"}, {"version": "v4", "created": "Mon, 23 Sep 2019 23:18:18 GMT"}, {"version": "v5", "created": "Tue, 2 Mar 2021 03:38:11 GMT"}, {"version": "v6", "created": "Thu, 18 Mar 2021 04:32:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Feng", "Jianwei", ""], ["Huang", "Dong", ""]]}, {"id": "1808.00117", "submitter": "Rohan Garg", "authors": "Rohan Garg, Apoore Mohan, Michael Sullivan, Gene Cooperman", "title": "CRUM: Checkpoint-Restart Support for CUDA's Unified Memory", "comments": "22 pages; 5 figures; accepted at IEEE Cluster-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unified Virtual Memory (UVM) was recently introduced on recent NVIDIA GPUs.\nThrough software and hardware support, UVM provides a coherent shared memory\nacross the entire heterogeneous node, migrating data as appropriate. The older\nCUDA programming style is akin to older large-memory UNIX applications which\nused to directly load and unload memory segments. Newer CUDA programs have\nstarted taking advantage of UVM for the same reasons of superior\nprogrammability that UNIX applications long ago switched to assuming the\npresence of virtual memory. Therefore, checkpointing of UVM will become\nincreasingly important, especially as NVIDIA CUDA continues to gain wider\npopularity: 87 of the top 500 supercomputers in the latest listings are\nGPU-accelerated, with a current trend of ten additional GPU-based\nsupercomputers each year.\n  A new scalable checkpointing mechanism, CRUM (Checkpoint-Restart for Unified\nMemory), is demonstrated for hybrid CUDA/MPI computations across multiple\ncomputer nodes. CRUM supports a fast, forked checkpointing, which mostly\noverlaps the CUDA computation with storage of the checkpoint image in stable\nstorage. The runtime overhead of using CRUM is 6% on average, and the time for\nforked checkpointing is seen to be a factor of up to 40 times less than\ntraditional, synchronous checkpointing.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 00:13:44 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Garg", "Rohan", ""], ["Mohan", "Apoore", ""], ["Sullivan", "Michael", ""], ["Cooperman", "Gene", ""]]}, {"id": "1808.00120", "submitter": "Guodong Shi", "authors": "Yang Liu, Junfeng Wu, Ian Manchester, Guodong Shi", "title": "Dynamical Privacy in Distributed Computing -- Part II: PPSC Gossip\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first part of the paper, we have studied the computational privacy\nrisks in distributed computing protocols against local or global dynamics\neavesdroppers, and proposed a Privacy-Preserving-Summation-Consistent (PPSC)\nmechanism as a generic privacy encryption subroutine for consensus-based\ndistributed computations. In this part of this paper, we show that the\nconventional deterministic and random gossip algorithms can be used to realize\nthe PPSC mechanism over a given network. At each time step, a node is selected\nto interact with one of its neighbors via deterministic or random gossiping.\nSuch node generates a random number as its new state, and sends the subtraction\nbetween its current state and that random number to the neighbor; then the\nneighbor updates its state by adding the received value to its current state.\nWe establish concrete privacy-preservation conditions by proving the\nimpossibility for the reconstruction of the network input from the output of\nthe gossip-based PPSC mechanism against eavesdroppers with full network\nknowledge, and by showing that the PPSC mechanism can achieve differential\nprivacy at arbitrary privacy levels. The convergence is characterized\nexplicitly and analytically for both deterministic and randomized gossiping,\nwhich is essentially achieved in a finite number of steps. Additionally, we\nillustrate that the proposed algorithms can be easily made adaptive in\nreal-world applications by making realtime trade-offs between resilience\nagainst node dropout or communication failure and privacy preservation\ncapabilities.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 00:31:15 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 09:27:40 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Liu", "Yang", ""], ["Wu", "Junfeng", ""], ["Manchester", "Ian", ""], ["Shi", "Guodong", ""]]}, {"id": "1808.00214", "submitter": "Yukiko Yamauchi", "authors": "Keisuke Doi, Yukiko Yamauchi, Shuji Kijima and Masafumi Yamashita", "title": "Exploration of Finite 2D Square Grid by a Metamorphic Robotic System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exploration of finite 2D square grid by a metamorphic robotic\nsystem consisting of anonymous oblivious modules. The number of possible shapes\nof a metamorphic robotic system grows as the number of modules increases. The\nshape of the system serves as its memory and shows its functionality. We\nconsider the effect of global compass on the minimum number of modules\nnecessary to explore a finite 2D square grid. We show that if the modules agree\non the directions (north, south, east, and west), three modules are necessary\nand sufficient for exploration from an arbitrary initial configuration,\notherwise five modules are necessary and sufficient for restricted initial\nconfigurations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 08:06:59 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 08:25:55 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Doi", "Keisuke", ""], ["Yamauchi", "Yukiko", ""], ["Kijima", "Shuji", ""], ["Yamashita", "Masafumi", ""]]}, {"id": "1808.00334", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan, Kashish Ara Shakil, Mansaf Alam", "title": "PABED A Tool for Big Education Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing and big data have risen to become the most popular\ntechnologies of the modern world. Apparently, the reason behind their immense\npopularity is their wide range of applicability as far as the areas of interest\nare concerned. Education and research remain one of the most obvious and\nbefitting application areas. This research paper introduces a big data\nanalytics tool, PABED Project Analyzing Big Education Data, for the education\nsector that makes use of cloud-based technologies. This tool is implemented\nusing Google BigQuery and R programming language and allows comparison of\nundergraduate enrollment data for different academic years. Although, there are\nmany proposed applications of big data in education, there is a lack of tools\nthat can actualize the concept into practice. PABED is an effort in this\ndirection. The implementation and testing details of the project have been\ndescribed in this paper. This tool validates the use of cloud computing and big\ndata technologies in education and shall head start development of more\nsophisticated educational intelligence tools.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 09:30:05 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Khan", "Samiya", ""], ["Shakil", "Kashish Ara", ""], ["Alam", "Mansaf", ""]]}, {"id": "1808.00348", "submitter": "Muhammad Bilal", "authors": "Muhammad Bilal", "title": "Network-Coding Approach for Information-Centric Networking", "comments": "10 pages, 8 figures, accepted for publication in a future issue of\n  the IEEE systems Journal", "journal-ref": "IEEE Systems Journal, vol. 13, no. 2, pp. 1376-1385, June 2019", "doi": "10.1109/JSYST.2018.2862913", "report-no": null, "categories": "cs.NI cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current internet architecture is inefficient in fulfilling the demands of\nnewly emerging internet applications. To address this issue, several\nover-the-top (OTT) application-level solutions have been employed, making the\noverall architecture very complex. Information-centric-networking (ICN)\narchitecture has emerged as a promising alternative solution. The ICN\narchitecture decouples the content from the host at the network level and\nsupports the temporary storage of content in an in-network cache.\nFundamentally, the ICN can be considered a multisource, multicast\ncontent-delivery solution. Because of the benefits of network coding in\nmulticasting scenarios and proven benefits in distributed storage networks, the\nnetwork coding is apt for the ICN architecture. In this study, we propose a\nsolvable linear network-coding scheme for the ICN architecture. We also propose\na practical implementation of the network-coding scheme for the ICN,\nparticularly for the content-centric network (CCN) architecture, which is\ntermed the coded CCN (CCCN). The performance results show that the\nnetwork-coding scheme improves the performance of the CCN and significantly\nreduces the network traffic and average download delay.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 14:56:59 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 07:42:25 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Bilal", "Muhammad", ""]]}, {"id": "1808.00386", "submitter": "Martin Bauer", "authors": "Erno Kovacs, Martin Bauer, Jaeho Kim, Jaeseok Yun, Franck Le Gall,\n  Mengxuan Zhao", "title": "Standards-Based Worldwide Semantic Interoperability for IoT", "comments": "European Union (EU), FP7 FI-CORE, grant agreement No 632893, Horizon\n  2020 FIESTA grant agreement No 643943, EU-South Korea, Horizon 2020 Wise-IoT\n  grant agreement 723156, South Korea, IITP, Korea government MSIP\n  No.B0184-15-1003", "journal-ref": "E. Kovacs, M. Bauer, J. Kim, J. Yun, F. Le Gall and M. Zhao,\n  \"Standards-Based Worldwide Semantic Interoperability for IoT,\" in IEEE\n  Communications Magazine, vol. 54, no. 12, pp. 40-46, December 2016", "doi": "10.1109/MCOM.2016.1600460CM", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global IoT services (GIoTS) are combining locally available IoT resources\nwith Cloud-based services. They are targeting world-wide services. GIoTS\nrequire interoperability between the locally installed heterogeneous IoT\nsystems. Semantic processing is an important technology to enable data\nmediation as well as knowledge-based processing. This paper explains a system\narchitecture for achieving world-wide semantic interoperability using\ninternational standards like oneM2M and the OMA NGSI-9/10 context interfaces\n(as used in the European Future Internet Platform FIWARE). Semantics also\nenables the use of Knowledge-based Semantic Processing Agents. Furthermore, we\nexplain how semantic verification enables the testing of such complex systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:44:55 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Kovacs", "Erno", ""], ["Bauer", "Martin", ""], ["Kim", "Jaeho", ""], ["Yun", "Jaeseok", ""], ["Gall", "Franck Le", ""], ["Zhao", "Mengxuan", ""]]}, {"id": "1808.00481", "submitter": "Yuanhao Wei", "authors": "Yuanhao Wei", "title": "Space Complexity of Implementing Large Shared Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove two new space lower bounds for the problem of implementing a large\nshared register using smaller physical shared registers. We focus on the case\nwhere both the implemented and physical registers are single-writer, which\nmeans they can be accessed concurrently by multiple readers but only by a\nsingle writer. To strengthen our lower bounds, we let the physical registers be\natomic and we only require the implemented register to be regular. Furthermore,\nthe lower bounds hold for obstruction-free implementations, which means they\nalso hold for lock-free and wait-free implementations.\n  If $m$ is the number values representable by the large register and $b$ is\nthe number of values representable by each physical register, our first lower\nbound says that any obstruction-free implementation that has an invisible\nreader requires at least $\\lceil \\frac{m-1}{b-1} \\rceil$ physical registers. A\nreader is considered invisible if it never writes to shared registers. This\nlower bound is tight for the invisible reader case. We also prove a $\\lceil\n\\min(\\frac{m-1}{b-1}, r+\\frac{\\log{m}}{\\log{b}}) \\rceil$ space lower bound for\nthe general case, which covers both visible and invisible readers. In this\nbound, $r$ represents the number of readers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 18:00:50 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Wei", "Yuanhao", ""]]}, {"id": "1808.00556", "submitter": "Maxim Belkin", "authors": "Maxim Belkin, Roland Haas, Galen Wesley Arnold, Hon Wai Leong, Eliu A.\n  Huerta, David Lesny, Mark Neubauer", "title": "Container solutions for HPC Systems: A Case Study of Using Shifter on\n  Blue Waters", "comments": "8 pages, 7 figures, in PEARC '18: Proceedings of Practice and\n  Experience in Advanced Research Computing, July 22--26, 2018, Pittsburgh, PA,\n  USA", "journal-ref": null, "doi": "10.1145/3219104.3219145", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Software container solutions have revolutionized application development\napproaches by enabling lightweight platform abstractions within the so-called\n\"containers.\" Several solutions are being actively developed in attempts to\nbring the benefits of containers to high-performance computing systems with\ntheir stringent security demands on the one hand and fundamental resource\nsharing requirements on the other.\n  In this paper, we discuss the benefits and short-comings of such solutions\nwhen deployed on real HPC systems and applied to production scientific\napplications.We highlight use cases that are either enabled by or significantly\nbenefit from such solutions. We discuss the efforts by HPC system\nadministrators and support staff to support users of these type of workloads on\nHPC systems not initially designed with these workloads in mind focusing on\nNCSA's Blue Waters system.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 20:41:26 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Belkin", "Maxim", ""], ["Haas", "Roland", ""], ["Arnold", "Galen Wesley", ""], ["Leong", "Hon Wai", ""], ["Huerta", "Eliu A.", ""], ["Lesny", "David", ""], ["Neubauer", "Mark", ""]]}, {"id": "1808.00684", "submitter": "Andre Merzky", "authors": "Andre Merzky, Ming Tai Ha, Matteo Turilli, Shantenu Jha", "title": "Synapse: Synthetic Application Profiler and Emulator", "comments": "Large portions of this work originally appeared as arXiv:1506.00272,\n  which was subsequently published as a workshop paper. This is an extended\n  version published in the \"Journal of Computational Science\"", "journal-ref": "Journal of Computational Science, 27C (2018) pp. 329-344", "doi": "10.1016/j.jocs.2018.06.012", "report-no": "01", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need to emulate workload execution characteristics on\nhigh-performance and distributed heterogeneous resources, we introduce Synapse.\nSynapse is used as a proxy application (or \"representative application\") for\nreal workloads, with the advantage that it can be tuned in different ways and\ndimensions, and also at levels of granularity that are not possible with real\napplications. Synapse has a platform-independent application profiler, and has\nthe ability to emulate profiled workloads on a variety of resources.\nExperiments show that the automated profiling performed using Synapse captures\nan application's characteristics with high fidelity. The emulation of an\napplication using Synapse can reproduce the application's execution behavior in\nthe original run-time environment, and can also reproduce those behaviors on\ndifferent run-time environments.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 06:36:21 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Merzky", "Andre", ""], ["Ha", "Ming Tai", ""], ["Turilli", "Matteo", ""], ["Jha", "Shantenu", ""]]}, {"id": "1808.00688", "submitter": "Alexey Gotsman", "authors": "Gregory Chockler and Alexey Gotsman", "title": "Multi-Shot Distributed Transaction Commit (Extended Version)", "comments": "Extended version of a paper in the International Symposium on\n  Distributed Computing (DISC'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic Commit Problem (ACP) is a single-shot agreement problem similar to\nconsensus, meant to model the properties of transaction commit protocols in\nfault-prone distributed systems. We argue that ACP is too restrictive to\ncapture the complexities of modern transactional data stores, where commit\nprotocols are integrated with concurrency control, and their executions for\ndifferent transactions are interdependent. As an alternative, we introduce\nTransaction Certification Service (TCS), a new formal problem that captures\nsafety guarantees of multi-shot transaction commit protocols with integrated\nconcurrency control. TCS is parameterized by a certification function that can\nbe instantiated to support common isolation levels, such as serializability and\nsnapshot isolation. We then derive a provably correct crash-resilient protocol\nfor implementing TCS through successive refinement. Our protocol achieves a\nbetter time complexity than mainstream approaches that layer two-phase commit\non top of Paxos-style replication.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 06:48:15 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 14:23:53 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Chockler", "Gregory", ""], ["Gotsman", "Alexey", ""]]}, {"id": "1808.00822", "submitter": "Duong Nguyen", "authors": "Duong Nguyen, Sandeep S. Kulkarni, Ajoy K. Datta", "title": "Benefit of Self-Stabilizing Protocols in Eventually Consistent Key-Value\n  Stores: A Case Study", "comments": "submission to ICDCN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the implementation of distributed programs in\nusing a key-value store where the state of the nodes is stored in a replicated\nand partitioned data store to improve performance and reliability. Applications\nof such algorithms occur in weather monitoring, social media, etc. We argue\nthat these applications should be designed to be stabilizing so that they\nrecover from an arbitrary state to a legitimate state. Specifically, if we use\na stabilizing algorithm then we can work with more efficient implementations\nthat provide eventual consistency rather than sequential consistency where the\ndata store behaves as if there is just one copy of the data. We find that,\nalthough the use of eventual consistency results in consistency violation\nfaults (cvf) where some node executes its action incorrectly because it relies\non an older version of the data, the overall performance of the resulting\nprotocol is better. We use experimental analysis to evaluate the expected\nimprovement. We also identify other variations of stabilization that can\nprovide additional guarantees in the presence of eventual consistency.Finally,\nwe note that if the underlying algorithm is not stabilizing, even a single \\cvf\nmay cause the algorithm to fail completely thereby making it impossible to\nbenefit from this approach.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 14:10:44 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 18:34:31 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Nguyen", "Duong", ""], ["Kulkarni", "Sandeep S.", ""], ["Datta", "Ajoy K.", ""]]}, {"id": "1808.00829", "submitter": "Sebastian Eibl", "authors": "Sebastian Eibl and Ulrich R\\\"ude", "title": "A Systematic Comparison of Dynamic Load Balancing Algorithms for\n  Massively Parallel Rigid Particle Dynamics", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2019.06.020", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As compute power increases with time, more involved and larger simulations\nbecome possible. However, it gets increasingly difficult to efficiently use the\nprovided computational resources. Especially in particle-based simulations with\na spatial domain partitioning large load imbalances can occur due to the\nsimulation being dynamic. Then a static domain partitioning may not be\nsuitable. This can deteriorate the overall runtime of the simulation\nsignificantly. Sophisticated load balancing strategies must be designed to\nalleviate this problem. In this paper we conduct a systematic evaluation of the\nperformance of six different load balancing algorithms. Our tests cover a wide\nrange of simulation sizes, and employ one of the largest supercomputers\navailable. In particular we study the runtime and memory complexity of all\ncomponents of the simulation carefully. When progressing to extreme scale\nsimulations it is essential to identify bottlenecks and to predict the scaling\nbehaviour. Scaling experiments are shown for up to over one million processes.\nThe performance of each algorithm is analyzed with respect to the quality of\nthe load balancing and its runtime costs. For all tests, the waLBerla\nmultiphysics framework is employed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 14:22:03 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 08:06:50 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Eibl", "Sebastian", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1808.00896", "submitter": "Christian W\\\"ulker", "authors": "Denis-Michael Lux, Christian W\\\"ulker, Gregory S. Chirikjian", "title": "Parallelization of the FFT on SO(3)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a work-optimal parallelization of Kostelec and Rockmore's\nwell-known fast Fourier transform and its inverse on the three-dimensional\nrotation group SO(3) is designed, implemented, and tested. To this end, the\nsequential algorithms are reviewed briefly first. In the subsequent design and\nimplementation of the parallel algorithms, we use the well-known Forster (PCAM)\nmethod and the OpenMP standard. The parallelization itself is based on\nsymmetries of the underlying basis functions and a geometric approach in which\nthe resulting index range is transformed in such a way that distinct work\npackages can be distributed efficiently to the computation nodes. The benefit\nof the parallel algorithms in practice is demonstrated in a speedup- and\nefficiency-assessing benchmark test on a system with 64 cores. Here, for the\nfirst time, we present positive results for the full transforms for the both\naccuracy- and memory-critical bandwidth 512. Using all 64 available cores, the\nspeedup for the largest considered bandwidths 128, 256, and 512 amounted to\n29.57, 36.86, and 34.36 in the forward, and 24.57, 26.69, and 24.25 in the\ninverse transform, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 16:32:36 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Lux", "Denis-Michael", ""], ["W\u00fclker", "Christian", ""], ["Chirikjian", "Gregory S.", ""]]}, {"id": "1808.00963", "submitter": "Timo Bingmann", "authors": "Timo Bingmann", "title": "Scalable String and Suffix Sorting: Algorithms, Techniques, and Tools", "comments": "396 pages, dissertation, Karlsruher Instituts f\\\"ur Technologie\n  (2018). arXiv admin note: text overlap with arXiv:1101.3448 by other authors", "journal-ref": null, "doi": "10.5445/IR/1000085031", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation focuses on two fundamental sorting problems: string sorting\nand suffix sorting. The first part considers parallel string sorting on\nshared-memory multi-core machines, the second part external memory suffix\nsorting using the induced sorting principle, and the third part distributed\nexternal memory suffix sorting with a new distributed algorithmic big data\nframework named Thrill.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 16:38:49 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Bingmann", "Timo", ""]]}, {"id": "1808.01081", "submitter": "Shengli Zhang", "authors": "Dongyan Huang, Xiaoli Ma, and Shengli Zhang", "title": "Performance Analysis of the Raft Consensus Algorithm for Private\n  Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus is one of the key problems in blockchains. There are many articles\nanalyzing the performance of threat models for blockchains. But the network\nstability seems lack of attention, which in fact affects the blockchain\nperformance. This paper studies the performance of a well adopted consensus\nalgorithm, Raft, in networks with non-negligible packet loss rate. In\nparticular, we propose a simple but accurate analytical model to analyze the\ndistributed network split probability. At a given time, we explicitly present\nthe network split probability as a function of the network size, the packet\nloss rate, and the election timeout period. To validate our analysis, we\nimplement a Raft simulator and the simulation results coincide with the\nanalytical results. With the proposed model, one can predict the network split\ntime and probability in theory and optimize the parameters in Raft consensus\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 03:57:06 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Huang", "Dongyan", ""], ["Ma", "Xiaoli", ""], ["Zhang", "Shengli", ""]]}, {"id": "1808.01093", "submitter": "Kai Wu", "authors": "Kai Wu, Qiang Guan, Nathan DeBardeleben, Dong Li", "title": "Characterization and Comparison of Application Resilience for Serial and\n  Parallel Executions", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": "LA-UR-17-26470", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Soft error of exascale application is a challenge problem in modern HPC. In\norder to quantify an application's resilience and vulnerability, the\napplication-level fault injection method is widely adopted by HPC users.\nHowever, it is not easy since users need to inject a large number of faults to\nensure statistical significance, especially for parallel version program.\nNormally, parallel execution is more complex and requires more hardware\nresources than its serial execution. Therefore, it is essential that we can\npredict error rate of parallel application based on its corresponding serial\nversion. In this poster, we characterize fault pattern in serial and parallel\nexecutions. We find first there are same fault sources in serial and parallel\nexecution. Second, parallel execution also has some unique fault sources\ncompared with serial executions. Those unique fault sources are important for\nus to understand the difference of fault pattern between serial and parallel\nexecutions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 05:55:47 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wu", "Kai", ""], ["Guan", "Qiang", ""], ["DeBardeleben", "Nathan", ""], ["Li", "Dong", ""]]}, {"id": "1808.01166", "submitter": "Erich Schikuta", "authors": "Erich Schikuta, Helmut Wanek, Heinz Stockinger, Kurt Stockinger,\n  Thomas F\\\"urle, Oliver Jorns, Christoph L\\\"offelhardt, Peter Brezany, Minh\n  Dang, Thomas M\\\"uck", "title": "ViPIOS - VIenna Parallel Input Output System: Language, Compiler and\n  Advanced Data Structure Support for Parallel I/O Operations", "comments": "210 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an increasing number of data intensive scientific applications, parallel\nI/O concepts are a major performance issue. Tackling this issue, we develop an\ninput/output system designed for highly efficient, scalable and conveniently\nusable parallel I/O on distributed memory systems. The main focus of this\nresearch is the parallel I/O runtime system support provided for\nsoftware-generated programs produced by parallelizing compilers in the context\nof High Performance FORTRAN efforts. Specifically, our design aims for the\nVienna Fortran Compilation System.\n  In our research project we investigate the I/O problem from a runtime system\nsupport perspective. We focus on the design of an advanced parallel I/O\nsupport, called ViPIOS (VIenna Parallel I/O System), to be targeted by language\ncompilers supporting the same programming model like High Performance Fortran\n(HPF). The ViPIOS design is partly influenced by the concepts of parallel\ndatabase technology.\n  At the beginning of the project we developed a formal model, which forms a\ntheoretical framework on which the ViPIOS design is based. This model describes\nthe mapping of the problem specific data space starting from the application\nprogram data structures down to the physical layout on disk across several\nintermediate representation levels.\n  Based on this formal model we designed and developed an I/O runtime system,\nViPIOS, which provides support for several issues, as - parallel access to\nfiles for read/write operations, - optimization of data-layout on disks, -\nredistribution of data stored on disks, - communication of out-of-core (OOC)\ndata, and - many optimizations including data prefetching from disks based on\nthe access pattern knowledge extracted from the program by the compiler or\nprovided by a user specification.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 11:52:17 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Schikuta", "Erich", ""], ["Wanek", "Helmut", ""], ["Stockinger", "Heinz", ""], ["Stockinger", "Kurt", ""], ["F\u00fcrle", "Thomas", ""], ["Jorns", "Oliver", ""], ["L\u00f6ffelhardt", "Christoph", ""], ["Brezany", "Peter", ""], ["Dang", "Minh", ""], ["M\u00fcck", "Thomas", ""]]}, {"id": "1808.01353", "submitter": "Eduard Gibert Renart", "authors": "Eduard Gibert Renart, Daniel Balouek-Thomert and Manish Parashar", "title": "Edge Based Data-Driven Pipelines (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This research reports investigates an edge on-device stream processing\nplatform, which extends the serverless com- puting model to the edge to help\nfacilitate real-time data analytics across the cloud and edge in a uniform\nmanner. We investigate associated use cases and architectural design. We\ndeployed and tested our system on edge devices (Raspberry Pi and Android\nPhone), which proves that stream processing analytics can be performed at the\nedge of the network with single board computers in a real-time fashion.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:53:19 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Renart", "Eduard Gibert", ""], ["Balouek-Thomert", "Daniel", ""], ["Parashar", "Manish", ""]]}, {"id": "1808.01561", "submitter": "Changting Lin", "authors": "Changting Lin, Ning Ma, Xun Wang, Zhenguang Liu, Jianhai Chen,\n  Shouling Ji", "title": "Rapido: A Layer2 Payment System for Decentralized Currencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin blockchain faces the bitcoin scalability problem, for which bitcoin's\nblocks contain the transactions on the bitcoin network. The on-chain\ntransaction processing capacity of the bitcoin network is limited by the\naverage block creation time of 10 minutes and the block size limit. These\njointly constrain the network's throughput. The transaction processing capacity\nmaximum is estimated between 3.3 and 7 transactions per second (TPS). A Layer2\nNetwork, named Lightning Network, is proposed and activated solutions to\naddress this problem. LN operates on top of the bitcoin network as a cache to\nallow payments to be affected that are not immediately put on the blockchain.\nHowever, it also brings some drawbacks. In this paper, we observe a specific\npayment issue among current LN, which requires additional claims to blockchain\nand is time-consuming. We call the issue as shares issue. Therefore, we propose\nRapido to explicitly address the shares issue. Furthermore, a new smart\ncontract, D-HTLC, is equipped with Rapido as the payment protocol. We finally\nprovide a proof of concept implementation and simulation for both Rapido and\nLN, in which Rapdio not only mitigates the shares issue but also mitigates the\nskewness issue thus is proved to be more applicable for various transactions\nthan LN.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 05:40:19 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 08:19:31 GMT"}, {"version": "v3", "created": "Sat, 11 Aug 2018 12:31:31 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Lin", "Changting", ""], ["Ma", "Ning", ""], ["Wang", "Xun", ""], ["Liu", "Zhenguang", ""], ["Chen", "Jianhai", ""], ["Ji", "Shouling", ""]]}, {"id": "1808.02134", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Yu Chen, Sejun Song, Timothy R. Faughnan", "title": "Kerman: A Hybrid Lightweight Tracking Algorithm to Enable Smart\n  Surveillance as an Edge Service", "comments": "Submitted to IEEE CCNC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing pushes the cloud computing boundaries beyond uncertain network\nresource by leveraging computational processes close to the source and target\nof data. Time-sensitive and data-intensive video surveillance applications\nbenefit from on-site or near-site data mining. In recent years, many smart\nvideo surveillance approaches are proposed for object detection and tracking by\nusing Artificial Intelligence (AI) and Machine Learning (ML) algorithms.\nHowever, it is still hard to migrate those computing and data-intensive tasks\nfrom Cloud to Edge due to the high computational requirement. In this paper, we\nenvision to achieve intelligent surveillance as an edge service by proposing a\nhybrid lightweight tracking algorithm named Kerman (Kernelized Kalman filter).\nKerman is a decision tree based hybrid Kernelized Correlation Filter (KCF)\nalgorithm proposed for human object tracking, which is coupled with a\nlightweight Convolutional Neural Network (L-CNN) for high performance. The\nproposed Kerman algorithm has been implemented on a couple of single board\ncomputers (SBC) as edge devices and validated using real-world surveillance\nvideo streams. The experimental results are promising that the Kerman algorithm\nis able to track the object of interest with a decent accuracy at a resource\nconsumption affordable by edge devices.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 22:13:33 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""], ["Song", "Sejun", ""], ["Faughnan", "Timothy R.", ""]]}, {"id": "1808.02144", "submitter": "Bogdan Chlebus", "authors": "Bogdan S. Chlebus and Vicent Cholvi and Pawel Garncarek and Tomasz\n  Jurdzinski and Dariusz R. Kowalski", "title": "Routing in Wireless Networks with Interferences", "comments": null, "journal-ref": "Routing in Wireless Networks with Interferences. IEEE\n  Communications Letters, 21(9) : 2105 - 2108, 2017", "doi": "10.1109/LCOMM.2017.2716348", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dynamic routing in multi-hop wireless networks with adversarial\ntraffic. The model of wireless communication incorporates interferences caused\nby packets' arrivals into the same node that overlap in time. We consider two\nclasses of adversaries: balanced and unbalanced. We demonstrate that, for each\nrouting algorithm and an unbalanced adversary, the algorithm is unstable\nagainst this adversary in some networks. We develop a routing algorithm that\nhas bounded packet latency against each balanced adversary.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 22:50:48 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Chlebus", "Bogdan S.", ""], ["Cholvi", "Vicent", ""], ["Garncarek", "Pawel", ""], ["Jurdzinski", "Tomasz", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "1808.02216", "submitter": "Elijah Hradovich", "authors": "Elijah Hradovich, Marek Klonowski and Dariusz R. Kowalski", "title": "Contention resolution on a restrained channel", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine deterministic broadcasting on multiple-access channels for a\nscenario when packets are injected continuously by an adversary to the buffers\nof the devices at rate $\\rho$ packages per round. The aim is to maintain system\nstability, that is, bounded queues. In contrast to previous work we assume that\nthere is a strict limit of available power, defined as the total number of\nstations allowed to transmit or listen to the channel at a given time, that can\nnever be exceeded. We study how this constraint influences the quality of\nservices with particular focus on stability. We show that in the regime of\ndeterministic algorithms, the significance of energy restriction depends\nstrongly on communication capabilities of broadcasting protocols. For the\nadaptive and full-sensing protocols, wherein stations may substantially adopt\ntheir behavior to the injection pattern, one can construct efficient algorithms\nusing very small amounts of power without sacrificing throughput or stability\nof the system. In particular, we construct constant-energy adaptive and full\nsensing protocols stable for $\\rho=1$ and any $\\rho<1$, respectively, even for\nworst case (adversarial) injection patterns. Surprisingly, for the case of\nacknowledgment based algorithms that cannot adopt to the situation on the\nchannel (i.e., their transmitting pattern is fixed in advance), limiting power\nleads to reducing the throughput. That is, for this class of protocols in order\nto preserve stability we need to reduce injection rate significantly. We\nsupport our theoretical analysis by simulation results of algorithms\nconstructed in the paper. We depict how they work for systems of moderate,\nrealistic sizes. We also provide a comprehensive simulation to compare our\nalgorithms with backoff algorithms, which are common in real-world\nimplementations, in terms of queue sizes and energy consumption.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 05:17:46 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 13:08:46 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hradovich", "Elijah", ""], ["Klonowski", "Marek", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "1808.02231", "submitter": "Gabriele D'Angelo", "authors": "Antonio Magnani, Gabriele D'Angelo, Stefano Ferretti, Moreno Marzolla", "title": "Anonymity and Confidentiality in Secure Distributed Simulation", "comments": "Proceedings of the IEEE/ACM International Symposium on Distributed\n  Simulation and Real Time Applications (DS-RT 2018)", "journal-ref": null, "doi": "10.1109/DISTRA.2018.8600922", "report-no": null, "categories": "cs.DC cs.CR cs.MA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on data confidentiality, integrity and availability is gaining\nmomentum in the ICT community, due to the intrinsically insecure nature of the\nInternet. While many distributed systems and services are now based on secure\ncommunication protocols to avoid eavesdropping and protect confidentiality, the\ntechniques usually employed in distributed simulations do not consider these\nissues at all. This is probably due to the fact that many real-world simulators\nrely on monolithic, offline approaches and therefore the issues above do not\napply. However, the complexity of the systems to be simulated, and the rise of\ndistributed and cloud based simulation, now impose the adoption of secure\nsimulation architectures. This paper presents a solution to ensure both\nanonymity and confidentiality in distributed simulations. A performance\nevaluation based on an anonymized distributed simulator is used for quantifying\nthe performance penalty for being anonymous. The obtained results show that\nthis is a viable solution.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:04:54 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 12:31:12 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Magnani", "Antonio", ""], ["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1808.02240", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura, Deniz Gunduz, Sennur Ulukus", "title": "Speeding Up Distributed Gradient Descent by Utilizing Non-persistent\n  Stragglers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed gradient descent (DGD) is an efficient way of implementing\ngradient descent (GD), especially for large data sets, by dividing the\ncomputation tasks into smaller subtasks and assigning to different computing\nservers (CSs) to be executed in parallel. In standard parallel execution,\nper-iteration waiting time is limited by the execution time of the straggling\nservers. Coded DGD techniques have been introduced recently, which can tolerate\nstraggling servers via assigning redundant computation tasks to the CSs. In\nmost of the existing DGD schemes, either with coded computation or coded\ncommunication, the non-straggling CSs transmit one message per iteration once\nthey complete all their assigned computation tasks. However, although the\nstraggling servers cannot complete all their assigned tasks, they are often\nable to complete a certain portion of them. In this paper, we allow multiple\ntransmissions from each CS at each iteration in order to make sure a maximum\nnumber of completed computations can be reported to the aggregating server\n(AS), including the straggling servers. We numerically show that the average\ncompletion time per iteration can be reduced significantly by slightly\nincreasing the communication load per server.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:49:25 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 21:16:43 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 14:30:26 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Ozfatura", "Emre", ""], ["Gunduz", "Deniz", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1808.02252", "submitter": "Shixiong Zhao Mr", "authors": "Xusheng Chen, Shixiong Zhao, Ji Qi, Jianyu Jiang, Haoze Song, Cheng\n  Wang, Tsz On Li, T.-H. Hubert Chan, Fengwei Zhang, Xiapu Luo, Sen Wang, Gong\n  Zhang, Heming Cui", "title": "Efficient and DoS-resistant Consensus for Permissioned Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing permissioned blockchain systems designate a fixed and explicit group\nof committee nodes to run a consensus protocol that confirms the same sequence\nof blocks among all nodes. Unfortunately, when such a permissioned blockchain\nruns in a large scale on the Internet, these explicit committee nodes can be\neasily turned down by denial-of-service (DoS) or network partition attacks.\nAlthough work proposes scalable BFT protocols that run on a larger number of\ncommittee nodes, their efficiency drops dramatically when only a small number\nof nodes are attacked.\n  In this paper, our EGES protocol leverages Intel SGX to develop a new\nabstraction called \"stealth committee\", which effectively hides the committee\nnodes into a large pool of fake committee nodes. EGES selects a distinct group\nof stealth committee for each block and confirms the same sequence of blocks\namong all nodes with overwhelming probability. Evaluation on typical\ngeo-distributed settings shows that: (1)EGES is the first permissioned\nblockchain's consensus protocol that can tolerate tough DoS and network\npartition attacks; and (2) EGES achieves comparable throughput and latency as\nexisting permissioned blockchains' protocols\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 08:13:44 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 09:09:46 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 08:12:15 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 03:11:41 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Xusheng", ""], ["Zhao", "Shixiong", ""], ["Qi", "Ji", ""], ["Jiang", "Jianyu", ""], ["Song", "Haoze", ""], ["Wang", "Cheng", ""], ["Li", "Tsz On", ""], ["Chan", "T. -H. Hubert", ""], ["Zhang", "Fengwei", ""], ["Luo", "Xiapu", ""], ["Wang", "Sen", ""], ["Zhang", "Gong", ""], ["Cui", "Heming", ""]]}, {"id": "1808.02254", "submitter": "Sara Kardani Moghaddam", "authors": "Sara Kardani-Moghaddam, Rajkumar Buyya, and Kotagiri Ramamohanarao", "title": "Performance-Aware Management of Cloud Resources: A Taxonomy and Future\n  Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic nature of the cloud environment has made distributed resource\nmanagement process a challenge for cloud service providers. The importance of\nmaintaining the quality of service in accordance with customer expectations as\nwell as the highly dynamic nature of cloud-hosted applications add new levels\nof complexity to the process. Advances to the big data learning approaches have\nshifted conventional static capacity planning solutions to complex\nperformance-aware resource management methods. It is shown that the process of\ndecision making for resource adjustment is closely related to the behaviour of\nthe system including the utilization of resources and application components.\nTherefore, a continuous monitoring of system attributes and performance metrics\nprovide the raw data for the analysis of problems affecting the performance of\nthe application. Data analytic methods such as statistical and machine learning\napproaches offer the required concepts, models and tools to dig into the data,\nfind general rules, patterns and characteristics that define the functionality\nof the system. Obtained knowledge form the data analysis process helps to find\nout about the changes in the workloads, faulty components or problems that can\ncause system performance to degrade. A timely reaction to performance\ndegradations can avoid violations of the service level agreements by performing\nproper corrective actions including auto-scaling or other resource adjustment\nsolutions. In this paper, we investigate the main requirements and limitations\nin cloud resource management including a study of the approaches in workload\nand anomaly analysis in the context of the performance management in the cloud.\nA taxonomy of the works on this problem is presented which identifies the main\napproaches in existing researches from data analysis side to resource\nadjustment techniques.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 08:27:02 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Kardani-Moghaddam", "Sara", ""], ["Buyya", "Rajkumar", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "1808.02318", "submitter": "Marco Capuccini", "authors": "Marco Capuccini, Martin Dahl\\\"o, Salman Toor and Ola Spjuth", "title": "MaRe: a MapReduce-Oriented Framework for Processing Big Data with\n  Application Containers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Life science is increasingly driven by Big Data analytics, and\nthe MapReduce programming model has been proven successful for data-intensive\nanalyses. However, current MapReduce frameworks offer poor support for reusing\nexisting processing tools in bioinformatics pipelines. Further, these\nframeworks do not have native support for application containers, which are\nbecoming popular in scientific data processing.\n  Results. Here we present MaRe, a programming model with an associated\nopen-source implementation, which introduces support for application containers\nin MapReduce. MaRe is based on Apache Spark and Docker, the MapReduce framework\nand container engine that have collected the largest open source community,\nthus providing interoperability with the cutting-edge software ecosystem. We\ndemonstrate MaRe on two data-intensive applications in life science, showing\nease of use and scalability.\n  Conclusions. MaRe enables scalable data-intensive processing in life science\nwith MapReduce and application containers. When compared with current best\npractices, that involve the use of workflow systems, MaRe has the advantage of\nproviding data locality, ingestion from heterogeneous storage systems and\ninteractive processing. MaRe is generally-applicable and available as open\nsource software.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 12:14:49 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 15:17:09 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Capuccini", "Marco", ""], ["Dahl\u00f6", "Martin", ""], ["Toor", "Salman", ""], ["Spjuth", "Ola", ""]]}, {"id": "1808.02546", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Silvio Lattanzi, Vahab Mirrokni", "title": "Parallel and Streaming Algorithms for K-Core Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-core decomposition is a fundamental primitive in many machine\nlearning and data mining applications. We present the first distributed and the\nfirst streaming algorithms to compute and maintain an approximate $k$-core\ndecomposition with provable guarantees. Our algorithms achieve rigorous bounds\non space complexity while bounding the number of passes or number of rounds of\ncomputation. We do so by presenting a new powerful sketching technique for\n$k$-core decomposition, and then by showing it can be computed efficiently in\nboth streaming and MapReduce models. Finally, we confirm the effectiveness of\nour sketching technique empirically on a number of publicly available graphs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 20:31:39 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 22:24:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1808.02621", "submitter": "Soojeong Kim", "authors": "Soojeong Kim, Gyeong-In Yu, Hojin Park, Sungwoo Cho, Eunji Jeong,\n  Hyeonmin Ha, Sanha Lee, Joo Seong Jeong, Byung-Gon Chun", "title": "Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The employment of high-performance servers and GPU accelerators for training\ndeep neural network models have greatly accelerated recent advances in deep\nlearning (DL). DL frameworks, such as TensorFlow, MXNet, and Caffe2, have\nemerged to assist DL researchers to train their models in a distributed manner.\nAlthough current DL frameworks scale well for image classification models,\nthere remain opportunities for scalable distributed training on natural\nlanguage processing (NLP) models. We found that current frameworks show\nrelatively low scalability on training NLP models due to the lack of\nconsideration to the difference in sparsity of model parameters. In this paper,\nwe propose Parallax, a framework that optimizes data parallel training by\nutilizing the sparsity of model parameters. Parallax introduces a hybrid\napproach that combines Parameter Server and AllReduce architectures to optimize\nthe amount of data transfer according to the sparsity. Experiments show that\nParallax built atop TensorFlow achieves scalable training throughput on both\ndense and sparse models while requiring little effort from its users. Parallax\nachieves up to 2.8x, 6.02x speedup for NLP models than TensorFlow and Horovod\nwith 48 GPUs, respectively. The training speed for the image classification\nmodels is equal to Horovod and 1.53x faster than TensorFlow.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 04:48:14 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 15:57:45 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 05:57:38 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Kim", "Soojeong", ""], ["Yu", "Gyeong-In", ""], ["Park", "Hojin", ""], ["Cho", "Sungwoo", ""], ["Jeong", "Eunji", ""], ["Ha", "Hyeonmin", ""], ["Lee", "Sanha", ""], ["Jeong", "Joo Seong", ""], ["Chun", "Byung-Gon", ""]]}, {"id": "1808.02638", "submitter": "Xinsheng Qin", "authors": "Xinsheng Qin, Randall J. LeVeque, Michael R. Motley", "title": "Accelerating wave-propagation algorithms with adaptive mesh refinement\n  using the Graphics Processing Unit (GPU)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clawpack is a library for solving nonlinear hyperbolic partial differential\nequations using high-resolution finite volume methods based on Riemann solvers\nand limiters. It supports Adaptive Mesh Refinement (AMR), which is essential in\nsolving multi-scale problems. Recently, we added capabilities to accelerate the\ncode by using the Graphics Process Unit (GPU). Routines that manage CPU and GPU\nAMR data and facilitate the execution of GPU kernels are added. Customized and\nCPU thread-safe memory managers are designed to manage GPU and CPU memory\npools, which is essential in eliminating the overhead of memory allocation and\nde-allocation. A global reduction is conducted every time step for dynamically\nadjusting the time step based on Courant number restrictions. Some small GPU\nkernels are merged into bigger kernels, which greatly reduces kernel launching\noverhead. A speed-up between $2$ and $3$ for the total running time is observed\nin an acoustics benchmark problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 06:21:56 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Qin", "Xinsheng", ""], ["LeVeque", "Randall J.", ""], ["Motley", "Michael R.", ""]]}, {"id": "1808.02838", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far and Emina Soljanin", "title": "On the Effect of Task-to-Worker Assignment in Distributed Computing\n  Systems with Stragglers", "comments": "Accepted at the 56th Annual Allerton Conference on Communication,\n  Control, and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the expected completion time of some recently proposed algorithms\nfor distributed computing which redundantly assign computing tasks to multiple\nmachines in order to tolerate a certain number of machine failures. We\nanalytically show that not only the amount of redundancy but also the\ntask-to-machine assignments affect the latency in a distributed system. We\nstudy systems with a fixed number of computing tasks that are split in possibly\noverlapping batches, and independent exponentially distributed machine service\ntimes. We show that, for such systems, the uniform replication of non-\noverlapping (disjoint) batches of computing tasks achieves the minimum expected\ncomputing time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:51:18 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""]]}, {"id": "1808.02919", "submitter": "Yue Cheng", "authors": "Yue Cheng, Zheng Chai, Ali Anwar", "title": "Characterizing Co-located Datacenter Workloads: An Alibaba Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Warehouse-scale cloud datacenters co-locate workloads with different and\noften complementary characteristics for improved resource utilization. To\nbetter understand the challenges in managing such intricate, heterogeneous\nworkloads while providing quality-assured resource orchestration and user\nexperience, we analyze Alibaba's co-located workload trace, the first publicly\navailable dataset with precise information about the category of each job. Two\ntypes of workload---long-running, user-facing, containerized production jobs,\nand transient, highly dynamic, non-containerized, and non-production batch\njobs---are running on a shared cluster of 1313 machines. Our multifaceted\nanalysis reveals insights that we believe are useful for system designers and\nIT practitioners working on cluster management systems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 19:39:22 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 20:57:56 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Cheng", "Yue", ""], ["Chai", "Zheng", ""], ["Anwar", "Ali", ""]]}, {"id": "1808.03050", "submitter": "Ramy Amer Mr", "authors": "Ramy Amer and M. Majid Butt and Hesham ElSawy and Mehdi Bennis and\n  Jacek Kibi{\\l}da and Nicola Marchetti", "title": "On Minimizing Energy Consumption for D2D Clustered Caching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and solve the energy minimization problem for a clustered\ndevice-to-device (D2D) network with cache-enabled mobile devices. Devices are\ndistributed according to a Poisson cluster process (PCP) and are assumed to\nhave a surplus memory which is exploited to proactively cache files from a\nlibrary. Devices can retrieve the requested files from their caches, from\nneighboring devices in their proximity (cluster), or from the base station as a\nlast resort. We minimize the energy consumption of the proposed network under a\nrandom prob- abilistic caching scheme, where files are independently cached\naccording to a specific probability distribution. A closed-form expression for\nthe D2D coverage probability is obtained. The energy consumption problem is\nthen formulated as a function of the caching distribution, and the optimal\nprobabilistic caching distribution is obtained. Results reveal that the\nproposed caching distribution reduces energy consumption up to 33% as compared\nto caching popular files scheme.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 08:04:48 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Amer", "Ramy", ""], ["Butt", "M. Majid", ""], ["ElSawy", "Hesham", ""], ["Bennis", "Mehdi", ""], ["Kibi\u0142da", "Jacek", ""], ["Marchetti", "Nicola", ""]]}, {"id": "1808.03289", "submitter": "Muhammad Bilal", "authors": "Muhammad Bilal, Shin-Gak Kang and Sangheon Pack", "title": "Effective Caching for the Secure Content Distribution in\n  Information-Centric Networking", "comments": "7 pages, 9 figures, 2018 IEEE 87th Vehicular Technology Conference\n  (VTC Spring)", "journal-ref": "IEEE Proceedings of 87th Vehicular Technology Conference (VTC\n  Spring), Porto, 2018, pp. 1-7", "doi": "10.1109/VTCSpring.2018.8417854", "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The secure distribution of protected content requires consumer authentication\nand involves the conventional method of end-to-end encryption. However, in\ninformation-centric networking (ICN) the end-to-end encryption makes the\ncontent caching ineffective since encrypted content stored in a cache is\nuseless for any consumer except those who know the encryption key. For\neffective caching of encrypted content in ICN, we propose a novel scheme,\ncalled the Secure Distribution of Protected Content (SDPC). SDPC ensures that\nonly authenticated consumers can access the content. The SDPC is a lightweight\nauthentication and key distribution protocol; it allows consumer nodes to\nverify the originality of the published article by using a symmetric key\nencryption. The security of the SDPC was proved with BAN logic and Scyther tool\nverification.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:09:55 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Bilal", "Muhammad", ""], ["Kang", "Shin-Gak", ""], ["Pack", "Sangheon", ""]]}, {"id": "1808.03380", "submitter": "Sunny Katkuri", "authors": "Sunny Katkuri", "title": "A survey of data transfer and storage techniques in prevalent\n  cryptocurrencies and suggested improvements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This thesis focuses on aspects related to the functioning of the gossip\nnetworks underlying three relatively popular cryptocurrencies: Ethereum, Nano\nand IOTA.\n  We look at topics such as automatic discovery of peers when a new node joins\nthe network, bandwidth usage of a node, message passing protocols and storage\nschemas and optimizations for the shared ledger. We believe this is a topic\nthat is often overlooked in works about blockchains and cryptocurrencies.\nVulnerabilities and inefficiencies attain a higher significance than ones in a\nregular open source project because of the rather direct financial implications\nof these projects. Barring Bitcoin, a network that has been around for nearly\n10 years, no other project has substantial documentation for its operational\ndetails other than scattered and sparse pages in the source code repositories.\nAlmost all of the content described here has been extracted by studying the\nsource code of the reference implementations of these projects.\n  We evaluate the use of Invertible Bloom Lookup Tables and the Graphene\nprotocol to decrease block propagation times and bandwidth usage of certain\nmessages. We perform realistic simulations that show significant improvements.\nWe provide a complete implementation of Graphene in Geth, Ethereum's main node\nsoftware and test this implementation against the main Ethereum blockchain.\n  We also crawled the chosen cryptocurrency networks for publicly visible nodes\nand provide an Autonomous System-level breakdown of these nodes with the end\ngoal of estimating the ease of performing attacks such as BGP hijacks and their\nimpact.\n  Code written for implementing Graphene in Geth, performing various\nsimulations and for other miscellaneous tasks has been uploaded to Github at\nhttps://github.com/sunfinite/masters-thesis.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 00:59:50 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Katkuri", "Sunny", ""]]}, {"id": "1808.03643", "submitter": "Mihai-Alin Badiu", "authors": "Mihai-Alin Badiu, David Saad, and Justin P. Coon", "title": "Self-Organization Scheme for Balanced Routing in Large-Scale Multi-Hop\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-organization scheme for cost-effective and load-balanced\nrouting in multi-hop networks. To avoid overloading nodes that provide\nfavourable routing conditions, we assign each node with a cost function that\npenalizes high loads. Thus, finding routes to sink nodes is formulated as an\noptimization problem in which the global objective function strikes a balance\nbetween route costs and node loads. We apply belief propagation (its min-sum\nversion) to solve the network optimization problem and obtain a distributed\nalgorithm whereby the nodes collectively discover globally optimal routes by\nperforming low-complexity computations and exchanging messages with their\nneighbours. We prove that the proposed method converges to the global optimum\nafter a finite number of local exchanges of messages. Finally, we demonstrate\nnumerically our framework's efficacy in balancing the node loads and study the\ntrade-off between load reduction and total cost minimization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 07:13:44 GMT"}], "update_date": "2018-08-18", "authors_parsed": [["Badiu", "Mihai-Alin", ""], ["Saad", "David", ""], ["Coon", "Justin P.", ""]]}, {"id": "1808.03843", "submitter": "Wei Tan", "authors": "Wei Tan, Shiyu Chang, Liana Fong, Cheng Li, Zijun Wang, Liangliang Cao", "title": "Matrix Factorization on GPUs with Memory Optimization and Approximate\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) discovers latent features from observations, which\nhas shown great promises in the fields of collaborative filtering, data\ncompression, feature extraction, word embedding, etc. While many\nproblem-specific optimization techniques have been proposed, alternating least\nsquare (ALS) remains popular due to its general applicability e.g. easy to\nhandle positive-unlabeled inputs, fast convergence and parallelization\ncapability. Current MF implementations are either optimized for a single\nmachine or with a need of a large computer cluster but still are insufficient.\nThis is because a single machine provides limited compute power for large-scale\ndata while multiple machines suffer from the network communication bottleneck.\n  To address the aforementioned challenge, accelerating ALS on graphics\nprocessing units (GPUs) is a promising direction. We propose the novel approach\nin enhancing the MF efficiency via both memory optimization and approximate\ncomputing. The former exploits GPU memory hierarchy to increase data reuse,\nwhile the later reduces unnecessary computing without hurting the convergence\nof learning algorithms. Extensive experiments on large-scale datasets show that\nour solution not only outperforms the competing CPU solutions by a large margin\nbut also has a 2x-4x performance gain compared to the state-of-the-art GPU\nsolutions. Our implementations are open-sourced and publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 17:36:10 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Tan", "Wei", ""], ["Chang", "Shiyu", ""], ["Fong", "Liana", ""], ["Li", "Cheng", ""], ["Wang", "Zijun", ""], ["Cao", "Liangliang", ""]]}, {"id": "1808.03880", "submitter": "Yaron Singer", "authors": "Eric Balkanski and Yaron Singer", "title": "Parallelization does not Accelerate Convex Optimization: Adaptivity\n  Lower Bounds for Non-smooth Convex Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the limitations of parallelization in convex\noptimization. A convenient approach to study parallelization is through the\nprism of \\emph{adaptivity} which is an information theoretic measure of the\nparallel runtime of an algorithm [BS18]. Informally, adaptivity is the number\nof sequential rounds an algorithm needs to make when it can execute\npolynomially-many queries in parallel at every round. For combinatorial\noptimization with black-box oracle access, the study of adaptivity has recently\nled to exponential accelerations in parallel runtime and the natural question\nis whether dramatic accelerations are achievable for convex optimization.\n  For the problem of minimizing a non-smooth convex function $f:[0,1]^n\\to\n\\mathbb{R}$ over the unit Euclidean ball, we give a tight lower bound that\nshows that even when $\\texttt{poly}(n)$ queries can be executed in parallel,\nthere is no randomized algorithm with $\\tilde{o}(n^{1/3})$ rounds of adaptivity\nthat has convergence rate that is better than those achievable with a\none-query-per-round algorithm. A similar lower bound was obtained by Nemirovski\n[Nem94], however that result holds for the $\\ell_{\\infty}$-setting instead of\n$\\ell_2$. In addition, we also show a tight lower bound that holds for\nLipschitz and strongly convex functions.\n  At the time of writing this manuscript we were not aware of Nemirovski's\nresult. The construction we use is similar to the one in [Nem94], though our\nanalysis is different. Due to the close relationship between this work and\n[Nem94], we view the research contribution of this manuscript limited and it\nshould serve as an instructful approach to understanding lower bounds for\nparallel optimization.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 01:56:17 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 19:29:57 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Balkanski", "Eric", ""], ["Singer", "Yaron", ""]]}, {"id": "1808.03884", "submitter": "Cameron Musco", "authors": "Nancy Lynch and Cameron Musco", "title": "A Basic Compositional Model for Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a formal, mathematical foundation for modeling and\nreasoning about the behavior of $synchronous$, $stochastic$ $Spiking$ $Neural$\n$Networks$ $(SNNs)$. We define a basic SNN model, in which a neuron's only\nstate is a Boolean value indicating whether the neuron is currently firing. We\nalso define the $external\\ behavior$ of an SNN. We define two operators on\nSNNs: a $composition\\ operator$, which supports modeling of SNNs as\ncombinations of smaller SNNs, and a $hiding\\ operator$, which reclassifies some\noutput behavior of an SNN as internal. We prove results describing how the\nexternal behavior of a network built using these operators is related to the\nexternal behavior of the component networks. Finally, we give a formal\ndefinition of a $problem$ to be solved by an SNN, and give basic results\nshowing how the composition and hiding operators affect the problems that are\nsolved by the networks. We illustrate our definitions with three examples: a\nBoolean circuit constructed from gates, an $Attention$ network constructed from\na $Winner$-$Take$-$All$ network and a $Filter$ network, and a toy example\ninvolving combining two networks in a cyclic fashion.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 02:38:36 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:04:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lynch", "Nancy", ""], ["Musco", "Cameron", ""]]}, {"id": "1808.04118", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang and Keyou You", "title": "AsySPA: An Exact Asynchronous Algorithm for Convex Optimization Over\n  Digraphs", "comments": "Accepted by IEEE Transactions on Automatic Control. 15 pages, 9\n  figures", "journal-ref": null, "doi": "10.1109/TAC.2019.2930234", "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel exact distributed asynchronous subgradient-push\nalgorithm (AsySPA) to solve an additive cost optimization problem over directed\ngraphs where each node only has access to a local convex function and updates\nasynchronously with an arbitrary rate. Specifically, each node of a strongly\nconnected digraph does not wait for updates from other nodes but simply starts\na new update within any bounded time interval by using local information\navailable from its in-neighbors. \"Exact\" means that every node of the AsySPA\ncan asymptotically converge to the same optimal solution, even under different\nupdate rates among nodes and bounded communication delays. To address uneven\nupdate rates, we design a simple mechanism to adaptively adjust stepsizes per\nupdate in each node, which is substantially different from the existing works.\nThen, we construct a delay-free augmented system to address asynchrony and\ndelays, and study its convergence by proposing a generalized subgradient\nalgorithm, which clearly has its own significance and helps us to explicitly\nevaluate the convergence rate of the AsySPA. Finally, we demonstrate advantages\nof the AsySPA in both theory and simulation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 09:26:01 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 05:23:33 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 15:19:06 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Zhang", "Jiaqi", ""], ["You", "Keyou", ""]]}, {"id": "1808.04143", "submitter": "Xi Zheng", "authors": "Babin Bhandari, James Zheng", "title": "A Preliminary Study On Emerging Cloud Computing Security Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is the internet based provisioning of the computing\nresources, software, and information on demand. Cloud Computing is referred to\nas one of most recent emerging paradigms of computing utilities. Since Cloud\ncomputing is the dominant infrastructure of the shared services over the\ninternet, it is important to be aware of the security risk and the challenges\nassociated with this emerging computing paradigm. This survey provides a brief\nintroduction to the cloud computing, its major characteristics, and service\nmodels. It also explores cloud security threats, lists a few security solutions\n, and proposes a promsing research direction to deal with the evolving security\nchallenges in Cloud computing.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 10:45:38 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Bhandari", "Babin", ""], ["Zheng", "James", ""]]}, {"id": "1808.04201", "submitter": "Michael Schaffner", "authors": "Michael Schaffner, Luca Benini", "title": "On the Feasibility of FPGA Acceleration of Molecular Dynamics\n  Simulations", "comments": "Technical Report, 16 Pages, 4 Tables, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical molecular dynamics (MD) simulations are important tools in life and\nmaterial sciences since they allow studying chemical and biological processes\nin detail. However, the inherent scalability problem of particle-particle\ninteractions and the sequential dependency of subsequent time steps render MD\ncomputationally intensive and difficult to scale. To this end, specialized\nFPGA-based accelerators have been repeatedly proposed to ameliorate this\nproblem. However, to date none of the leading MD simulation packages fully\nsupport FPGA acceleration and a direct comparison of GPU versus FPGA\naccelerated codes has remained elusive so far. With this report, we aim at\nclarifying this issue by comparing measured application performance on\nGPU-dense compute nodes with performance and cost estimates of a FPGA-based\nsingle- node system. Our results show that an FPGA-based system can indeed\noutperform a similarly configured GPU-based system, but the overall\napplication-level speedup remains in the order of 2x due to software overheads\non the host. Considering the price for GPU and FPGA solutions, we observe that\nGPU-based solutions provide the better cost/performance tradeoff, and hence\npure FPGA-based solutions are likely not going to be commercially viable.\nHowever, we also note that scaled multi-node systems could potentially benefit\nfrom a hybrid composition, where GPUs are used for compute intensive parts and\nFPGAs for latency and communication sensitive tasks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 10:03:37 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Schaffner", "Michael", ""], ["Benini", "Luca", ""]]}, {"id": "1808.04224", "submitter": "Georgios Andreadis", "authors": "Georgios Andreadis, Laurens Versluis, Fabian Mastenbroek, Alexandru\n  Iosup", "title": "A Reference Architecture for Datacenter Scheduling: Extended Technical\n  Report", "comments": "Technical Report on the homonym article accepted for publication at\n  Supercomputing 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datacenters act as cloud-infrastructure to stakeholders across industry,\ngovernment, and academia. To meet growing demand yet operate efficiently,\ndatacenter operators employ increasingly more sophisticated scheduling systems,\nmechanisms, and policies. Although many scheduling techniques already exist,\nrelatively little research has gone into the abstraction of the scheduling\nprocess itself, hampering design, tuning, and comparison of existing\ntechniques. In this work, we propose a reference architecture for datacenter\nschedulers. The architecture follows five design principles: components with\nclearly distinct responsibilities, grouping of related components where\npossible, separation of mechanism from policy, scheduling as complex workflow,\nand hierarchical multi-scheduler structure. To demonstrate the validity of the\nreference architecture, we map to it state-of-the-art datacenter schedulers. We\nfind scheduler-stages are commonly underspecified in peer-reviewed\npublications. Through trace-based simulation and real-world experiments, we\nshow underspecification of scheduler-stages can lead to significant variations\nin performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 14:00:02 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Andreadis", "Georgios", ""], ["Versluis", "Laurens", ""], ["Mastenbroek", "Fabian", ""], ["Iosup", "Alexandru", ""]]}, {"id": "1808.04311", "submitter": "Xiaofan Zhang", "authors": "Junsong Wang, Qiuwen Lou, Xiaofan Zhang, Chao Zhu, Yonghua Lin, Deming\n  Chen", "title": "Design Flow of Accelerating Hybrid Extremely Low Bit-width Neural\n  Network in Embedded FPGA", "comments": "Accepted by International Conference on Field-Programmable Logic and\n  Applications (FPL'2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network accelerators with low latency and low energy consumption are\ndesirable for edge computing. To create such accelerators, we propose a design\nflow for accelerating the extremely low bit-width neural network (ELB-NN) in\nembedded FPGAs with hybrid quantization schemes. This flow covers both network\ntraining and FPGA-based network deployment, which facilitates the design space\nexploration and simplifies the tradeoff between network accuracy and\ncomputation efficiency. Using this flow helps hardware designers to deliver a\nnetwork accelerator in edge devices under strict resource and power\nconstraints. We present the proposed flow by supporting hybrid ELB settings\nwithin a neural network. Results show that our design can deliver very high\nperformance peaking at 10.3 TOPS and classify up to 325.3 image/s/watt while\nrunning large-scale neural networks for less than 5W using embedded FPGA. To\nthe best of our knowledge, it is the most energy efficient solution in\ncomparison to GPU or other FPGA implementations reported so far in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 09:24:57 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 18:16:49 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Wang", "Junsong", ""], ["Lou", "Qiuwen", ""], ["Zhang", "Xiaofan", ""], ["Zhu", "Chao", ""], ["Lin", "Yonghua", ""], ["Chen", "Deming", ""]]}, {"id": "1808.04345", "submitter": "Michael Jones", "authors": "Michael Jones, Jeremy Kepner, Bradley Orchard, Albert Reuther, William\n  Arcand, David Bestor, Bill Bergeron, Chansup Byun, Vijay Gadepally, Michael\n  Houle, Matthew Hubbell, Anna Klein, Lauren Milechin, Julia Mullen, Andrew\n  Prout, Antonio Rosa, Siddharth Samsi, Charles Yee, Peter Michaleas", "title": "Interactive Launch of 16,000 Microsoft Windows Instances on a\n  Supercomputer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation, machine learning, and data analysis require a wide range of\nsoftware which can be dependent upon specific operating systems, such as\nMicrosoft Windows. Running this software interactively on massively parallel\nsupercomputers can present many challenges. Traditional methods of scaling\nMicrosoft Windows applications to run on thousands of processors have typically\nrelied on heavyweight virtual machines that can be inefficient and slow to\nlaunch on modern manycore processors. This paper describes a unique approach\nusing the Lincoln Laboratory LLMapReduce technology in combination with the\nWine Windows compatibility layer to rapidly and simultaneously launch and run\nMicrosoft Windows applications on thousands of cores on a supercomputer.\nSpecifically, this work demonstrates launching 16,000 Microsoft Windows\napplications in 5 minutes running on 16,000 processor cores. This capability\nsignificantly broadens the range of applications that can be run at large scale\non a supercomputer.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 17:45:51 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Jones", "Michael", ""], ["Kepner", "Jeremy", ""], ["Orchard", "Bradley", ""], ["Reuther", "Albert", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Klein", "Anna", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julia", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Michaleas", "Peter", ""]]}, {"id": "1808.04349", "submitter": "Giuseppe Antonio Di Luna", "authors": "Shantanu Das, Giuseppe Antonio Di Luna, Leszek A. Gasieniec", "title": "Patrolling on Dynamic Ring Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of patrolling the nodes of a network collaboratively by\na team of mobile agents, such that each node of the network is visited by at\nleast one agent once in every $I(n)$ time units, with the objective of\nminimizing the idle time $I(n)$. While patrolling has been studied previously\nfor static networks, we investigate the problem on dynamic networks with a\nfixed set of nodes, but dynamic edges. In particular, we consider\n1-interval-connected ring networks and provide various patrolling algorithms\nfor such networks, for $k=2$ or $k>2$ agents. We also show almost matching\nlower bounds that hold even for the best starting configurations. Thus, our\nalgorithms achieve close to optimal idle time. Further, we show a clear\nseparation in terms of idle time, for agents that have prior knowledge of the\ndynamic networks compared to agents that do not have such knowledge. This paper\nprovides the first known results for collaborative patrolling on dynamic\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 17:52:53 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Das", "Shantanu", ""], ["Di Luna", "Giuseppe Antonio", ""], ["Gasieniec", "Leszek A.", ""]]}, {"id": "1808.04357", "submitter": "Jiarui Fang", "authors": "Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh", "title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "comments": "10 pages. Journal of Parallel and Distributed Computing, 2019", "journal-ref": null, "doi": "10.1016/j.jpdc.2019.05.016", "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism has become a dominant method to scale Deep Neural Network\n(DNN) training across multiple nodes. Since synchronizing a large number of\ngradients of the local model can be a bottleneck for large-scale distributed\ntraining, compressing communication data has gained widespread attention\nrecently. Among several recent proposed compression algorithms, Residual\nGradient Compression (RGC) is one of the most successful approaches---it can\nsignificantly compress the transmitting message size (0.1\\% of the gradient\nsize) of each node and still achieve correct accuracy and the same convergence\nspeed. However, the literature on compressing deep networks focuses almost\nexclusively on achieving good theoretical compression rate, while the\nefficiency of RGC in real distributed implementation has been less\ninvestigated. In this paper, we develop an RGC-based system that is able to\nreduce the end-to-end training time on real-world multi-GPU systems. Our\nproposed design called RedSync, which introduces a set of optimizations to\nreduce communication bandwidth requirement while introducing limited overhead.\nWe evaluate the performance of RedSync on two different multiple GPU platforms,\nincluding 128 GPUs of a supercomputer and an 8-GPU server. Our test cases\ninclude image classification tasks on Cifar10 and ImageNet, and language\nmodeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high\ncommunication to computation ratio, which have long been considered with poor\nscalability, RedSync brings significant performance improvements.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 19:02:47 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 03:25:36 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 09:48:26 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fang", "Jiarui", ""], ["Fu", "Haohuan", ""], ["Yang", "Guangwen", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1808.04479", "submitter": "Seyyedali Hosseinalipour", "authors": "Seyyedali Hosseinalipour, Anuj Nayak and Huaiyu Dai", "title": "Power-Aware Allocation of Graph Jobs in Geo-Distributed Cloud Networks", "comments": "16 pages, 9 figures", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, 2019", "doi": "10.1109/TPDS.2019.2943457", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big-data, the jobs submitted to the clouds exhibit complicated\nstructures represented by graphs, where the nodes denote the sub-tasks each of\nwhich can be accommodated at a slot in a server, while the edges indicate the\ncommunication constraints among the sub-tasks. We develop a framework for\nefficient allocation of graph jobs in geo-distributed cloud networks (GDCNs),\nexplicitly considering the power consumption of the datacenters (DCs). We\naddress the following two challenges arising in graph job allocation: i) the\nallocation problem belongs to NP-hard nonlinear integer programming; ii) the\nallocation requires solving the NP-complete sub-graph isomorphism problem,\nwhich is particularly cumbersome in large-scale GDCNs. We develop a suite of\nefficient solutions for GDCNs of various scales. For small-scale GDCNs, we\npropose an analytical approach based on convex programming. For medium-scale\nGDCNs, we develop a distributed allocation algorithm exploiting the processing\npower of DCs in parallel. Afterward, we provide a novel low-complexity\n(decentralized) sub-graph extraction method, based on which we introduce cloud\ncrawlers aiming to extract allocations of good potentials for large-scale\nGDCNs. Given these suggested strategies, we further investigate strategy\nselection under both fixed and adaptive DC pricing schemes, and propose an\nonline learning algorithm for each.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 22:15:03 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 02:06:53 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 16:00:09 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 16:06:35 GMT"}, {"version": "v5", "created": "Thu, 14 Nov 2019 19:36:11 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Hosseinalipour", "Seyyedali", ""], ["Nayak", "Anuj", ""], ["Dai", "Huaiyu", ""]]}, {"id": "1808.04487", "submitter": "Andreas Mang", "authors": "Andreas Mang and Amir Gholami and Christos Davatzikos and George Biros", "title": "CLAIRE: A distributed-memory solver for constrained large deformation\n  diffeomorphic image registration", "comments": "37 pages;", "journal-ref": "SIAM Journal on Scientific Computing, 41(5):C548-C584, 2019", "doi": "10.1137/18M1207818", "report-no": null, "categories": "math.OC cs.CV cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With this work, we release CLAIRE, a distributed-memory implementation of an\neffective solver for constrained large deformation diffeomorphic image\nregistration problems in three dimensions. We consider an optimal control\nformulation. We invert for a stationary velocity field that parameterizes the\ndeformation map. Our solver is based on a globalized, preconditioned, inexact\nreduced space Gauss--Newton--Krylov scheme. We exploit state-of-the-art\ntechniques in scientific computing to develop an effective solver that scales\nto thousands of distributed memory nodes on high-end clusters. We present the\nformulation, discuss algorithmic features, describe the software package, and\nintroduce an improved preconditioner for the reduced space Hessian to speed up\nthe convergence of our solver. We test registration performance on synthetic\nand real data. We demonstrate registration accuracy on several neuroimaging\ndatasets. We compare the performance of our scheme against different flavors of\nthe Demons algorithm for diffeomorphic image registration. We study convergence\nof our preconditioner and our overall algorithm. We report scalability results\non state-of-the-art supercomputing platforms. We demonstrate that we can solve\nregistration problems for clinically relevant data sizes in two to four minutes\non a standard compute node with 20 cores, attaining excellent data fidelity.\nWith the present work we achieve a speedup of (on average) 5$\\times$ with a\npeak performance of up to 17$\\times$ compared to our former work.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 22:59:25 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 21:50:57 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mang", "Andreas", ""], ["Gholami", "Amir", ""], ["Davatzikos", "Christos", ""], ["Biros", "George", ""]]}, {"id": "1808.04579", "submitter": "Aaron Montag", "authors": "Aaron Montag and J\\\"urgen Richter-Gebert", "title": "Bringing Together Dynamic Geometry Software and the Graphics Processing\n  Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.SC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We equip dynamic geometry software (DGS) with a user-friendly method that\nenables massively parallel calculations on the graphics processing unit (GPU).\nThis interplay of DGS and GPU opens up various applications in education and\nmathematical research. The GPU-aided discovery of mathematical properties,\ninteractive visualizations of algebraic surfaces (raycasting), the mathematical\ndeformation of images and footage in real-time, and computationally demanding\nnumerical simulations of PDEs are examples from the long and versatile list of\nnew domains that our approach makes accessible within a DGS. We ease the\ndevelopment of complex (mathematical) visualizations and provide a\nrapid-prototyping scheme for general-purpose computations (GPGPU).\n  The possibility to program both CPU and GPU with the use of only one\nhigh-level (scripting) programming language is a crucial aspect of our concept.\nWe embed shader programming seamlessly within a high-level (scripting)\nprogramming environment. The aforementioned requires the symbolic process of\nthe transcompilation of a high-level programming language into shader\nprogramming language for GPU and, in this article, we address the challenge of\nthe automatic translation of a high-level programming language to a shader\nlanguage of the GPU. To maintain platform independence and the possibility to\nuse our technology on modern devices, we focus on a realization through WebGL.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 08:20:36 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Montag", "Aaron", ""], ["Richter-Gebert", "J\u00fcrgen", ""]]}, {"id": "1808.04761", "submitter": "Mengjia Yan", "authors": "Mengjia Yan, Christopher Fletcher, Josep Torrellas", "title": "Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are fast becoming ubiquitous for their ability to\nattain good accuracy in various machine learning tasks. A DNN's architecture\n(i.e., its hyper-parameters) broadly determines the DNN's accuracy and\nperformance, and is often confidential. Attacking a DNN in the cloud to obtain\nits architecture can potentially provide major commercial value. Further,\nattaining a DNN's architecture facilitates other, existing DNN attacks.\n  This paper presents Cache Telepathy: a fast and accurate mechanism to steal a\nDNN's architecture using the cache side channel. Our attack is based on the\ninsight that DNN inference relies heavily on tiled GEMM (Generalized Matrix\nMultiply), and that DNN architecture parameters determine the number of GEMM\ncalls and the dimensions of the matrices used in the GEMM functions. Such\ninformation can be leaked through the cache side channel.\n  This paper uses Prime+Probe and Flush+Reload to attack VGG and ResNet DNNs\nrunning OpenBLAS and Intel MKL libraries. Our attack is effective in helping\nobtain the architectures by very substantially reducing the search space of\ntarget DNN architectures. For example, for VGG using OpenBLAS, it reduces the\nsearch space from more than $10^{35}$ architectures to just 16.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:50:15 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Yan", "Mengjia", ""], ["Fletcher", "Christopher", ""], ["Torrellas", "Josep", ""]]}, {"id": "1808.04849", "submitter": "Wade Schulz", "authors": "Jacob McPadden, Thomas JS Durant, Dustin R Bunch, Andreas Coppi,\n  Nathan Price, Kris Rodgerson, Charles J Torre Jr, William Byron, H Patrick\n  Young, Allen L Hsiao, Harlan M Krumholz, Wade L Schulz", "title": "A Scalable Data Science Platform for Healthcare and Precision Medicine\n  Research", "comments": "8 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To (1) demonstrate the implementation of a data science platform\nbuilt on open-source technology within a large, academic healthcare system and\n(2) describe two computational healthcare applications built on such a\nplatform. Materials and Methods: A data science platform based on several open\nsource technologies was deployed to support real-time, big data workloads. Data\nacquisition workflows for Apache Storm and NiFi were developed in Java and\nPython to capture patient monitoring and laboratory data for downstream\nanalytics. Results: The use of emerging data management approaches along with\nopen-source technologies such as Hadoop can be used to create integrated data\nlakes to store large, real-time data sets. This infrastructure also provides a\nrobust analytics platform where healthcare and biomedical research data can be\nanalyzed in near real-time for precision medicine and computational healthcare\nuse cases. Discussion: The implementation and use of integrated data science\nplatforms offer organizations the opportunity to combine traditional data sets,\nincluding data from the electronic health record, with emerging big data\nsources, such as continuous patient monitoring and real-time laboratory\nresults. These platforms can enable cost-effective and scalable analytics for\nthe information that will be key to the delivery of precision medicine\ninitiatives. Conclusion: Organizations that can take advantage of the technical\nadvances found in data science platforms will have the opportunity to provide\ncomprehensive access to healthcare data for computational healthcare and\nprecision medicine research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 18:31:22 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["McPadden", "Jacob", ""], ["Durant", "Thomas JS", ""], ["Bunch", "Dustin R", ""], ["Coppi", "Andreas", ""], ["Price", "Nathan", ""], ["Rodgerson", "Kris", ""], ["Torre", "Charles J", "Jr"], ["Byron", "William", ""], ["Young", "H Patrick", ""], ["Hsiao", "Allen L", ""], ["Krumholz", "Harlan M", ""], ["Schulz", "Wade L", ""]]}, {"id": "1808.04866", "submitter": "Clement Fung", "authors": "Clement Fung, Chris J.M. Yoon, Ivan Beschastnikh", "title": "Mitigating Sybils in Federated Learning Poisoning", "comments": "16 pages, Extended technical version of conference paper \"The\n  Limitations of Federated Learning in Sybil Settings\" accepted at RAID 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) over distributed multi-party data is required for a\nvariety of domains. Existing approaches, such as federated learning, collect\nthe outputs computed by a group of devices at a central aggregator and run\niterative algorithms to train a globally shared model. Unfortunately, such\napproaches are susceptible to a variety of attacks, including model poisoning,\nwhich is made substantially worse in the presence of sybils.\n  In this paper we first evaluate the vulnerability of federated learning to\nsybil-based poisoning attacks. We then describe \\emph{FoolsGold}, a novel\ndefense to this problem that identifies poisoning sybils based on the diversity\nof client updates in the distributed learning process. Unlike prior work, our\nsystem does not bound the expected number of attackers, requires no auxiliary\ninformation outside of the learning process, and makes fewer assumptions about\nclients and their data.\n  In our evaluation we show that FoolsGold exceeds the capabilities of existing\nstate of the art approaches to countering sybil-based label-flipping and\nbackdoor poisoning attacks. Our results hold for different distributions of\nclient data, varying poisoning targets, and various sybil strategies.\n  Code can be found at: https://github.com/DistributedML/FoolsGold\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:20:35 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 19:32:10 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 00:14:25 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 17:54:48 GMT"}, {"version": "v5", "created": "Wed, 15 Jul 2020 15:35:17 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Fung", "Clement", ""], ["Yoon", "Chris J. M.", ""], ["Beschastnikh", "Ivan", ""]]}, {"id": "1808.04883", "submitter": "Lie He", "authors": "Lie He, An Bian and Martin Jaggi", "title": "COLA: Decentralized Linear Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized machine learning is a promising emerging paradigm in view of\nglobal challenges of data ownership and privacy. We consider learning of linear\nclassification and regression models, in the setting where the training data is\ndecentralized over many user devices, and the learning algorithm must run\non-device, on an arbitrary communication network, without a central\ncoordinator. We propose COLA, a new decentralized training algorithm with\nstrong theoretical guarantees and superior practical performance. Our framework\novercomes many limitations of existing methods, and achieves communication\nefficiency, scalability, elasticity as well as resilience to changes in data\nand participating devices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 11:45:16 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 20:24:00 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 17:29:52 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 22:13:40 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["He", "Lie", ""], ["Bian", "An", ""], ["Jaggi", "Martin", ""]]}, {"id": "1808.05056", "submitter": "Alexey Lastovetsky", "authors": "Daniel Hanlon, Hamidreza Khalighzadeh, Ravi Reddy Manumachu and Alexey\n  Lastovetsky", "title": "libhclooc: Software Library Facilitating Out-of-core Implementations of\n  Accelerator Kernels on Hybrid Computing Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware accelerators such as Graphics Processing Units (GPUs), Intel Xeon\nPhi co-processors (PHIs), and Field-Programmable Gate Arrays (FPGAs) are now\nubiquitous in extreme-scale high performance computing (HPC), cloud, and Big\ndata platforms to facilitate execution of workloads that demand high energy\nefficiency. They present unique interfaces and programming models therefore\nposing several limitations, which must be addressed to facilitate execution of\nlarge workloads. There is no library providing a unifying interface that allows\nprogrammers to write reusable out-of-core implementations of their\ndata-parallel kernels that can run efficiently on different mainstream\naccelerators such as GPUs, PHIs, and FPGAs. We address this shortage in this\npaper. We present a library called libhclooc, which provides a unifying\ninterface facilitating out-of-core implementations for data parallel kernels on\nthe three different mainstream accelerators (GPUs, Intel Xeon Phis, FPGAs). We\nimplement out-of-core matrix-matrix multiplication (MMOOC) using the libhclooc\nAPI and demonstrate its superior performance over vendor implementations. We\nshow that it suffers from a maximum overhead of 10%, 4%, and 8% (due to\nabstraction) compared to the state-of-the-art optimised implementations for\nNvidia K40c GPU, Nvidia P100 PCIe GPU, and Intel Xeon Phi 3120P respectively.\nWe also show that using libhclooc API reduces the number of lines of code (LOC)\nby 75% thereby drastically improving programmer productivity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 12:38:19 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Hanlon", "Daniel", ""], ["Khalighzadeh", "Hamidreza", ""], ["Manumachu", "Ravi Reddy", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1808.05078", "submitter": "Heejin Park", "authors": "Heejin Park, Shuang Zhai, Long Lu, Felix Xiaozhu Lin", "title": "StreamBox-TZ: Secure Stream Analytics at the Edge with TrustZone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is compelling to process large streams of IoT data on the cloud\nedge, doing so exposes the data to a sophisticated, vulnerable software stack\non the edge and hence security threats. To this end, we advocate isolating the\ndata and its computations in a trusted execution environment (TEE) on the edge,\nshielding them from the remaining edge software stack which we deem untrusted.\nThis approach faces two major challenges: (1) executing high-throughput,\nlow-delay stream analytics in a single TEE, which is constrained by a low\ntrusted computing base (TCB) and limited physical memory; (2) verifying\nexecution of stream analytics as the execution involves untrusted software\ncomponents on the edge. In response, we present StreamBox-TZ (SBT), a stream\nanalytics engine for an edge platform that offers strong data security,\nverifiable results, and good performance. SBT contributes a data plane designed\nand optimized for a TEE based on ARM TrustZone. It supports continuous remote\nattestation for analytics correctness and result freshness while incurring low\noverhead. SBT only adds 42.5 KB executable to the TCB (16% of the entire TCB).\nOn an octa core ARMv8 platform, it delivers the state-of-the-art performance by\nprocessing input events up to 140 MB/sec (12M events/sec) with sub-second\ndelay. The overhead incurred by SBT's security mechanism is less than 25%.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 21:21:50 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 20:52:44 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Park", "Heejin", ""], ["Zhai", "Shuang", ""], ["Lu", "Long", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "1808.05156", "submitter": "Yixin Tao", "authors": "Richard Cole and Yixin Tao", "title": "An Analysis of Asynchronous Stochastic Accelerated Coordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent, and coordinate descent in particular, are core tools in\nmachine learning and elsewhere. Large problem instances are common. To help\nsolve them, two orthogonal approaches are known: acceleration and parallelism.\nIn this work, we ask whether they can be used simultaneously. The answer is\n\"yes\".\n  More specifically, we consider an asynchronous parallel version of the\naccelerated coordinate descent algorithm proposed and analyzed by Lin, Liu and\nXiao (SIOPT'15). We give an analysis based on the efficient implementation of\nthis algorithm. The only constraint is a standard bounded asynchrony\nassumption, namely that each update can overlap with at most q others. (q is at\nmost the number of processors times the ratio in the lengths of the longest and\nshortest updates.) We obtain the following three results:\n  1. A linear speedup for strongly convex functions so long as q is not too\nlarge.\n  2. A substantial, albeit sublinear, speedup for strongly convex functions for\nlarger q.\n  3. A substantial, albeit sublinear, speedup for convex functions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:46:56 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Cole", "Richard", ""], ["Tao", "Yixin", ""]]}, {"id": "1808.05212", "submitter": "Heinz Bauschke", "authors": "Alan W. Paeth", "title": "Non-Interfering Concurrent Exchange (NICE) Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studying the statistical frequency of exchange in comparison-exchange (CE)\nnetworks we discover a new elementary form of comparison-exchange which we name\nthe \"2-op\". The operation supports concurrent and non-interfering operations of\ntwo traditional CEs upon one shared element. More than merely improving overall\nstatistical performance, the introduction of NICE (non-interfering CE) networks\nlowers long-held bounds in the number of stages required for sorting tasks.\nCode-based CEs also benefit from improved average/worst case run time costs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 22:57:57 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Paeth", "Alan W.", ""]]}, {"id": "1808.05325", "submitter": "Timur Bazhirov", "authors": "Protik Das, Timur Bazhirov", "title": "Electronic properties of binary compounds with high fidelity and high\n  throughput", "comments": "10 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1807.05623", "journal-ref": null, "doi": "10.1088/1742-6596/1290/1/012011", "report-no": null, "categories": "cond-mat.mtrl-sci cond-mat.other cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present example applications of an approach to high-throughput\nfirst-principles calculations of the electronic properties of materials\nimplemented within the Exabyte.io platform. We deploy computational techniques\nbased on the Density Functional Theory with both Generalized Gradient\nApproximation (GGA) and Hybrid Screened Exchange (HSE) in order to extract the\nelectronic band gaps and band structures for a set of 775 binary compounds. We\nfind that for HSE, the average relative error fits within 22%, whereas for GGA\nit is 49%. We find the average calculation time on an up-to-date server\ncentrally available from a public cloud provider to fit within 1.2 and 36 hours\nfor GGA and HSE, respectively. The results and the associated data, including\nthe materials and simulation workflows, are standardized and made available\nonline in an accessible, repeatable and extensible setting.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 02:18:08 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Das", "Protik", ""], ["Bazhirov", "Timur", ""]]}, {"id": "1808.05338", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Limitations of performance of Exascale Applications and supercomputers\n  they are running on", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper highlights that the cooperation of the components of the computing\nsystems receives even more focus in the coming age of exascale computing. It\ndiscovers that inherent performance limitations exist and identifies the major\ncritical contributions of the performance on many-many processor systems. The\nextended and reinterpreted simple Amdahl model describes the behavior of the\nexisting supercomputers surprisingly well, and explains some mystical\nhappenings around high-performance computing. It is pointed out that using the\npresent technology and paradigm only marginal development of performance is\npossible, and that the major obstacle towards higher performance applications\nis the 70-years old computing paradigm itself. A way to step forward is also\nsuggested\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 03:45:53 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1808.05389", "submitter": "Dominik Kaaser", "authors": "Petra Berenbrink and Tom Friedetzky and Dominik Kaaser and Peter Kling", "title": "Simple Load Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following load balancing process for $m$ tokens distributed\narbitrarily among $n$ nodes connected by a complete graph: In each time step a\npair of nodes is selected uniformly at random. Let $\\ell_1$ and $\\ell_2$ be\ntheir respective number of tokens. The two nodes exchange tokens such that they\nhave $\\lceil(\\ell_1 + \\ell_2)/2\\rceil$ and $\\lfloor(\\ell_1 + \\ell_2)/2\\rfloor$\ntokens, respectively. We provide a simple analysis showing that this process\nreaches almost perfect balance within $O(n\\log{n} + n \\log{\\Delta})$ steps,\nwhere $\\Delta$ is the maximal initial load difference between any two nodes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:36:14 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Berenbrink", "Petra", ""], ["Friedetzky", "Tom", ""], ["Kaaser", "Dominik", ""], ["Kling", "Peter", ""]]}, {"id": "1808.05405", "submitter": "Alexey Lastovetsky", "authors": "Semyon Khokhriakov, Ravi Reddy, Alexey Lastovetsky", "title": "Novel Model-based Methods for Performance Optimization of Multithreaded\n  2D Discrete Fourier Transform on Multicore Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use multithreaded fast Fourier transforms provided in three\nhighly optimized packages, FFTW-2.1.5, FFTW-3.3.7, and Intel MKL FFT, to\npresent a novel model-based parallel computing technique as a very effective\nand portable method for optimization of scientific multithreaded routines for\nperformance, especially in the current multicore era where the processors have\nabundant number of cores. We propose two optimization methods, PFFT-FPM and\nPFFT-FPM-PAD, based on this technique. They compute 2D-DFT of a complex signal\nmatrix of size NxN using p abstract processors. Both algorithms take as inputs,\ndiscrete 3D functions of performance against problem size of the processors and\noutput the transformed signal matrix. Based on our experiments on a modern\nIntel Haswell multicore server consisting of 36 physical cores, the average and\nmaximum speedups observed for PFFT-FPM using FFTW-3.3.7 are 1.9x and 6.8x\nrespectively and the average and maximum speedups observed using Intel MKL FFT\nare 1.3x and 2x respectively. The average and maximum speedups observed for\nPFFT-FPM-PAD using FFTW-3.3.7 are 2x and 9.4x respectively and the average and\nmaximum speedups observed using Intel MKL FFT are 1.4x and 5.9x respectively.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 10:34:29 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Khokhriakov", "Semyon", ""], ["Reddy", "Ravi", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1808.05567", "submitter": "Evangelos Georganas", "authors": "Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj\n  Kalamkar, Greg Henry, Hans Pabst, Alexander Heinecke", "title": "Anatomy Of High-Performance Deep Learning Convolutions On SIMD\n  Architectures", "comments": "Accepted to SC18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution layers are prevalent in many classes of deep neural networks,\nincluding Convolutional Neural Networks (CNNs) which provide state-of-the-art\nresults for tasks like image recognition, neural machine translation and speech\nrecognition. The computationally expensive nature of a convolution operation\nhas led to the proliferation of implementations including matrix-matrix\nmultiplication formulation, and direct convolution primarily targeting GPUs. In\nthis paper, we introduce direct convolution kernels for x86 architectures, in\nparticular for Xeon and XeonPhi systems, which are implemented via a dynamic\ncompilation approach. Our JIT-based implementation shows close to theoretical\npeak performance, depending on the setting and the CPU architecture at hand. We\nadditionally demonstrate how these JIT-optimized kernels can be integrated into\na lightweight multi-node graph execution model. This illustrates that single-\nand multi-node runs yield high efficiencies and high image-throughputs when\nexecuting state-of-the-art image recognition tasks on CPUs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:18:44 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 21:08:32 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Georganas", "Evangelos", ""], ["Avancha", "Sasikanth", ""], ["Banerjee", "Kunal", ""], ["Kalamkar", "Dhiraj", ""], ["Henry", "Greg", ""], ["Pabst", "Hans", ""], ["Heinecke", "Alexander", ""]]}, {"id": "1808.05696", "submitter": "Federico Terraneo", "authors": "Federico Terraneo, Fabiano Riccardi, Alberto Leva", "title": "Jitter-compensated VHT and its application to WSN clock synchronization", "comments": null, "journal-ref": null, "doi": "10.1109/RTSS.2017.00033", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and energy-efficient clock synchronization is an enabler for many\napplications of Wireless Sensor Networks. A fine-grained synchronization is\nbeneficial both at the system level, for example to favor deterministic radio\nprotocols, and at the application level, when network-wide event timestamping\nis required. However, there is a tradeoff between the resolution of a WSN\nnode's timekeeping device and its energy consumption. The Virtual\nHigh-resolution Timer (VHT) is an innovative solution, that was proposed to\novercome this tradeoff. It combines a high-resolution oscillator to a low-power\none, turning off the former when not needed. In this paper we improve VHT by\nfirst identifying the jitter of the low-power oscillator as the current limit\nto the technique, and then proposing an enhanced solution that synchronizes the\nfast and the slow clock, rejecting the said jitter. The improved VHT is also\nless demanding than the original technique in terms of hardware resources.\nExperimental results show the achieved advantages in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 22:25:34 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Terraneo", "Federico", ""], ["Riccardi", "Fabiano", ""], ["Leva", "Alberto", ""]]}, {"id": "1808.05698", "submitter": "Mohammad Roohitavaf", "authors": "Mohammad Roohitavaf, Jung-Sang Ahn, Woon-Hak Kang, Kun Ren, Gene\n  Zhang, Sami Ben-Romdhane, Sandeep S. Kulkarni", "title": "Session Guarantees with Raft and Hybrid Logical Clocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eventual consistency is a popular consistency model for geo-replicated data\nstores. Although eventual consistency provides high performance and\navailability, it can cause anomalies that make programming complex for\napplication developers. Session guarantees can remove some of these anomalies\nwhile causing much lower overhead compared with stronger consistency models. In\nthis paper, we provide a protocol for providing session guarantees for NuKV, a\nkey-value store developed for services with very high availability and\nperformance requirements at eBay. NuKV relies on the Raft protocol for\nreplication inside datacenters, and uses eventual consistency for replication\namong datacenters. We provide modified versions of conventional session\nguarantees to avoid the problem of slowdown cascades in systems with large\nnumbers of partitions. We also use Hybrid Logical Clocks to eliminate the need\nfor delaying write operations to satisfy session guarantees. Our experiments\nshow that our protocol provides session guarantees with a negligible overhead\nwhen compared with eventual consistency.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 22:54:25 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Roohitavaf", "Mohammad", ""], ["Ahn", "Jung-Sang", ""], ["Kang", "Woon-Hak", ""], ["Ren", "Kun", ""], ["Zhang", "Gene", ""], ["Ben-Romdhane", "Sami", ""], ["Kulkarni", "Sandeep S.", ""]]}, {"id": "1808.05809", "submitter": "Ran Ben Basat", "authors": "Ran Ben-Basat, Guy Even, Ken-ichi Kawarabayashi, Gregory Schwartzman", "title": "Optimal Distributed Weighted Set Cover Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a time-optimal deterministic distributed algorithm for\napproximating a minimum weight vertex cover in hypergraphs of rank $f$. This\nproblem is equivalent to the Minimum Weight Set Cover Problem in which the\nfrequency of every element is bounded by $f$. The approximation factor of our\nalgorithm is $(f+\\epsilon)$. Let $\\Delta$ denote the maximum degree in the\nhypergraph. Our algorithm runs in the CONGEST model and requires\n$O(\\log{\\Delta} / \\log \\log \\Delta)$ rounds, for constants $\\epsilon \\in (0,1]$\nand $f\\in\\mathbb N^+$. This is the first distributed algorithm for this problem\nwhose running time does not depend on the vertex weights or the number of\nvertices. Thus adding another member to the exclusive family of \\emph{provably\noptimal} distributed algorithms.\n  For constant values of $f$ and $\\epsilon$, our algorithm improves over the\n$(f+\\epsilon)$-approximation algorithm of \\cite{KuhnMW06} whose running time is\n$O(\\log \\Delta + \\log W)$, where $W$ is the ratio between the largest and\nsmallest vertex weights in the graph.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 09:50:30 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Ben-Basat", "Ran", ""], ["Even", "Guy", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1808.05933", "submitter": "Amir Daneshmand", "authors": "Amir Daneshmand and Ying Sun and Gesualdo Scutari and Francisco\n  Facchinei and Brian M. Sadler", "title": "Decentralized Dictionary Learning Over Time-Varying Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies Dictionary Learning problems wherein the learning task is\ndistributed over a multi-agent network, modeled as a time-varying directed\ngraph. This formulation is relevant, for instance, in Big Data scenarios where\nmassive amounts of data are collected/stored in different locations (e.g.,\nsensors, clouds) and aggregating and/or processing all data in a fusion center\nmight be inefficient or unfeasible, due to resource limitations, communication\noverheads or privacy issues. We develop a unified decentralized algorithmic\nframework for this class of nonconvex problems, which is proved to converge to\nstationary solutions at a sublinear rate. The new method hinges on Successive\nConvex Approximation techniques, coupled with a decentralized tracking\nmechanism aiming at locally estimating the gradient of the smooth part of the\nsum-utility. To the best of our knowledge, this is the first provably\nconvergent decentralized algorithm for Dictionary Learning and, more generally,\nbi-convex problems over (time-varying) (di)graphs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 17:20:06 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 17:56:54 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Daneshmand", "Amir", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""], ["Facchinei", "Francisco", ""], ["Sadler", "Brian M.", ""]]}, {"id": "1808.06008", "submitter": "Liang Bao", "authors": "Liang Bao, Xin Liu and Weizhao Chen", "title": "Learning-based Automatic Parameter Tuning for Big Data Analytics\n  Frameworks", "comments": "12 pages, submitted to IEEE BigData 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics frameworks (BDAFs) have been widely used for data\nprocessing applications. These frameworks provide a large number of\nconfiguration parameters to users, which leads to a tuning issue that\noverwhelms users. To address this issue, many automatic tuning approaches have\nbeen proposed. However, it remains a critical challenge to generate enough\nsamples in a high-dimensional parameter space within a time constraint. In this\npaper, we present AutoTune--an automatic parameter tuning system that aims to\noptimize application execution time on BDAFs. AutoTune first constructs a\nsmaller-scale testbed from the production system so that it can generate more\nsamples, and thus train a better prediction model, under a given time\nconstraint. Furthermore, the AutoTune algorithm produces a set of samples that\ncan provide a wide coverage over the high-dimensional parameter space, and\nsearches for more promising configurations using the trained prediction model.\nAutoTune is implemented and evaluated using the Spark framework and HiBench\nbenchmark deployed on a public cloud. Extensive experimental results illustrate\nthat AutoTune improves on default configurations by 63.70% on average, and on\nthe five state-of-the-art tuning algorithms by 6%-23%.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 21:53:09 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Bao", "Liang", ""], ["Liu", "Xin", ""], ["Chen", "Weizhao", ""]]}, {"id": "1808.06074", "submitter": "Jyothi Krishna V S", "authors": "Jyothi Krishna V S and Shankar Balachandran", "title": "Compiler Enhanced Scheduling for OpenMP for Heterogeneous\n  Multiprocessors", "comments": "6 Pages, 4 figures, Presented in 2nd EEHCO (Energy Efficiency with\n  Heterogenous Computing) Workshop in Prague 2016 (Part of HiPEAC event 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling in Asymmetric Multicore Processors (AMP), a special case of\nHeterogeneous Multiprocessors, is a widely studied topic. The scheduling\ntechniques which are mostly runtime do not usually consider parallel\nprogramming pattern used in parallel programming frameworks like OpenMP. On the\nother hand, current compilers for these parallel programming platforms are\nhardware oblivious which prevent any compile-time optimization for platforms\nlike big.LITTLE and has to completely rely on runtime optimization. In this\npaper, we propose a hardware-aware Compiler Enhanced Scheduling (CES) where the\ncommon compiler transformations are coupled with compiler added scheduling\ncommands to take advantage of the hardware asymmetry and improve the runtime\nefficiency. We implement a compiler for OpenMP and demonstrate its efficiency\nin Samsung Exynos with big.LITTLE architecture. On an average, we see 18%\nreduction in runtime and 14% reduction in energy consumption in standard NPB\nand FSU benchmarks with CES across multiple frequencies and core configurations\nin big.LITTLE.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 11:39:54 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["S", "Jyothi Krishna V", ""], ["Balachandran", "Shankar", ""]]}, {"id": "1808.06094", "submitter": "Jia Zou", "authors": "Jia Zou, Arun Iyengar, Chris Jermaine", "title": "Pangea: Monolithic Distributed Storage for Data Analytics", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage and memory systems for modern data analytics are heavily layered,\nmanaging shared persistent data, cached data, and non-shared execution data in\nseparate systems such as distributed file system like HDFS, in-memory file\nsystem like Alluxio and computation framework like Spark. Such layering\nintroduces significant performance and management costs for copying data across\nlayers redundantly and deciding proper resource allocation for all layers. In\nthis paper we propose a single system called Pangea that can manage all\ndata---both intermediate and long-lived data, and their buffer/caching, data\nplacement optimization, and failure recovery---all in one monolithic storage\nsystem, without any layering. We present a detailed performance evaluation of\nPangea and show that its performance compares favorably with several widely\nused layered systems such as Spark.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 15:46:40 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 16:11:19 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Zou", "Jia", ""], ["Iyengar", "Arun", ""], ["Jermaine", "Chris", ""]]}, {"id": "1808.06113", "submitter": "Sanaa Mohamed", "authors": "Sanaa Hamid Mohamed, Taisir E. H. El-Gorashi, and Jaafar M. H.\n  Elmirghani", "title": "Energy Efficiency of Server-Centric PON Data Center Architecture for Fog\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we utilize Mixed Integer Linear Programming (MILP) models to\ncompare the energy efficiency and performance of a server-centric Passive\nOptical Networks (PON)-based data centers design with different data centers\nnetworking topologies for the use in fog computing. For representative\nMapReduce workloads, completion time results indicate that the server-centric\nPON-based design achieves 67% reduction in the energy consumption compared to\nDCell with equivalent performance.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 18:57:32 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Mohamed", "Sanaa Hamid", ""], ["El-Gorashi", "Taisir E. H.", ""], ["Elmirghani", "Jaafar M. H.", ""]]}, {"id": "1808.06115", "submitter": "Sanaa Mohamed", "authors": "Sanaa Hamid Mohamed, Taisir E. H. El-Gorashi, and Jaafar M. H.\n  Elmirghani", "title": "Impact of Link Failures on the Performance of MapReduce in Data Center\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we utilize Mixed Integer Linear Programming (MILP) models to\ndetermine the impact of link failures on the performance of shuffling\noperations in MapReduce when different data center network (DCN) topologies are\nused. For a set of non-fatal single and multi-links failures, the results\nindicate that different DCNs experience different completion time degradations\nranging between 5% and 40%. The best performance under links failures is\nachieved by a server-centric PON-based DCN.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 19:02:53 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Mohamed", "Sanaa Hamid", ""], ["El-Gorashi", "Taisir E. H.", ""], ["Elmirghani", "Jaafar M. H.", ""]]}, {"id": "1808.06332", "submitter": "Shashikant Ilager Mr", "authors": "Shashikant Ilager, Rajeev Wankar, Raghavendra Kune and Rajkumar Buyya", "title": "GPU PaaS Computation Model in Aneka Cloud Computing Environment", "comments": "Submitted as book chapter, under processing, 32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the surge in the volume of data generated and rapid advancement in\nArtificial Intelligence (AI) techniques like machine learning and deep\nlearning, the existing traditional computing models have become inadequate to\nprocess an enormous volume of data and the complex application logic for\nextracting intrinsic information. Computing accelerators such as Graphics\nprocessing units (GPUs) have become de facto SIMD computing system for many big\ndata and machine learning applications. On the other hand, the traditional\ncomputing model has gradually switched from conventional ownership-based\ncomputing to subscription-based cloud computing model. However, the lack of\nprogramming models and frameworks to develop cloud-native applications in a\nseamless manner to utilize both CPU and GPU resources in the cloud has become a\nbottleneck for rapid application development. To support this application\ndemand for simultaneous heterogeneous resource usage, programming models and\nnew frameworks are needed to manage the underlying resources effectively. Aneka\nis emerged as a popular PaaS computing model for the development of Cloud\napplications using multiple programming models like Thread, Task, and MapReduce\nin a single container .NET platform. Since, Aneka addresses MIMD application\ndevelopment that uses CPU based resources and GPU programming like CUDA is\ndesigned for SIMD application development, here, the chapter discusses GPU PaaS\ncomputing model for Aneka Clouds for rapid cloud application development for\n.NET platforms. The popular opensource GPU libraries are utilized and\nintegrated it into the existing Aneka task programming model. The scheduling\npolicies are extended that automatically identify GPU machines and schedule\nrespective tasks accordingly. A case study on image processing is discussed to\ndemonstrate the system, which has been built using PaaS Aneka SDKs and CUDA\nlibrary.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 07:47:47 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Ilager", "Shashikant", ""], ["Wankar", "Rajeev", ""], ["Kune", "Raghavendra", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1808.06334", "submitter": "Jeffrey Young", "authors": "Will Powell, Jason Riedy, Jeffrey S. Young, Thomas M. Conte", "title": "Wrangling Rogues: Managing Experimental Post-Moore Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rogues Gallery is a new experimental testbed that is focused on tackling\n\"rogue\" architectures for the Post-Moore era of computing. While some of these\ndevices have roots in the embedded and high-performance computing spaces,\nmanaging current and emerging technologies provides a challenge for system\nadministration that are not always foreseen in traditional data center\nenvironments.\n  We present an overview of the motivations and design of the initial Rogues\nGallery testbed and cover some of the unique challenges that we have seen and\nforesee with upcoming hardware prototypes for future post-Moore research.\nSpecifically, we cover the networking, identity management, scheduling of\nresources, and tools and sensor access aspects of the Rogues Gallery and\ntechniques we have developed to manage these new platforms.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 07:57:47 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 14:37:33 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 20:52:05 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2019 00:45:12 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Powell", "Will", ""], ["Riedy", "Jason", ""], ["Young", "Jeffrey S.", ""], ["Conte", "Thomas M.", ""]]}, {"id": "1808.06411", "submitter": "Christian Schulz", "authors": "Sebastian Schlag, Christian Schulz, Daniel Seemaier, Darren Strash", "title": "Scalable Edge Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-centric distributed computations have appeared as a recent technique to\nimprove the shortcomings of think-like-a-vertex algorithms on large scale-free\nnetworks. In order to increase parallelism on this model, edge partitioning -\npartitioning edges into roughly equally sized blocks - has emerged as an\nalternative to traditional (node-based) graph partitioning. In this work, we\ngive a distributed memory parallel algorithm to compute high-quality edge\npartitions in a scalable way. Our algorithm scales to networks with billions of\nedges, and runs efficiently on thousands of PEs. Our technique is based on a\nfast parallelization of split graph construction and a use of advanced node\npartitioning algorithms. Our extensive experiments show that our algorithm has\nhigh quality on large real-world networks and large hyperbolic random graphs,\nwhich have a power law degree distribution and are therefore specifically\ntargeted by edge partitioning\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 12:09:31 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 13:14:42 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Schlag", "Sebastian", ""], ["Schulz", "Christian", ""], ["Seemaier", "Daniel", ""], ["Strash", "Darren", ""]]}, {"id": "1808.06705", "submitter": "Paul Burkhardt", "authors": "Paul Burkhardt", "title": "Graph connectivity in log steps using label propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fastest deterministic algorithms for connected components take\nlogarithmic time and perform superlinear work on a Parallel Random Access\nMachine (PRAM). These algorithms maintain a spanning forest by merging and\ncompressing trees, which requires pointer-chasing operations that increase\nmemory access latency and are limited to shared-memory systems. Many of these\nPRAM algorithms are also very complicated to implement. Another popular method\nis \"leader-contraction\" where the challenge is to select a constant fraction of\nleaders that are adjacent to a constant fraction of non-leaders with high\nprobability. Instead we investigate label propagation because it is\ndeterministic and does not rely on pointer-chasing. Label propagation exchanges\nrepresentative labels within a component using simple graph traversal, but it\nis inherently difficult to complete in a sublinear number of steps. We are able\nto solve the problems with label propagation for graph connectivity.\n  We introduce a surprisingly simple framework for deterministic graph\nconnectivity using label propagation that is easily adaptable to many\ncomputational models. It propagates directed edges and alternates edge\ndirection to achieve linear edge count each step and sublinear convergence. We\npresent new algorithms in PRAM, Stream, and MapReduce for a simple, undirected\ngraph $G=(V,E)$ with $n=|V|$ vertices, $m=|E|$ edges. Our approach takes $O(m)$\nwork each step, but we can only prove logarithmic convergence on a path graph.\nIt was conjectured by Liu and Tarjan (2019) to take $O(\\log n)$ steps or\npossibly $O(\\log^2 n)$ steps. We leave the proof of convergence as an open\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 22:01:08 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 17:09:03 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 13:23:35 GMT"}, {"version": "v4", "created": "Mon, 3 May 2021 17:14:58 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Burkhardt", "Paul", ""]]}, {"id": "1808.06942", "submitter": "Ignacio Ramirez", "authors": "Ignacio Francisco Ram\\'irez Paulino", "title": "PACO: Global Signal Restoration via PAtch COnsensus", "comments": "Submitted to Siam SIIMS for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many signal processing algorithms break the target signal into overlapping\nsegments (also called windows, or patches), process them separately, and then\nstitch them back into place to produce a unified output. At the overlaps, the\nfinal value of those samples that are estimated more than once needs to be\ndecided in some way. Averaging, the simplest approach, often leads to\nunsatisfactory results. Significant work has been devoted to this issue in\nrecent years. Several works explore the idea of a weighted average of the\noverlapped patches and/or pixels; others promote agreement (consensus) between\nthe patches at their intersections. Agreement can be either encouraged or\nimposed as a hard constraint. This work develops on the latter case. The result\nis a variational signal processing framework, named PACO, which features a\nnumber of appealing theoretical and practical properties. The PACO framework\nconsists of a variational formulation that fits a wide variety of problems, and\na general ADMMbased algorithm for minimizing the resulting energies. As a\nbyproduct, we show that the consensus step of the algorithm, which is the main\nbottleneck of similar methods, can be solved efficiently and easily for any\narbitrary patch decomposition scheme. We demonstrate the flexibility and power\nof PACO on three different problems: image inpainting (which we have already\ncovered in previous works), image denoising, and contrast enhancement, using\ndifferent cost functions including Laplacian and Gaussian Mixture Models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 15:20:46 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 20:48:24 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 18:02:53 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Paulino", "Ignacio Francisco Ram\u00edrez", ""]]}, {"id": "1808.07027", "submitter": "James Sullivan", "authors": "James Sullivan, Collin Weir, Austin Reichert, R. Todd Evans, W. Cyrus\n  Proctor, Nicolas Thorne", "title": "Student Cluster Competition 2017, Team University ofTexas at\n  Austin/Texas State University: Reproducing Vectorization of the Tersoff\n  Multi-Body Potential on the Intel Skylake and NVIDIA V100 Architectures", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.parco.2018.08.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper satisfies the reproducibility challenge of the Student Cluster\nCompetition at Supercomputing 2017. We attempted to reproduce the results of\nH\\\"{o}hnerbach et al. (2016) for an implementation of a vectorized code for the\nTersoff multi-body potential kernel of the molecular dynamics code Large-scale\nAtomic/Molecular Massively Parallel Simulator (LAMMPS). We investigated\naccuracy, optimization performance, and scaling with our Intel CPU and NVIDIA\nGPU based cluster.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 17:24:53 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Sullivan", "James", ""], ["Weir", "Collin", ""], ["Reichert", "Austin", ""], ["Evans", "R. Todd", ""], ["Proctor", "W. Cyrus", ""], ["Thorne", "Nicolas", ""]]}, {"id": "1808.07252", "submitter": "Ivano Notarnicola", "authors": "Ivano Notarnicola, Ying Sun, Gesualdo Scutari, Giuseppe Notarstefano", "title": "Distributed Big-Data Optimization via Block-wise Gradient Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed big-data nonconvex optimization in multi-agent networks.\nWe consider the (constrained) minimization of the sum of a smooth (possibly)\nnonconvex function, i.e., the agents' sum-utility, plus a convex (possibly)\nnonsmooth regularizer. Our interest is on big-data problems in which there is a\nlarge number of variables to optimize. If treated by means of standard\ndistributed optimization algorithms, these large-scale problems may be\nintractable due to the prohibitive local computation and communication burden\nat each node. We propose a novel distributed solution method where, at each\niteration, agents update in an uncoordinated fashion only one block of the\nentire decision vector. To deal with the nonconvexity of the cost function, the\nnovel scheme hinges on Successive Convex Approximation (SCA) techniques\ncombined with a novel block-wise perturbed push-sum consensus protocol, which\nis instrumental to perform local block-averaging operations and tracking of\ngradient averages. Asymptotic convergence to stationary solutions of the\nnonconvex problem is established. Finally, numerical results show the\neffectiveness of the proposed algorithm and highlight how the block dimension\nimpacts on the communication overhead and practical convergence speed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 07:30:06 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 05:52:36 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Notarnicola", "Ivano", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1808.07412", "submitter": "Charith Mendis", "authors": "Charith Mendis, Alex Renda, Saman Amarasinghe and Michael Carbin", "title": "Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation\n  using Deep Neural Networks", "comments": "Published at 36th International Conference on Machine Learning (ICML)\n  2019", "journal-ref": "Proceedings of Machine Learning Research - Volume 97 (ICML 2019)", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the number of clock cycles a processor takes to execute a block of\nassembly instructions in steady state (the throughput) is important for both\ncompiler designers and performance engineers. Building an analytical model to\ndo so is especially complicated in modern x86-64 Complex Instruction Set\nComputer (CISC) machines with sophisticated processor microarchitectures in\nthat it is tedious, error prone, and must be performed from scratch for each\nprocessor generation. In this paper we present Ithemal, the first tool which\nlearns to predict the throughput of a set of instructions. Ithemal uses a\nhierarchical LSTM--based approach to predict throughput based on the opcodes\nand operands of instructions in a basic block. We show that Ithemal is more\naccurate than state-of-the-art hand-written tools currently used in compiler\nbackends and static machine code analyzers. In particular, our model has less\nthan half the error of state-of-the-art analytical models (LLVM's llvm-mca and\nIntel's IACA). Ithemal is also able to predict these throughput values just as\nfast as the aforementioned tools, and is easily ported across a variety of\nprocessor microarchitectures with minimal developer effort.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 03:40:21 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 13:32:47 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Mendis", "Charith", ""], ["Renda", "Alex", ""], ["Amarasinghe", "Saman", ""], ["Carbin", "Michael", ""]]}, {"id": "1808.07545", "submitter": "Pei Peng", "authors": "Pei Peng and Emina Soljanin", "title": "On Distributed Storage Allocations of Large Files for Maximum Service\n  Rate", "comments": "This paper is accepted by 56th Annual Allerton Conference on\n  Communication, Control, and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allocation of (redundant) file chunks throughout a distributed storage system\naffects important performance metrics such as the probability of file recovery,\ndata download time, or the service rate of the system under a given data access\nmodel. This paper is concerned with the service rate under the assumption that\nthe stored data is large and its download time is not negligible. We focus on\nquasi-uniform storage allocations and provide a service rate analysis for two\ncommon data access models. We find that the optimal allocation varies in\naccordance with different system parameters. This was not the case under the\nassumption that the download time does not scale with the size of data, where\nthe minimal spreading allocation was previously found to be universally\noptimal.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:30:12 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Peng", "Pei", ""], ["Soljanin", "Emina", ""]]}, {"id": "1808.07576", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Gauri Joshi", "title": "Cooperative SGD: A unified Framework for the Design and Analysis of\n  Communication-Efficient SGD Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication-efficient SGD algorithms, which allow nodes to perform local\nupdates and periodically synchronize local models, are highly effective in\nimproving the speed and scalability of distributed SGD. However, a rigorous\nconvergence analysis and comparative study of different communication-reduction\nstrategies remains a largely open problem. This paper presents a unified\nframework called Cooperative SGD that subsumes existing communication-efficient\nSGD algorithms such as periodic-averaging, elastic-averaging and decentralized\nSGD. By analyzing Cooperative SGD, we provide novel convergence guarantees for\nexisting algorithms. Moreover, this framework enables us to design new\ncommunication-efficient SGD algorithms that strike the best balance between\nreducing communication overhead and achieving fast error convergence with low\nerror floor.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 22:06:26 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 00:45:15 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 17:45:23 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Wang", "Jianyu", ""], ["Joshi", "Gauri", ""]]}, {"id": "1808.07788", "submitter": "Daniel Gall", "authors": "Thom Fr\\\"uhwirth and Daniel Gall", "title": "Exploring Parallel Execution Strategies for Constraint Handling Rules -\n  Work-in-Progress Report", "comments": "16 pages, Accepted for presentation in WFLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Handling Rules (CHR) is a declarative rule-based formalism and\nlanguage. Concurrency is inherent as rules can be applied to subsets of\nconstraints in parallel. Parallel implementations of CHR, be it in software, be\nit in hardware, use different execution strategies for parallel execution of\nCHR programs depending on the implementation language.\n  In this report, our goal is to analyze parallel execution of CHR programs\nfrom a more general conceptual perspective. We want to experimentally see what\nis possible when CHR programs are automatically parallelized. For this purpose,\na sequential simulation of parallel CHR execution is used to systematically\nencode different parallel execution strategies. In exhaustive experiments on\nsome typical examples from the literature, parallel and sequential execution\ncan be compared to each other. The number of processors can be bounded or\nunbounded for a more theoretical analysis. As a result, some preliminary but\nindicative observations on the influence of the execution strategy can be made\nfor the different problem classes and in general.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 14:57:47 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Fr\u00fchwirth", "Thom", ""], ["Gall", "Daniel", ""]]}, {"id": "1808.08040", "submitter": "Ming-Chang Lee", "authors": "Ming-Chang Lee, Jia-Chun Lin, Ramin Yahyapour", "title": "Hybrid Job-driven Scheduling for Virtual MapReduce Clusters", "comments": "13 pages and 17 figures", "journal-ref": "IEEE Transactions On Parallel And Distributed Systems, Vol. 27,\n  NO. 6, June 2016", "doi": "10.1109/TPDS.2015.2463817", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is cost-efficient for a tenant with a limited budget to establish a\nvirtual MapReduce cluster by renting multiple virtual private servers (VPSs)\nfrom a VPS provider. To provide an appropriate scheduling scheme for this type\nof computing environment, we propose in this paper a hybrid job-driven\nscheduling scheme (JoSS for short) from a tenant's perspective. JoSS provides\nnot only job level scheduling, but also map-task level scheduling and\nreduce-task level scheduling. JoSS classifies MapReduce jobs based on job scale\nand job type and designs an appropriate scheduling policy to schedule each\nclass of jobs. The goal is to improve data locality for both map tasks and\nreduce tasks, avoid job starvation, and improve job execution performance. Two\nvariations of JoSS are further introduced to separately achieve a better\nmap-data locality and a faster task assignment. We conduct extensive\nexperiments to evaluate and compare the two variations with current scheduling\nalgorithms supported by Hadoop. The results show that the two variations\noutperform the other tested algorithms in terms of map-data locality,\nreduce-data locality, and network overhead without incurring significant\noverhead. In addition, the two variations are separately suitable for different\nMapReduce-workload scenarios and provide the best job performance among all\ntested algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 08:05:44 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 08:44:43 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Lee", "Ming-Chang", ""], ["Lin", "Jia-Chun", ""], ["Yahyapour", "Ramin", ""]]}, {"id": "1808.08106", "submitter": "Joseph Schuchart", "authors": "Joseph Schuchart, Daniel Hackenberg, Robert Sch\\\"one, Thomas Ilsche,\n  Ramkumar Nagappan, Michael K. Patterson", "title": "The Shift from Processor Power Consumption to Performance Variations:\n  Fundamental Implications at Scale", "comments": null, "journal-ref": "Computer Science - Research and Development, Vol. 31, pp.\n  197--205, Nov 2016", "doi": "10.1007/s00450-016-0327-2", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Intel Haswell-EP processor generation introduces several major\nadvancements of power control and energy-efficiency features. For\ncomputationally intense applications using advanced vector extension (AVX)\ninstructions, the processor cannot continuously operate at full speed but\ninstead reduces its frequency below the nominal frequency to maintain\noperations within thermal design power (TDP) limitations. Moreover, the running\naverage power limitation (RAPL) mechanism to enforce the TDP limitation has\nchanged from a modeling to a measurement approach. The combination of these two\nnovelties have significant implications. Through measurements on an Intel Sandy\nBridge-EP cluster, we show that previous generations have sustained homogeneous\nperformance across multiple CPUs and compensated for hardware manufacturing\nvariability through varying power consumption. In contrast, our measurements on\na Petaflop Haswell system show that this generation exhibits rather homogeneous\npower consumption limited by the TDP and capped by the improved RAPL while\nproviding inhomogeneous performance under full load. Since all of these\ncontrols are transparent to the user, this behavior is likely to complicate\nperformance analysis tasks and impact tightly coupled parallel applications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 12:40:03 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Schuchart", "Joseph", ""], ["Hackenberg", "Daniel", ""], ["Sch\u00f6ne", "Robert", ""], ["Ilsche", "Thomas", ""], ["Nagappan", "Ramkumar", ""], ["Patterson", "Michael K.", ""]]}, {"id": "1808.08172", "submitter": "Christian Glusa", "authors": "Christian Glusa, Paritosh Ramanan, Erik G. Boman, Edmond Chow,\n  Sivasankaran Rajamanickam", "title": "Asynchronous One-Level and Two-Level Domain Decomposition Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel implementations of linear iterative solvers generally alternate\nbetween phases of data exchange and phases of local computation. Increasingly\nlarge problem sizes on more heterogeneous systems make load balancing and\nnetwork layout very challenging tasks. In particular, global communication\npatterns such as inner products become increasingly limiting at scale. We\nexplore the use of asynchronous communication based on one-sided MPI primitives\nin a multitude of domain decomposition solvers. In particular, a scalable\nasynchronous two-level method is presented. We discuss practical issues\nencountered in the development of a scalable solver and show experimental\nresults obtained on state-of-the-art supercomputer systems that illustrate the\nbenefits of asynchronous solvers in load balanced as well as load imbalanced\nscenarios. Using the novel method, we can observe speed-ups of up to 4x over\nits classical synchronous equivalent.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 15:27:22 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 01:51:35 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Glusa", "Christian", ""], ["Ramanan", "Paritosh", ""], ["Boman", "Erik G.", ""], ["Chow", "Edmond", ""], ["Rajamanickam", "Sivasankaran", ""]]}, {"id": "1808.08289", "submitter": "Ming-Chang Lee", "authors": "JIa-Chun Lin, Ming-Chang Lee", "title": "Performance evaluation of job schedulers on Hadoop YARN", "comments": "18 pages and 16 figures", "journal-ref": "Concurrency and Computation: Practice and Experience (CCPE), vol.\n  28, no. 9, 2016, pp. 2711-2728", "doi": "10.1002/cpe.3736", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the limitation of Hadoop on scalability, resource sharing, and\napplication support, the open-source community proposes the next generation of\nHadoop's compute platform called Yet Another Resource Negotiator (YARN) by\nseparating resource management functions from the programming model. This\nseparation enables various application types to run on YARN in parallel. To\nachieve fair resource sharing and high resource utilization, YARN provides the\ncapacity scheduler and the fair scheduler. However, the performance impacts of\nthe two schedulers are not clear when mixed applications run on a YARN cluster.\nTherefore, in this paper, we study four scheduling-policy combinations (SPCs\nfor short) derived from the two schedulers and then evaluate the four SPCs in\nextensive scenarios, which consider not only four application types, but also\nthree different queue structures for organizing applications. The experimental\nresults enable YARN managers to comprehend the influences of different SPCs and\ndifferent queue structures on mixed applications. The results also help them to\nselect a proper SPC and an appropriate queue structure to achieve better\napplication execution performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 19:51:51 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 08:51:52 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Lin", "JIa-Chun", ""], ["Lee", "Ming-Chang", ""]]}, {"id": "1808.08353", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally, Jeremy Kepner, Lauren Milechin, William Arcand, David\n  Bestor, Bill Bergeron, Chansup Byun, Matthew Hubbell, Micheal Houle, Micheal\n  Jones, Peter Michaleas, Julie Mullen, Andrew Prout, Antonio Rosa, Charles\n  Yee, Siddharth Samsi, Albert Reuther", "title": "Hyperscaling Internet Graph Analysis with D4M on the MIT SuperCloud", "comments": "Accepted to IEEE HPEC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting anomalous behavior in network traffic is a major challenge due to\nthe volume and velocity of network traffic. For example, a 10 Gigabit Ethernet\nconnection can generate over 50 MB/s of packet headers. For global network\nproviders, this challenge can be amplified by many orders of magnitude.\nDevelopment of novel computer network traffic analytics requires: high level\nprogramming environments, massive amount of packet capture (PCAP) data, and\ndiverse data products for \"at scale\" algorithm pipeline development. D4M\n(Dynamic Distributed Dimensional Data Model) combines the power of sparse\nlinear algebra, associative arrays, parallel processing, and distributed\ndatabases (such as SciDB and Apache Accumulo) to provide a scalable data and\ncomputation system that addresses the big data problems associated with network\nanalytics development. Combining D4M with the MIT SuperCloud manycore\nprocessors and parallel storage system enables network analysts to\ninteractively process massive amounts of data in minutes. To demonstrate these\ncapabilities, we have implemented a representative analytics pipeline in D4M\nand benchmarked it on 96 hours of Gigabit PCAP data with MIT SuperCloud. The\nentire pipeline from uncompressing the raw files to database ingest was\nimplemented in 135 lines of D4M code and achieved speedups of over 20,000.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 04:06:12 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""], ["Milechin", "Lauren", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Micheal", ""], ["Jones", "Micheal", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Samsi", "Siddharth", ""], ["Reuther", "Albert", ""]]}, {"id": "1808.08356", "submitter": "Hyowoon Seo", "authors": "Hyowoon Seo, Jihong Park, Mehdi Bennis, and Wan Choi", "title": "Consensus-Before-Talk: Distributed Dynamic Spectrum Access via\n  Distributed Spectrum Ledger Technology", "comments": "IEEE International Symposium on Dynamic Spectrum Access Networks 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes Consensus-Before-Talk (CBT), a spectrum etiquette\narchitecture leveraged by distributed ledger technology (DLT). In CBT,\nsecondary users' spectrum access requests reach a consensus in a distributed\nway, thereby enabling collision-free distributed dynamic spectrum access. To\nachieve this consensus, the secondary users need to pay for the extra request\nexchanging delays. Incorporating the consensus delay, the end-to-end latency\nunder CBT is investigated. Both the latency analysis and numerical evaluation\nvalidate that the proposed CBT achieves the lower end-to-end latency\nparticularly under severe secondary user traffic, compared to the\nListen-Before-Talk (LBT) benchmark scheme.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 05:17:21 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Seo", "Hyowoon", ""], ["Park", "Jihong", ""], ["Bennis", "Mehdi", ""], ["Choi", "Wan", ""]]}, {"id": "1808.08406", "submitter": "Zsolt Istv\\'an", "authors": "Lucas Kuhring, Zsolt Istv\\'an, Alessandro Sorniotti, Marko Vukoli\\'c", "title": "StreamChain: Rethinking Blockchain for Datacenters", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned blockchains promise secure decentralized data management in\nbusiness-to-business use-cases. In contrast to Bitcoin and similar public\nblockchains which rely on Proof-of-Work for consensus and are deployed on\nthousands of geo-distributed nodes, business-to-business use-cases (such as\nsupply chain management and banking) require significantly fewer nodes, cheaper\nconsensus, and are often deployed in datacenter-like environments with fast\nnetworking. However, permissioned blockchains often follow the architectural\nthinkining behind their WAN-oriented public relatives, which results in\nend-to-end latencies several orders of magnitude higher than necessary.\n  In this work, we propose StreamChain, a permissioned blockchain design that\neliminates blocks in favor of processing transactions in a streaming fashion.\nThis results in a drastically lower latency without reducing throughput or\nforfeiting reliability and security guarantees. To demonstrate the wide\napplicability of our design, we prototype StreamChain based on the Hyperledger\nFabric, and show that it delivers latency two orders of magnitude lower than\nFabric, while sustaining similar throughput. This performance makes StreamChain\na potential alternative to traditional databases and, thanks to its streaming\nparadigm, enables further research around reducing latency through relying on\nmodern hardware in datacenters.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 10:57:06 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 17:02:50 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Kuhring", "Lucas", ""], ["Istv\u00e1n", "Zsolt", ""], ["Sorniotti", "Alessandro", ""], ["Vukoli\u0107", "Marko", ""]]}, {"id": "1808.08424", "submitter": "Rajmohan C", "authors": "Rajmohan C, Pranay Lohia, Himanshu Gupta, Siddhartha Brahma, Mauricio\n  Hernandez, Sameep Mehta", "title": "Efficiently Processing Workflow Provenance Queries on SPARK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how we can leverage Spark platform for\nefficiently processing provenance queries on large volumes of workflow\nprovenance data. We focus on processing provenance queries at attribute-value\nlevel which is the finest granularity available. We propose a novel weakly\nconnected component based framework which is carefully engineered to quickly\ndetermine a minimal volume of data containing the entire lineage of the queried\nattribute-value. This minimal volume of data is then processed to figure out\nthe provenance of the queried attribute-value. The proposed framework computes\nweakly connected components on the workflow provenance graph and further\npartitions the large components as a collection of weakly connected sets. The\nframework exploits the workflow dependency graph to effectively partition the\nlarge components into a collection of weakly connected sets. We study the\neffectiveness of the proposed framework through experiments on a provenance\ntrace obtained from a real-life unstructured text curation workflow. On\nprovenance graphs containing upto 500M nodes and edges, we show that the\nproposed framework answers provenance queries in real-time and easily\noutperforms the naive approaches.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 13:38:22 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 11:42:12 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["C", "Rajmohan", ""], ["Lohia", "Pranay", ""], ["Gupta", "Himanshu", ""], ["Brahma", "Siddhartha", ""], ["Hernandez", "Mauricio", ""], ["Mehta", "Sameep", ""]]}, {"id": "1808.08474", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri", "title": "A Taxonomy on Big Data: Survey", "comments": "15 pages, 15 figures, 5 tables, and a survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Big Data is the most popular paradigm nowadays and it has almost no\nuntouched area. For instance, science, engineering, economics, business, social\nscience, and government. The Big Data are used to boost up the organization\nperformance using massive amount of dataset. The Data are assets of the\norganization, and these data gives revenue to the organizations. Therefore, the\nBig Data is spawning everywhere to enhance the organizations' revenue. Thus,\nmany new technologies emerging based on Big Data. In this paper, we present the\ntaxonomy of Big Data. Besides, we present in-depth insight on the Big Data\nparadigm.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 21:47:23 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 23:52:44 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 17:43:46 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Patgiri", "Ripon", ""]]}, {"id": "1808.08512", "submitter": "Wanling Gao", "authors": "Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Fei\n  Tang, Biwei Xie, Chen Zheng, Xu Wen, Xiwen He, Hainan Ye, Rui Ren", "title": "Data Motifs: A Lens Towards Fully Understanding Big Data and AI\n  Workloads", "comments": "The paper will be published on The 27th International Conference on\n  Parallel Architectures and Compilation Techniques (PACT18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity and diversity of big data and AI workloads make understanding\nthem difficult and challenging. This paper proposes a new approach to modelling\nand characterizing big data and AI workloads. We consider each big data and AI\nworkload as a pipeline of one or more classes of units of computation performed\non different initial or intermediate data inputs. Each class of unit of\ncomputation captures the common requirements while being reasonably divorced\nfrom individual implementations, and hence we call it a data motif. For the\nfirst time, among a wide variety of big data and AI workloads, we identify\neight data motifs that take up most of the run time of those workloads,\nincluding Matrix, Sampling, Logic, Transform, Set, Graph, Sort and Statistic.\nWe implement the eight data motifs on different software stacks as the micro\nbenchmarks of an open-source big data and AI benchmark suite ---BigDataBench\n4.0 (publicly available from http://prof.ict.ac.cn/BigDataBench), and perform\ncomprehensive characterization of those data motifs from perspective of data\nsizes, types, sources, and patterns as a lens towards fully understanding big\ndata and AI workloads. We believe the eight data motifs are promising\nabstractions and tools for not only big data and AI benchmarking, but also\ndomain-specific hardware and software co-design.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 06:52:25 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Gao", "Wanling", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Zheng", "Daoyi", ""], ["Tang", "Fei", ""], ["Xie", "Biwei", ""], ["Zheng", "Chen", ""], ["Wen", "Xu", ""], ["He", "Xiwen", ""], ["Ye", "Hainan", ""], ["Ren", "Rui", ""]]}, {"id": "1808.08521", "submitter": "Ahmet Sayar", "authors": "Suleyman Eken, Eray Aydin, Ahmet Sayar", "title": "DIFET: Distributed Feature Extraction Tool For High Spatial Resolution\n  Remote Sensing Images", "comments": "Presented at 4th International GeoAdvances Workshop", "journal-ref": null, "doi": "10.5194/isprs-annals-IV-4-W4-209-2017", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose distributed feature extraction tool from high\nspatial resolution remote sensing images. Tool is based on Apache Hadoop\nframework and Hadoop Image Processing Interface. Two corner detection (Harris\nand Shi-Tomasi) algorithms and five feature descriptors (SIFT, SURF, FAST,\nBRIEF, and ORB) are considered. Robustness of the tool in the task of feature\nextraction from LandSat-8 imageries are evaluated in terms of horizontal\nscalability.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 08:58:11 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Eken", "Suleyman", ""], ["Aydin", "Eray", ""], ["Sayar", "Ahmet", ""]]}, {"id": "1808.08522", "submitter": "Ahmet Sayar", "authors": "Hayrunnisa Sari, Suleyman Eken, Ahmet Sayar", "title": "An Approach For Stitching Satellite Images In A Bigdata Mapreduce\n  Framework", "comments": "Presented at 4th International GeoAdvances Workshop", "journal-ref": null, "doi": "10.5194/isprs-annals-IV-4-W4-351-2017", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we present a two-step map/reduce framework to stitch satellite\nmosaic images. The proposed system enable recognition and extraction of objects\nwhose parts falling in separate satellite mosaic images. However this is a time\nand resource consuming process. The major aim of the study is improving the\nperformance of the image stitching processes by utilizing big data framework.\nTo realize this, we first convert the images into bitmaps (first mapper) and\nthen String formats in the forms of 255s and 0s (second mapper), and finally,\nfind the best possible matching position of the images by a reduce function.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 08:58:31 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Sari", "Hayrunnisa", ""], ["Eken", "Suleyman", ""], ["Sayar", "Ahmet", ""]]}, {"id": "1808.08528", "submitter": "Ahmet Sayar", "authors": "Suleyman Eken and Ahmet Sayar", "title": "A MapReduce based Big-data Framework for Object Extraction from Mosaic\n  Satellite Images", "comments": "Proceedings of Doctoral Consortium on Internet of Things At Roma,\n  Italy 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework stitching of vector representations of large scale\nraster mosaic images in distributed computing model. In this way, the negative\neffect of the lack of resources of the central system and scalability problem\ncan be eliminated. The product obtained by this study can be used in\napplications requiring spatial and temporal analysis on big satellite map\nimages. This study also shows that big data frameworks are not only used in\napplications of text-based data mining and machine learning algorithms, but\nalso used in applications of algorithms in image processing. The effectiveness\nof the product realized with this project is also going to be proven by\nscalability and performance tests performed on real world LandSat-8 satellite\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 10:32:40 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Eken", "Suleyman", ""], ["Sayar", "Ahmet", ""]]}, {"id": "1808.08585", "submitter": "Linjing Li", "authors": "Jiaqi Liang, Linjing Li, Daniel Zeng", "title": "Evolutionary dynamics of cryptocurrency transaction networks: An\n  empirical study", "comments": null, "journal-ref": "PLoS ONE 13(8): e0202202 (2018)", "doi": "10.1371/journal.pone.0202202", "report-no": null, "categories": "q-fin.ST cs.CR cs.DC econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryptocurrency is a well-developed blockchain technology application that is\ncurrently a heated topic throughout the world. The public availability of\ntransaction histories offers an opportunity to analyze and compare different\ncryptocurrencies. In this paper, we present a dynamic network analysis of three\nrepresentative blockchain-based cryptocurrencies: Bitcoin, Ethereum, and\nNamecoin. By analyzing the accumulated network growth, we find that, unlike\nmost other networks, these cryptocurrency networks do not always densify over\ntime, and they are changing all the time with relatively low node and edge\nrepetition ratios. Therefore, we then construct separate networks on a monthly\nbasis, trace the changes of typical network characteristics (including degree\ndistribution, degree assortativity, clustering coefficient, and the largest\nconnected component) over time, and compare the three. We find that the degree\ndistribution of these monthly transaction networks cannot be well fitted by the\nfamous power-law distribution, at the same time, different currency still has\ndifferent network properties, e.g., both Bitcoin and Ethereum networks are\nheavy-tailed with disassortative mixing, however, only the former can be\ntreated as a small world. These network properties reflect the evolutionary\ncharacteristics and competitive power of these three cryptocurrencies and\nprovide a foundation for future research.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 16:27:21 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Liang", "Jiaqi", ""], ["Li", "Linjing", ""], ["Zeng", "Daniel", ""]]}, {"id": "1808.08877", "submitter": "Romaric Duvignau", "authors": "Romaric Duvignau, Vincenzo Gulisano, Marina Papatriantafilou, Vladimir\n  Savic", "title": "Piecewise Linear Approximation in Data Streaming: Algorithmic\n  Implementations and Experimental Analysis", "comments": "12 pages+1 for references, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise Linear Approximation (PLA) is a well-established tool to reduce the\nsize of the representation of time series by approximating the series by a\nsequence of line segments while keeping the error introduced by the\napproximation within some predetermined threshold. With the recent rise of edge\ncomputing, PLA algorithms find a complete new set of applications with the\nemphasis on reducing the volume of streamed data. In this study, we identify\ntwo scenarios set in a data-stream processing context: data reduction in sensor\ntransmissions and datacenter storage. In connection to those scenarios, we\nidentify several streaming metrics and propose streaming protocols as\nalgorithmic implementations of several state of the art PLA techniques. In an\nexperimental evaluation, we measure the quality of the reviewed methods and\nprotocols and evaluate their performance against those streaming statistics.\nAll known methods have deficiencies when it comes to handling streaming-like\ndata, e.g. inflation of the input stream, high latency or poor average error.\nOur experimental results highlight the challenges raised when transferring\nthose classical methods into the stream processing world and present\nalternative techniques to overcome them and balance the related trade-offs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 15:11:28 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 12:17:25 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Duvignau", "Romaric", ""], ["Gulisano", "Vincenzo", ""], ["Papatriantafilou", "Marina", ""], ["Savic", "Vladimir", ""]]}, {"id": "1808.08913", "submitter": "Mahsa Eftekhari Hesari", "authors": "David Doty and Mahsa Eftekhari", "title": "Efficient size estimation and impossibility of termination in uniform\n  dense population protocols", "comments": "Using leaderless phase clock", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study uniform population protocols: networks of anonymous agents whose\npairwise interactions are chosen at random, where each agent uses an identical\ntransition algorithm that does not depend on the population size $n$. Many\nexisting polylog$(n)$ time protocols for leader election and majority\ncomputation are nonuniform: to operate correctly, they require all agents to be\ninitialized with an approximate estimate of $n$ (specifically, the exact value\n$\\lfloor \\log n \\rfloor$). Our first main result is a uniform protocol for\ncalculating $\\log(n) \\pm O(1)$ with high probability in $O(\\log^2 n)$ time and\n$O(\\log^4 n)$ states ($O(\\log \\log n)$ bits of memory). The protocol is\nconverging but not terminating: it does not signal when the estimate is close\nto the true value of $\\log n$. If it could be made terminating, this would\nallow composition with protocols, such as those for leader election or\nmajority, that require a size estimate initially, to make them uniform (though\nwith a small probability of failure). We do show how our main protocol can be\nindirectly composed with others in a simple and elegant way, based on the\nleaderless phase clock, demonstrating that those protocols can in fact be made\nuniform. However, our second main result implies that the protocol cannot be\nmade terminating, a consequence of a much stronger result: a uniform protocol\nfor any task requiring more than constant time cannot be terminating even with\nprobability bounded above 0, if infinitely many initial configurations are\ndense: any state present initially occupies $\\Omega(n)$ agents. (In particular,\nno leader is allowed.) Crucially, the result holds no matter the memory or time\npermitted. Finally, we show that with an initial leader, our size-estimation\nprotocol can be made terminating with high probability, with the same\nasymptotic time and space bounds.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 16:29:15 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 21:29:28 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 18:48:51 GMT"}, {"version": "v4", "created": "Sun, 28 Jul 2019 18:02:26 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Doty", "David", ""], ["Eftekhari", "Mahsa", ""]]}, {"id": "1808.09005", "submitter": "Ming-Chang Lee", "authors": "Jia-Chun Lin, Ming-Chang Lee, Ingrid Chieh Yu, Einar Broch Johnsen", "title": "Modeling and Simulation of Spark Streaming", "comments": "7 pages and 13 figures. This paper is published in IEEE 32nd\n  International Conference on Advanced Information Networking and Applications\n  (AINA 2018)", "journal-ref": null, "doi": "10.1109/AINA.2018.00068", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more and more devices connect to Internet of Things, unbounded streams of\ndata will be generated, which have to be processed \"on the fly\" in order to\ntrigger automated actions and deliver real-time services. Spark Streaming is a\npopular realtime stream processing framework. To make efficient use of Spark\nStreaming and achieve stable stream processing, it requires a careful interplay\nbetween different parameter configurations. Mistakes may lead to significant\nresource overprovisioning and bad performance. To alleviate such issues, this\npaper develops an executable and configurable model named SSP (stands for Spark\nStreaming Processing) to model and simulate Spark Streaming. SSP is written in\nABS, which is a formal, executable, and object-oriented language for modeling\ndistributed systems by means of concurrent object groups. SSP allows users to\nrapidly evaluate and compare different parameter configurations without\ndeploying their applications on a cluster/cloud. The simulation results show\nthat SSP is able to mimic Spark Streaming in different scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 19:25:41 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 09:06:22 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Lin", "Jia-Chun", ""], ["Lee", "Ming-Chang", ""], ["Yu", "Ingrid Chieh", ""], ["Johnsen", "Einar Broch", ""]]}, {"id": "1808.09571", "submitter": "Bruno Silva", "authors": "Lucas C. Villa Real, Bruno Silva", "title": "Full Speed Ahead: 3D Spatial Database Acceleration with GPUs", "comments": "ADMS@ VLDB (Very Large Database Conference) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many industries rely on visual insights to support decision- making processes\nin their businesses. In mining, the analysis of drills and geological shapes,\nrepresented as 3D geometries, is an important tool to assist geologists on the\nsearch for new ore deposits. Aeronautics manipulate high-resolution geometries\nwhen designing a new aircraft aided by the numerical simulation of\naerodynamics. In common, these industries require scalable databases that\ncompute spatial relationships and measurements so that decision making can be\nconducted without lags. However, as we show in this study, most database\nsystems either lack support for handling 3D geometries or show poor performance\nwhen given a sheer volume of data to work with. This paper presents a pluggable\nacceleration engine for spatial database systems that can improve the\nperformance of spatial operations by more than 3000x through GPU offloading. We\nfocus on the design and evaluation of our plug-in for the PostgreSQL database\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 22:55:44 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Real", "Lucas C. Villa", ""], ["Silva", "Bruno", ""]]}, {"id": "1808.09751", "submitter": "Andreas Kurth", "authors": "Andreas Kurth, Pirmin Vogel, Andrea Marongiu, Luca Benini", "title": "Scalable and Efficient Virtual Memory Sharing in Heterogeneous SoCs with\n  TLB Prefetching and MMU-Aware DMA Engine", "comments": "9 pages, 5 figures. Accepted for publication in Proceedings of the\n  36th IEEE International Conference on Computer Design (ICCD), October 7-10,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared virtual memory (SVM) is key in heterogeneous systems on chip (SoCs),\nwhich combine a general-purpose host processor with a many-core accelerator,\nboth for programmability and to avoid data duplication. However, SVM can bring\na significant run time overhead when translation lookaside buffer (TLB) entries\nare missing. Moreover, allowing DMA burst transfers to write SVM traditionally\nrequires buffers to absorb transfers that miss in the TLB. These buffers have\nto be overprovisioned for the maximum burst size, wasting precious on-chip\nmemory, and stall all SVM accesses once they are full, hampering the\nscalability of parallel accelerators.\n  In this work, we present our SVM solution that avoids the majority of TLB\nmisses with prefetching, supports parallel burst DMA transfers without\nadditional buffers, and can be scaled with the workload and number of parallel\nprocessors. Our solution is based on three novel concepts: To minimize the rate\nof TLB misses, the TLB is proactively filled by compiler-generated Prefetching\nHelper Threads, which use run-time information to issue timely prefetches. To\nreduce the latency of TLB misses, misses are handled by a variable number of\nparallel Miss Handling Helper Threads. To support parallel burst DMA transfers\nto SVM without additional buffers, we add lightweight hardware to a standard\nDMA engine to detect and react to TLB misses. Compared to the state of the art,\nour work improves accelerator performance for memory-intensive kernels by up to\n4x and by up to 60% for irregular and regular memory access patterns,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 12:18:12 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Kurth", "Andreas", ""], ["Vogel", "Pirmin", ""], ["Marongiu", "Andrea", ""], ["Benini", "Luca", ""]]}, {"id": "1808.10011", "submitter": "Timur Bazhirov", "authors": "Timur Bazhirov, E. X. Abot", "title": "Fast and accessible first-principles calculations of vibrational\n  properties of materials", "comments": "8 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cond-mat.other cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present example applications of an approach to first-principles\ncalculations of vibrational properties of materials implemented within the\nExabyte.io platform. We deploy models based on the Density Functional\nPerturbation Theory to extract the phonon dispersion relations and densities of\nstates for an example set of 35 samples and find the results to be in agreement\nwith prior similar calculations. We construct modeling workflows that are both\naccessible, accurate, and efficient with respect to the human time involved.\nThis is achieved through efficient parallelization of the tasks for the\nindividual vibrational modes. We report achieved speedups in the 10-100 range,\napproximately, and maximum attainable speedups in the 30-300 range,\ncorrespondingly. We analyze the execution times on the current up-to-date\ncomputational infrastructure centrally available from a public cloud provider.\nResults and all associated data, including the materials and simulation\nworkflows, are made available online in an accessible, repeatable and\nextensible setting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 18:48:16 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Bazhirov", "Timur", ""], ["Abot", "E. X.", ""]]}, {"id": "1808.10097", "submitter": "Behnam Dezfouli", "authors": "Immanuel Amirtharaj, Tai Groot, Behnam Dezfouli", "title": "Profiling and Improving the Duty-Cycling Performance of Linux-based IoT\n  Devices", "comments": "SCU's IoT Research Lab", "journal-ref": null, "doi": null, "report-no": "TR-SCU-SIOTLAB-AUG2018-PALLEX", "categories": "cs.OS cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the energy consumption of Linux-based devices is an essential step\ntowards their wide deployment in various IoT scenarios. Energy saving methods\nsuch as duty-cycling aim to address this constraint by limiting the amount of\ntime the device is powered on. In this work we study and improve the amount of\ntime a Linux-based IoT device is powered on to accomplish its tasks. We analyze\nthe processes of system boot up and shutdown on two platforms, the Raspberry Pi\n3 and Raspberry Pi Zero Wireless, and enhance duty-cycling performance by\nidentifying and disabling time-consuming or unnecessary units initialized in\nthe userspace. We also study whether SD card speed and SD card capacity\nutilization affect boot up duration and energy consumption. In addition, we\npropose 'Pallex', a parallel execution framework built on top of the 'systemd\ninit' system to run a user application concurrently with userspace\ninitialization. We validate the performance impact of Pallex when applied to\nvarious IoT application scenarios: (i) capturing an image, (ii) capturing and\nencrypting an image, (iii) capturing and classifying an image using the the\nk-nearest neighbor algorithm, and (iv) capturing images and sending them to a\ncloud server. Our results show that system lifetime is increased by 18.3%,\n16.8%, 13.9% and 30.2%, for these application scenarios, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 03:09:40 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 20:36:43 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Amirtharaj", "Immanuel", ""], ["Groot", "Tai", ""], ["Dezfouli", "Behnam", ""]]}, {"id": "1808.10292", "submitter": "Alexandros Gerbessiotis", "authors": "Alexandros V. Gerbessiotis", "title": "A study of integer sorting on multicores", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.09495,\n  arXiv:1608.08648", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer sorting on multicores and GPUs can be realized by a variety of\napproaches that include variants of distribution-based methods such as\nradix-sort, comparison-oriented algorithms such as deterministic regular\nsampling and random sampling parallel sorting, and network-based algorithms\nsuch as Batcher's bitonic sorting algorithm.\n  In this work we present an experimental study of integer sorting on multicore\nprocessors. We have implemented serial and parallel radix-sort for various\nradixes, deterministic regular oversampling and random oversampling parallel\nsorting, and also some previously little explored or unexplored variants of\nbitonic-sort and odd-even transposition sort.\n  The study uses multithreading and multiprocessing parallel programming\nlibraries with the C language implementations working under Open MPI,\nMulticoreBSP, and BSPlib utilizing the same source code.\n  A secondary objective is to attempt to model the performance of these\nalgorithm implementations under the MBSP (Multi-memory BSP) model. We first\nprovide some general high-level observations on the performance of these\nimplementations. If we can conclude anything is that accurate prediction of\nperformance by taking into consideration architecture dependent features such\nas the structure and characteristics of multiple memory hierarchies is\ndifficult and more often than not untenable. To some degree this is affected by\nthe overhead imposed by the high-level library used in the programming effort.\nWe can still draw however some reliable conclusions and reason about the\nperformance of these implementations using the MBSP model, thus making MBSP\nuseful and usable.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:28:35 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Gerbessiotis", "Alexandros V.", ""]]}, {"id": "1808.10300", "submitter": "Michael Feldmann M. Sc.", "authors": "Michael Feldmann, Christina Kolb and Christian Scheideler", "title": "Self-stabilizing Overlays for high-dimensional Monotonic Searchability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the concept of monotonic searchability for self-stabilizing systems\nfrom one to multiple dimensions. A system is self-stabilizing if it can recover\nto a legitimate state from any initial illegal state. These kind of systems are\nmost often used in distributed applications. Monotonic searchability provides\nguarantees when searching for nodes while the recovery process is going on.\nMore precisely, if a search request started at some node $u$ succeeds in\nreaching its destination $v$, then all future search requests from $u$ to $v$\nsucceed as well. Although there already exists a self-stabilizing protocol for\na two-dimensional topology and an universal approach for monotonic\nsearchability, it is not clear how both of these concepts fit together\neffectively. The latter concept even comes with some restrictive assumptions on\nmessages, which is not the case for our protocol. We propose a simple novel\nprotocol for a self-stabilizing two-dimensional quadtree that satisfies\nmonotonic searchability. Our protocol can easily be extended to higher\ndimensions and offers routing in $\\mathcal O(\\log n)$ hops for any search\nrequest.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 13:54:48 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Feldmann", "Michael", ""], ["Kolb", "Christina", ""], ["Scheideler", "Christian", ""]]}, {"id": "1808.10375", "submitter": "Yao-Lung Leo Fang", "authors": "Zhihua Dong, Yao-Lung L. Fang, Xiaojing Huang, Hanfei Yan, Sungsoo Ha,\n  Wei Xu, Yong S. Chu, Stuart I. Campbell, and Meifeng Lin", "title": "High-Performance Multi-Mode Ptychography Reconstruction on Distributed\n  GPUs", "comments": "work presented in NYSDS 2018", "journal-ref": null, "doi": "10.1109/NYSDS.2018.8538964", "report-no": null, "categories": "physics.comp-ph cond-mat.mes-hall cs.DC physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ptychography is an emerging imaging technique that is able to provide\nwavelength-limited spatial resolution from specimen with extended lateral\ndimensions. As a scanning microscopy method, a typical two-dimensional image\nrequires a number of data frames. As a diffraction-based imaging technique, the\nreal-space image has to be recovered through iterative reconstruction\nalgorithms. Due to these two inherent aspects, a ptychographic reconstruction\nis generally a computation-intensive and time-consuming process, which limits\nthe throughput of this method. We report an accelerated version of the\nmulti-mode difference map algorithm for ptychography reconstruction using\nmultiple distributed GPUs. This approach leverages available scientific\ncomputing packages in Python, including mpi4py and PyCUDA, with the core\ncomputation functions implemented in CUDA C. We find that interestingly even\nwith MPI collective communications, the weak scaling in the number of GPU nodes\ncan still remain nearly constant. Most importantly, for realistic diffraction\nmeasurements, we observe a speedup ranging from a factor of $10$ to $10^3$\ndepending on the data size, which reduces the reconstruction time remarkably\nfrom hours to typically about 1 minute and is thus critical for real-time data\nprocessing and visualization.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 16:04:27 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Dong", "Zhihua", ""], ["Fang", "Yao-Lung L.", ""], ["Huang", "Xiaojing", ""], ["Yan", "Hanfei", ""], ["Ha", "Sungsoo", ""], ["Xu", "Wei", ""], ["Chu", "Yong S.", ""], ["Campbell", "Stuart I.", ""], ["Lin", "Meifeng", ""]]}, {"id": "1808.10776", "submitter": "Jaroslaw Zola", "authors": "Frank Schoeneman, Jaroslaw Zola", "title": "Scalable Manifold Learning for Big Data with Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear spectral dimensionality reduction methods, such as Isomap, remain\nimportant technique for learning manifolds. However, due to computational\ncomplexity, exact manifold learning using Isomap is currently impossible from\nlarge-scale data. In this paper, we propose a distributed memory framework\nimplementing end-to-end exact Isomap under Apache Spark model. We show how each\ncritical step of the Isomap algorithm can be efficiently realized using basic\nSpark model, without the need to provision data in the secondary storage. We\nshow how the entire method can be implemented using PySpark, offloading compute\nintensive linear algebra routines to BLAS. Through experimental results, we\ndemonstrate excellent scalability of our method, and we show that it can\nprocess datasets orders of magnitude larger than what is currently possible,\nusing a 25-node parallel~cluster.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:32:10 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Schoeneman", "Frank", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1808.10810", "submitter": "Jia Kan", "authors": "Jia Kan, Shangzhe Chen, Xin Huang", "title": "Improve Blockchain Performance using Graph Data Structure and Parallel\n  Mining", "comments": "6 pages, 2 figures, IEEE HotICN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology is ushering in another break-out year, the challenge of\nblockchain still remains to be solved. This paper analyzes the features of\nBitcoin and Bitcoin-NG system based on blockchain, proposes an improved method\nof implementing blockchain systems by replacing the structure of the original\nchain with the graph data structure. It was named GraphChain. Each block\nrepresents a transaction and contains the balance status of the traders.\nAdditionally, as everyone knows all the transactions in Bitcoin system will be\nbaled by only one miner that will result in a lot of wasted effort, so another\nway to improve resource utilization is to change the original way to compete\nfor miner to election and parallel mining. Researchers simulated blockchain\nwith graph structure and parallel mining through python, and suggested the\nconceptual new graph model which can improve both capacity and performance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 15:37:00 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 03:15:36 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Kan", "Jia", ""], ["Chen", "Shangzhe", ""], ["Huang", "Xin", ""]]}]