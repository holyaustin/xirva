[{"id": "1111.0064", "submitter": "EPTCS", "authors": "Ji\\v{r}\\'i Barnat, Keijo Heljanko", "title": "Proceedings 10th International Workshop on Parallel and Distributed\n  Methods in verifiCation", "comments": "EPTCS 72, 2011", "journal-ref": null, "doi": "10.4204/EPTCS.72", "report-no": null, "categories": "cs.DC cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the 10th International Workshop on\nParallel and Distributed Methods in verifiCation (PDMC 2011) that took place in\nSnowbird, Utah, on July 14, 2011. The workshop was co-located with 23rd\nInternational Conference on Computer Aided Verification (CAV 2011). The PDMC\nworkshop series covers all aspects related to the verification and analysis of\nvery large and complex systems using, in particular, methods and techniques\nthat exploit contemporary, hence parallel, hardware architectures. To celebrate\nthe 10th anniversary of PDMC, the workshop consisted of a half day invited\nsession together and a half day session of regular contributed presentations.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:21:56 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Barnat", "Ji\u0159\u00ed", ""], ["Heljanko", "Keijo", ""]]}, {"id": "1111.0242", "submitter": "Anita Sobe", "authors": "Anita Sobe, Wilfried Elmenreich and Laszlo B\\\"osz\\\"ormenyi", "title": "Storage Balancing in Self-organizing Multimedia Delivery Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR/ITEC/01/2.13", "categories": "cs.MM cs.DC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the current bio-inspired delivery networks set their focus on search,\ne.g., by using artificial ants. If the network size and, therefore, the search\nspace gets too large, the users experience high delays until the requested\ncontent can be consumed. In previous work, we proposed different replication\nstrategies to reduce the search space. In this report we further evaluate\nmeasures for storage load balancing, because peers are most likely limited in\nspace. We periodically apply clean-ups if a certain storage level is reached.\nFor our evaluations we combine the already introduced replication measures with\nleast recently used (LRU), least frequently used (LFU) and a hormone-based\nclean-up. The goal is to elaborate a combination that leads to low delays while\nthe replica utilization is high.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2011 16:52:10 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Sobe", "Anita", ""], ["Elmenreich", "Wilfried", ""], ["B\u00f6sz\u00f6rmenyi", "Laszlo", ""]]}, {"id": "1111.0321", "submitter": "Yoann Dieudonn\\'e", "authors": "Yoann Dieudonn\\'e and Andrzej Pelc", "title": "Anonymous Meeting in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A team consisting of an unknown number of mobile agents, starting from\ndifferent nodes of an unknown network, possibly at different times, have to\nmeet at the same node. Agents are anonymous (identical), execute the same\ndeterministic algorithm and move in synchronous rounds along links of the\nnetwork. Which configurations are gatherable and how to gather all of them\ndeterministically by the same algorithm?\n  We give a complete solution of this gathering problem in arbitrary networks.\nWe characterize all gatherable configurations and give two universal\ndeterministic gathering algorithms, i.e., algorithms that gather all gatherable\nconfigurations. The first algorithm works under the assumption that an upper\nbound n on the size of the network is known. In this case our algorithm\nguarantees gathering with detection, i.e., the existence of a round for any\ngatherable configuration, such that all agents are at the same node and all\ndeclare that gathering is accomplished. If no upper bound on the size of the\nnetwork is known, we show that a universal algorithm for gathering with\ndetection does not exist. Hence, for this harder scenario, we construct a\nsecond universal gathering algorithm, which guarantees that, for any gatherable\nconfiguration, all agents eventually get to one node and stop, although they\ncannot tell if gathering is over. The time of the first algorithm is polynomial\nin the upper bound n on the size of the network, and the time of the second\nalgorithm is polynomial in the (unknown) size itself.\n  Our results have an important consequence for the leader election problem for\nanonymous agents in arbitrary graphs. For anonymous agents in graphs, leader\nelection turns out to be equivalent to gathering with detection. Hence, as a\nby-product, we obtain a complete solution of the leader election problem for\nanonymous agents in arbitrary graphs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2011 21:18:10 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2012 19:44:25 GMT"}, {"version": "v3", "created": "Sun, 13 Mar 2016 09:49:08 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Dieudonn\u00e9", "Yoann", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1111.0370", "submitter": "EPTCS", "authors": "Peter Bulychev (Aalborg University), Alexandre David (Aalborg\n  University), Kim Guldstrand Larsen (Aalborg University), Marius\n  Miku\\v{c}ionis (Aalborg University), Axel Legay (Aalborg University and INRIA\n  Rennes)", "title": "Distributed Parametric and Statistical Model Checking", "comments": "In Proceedings PDMC 2011, arXiv:1111.0064", "journal-ref": "EPTCS 72, 2011, pp. 30-42", "doi": "10.4204/EPTCS.72.4", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical Model Checking (SMC) is a trade-off between testing and formal\nverification. The core idea of the approach is to conduct some simulations of\nthe system and verify if they satisfy some given property. In this paper we\nshow that SMC is easily parallelizable on a master/slaves architecture by\nintroducing a series of algorithms that scale almost linearly with respect to\nthe number of slave computers. Our approach has been implemented in the UPPAAL\nSMC toolset and applied on non-trivial case studies.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 03:04:23 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Bulychev", "Peter", "", "Aalborg University"], ["David", "Alexandre", "", "Aalborg\n  University"], ["Larsen", "Kim Guldstrand", "", "Aalborg University"], ["Miku\u010dionis", "Marius", "", "Aalborg University"], ["Legay", "Axel", "", "Aalborg University and INRIA\n  Rennes"]]}, {"id": "1111.0371", "submitter": "EPTCS", "authors": "Youssef Hamadi (Microsoft Research), Joao Marques-Silva (University\n  College Dublin), Christoph M. Wintersteiger (Microsoft Research)", "title": "Lazy Decomposition for Distributed Decision Procedures", "comments": "In Proceedings PDMC 2011, arXiv:1111.0064", "journal-ref": "EPTCS 72, 2011, pp. 43-54", "doi": "10.4204/EPTCS.72.5", "report-no": null, "categories": "cs.LO cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of automated tools for software and hardware\nverification puts ever increasing demands on the underlying decision\nprocedures. This paper presents a framework for distributed decision procedures\n(for first-order problems) based on Craig interpolation. Formulas are\ndistributed in a lazy fashion, i.e., without the use of costly decomposition\nalgorithms. Potential models which are shown to be incorrect are reconciled\nthrough the use of Craig interpolants. Experimental results on challenging\npropositional satisfiability problems indicate that our method is able to\noutperform traditional solving techniques even without the use of additional\nresources.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 03:04:28 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Hamadi", "Youssef", "", "Microsoft Research"], ["Marques-Silva", "Joao", "", "University\n  College Dublin"], ["Wintersteiger", "Christoph M.", "", "Microsoft Research"]]}, {"id": "1111.0372", "submitter": "EPTCS", "authors": "Temesghen Kahsai (The University of Iowa), Cesare Tinelli (The\n  University of Iowa)", "title": "PKind: A parallel k-induction based model checker", "comments": "In Proceedings PDMC 2011, arXiv:1111.0064", "journal-ref": "EPTCS 72, 2011, pp. 55-62", "doi": "10.4204/EPTCS.72.6", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PKind is a novel parallel k-induction-based model checker of invariant\nproperties for finite- or infinite-state Lustre programs. Its architecture,\nwhich is strictly message-based, is designed to minimize synchronization delays\nand easily accommodate the incorporation of incremental invariant generators to\nenhance basic k-induction. We describe PKind's functionality and main features,\nand present experimental evidence that PKind significantly speeds up the\nverification of safety properties and, due to incremental invariant generation,\nalso considerably increases the number of provable ones.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 03:04:34 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Kahsai", "Temesghen", "", "The University of Iowa"], ["Tinelli", "Cesare", "", "The\n  University of Iowa"]]}, {"id": "1111.0373", "submitter": "EPTCS", "authors": "Nikola Bene\\v{s} (FI MU), Ivana \\v{C}ern\\'a (FI MU), Milan\n  K\\v{r}iv\\'anek", "title": "CoInDiVinE: Parallel Distributed Model Checker for Component-Based\n  Systems", "comments": "In Proceedings PDMC 2011, arXiv:1111.0064", "journal-ref": "EPTCS 72, 2011, pp. 63-67", "doi": "10.4204/EPTCS.72.7", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CoInDiVinE is a tool for parallel distributed model checking of interactions\namong components in hierarchical component-based systems. The tool extends the\nDiVinE framework with a new input language (component-interaction automata) and\na property specification logic (CI-LTL). As the language differs from the input\nlanguage of DiVinE, our tool employs a new state space generation algorithm\nthat also supports partial order reduction. Experiments indicate that the tool\nhas good scaling properties when run in parallel setting.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 03:04:40 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Bene\u0161", "Nikola", "", "FI MU"], ["\u010cern\u00e1", "Ivana", "", "FI MU"], ["K\u0159iv\u00e1nek", "Milan", ""]]}, {"id": "1111.0374", "submitter": "EPTCS", "authors": "Stefan Vijzelaar (VU University Amsterdam), Kees Verstoep (VU\n  University Amsterdam), Wan Fokkink (VU University Amsterdam), Henri Bal (VU\n  University Amsterdam)", "title": "Distributed MAP in the SpinJa Model Checker", "comments": "In Proceedings PDMC 2011, arXiv:1111.0064", "journal-ref": "EPTCS 72, 2011, pp. 84-90", "doi": "10.4204/EPTCS.72.9", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spin in Java (SpinJa) is an explicit state model checker for the Promela\nmodelling language also used by the SPIN model checker. Designed to be\nextensible and reusable, the implementation of SpinJa follows a layered\napproach in which each new layer extends the functionality of the previous one.\nWhile SpinJa has preliminary support for shared-memory model checking, it did\nnot yet support distributed-memory model checking. This tool paper presents a\ndistributed implementation of a maximal accepting predecessors (MAP) search\nalgorithm on top of SpinJa.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 03:04:55 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Vijzelaar", "Stefan", "", "VU University Amsterdam"], ["Verstoep", "Kees", "", "VU\n  University Amsterdam"], ["Fokkink", "Wan", "", "VU University Amsterdam"], ["Bal", "Henri", "", "VU\n  University Amsterdam"]]}, {"id": "1111.0375", "submitter": "EPTCS", "authors": "Anton Wijs (Eindhoven University of Technology)", "title": "The HIVE Tool for Informed Swarm State Space Exploration", "comments": "In Proceedings PDMC 2011, arXiv:1111.0064", "journal-ref": "EPTCS 72, 2011, pp. 91-98", "doi": "10.4204/EPTCS.72.10", "report-no": null, "categories": "cs.SE cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm verification and parallel randomised depth-first search are very\neffective parallel techniques to hunt bugs in large state spaces. In case bugs\nare absent, however, scalability of the parallelisation is completely lost. In\nrecent work, we proposed a mechanism to inform the workers which parts of the\nstate space to explore. This mechanism is compatible with any action-based\nformalism, where a state space can be represented by a labelled transition\nsystem. With this extension, each worker can be strictly bounded to explore\nonly a small fraction of the state space at a time. In this paper, we present\nthe HIVE tool together with two search algorithms which were added to the\nLTSmin tool suite to both perform a preprocessing step, and execute a bounded\nworker search. The new tool is used to coordinate informed swarm explorations,\nand the two new LTSmin algorithms are employed for preprocessing a model and\nperforming the individual searches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 03:05:00 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Wijs", "Anton", "", "Eindhoven University of Technology"]]}, {"id": "1111.0583", "submitter": "Andrea Clementi", "authors": "Andrea Clementi, Riccardo Silvestri, and Luca Trevisan", "title": "Information Spreading in Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general approach to study the flooding time (a measure of how\nfast information spreads) in dynamic graphs (graphs whose topology changes with\ntime according to a random process).\n  We consider arbitrary converging Markovian dynamic graph process, that is,\nprocesses in which the topology of the graph at time $t$ depends only on its\ntopology at time $t-1$ and which have a unique stationary distribution. The\nmost well studied models of dynamic graphs are all Markovian and converging.\n  Under general conditions, we bound the flooding time in terms of the mixing\ntime of the dynamic graph process. We recover, as special cases of our result,\nbounds on the flooding time for the \\emph{random trip} model and the\n\\emph{random path} models; previous analysis techniques provided bounds only in\nrestricted settings for such models. Our result also provides the first bound\nfor the \\emph{random waypoint} model (which is tight for certain ranges of\nparameters) whose analysis had been an important open question.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 17:53:30 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2012 17:16:05 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Clementi", "Andrea", ""], ["Silvestri", "Riccardo", ""], ["Trevisan", "Luca", ""]]}, {"id": "1111.0594", "submitter": "Andrey Nikolaev", "authors": "Andrey Nikolaev", "title": "Exploring Oracle RDBMS latches using Solaris DTrace", "comments": "14 pages, 6 figures, 6 tables. MEDIAS 2011 Conference. Limassol,\n  Cyprus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rise of hundreds cores technologies bring again to the first plan the problem\nof interprocess synchronization in database engines. Spinlocks are widely used\nin contemporary DBMS to synchronize processes at microsecond timescale. Latches\nare Oracle RDBMS specific spinlocks. The latch contention is common to observe\nin contemporary high concurrency OLTP environments.\n  In contrast to system spinlocks used in operating systems kernels, latches\nwork in user context. Such user level spinlocks are influenced by context\npreemption and multitasking. Until recently there were no direct methods to\nmeasure effectiveness of user spinlocks. This became possible with the\nemergence of Solaris 10 Dynamic Tracing framework. DTrace allows tracing and\nprofiling both OS and user applications.\n  This work investigates the possibilities to diagnose and tune Oracle latches.\nIt explores the contemporary latch realization and spinning-blocking\nstrategies, analyses corresponding statistic counters.\n  A mathematical model developed to estimate analytically the effect of tuning\n_SPIN_COUNT value.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 18:20:36 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Nikolaev", "Andrey", ""]]}, {"id": "1111.0627", "submitter": "EPTCS", "authors": "Ji\\v{r}\\'i Barnat, Petr Bauch, Lubo\\v{s} Brim, Milan \\v{C}e\\v{s}ka", "title": "Computing Optimal Cycle Mean in Parallel on CUDA", "comments": "In Proceedings PDMC 2011, arXiv:1111.0064", "journal-ref": "EPTCS 72, 2011, pp. 68-83", "doi": "10.4204/EPTCS.72.8", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of optimal cycle mean in a directed weighted graph has many\napplications in program analysis, performance verification in particular. In\nthis paper we propose a data-parallel algorithmic solution to the problem and\nshow how the computation of optimal cycle mean can be efficiently accelerated\nby means of CUDA technology. We show how the problem of computation of optimal\ncycle mean is decomposed into a sequence of data-parallel graph computation\nprimitives and show how these primitives can be implemented and optimized for\nCUDA computation. Finally, we report a fivefold experimental speed up on graphs\nrepresenting models of distributed systems when compared to best sequential\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 03:04:48 GMT"}], "update_date": "2011-11-04", "authors_parsed": [["Barnat", "Ji\u0159\u00ed", ""], ["Bauch", "Petr", ""], ["Brim", "Lubo\u0161", ""], ["\u010ce\u0161ka", "Milan", ""]]}, {"id": "1111.0875", "submitter": "Aditya Kurve", "authors": "Aditya Kurve, Christopher Griffin, David J. Miller and George Kesidis", "title": "Game Theoretic Iterative Partitioning for Dynamic Load Balancing in\n  Distributed Network Simulation", "comments": "Requires a more thorough study on actual simulator platform", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High fidelity simulation of large-sized complex networks can be realized on a\ndistributed computing platform that leverages the combined resources of\nmultiple processors or machines. In a discrete event driven simulation, the\nassignment of logical processes (LPs) to machines is a critical step that\naffects the computational and communication burden on the machines, which in\nturn affects the simulation execution time of the experiment. We study a\nnetwork partitioning game wherein each node (LP) acts as a selfish player. We\nderive two local node-level cost frameworks which are feasible in the sense\nthat the aggregate state information required to be exchanged between the\nmachines is independent of the size of the simulated network model. For both\ncost frameworks, we prove the existence of stable Nash equilibria in pure\nstrategies. Using iterative partition improvements, we propose game theoretic\npartitioning algorithms based on the two cost criteria and show that each\ndescends in a global cost. To exploit the distributed nature of the system, the\nalgorithm is distributed, with each node's decision based on its local\ninformation and on a few global quantities which can be communicated\nmachine-to-machine. We demonstrate the performance of our partitioning\nalgorithm on an optimistic discrete event driven simulation platform that\nmodels an actual parallel simulator.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2011 15:21:25 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2012 23:16:54 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Kurve", "Aditya", ""], ["Griffin", "Christopher", ""], ["Miller", "David J.", ""], ["Kesidis", "George", ""]]}, {"id": "1111.0922", "submitter": "Markus Wittmann", "authors": "Markus Wittmann, Thomas Zeiser, Georg Hager, Gerhard Wellein", "title": "Comparison of different Propagation Steps for the Lattice Boltzmann\n  Method", "comments": "17 pages, 11 figures, 8 tables, preprint submitted to Computers &\n  Mathematics with Applications", "journal-ref": "Computers & Mathematics with Applications, Volume 65, Issue 6,\n  Pages 924-935 (2013)", "doi": "10.1016/j.camwa.2012.05.002", "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several possibilities exist to implement the propagation step of the lattice\nBoltzmann method. This paper describes common implementations which are\ncompared according to the number of memory transfer operations they require per\nlattice node update. A memory bandwidth based performance model is then used to\nobtain an estimation of the maximal reachable performance on different\nmachines. A subset of the discussed implementations of the propagation step\nwere benchmarked on different Intel and AMD-based compute nodes using the\nframework of an existing flow solver which is specially adapted to simulate\nflow in porous media. Finally the estimated performance is compared to the\nmeasured one. As expected, the number of memory transfers has a significant\nimpact on performance. Advanced approaches for the propagation step like \"AA\npattern\" or \"Esoteric Twist\" require more implementation effort but sustain\nsignificantly better performance than non-naive straight forward\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2011 17:24:48 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Wittmann", "Markus", ""], ["Zeiser", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1111.1129", "submitter": "Markus Wittmann", "authors": "Markus Wittmann, Thomas Zeiser, Georg Hager, Gerhard Wellein", "title": "Domain decomposition and locality optimization for large-scale lattice\n  Boltzmann simulations", "comments": null, "journal-ref": "Computers & Fluids, Volume 80, Pages 283-289 (2013)", "doi": "10.1016/j.compfluid.2012.02.007", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, parallel and distributed algorithm for setting up and\npartitioning a sparse representation of a regular discretized simulation\ndomain. This method is scalable for a large number of processes even for\ncomplex geometries and ensures load balance between the domains, reasonable\ncommunication interfaces, and good data locality within the domain. Applying\nthis scheme to a list-based lattice Boltzmann flow solver can achieve similar\nor even higher flow solver performance than widely used standard graph\npartition based tools such as METIS and PT-SCOTCH.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2011 13:52:36 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Wittmann", "Markus", ""], ["Zeiser", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1111.1170", "submitter": "Benjamin Morandi", "authors": "Benjamin Morandi, Sebastian Nanz, Bertrand Meyer", "title": "Record-replay debugging for the SCOOP concurrency model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To support developers in writing reliable and efficient concurrent programs,\nnovel concurrent programming abstractions have been proposed in recent years.\nProgramming with such abstractions requires new analysis tools because the\nexecution semantics often differs considerably from established models. We\npresent a record-replay technique for programs written in SCOOP, an\nobject-oriented programming model for concurrency. The resulting tool enables\ndevelopers to reproduce the nondeterministic execution of a concurrent program,\na necessary prerequisite for debugging and testing.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2011 16:15:14 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2012 15:29:44 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2012 15:43:04 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Morandi", "Benjamin", ""], ["Nanz", "Sebastian", ""], ["Meyer", "Bertrand", ""]]}, {"id": "1111.1373", "submitter": "Jason Spencer", "authors": "Jason Spencer", "title": "Speculative Parallel Evaluation Of Classification Trees On GPGPU Compute\n  Engines", "comments": "14 pages, 4 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of optimizing classification tree evaluation for\non-line and real-time applications by using GPUs. Looking at trees with\ncontinuous attributes often used in image segmentation, we first put the\nexisting algorithms for serial and data-parallel evaluation on solid footings.\nWe then introduce a speculative parallel algorithm designed for single\ninstruction, multiple data (SIMD) architectures commonly found in GPUs. A\ntheoretical analysis shows how the run times of data and speculative\ndecompositions compare assuming independent processors. To compare the\nalgorithms in the SIMD environment, we implement both on a CUDA 2.0\narchitecture machine and compare timings to a serial CPU implementation.\nVarious optimizations and their effects are discussed, and results are given\nfor all algorithms. Our specific tests show a speculative algorithm improves\nrun time by 25% compared to a data decomposition.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2011 04:49:31 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Spencer", "Jason", ""]]}, {"id": "1111.1492", "submitter": "Taisuke Izumi", "authors": "Taisuke Izumi and Samia Souissi and Yoshiaki Katayama and Nobuhiro\n  Inuzuka and Xavier D\\'efago and Koichi Wada and Masafumi Yamashita", "title": "The Gathering Problem for Two Oblivious Robots with Unreliable Compasses", "comments": "23 pages, 10 figures, to appear at SIAM Journal on Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymous mobile robots are often classified into synchronous,\nsemi-synchronous and asynchronous robots when discussing the pattern formation\nproblem. For semi-synchronous robots, all patterns formable with memory are\nalso formable without memory, with the single exception of forming a point\n(i.e., the gathering) by two robots. However, the gathering problem for two\nsemi-synchronous robots without memory is trivially solvable when their local\ncoordinate systems are consistent, and the impossibility proof essentially uses\nthe inconsistencies in their coordinate systems. Motivated by this, this paper\ninvestigates the magnitude of consistency between the local coordinate systems\nnecessary and sufficient to solve the gathering problem for two oblivious\nrobots under semi-synchronous and asynchronous models. To discuss the magnitude\nof consistency, we assume that each robot is equipped with an unreliable\ncompass, the bearings of which may deviate from an absolute reference\ndirection, and that the local coordinate system of each robot is determined by\nits compass. We consider two families of unreliable compasses, namely,static\ncompasses with constant bearings, and dynamic compasses the bearings of which\ncan change arbitrarily.\n  For each of the combinations of robot and compass models, we establish the\ncondition on deviation \\phi that allows an algorithm to solve the gathering\nproblem, where the deviation is measured by the largest angle formed between\nthe x-axis of a compass and the reference direction of the global coordinate\nsystem: \\phi < \\pi/2 for semi-synchronous and asynchronous robots with static\ncompasses, \\phi < \\pi/4 for semi-synchronous robots with dynamic compasses, and\n\\phi < \\pi/6 for asynchronous robots with dynamic compasses. Except for\nasynchronous robots with dynamic compasses, these sufficient conditions are\nalso necessary.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 06:00:11 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Izumi", "Taisuke", ""], ["Souissi", "Samia", ""], ["Katayama", "Yoshiaki", ""], ["Inuzuka", "Nobuhiro", ""], ["D\u00e9fago", "Xavier", ""], ["Wada", "Koichi", ""], ["Yamashita", "Masafumi", ""]]}, {"id": "1111.1750", "submitter": "Kanat Tangwongsan", "authors": "Guy E. Blelloch, Anupam Gupta, Ioannis Koutis, Gary L. Miller, Richard\n  Peng, Kanat Tangwongsan", "title": "Near Linear-Work Parallel SDD Solvers, Low-Diameter Decomposition, and\n  Low-Stretch Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and analysis of a near linear-work parallel algorithm\nfor solving symmetric diagonally dominant (SDD) linear systems. On input of a\nSDD $n$-by-$n$ matrix $A$ with $m$ non-zero entries and a vector $b$, our\nalgorithm computes a vector $\\tilde{x}$ such that $\\norm[A]{\\tilde{x} - A^+b}\n\\leq \\vareps \\cdot \\norm[A]{A^+b}$ in $O(m\\log^{O(1)}{n}\\log{\\frac1\\epsilon})$\nwork and $O(m^{1/3+\\theta}\\log \\frac1\\epsilon)$ depth for any fixed $\\theta >\n0$.\n  The algorithm relies on a parallel algorithm for generating low-stretch\nspanning trees or spanning subgraphs. To this end, we first develop a parallel\ndecomposition algorithm that in polylogarithmic depth and $\\otilde(|E|)$ work,\npartitions a graph into components with polylogarithmic diameter such that only\na small fraction of the original edges are between the components. This can be\nused to generate low-stretch spanning trees with average stretch\n$O(n^{\\alpha})$ in $O(n^{1+\\alpha})$ work and $O(n^{\\alpha})$ depth.\nAlternatively, it can be used to generate spanning subgraphs with\npolylogarithmic average stretch in $\\otilde(|E|)$ work and polylogarithmic\ndepth. We apply this subgraph construction to derive a parallel linear system\nsolver. By using this solver in known applications, our results imply improved\nparallel randomized algorithms for several problems, including single-source\nshortest paths, maximum flow, minimum-cost flow, and approximate maximum flow.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 21:17:09 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Gupta", "Anupam", ""], ["Koutis", "Ioannis", ""], ["Miller", "Gary L.", ""], ["Peng", "Richard", ""], ["Tangwongsan", "Kanat", ""]]}, {"id": "1111.2208", "submitter": "Ruchi Tuli Dr.", "authors": "Ruchi Tuli, Parveen Kumar", "title": "Minimum Process Coordinated Checkpointing Scheme for Ad Hoc Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wireless mobile ad hoc network (MANET) architecture is one consisting of\na set of mobile hosts capable of communicating with each other without the\nassistance of base stations. This has made possible creating a mobile\ndistributed computing environment and has also brought several new challenges\nin distributed protocol design. In this paper, we study a very fundamental\nproblem, the fault tolerance problem, in a MANET environment and propose a\nminimum process coordinated checkpointing scheme. Since potential problems of\nthis new environment are insufficient power and limited storage capacity, the\nproposed scheme tries to reduce the amount of information saved for recovery.\nThe MANET structure used in our algorithm is hierarchical based. The scheme is\nbased for Cluster Based Routing Protocol (CBRP) which belongs to a class of\nHierarchical Reactive routing protocols. The protocol proposed by us is\nnonblocking coordinated checkpointing algorithm suitable for ad hoc\nenvironments. It produces a consistent set of checkpoints; the algorithm makes\nsure that only minimum number of nodes in the cluster are required to take\ncheckpoints; it uses very few control messages. Performance analysis shows that\nour algorithm outperforms the existing related works and is a novel idea in the\nfield. Firstly, we describe an organization of the cluster. Then we propose a\nminimum process coordinated checkpointing scheme for cluster based ad hoc\nrouting protocols.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2011 14:01:07 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Tuli", "Ruchi", ""], ["Kumar", "Parveen", ""]]}, {"id": "1111.2228", "submitter": "Francesco Silvestri", "authors": "Andrea Pietracaprina, Geppino Pucci, Matteo Riondato, Francesco\n  Silvestri, Eli Upfal", "title": "Space-Round Tradeoffs for MapReduce Computations", "comments": null, "journal-ref": "Final version in Proc. of the 26th ACM international conference on\n  Supercomputing, pages 235-244, 2012", "doi": "10.1145/2304576.2304607", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores fundamental modeling and algorithmic issues arising in the\nwell-established MapReduce framework. First, we formally specify a\ncomputational model for MapReduce which captures the functional flavor of the\nparadigm by allowing for a flexible use of parallelism. Indeed, the model\ndiverges from a traditional processor-centric view by featuring parameters\nwhich embody only global and local memory constraints, thus favoring a more\ndata-centric view. Second, we apply the model to the fundamental computation\ntask of matrix multiplication presenting upper and lower bounds for both dense\nand sparse matrix multiplication, which highlight interesting tradeoffs between\nspace and round complexity. Finally, building on the matrix multiplication\nresults, we derive further space-round tradeoffs on matrix inversion and\nmatching.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2011 15:13:40 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Riondato", "Matteo", ""], ["Silvestri", "Francesco", ""], ["Upfal", "Eli", ""]]}, {"id": "1111.2237", "submitter": "Oleg Titov", "authors": "Elena Legchekova, Oleg Titov", "title": "Choosing the best resource by method of mamdani", "comments": "Article in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A method for selecting the best service for the storage of information by\nMamdani.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2011 20:35:30 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Legchekova", "Elena", ""], ["Titov", "Oleg", ""]]}, {"id": "1111.2412", "submitter": "Dinesh C", "authors": "C. Dinesh (Mailam Engineering College)", "title": "Secured Data Consistency and Storage Way in Untrusted Cloud using Server\n  Management Algorithm", "comments": "6 pages,3 figures. I am the only author of this title and related\n  information; International Journal of Computer Applications (0975 - 8887)\n  Volume 31- No.6, October 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is very challenging part to keep safely all required data that are needed\nin many applications for user in cloud. Storing our data in cloud may not be\nfully trustworthy. Since client doesn't have copy of all stored data, he has to\ndepend on Cloud Service Provider. But dynamic data operations, Read-Solomon and\nverification token construction methods don't tell us about total storage\ncapacity of server allocated space before and after the data addition in cloud.\nSo we have to introduce a new proposed system of efficient storage measurement\nand space comparison algorithm with time management for measuring the total\nallocated storage area before and after the data insertion in cloud. So by\nusing our proposed scheme, the value or weight of stored data before and after\nis measured by client with specified time in cloud storage area with accuracy.\nAnd here we also have proposed the multi-server restore point in server failure\ncondition. If there occurs any server failure, by using this scheme the data\ncan be recovered automatically in cloud server. Our proposed scheme efficiently\nchecks space for the in-outsourced data to maintain integrity. Here the TPA\nnecessarily doesn't have the delegation to audit user's data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2011 08:05:38 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Dinesh", "C.", "", "Mailam Engineering College"]]}, {"id": "1111.2418", "submitter": "Dinesh C", "authors": "C. Dinesh (Mailam Engineering College)", "title": "Data Integrity and Dynamic Storage Way in Cloud Computing", "comments": "1 figure,6 pages, i am the only author for this title and related\n  information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not an easy task to securely maintain all essential data where it has\nthe need in many applications for clients in cloud. To maintain our data in\ncloud, it may not be fully trustworthy because client doesn't have copy of all\nstored data. But any authors don't tell us data integrity through its user and\nCSP level by comparison before and after the data update in cloud. So we have\nto establish new proposed system for this using our data reading protocol\nalgorithm to check the integrity of data before and after the data insertion in\ncloud. Here the security of data before and after is checked by client with the\nhelp of CSP using our \"effective automatic data reading protocol from user as\nwell as cloud level into the cloud\" with truthfulness. Also we have proposed\nthe multi-server data comparison algorithm with the calculation of overall data\nin each update before its outsourced level for server restore access point for\nfuture data recovery from cloud data server. Our proposed scheme efficiently\nchecks integrity in efficient manner so that data integrity as well as security\ncan be maintained in all cases by considering drawbacks of existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2011 08:23:47 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Dinesh", "C.", "", "Mailam Engineering College"]]}, {"id": "1111.2693", "submitter": "Nicolas Nicolaou", "authors": "Chryssis Georgiou and Nicolas C. Nicolaou", "title": "On the Practicality of Atomic MWMR Register Implementations", "comments": "18 pages, 14 figures, 3 tables, Technical Report, Full Version of an\n  Article appearing in the Proceedings of the 10th International Symposium on\n  Parallel and Distributed Processing with Applications (ISPA 2012), Leganes,\n  Madrid, July 2012", "journal-ref": null, "doi": null, "report-no": "UCY-CS-TR-11-08", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-writer/multiple-reader (MWMR) atomic register implementations\nprovide precise consistency guarantees, in the asynchronous, crash-prone,\nmessage passing environment. Fast MWMR atomic register implementations were\nfirst introduced in Englert et al. 2009. Fastness is measured in terms of the\nnumber of single round read and write operations that does not sacrifice\ncorrectness. In Georgiou et al. 2011 was shown, however, that decreasing the\ncommunication cost is not enough in these implementations. In particular,\nconsidering that the performance is measured in terms of the latency of read\nand write operations due to both (a) communication delays and (b)local\ncomputation, they introduced two new algorithms that traded communication for\nreducing computation. As computation is still part of the algorithms, someone\nmay wonder: What is the trade-off between communication and local computation\nin real-time systems?\n  In this work we conduct an experimental performance evaluation of four MWMR\natomic register implementations: SFW from Englert et al. 2009, APRX-SFW and\nCWFR from Georgiou at al. 2011, and the generalization of the traditional\nalgorithm of Attiya et al. 1996 in the MWMR environment, which we call SIMPLE.\nWe implement and evaluate the algorithms on NS2, a single-processor simulator,\nand on PlanetLab, a planetary-scale real-time network platform. Our comparison\nprovides an empirical answer to the above question and demonstrates the\npracticality of atomic MWMR register implementations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2011 09:50:01 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2012 13:51:10 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Georgiou", "Chryssis", ""], ["Nicolaou", "Nicolas C.", ""]]}, {"id": "1111.3022", "submitter": "Yu Huang", "authors": "Yiling Yang, Yu Huang, Jiannong Cao, Xiaoxing Ma, Jian Lu", "title": "Design of a Sliding Window over Asynchronous Event Streams", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of sensing and monitoring applications motivates adoption\nof the event stream model of computation. Though sliding windows are widely\nused to facilitate effective event stream processing, it is greatly challenged\nwhen the event sources are distributed and asynchronous. To address this\nchallenge, we first show that the snapshots of the asynchronous event streams\nwithin the sliding window form a convex distributive lattice (denoted by\nLat-Win). Then we propose an algorithm to maintain Lat-Win at runtime. The\nLat-Win maintenance algorithm is implemented and evaluated on the open-source\ncontext-aware middleware we developed. The evaluation results first show the\nnecessity of adopting sliding windows over asynchronous event streams. Then\nthey show the performance of detecting specified predicates within Lat-Win,\neven when faced with dynamic changes in the computing environment.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2011 15:20:49 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Yang", "Yiling", ""], ["Huang", "Yu", ""], ["Cao", "Jiannong", ""], ["Ma", "Xiaoxing", ""], ["Lu", "Jian", ""]]}, {"id": "1111.3096", "submitter": "Kamal Ahmat", "authors": "Hassan Gobjuka and Kamal Ahmat", "title": "vFlow: A GUI-Based Tool for Building Batch Applications for Cloud\n  Computing", "comments": "IEEE INFOCOM 2011 Demo Session, 2 Pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce vFlow - A framework for rapid designing of batch\nprocessing applications for Cloud Computing environment. vFlow batch processing\nsystem extracts tasks from the vPlans diagrams, systematically captures the\ndynamics in batch application management tasks, and translates them to Cloud\nenvironment API, named vDocuments, that can be used to execute batch processing\napplications. vDocuments do not only enable the complete execution of low-level\nconfiguration management tasks, but also allow the construction of more\nsophisticated tasks, while imposing additional reasoning logic to realize batch\napplication management objectives in Cloud environments. We present the design\nof the vFlow framework and illustrate its utility by presenting the\nimplementation of several sophisticated operational tasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2011 05:05:11 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Gobjuka", "Hassan", ""], ["Ahmat", "Kamal", ""]]}, {"id": "1111.3165", "submitter": "Pardeep Kumar", "authors": "Pardeep Kumar, Vivek Kumar Sehgal, Durg Singh Chauhan, P. K. Gupta and\n  Manoj Diwakar", "title": "Effective Ways of Secure, Private and Trusted Cloud Computing", "comments": "10 pages,6 figures; IJCSI International Journal of Computer Science\n  Issues, May 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is an Internet-based computing, where shared resources,\nsoftware and information, are provided to computers and devices on-demand. It\nprovides people the way to share distributed resources and services that belong\nto different organization. Since cloud computing uses distributed resources in\nopen environment, thus it is important to provide the security and trust to\nshare the data for developing cloud computing applications. In this paper we\nassess how can cloud providers earn their customers' trust and provide the\nsecurity, privacy and reliability, when a third party is processing sensitive\ndata in a remote machine located in various countries? A concept of utility\ncloud has been represented to provide the various services to the users.\nEmerging technologies can help address the challenges of Security, Privacy and\nTrust in cloud computing.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2011 10:28:59 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Kumar", "Pardeep", ""], ["Sehgal", "Vivek Kumar", ""], ["Chauhan", "Durg Singh", ""], ["Gupta", "P. K.", ""], ["Diwakar", "Manoj", ""]]}, {"id": "1111.3334", "submitter": "Partha Sarathi Mandal Dr.", "authors": "Abhishek Kr. Singh, Bollibisai Giridhar, Partha Sarathi Mandal", "title": "Fixing Data Anomalies with Prediction Based Algorithm in Wireless Sensor\n  Networks", "comments": "6 pages, 8 figures, The paper has been accepted for presentation at\n  7th IEEE Conference on Wireless Communication and Sensor Networks (WCSN-2011)\n  December 05-09, 2011, Panna National Park, Madhya Pradesh, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data inconsistencies are present in the data collected over a large wireless\nsensor network (WSN), usually deployed for any kind of monitoring applications.\nBefore passing this data to some WSN applications for decision making, it is\nnecessary to ensure that the data received are clean and accurate. In this\npaper, we have used a statistical tool to examine the past data to fit in a\nhighly sophisticated prediction model i.e., ARIMA for a given sensor node and\nwith this, the model corrects the data using forecast value if any data anomaly\nexists there. Another scheme is also proposed for detecting data anomaly at\nsink among the aggregated data in the data are received from a particular\nsensor node. The effectiveness of our methods are validated by data collected\nover a real WSN application consisting of Crossbow IRIS Motes\n\\cite{Crossbow:2009}.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2011 19:43:04 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Singh", "Abhishek Kr.", ""], ["Giridhar", "Bollibisai", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "1111.3806", "submitter": "Aki Saarinen", "authors": "Aki Saarinen, Matti Siekkinen, Yu Xiao, Jukka K. Nurminen, Matti\n  Kemppainen, Pan Hui", "title": "Offloadable Apps using SmartDiet: Towards an analysis toolkit for mobile\n  application developers", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offloading work to cloud is one of the proposed solutions for increasing the\nbattery life of mobile devices. Most prior research has focused on\ncomputation-intensive applications, even though such applications are not the\nmost popular ones. In this paper, we first study the feasibility of\nmethod-level offloading in network-intensive applications, using an open source\nTwitter client as an example. Our key observation is that implementing\noffloading transparently to the developer is difficult: various constraints\nheavily limit the offloading possibilities, and estimation of the potential\nbenefit is challenging. We then propose a toolkit, SmartDiet, to assist mobile\napplication developers in creating code which is suitable for energy-efficient\noffloading. SmartDiet provides fine-grained offloading constraint\nidentification and energy usage analysis for Android applications. In addition\nto outlining the overall functionality of the toolkit, we study some of its key\nmechanisms and identify the remaining challenges.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2011 14:00:03 GMT"}], "update_date": "2011-11-17", "authors_parsed": [["Saarinen", "Aki", ""], ["Siekkinen", "Matti", ""], ["Xiao", "Yu", ""], ["Nurminen", "Jukka K.", ""], ["Kemppainen", "Matti", ""], ["Hui", "Pan", ""]]}, {"id": "1111.4499", "submitter": "Somayeh Kafaie", "authors": "Somayeh Kafaie, Omid Kashefi and Mohsen Sharifi", "title": "A Low-Energy Fast Cyber Foraging Mechanism for Mobile Devices", "comments": "12 pages, 7 figures, International Journal of Wireless & Mobile\n  Networks (IJWMN)", "journal-ref": "International Journal of Wireless & Mobile Networks (IJWMN) Vol.\n  3, No. 5, October 2011", "doi": "10.5121/ijwmn.2011.3516", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever increasing demands for using resource-constrained mobile devices for\nrunning more resource intensive applications nowadays has initiated the\ndevelopment of cyber foraging solutions that offload parts or whole\ncomputational intensive tasks to more powerful surrogate stationary computers\nand run them on behalf of mobile devices as required. The choice of proper mix\nof mobile devices and surrogates has remained an unresolved challenge though.\nIn this paper, we propose a new decision-making mechanism for cyber foraging\nsystems to select the best locations to run an application, based on context\nmetrics such as the specifications of surrogates, the specifications of mobile\ndevices, application specification, and communication network specification.\nExperimental results show faster response time and lower energy consumption of\nbenched applications compared to when applications run wholly on mobile devices\nand when applications are offloaded to surrogates blindly for execution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2011 21:37:36 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Kafaie", "Somayeh", ""], ["Kashefi", "Omid", ""], ["Sharifi", "Mohsen", ""]]}, {"id": "1111.4533", "submitter": "Lluis Pamies-Juarez", "authors": "Lluis Pamies-Juarez and Anwitaman Datta and Fr\\'ed\\'erique Oggier", "title": "In-Network Redundancy Generation for Opportunistic Speedup of Backup", "comments": null, "journal-ref": "Future Generation Computer Systems (Volume 29, Issue 6, August\n  2013, Pages 1353-1362)", "doi": "10.1016/j.future.2013.02.009", "report-no": null, "categories": "cs.DC cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erasure coding is a storage-efficient alternative to replication for\nachieving reliable data backup in distributed storage systems. During the\nstorage process, traditional erasure codes require a unique source node to\ncreate and upload all the redundant data to the different storage nodes.\nHowever, such a source node may have limited communication and computation\ncapabilities, which constrain the storage process throughput. Moreover, the\nsource node and the different storage nodes might not be able to send and\nreceive data simultaneously -- e.g., nodes might be busy in a datacenter\nsetting, or simply be offline in a peer-to-peer setting -- which can further\nthreaten the efficacy of the overall storage process. In this paper we propose\nan \"in-network\" redundancy generation process which distributes the data\ninsertion load among the source and storage nodes by allowing the storage nodes\nto generate new redundant data by exchanging partial information among\nthemselves, improving the throughput of the storage process. The process is\ncarried out asynchronously, utilizing spare bandwidth and computing resources\nfrom the storage nodes. The proposed approach leverages on the local\nrepairability property of newly proposed erasure codes tailor made for the\nneeds of distributed storage systems. We analytically show that the performance\nof this technique relies on an efficient usage of the spare node resources, and\nwe derive a set of scheduling algorithms to maximize the same. We\nexperimentally show, using availability traces from real peer-to-peer\napplications as well as Google data center availability and workload traces,\nthat our algorithms can, depending on the environment characteristics, increase\nthe throughput of the storage process significantly (up to 90% in data centers,\nand 60% in peer-to-peer settings) with respect to the classical naive data\ninsertion approach.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2011 04:50:34 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 11:00:58 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Pamies-Juarez", "Lluis", ""], ["Datta", "Anwitaman", ""], ["Oggier", "Fr\u00e9d\u00e9rique", ""]]}, {"id": "1111.4825", "submitter": "Eduardo Montijano", "authors": "Eduardo Montijano, Juan I. Montijano, Carlos Sagues", "title": "Chebyshev Polynomials in Distributed Consensus Applications", "comments": "Preprint submitted for its publication in a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the use of Chebyshev polynomials in distributed\nconsensus applications. We study the properties of these polynomials to propose\na distributed algorithm that reaches the consensus in a fast way. The algorithm\nis expressed in the form of a linear iteration and, at each step, the agents\nonly require to transmit their current state to their neighbors. The difference\nwith respect to previous approaches is that the update rule used by the network\nis based on the second order difference equation that describes the Chebyshev\npolynomials of first kind. As a consequence, we show that our algorithm\nachieves the consensus using far less iterations than other approaches. We\ncharacterize the main properties of the algorithm for both, fixed and switching\ncommunication topologies. The main contribution of the paper is the study of\nthe properties of the Chebyshev polynomials in distributed consensus\napplications, proposing an algorithm that increases the convergence rate with\nrespect to existing approaches. Theoretical results, as well as experiments\nwith synthetic data, show the benefits using our algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2011 10:57:02 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2011 10:23:58 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2012 16:20:13 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Montijano", "Eduardo", ""], ["Montijano", "Juan I.", ""], ["Sagues", "Carlos", ""]]}, {"id": "1111.5123", "submitter": "Erwan Le Merrer", "authors": "Olivier Heen, Erwan Le Merrer, Christoph Neumann, St\\'ephane Onno", "title": "Pretty Private Group Management", "comments": null, "journal-ref": "Symposium on Reliable Distributed Systems (2012) 191-200", "doi": "10.1109/SRDS.2012.20", "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group management is a fundamental building block of today's Internet\napplications. Mailing lists, chat systems, collaborative document edition but\nalso online social networks such as Facebook and Twitter use group management\nsystems. In many cases, group security is required in the sense that access to\ndata is restricted to group members only. Some applications also require\nprivacy by keeping group members anonymous and unlinkable. Group management\nsystems routinely rely on a central authority that manages and controls the\ninfrastructure and data of the system. Personal user data related to groups\nthen becomes de facto accessible to the central authority. In this paper, we\npropose a completely distributed approach for group management based on\ndistributed hash tables. As there is no enrollment to a central authority, the\ncreated groups can be leveraged by various applications. Following this\nparadigm we describe a protocol for such a system. We consider security and\nprivacy issues inherently introduced by removing the central authority and\nprovide a formal validation of security properties of the system using AVISPA.\nWe demonstrate the feasibility of this protocol by implementing a prototype\nrunning on top of Vuze's DHT.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2011 08:33:27 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Heen", "Olivier", ""], ["Merrer", "Erwan Le", ""], ["Neumann", "Christoph", ""], ["Onno", "St\u00e9phane", ""]]}, {"id": "1111.5239", "submitter": "David Shuman", "authors": "David I Shuman, Pierre Vandergheynst, Daniel Kressner, and Pascal\n  Frossard", "title": "Distributed Signal Processing via Chebyshev Polynomial Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unions of graph multiplier operators are an important class of linear\noperators for processing signals defined on graphs. We present a novel method\nto efficiently distribute the application of these operators. The proposed\nmethod features approximations of the graph multipliers by shifted Chebyshev\npolynomials, whose recurrence relations make them readily amenable to\ndistributed computation. We demonstrate how the proposed method can be applied\nto distributed processing tasks such as smoothing, denoising, inverse\nfiltering, and semi-supervised classification, and show that the communication\nrequirements of the method scale gracefully with the size of the network.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2011 16:15:32 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2011 05:46:30 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 13:51:47 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Shuman", "David I", ""], ["Vandergheynst", "Pierre", ""], ["Kressner", "Daniel", ""], ["Frossard", "Pascal", ""]]}, {"id": "1111.5391", "submitter": "Qiang Dong", "authors": "Qiang Dong, Hui Gao, Yan Fu and Xiaofan Yang", "title": "Hamiltonian Connectivity of Twisted Hypercube-Like Networks under the\n  Large Fault Model", "comments": "22 pages, 26 figures, Submitted to IEEE TPDS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twisted hypercube-like networks (THLNs) are an important class of\ninterconnection networks for parallel computing systems, which include most\npopular variants of the hypercubes, such as crossed cubes, M\\\"obius cubes,\ntwisted cubes and locally twisted cubes. This paper deals with the\nfault-tolerant hamiltonian connectivity of THLNs under the large fault model.\nLet $G$ be an $n$-dimensional THLN and $F \\subseteq V(G)\\bigcup E(G)$, where $n\n\\geq 7$ and $|F| \\leq 2n - 10$. We prove that for any two nodes $u,v \\in V(G -\nF)$ satisfying a simple necessary condition on neighbors of $u$ and $v$, there\nexists a hamiltonian or near-hamiltonian path between $u$ and $v$ in $G-F$. The\nresult extends further the fault-tolerant graph embedding capability of THLNs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 03:05:45 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Dong", "Qiang", ""], ["Gao", "Hui", ""], ["Fu", "Yan", ""], ["Yang", "Xiaofan", ""]]}, {"id": "1111.5528", "submitter": "Guillaume Aupy", "authors": "Guillaume Aupy, Anne Benoit and Yves Robert", "title": "Energy-aware scheduling under reliability and makespan constraints", "comments": "22 pages. A 10 pages version should appear in HiPC'12", "journal-ref": null, "doi": null, "report-no": "Inria Research Report 7757", "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a task graph mapped on a set of homogeneous processors. We aim at\nminimizing the energy consumption while enforcing two constraints: a prescribed\nbound on the execution time (or makespan), and a reliability threshold. Dynamic\nvoltage and frequency scaling (DVFS) is an approach frequently used to reduce\nthe energy consumption of a schedule, but slowing down the execution of a task\nto save energy is decreasing the reliability of the execution. In this work, to\nimprove the reliability of a schedule while reducing the energy consumption, we\nallow for the re-execution of some tasks. We assess the complexity of the\ntri-criteria scheduling problem (makespan, reliability, energy) of deciding\nwhich task to re-execute, and at which speed each execution of a task should be\ndone, with two different speed models: either processors can have arbitrary\nspeeds (continuous model), or a processor can run at a finite number of\ndifferent speeds and change its speed during a computation (VDD model). We\npropose several novel tri-criteria scheduling heuristics under the continuous\nspeed model, and we evaluate them through a set of simulations. The two best\nheuristics turn out to be very efficient and complementary.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 15:42:57 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2012 12:06:15 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Aupy", "Guillaume", ""], ["Benoit", "Anne", ""], ["Robert", "Yves", ""]]}, {"id": "1111.5596", "submitter": "Frank Winter", "authors": "Frank Winter", "title": "Accelerating QDP++/Chroma on GPUs", "comments": "7 pages, 4 figures; Talk given at the XXIX International Symposium on\n  Lattice Field Theory - Lattice 2011, Lake Tahoe, California, USA", "journal-ref": null, "doi": null, "report-no": "Edinburgh 2011/35", "categories": "hep-lat cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensions to the C++ implementation of the QCD Data Parallel Interface are\nprovided enabling acceleration of expression evaluation on NVIDIA GPUs. Single\nexpressions are off-loaded to the device memory and execution domain leveraging\nthe Portable Expression Template Engine and using Just-in-Time compilation\ntechniques. Memory management is automated by a software implementation of a\ncache controlling the GPU's memory. Interoperability with existing Krylov space\nsolvers is demonstrated and special attention is paid on 'Chroma readiness'.\nNon-kernel routines in lattice QCD calculations typically not subject of\nhand-tuned optimisations are accelerated which can reduce the effects otherwise\nsuffered from Amdahl's Law.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 19:48:51 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Winter", "Frank", ""]]}, {"id": "1111.5775", "submitter": "Wim H. Hesselink", "authors": "Wim H. Hesselink", "title": "Partial mutual exclusion for infinitely many processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial mutual exclusion is the drinking philosophers problem for complete\ngraphs. It is the problem that a process may enter a critical section CS of its\ncode only when some finite set nbh of other processes are not in their critical\nsections. For each execution of CS, the set nbh can be given by the\nenvironment. We present a starvation free solution of this problem in a setting\nwith infinitely many processes, each with finite memory, that communicate by\nasynchronous messages. The solution has the property of first-come\nfirst-served, in so far as this can be guaranteed by asynchronous messages. For\nevery execution of CS and every process in nbh, between three and six messages\nare needed. The correctness of the solution is argued with invariants and\ntemporal logic. It has been verified with the proof assistant PVS.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2011 14:17:05 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2011 12:41:39 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Hesselink", "Wim H.", ""]]}, {"id": "1111.6087", "submitter": "Carlos Baquero", "authors": "Paulo S\\'ergio Almeida, Carlos Baquero, Alcino Cunha", "title": "Fast Distributed Computation of Distances in Networks", "comments": "12 pages", "journal-ref": "IEEE 51st Annual Conference on Decision and Control (2012),\n  5215-5220", "doi": "10.1109/CDC.2012.6426872", "report-no": null, "categories": "cs.DC cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a distributed algorithm to simultaneously compute the\ndiameter, radius and node eccentricity in all nodes of a synchronous network.\nSuch topological information may be useful as input to configure other\nalgorithms. Previous approaches have been modular, progressing in sequential\nphases using building blocks such as BFS tree construction, thus incurring\nlonger executions than strictly required. We present an algorithm that, by\ntimely propagation of available estimations, achieves a faster convergence to\nthe correct values. We show local criteria for detecting convergence in each\nnode. The algorithm avoids the creation of BFS trees and simply manipulates\nsets of node ids and hop counts. For the worst scenario of variable start\ntimes, each node i with eccentricity ecc(i) can compute: the node eccentricity\nin diam(G)+ecc(i)+2 rounds; the diameter in 2*diam(G)+ecc(i)+2 rounds; and the\nradius in diam(G)+ecc(i)+2*radius(G) rounds.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2011 19:12:07 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Almeida", "Paulo S\u00e9rgio", ""], ["Baquero", "Carlos", ""], ["Cunha", "Alcino", ""]]}, {"id": "1111.6218", "submitter": "Sabu Thampi m", "authors": "Amritha Sampath, Tripti. C, Sabu M. Thampi", "title": "An ACO Algorithm for Effective Cluster Head Selection", "comments": "7 pages, 5 figures, International Journal of Advances in Information\n  Technology (JAIT); ISSN: 1798-2340; Academy Publishers, Finland", "journal-ref": "International Journal of Advances in Information Technology\n  (JAIT), Vol. 2, No. 1, February 2011, pp. 50-56", "doi": "10.4304/jait.2.1.50-56", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an effective algorithm for selecting cluster heads in\nmobile ad hoc networks using ant colony optimization. A cluster in an ad hoc\nnetwork consists of a cluster head and cluster members which are at one hop\naway from the cluster head. The cluster head allocates the resources to its\ncluster members. Clustering in MANET is done to reduce the communication\noverhead and thereby increase the network performance. A MANET can have many\nclusters in it. This paper presents an algorithm which is a combination of the\nfour main clustering schemes- the ID based clustering, connectivity based,\nprobability based and the weighted approach. An Ant colony optimization based\napproach is used to minimize the number of clusters in MANET. This can also be\nconsidered as a minimum dominating set problem in graph theory. The algorithm\nconsiders various parameters like the number of nodes, the transmission range\netc. Experimental results show that the proposed algorithm is an effective\nmethodology for finding out the minimum number of cluster heads.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2011 02:35:36 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Sampath", "Amritha", ""], ["C", "Tripti.", ""], ["Thampi", "Sabu M.", ""]]}, {"id": "1111.6324", "submitter": "Ata Turk", "authors": "Ata Turk, Cevdet Aykanat, G. Vehbi Demirci, Sebastian von Alfthan,\n  Ilja Honkonen", "title": "Improving the Load Balancing Performance of Vlasiator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This whitepaper describes the load-balancing performance issues that are\nobserved and tackled during the petascaling of the Vlasiator codes. Vlasiator\nis a Vlasov-hybrid simulation code developed in Finnish Meteorological\nInstitute (FMI). Vlasiator models the communications associated with the\nspatial grid operated on as a hypergraph and partitions the grid using the\nparallel hypergraph partitioning scheme (PHG) of the Zoltan partitioning\nframework. The result of partitioning determines the distribution of grid cells\nto processors. It is observed that the partitioning phase takes a substantial\npercentage of the overall computation time. Alternative\n(graph-partitioning-based) schemes that perform almost as well as the\nhypergraph partitioning scheme and that require less preprocessing overhead and\nbetter balance are proposed and investigated. A comparison in terms of effect\non running time, preprocessing overhead and load-balancing quality of Zoltan's\nPHG, ParMeTiS, and PT-SCOTCH are presented. Test results on J\\\"uelich\nBlueGene/P cluster are presented.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 00:50:25 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Turk", "Ata", ""], ["Aykanat", "Cevdet", ""], ["Demirci", "G. Vehbi", ""], ["von Alfthan", "Sebastian", ""], ["Honkonen", "Ilja", ""]]}, {"id": "1111.6374", "submitter": "Edoardo Di Napoli", "authors": "Jos\\'e I. Aliaga (1), Paolo Bientinesi (2), Davor Davidovi\\'c (3),\n  Edoardo Di Napoli (4), Francisco D. Igual (1), and Enrique S. Quintana-Ort\\'i\n  (1) ((1) Depto. de Ingenier\\'ia y Ciencia de Computadores, Universidad Jaume\n  I, (2) RWTH-Aachen University, (3) Institut Ruder Boskov\\'ic, Centarza\n  Informatiku i Racunarstvo, (4) JSC, Forschungszentrum J\\\"ulich)", "title": "Solving Dense Generalized Eigenproblems on Multi-threaded Architectures", "comments": "5 tables and 4 figures. In press by Applied Mathematics and\n  Computation. Accepted version", "journal-ref": null, "doi": "10.1016/j.amc.2012.05.020", "report-no": null, "categories": "cs.PF cond-mat.mtrl-sci cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two approaches to compute a portion of the spectrum of dense\nsymmetric definite generalized eigenproblems: one is based on the reduction to\ntridiagonal form, and the other on the Krylov-subspace iteration. Two\nlarge-scale applications, arising in molecular dynamics and material science,\nare employed to investigate the contributions of the application, architecture,\nand parallelism of the method to the performance of the solvers. The\nexperimental results on a state-of-the-art 8-core platform, equipped with a\ngraphics processing unit (GPU), reveal that in real applications, iterative\nKrylov-subspace methods can be a competitive approach also for the solution of\ndense problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 08:52:04 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2012 17:42:58 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Aliaga", "Jos\u00e9 I.", ""], ["Bientinesi", "Paolo", ""], ["Davidovi\u0107", "Davor", ""], ["Di Napoli", "Edoardo", ""], ["Igual", "Francisco D.", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""]]}, {"id": "1111.6583", "submitter": "David Ketcheson", "authors": "David I. Ketcheson, Kyle T. Mandli, Aron Ahmadia, Amal Alghamdi,\n  Manuel Quezada, Matteo Parsani, Matthew G. Knepley, Matthew Emmett", "title": "PyClaw: Accessible, Extensible, Scalable Tools for Wave Propagation\n  Problems", "comments": null, "journal-ref": "SISC 34(4):C210-C231 (2012)", "doi": "10.1137/110856976", "report-no": null, "categories": "math.NA cs.DC cs.MS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of scientific software involves tradeoffs between ease of use,\ngenerality, and performance. We describe the design of a general hyperbolic PDE\nsolver that can be operated with the convenience of MATLAB yet achieves\nefficiency near that of hand-coded Fortran and scales to the largest\nsupercomputers. This is achieved by using Python for most of the code while\nemploying automatically-wrapped Fortran kernels for computationally intensive\nroutines, and using Python bindings to interface with a parallel computing\nlibrary and other numerical packages. The software described here is PyClaw, a\nPython-based structured grid solver for general systems of hyperbolic PDEs\n\\cite{pyclaw}. PyClaw provides a powerful and intuitive interface to the\nalgorithms of the existing Fortran codes Clawpack and SharpClaw, simplifying\ncode development and use while providing massive parallelism and scalable\nsolvers via the PETSc library. The package is further augmented by use of\nPyWENO for generation of efficient high-order weighted essentially\nnon-oscillatory reconstruction code. The simplicity, capability, and\nperformance of this approach are demonstrated through application to example\nproblems in shallow water flow, compressible flow and elasticity.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2011 10:53:39 GMT"}, {"version": "v2", "created": "Sat, 12 May 2012 17:40:07 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Ketcheson", "David I.", ""], ["Mandli", "Kyle T.", ""], ["Ahmadia", "Aron", ""], ["Alghamdi", "Amal", ""], ["Quezada", "Manuel", ""], ["Parsani", "Matteo", ""], ["Knepley", "Matthew G.", ""], ["Emmett", "Matthew", ""]]}, {"id": "1111.6661", "submitter": "Amr Hassan", "authors": "A. H. Hassan, C. J. Fluke, and D. G. Barnes", "title": "Unleashing the Power of Distributed CPU/GPU Architectures: Massive\n  Astronomical Data Analysis and Visualization case study", "comments": "4 Pages, 1 figures, To appear in the proceedings of ADASS XXI, ed.\n  P.Ballester and D.Egret, ASP Conf. Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Upcoming and future astronomy research facilities will systematically\ngenerate terabyte-sized data sets moving astronomy into the Petascale data era.\nWhile such facilities will provide astronomers with unprecedented levels of\naccuracy and coverage, the increases in dataset size and dimensionality will\npose serious computational challenges for many current astronomy data analysis\nand visualization tools. With such data sizes, even simple data analysis tasks\n(e.g. calculating a histogram or computing data minimum/maximum) may not be\nachievable without access to a supercomputing facility.\n  To effectively handle such dataset sizes, which exceed today's single machine\nmemory and processing limits, we present a framework that exploits the\ndistributed power of GPUs and many-core CPUs, with a goal of providing data\nanalysis and visualizing tasks as a service for astronomers. By mixing shared\nand distributed memory architectures, our framework effectively utilizes the\nunderlying hardware infrastructure handling both batched and real-time data\nanalysis and visualization tasks. Offering such functionality as a service in a\n\"software as a service\" manner will reduce the total cost of ownership, provide\nan easy to use tool to the wider astronomical community, and enable a more\noptimized utilization of the underlying hardware infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 01:34:45 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Hassan", "A. H.", ""], ["Fluke", "C. J.", ""], ["Barnes", "D. G.", ""]]}, {"id": "1111.6756", "submitter": "Riyadh Baghdadi", "authors": "Riyadh Baghdadi, Albert Cohen, Cedric Bastoul, Louis-Noel Pouchet and\n  Lawrence Rauchwerger", "title": "The Potential of Synergistic Static, Dynamic and Speculative Loop Nest\n  Optimizations for Automatic Parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in automatic parallelization of loop-centric programs started with\nstatic analysis, then broadened its arsenal to include dynamic\ninspection-execution and speculative execution, the best results involving\nhybrid static-dynamic schemes. Beyond the detection of parallelism in a\nsequential program, scalable parallelization on many-core processors involves\nhard and interesting parallelism adaptation and mapping challenges. These\nchallenges include tailoring data locality to the memory hierarchy, structuring\nindependent tasks hierarchically to exploit multiple levels of parallelism,\ntuning the synchronization grain, balancing the execution load, decoupling the\nexecution into thread-level pipelines, and leveraging heterogeneous hardware\nwith specialized accelerators. The polyhedral framework allows to model,\nconstruct and apply very complex loop nest transformations addressing most of\nthe parallelism adaptation and mapping challenges. But apart from\nhardware-specific, back-end oriented transformations (if-conversion, trace\nscheduling, value prediction), loop nest optimization has essentially ignored\ndynamic and speculative techniques. Research in polyhedral compilation recently\nreached a significant milestone towards the support of dynamic, data-dependent\ncontrol flow. This opens a large avenue for blending dynamic analyses and\nspeculative techniques with advanced loop nest optimizations. Selecting\nreal-world examples from SPEC benchmarks and numerical kernels, we make a case\nfor the design of synergistic static, dynamic and speculative loop\ntransformation techniques. We also sketch the embedding of dynamic information,\nincluding speculative assumptions, in the heart of affine transformation search\nspaces.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 10:40:44 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Baghdadi", "Riyadh", ""], ["Cohen", "Albert", ""], ["Bastoul", "Cedric", ""], ["Pouchet", "Louis-Noel", ""], ["Rauchwerger", "Lawrence", ""]]}, {"id": "1111.7025", "submitter": "Ilche Georgievski", "authors": "Il\\v{c}e Georgievski, Alexander Lazovik and Marco Aiello", "title": "Task Interaction in an HTN Planner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Task Network (HTN) planning uses task decomposition to plan for\nan executable sequence of actions as a solution to a problem. In order to\nreason effectively, an HTN planner needs expressive domain knowledge. For\ninstance, a simplified HTN planning system such as JSHOP2 uses such\nexpressivity and avoids some task interactions due to the increased complexity\nof the planning process. We address the possibility of simplifying the domain\nrepresentation needed for an HTN planner to find good solutions, especially in\nreal-world domains describing home and building automation environments. We\nextend the JSHOP2 planner to reason about task interaction that happens when\ntask's effects are already achieved by other tasks. The planner then prunes\nsome of the redundant searches that can occur due to the planning process's\ninterleaving nature. We evaluate the original and our improved planner on two\nbenchmark domains. We show that our planner behaves better by using simplified\ndomain knowledge and outperforms JSHOP2 in a number of relevant cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 00:31:47 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Georgievski", "Il\u010de", ""], ["Lazovik", "Alexander", ""], ["Aiello", "Marco", ""]]}]