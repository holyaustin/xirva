[{"id": "1309.0052", "submitter": "Kamran Karimi", "authors": "Kamran Karimi, Aleks G. Pamir, M. Haris Afzal", "title": "Accelerating a Cloud-Based Software GNSS Receiver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss ways to reduce the execution time of a software\nGlobal Navigation Satellite System (GNSS) receiver that is meant for offline\noperation in a cloud environment. Client devices record satellite signals they\nreceive, and send them to the cloud, to be processed by this software. The goal\nof this project is for each client request to be processed as fast as possible,\nbut also to increase total system throughput by making sure as many requests as\npossible are processed within a unit of time. The characteristics of our\napplication provided both opportunities and challenges for increasing\nperformance. We describe the speedups we obtained by enabling the software to\nexploit multi-core CPUs and GPGPUs. We mention which techniques worked for us\nand which did not. To increase throughput, we describe how we control the\nresources allocated to each invocation of the software to process a client\nrequest, such that multiple copies of the application can run at the same time.\nWe use the notion of effective running time to measure the system's throughput\nwhen running multiple instances at the same time, and show how we can determine\nwhen the system's computing resources have been saturated.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2013 01:18:14 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2013 05:51:08 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Karimi", "Kamran", ""], ["Pamir", "Aleks G.", ""], ["Afzal", "M. Haris", ""]]}, {"id": "1309.0186", "submitter": "K. V. Rashmi", "authors": "K. V. Rashmi, Nihar B. Shah, Dikang Gu, Hairong Kuang, Dhruba\n  Borthakur, and Kannan Ramchandran", "title": "A Solution to the Network Challenges of Data Recovery in Erasure-coded\n  Distributed Storage Systems: A Study on the Facebook Warehouse Cluster", "comments": "In proceedings of USENIX HotStorage, San Jose, June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erasure codes, such as Reed-Solomon (RS) codes, are being increasingly\nemployed in data centers to combat the cost of reliably storing large amounts\nof data. Although these codes provide optimal storage efficiency, they require\nsignificantly high network and disk usage during recovery of missing data. In\nthis paper, we first present a study on the impact of recovery operations of\nerasure-coded data on the data-center network, based on measurements from\nFacebook's warehouse cluster in production. To the best of our knowledge, this\nis the first study of its kind available in the literature. Our study reveals\nthat recovery of RS-coded data results in a significant increase in network\ntraffic, more than a hundred terabytes per day, in a cluster storing multiple\npetabytes of RS-coded data.\n  To address this issue, we present a new storage code using our recently\nproposed \"Piggybacking\" framework, that reduces the network and disk usage\nduring recovery by 30% in theory, while also being storage optimal and\nsupporting arbitrary design parameters. The implementation of the proposed code\nin the Hadoop Distributed File System (HDFS) is underway. We use the\nmeasurements from the warehouse cluster to show that the proposed code would\nlead to a reduction of close to fifty terabytes of cross-rack traffic per day.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 06:46:41 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Rashmi", "K. V.", ""], ["Shah", "Nihar B.", ""], ["Gu", "Dikang", ""], ["Kuang", "Hairong", ""], ["Borthakur", "Dhruba", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1309.0215", "submitter": "Mian Lu", "authors": "Mian Lu, Lei Zhang, Huynh Phung Huynh, Zhongliang Ong, Yun Liang,\n  Bingsheng He, Rick Siow Mong Goh, Richard Huynh", "title": "Optimizing the MapReduce Framework on Intel Xeon Phi Coprocessor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ease-of-programming, flexibility and yet efficiency, MapReduce has\nbecome one of the most popular frameworks for building big-data applications.\nMapReduce was originally designed for distributed-computing, and has been\nextended to various architectures, e,g, multi-core CPUs, GPUs and FPGAs. In\nthis work, we focus on optimizing the MapReduce framework on Xeon Phi, which is\nthe latest product released by Intel based on the Many Integrated Core\nArchitecture. To the best of our knowledge, this is the first work to optimize\nthe MapReduce framework on the Xeon Phi.\n  In our work, we utilize advanced features of the Xeon Phi to achieve high\nperformance. In order to take advantage of the SIMD vector processing units, we\npropose a vectorization friendly technique for the map phase to assist the\nauto-vectorization as well as develop SIMD hash computation algorithms.\nFurthermore, we utilize MIMD hyper-threading to pipeline the map and reduce to\nimprove the resource utilization. We also eliminate multiple local arrays but\nuse low cost atomic operations on the global array for some applications, which\ncan improve the thread scalability and data locality due to the coherent L2\ncaches. Finally, for a given application, our framework can either\nautomatically detect suitable techniques to apply or provide guideline for\nusers at compilation time. We conduct comprehensive experiments to benchmark\nthe Xeon Phi and compare our optimized MapReduce framework with a\nstate-of-the-art multi-core based MapReduce framework (Phoenix++). By\nevaluating six real-world applications, the experimental results show that our\noptimized framework is 1.2X to 38X faster than Phoenix++ for various\napplications on the Xeon Phi.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 12:39:30 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Lu", "Mian", ""], ["Zhang", "Lei", ""], ["Huynh", "Huynh Phung", ""], ["Ong", "Zhongliang", ""], ["Liang", "Yun", ""], ["He", "Bingsheng", ""], ["Goh", "Rick Siow Mong", ""], ["Huynh", "Richard", ""]]}, {"id": "1309.0363", "submitter": "Florian Meyer", "authors": "Florian Meyer, Ondrej Hlinka, and Franz Hlawatsch", "title": "Sigma Point Belief Propagation", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sigma point (SP) filter, also known as unscented Kalman filter, is an\nattractive alternative to the extended Kalman filter and the particle filter.\nHere, we extend the SP filter to nonsequential Bayesian inference corresponding\nto loopy factor graphs. We propose sigma point belief propagation (SPBP) as a\nlow-complexity approximation of the belief propagation (BP) message passing\nscheme. SPBP achieves approximate marginalizations of posterior distributions\ncorresponding to (generally) loopy factor graphs. It is well suited for\ndecentralized inference because of its low communication requirements. For a\ndecentralized, dynamic sensor localization problem, we demonstrate that SPBP\ncan outperform nonparametric (particle-based) BP while requiring significantly\nless computations and communications.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 10:59:38 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2013 12:42:10 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Meyer", "Florian", ""], ["Hlinka", "Ondrej", ""], ["Hlawatsch", "Franz", ""]]}, {"id": "1309.0392", "submitter": "Philipp Hupp", "authors": "Philipp Hupp", "title": "Hierarchization for the Sparse Grid Combination Technique", "comments": "7 pages, 9 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse grid combination technique provides a framework to solve high\ndimensional numerical problems with standard solvers. Hierarchization is\npreprocessing step facilitating the communication needed for the combination\ntechnique. The derived hierarchization algorithm outperforms the baseline by up\nto 30x and achieves close to 5% of peak performance. It also shows stable\nperformance for the tested data sets of up to 1 GB.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 16:52:04 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Hupp", "Philipp", ""]]}, {"id": "1309.0634", "submitter": "Anastasios Gounaris", "authors": "Georgios Koutsoumpakis, Iakovos Koutsoumpakis and Anastasios Gounaris", "title": "Skew Handling in Aggregate Streaming Queries on GPUs", "comments": "11 pages, 11 Postscript figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the data to be processed by database systems has grown so large\nthat any conventional, centralized technique is inadequate. At the same time,\ngeneral purpose computation on GPU (GPGPU) recently has successfully drawn\nattention from the data management community due to its ability to achieve\nsignificant speed-ups at a small cost. Efficient skew handling is a well-known\nproblem in parallel queries, independently of the execution environment. In\nthis work, we investigate solutions to the problem of load imbalances in\nparallel aggregate queries on GPUs that are caused by skewed data. We present a\ngeneric load-balancing framework along with several instantiations, which we\nexperimentally evaluate. To the best of our knowledge, this is the first\nattempt to present runtime load-balancing techniques for database operations on\nGPUs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 10:25:20 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Koutsoumpakis", "Georgios", ""], ["Koutsoumpakis", "Iakovos", ""], ["Gounaris", "Anastasios", ""]]}, {"id": "1309.0787", "submitter": "Furong Huang", "authors": "Furong Huang, U. N. Niranjan, Mohammad Umar Hakeem, Animashree\n  Anandkumar", "title": "Online Tensor Methods for Learning Latent Variable Models", "comments": "JMLR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an online tensor decomposition based approach for two latent\nvariable modeling problems namely, (1) community detection, in which we learn\nthe latent communities that the social actors in social networks belong to, and\n(2) topic modeling, in which we infer hidden topics of text articles. We\nconsider decomposition of moment tensors using stochastic gradient descent. We\nconduct optimization of multilinear operations in SGD and avoid directly\nforming the tensors, to save computational and storage costs. We present\noptimized algorithm in two platforms. Our GPU-based implementation exploits the\nparallelism of SIMD architectures to allow for maximum speed-up by a careful\noptimization of storage and data transfer, whereas our CPU-based implementation\nuses efficient sparse matrix computations and is suitable for large sparse\ndatasets. For the community detection problem, we demonstrate accuracy and\ncomputational efficiency on Facebook, Yelp and DBLP datasets, and for the topic\nmodeling problem, we also demonstrate good performance on the New York Times\ndataset. We compare our results to the state-of-the-art algorithms such as the\nvariational method, and report a gain of accuracy and a gain of several orders\nof magnitude in the execution time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 19:30:55 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 20:56:08 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2013 01:58:14 GMT"}, {"version": "v4", "created": "Mon, 31 Mar 2014 17:24:07 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2015 04:26:19 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Huang", "Furong", ""], ["Niranjan", "U. N.", ""], ["Hakeem", "Mohammad Umar", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1309.0874", "submitter": "Rachit Agarwal", "authors": "Rachit Agarwal, Matthew Caesar, P. Brighten Godfrey, Ben Y. Zhao", "title": "Shortest Paths in Microseconds", "comments": "Extended version of WOSN'12 paper: new techniques (reduced memory,\n  faster computations), distributed (MapReduce) algorithm, multiple paths\n  between a source-destination pair", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing shortest paths is a fundamental primitive for several social\nnetwork applications including socially-sensitive ranking, location-aware\nsearch, social auctions and social network privacy. Since these applications\ncompute paths in response to a user query, the goal is to minimize latency\nwhile maintaining feasible memory requirements. We present ASAP, a system that\nachieves this goal by exploiting the structure of social networks.\n  ASAP preprocesses a given network to compute and store a partial shortest\npath tree (PSPT) for each node. The PSPTs have the property that for any two\nnodes, each edge along the shortest path is with high probability contained in\nthe PSPT of at least one of the nodes. We show that the structure of social\nnetworks enable the PSPT of each node to be an extremely small fraction of the\nentire network; hence, PSPTs can be stored efficiently and each shortest path\ncan be computed extremely quickly.\n  For a real network with 5 million nodes and 69 million edges, ASAP computes a\nshortest path for most node pairs in less than 49 microseconds per pair. ASAP,\nunlike any previous technique, also computes hundreds of paths (along with\ncorresponding distances) between any node pair in less than 100 microseconds.\nFinally, ASAP admits efficient implementation on distributed programming\nframeworks like MapReduce.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 23:43:10 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Agarwal", "Rachit", ""], ["Caesar", "Matthew", ""], ["Godfrey", "P. Brighten", ""], ["Zhao", "Ben Y.", ""]]}, {"id": "1309.1049", "submitter": "F\\'elix Cuadrado", "authors": "Luis Vaquero, Felix Cuadrado, Dionysios Logothetis and Claudio\n  Martella", "title": "xDGP: A Dynamic Graph Processing System with Adaptive Partitioning", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world systems, such as social networks, rely on mining efficiently\nlarge graphs, with hundreds of millions of vertices and edges. This volume of\ninformation requires partitioning the graph across multiple nodes in a\ndistributed system. This has a deep effect on performance, as traversing edges\ncut between partitions incurs a significant performance penalty due to the cost\nof communication. Thus, several systems in the literature have attempted to\nimprove computational performance by enhancing graph partitioning, but they do\nnot support another characteristic of real-world graphs: graphs are inherently\ndynamic, their topology evolves continuously, and subsequently the optimum\npartitioning also changes over time.\n  In this work, we present the first system that dynamically repartitions\nmassive graphs to adapt to structural changes. The system optimises graph\npartitioning to prevent performance degradation without using data replication.\nThe system adopts an iterative vertex migration algorithm that relies on local\ninformation only, making complex coordination unnecessary. We show how the\nimprovement in graph partitioning reduces execution time by over 50%, while\nadapting the partitioning to a large number of changes to the graph in three\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 14:36:17 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2013 09:01:55 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2013 15:59:50 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Vaquero", "Luis", ""], ["Cuadrado", "Felix", ""], ["Logothetis", "Dionysios", ""], ["Martella", "Claudio", ""]]}, {"id": "1309.1101", "submitter": "Jeremy Cohen", "authors": "Jeremy Cohen, Chris Cantwell, Neil Chue Hong, David Moxey, Malcolm\n  Illingworth, Andrew Turner, John Darlington and Spencer Sherwin", "title": "Simplifying the Development, Use and Sustainability of HPC Software", "comments": "4 page position paper, submission to WSSSPE13 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing software to undertake complex, compute-intensive scientific\nprocesses requires a challenging combination of both specialist domain\nknowledge and software development skills to convert this knowledge into\nefficient code. As computational platforms become increasingly heterogeneous\nand newer types of platform such as Infrastructure-as-a-Service (IaaS) cloud\ncomputing become more widely accepted for HPC computations, scientists require\nmore support from computer scientists and resource providers to develop\nefficient code and make optimal use of the resources available to them. As part\nof the libhpc stage 1 and 2 projects we are developing a framework to provide a\nricher means of job specification and efficient execution of complex scientific\nsoftware on heterogeneous infrastructure. The use of such frameworks has\nimplications for the sustainability of scientific software. In this paper we\nset out our developing understanding of these challenges based on work carried\nout in the libhpc project.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 16:58:22 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Cohen", "Jeremy", ""], ["Cantwell", "Chris", ""], ["Hong", "Neil Chue", ""], ["Moxey", "David", ""], ["Illingworth", "Malcolm", ""], ["Turner", "Andrew", ""], ["Darlington", "John", ""], ["Sherwin", "Spencer", ""]]}, {"id": "1309.1114", "submitter": "Marzia Rivi", "authors": "Marzia Rivi, Claudio Gheller, Tim Dykes, Mel Krokos, Klaus Dolag", "title": "GPU Accelerated Particle Visualization with Splotch", "comments": "25 pages, 9 figures. Astronomy and Computing (2014)", "journal-ref": "Astronomy and Computing 2014, 5: 9-18", "doi": "10.1016/j.ascom.2014.03.001", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Splotch is a rendering algorithm for exploration and visual discovery in\nparticle-based datasets coming from astronomical observations or numerical\nsimulations. The strengths of the approach are production of high quality\nimagery and support for very large-scale datasets through an effective mix of\nthe OpenMP and MPI parallel programming paradigms. This article reports our\nexperiences in re-designing Splotch for exploiting emerging HPC architectures\nnowadays increasingly populated with GPUs. A performance model is introduced\nfor data transfers, computations and memory access, to guide our re-factoring\nof Splotch. A number of parallelization issues are discussed, in particular\nrelating to race conditions and workload balancing, towards achieving optimal\nperformances. Our implementation was accomplished by using the CUDA programming\nparadigm. Our strategy is founded on novel schemes achieving optimized data\norganisation and classification of particles. We deploy a reference simulation\nto present performance results on acceleration gains and scalability. We\nfinally outline our vision for future work developments including possibilities\nfor further optimisations and exploitation of emerging technologies.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 17:36:46 GMT"}, {"version": "v2", "created": "Sun, 23 Mar 2014 18:18:03 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Rivi", "Marzia", ""], ["Gheller", "Claudio", ""], ["Dykes", "Tim", ""], ["Krokos", "Mel", ""], ["Dolag", "Klaus", ""]]}, {"id": "1309.1208", "submitter": "Abdallah Shami Dr.", "authors": "Mohamed Abu Sharkh, Manar Jammal, Abdallah Shami, and Abdelkader Ouda", "title": "Resource Allocation in a Network-Based Cloud Computing Environment:\n  Design Challenges", "comments": "To appear in IEEE Communications Magazine, November 2013", "journal-ref": null, "doi": "10.1109/MCOM.2013.6658651", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is an increasingly popular computing paradigm, now proving a\nnecessity for utility computing services. Each provider offers a unique service\nportfolio with a range of resource configurations. Resource provisioning for\ncloud services in a comprehensive way is crucial to any resource allocation\nmodel. Any model should consider both computational resources and network\nresources to accurately represent and serve practical needs. Another aspect\nthat should be considered while provisioning resources is energy consumption.\nThis aspect is getting more attention from industry and governments parties.\nCalls of support for the green clouds are gaining momentum. With that in mind,\nresource allocation algorithms aim to accomplish the task of scheduling virtual\nmachines on data center servers and then scheduling connection requests on the\nnetwork paths available while complying with the problem constraints. Several\nexternal and internal factors that affect the performance of resource\nallocation models are introduced in this paper. These factors are discussed in\ndetail and research gaps are pointed out. Design challenges are discussed with\nthe aim of providing a reference to be used when designing a comprehensive\nenergy aware resource allocation model for cloud computing data centers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 23:45:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sharkh", "Mohamed Abu", ""], ["Jammal", "Manar", ""], ["Shami", "Abdallah", ""], ["Ouda", "Abdelkader", ""]]}, {"id": "1309.1230", "submitter": "Kerry A. Seitz Jr.", "authors": "Kerry A. Seitz Jr. (1), Alex Kennedy (1), Owen Ransom (2), Bassam A.\n  Younis (2), and John D. Owens (3) ((1) Department of Computer Science,\n  University of California, Davis (2) Department of Civil and Environmental\n  Engineering, University of California, Davis, (3) Department of Electrical\n  and Computer Engineering, University of California, Davis)", "title": "A GPU Implementation for Two-Dimensional Shallow Water Modeling", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a GPU implementation of a two-dimensional shallow\nwater model. Water simulations are useful for modeling floods, river/reservoir\nbehavior, and dam break scenarios. Our GPU implementation shows vast\nperformance improvements over the original Fortran implementation. By taking\nadvantage of the GPU, researchers and engineers will be able to study water\nsystems more efficiently and in greater detail.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 04:20:02 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Seitz", "Kerry A.", "Jr."], ["Kennedy", "Alex", ""], ["Ransom", "Owen", ""], ["Younis", "Bassam A.", ""], ["Owens", "John D.", ""]]}, {"id": "1309.1272", "submitter": "EPTCS", "authors": "Simon Martiel (Universit\\'e Nice Sophia Antipolis), Bruno Martin\n  (Universit\\'e Nice Sophia Antipolis)", "title": "Intrinsic Universality of Causal Graph Dynamics", "comments": "In Proceedings MCU 2013, arXiv:1309.1043", "journal-ref": "EPTCS 128, 2013, pp. 137-149", "doi": "10.4204/EPTCS.128.19", "report-no": null, "categories": "cs.DM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal graph dynamics are transformations over graphs that capture two\nimportant symmetries of physics, namely causality and homogeneity. They can be\nequivalently defined as continuous and translation invariant transformations or\nfunctions induced by a local rule applied simultaneously on every vertex of the\ngraph. Intrinsic universality is the ability of an instance of a model to\nsimulate every other instance of the model while preserving the structure of\nthe computation at every step of the simulation. In this work we present the\nconstruction of a family of intrinsically universal instances of causal graphs\ndynamics, each instance being able to simulate a subset of instances.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 08:08:04 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Martiel", "Simon", "", "Universit\u00e9 Nice Sophia Antipolis"], ["Martin", "Bruno", "", "Universit\u00e9 Nice Sophia Antipolis"]]}, {"id": "1309.1274", "submitter": "EPTCS", "authors": "Dmitry A. Zaitsev (International Humanitarian University, Professor)", "title": "A Small Universal Petri Net", "comments": "In Proceedings MCU 2013, arXiv:1309.1043. the smallest known\n  universal Petri net", "journal-ref": "EPTCS 128, 2013, pp. 190-202", "doi": "10.4204/EPTCS.128.22", "report-no": null, "categories": "cs.FL cs.CC cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A universal deterministic inhibitor Petri net with 14 places, 29 transitions\nand 138 arcs was constructed via simulation of Neary and Woods' weakly\nuniversal Turing machine with 2 states and 4 symbols; the total time complexity\nis exponential in the running time of their weak machine. To simulate the blank\nwords of the weakly universal Turing machine, a couple of dedicated transitions\ninsert their codes when reaching edges of the working zone. To complete a chain\nof a given Petri net encoding to be executed by the universal Petri net, a\ntranslation of a bi-tag system into a Turing machine was constructed. The\nconstructed Petri net is universal in the standard sense; a weaker form of\nuniversality for Petri nets was not introduced in this work.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 08:08:46 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Zaitsev", "Dmitry A.", "", "International Humanitarian University, Professor"]]}, {"id": "1309.1428", "submitter": "Charles Ferenbaugh", "authors": "Charles R. Ferenbaugh", "title": "Experiments in Sustainable Software Practices for Future Architectures", "comments": "4 pages. Submitted to \"Workshop on Sustainable Software for Science:\n  Practice and Experiences,\" November, 2013, Denver, CO, USA", "journal-ref": null, "doi": null, "report-no": "LA-UR-13-26936", "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of rewriting large physics codes at Los Alamos National\nLaboratory to perform well on new architectures such as many-core, GPU, and\nIntel MIC, we have found a number of areas in which sustainable software\npractices can provide significant advantages. We describe several specific\nadvantages of sustainable practices for future architectures, and report on two\nsmall experimental projects at LANL intended to raise awareness of new software\npractices and programming approaches for new architectures.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 18:39:18 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Ferenbaugh", "Charles R.", ""]]}, {"id": "1309.1630", "submitter": "Frederic Suter", "authors": "Henri Casanova and Arnaud Giersch and Arnaud Legrand and Martin\n  Quinson and Fr\\'ed\\'eric Suter", "title": "SimGrid: a Sustained Effort for the Versatile Simulation of Large Scale\n  Distributed Systems", "comments": "4 pages, submission to WSSSPE'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Simgrid, a toolkit for the versatile simulation of\nlarge scale distributed systems, whose development effort has been sustained\nfor the last fifteen years. Over this time period SimGrid has evolved from a\none-laboratory project in the U.S. into a scientific instrument developed by an\ninternational collaboration. The keys to making this evolution possible have\nbeen securing of funding, improving the quality of the software, and increasing\nthe user base. In this paper we describe how we have been able to make advances\non all three fronts, on which we plan to intensify our efforts over the\nupcoming years.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 13:16:20 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Casanova", "Henri", ""], ["Giersch", "Arnaud", ""], ["Legrand", "Arnaud", ""], ["Quinson", "Martin", ""], ["Suter", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1309.1813", "submitter": "Ketan Maheshwari", "authors": "Ketan Maheshwari, David Kelly, Scott J. Krieder, Justin M. Wozniak,\n  Daniel S. Katz, Mei Zhi-Gang, Mainak Mookherjee", "title": "Reusability in Science: From Initial User Engagement to Dissemination of\n  Results", "comments": "5 pages, WSSSPE 2013 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Effective use of parallel and distributed computing in science depends upon\nmultiple interdependent entities and activities that form an ecosystem. Active\nengagement between application users and technology catalysts is a crucial\nactivity that forms an integral part of this ecosystem. Technology catalysts\nplay a crucial role benefiting communities beyond a single user group. An\neffective user-engagement, use and reuse of tools and techniques has a broad\nimpact on software sustainability. From our experience, we sketch a life-cycle\nfor user-engagement activity in scientific computational environment and posit\nthat application level reusability promotes software sustainability. We\ndescribe our experience in engaging two user groups from different scientific\ndomains reusing a common software and configuration on different computational\ninfrastructures.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 03:39:03 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Maheshwari", "Ketan", ""], ["Kelly", "David", ""], ["Krieder", "Scott J.", ""], ["Wozniak", "Justin M.", ""], ["Katz", "Daniel S.", ""], ["Zhi-Gang", "Mei", ""], ["Mookherjee", "Mainak", ""]]}, {"id": "1309.1828", "submitter": "Shel Swenson", "authors": "Shel Swenson, Yogesh Simmhan, Viktor Prasanna, Manish Parashar, Jason\n  Riedy, David Bader and Richard Vuduc", "title": "Sustainable Software Development for Next-Gen Sequencing (NGS)\n  Bioinformatics on Emerging Platforms", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  DNA sequence analysis is fundamental to life science research. The rapid\ndevelopment of next generation sequencing (NGS) technologies, and the richness\nand diversity of applications it makes feasible, have created an enormous gulf\nbetween the potential of this technology and the development of computational\nmethods to realize this potential. Bridging this gap holds possibilities for\nbroad impacts toward multiple grand challenges and offers unprecedented\nopportunities for software innovation and research. We argue that NGS-enabled\napplications need a critical mass of sustainable software to benefit from\nemerging computing platforms' transformative potential. Accumulating the\nnecessary critical mass will require leaders in computational biology,\nbioinformatics, computer science, and computer engineering work together to\nidentify core opportunity areas, critical software infrastructure, and software\nsustainability challenges. Furthermore, due to the quickly changing nature of\nboth bioinformatics software and accelerator technology, we conclude that\ncreating sustainable accelerated bioinformatics software means constructing a\nsustainable bridge between the two fields. In particular, sustained\ncollaboration between domain developers and technology experts is needed to\ndevelop the accelerated kernels, libraries, frameworks and middleware that\ncould provide the needed flexible link from NGS bioinformatics applications to\nemerging platforms.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 06:51:49 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2013 18:22:16 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Swenson", "Shel", ""], ["Simmhan", "Yogesh", ""], ["Prasanna", "Viktor", ""], ["Parashar", "Manish", ""], ["Riedy", "Jason", ""], ["Bader", "David", ""], ["Vuduc", "Richard", ""]]}, {"id": "1309.1889", "submitter": "Jana Paz\\'urikov\\'a", "authors": "Jana Paz\\'urikov\\'a and Lud\\v{e}k Matyska", "title": "Parallel-in-time method for calculation of long-range electrostatic\n  interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large molecular dynamics simulations (millions of atoms, tens of\nmicroseconds, thousands of processors) hit the strong scalability wall:\nsimulation on twice as many processors does not take half the time. Inspired by\nlarge N-body space simulations, we suggest to calculate the bottleneck---the\nlong-range interactions---parallel in time. This technical report aims to\npresent the combination of parareal method and multilevel summation method. We\nthoroughly describe both methods and reasons for their particular combination.\nWe also propose several optimizations that should provide the acceleration by\nan order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 18:04:24 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2013 12:33:08 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Paz\u00farikov\u00e1", "Jana", ""], ["Matyska", "Lud\u011bk", ""]]}, {"id": "1309.1983", "submitter": "Mark Mawson", "authors": "Mark Mawson, Alistair Revell", "title": "Memory transfer optimization for a lattice Boltzmann solver on Kepler\n  architecture nVidia GPUs", "comments": "12 pages", "journal-ref": null, "doi": "10.1016/j.cpc.2014.06.003", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lattice Boltzmann method (LBM) for solving fluid flow is naturally well\nsuited to an efficient implementation for massively parallel computing, due to\nthe prevalence of local operations in the algorithm. This paper presents and\nanalyses the performance of a 3D lattice Boltzmann solver, optimized for third\ngeneration nVidia GPU hardware, also known as `Kepler'. We provide a review of\nprevious optimisation strategies and analyse data read/write times for\ndifferent memory types. In LBM, the time propagation step (known as streaming),\ninvolves shifting data to adjacent locations and is central to parallel\nperformance; here we examine three approaches which make use of different\nhardware options. Two of which make use of `performance enhancing' features of\nthe GPU; shared memory and the new shuffle instruction found in Kepler based\nGPUs. These are compared to a standard transfer of data which relies instead on\noptimised storage to increase coalesced access. It is shown that the more\nsimple approach is most efficient; since the need for large numbers of\nregisters per thread in LBM limits the block size and thus the efficiency of\nthese special features is reduced. Detailed results are obtained for a D3Q19\nLBM solver, which is benchmarked on nVidia K5000M and K20C GPUs. In the latter\ncase the use of a read-only data cache is explored, and peak performance of\nover 1036 Million Lattice Updates Per Second (MLUPS) is achieved. The\nappearance of a periodic bottleneck in the solver performance is also reported,\nbelieved to be hardware related; spikes in iteration-time occur with a\nfrequency of around 11Hz for both GPUs, independent of the size of the problem.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2013 17:37:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Mawson", "Mark", ""], ["Revell", "Alistair", ""]]}, {"id": "1309.2328", "submitter": "Olivier Serres", "authors": "Olivier Serres, Abdullah Kayi, Ahmad Anbar, Tarek El-Ghazawi", "title": "Hardware Support for Address Mapping in PGAS Languages; a UPC Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Partitioned Global Address Space (PGAS) programming model strikes a\nbalance between the locality-aware, but explicit, message-passing model and the\neasy-to-use, but locality-agnostic, shared memory model. However, the PGAS rich\nmemory model comes at a performance cost which can hinder its potential for\nscalability and performance. To contain this overhead and achieve full\nperformance, compiler optimizations may not be sufficient and manual\noptimizations are typically added. This, however, can severely limit the\nproductivity advantage. Such optimizations are usually targeted at reducing\naddress translation overheads for shared data structures. This paper proposes a\nhardware architectural support for PGAS, which allows the processor to\nefficiently handle shared addresses. This eliminates the need for such\nhand-tuning, while maintaining the performance and productivity of PGAS\nlanguages. We propose to avail this hardware support to compilers by\nintroducing new instructions to efficiently access and traverse the PGAS memory\nspace. A prototype compiler is realized by extending the Berkeley Unified\nParallel C (UPC) compiler. It allows unmodified code to use the new\ninstructions without the user intervention, thereby creating a real productive\nprogramming environment. Two implementations are realized: the first is\nimplemented using the full system simulator Gem5, which allows the evaluation\nof the performance gain. The second is implemented using a softcore processor\nLeon3 on an FPGA to verify the implementability and to parameterize the cost of\nthe new hardware and its instructions. The new instructions show promising\nresults for the NAS Parallel Benchmarks implemented in UPC. A speedup of up to\n5.5x is demonstrated for unmodified and unoptimized codes. Unoptimized code\nperformance using this hardware was shown to also surpass the performance of\nmanually optimized code by up to 10%.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 21:20:13 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Serres", "Olivier", ""], ["Kayi", "Abdullah", ""], ["Anbar", "Ahmad", ""], ["El-Ghazawi", "Tarek", ""]]}, {"id": "1309.2334", "submitter": "Saleh Almowuena", "authors": "Saleh Almowuena", "title": "An Efficient Key Agreement Scheme for Wireless Sensor Networks Using\n  Third Parties", "comments": "15 pages, 6 figures", "journal-ref": "International Journal of Ad hoc, Sensor & Ubiquitous Computing\n  (IJASUC) Vol.4, No.4, August 2013", "doi": "10.5121/ijasuc.2013.4401", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the challenging field of security for wireless\nsensor networks by introducing a key agreement scheme in which sensor nodes\ncreate secure radio connections with their neighbours depending on the aid of\nthird parties. These third parties are responsible only for the pair-wise key\nestablishment among sensor nodes, so they do not observe the physical\nphenomenon nor route data packets to other nodes. The proposed method is\nexplained here with respect to four important issues: how secret shares are\ndistributed, how local neighbours are discovered, how legitimate third parties\nare verified, and how secure channels are established. Moreover, the\nperformance of the scheme is analyzed with regards to five metrics: local\nconnectivity, resistance to node capture, memory usage, communication overhead,\nand computational burden. Our scheme not only secures the transmission channels\nof nodes but also guarantees high local connectivity of the sensor network, low\nusage of memory resources, perfect network resilience against node capture, and\ncomplete prevention against impersonation attacks. As it is demonstrated in\nthis paper, using a number of third parties equals to 10% of the total number\nof sensor nodes in the area of interest, the proposed method can achieve at\nleast 99.42% local connectivity with a very low usage of available storage\nresources (less than 385 bits on each sensor node).\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 21:56:20 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Almowuena", "Saleh", ""]]}, {"id": "1309.2357", "submitter": "Anne C. Elster", "authors": "Anne C. Elster", "title": "Software for Science: Some Personal Reflections", "comments": "4-page draft for SC13 workshop WSSSPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  As computer systems become more and more complex, software and tools lag more\nand more behind. This is especially true for scientific software that often\ndemands high performance, and thus needs to take advantage of parallelisms,\nmemory hierarchies and other software and systems. How do we help bridge this\never-increasing gap?\n  This paper describes some of my experiences and thoughts regarding licensing,\ncode sharing, code maintenance, open access publishing, and educations and\ntraining. Details include my recent experiences with getting industrial funding\nfor GPL licensed software, BSD issues, sharing code on GitHub, and how I\ninspire students to take my 4th year Parallel Computing elective which this\nsemester has over 50 students enrolled. Some thoughts and comments regarding\nwhy both optimization and data locality are such a central issue for scientific\nsoftware is also included.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 01:33:59 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Elster", "Anne C.", ""]]}, {"id": "1309.2426", "submitter": "Rashmi Rai", "authors": "Rashmi Rai, G.Sahoo, S.Mehfuz", "title": "Securing Software as a Service Model of Cloud Computing: Issues and\n  Solutions", "comments": null, "journal-ref": "International Journal on Cloud Computing: Services and\n  Architecture (IJCCSA) ,Vol.3, No.4, August 2013", "doi": "10.5121/ijccsa.2013.3401", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing, undoubtedly, has become the buzzword in the IT industry\ntoday. Looking at the potential impact it has on numerous business applications\nas well as in our everyday life, it can certainly be said that this disruptive\ntechnology is here to stay. Many of the features that make cloud computing\nattractive, have not just challenged the existing security system, but have\nalso revealed new security issues. This paper provides an insightful analysis\nof the existing status on cloud computing security issues based on a detailed\nsurvey carried by the author. It also makes an attempt to describe the security\nchallenges in Software as a Service (SaaS) model of cloud computing and also\nendeavors to provide future security research directions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 09:23:16 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Rai", "Rashmi", ""], ["Sahoo", "G.", ""], ["Mehfuz", "S.", ""]]}, {"id": "1309.2444", "submitter": "Marco Guazzone", "authors": "Marco Guazzone, Cosimo Anglano, Matteo Sereno", "title": "A Game-Theoretic Approach to Distributed Coalition Formation in\n  Energy-Aware Cloud Federations (Extended Version)", "comments": "Added publisher info and citation notice", "journal-ref": "2014 14th IEEE/ACM International Symposium on Cluster, Cloud and\n  Grid Computing (CCGrid), 2014, pp. 618-625", "doi": "10.1109/CCGrid.2014.37", "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federations among sets of Cloud Providers (CPs), whereby a set of CPs agree\nto mutually use their own resources to run the VMs of other CPs, are considered\na promising solution to the problem of reducing the energy cost. In this paper,\nwe address the problem of federation formation for a set of CPs, whose solution\nis necessary to exploit the potential of cloud federations for the reduction of\nthe energy bill. We devise a distributed algorithm, based on cooperative game\ntheory, that allows a set of CPs to cooperatively set up their federations in\nsuch a way that their individual profit is increased with respect to the case\nin which they work in isolation, and we show that, by using our algorithm and\nthe proposed CPs' utility function, they are able to self-organize into\nNash-stable federations and, by means of iterated executions, to adapt\nthemselves to environmental changes. Numerical results are presented to\ndemonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 10:25:11 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 08:24:14 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2013 11:35:37 GMT"}, {"version": "v4", "created": "Fri, 15 Nov 2013 14:08:30 GMT"}, {"version": "v5", "created": "Fri, 31 Jan 2014 20:40:29 GMT"}, {"version": "v6", "created": "Mon, 22 Feb 2016 15:41:34 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Guazzone", "Marco", ""], ["Anglano", "Cosimo", ""], ["Sereno", "Matteo", ""]]}, {"id": "1309.2734", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Koki Murano, Tomoyoshi Shimobaba, Atsushi Sugiyama, Naoki Takada,\n  Takashi Kakue, Minoru Oikawa, Tomoyoshi Ito", "title": "Fast computation of computer-generated hologram using Xeon Phi\n  coprocessor", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2014.06.010", "report-no": null, "categories": "physics.comp-ph cs.DC physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report fast computation of computer-generated holograms (CGHs) using Xeon\nPhi coprocessors, which have massively x86-based processors on one chip,\nrecently released by Intel. CGHs can generate arbitrary light wavefronts, and\ntherefore, are promising technology for many applications: for example,\nthree-dimensional displays, diffractive optical elements, and the generation of\narbitrary beams. CGHs incur enormous computational cost. In this paper, we\ndescribe the implementations of several CGH generating algorithms on the Xeon\nPhi, and the comparisons in terms of the performance and the ease of\nprogramming between the Xeon Phi, a CPU and graphics processing unit (GPU).\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 05:43:05 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Murano", "Koki", ""], ["Shimobaba", "Tomoyoshi", ""], ["Sugiyama", "Atsushi", ""], ["Takada", "Naoki", ""], ["Kakue", "Takashi", ""], ["Oikawa", "Minoru", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1309.2772", "submitter": "Pierre Sutra", "authors": "Pierre Sutra, Etienne Rivi\\`ere, Pascal Felber", "title": "A Practical Distributed Universal Construction with Unknown Participants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern distributed systems employ atomic read-modify-write primitives to\ncoordinate concurrent operations. Such primitives are typically built on top of\na central server, or rely on an agreement protocol. Both approaches provide a\nuniversal construction, that is, a general mechanism to construct atomic and\nresponsive objects. These two techniques are however known to be inherently\ncostly. As a consequence, they may result in bottlenecks in applications using\nthem for coordination. In this paper, we investigate another direction to\nimplement a universal construction. Our idea is to delegate the implementation\nof the universal construction to the clients, and solely implement a\ndistributed shared atomic memory at the servers side. The construction we\npropose is obstruction-free. It can be implemented in a purely asynchronous\nmanner, and it does not assume the knowledge of the participants. It is built\non top of grafarius and racing objects, two novel shared abstractions that we\nintroduce in detail. To assess the benefits of our approach, we present a\nprototype implementation on top of the Cassandra data store, and compare it\nempirically to the Zookeeper coordination service.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 09:35:39 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 10:31:53 GMT"}, {"version": "v3", "created": "Wed, 21 May 2014 07:34:42 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Sutra", "Pierre", ""], ["Rivi\u00e8re", "Etienne", ""], ["Felber", "Pascal", ""]]}, {"id": "1309.3139", "submitter": "Steffen Limmer", "authors": "Steffen Limmer, Slawomir Stanczak, Mario Goldenbaum, Renato L. G.\n  Cavalcante", "title": "Exploiting Interference for Efficient Distributed Computation in\n  Cluster-based Wireless Sensor Networks", "comments": "Accepted for publication at IEEE Global Conference on Signal and\n  Information Processing (GlobalSIP 2013)", "journal-ref": null, "doi": "10.1109/GlobalSIP.2013.6737045", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This invited paper presents some novel ideas on how to enhance the\nperformance of consensus algorithms in distributed wireless sensor networks,\nwhen communication costs are considered. Of particular interest are consensus\nalgorithms that exploit the broadcast property of the wireless channel to boost\nthe performance in terms of convergence speeds. To this end, we propose a novel\nclustering based consensus algorithm that exploits interference for\ncomputation, while reducing the energy consumption in the network. The\nresulting optimization problem is a semidefinite program, which can be solved\noffline prior to system startup.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 12:33:09 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 08:18:03 GMT"}, {"version": "v3", "created": "Wed, 3 Sep 2014 15:16:39 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Limmer", "Steffen", ""], ["Stanczak", "Slawomir", ""], ["Goldenbaum", "Mario", ""], ["Cavalcante", "Renato L. G.", ""]]}, {"id": "1309.3150", "submitter": "Michael Borokhovich", "authors": "Michael Borokhovich, Stefan Schmid", "title": "How (Not) to Shoot in Your Foot with SDN Local Fast Failover: A\n  Load-Connectivity Tradeoff", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the resilient routing and (in-band) fast failover\nmechanisms supported in Software-Defined Networks (SDN). We analyze the\npotential benefits and limitations of such failover mechanisms, and focus on\ntwo main metrics: (1) correctness (in terms of connectivity and loop-freeness)\nand (2) load-balancing. We make the following contributions. First, we show\nthat in the worst-case (i.e., under adversarial link failures), the usefulness\nof local failover is rather limited: already a small number of failures will\nviolate connectivity properties under any fast failover policy, even though the\nunderlying substrate network remains highly connected. We then present\nrandomized and deterministic algorithms to compute resilient forwarding sets;\nthese algorithms achieve an almost optimal tradeoff. Our worst-case analysis is\ncomplemented with a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 13:29:00 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 14:46:30 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Borokhovich", "Michael", ""], ["Schmid", "Stefan", ""]]}, {"id": "1309.3151", "submitter": "Federica Garin", "authors": "Federica Garin (INRIA Grenoble Rh\\^one-Alpes / Gipsa-lab, GIPSA-lab),\n  Ye Yuan", "title": "Distributed privacy-preserving network size computation: A\n  system-identification based method", "comments": "52nd IEEE Conference on Decision and Control (CDC 2013) (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose an algorithm for computing the network size of\ncommunicating agents. The algorithm is distributed: a) it does not require a\nleader selection; b) it only requires local exchange of information, and; c)\nits design can be implemented using local information only, without any global\ninformation about the network. It is privacy-preserving, namely it does not\nrequire to propagate identifying labels. This algorithm is based on system\nidentification, and more precisely on the identification of the order of a\nsuitably-constructed discrete-time linear time-invariant system over some\nfinite field. We provide a probabilistic guarantee for any randomly picked node\nto correctly compute the number of nodes in the network. Moreover, numerical\nimplementation has been taken into account to make the algorithm applicable to\nnetworks of hundreds of nodes, and therefore make the algorithm applicable in\nreal-world sensor or robotic networks. We finally illustrate our results in\nsimulation and conclude the paper with discussions on how our technique differs\nfrom a previously-known strategy based on statistical inference.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 14:52:15 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Garin", "Federica", "", "INRIA Grenoble Rh\u00f4ne-Alpes / Gipsa-lab, GIPSA-lab"], ["Yuan", "Ye", ""]]}, {"id": "1309.3187", "submitter": "Roberto As\\'in-Ach\\'a", "authors": "Roberto As\\'in, Juan Olate and Leo Ferres", "title": "Cache Performance Study of Portfolio-Based Parallel CDCL SAT Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel SAT solvers are becoming mainstream. Their performance has made them\nwin the past two SAT competitions consecutively and are in the limelight of\nresearch and industry. The problem is that it is not known exactly what is\nneeded to make them perform even better; that is, how to make them solve more\nproblems in less time. Also, it is also not know how well they scale in massive\nmulti-core environments which, predictably, is the scenario of comming new\nhardware. In this paper we show that cache contention is a main culprit of a\nslowing down in scalability, and provide empirical results that for some type\nof searches, physically sharing the clause Database between threads is\nbeneficial.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 15:12:07 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["As\u00edn", "Roberto", ""], ["Olate", "Juan", ""], ["Ferres", "Leo", ""]]}, {"id": "1309.3200", "submitter": "Paolo Di Lorenzo", "authors": "Paolo Di Lorenzo, Sergio Barbarossa", "title": "Distributed Estimation and Control of Algebraic Connectivity over Random\n  Graphs", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": "IEEE Taransactions on Signal Processing vol. 62, no. 21, pp.\n  5615-5628, Nov. 2014", "doi": "10.1109/TSP.2014.2355778", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a distributed algorithm for the estimation and\ncontrol of the connectivity of ad-hoc networks in the presence of a random\ntopology. First, given a generic random graph, we introduce a novel stochastic\npower iteration method that allows each node to estimate and track the\nalgebraic connectivity of the underlying expected graph. Using results from\nstochastic approximation theory, we prove that the proposed method converges\nalmost surely (a.s.) to the desired value of connectivity even in the presence\nof imperfect communication scenarios. The estimation strategy is then used as a\nbasic tool to adapt the power transmitted by each node of a wireless network,\nin order to maximize the network connectivity in the presence of realistic\nMedium Access Control (MAC) protocols or simply to drive the connectivity\ntoward a desired target value. Numerical results corroborate our theoretical\nfindings, thus illustrating the main features of the algorithm and its\nrobustness to fluctuations of the network graph due to the presence of random\nlink failures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 15:43:32 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2014 09:37:10 GMT"}, {"version": "v3", "created": "Tue, 8 Apr 2014 10:11:35 GMT"}, {"version": "v4", "created": "Wed, 3 Sep 2014 09:12:52 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Di Lorenzo", "Paolo", ""], ["Barbarossa", "Sergio", ""]]}, {"id": "1309.3324", "submitter": "Peter Alvaro", "authors": "Peter Alvaro, Neil Conway, Joseph M. Hellerstein, David Maier", "title": "Blazes: Coordination Analysis for Distributed Programs", "comments": "Updated to include additional materials from the original technical\n  report: derivation rules, output stream labels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed consistency is perhaps the most discussed topic in distributed\nsystems today. Coordination protocols can ensure consistency, but in practice\nthey cause undesirable performance unless used judiciously. Scalable\ndistributed architectures avoid coordination whenever possible, but\nunder-coordinated systems can exhibit behavioral anomalies under fault, which\nare often extremely difficult to debug. This raises significant challenges for\ndistributed system architects and developers. In this paper we present Blazes,\na cross-platform program analysis framework that (a) identifies program\nlocations that require coordination to ensure consistent executions, and (b)\nautomatically synthesizes application-specific coordination code that can\nsignificantly outperform general-purpose techniques. We present two case\nstudies, one using annotated programs in the Twitter Storm system, and another\nusing the Bloom declarative language.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 22:37:10 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 19:29:29 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Alvaro", "Peter", ""], ["Conway", "Neil", ""], ["Hellerstein", "Joseph M.", ""], ["Maier", "David", ""]]}, {"id": "1309.3458", "submitter": "Gabriele D'Angelo", "authors": "Moreno Marzolla, Gabriele D'Angelo, Marco Mandrioli", "title": "A Parallel Data Distribution Management Algorithm", "comments": "In proc. of the IEEE/ACM International Symposium on Distributed\n  Simulation and Real Time Applications (DS-RT 2013), oct 30-nov 1, 2013,\n  Delft, the Netherlands", "journal-ref": null, "doi": "10.1109/DS-RT.2013.23", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying intersections among a set of d-dimensional rectangular regions\n(d-rectangles) is a common problem in many simulation and modeling\napplications. Since algorithms for computing intersections over a large number\nof regions can be computationally demanding, an obvious solution is to take\nadvantage of the multiprocessing capabilities of modern multicore processors.\nUnfortunately, many solutions employed for the Data Distribution Management\nservice of the High Level Architecture are either inefficient, or can only\npartially be parallelized. In this paper we propose the Interval Tree Matching\n(ITM) algorithm for computing intersections among d-rectangles. ITM is based on\na simple Interval Tree data structure, and exhibits an embarrassingly parallel\nstructure. We implement the ITM algorithm, and compare its sequential\nperformance with two widely used solutions (brute force and sort-based\nmatching). We also analyze the scalability of ITM on shared-memory multicore\nprocessors. The results show that the sequential implementation of ITM is\ncompetitive with sort-based matching; moreover, the parallel implementation\nprovides good speedup on multicore processors.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 14:02:28 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 07:52:07 GMT"}, {"version": "v3", "created": "Tue, 17 May 2016 12:25:56 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Marzolla", "Moreno", ""], ["D'Angelo", "Gabriele", ""], ["Mandrioli", "Marco", ""]]}, {"id": "1309.3541", "submitter": "David Bruhwiler", "authors": "Jean-Luc Vay, Cameron G. R. Geddes, Alice Koniges, Alex Friedman,\n  David P. Grote, David L. Bruhwiler, John P. Verboncoeur", "title": "White Paper on DOE-HEP Accelerator Modeling Science Activities", "comments": "4 pages, 0 figures, originally presented at the US Department of\n  Energy workshop: Community Summer Study", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.acc-ph cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Toward the goal of maximizing the impact of computer modeling on the design\nof future particle accelerators and the development of new accelerator\ntechniques & technologies, this white paper presents the rationale for: (a)\nstrengthening and expanding programmatic activities in accelerator modeling\nscience within the Department of Energy (DOE) Office of High Energy Physics\n(HEP) and (b) increasing the community-wide coordination and integration of\ncode development.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 19:08:15 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 04:37:26 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Vay", "Jean-Luc", ""], ["Geddes", "Cameron G. R.", ""], ["Koniges", "Alice", ""], ["Friedman", "Alex", ""], ["Grote", "David P.", ""], ["Bruhwiler", "David L.", ""], ["Verboncoeur", "John P.", ""]]}, {"id": "1309.3783", "submitter": "Pedro Gonnet", "authors": "Pedro Gonnet, Matthieu Schaller, Tom Theuns, Aidan B. G. Chalk", "title": "SWIFT: Fast algorithms for multi-resolution SPH on multi-core\n  architectures", "comments": "8th International SPHERIC Workshop, Trondheim, Norway, June 3--6,\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach to neighbour-finding in Smoothed\nParticle Hydrodynamics (SPH) simulations with large dynamic range in smoothing\nlength. This approach is based on hierarchical cell decompositions, sorted\ninteractions, and a task-based formulation. It is shown to be faster than\ntraditional tree-based codes, and to scale better than domain\ndecomposition-based approaches on shared-memory parallel architectures such as\nmulti-cores.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2013 17:53:42 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Gonnet", "Pedro", ""], ["Schaller", "Matthieu", ""], ["Theuns", "Tom", ""], ["Chalk", "Aidan B. G.", ""]]}, {"id": "1309.3830", "submitter": "Haiyang Qian", "authors": "Haiyang Qian, Fu Li, Ravishankar Ravindran, Deep Medhi", "title": "Energy-Aware Aggregation of Dynamic Temporal Workload in Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data center providers seek to minimize their total cost of ownership (TCO),\nwhile power consumption has become a social concern. We present formulations to\nminimize server energy consumption and server cost under three different data\ncenter server setups (homogeneous, heterogeneous, and hybrid hetero-homogeneous\nclusters) with dynamic temporal workload. Our studies show that the homogeneous\nmodel significantly differs from the heterogeneous model in computational time\n(by an order of magnitude). To be able to compute optimal configurations in\nnear real-time for large scale data centers, we propose two modes, aggregation\nby maximum and aggregation by mean. In addition, we propose two aggregation\nmethods, static (periodic) aggregation and dynamic (aperiodic) aggregation. We\nfound that in the aggregation by maximum mode, the dynamic aggregation resulted\nin cost savings of up to approximately 18% over the static aggregation. In the\naggregation by mean mode, the dynamic aggregation by mean could save up to\napproximately 50% workload rearrangement compared to the static aggregation by\nmean mode. Overall, our methodology helps to understand the trade-off in\nenergy-aware aggregation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 05:13:32 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Qian", "Haiyang", ""], ["Li", "Fu", ""], ["Ravindran", "Ravishankar", ""], ["Medhi", "Deep", ""]]}, {"id": "1309.4349", "submitter": "Mateusz  Lis", "authors": "M. Lis, L. Pintal", "title": "Report: GPU Based Massive Parallel Kawasaki Kinetics In Monte Carlo\n  Modelling of Lipid Microdomains", "comments": null, "journal-ref": null, "doi": null, "report-no": "PRE 5/2012", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper introduces novel method of simulation of lipid biomembranes based\non Metropolis Hastings algorithm and Graphic Processing Unit computational\npower. Method gives up to 55 times computational boost in comparison to\nclassical computations. Extensive study of algorithm correctness is provided.\nAnalysis of simulation results and results obtained with classical simulation\nmethodologies are presented.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 15:29:17 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Lis", "M.", ""], ["Pintal", "L.", ""]]}, {"id": "1309.4507", "submitter": "Oleg Mazonka", "authors": "Vlad Popov and Oleg Mazonka", "title": "Faster Fair Solution for the Reader-Writer Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast fair solution for Reader-Writer Problem is presented.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 00:08:18 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Popov", "Vlad", ""], ["Mazonka", "Oleg", ""]]}, {"id": "1309.4887", "submitter": "Tilo Wettig", "authors": "Nils Meyer, Manfred Ries, Stefan Solbrig, Tilo Wettig", "title": "iDataCool: HPC with Hot-Water Cooling and Energy Reuse", "comments": "12 pages, 7 figures, proceedings of ISC 2013", "journal-ref": "Lecture Notes in Computer Science 7905 (2013) 383", "doi": "10.1007/978-3-642-38750-0_29", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  iDataCool is an HPC architecture jointly developed by the University of\nRegensburg and the IBM Research and Development Lab B\\\"oblingen. It is based on\nIBM's iDataPlex platform, whose air-cooling solution was replaced by a custom\nwater-cooling solution that allows for cooling water temperatures of 70C/158F.\nThe system is coupled to an adsorption chiller by InvenSor that operates\nefficiently at these temperatures. Thus a significant portion of the energy\nspent on HPC can be recovered in the form of chilled water, which can then be\nused to cool other parts of the computing center. We describe the architecture\nof iDataCool and present benchmarks of the cooling performance and the energy\n(reuse) efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 07:53:10 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Meyer", "Nils", ""], ["Ries", "Manfred", ""], ["Solbrig", "Stefan", ""], ["Wettig", "Tilo", ""]]}, {"id": "1309.5150", "submitter": "EPTCS", "authors": "Benedikt Nordhoff (Westf\\\"alische Wilhelms-Universit\\\"at M\\\"unster,\n  Germany), Markus M\\\"uller-Olm (Westf\\\"alische Wilhelms-Universit\\\"at\n  M\\\"unster, Germany), Peter Lammich (Technische Universit\\\"at M\\\"unchen,\n  Germany)", "title": "Iterable Forward Reachability Analysis of Monitor-DPNs", "comments": "In Proceedings Festschrift for Dave Schmidt, arXiv:1309.4557", "journal-ref": "EPTCS 129, 2013, pp. 384-403", "doi": "10.4204/EPTCS.129.24", "report-no": null, "categories": "cs.LO cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a close connection between data-flow analysis and model checking as\nobserved and studied in the nineties by Steffen and Schmidt. This indicates\nthat automata-based analysis techniques developed in the realm of\ninfinite-state model checking can be applied as data-flow analyzers that\ninterpret complex control structures, which motivates the development of such\nanalysis techniques for ever more complex models. One approach proposed by\nEsparza and Knoop is based on computation of predecessor or successor sets for\nsets of automata configurations. Our goal is to adapt and exploit this approach\nfor analysis of multi-threaded Java programs. Specifically, we consider the\nmodel of Monitor-DPNs for concurrent programs. Monitor-DPNs precisely model\nunbounded recursion, dynamic thread creation, and synchronization via\nwell-nested locks with finite abstractions of procedure- and thread-local\nstate. Previous work on this model showed how to compute regular predecessor\nsets of regular configurations and tree-regular successor sets of a fixed\ninitial configuration. By combining and extending different previously\ndeveloped techniques we show how to compute tree-regular successor sets of\ntree-regular sets. Thereby we obtain an iterable, lock-sensitive forward\nreachability analysis. We implemented the analysis for Java programs and\napplied it to information flow control and data race detection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 01:46:11 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Nordhoff", "Benedikt", "", "Westf\u00e4lische Wilhelms-Universit\u00e4t M\u00fcnster,\n  Germany"], ["M\u00fcller-Olm", "Markus", "", "Westf\u00e4lische Wilhelms-Universit\u00e4t\n  M\u00fcnster, Germany"], ["Lammich", "Peter", "", "Technische Universit\u00e4t M\u00fcnchen,\n  Germany"]]}, {"id": "1309.5301", "submitter": "Zhiyu Liu", "authors": "Maurice Herlihy and Zhiyu Liu", "title": "Well-Structured Futures and Cache Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fork-join parallelism, a sequential program is split into a directed\nacyclic graph of tasks linked by directed dependency edges, and the tasks are\nexecuted, possibly in parallel, in an order consistent with their dependencies.\nA popular and effective way to extend fork-join parallelism is to allow threads\nto create futures. A thread creates a future to hold the results of a\ncomputation, which may or may not be executed in parallel. That result is\nreturned when some thread touches that future, blocking if necessary until the\nresult is ready.\n  Recent research has shown that while futures can, of course, enhance\nparallelism in a structured way, they can have a deleterious effect on cache\nlocality. In the worst case, futures can incur $\\Omega(P T_\\infty + t\nT_\\infty)$ deviations, which implies $\\Omega(C P T_\\infty + C t T_\\infty)$\nadditional cache misses, where $C$ is the number of cache lines, $P$ is the\nnumber of processors, $t$ is the number of touches, and $T_\\infty$ is the\n\\emph{computation span}. Since cache locality has a large impact on software\nperformance on modern multicores, this result is troubling.\n  In this paper, however, we show that if futures are used in a simple,\ndisciplined way, then the situation is much better: if each future is touched\nonly once, either by the thread that created it, or by a thread to which the\nfuture has been passed from the thread that created it, then parallel\nexecutions with work stealing can incur at most $O(C P T^2_\\infty)$ additional\ncache misses, a substantial improvement. This structured use of futures is\ncharacteristic of many (but not all) parallel applications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 15:30:14 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 19:43:08 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 04:07:09 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Herlihy", "Maurice", ""], ["Liu", "Zhiyu", ""]]}, {"id": "1309.5442", "submitter": "Josef Spillner", "authors": "Josef Spillner and Andrii Chaichenko and Andrey Brito and Francisco\n  Brasileiro and Alexander Schill", "title": "Operating the Cloud from Inside Out", "comments": "5 pages, 9 figures, 1 table. Presented at the HPI Cloud Symposium,\n  Potsdam, Germany, September 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual machine images and instances (VMs) in cloud computing centres are\ntypically designed as isolation containers for applications, databases and\nnetworking functions. In order to build complex distributed applications,\nmultiple virtual machines must be connected, orchestrated and combined with\nplatform and infrastructure services from the hosting environment. There are\nseveral reasons why sometimes it is beneficial to introduce a new layer,\nCloud-in-a-VM, which acts as a portable management interface to a cluster of\nVMs. We reason about the benefits and present our Cloud-in-a-VM implementation\ncalled Nested Cloud which allows consumers to become light-weight cloud\noperators on demand and reap multiple advantages, including fully utilised\nresource allocations. The practical usefulness and the performance of the\nintermediate cloud stack VM are evaluated in a marketplace scenario.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 06:43:06 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Spillner", "Josef", ""], ["Chaichenko", "Andrii", ""], ["Brito", "Andrey", ""], ["Brasileiro", "Francisco", ""], ["Schill", "Alexander", ""]]}, {"id": "1309.5457", "submitter": "Sweta Gajbhiye Ms", "authors": "Shweta Gajbhiye, S. P. Karmore", "title": "Cable Fault Monitoring and Indication: A Review", "comments": "05 pages, 05 figures, IJCSN Journal, Vol 2, Issue 4, August 2013\n  IJCSN - International Journal of Computer Science and Network (August 2013)", "journal-ref": null, "doi": null, "report-no": "IJCSN-2013-2-4-25", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underground cable power transmission and distribution system are susceptible\nto faults. Accurate fault location for transmission lines is of vital\nimportance. A quick detection and analysis of faults is necessity of power\nretailers and distributors. This paper reviews various fault locating methods\nand highly computational methods proposed by research community that are\ncurrently in use. The paper also presents some guidelines for design of fault\nlocation and remote indication, for reducing power outages and reducing heavy\nloss of revenue.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 10:01:15 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Gajbhiye", "Shweta", ""], ["Karmore", "S. P.", ""]]}, {"id": "1309.5478", "submitter": "Roshan D'Souza", "authors": "Ivan Komarov, Ali Dashti, Roshan D'Souza", "title": "Fast $k$-NNG construction with GPU-based quick multi-select", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0092409", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we describe a new brute force algorithm for building the\n$k$-Nearest Neighbor Graph ($k$-NNG). The $k$-NNG algorithm has many\napplications in areas such as machine learning, bio-informatics, and clustering\nanalysis. While there are very efficient algorithms for data of low dimensions,\nfor high dimensional data the brute force search is the best algorithm. There\nare two main parts to the algorithm: the first part is finding the distances\nbetween the input vectors which may be formulated as a matrix multiplication\nproblem. The second is the selection of the $k$-NNs for each of the query\nvectors. For the second part, we describe a novel graphics processing unit\n(GPU) -based multi-select algorithm based on quick sort. Our optimization makes\nclever use of warp voting functions available on the latest GPUs along with\nuse-controlled cache. Benchmarks show significant improvement over\nstate-of-the-art implementations of the $k$-NN search on GPUs.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 14:00:32 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Komarov", "Ivan", ""], ["Dashti", "Ali", ""], ["D'Souza", "Roshan", ""]]}, {"id": "1309.5522", "submitter": "Wojciech Golab", "authors": "Wojciech Golab and Jeremy Hurwitz and Xiaozhou (Steve) Li", "title": "On the k-Atomicity-Verification Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Internet-scale storage systems often provide weak consistency in\nexchange for better performance and resilience. An important weak consistency\nproperty is k-atomicity, which bounds the staleness of values returned by read\noperations. The k-atomicity-verification problem (or k-AV for short) is the\nproblem of deciding whether a given history of operations is k-atomic. The 1-AV\nproblem is equivalent to verifying atomicity/linearizability, a well-known and\nsolved problem. However, for k > 2, no polynomial-time k-AV algorithm is known.\n  This paper makes the following contributions towards solving the k-AV\nproblem. First, we present a simple 2- AV algorithm called LBT, which is likely\nto be efficient (quasilinear) for histories that arise in practice, although it\nis less efficient (quadratic) in the worst case. Second, we present a more\ninvolved 2-AV algorithm called FZF, which runs efficiently (quasilinear) even\nin the worst case. To our knowledge, these are the first algorithms that solve\nthe 2-AV problem fully. Third, we show that the weighted k-AV problem, a\nnatural extension of the k-AV problem, is NP-complete.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 19:32:53 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Golab", "Wojciech", "", "Steve"], ["Hurwitz", "Jeremy", "", "Steve"], ["Xiaozhou", "", "", "Steve"], ["Li", "", ""]]}, {"id": "1309.5671", "submitter": "Nicolas Schiper", "authors": "Robbert Van Renesse, Nicolas Schiper, and Fred B. Schneider", "title": "Vive la Diff\\'erence: Paxos vs. Viewstamped Replication vs. Zab", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paxos, Viewstamped Replication, and Zab are replication protocols that ensure\nhigh-availability in asynchronous environments with crash failures. Various\nclaims have been made about similarities and differences between these\nprotocols. But how does one determine whether two protocols are the same, and\nif not, how significant the differences are?\n  We propose to address these questions using refinement mappings, where\nprotocols are expressed as succinct specifications that are progressively\nrefined to executable implementations. Doing so enables a principled\nunderstanding of the correctness of the different design decisions that went\ninto implementing the various protocols. Additionally, it allowed us to\nidentify key differences that have a significant impact on performance.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 00:31:36 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 20:08:06 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2014 23:40:26 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Van Renesse", "Robbert", ""], ["Schiper", "Nicolas", ""], ["Schneider", "Fred B.", ""]]}, {"id": "1309.5803", "submitter": "Henrik Ohlsson", "authors": "Henrik Ohlsson, Tianshi Chen, Sina Khoshfetrat Pakazad, Lennart Ljung\n  and S. Shankar Sastry", "title": "Scalable Anomaly Detection in Large Homogenous Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in large populations is a challenging but highly relevant\nproblem. The problem is essentially a multi-hypothesis problem, with a\nhypothesis for every division of the systems into normal and anomal systems.\nThe number of hypothesis grows rapidly with the number of systems and\napproximate solutions become a necessity for any problems of practical\ninterests. In the current paper we take an optimization approach to this\nmulti-hypothesis problem. We first observe that the problem is equivalent to a\nnon-convex combinatorial optimization problem. We then relax the problem to a\nconvex problem that can be solved distributively on the systems and that stays\ncomputationally tractable as the number of systems increase. An interesting\nproperty of the proposed method is that it can under certain conditions be\nshown to give exactly the same result as the combinatorial multi-hypothesis\nproblem and the relaxation is hence tight.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 14:38:01 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Ohlsson", "Henrik", ""], ["Chen", "Tianshi", ""], ["Pakazad", "Sina Khoshfetrat", ""], ["Ljung", "Lennart", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1309.5826", "submitter": "Stefan Schmid", "authors": "Chen Avin, Omer Dunay and Stefan Schmid", "title": "Simple Destination-Swap Strategies for Adaptive Intra- and Inter-Tenant\n  VM Migration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the opportunities and limitations of adaptive virtual\nmachine (VM) migration to reduce communication costs in a virtualized\nenvironment. We introduce a new formal model for the problem of online VM\nmigration in two scenarios: (1) VMs can be migrated arbitrarily in the\nsubstrate network; e.g., a private cloud provider may have an incentive to\nreduce the overall communication cost in the network. (2) VMs can only be\nmigrated within a given tenant; e.g., a user that was assigned a set of\nphysical machines may exchange the functionality of the VMs on these machines.\n  We propose a simple class of Destination-Swap algorithms which are based on\nan aggressive collocation strategy (inspired by splay datastructures) and which\nmaintain a minimal and local amount of per-node (amortized cost) information to\ndecide where to migrate a VM and how; thus, the algorithms react quickly to\nchanges in the load. The algorithms come in two main flavors, an indirect and\ndistributed variant which keeps existing VM placements local, and a direct\nvariant which keeps the number of affected VMs small.\n  We show that naturally, inter-tenant optimizations yield a larger potential\nfor optimization, but generally also a tenant itself can improve its embedding.\nMoreover, there exists an interesting tradeoff between direct and indirect\nstrategies: indirect variants are preferable under skewed and sparse\ncommunication patterns due to their locality properties.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 14:43:57 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Avin", "Chen", ""], ["Dunay", "Omer", ""], ["Schmid", "Stefan", ""]]}, {"id": "1309.5885", "submitter": "Peter Richtarik", "authors": "Olivier Fercoq and Peter Richt\\'arik", "title": "Smooth minimization of nonsmooth functions with parallel coordinate\n  descent methods", "comments": "39 pages, 1 algorithm, 3 figures, 2 tables", "journal-ref": null, "doi": "10.1007/978-3-030-12119-8_4", "report-no": null, "categories": "cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of a family of randomized parallel coordinate\ndescent methods for minimizing the sum of a nonsmooth and separable convex\nfunctions. The problem class includes as a special case L1-regularized L1\nregression and the minimization of the exponential loss (\"AdaBoost problem\").\nWe assume the input data defining the loss function is contained in a sparse\n$m\\times n$ matrix $A$ with at most $\\omega$ nonzeros in each row. Our methods\nneed $O(n \\beta/\\tau)$ iterations to find an approximate solution with high\nprobability, where $\\tau$ is the number of processors and $\\beta = 1 +\n(\\omega-1)(\\tau-1)/(n-1)$ for the fastest variant. The notation hides\ndependence on quantities such as the required accuracy and confidence levels\nand the distance of the starting iterate from an optimal point. Since\n$\\beta/\\tau$ is a decreasing function of $\\tau$, the method needs fewer\niterations when more processors are used. Certain variants of our algorithms\nperform on average only $O(\\nnz(A)/n)$ arithmetic operations during a single\niteration per processor and, because $\\beta$ decreases when $\\omega$ does,\nfewer iterations are needed for sparser problems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 17:24:00 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Fercoq", "Olivier", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1309.6109", "submitter": "\\'Alvaro L\\'opez Garc\\'ia", "authors": "\\'Alvaro L\\'opez Garc\\'ia, Enol Fern\\'andez del Castillo", "title": "Analysis of Scientific Cloud Computing requirements", "comments": null, "journal-ref": "7th Iberian Grid Infrastructure Conference (IBERGRID 2013)\n  proceedings, pp 147-158", "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the requirements of enterprise and web applications have driven the\ndevelopment of Cloud computing, some of its key features, such as customized\nenvironments and rapid elasticity, could also benefit scientific applications.\nHowever, neither virtualization techniques nor Cloud-like access to resources\nis common in scientific computing centers due to the negative perception of the\nimpact that virtualization techniques introduce.\n  In this paper we discuss the feasibility of the IaaS cloud model to satisfy\nsome of the computational science requirements and the main drawbacks that need\nto be addressed by cloud resource providers so that the maximum benefit can be\nobtained from a given cloud infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 10:54:59 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 12:55:31 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Garc\u00eda", "\u00c1lvaro L\u00f3pez", ""], ["del Castillo", "Enol Fern\u00e1ndez", ""]]}, {"id": "1309.6132", "submitter": "Naftaly Minsky", "authors": "Naftaly Minsky", "title": "An Approach to Modularization of Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularization is an important architectural principle underlying many types\nof complex systems. It tends to tame the complexity of systems, to facilitate\ntheir management, and to enhance their flexibility with respect to evolution.\nIn software, modularization has been practiced and studied thoroughly in local,\ni.e. non-distributed systems. But very little attention has been paid so far to\nmodularization in distributed systems. This is, in part, because distributed\nsystems are inherently modularized, in the sense that the internals of each\ncomponent of such a system is inaccessible to other components, thus satisfying\nthe Parnas hiding principle. It is, however, the thesis of this paper that\nthere is much to be gained by being able to treat groups of distributed\ncomponents as modules, called here distributed modules. And that besides the\nconventional hiding principle, distributed modularization should provide\nadditional capabilities, which rarely, if ever, figure in conventional\nmodularized systems. These capabilities include, but are not limited to: the\nability to impose constraints on which kind of messages can be sent from a\ngiven distributed-module to its outside; and the ability to create AOP-like\ncrosscutting modules. This paper introduces a model of modular distributed\nsystem, orMDS, which satisfies such capabilities, and which is implemented via\nthe LGI middleware.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 12:46:24 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Minsky", "Naftaly", ""]]}, {"id": "1309.6452", "submitter": "Adam Barker", "authors": "Michael Luckeneder and Adam Barker", "title": "Location, Location, Location: Data-Intensive Distributed Computing in\n  the Cloud", "comments": "Preprint of paper to appear in IEEE CloudCom 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When orchestrating highly distributed and data-intensive Web service\nworkflows the geographical placement of the orchestration engine can greatly\naffect the overall performance of a workflow. Orchestration engines are\ntypically run from within an organisations' network, and may have to transfer\ndata across long geographical distances, which in turn increases execution time\nand degrades the overall performance of a workflow. In this paper we present\nCloudForecast: a Web service framework and analysis tool which given a workflow\nspecification, computes the optimal Amazon EC2 Cloud region to automatically\ndeploy the orchestration engine and execute the workflow. We use geographical\ndistance of the workflow, network latency and HTTP round-trip time between\nAmazon Cloud regions and the workflow nodes to find a ranking of Cloud regions.\nThis combined set of simple metrics effectively predicts where the workflow\norchestration engine should be deployed in order to reduce overall execution\ntime.\n  We evaluate our approach by executing randomly generated data-intensive\nworkflows deployed on the PlanetLab platform in order to rank Amazon EC2 Cloud\nregions. Our experimental results show that our proposed optimisation strategy,\ndepending on the particular workflow, can speed up execution time on average by\n82.25% compared to local execution. We also show that the standard deviation of\nexecution time is reduced by an average of almost 65% using the optimisation\nstrategy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 10:01:27 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 17:56:49 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Luckeneder", "Michael", ""], ["Barker", "Adam", ""]]}, {"id": "1309.6603", "submitter": "Quentin Bramas", "authors": "Quentin Bramas (LIP6), S\\'ebastien Tixeuil (LIP6, LINCS, IUF)", "title": "The Random Bit Complexity of Mobile Robots Scattering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scattering $n$ robots in a two dimensional\ncontinuous space. As this problem is impossible to solve in a deterministic\nmanner, all solutions must be probabilistic. We investigate the amount of\nrandomness (that is, the number of random bits used by the robots) that is\nrequired to achieve scattering. We first prove that $n \\log n$ random bits are\nnecessary to scatter $n$ robots in any setting. Also, we give a sufficient\ncondition for a scattering algorithm to be random bit optimal. As it turns out\nthat previous solutions for scattering satisfy our condition, they are hence\nproved random bit optimal for the scattering problem. Then, we investigate the\ntime complexity of scattering when strong multiplicity detection is not\navailable. We prove that such algorithms cannot converge in constant time in\nthe general case and in $o(\\log \\log n)$ rounds for random bits optimal\nscattering algorithms. However, we present a family of scattering algorithms\nthat converge as fast as needed without using multiplicity detection. Also, we\nput forward a specific protocol of this family that is random bit optimal ($n\n\\log n$ random bits are used) and time optimal ($\\log \\log n$ rounds are used).\nThis improves the time complexity of previous results in the same setting by a\n$\\log n$ factor. Aside from characterizing the random bit complexity of mobile\nrobot scattering, our study also closes its time complexity gap with and\nwithout strong multiplicity detection (that is, $O(1)$ time complexity is only\nachievable when strong multiplicity detection is available, and it is possible\nto approach it as needed otherwise).\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 18:37:40 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 10:58:03 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Bramas", "Quentin", "", "LIP6"], ["Tixeuil", "S\u00e9bastien", "", "LIP6, LINCS, IUF"]]}, {"id": "1309.6610", "submitter": "Bogdan Chlebus", "authors": "Lakshmi Anantharamu and Bogdan S. Chlebus and Mariusz A. Rokicki", "title": "Adversarial Multiple Access Channels with Individual Injection Rates", "comments": null, "journal-ref": "Adversarial multiple access channels with individual injection\n  rates. Theory of Computing Systems, 61(3): 820 - 850, 2017", "doi": "10.1007/s00224-016-9725-x", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study deterministic distributed broadcasting in synchronous\nmultiple-access channels. Packets are injected into $n$ nodes by a window-type\nadversary that is constrained by a window $w$ and injection rates individually\nassigned to all nodes. We investigate what queue size and packet latency can be\nachieved with the maximum aggregate injection rate of one packet per round,\ndepending on properties of channels and algorithms. We give a non-adaptive\nalgorithm for channels with collision detection and an adaptive algorithm for\nchannels without collision detection that achieve $O(\\min(n+w,w\\log n))$ packet\nlatency. We show that packet latency has to be either $\\Omega(w \\max (1,\\log_w\nn))$, when $w\\le n$, or $\\Omega(w+n)$, when $w>n$, as a matching lower bound to\nthese algorithms. We develop a non-adaptive algorithm for channels without\ncollision detection that achieves $O(n+w)$ queue size and $O(nw)$ packet\nlatency. This is in contrast with the adversarial model of global injection\nrates, in which non-adaptive algorithms with bounded packet latency do not\nexist (Chlebus et al. Distributed Computing 22(2): 93 - 116, 2009). Our\nalgorithm avoids collisions produced by simultaneous transmissions; we show\nthat any algorithm with this property must have $\\Omega(nw)$ packet latency.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 19:05:26 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 01:12:06 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 21:53:32 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Anantharamu", "Lakshmi", ""], ["Chlebus", "Bogdan S.", ""], ["Rokicki", "Mariusz A.", ""]]}, {"id": "1309.6723", "submitter": "Sheida Dayyani", "authors": "Sheida Dayyani, Mohammad Reza Khayyambashi", "title": "A Comparative Study of Replication Techniques in Grid Computing Systems", "comments": "10 pages, 3 figures, 4 table", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 11, No. 9, September 2013", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid Computing is a type of parallel and distributed systems that is designed\nto provide reliable access to data and computational resources in wide area\nnetworks. These resources are distributed in different geographical locations,\nhowever are organized to provide an integrated service. Effective data\nmanagement in today`s enterprise environment is an important issue. Also,\nPerformance is one of the challenges of using these environments. For improving\nthe performance of file access and easing the sharing amongst distributed\nsystems, replication techniques are used. Data replication is a common method\nused in distributed environments, where essential data is stored in multiple\nlocations, so that a user can access the data from a site in his area. In this\npaper, we present a survey on basic and new replication techniques that have\nbeen proposed by other researchers. After that, we have a full comparative\nstudy on these replication strategies. Also, at the end of the paper, we\nsummarize the results and points of these replication techniques.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 05:31:20 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Dayyani", "Sheida", ""], ["Khayyambashi", "Mohammad Reza", ""]]}, {"id": "1309.6978", "submitter": "Othon Michail", "authors": "Othon Michail, Paul G. Spirakis", "title": "Simple and Efficient Local Codes for Distributed Stable Network\n  Construction", "comments": "43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study protocols so that populations of distributed processes\ncan construct networks. In order to highlight the basic principles of\ndistributed network construction we keep the model minimal in all respects. In\nparticular, we assume finite-state processes that all begin from the same\ninitial state and all execute the same protocol (i.e. the system is\nhomogeneous). Moreover, we assume pairwise interactions between the processes\nthat are scheduled by an adversary. The only constraint on the adversary\nscheduler is that it must be fair. In order to allow processes to construct\nnetworks, we let them activate and deactivate their pairwise connections. When\ntwo processes interact, the protocol takes as input the states of the processes\nand the state of the their connection and updates all of them. Initially all\nconnections are inactive and the goal is for the processes, after interacting\nand activating/deactivating connections for a while, to end up with a desired\nstable network. We give protocols (optimal in some cases) and lower bounds for\nseveral basic network construction problems such as spanning line, spanning\nring, spanning star, and regular network. We provide proofs of correctness for\nall of our protocols and analyze the expected time to convergence of most of\nthem under a uniform random scheduler that selects the next pair of interacting\nprocesses uniformly at random from all such pairs. Finally, we prove several\nuniversality results by presenting generic protocols that are capable of\nsimulating a Turing Machine (TM) and exploiting it in order to construct a\nlarge class of networks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 17:16:16 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 10:30:57 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Michail", "Othon", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "1309.7429", "submitter": "Manish Gupta", "authors": "Mit Sheth, Krishna Gopal Benerjee and Manish K. Gupta", "title": "Quorum Sensing for Regenerating Codes in Distributed Storage", "comments": "8 pages, 5 figures, submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage systems with replication are well known for storing large\namount of data. A large number of replication is done in order to provide\nreliability. This makes the system expensive. Various methods have been\nproposed over time to reduce the degree of replication and yet provide same\nlevel of reliability. One recently suggested scheme is of Regenerating codes,\nwhere a file is divided in to parts which are then processed by a coding\nmechanism and network coding to provide large number of parts. These are stored\nat various nodes with more than one part at each node. These codes can generate\nwhole file and can repair a failed node by contacting some out of total\nexisting nodes. This property ensures reliability in case of node failure and\nuses clever replication. This also optimizes bandwidth usage. In a practical\nscenario, the original file will be read and updated many times. With every\nupdate, we will have to update the data stored at many nodes. Handling multiple\nrequests at the same time will bring a lot of complexity. Reading and writing\nor multiple writing on the same data at the same time should also be prevented.\nIn this paper, we propose an algorithm that manages and executes all the\nrequests from the users which reduces the update complexity. We also try to\nkeep an adequate amount of availability at the same time. We use a voting based\nmechanism and form read, write and repair quorums. We have also done\nprobabilistic analysis of regenerating codes.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 05:30:49 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Sheth", "Mit", ""], ["Benerjee", "Krishna Gopal", ""], ["Gupta", "Manish K.", ""]]}, {"id": "1309.7695", "submitter": "EPTCS", "authors": "Daniela Besozzi (University of Milano), Giulio Caravagna (University\n  of Milano Bicocca), Paolo Cazzaniga (University of Bergamo), Marco Nobile\n  (University of Milano Bicocca), Dario Pescini (University of Milano Bicocca),\n  Alessandro Re (University of Milano Bicocca)", "title": "GPU-powered Simulation Methodologies for Biological Systems", "comments": "In Proceedings Wivace 2013, arXiv:1309.7122", "journal-ref": "EPTCS 130, 2013, pp. 87-91", "doi": "10.4204/EPTCS.130.14", "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of biological systems witnessed a pervasive cross-fertilization\nbetween experimental investigation and computational methods. This gave rise to\nthe development of new methodologies, able to tackle the complexity of\nbiological systems in a quantitative manner. Computer algorithms allow to\nfaithfully reproduce the dynamics of the corresponding biological system, and,\nat the price of a large number of simulations, it is possible to extensively\ninvestigate the system functioning across a wide spectrum of natural\nconditions. To enable multiple analysis in parallel, using cheap, diffused and\nhighly efficient multi-core devices we developed GPU-powered simulation\nalgorithms for stochastic, deterministic and hybrid modeling approaches, so\nthat also users with no knowledge of GPUs hardware and programming can easily\naccess the computing power of graphics engines.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 01:06:39 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Besozzi", "Daniela", "", "University of Milano"], ["Caravagna", "Giulio", "", "University\n  of Milano Bicocca"], ["Cazzaniga", "Paolo", "", "University of Bergamo"], ["Nobile", "Marco", "", "University of Milano Bicocca"], ["Pescini", "Dario", "", "University of Milano Bicocca"], ["Re", "Alessandro", "", "University of Milano Bicocca"]]}, {"id": "1309.7720", "submitter": "Ken-ichiro Ishikawa", "authors": "Ken-ichiro Ishikawa", "title": "ASURA: Scalable and Uniform Data Distribution Algorithm for Storage\n  Clusters", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale storage cluster systems need to manage a vast amount of data\nlocations. A naive data locations management maintains pairs of data ID and\nnodes storing the data in tables. However, it is not practical when the number\nof pairs is too large. To solve this problem, management using data\ndistribution algorithms, rather than management using tables, has been proposed\nin recent research. It can distribute data by determining the node for storing\nthe data based on the datum ID. Such data distribution algorithms require the\nability to handle the addition or removal of nodes, short calculation time and\nuniform data distribution in the capacity of each node. This paper proposes a\ndata distribution algorithm called ASURA (Advanced Scalable and Uniform storage\nby Random number Algorithm) that satisfies these requirements. It achieves\nfollowing four characteristics: 1) minimum data movement to maintain data\ndistribution according to node capacity when nodes are added or removed, even\nif data are replicated, 2) roughly sub-micro-seconds calculation time, 3) much\nlower than 1% maximum variability between nodes in data distribution, and 4)\ndata distribution according to the capacity of each node. The evaluation\nresults show that ASURA is qualitatively and quantitatively competitive against\nmajor data distribution algorithms such as Consistent Hashing, Weighted\nRendezvous Hashing and Random Slicing. The comparison results show benefits of\neach algorithm; they show that ASURA has advantage in large scale-out storage\nclusters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 04:48:11 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 10:41:49 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Ishikawa", "Ken-ichiro", ""]]}, {"id": "1309.7841", "submitter": "Rajesh Sundaresan", "authors": "Vivek S. Borkar, Rahul Makhijani, Rajesh Sundaresan", "title": "Asynchronous Gossip for Averaging and Spectral Ranking", "comments": "14 pages, 7 figures. Minor revision", "journal-ref": null, "doi": "10.1109/JSTSP.2014.2320229", "report-no": null, "categories": "cs.DC cs.IT cs.SY math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two variants of the classical gossip algorithm. The first variant\nis a version of asynchronous stochastic approximation. We highlight a\nfundamental difficulty associated with the classical asynchronous gossip\nscheme, viz., that it may not converge to a desired average, and suggest an\nalternative scheme based on reinforcement learning that has guaranteed\nconvergence to the desired average. We then discuss a potential application to\na wireless network setting with simultaneous link activation constraints. The\nsecond variant is a gossip algorithm for distributed computation of the\nPerron-Frobenius eigenvector of a nonnegative matrix. While the first variant\ndraws upon a reinforcement learning algorithm for an average cost controlled\nMarkov decision problem, the second variant draws upon a reinforcement learning\nalgorithm for risk-sensitive control. We then discuss potential applications of\nthe second variant to ranking schemes, reputation networks, and principal\ncomponent analysis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 13:42:11 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 05:38:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Borkar", "Vivek S.", ""], ["Makhijani", "Rahul", ""], ["Sundaresan", "Rajesh", ""]]}]