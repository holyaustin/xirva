[{"id": "1802.00056", "submitter": "Andre van Hoorn", "authors": "Christoph Heger, Andr\\'e van Hoorn, Du\\v{s}an Okanovic, Stefan Siegl,\n  Christian V\\\"ogele, Alexander Wert", "title": "diagnoseIT: Expertengest\\\"utzte automatische Diagnose von\n  Performance-Probleme in Enterprise-Anwendungen (Abschlussbericht)", "comments": "The language of the report is German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.PF q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the final report of the collaborative research project diagnoseIT on\nexpert-guided automatic diagnosis of performance problems in enterprise\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 03:34:10 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Heger", "Christoph", ""], ["van Hoorn", "Andr\u00e9", ""], ["Okanovic", "Du\u0161an", ""], ["Siegl", "Stefan", ""], ["V\u00f6gele", "Christian", ""], ["Wert", "Alexander", ""]]}, {"id": "1802.00082", "submitter": "Faria Kalim", "authors": "Faria Kalim, Le Xu, Sharanya Bathey, Richa Meherwal, Indranil Gupta", "title": "Henge: Intent-driven Multi-Tenant Stream Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Henge, a system to support intent-based multi-tenancy in modern\nstream processing applications. Henge supports multi-tenancy as a first-class\ncitizen: everyone inside an organization can now submit their stream processing\njobs to a single, shared, consolidated cluster. Additionally, Henge allows each\ntenant (job) to specify its own intents (i.e., requirements) as a Service Level\nObjective (SLO) that captures latency and/or throughput. In a multi-tenant\ncluster, the Henge scheduler adapts continually to meet jobs' SLOs in spite of\nlimited cluster resources, and under dynamic input workloads. SLOs are soft and\nare based on utility functions. Henge continually tracks SLO satisfaction, and\nwhen jobs miss their SLOs, it wisely navigates the state space to perform\nresource allocations in real time, maximizing total system utility achieved by\nall jobs in the system. Henge is integrated in Apache Storm and we present\nexperimental results using both production topologies and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 22:04:50 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Kalim", "Faria", ""], ["Xu", "Le", ""], ["Bathey", "Sharanya", ""], ["Meherwal", "Richa", ""], ["Gupta", "Indranil", ""]]}, {"id": "1802.00245", "submitter": "Xiaoda Zhang", "authors": "Xiaoda Zhang, Zhuzhong Qian, Sheng Zhang, Yize Li, Xiangbo Li,\n  Xiaoliang Wang, Sanglu Lu", "title": "Towards Reliable (and Efficient) Job Executions in a Practical\n  Geo-distributed Data Analytics System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-distributed data analytics are increasingly common to derive useful\ninformation in large organisations. Naive extension of existing cluster-scale\ndata analytics systems to the scale of geo-distributed data centers faces\nunique challenges including WAN bandwidth limits, regulatory constraints,\nchangeable/unreliable runtime environment, and monetary costs. Our goal in this\nwork is to develop a practical geo-distribued data analytics system that (1)\nemploys an intelligent mechanism for jobs to efficiently utilize (adjust to)\nthe resources (changeable environment) across data centers; (2) guarantees the\nreliability of jobs due to the possible failures; and (3) is generic and\nflexible enough to run a wide range of data analytics jobs without requiring\nany changes.\n  To this end, we present a new, general geo-distributed data analytics system,\nHOUTU, that is composed of multiple autonomous systems, each operating in a\nsovereign data center. HOUTU maintains a job manager (JM) for a geo-distributed\njob in each data center, so that these replicated JMs could individually and\ncooperatively manage resources and assign tasks. Our experiments on the\nprototype of HOUTU running across four Alibaba Cloud regions show that HOUTU\nprovides nearly efficient job performance as in the existing centralized\narchitecture, and guarantees reliable job executions when facing failures.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 11:21:32 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 02:47:48 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 07:09:52 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zhang", "Xiaoda", ""], ["Qian", "Zhuzhong", ""], ["Zhang", "Sheng", ""], ["Li", "Yize", ""], ["Li", "Xiangbo", ""], ["Wang", "Xiaoliang", ""], ["Lu", "Sanglu", ""]]}, {"id": "1802.00330", "submitter": "Liangyu Chen", "authors": "Dang Lin, Liangyu Chen", "title": "An efficient algorithm for global interval solution of nonlinear\n  algebraic equations and its GPGPU implementation", "comments": "21pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving nonlinear algebraic equations is a classic mathematics problem, and\ncommon in scientific researches and engineering applications. There are many\nnumeric, symbolic and numeric-symbolic methods of solving (real) solutions.\nUnlucky, these methods are constrained by some factors, e.g., high complexity,\nslow serial calculation, and the notorious intermediate expression expansion.\nEspecially when the count of variables is larger than six, the efficiency is\ndecreasing drastically. In this paper, according to the property of physical\nworld, we pay attention to nonlinear algebraic equations whose variables are in\nfixed constraints, and get meaningful real solutions. Combining with\nparallelism of GPGPU, we present an efficient algorithm, by searching the\nsolution space globally and solving the nonlinear algebraic equations with real\ninterval solutions. Furthermore, we realize the Hansen-Sengupta method on\nGPGPU. The experiments show that our method can solve many nonlinear algebraic\nequations, and the results are accurate and more efficient compared to\ntraditional serial methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 13:29:50 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Lin", "Dang", ""], ["Chen", "Liangyu", ""]]}, {"id": "1802.00438", "submitter": "Hamid Reza Zohouri", "authors": "Hamid Reza Zohouri, Artur Podobas, Satoshi Matsuoka", "title": "Combined Spatial and Temporal Blocking for High-Performance Stencil\n  Computation on FPGAs Using OpenCL", "comments": "FPGA '18: 2018 ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays", "journal-ref": null, "doi": "10.1145/3174243.3174248", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in High Level Synthesis tools have attracted software\nprogrammers to accelerate their high-performance computing applications on\nFPGAs. Even though it has been shown that FPGAs can compete with GPUs in terms\nof performance for stencil computation, most previous work achieve this by\navoiding spatial blocking and restricting input dimensions relative to FPGA\non-chip memory. In this work we create a stencil accelerator using Intel FPGA\nSDK for OpenCL that achieves high performance without having such restrictions.\nWe combine spatial and temporal blocking to avoid input size restrictions, and\nemploy multiple FPGA-specific optimizations to tackle issues arisen from the\nadded design complexity. Accelerator parameter tuning is guided by our\nperformance model, which we also use to project performance for the upcoming\nIntel Stratix 10 devices. On an Arria 10 GX 1150 device, our accelerator can\nreach up to 760 and 375 GFLOP/s of compute performance, for 2D and 3D stencils,\nrespectively, which rivals the performance of a highly-optimized GPU\nimplementation. Furthermore, we estimate that the upcoming Stratix 10 devices\ncan achieve a performance of up to 3.5 TFLOP/s and 1.6 TFLOP/s for 2D and 3D\nstencil computation, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 10:55:40 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Zohouri", "Hamid Reza", ""], ["Podobas", "Artur", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1802.00535", "submitter": "Alexander Brown", "authors": "Alexander Brown, Saurabh Garg, James Montgomery", "title": "Scalable Preprocessing of High Volume Bird Acoustic Data", "comments": "28 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we examine the problem of efficiently preprocessing high volume\nbird acoustic data. We combine several existing preprocessing steps including\nnoise reduction approaches into a single efficient pipeline by examining each\nprocess individually. We then utilise a distributed computing architecture to\nimprove execution time. Using a master-slave model with data parallelisation,\nwe developed a near-linear automated scalable system, capable of preprocessing\nbird acoustic recordings 21.76 times faster with 32 cores over 8 virtual\nmachines, compared to a serial process. This work contributes to the research\narea of bioacoustic analysis, which is currently very active because of its\npotential to monitor animals quickly at low cost. Overcoming noise interference\nis a significant challenge in many bioacoustic studies, and the volume of data\nin these studies is increasing. Our work makes large scale bird acoustic\nanalyses more feasible by parallelising important bird acoustic processing\ntasks to significantly reduce execution times.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 01:52:36 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Brown", "Alexander", ""], ["Garg", "Saurabh", ""], ["Montgomery", "James", ""]]}, {"id": "1802.00673", "submitter": "Florian Schmidt", "authors": "Florian Schmidt and Mathias Niepert and Felipe Huici", "title": "Representation Learning for Resource Usage Prediction", "comments": "3 pages, 2 figures, SysML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating a model of a computer system that can be used for tasks such as\npredicting future resource usage and detecting anomalies is a challenging\nproblem. Most current systems rely on heuristics and overly simplistic\nassumptions about the workloads and system statistics. These heuristics are\ntypically a one-size-fits-all solution so as to be applicable in a wide range\nof applications and systems environments.\n  With this paper, we present our ongoing work of integrating systems telemetry\nranging from standard resource usage statistics to kernel and library calls of\napplications into a machine learning model. Intuitively, such a ML model\napproximates, at any point in time, the state of a system and allows us to\nsolve tasks such as resource usage prediction and anomaly detection. To achieve\nthis goal, we leverage readily-available information that does not require any\nchanges to the applications run on the system. We train recurrent neural\nnetworks to learn a model of the system under consideration. As a proof of\nconcept, we train models specifically to predict future resource usage of\nrunning applications.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 13:21:13 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Schmidt", "Florian", ""], ["Niepert", "Mathias", ""], ["Huici", "Felipe", ""]]}, {"id": "1802.00678", "submitter": "Matthieu Perrin", "authors": "Achour Most\\'efaoui, Matthieu Perrin, Michel Raynal", "title": "A Simple Object that Spans the Whole Consensus Hierarchy", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple generalization of the basic atomic read/write\nregister object, whose genericity parameter spans the whole set of integers and\nis such that its k-parameterized instance has exactly consensus number k. This\nobject, whose definition is pretty natural, is a sliding window register of\nsize k. Its interest lies in its simplicity and its genericity dimension which\nprovides a global view capturing the whole consensus hierarchy. Hence, this\nshort article must be seen as a simple pedagogical introduction to Herlihy's\nconsensus hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 13:31:32 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Most\u00e9faoui", "Achour", ""], ["Perrin", "Matthieu", ""], ["Raynal", "Michel", ""]]}, {"id": "1802.00688", "submitter": "Malika Bendechache", "authors": "Malika Bendechache, Nhien-An Le-Khac and M-Tahar Kechadi", "title": "Hierarchical Aggregation Approach for Distributed clustering of spatial\n  datasets", "comments": "6 pages. arXiv admin note: substantial text overlap with\n  arXiv:1704.03421", "journal-ref": null, "doi": "10.1109/ICDMW.2016.0158", "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new approach of distributed clustering for\nspatial datasets, based on an innovative and efficient aggregation technique.\nThis distributed approach consists of two phases: 1) local clustering phase,\nwhere each node performs a clustering on its local data, 2) aggregation phase,\nwhere the local clusters are aggregated to produce global clusters. This\napproach is characterised by the fact that the local clusters are represented\nin a simple and efficient way. And The aggregation phase is designed in such a\nway that the final clusters are compact and accurate while the overall process\nis efficient in both response time and memory allocation. We evaluated the\napproach with different datasets and compared it to well-known clustering\ntechniques. The experimental results show that our approach is very promising\nand outperforms all those algorithms\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 12:50:59 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Bendechache", "Malika", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1802.00699", "submitter": "Wanling Gao", "authors": "Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Fei\n  Tang, Biwei Xie, Chen Zheng and Qiang Yang", "title": "Data Dwarfs: A Lens Towards Fully Understanding Big Data and AI\n  Workloads", "comments": "19 pages, 16 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity and diversity of big data and AI workloads make understanding\nthem difficult and challenging. This paper proposes a new approach to\ncharacterizing big data and AI workloads. We consider each big data and AI\nworkload as a pipeline of one or more classes of unit of computations performed\non different initial or intermediate data inputs. Each class of unit of\ncomputation captures the common requirements while being reasonably divorced\nfrom individual implementations, and hence we call it a data dwarf. For the\nfirst time, among a wide variety of big data and AI workloads, we identify\neight data dwarfs that takes up most of run time, including Matrix, Sampling,\nLogic, Transform, Set, Graph, Sort and Statistic. We implement the eight data\ndwarfs on different software stacks as the micro benchmarks of an open-source\nbig data and AI benchmark suite, and perform comprehensive characterization of\nthose data dwarfs from perspective of data sizes, types, sources, and patterns\nas a lens towards fully understanding big data and AI workloads.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 04:26:21 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 07:38:18 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Gao", "Wanling", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Zheng", "Daoyi", ""], ["Tang", "Fei", ""], ["Xie", "Biwei", ""], ["Zheng", "Chen", ""], ["Yang", "Qiang", ""]]}, {"id": "1802.00706", "submitter": "Matthieu Perrin", "authors": "Achour Most\\'efaoui, Matthieu Perrin, Michel Raynal", "title": "Extending Causal Consistency to any Object Defined by a Sequential\n  Specification", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple generalization of causal consistency suited to\nany object defined by a sequential specification. As causality is captured by a\npartial order on the set of operations issued by the processes on shared\nobjects (concurrent operations are not ordered), it follows that causal\nconsistency allows different processes to have different views of each object\nhistory.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 14:48:21 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Most\u00e9faoui", "Achour", ""], ["Perrin", "Matthieu", ""], ["Raynal", "Michel", ""]]}, {"id": "1802.00728", "submitter": "Ron Daniel Jr.", "authors": "Darin McBeath and Ron Daniel Jr", "title": "Measuring Spark on AWS: A Case Study on Mining Scientific Publications\n  with Annotation Query", "comments": "19 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Annotation Query (AQ) is a program that provides the ability to query many\ndifferent types of NLP annotations on a text, as well as the original content\nand structure of the text. The query results may provide new annotations, or\nthey may select subsets of the content and annotations for deeper processing.\nLike GATE's Mimir, AQ is based on region algebras. Our AQ is implemented to run\non a Spark cluster. In this paper we look at how AQ's runtimes are affected by\nthe size of the collection, the number of nodes in the cluster, the type of\nnode, and the characteristics of the queries. Cluster size, of course, makes a\nlarge difference in performance so long as skew can be avoided. We find that\nthere is minimal difference in performance when persisting annotations\nserialized to local SSD drives as opposed to deserialized into local memory. We\nalso find that if the number of nodes is kept constant, then AWS'\nstorage-optimized instance performs the best. But if we factor in total cost,\nthe compute-optimized nodes provides the best performance relative to cost.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 15:24:33 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["McBeath", "Darin", ""], ["Daniel", "Ron", "Jr"]]}, {"id": "1802.00951", "submitter": "Chinnathambi Sathya", "authors": "Sathya Chinnathambi, Agilan Santhanam", "title": "Scheduling and Checkpointing optimization algorithm for Byzantine fault\n  tolerance in Cloud Clusters", "comments": "Cloud Computing, Byzantine Faults, Checkpointing, Scheduling, Fault\n  Tolerance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among those faults Byzantine faults offers serious challenge to fault\ntolerance mechanism, because it often go undetected at the initial stage and it\ncan easily propagate to other VMs before a detection is made. Consequently some\nof the mission critical application such as air traffic control, online baking\netc still staying away from the cloud for such reasons. However if a Byzantine\nfaults is not detected and tolerated at initial stage then applications such as\nbig data analytics can go completely wrong in spite of hours of computations\nperformed by the entire cloud. Therefore in the previous work a fool-proof\nByzantine fault detection has been proposed, as a continuation this work\ndesigns a scheduling algorithm (WSSS) and checkpoint optimization algorithm\n(TCC) to tolerate and eliminate the Byzantine faults before it makes any\nimpact. The WSSS algorithm keeps track of server performance which is part of\nVirtual Clusters to help allocate best performing server to mission critical\napplication. WSSS therefore ranks the servers based on a counter which monitors\nevery Virtual Nodes (VN) for time and performance failures. The TCC algorithm\nworks to generalize the possible Byzantine error prone region through\nmonitoring delay variation to start new VNs with previous checkpointing.\nMoreover it can stretch the state interval for performing and error free VNs in\nan effect to minimize the space, time and cost overheads caused by\ncheckpointing. The analysis is performed with plotting state transition and\nCloudSim based simulation. The result shows TCC reduces fault tolerance\noverhead exponentially and the WSSS allots virtual resources effectively\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 10:29:39 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chinnathambi", "Sathya", ""], ["Santhanam", "Agilan", ""]]}, {"id": "1802.00976", "submitter": "EPTCS", "authors": "Danilo Pianini (University of Bologna), Guido Salvaneschi", "title": "Proceedings First Workshop on Architectures, Languages and Paradigms for\n  IoT", "comments": null, "journal-ref": "EPTCS 264, 2018", "doi": "10.4204/EPTCS.264", "report-no": null, "categories": "cs.DC cs.MA cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 1st workshop on Architectures, Languages and Paradigms for IoT (ALP4IoT\n2017), was held in Turin on September 19th, 2017. ALP4IoT was a satellite event\nof the 13th International Conference on integrated Formal Methods (iFM 2017).\nThe workshop aimed at critically reviewing the state-of-the-art and the\nstate-of-the-practice of formal techniques and software methods for the IoT,\npresenting open problems and challenges and triggering a discussion among the\nparticipants with different views and backgrounds. The Internet of Things is\nushering a dramatic increase in number and variety of interconnected and smart\nobjects. Communication capabilities and computational power are growingly\nembedded in everyday devices, including personal smart devices, public\ndisplays, cars, drones, and electronic tags. This state of the things opens an\nunprecedented range of research opportunities: the inherent distribution,\nmobility, situatedness, and heterogeneity of such devices call for proper\nscientific understanding of the foundations of such systems as well as for\nnovel software methods. The workshop solicited original contributions on\narchitectures, languages, paradigms, and techniques with potential practical\nand theoretical impact on software systems targeting the IoT, welcoming\ninter-disciplinary approaches.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 13:33:37 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Pianini", "Danilo", "", "University of Bologna"], ["Salvaneschi", "Guido", ""]]}, {"id": "1802.01030", "submitter": "Marco Netto", "authors": "Bruno Silva, Marco A. S. Netto, Renato L. F. Cunha", "title": "JobPruner: A Machine Learning Assistant for Exploring Parameter Spaces\n  in HPC Applications", "comments": "13 pages, FGCS journal", "journal-ref": null, "doi": "10.1016/j.future.2018.02.002", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Performance Computing (HPC) applications are essential for scientists\nand engineers to create and understand models and their properties. These\nprofessionals depend on the execution of large sets of computational jobs that\nexplore combinations of parameter values. Avoiding the execution of unnecessary\njobs brings not only speed to these experiments, but also reductions in\ninfrastructure usage---particularly important due to the shift of these\napplications to HPC cloud platforms. Our hypothesis is that data generated by\nthese experiments can help users in identifying such jobs. To address this\nhypothesis we need to understand the similarity levels among multiple\nexperiments necessary for job elimination decisions and the steps required to\nautomate this process. In this paper we present a study and a machine\nlearning-based tool called JobPruner to support parameter exploration in HPC\nexperiments. The tool was evaluated with three real-world use cases from\ndifferent domains including seismic analysis and agronomy. We observed the tool\nreduced 93% of jobs in a single experiment, while improving quality in most\nscenarios. In addition, reduction in job executions was possible even\nconsidering past experiments with low correlations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 21:10:36 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 11:25:57 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 13:04:10 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Silva", "Bruno", ""], ["Netto", "Marco A. S.", ""], ["Cunha", "Renato L. F.", ""]]}, {"id": "1802.01065", "submitter": "Kijung Shin", "authors": "Kijung Shin, Bryan Hooi, Jisu Kim, and Christos Faloutsos", "title": "Detecting Group Anomalies in Tera-Scale Multi-Aspect Data via\n  Dense-Subtensor Mining", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we detect fraudulent lockstep behavior in large-scale multi-aspect\ndata (i.e., tensors)? Can we detect it when data are too large to fit in memory\nor even on a disk? Past studies have shown that dense subtensors in real-world\ntensors (e.g., social media, Wikipedia, TCP dumps, etc.) signal anomalous or\nfraudulent behavior such as retweet boosting, bot activities, and network\nattacks. Thus, various approaches, including tensor decomposition and search,\nhave been proposed for detecting dense subtensors rapidly and accurately.\nHowever, existing methods have low accuracy, or they assume that tensors are\nsmall enough to fit in main memory, which is unrealistic in many real-world\napplications such as social media and web. To overcome these limitations, we\npropose D-CUBE, a disk-based dense-subtensor detection method, which also can\nrun in a distributed manner across multiple machines. Compared to\nstate-of-the-art methods, D-CUBE is (1) Memory Efficient: requires up to 1,561X\nless memory and handles 1,000X larger data (2.6TB), (2) Fast: up to 7X faster\ndue to its near-linear scalability, (3) Provably Accurate: gives a guarantee on\nthe densities of the detected subtensors, and (4) Effective: spotted network\nattacks from TCP dumps and synchronized behavior in rating data most\naccurately.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 03:06:53 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 03:56:00 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 09:03:33 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Shin", "Kijung", ""], ["Hooi", "Bryan", ""], ["Kim", "Jisu", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1802.01070", "submitter": "Keith Schubert", "authors": "Paniz Karbasi, Ritchie Cai, Blake Schultze, Hanh Nguyen, Jones Reed,\n  Patrick Hall, Valentina Giacometti, Vladimir Bashkirov, Robert Johnson, Nick\n  Karonis, Jeffrey Olafsen, Caesar Ordonez, Keith E. Schubert, Reinhard W.\n  Schulte", "title": "A Highly Accelerated Parallel Multi-GPU based Reconstruction Algorithm\n  for Generating Accurate Relative Stopping Powers", "comments": "IEEE NSS/MIC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dose Proton Computed Tomography (pCT) is an evolving imaging modality\nthat is used in proton therapy planning which addresses the range uncertainty\nproblem. The goal of pCT is generating a 3D map of Relative Stopping Power\n(RSP) measurements with high accuracy within clinically required time frames.\nGenerating accurate RSP values within the shortest amount of time is considered\na key goal when developing a pCT software. The existing pCT softwares have\nsuccessfully met this time frame and even succeeded this time goal, but\nrequiring clusters with hundreds of processors.\n  This paper describes a novel reconstruction technique using two Graphics\nProcessing Unit (GPU) cores, such as is available on a single Nvidia P100. The\nproposed reconstruction technique is tested on both simulated and experimental\ndatasets and on two different systems namely Nvidia K40 and P100 GPUs from IBM\nand Cray. The experimental results demonstrate that our proposed reconstruction\nmethod meets both the timing and accuracy with the benefit of having reasonable\ncost, and efficient use of power.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 04:29:27 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Karbasi", "Paniz", ""], ["Cai", "Ritchie", ""], ["Schultze", "Blake", ""], ["Nguyen", "Hanh", ""], ["Reed", "Jones", ""], ["Hall", "Patrick", ""], ["Giacometti", "Valentina", ""], ["Bashkirov", "Vladimir", ""], ["Johnson", "Robert", ""], ["Karonis", "Nick", ""], ["Olafsen", "Jeffrey", ""], ["Ordonez", "Caesar", ""], ["Schubert", "Keith E.", ""], ["Schulte", "Reinhard W.", ""]]}, {"id": "1802.01110", "submitter": "Claude Tadonki Dr. HDR", "authors": "Claude Tadonki", "title": "HPC Curriculum and Associated Ressources in the Academic Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware support for high-performance computing (HPC) has so far been subject\nto significant advances. The pervasiveness of HPC systems, mainly made up with\nparallel computing units, makes it crucial to spread and vivify effective HPC\ncurricula. Besides didactic considerations, it appears very important to\nimplement HPC hardware infrastructures that will serves for practices, and also\nfor scientific and industrial requests. The latter ensures a valuable\nconnection with surrounding cutting-edge research activities in other topics\n({\\em life sciences, physics, data mining, applied mathematics, finance,\nquantitative economy, engineering sciences}, to name a few), and also with\nindustrial entities and services providers from their requests related to HPC\nmeans and expertise. This aspect is very important as it makes an HPC Center\nbecoming a social actor, while bringing real-life scenarios into the academic\ncontext. The current paper describes the major steps and objectives for a\nconsistent HPC curriculum, with specific analyses of particular contexts;\nsuggests how to technically set up operational HPC infrastructures; and\ndiscusses the connection with end-users, all these in both effective and\nprospective standpoints.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 11:17:14 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Tadonki", "Claude", ""]]}, {"id": "1802.01315", "submitter": "Peilun Li", "authors": "Peilun Li, Guosai Wang, Xiaoqi Chen, Wei Xu", "title": "Gosig: Scalable Byzantine Consensus on Adversarial Wide Area Network for\n  Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Byzantine fault tolerance (BFT) protocols face significant\nchallenges in the consortium blockchain scenario. On the one hand, we can make\nlittle assumptions about the reliability and security of the underlying\nInternet. On the other hand, the applications on consortium blockchains demand\na system as scalable as the Bit-coin but providing much higher performance, as\nwell as provable safety. We present a new BFT protocol, Gosig, that combines\ncrypto-based secret leader selection and multi-round voting in the protocol\nlayer with implementation layer optimizations such as gossip-based message\npropagation. In particular, Gosig guarantees safety even in a network fully\ncontrolled by adversaries, while providing provable liveness with\neasy-to-achieve network connectivity assumption. On a wide area testbed\nconsisting of 140 Amazon EC2 servers spanning 14 cities on five continents, we\nshow that Gosig can achieve over 4,000 transactions per second with less than 1\nminute transaction confirmation time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 09:55:12 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Li", "Peilun", ""], ["Wang", "Guosai", ""], ["Chen", "Xiaoqi", ""], ["Xu", "Wei", ""]]}, {"id": "1802.01548", "submitter": "Esteban Real", "authors": "Esteban Real, Alok Aggarwal, Yanping Huang and Quoc V Le", "title": "Regularized Evolution for Image Classifier Architecture Search", "comments": "Accepted for publication at AAAI 2019, the Thirty-Third AAAI\n  Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effort devoted to hand-crafting neural network image classifiers has\nmotivated the use of architecture search to discover them automatically.\nAlthough evolutionary algorithms have been repeatedly applied to neural network\ntopologies, the image classifiers thus discovered have remained inferior to\nhuman-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that\nsurpasses hand-designs for the first time. To do this, we modify the tournament\nselection evolutionary algorithm by introducing an age property to favor the\nyounger genotypes. Matching size, AmoebaNet-A has comparable accuracy to\ncurrent state-of-the-art ImageNet models discovered with more complex\narchitecture-search methods. Scaled to larger size, AmoebaNet-A sets a new\nstate-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled\ncomparison against a well known reinforcement learning algorithm, we give\nevidence that evolution can obtain results faster with the same hardware,\nespecially at the earlier stages of the search. This is relevant when fewer\ncompute resources are available. Evolution is, thus, a simple method to\neffectively discover high-quality architectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 18:20:52 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 18:24:29 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 00:10:00 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 06:21:47 GMT"}, {"version": "v5", "created": "Thu, 4 Oct 2018 00:11:37 GMT"}, {"version": "v6", "created": "Fri, 26 Oct 2018 05:56:00 GMT"}, {"version": "v7", "created": "Sat, 16 Feb 2019 23:28:16 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Real", "Esteban", ""], ["Aggarwal", "Alok", ""], ["Huang", "Yanping", ""], ["Le", "Quoc V", ""]]}, {"id": "1802.01639", "submitter": "John  Abe Mr.", "authors": "John Olorunfemi Abe and Burak Berk Ustundaug", "title": "A Data as a Service (DaaS) Model for GPU-based Data Analytics", "comments": "Accepted, 23 December 2017, by the IEEE IFIP NTMS Workshop on Big\n  Data and Emerging Trends WBD-ET 2018; it was later withdrawn because of\n  funding issues. An extended/enhanced version will be published in future\n  dates in related journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cloud-based services with resources to be provisioned for consumers are\nincreasingly the norm, especially with respect to Big data, spatiotemporal data\nmining and application services that impose a user's agreed Quality of Service\n(QoS) rules or Service Level Agreement (SLA). Considering the pervasive nature\nof data centers and cloud system, there is a need for a real-time analytics of\nthe systems considering cost, utility and energy. This work presents an overlay\nmodel of GPU system for Data As A Service (DaaS) to give a real-time data\nanalysis of network data, customers, investors and users' data from the\ndatacenters or cloud system. Using a modeled layer to define a learning\nprotocol and system, we give a custom, profitable system for DaaS on GPU. The\nGPU-enabled pre-processing and initial operations of the clustering model\nanalysis is promising as shown in the results. We examine the model on\nreal-world data sets to model a big data set or spatiotemporal data mining\nservices. We also produce results of our model with clustering, neural\nnetworks' Self-organizing feature maps (SOFM or SOM) to produce a distribution\nof the clustering for DaaS model. The experimental results thus far show a\npromising model that could enhance SLA and or QoS based DaaS.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 20:32:55 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Abe", "John Olorunfemi", ""], ["Ustundaug", "Burak Berk", ""]]}, {"id": "1802.01788", "submitter": "EPTCS", "authors": "Giorgio Audrito (1), Ferruccio Damiani (1), Mirko Viroli (2) ((1)\n  University of Torino, (2) University of Bologna)", "title": "Aggregate Graph Statistics", "comments": "In Proceedings ALP4IoT 2017, arXiv:1802.00976", "journal-ref": "EPTCS 264, 2018, pp. 18-22", "doi": "10.4204/EPTCS.264.2", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting statistic from graph-based data is an increasingly studied topic\nin the data mining community. We argue that these statistics have great value\nas well in dynamic IoT contexts: they can support complex computational\nactivities involving distributed coordination and provision of situation\nrecognition. We show that the HyperANF algorithm for calculating the\nneighbourhood function of vertices of a graph naturally allows for a fully\ndistributed and asynchronous implementation, thanks to a mapping to the field\ncalculus, a distribution model proposed for collective adaptive systems. This\nmapping gives evidence that the field calculus framework is well-suited to\naccommodate massively parallel computations over graphs. Furthermore, it\nprovides a new \"self-stabilising\" building block which can be used in aggregate\ncomputing in several contexts, there including improved leader election or\nnetwork vulnerabilities detection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 04:09:51 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Audrito", "Giorgio", ""], ["Damiani", "Ferruccio", ""], ["Viroli", "Mirko", ""]]}, {"id": "1802.01789", "submitter": "EPTCS", "authors": "Giorgio Audrito (1), Sergio Bergamini (1) ((1) University of Torino)", "title": "Resilient Blocks for Summarising Distributed Data", "comments": "In Proceedings ALP4IoT 2017, arXiv:1802.00976", "journal-ref": "EPTCS 264, 2018, pp. 23-26", "doi": "10.4204/EPTCS.264.3", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarising distributed data is a central routine for parallel programming,\nlying at the core of widely used frameworks such as the map/reduce paradigm. In\nthe IoT context it is even more crucial, being a privileged mean to allow\nlong-range interactions: in fact, summarising is needed to avoid data explosion\nin each computational unit.\n  We introduce a new algorithm for dynamic summarising of distributed data,\nweighted multi-path, improving over the state-of-the-art multi-path algorithm.\nWe validate the new algorithm in an archetypal scenario, taking into account\nsources of volatility of many sorts and comparing it to other existing\nimplementations. We thus show that weighted multi-path retains adequate\naccuracy even in high-variability scenarios where the other algorithms are\ndiverging significantly from the correct values.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 04:10:02 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Audrito", "Giorgio", "", "University of Torino"], ["Bergamini", "Sergio", "", "University of Torino"]]}, {"id": "1802.01818", "submitter": "Md Muzakkir Hussain", "authors": "Md. Muzakkir Hussain, Mohammad Saad Alam, M.M. Sufyan Beg", "title": "Fog Computing in IoT Aided Smart Grid Transition- Requirements,\n  Prospects, Status Quos and Challenges", "comments": "13 Pages, 1 table, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to unfolded developments in both the IT sectors viz. Intelligent\nTransportation and Information Technology contemporary Smart Grid (SG) systems\nare leveraged with smart devices and entities. Such infrastructures when\nbestowed with the Internet of Things (IoT) and sensor network make a universe\nof objects active and online. The traditional cloud deployment succumbs to meet\nthe analytics and computational exigencies decentralized, dynamic cum\nresource-time critical SG ecosystems. This paper synoptically inspects to what\nextent the cloud computing utilities can satisfy the mission-critical\nrequirements of SG ecosystems and which subdomains and services call for fog\nbased computing archetypes. The objective of this work is to comprehend the\napplicability of fog computing algorithms to interplay with the core centered\ncloud computing support, thus enabling to come up with a new breed of real-time\nand latency free SG services. The work also highlights the opportunities\nbrought by fog based SG deployments. Correspondingly, we also highlight the\nchallenges and research thrusts elucidated towards the viability of fog\ncomputing for successful SG Transition.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 06:49:05 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Hussain", "Md. Muzakkir", ""], ["Alam", "Mohammad Saad", ""], ["Beg", "M. M. Sufyan", ""]]}, {"id": "1802.01870", "submitter": "Xiao-Liang Wang", "authors": "Zhi Wang, Xiaoliang Wang, Zhuzhong Qian, Baoliu Ye, Sanglu Lu", "title": "RDMAvisor: Toward Deploying Scalable and Simple RDMA as a Service in\n  Datacenters", "comments": "submitted to USENIX ATC 2017, No.9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDMA is increasingly adopted by cloud computing platforms to provide low CPU\noverhead, low latency, high throughput network services. On the other hand,\nhowever, it is still challenging for developers to realize fast deployment of\nRDMA-aware applications in the datacenter, since the performance is highly\nrelated to many lowlevel details of RDMA operations. To address this problem,\nwe present a simple and scalable RDMA as Service (RaaS) to mitigate the impact\nof RDMA operational details. RaaS provides careful message buffer management to\nimprove CPU/memory utilization and improve the scalability of RDMA operations.\nThese optimized designs lead to simple and flexible programming model for\ncommon and knowledgeable users. We have implemented a prototype of RaaS, named\nRDMAvisor, and evaluated its performance on a cluster with a large number of\nconnections. Our experiment results demonstrate that RDMAvisor achieves high\nthroughput for thousand of connections and maintains low CPU and memory\noverhead through adaptive RDMA transport selection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 10:09:12 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Wang", "Zhi", ""], ["Wang", "Xiaoliang", ""], ["Qian", "Zhuzhong", ""], ["Ye", "Baoliu", ""], ["Lu", "Sanglu", ""]]}, {"id": "1802.01960", "submitter": "Bogdan Oancea", "authors": "Bogdan Oancea and Richard Pospisil", "title": "The performances of R GPU implementations of the GMRES method", "comments": null, "journal-ref": "Romanian Statistical Review, no 1, 2018, pp. 121-132", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although the performance of commodity computers has improved drastically with\nthe introduction of multicore processors and GPU computing, the standard R\ndistribution is still based on single-threaded model of computation, using only\na small fraction of the computational power available now for most desktops and\nlaptops. Modern statistical software packages rely on high performance\nimplementations of the linear algebra routines there are at the core of several\nimportant leading edge statistical methods. In this paper we present a GPU\nimplementation of the GMRES iterative method for solving linear systems. We\ncompare the performance of this implementation with a pure single threaded\nversion of the CPU. We also investigate the performance of our implementation\nusing different GPU packages available now for R such as gmatrix, gputools or\ngpuR which are based on CUDA or OpenCL frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 14:26:38 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Oancea", "Bogdan", ""], ["Pospisil", "Richard", ""]]}, {"id": "1802.02098", "submitter": "Camille Negrello", "authors": "Camille Negrello (LMT), Pierre Gosselet (LMT), Christian Rey", "title": "A new impedance accounting for short and long range effects in mixed\n  substructured formulations of nonlinear problems", "comments": null, "journal-ref": "International Journal for Numerical Methods in Engineering, Wiley,\n  2018, 10.1002/nme.5195", "doi": "10.1002/nme.5195", "report-no": null, "categories": "math.NA cs.DC cs.NA physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient method for solving large nonlinear problems combines Newton\nsolvers and Domain Decomposition Methods (DDM). In the DDM framework, the\nboundary conditions can be chosen to be primal, dual or mixed. The mixed\napproach presents the advantage to be eligible for the research of an optimal\ninterface parameter (often called impedance) which can increase the convergence\nrate. The optimal value for this parameter is often too expensive to be\ncomputed exactly in practice: an approximate version has to be sought for,\nalong with a compromise between efficiency and computational cost. In the\ncontext of parallel algorithms for solving nonlinear structural mechanical\nproblems, we propose a new heuristic for the impedance which combines short and\nlong range effects at a low computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 10:45:19 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Negrello", "Camille", "", "LMT"], ["Gosselet", "Pierre", "", "LMT"], ["Rey", "Christian", ""]]}, {"id": "1802.02573", "submitter": "Nandita Vijaykumar", "authors": "Nandita Vijaykumar, Kevin Hsieh, Gennady Pekhimenko, Samira Khan,\n  Ashish Shrestha, Saugata Ghose, Phillip B. Gibbons, Onur Mutlu", "title": "Zorua: Enhancing Programming Ease, Portability, and Performance in GPUs\n  by Decoupling Programming Models from Resource Management", "comments": null, "journal-ref": null, "doi": null, "report-no": "SAFARI Technical Report 2016-005", "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application resource specification--a static specification of several\nparameters such as the number of threads and the scratchpad memory usage per\nthread block--forms a critical component of the existing GPU programming\nmodels. This specification determines the performance of the application during\nexecution because the corresponding on-chip hardware resources are allocated\nand managed purely based on this specification. This tight coupling between the\nsoftware-provided resource specification and resource management in hardware\nleads to significant challenges in programming ease, portability, and\nperformance, as we demonstrate in this work.\n  Our goal in this work is to reduce the dependence of performance on the\nsoftware-provided resource specification to simultaneously alleviate the above\nchallenges. To this end, we introduce Zorua, a new resource virtualization\nframework, that decouples the programmer-specified resource usage of a GPU\napplication from the actual allocation in the on-chip hardware resources. Zorua\nenables this decoupling by virtualizing each resource transparently to the\nprogrammer.\n  We demonstrate that by providing the illusion of more resources than\nphysically available, Zorua offers several important benefits: (i) Programming\nEase: Zorua eases the burden on the programmer to provide code that is tuned to\nefficiently utilize the physically available on-chip resources. (ii)\nPortability: Zorua alleviates the necessity of re-tuning an application's\nresource usage when porting the application across GPU generations. (iii)\nPerformance: By dynamically allocating resources and carefully oversubscribing\nthem when necessary, Zorua improves or retains the performance of applications\nthat are already highly tuned to best utilize the resources. The holistic\nvirtualization provided by Zorua has many other potential uses which we\ndescribe in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 20:07:48 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Vijaykumar", "Nandita", ""], ["Hsieh", "Kevin", ""], ["Pekhimenko", "Gennady", ""], ["Khan", "Samira", ""], ["Shrestha", "Ashish", ""], ["Ghose", "Saugata", ""], ["Gibbons", "Phillip B.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1802.02652", "submitter": "Christopher Meiklejohn", "authors": "Christopher Meiklejohn, Heather Miller", "title": "Partisan: Enabling Cloud-Scale Erlang Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an alternative distribution layer for Erlang, named\nPartisan. Partisan is a topology-agnostic distributed programming model and\ndistribution layer that supports several network topologies for different\napplication scenarios: full mesh, peer-to-peer, client-server, and\npublish-subscribe. Partisan allows application developers to specify the\nnetwork topology at runtime, rather than encoding topology-specific concerns\ninto application code. Partisan additionally adds support for more channels,\nenabling users to distribute messages over multiple channels, sometimes in\nparallel.\n  We implement and evaluate Partisan in the Erlang programming language and use\nit in the evaluation of three scenarios. The first scenario compares the raw\nperformance between Distributed Erlang and Partisan, and shows that Partisan\nperforms on par with or better than Distributed Erlang. The second scenario\ndemonstrates that distributing traffic over multiple connections enables\nPartisan to perform up to 18x better under normal conditions, and up to 30x\nbetter in situations with network congestion and high concurrency. The third\nscenario demonstrates, using existing applications, that configuring the\ntopology at runtime allows applications to perform up to 13.5x better or scale\nto clusters of thousands of nodes over the general-purpose runtime distribution\nlayer.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 21:54:24 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Meiklejohn", "Christopher", ""], ["Miller", "Heather", ""]]}, {"id": "1802.02681", "submitter": "Christopher Meiklejohn", "authors": "Christopher S. Meiklejohn, Peter Van Roy", "title": "Towards A Systems Approach To Distributed Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is undeniable that most developers today are building distributed\napplications. However, most of these applications are developed by composing\nexisting systems together through unspecified APIs exposed to the application\ndeveloper. Systems are not going away: they solve a particular problem and most\napplications today need to rely on several of these systems working in concert.\nGiven this, we propose a research direction where higher-level languages with\nwell defined semantics target underlying systems infrastructure as a\nmiddle-ground.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 01:01:45 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Meiklejohn", "Christopher S.", ""], ["Van Roy", "Peter", ""]]}, {"id": "1802.02765", "submitter": "Sebastian Eibl", "authors": "Sebastian Eibl and Ulrich R\\\"ude", "title": "A local parallel communication algorithm for polydisperse rigid body\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of large ensembles of particles is usually parallelized by\npartitioning the domain spatially and using message passing to communicate\nbetween the processes handling neighboring subdomains. The particles are\nrepresented as individual geometric objects and are associated to the\nsubdomains. Handling collisions and migrating particles between subdomains, as\nrequired for proper parallel execution, requires a complex communication\nprotocol. Typically, the parallelization is restricted to handling only\nparticles that are smaller than a subdomain. In many applications, however,\nparticle sizes may vary drastically with some of them being larger than a\nsubdomain. In this article we propose a new communication and synchronization\nalgorithm that can handle the parallelization without size restrictions on the\nparticles. Despite the additional complexity and extended functionality, the\nnew algorithm introduces only minimal overhead. We demonstrate the scalability\nof the previous and the new communication algorithms up to almost two million\nparallel processes and for handling ten billion (1e10) geometrically resolved\nparticles on a state-of-the-art petascale supercomputer. Different scenarios\nare presented to analyze the performance of the new algorithm and to\ndemonstrate its capability to simulate polydisperse scenarios, where large\nindividual particles can extend across several subdomains.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 09:25:24 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 14:29:58 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Eibl", "Sebastian", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1802.02919", "submitter": "Istv\\'an M\\'odos", "authors": "Istv\\'an M\\'odos, P\\v{r}emysl \\v{S}\\r{u}cha, Roman V\\'aclav\\'ik, Jan\n  Smejkal, Zden\\v{e}k Hanz\\'alek", "title": "Adaptive online scheduling of tasks with anytime property on\n  heterogeneous resources", "comments": null, "journal-ref": "I. Modos, P. Sucha, R. Vaclavik, J. Smejkal, and Z. Hanzalek.\n  Adaptive online scheduling of tasks with anytime property on heterogeneous\n  resources. Computers Operations Research, 76:95 - 117, 2016", "doi": "10.1016/j.cor.2016.06.008", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An acceptable response time of a server is an important aspect in many\nclient-server applications; this is evident in situations in which the server\nis overloaded by many computationally intensive requests. In this work, we\nconsider that the requests, or in this case tasks, generated by the clients are\ninstances of optimization problems solved by anytime algorithms, i.e. the\nquality of the solution increases with the processing time of a task. These\ntasks are submitted to the server which schedules them to the available\ncomputational resources where the tasks are processed. To tackle the overload\nproblem, we propose a scheduling algorithm which combines traditional\nscheduling approaches with a quality control heuristic which adjusts the\nrequested quality of the solutions and thus changes the processing time of the\ntasks. Two efficient quality control heuristics are introduced: the first\nheuristic sets a global quality for all tasks, whereas the second heuristic\nsets the quality for each task independently. Moreover, in practice, the\nrelationship between the processing time and the quality is not known a priori.\nBecause it is crucial for scheduling algorithms to know at least the estimation\nof these relationships, we propose a general procedure for estimating these\nrelationships using information obtained from the already executed tasks.\nFinally, the performance of the proposed scheduling algorithm is demonstrated\non a real-world problem from the domain of personnel rostering with very good\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 15:27:19 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["M\u00f3dos", "Istv\u00e1n", ""], ["\u0160\u016fcha", "P\u0159emysl", ""], ["V\u00e1clav\u00edk", "Roman", ""], ["Smejkal", "Jan", ""], ["Hanz\u00e1lek", "Zden\u011bk", ""]]}, {"id": "1802.03013", "submitter": "Vi Tran", "authors": "Phuong Hoai Ha, Vi Ngoc-Nha Tran, Ibrahim Umar, Aras Atalar, Anders\n  Gidenstam, Paul Renaud-Goud, Philippas Tsigas, Ivan Walulya", "title": "D2.4 Report on the final prototype of programming abstractions for\n  energy-efficient inter-process communication", "comments": "146 pages. arXiv admin note: text overlap with arXiv:1611.05793,\n  arXiv:1605.08222", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work package 2 (WP2) aims to develop libraries for energy-efficient\ninter-process communication and data sharing on the EXCESS platforms. The\nDeliverable D2.4 reports on the final prototype of programming abstractions for\nenergy-efficient inter- process communication. Section 1 is the updated\noverview of the prototype of programming abstraction and devised power/energy\nmodels. The Section 2-6 contain the latest results of the four studies: i)\nGreenBST, a energy-efficient and concurrent search tree (cf. Section 2) ii)\nCustomization methodology for implementation of streaming aggregation in\nembedded systems (cf. Section 3) iii) Energy Model on CPU for Lock-free\nData-structures in Dynamic Environments (cf. Section 4.10) iv) A General and\nValidated Energy Complexity Model for Multithreaded Algorithms (cf. Section 5)\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 11:10:38 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Ha", "Phuong Hoai", ""], ["Tran", "Vi Ngoc-Nha", ""], ["Umar", "Ibrahim", ""], ["Atalar", "Aras", ""], ["Gidenstam", "Anders", ""], ["Renaud-Goud", "Paul", ""], ["Tsigas", "Philippas", ""], ["Walulya", "Ivan", ""]]}, {"id": "1802.03152", "submitter": "Yan Zhao", "authors": "Yan Zhao, Hongwei Liu, Yan Wang, Zhan Zhang, Decheng Zuo", "title": "Reducing the Upfront Cost of Private Clouds with Clairvoyant Virtual\n  Machine Placement", "comments": "Submitted to Journal of Supercomputing on 27th June, 2018, Revised on\n  12th December, 2018, Accepted on 15th December, 2018", "journal-ref": "The Journal of Supercomputing, 2019, 75(3)", "doi": "10.1007/s11227-018-02730-4", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although public clouds still occupy the largest portion of the total cloud\ninfrastructure, private clouds are attracting increasing interest from both\nindustry and academia because of their better security and privacy control.\nAccording to the existing studies, the high upfront cost is among the most\ncritical challenges associated with private clouds. To reduce cost and improve\nperformance, virtual machine placement (VMP) methods have been extensively\ninvestigated, however, few of these methods have focused on private clouds.\nThis paper proposes a heterogeneous and multidimensional clairvoyant dynamic\nbin packing (CDBP) model, in which the scheduler can conduct more efficient VMP\nprocesses using additional information on the arrival time and duration of\nvirtual machines to reduce the datacenter scale and thereby decrease the\nupfront cost of private clouds. In addition, a novel branch-and-bound algorithm\nwith a divide-and-conquer strategy (DCBB) is proposed to effectively and\nefficiently handle the derived problem. One state-of-the-art and several\nclassic VMP methods are also modified to adapt to the proposed model to observe\ntheir performance and compare with our proposed algorithm. Extensive\nexperiments are conducted on both real-world and synthetic workloads to\nevaluate the accuracy and efficiency of the algorithms. The experimental\nresults demonstrate that DCBB delivers near-optimal solutions with a\nconvergence rate that is much faster than those of the other search-based\nalgorithms evaluated. In particular, DCBB yields the optimal solution for a\nreal-world workload with an execution time that is an order of magnitude\nshorter than that required by the original branch-and-bound (BB) algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 07:15:07 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 10:27:16 GMT"}, {"version": "v3", "created": "Sat, 22 Dec 2018 02:03:18 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zhao", "Yan", ""], ["Liu", "Hongwei", ""], ["Wang", "Yan", ""], ["Zhang", "Zhan", ""], ["Zuo", "Decheng", ""]]}, {"id": "1802.03159", "submitter": "Jan Seeger", "authors": "Jan Seeger, Rohit A. Deshmukh and Arne Br\\\"oring", "title": "Running Distributed and Dynamic IoT Choreographies", "comments": "Submitted to GIoTS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT systems are growing larger and larger and are becoming suitable for basic\nautomation tasks. One of the features IoT automation systems can provide is\ndealing with a dynamic system -- Devices leaving and joining the system during\noperation. Additionally, IoT automation systems operate in a decentralized\nmanner. Current commercial automation systems have difficulty providing these\nfeatures. Integrating new devices into an automation system takes manual\nintervention. Additionally, automation systems also require central entities to\norchestrate the operation of participants. With smarter sensors and actors, we\ncan move control operations into software deployed on a decentralized network\nof devices, and provide support for dynamic systems. In this paper, we present\na framework for automation systems that demonstrates these two properties\n(distributed and dynamic). We represent applications as semantically described\ndata flows that are run decentrally on participating devices, and connected at\nruntime via rules. This allows integrating new devices into applications\nwithout manual interaction and removes central controllers from the equation.\nThis approach provides similar features to current automation systems (central\nengineering, multiple instantiation of applications), but enables distributed\nand dynamic operation. We demonstrate satisfying performance of the system via\na quantitative evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 07:51:32 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Seeger", "Jan", ""], ["Deshmukh", "Rohit A.", ""], ["Br\u00f6ring", "Arne", ""]]}, {"id": "1802.03160", "submitter": "Michal Dory", "authors": "Keren Censor-Hillel, Michal Dory", "title": "Distributed Spanner Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the fundamental network design problem of constructing approximate\nminimum spanners. Our contributions are for the distributed setting, providing\nboth algorithmic and hardness results.\n  Our main hardness result shows that an $\\alpha$-approximation for the minimum\ndirected $k$-spanner problem for $k \\geq 5$ requires $\\Omega(n\n/\\sqrt{\\alpha}\\log{n})$ rounds using deterministic algorithms or\n$\\Omega(\\sqrt{n }/\\sqrt{\\alpha}\\log{n})$ rounds using randomized ones, in the\nCONGEST model of distributed computing. Combined with the constant-round\n$O(n^{\\epsilon})$-approximation algorithm in the LOCAL model of [Barenboim,\nElkin and Gavoille, 2016], as well as a polylog-round\n$(1+\\epsilon)$-approximation algorithm in the LOCAL model that we show here,\nour lower bounds for the CONGEST model imply a strict separation between the\nLOCAL and CONGEST models. Notably, to the best of our knowledge, this is the\nfirst separation between these models for a local approximation problem.\n  Similarly, a separation between the directed and undirected cases is implied.\nWe also prove a nearly-linear lower bound for the minimum weighted $k$-spanner\nproblem for $k \\geq 4$, and we show lower bounds for the weighted 2-spanner\nproblem.\n  On the algorithmic side, apart from the aforementioned\n$(1+\\epsilon)$-approximation algorithm for minimum $k$-spanners, our main\ncontribution is a new distributed construction of minimum 2-spanners that uses\nonly polynomial local computations. Our algorithm has a guaranteed\napproximation ratio of $O(\\log(m/n))$ for a graph with $n$ vertices and $m$\nedges, which matches the best known ratio for polynomial time sequential\nalgorithms [Kortsarz and Peleg, 1994], and is tight if we restrict ourselves to\npolynomial local computations. Our approach allows us to extend our algorithm\nto work also for the directed, weighted, and client-server variants of the\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 08:01:28 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Dory", "Michal", ""]]}, {"id": "1802.03316", "submitter": "Jose Nunez-Yanez Dr", "authors": "Jose Nunez-Yanez, Mohammad Hosseinabady, Moslem Amiri, Andr\\'es\n  Rodr\\'iguez, Rafael Asenjo, Angeles Navarro, Rub\\'en Gran-Tejero and Dar\\'io\n  Su\\'arez-Gracia", "title": "Parallelizing Workload Execution in Embedded and High-Performance\n  Heterogeneous Systems", "comments": "Presented at HIP3ES, 2018", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2018/2", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a software-defined framework that enables the\nparallel utilization of all the programmable processing resources available in\nheterogeneous system-on-chip (SoC) including FPGA-based hardware accelerators\nand programmable CPUs. Two platforms with different architectures are\nconsidered, and a single C/C++ source code is used in both of them for the CPU\nand FPGA resources. Instead of simply using the hardware accelerator to offload\na task from the CPU, we propose a scheduler that dynamically distributes the\ntasks among all the resources to fully exploit all computing devices while\nminimizing load unbalance. The multi-architecture study compares an ARMV7 and\nARMV8 implementation with different number and type of CPU cores and also\ndifferent FPGA micro-architecture and size. We measure that both platforms\nbenefit from having the CPU cores assist FPGA execution at the same level of\nenergy requirements.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 15:52:15 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Nunez-Yanez", "Jose", ""], ["Hosseinabady", "Mohammad", ""], ["Amiri", "Moslem", ""], ["Rodr\u00edguez", "Andr\u00e9s", ""], ["Asenjo", "Rafael", ""], ["Navarro", "Angeles", ""], ["Gran-Tejero", "Rub\u00e9n", ""], ["Su\u00e1rez-Gracia", "Dar\u00edo", ""]]}, {"id": "1802.03430", "submitter": "Sinong Wang", "authors": "Sinong Wang, Jiashang Liu and Ness Shroff", "title": "Coded Sparse Matrix Multiplication", "comments": "new comparisons with existing sparse codes are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a large-scale and distributed matrix multiplication problem\n$C=A^{\\intercal}B$, where $C\\in\\mathbb{R}^{r\\times t}$, the coded computation\nplays an important role to effectively deal with \"stragglers\" (distributed\ncomputations that may get delayed due to few slow or faulty processors).\nHowever, existing coded schemes could destroy the significant sparsity that\nexists in large-scale machine learning problems, and could result in much\nhigher computation overhead, i.e., $O(rt)$ decoding time. In this paper, we\ndevelop a new coded computation strategy, we call \\emph{sparse code}, which\nachieves near \\emph{optimal recovery threshold}, \\emph{low computation\noverhead}, and \\emph{linear decoding time} $O(nnz(C))$. We implement our scheme\nand demonstrate the advantage of the approach over both uncoded and current\nfastest coded strategies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 19:49:36 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 21:18:04 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Wang", "Sinong", ""], ["Liu", "Jiashang", ""], ["Shroff", "Ness", ""]]}, {"id": "1802.03475", "submitter": "Min Ye", "authors": "Min Ye and Emmanuel Abbe", "title": "Communication-Computation Efficient Gradient Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops coding techniques to reduce the running time of\ndistributed learning tasks. It characterizes the fundamental tradeoff to\ncompute gradients (and more generally vector summations) in terms of three\nparameters: computation load, straggler tolerance and communication cost. It\nfurther gives an explicit coding scheme that achieves the optimal tradeoff\nbased on recursive polynomial constructions, coding both across data subsets\nand vector components. As a result, the proposed scheme allows to minimize the\nrunning time for gradient computations. Implementations are made on Amazon EC2\nclusters using Python with mpi4py package. Results show that the proposed\nscheme maintains the same generalization error while reducing the running time\nby $32\\%$ compared to uncoded schemes and $23\\%$ compared to prior coded\nschemes focusing only on stragglers (Tandon et al., ICML 2017).\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 23:13:24 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ye", "Min", ""], ["Abbe", "Emmanuel", ""]]}, {"id": "1802.03478", "submitter": "Bing Li", "authors": "Bing Li", "title": "Programming Requests/Responses with GreatFree in the Cloud Environment", "comments": "20 pages, 16 listings, 4 figures, 4 tables, International Journal of\n  Distributed and Parallel Systems, 2018", "journal-ref": null, "doi": "10.5121/ijdps.2018.9101", "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Programming request with GreatFree is an efficient programming technique to\nimplement distributed polling in the cloud computing environment. GreatFree is\na distributed programming environment through which diverse distributed systems\ncan be established through programming rather than configuring or scripting.\nGreatFree emphasizes the importance of programming since it offers developers\nthe opportunities to leverage their distributed knowledge and programming\nskills. Additionally, programming is the unique way to construct creative,\nadaptive and flexible systems to accommodate various distributed computing\nenvironments. With the support of GreatFree code-level Distributed\nInfrastructure Patterns, Distributed Operation Patterns and APIs, the difficult\nprocedure is accomplished in a programmable, rapid and highly-patterned manner,\ni.e., the programming behaviors are simplified as the repeatable operation of\nCopy-Paste-Replace. Since distributed polling is one of the fundamental\ntechniques to construct distributed systems, GreatFree provides developers with\nrelevant APIs and patterns to program requests/responses in the novel\nprogramming environment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 23:48:44 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Li", "Bing", ""]]}, {"id": "1802.03565", "submitter": "Nane Kratzke", "authors": "Nane Kratzke", "title": "About being the Tortoise or the Hare? - A Position Paper on Making Cloud\n  Applications too Fast and Furious for Attackers", "comments": "Preprint of CLOSER 2018 position paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud applications expose - beside service endpoints - also potential or\nactual vulnerabilities. And attackers have several advantages on their side.\nThey can select the weapons, the point of time and the point of attack. Very\noften cloud application security engineering efforts focus to harden the\nfortress walls but seldom assume that attacks may be successful. So, cloud\napplications rely on their defensive walls but seldom attack intruders\nactively. Biological systems are different. They accept that defensive \"walls\"\ncan be breached at several layers and therefore make use of an active and\nadaptive defense system to attack potential intruders - an immune system. This\nposition paper proposes such an immune system inspired approach to ensure that\neven undetected intruders can be purged out of cloud applications. This makes\nit much harder for intruders to maintain a presence on victim systems.\nEvaluation experiments with popular cloud service infrastructures (Amazon Web\nServices, Google Compute Engine, Azure and OpenStack) showed that this could\nminimize the undetected acting period of intruders down to minutes.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 10:16:20 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Kratzke", "Nane", ""]]}, {"id": "1802.03589", "submitter": "Ibrahim Riza Hallac", "authors": "Galip Aydin, Ibrahim Riza Hallac", "title": "Distributed Log Analysis on the Cloud Using MapReduce", "comments": null, "journal-ref": null, "doi": "10.17559/TV-20141006121105", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our work on designing a web based, distributed data\nanalysis system based on the popular MapReduce framework deployed on a small\ncloud; developed specifically for analyzing web server logs. The log analysis\nsystem consists of several cluster nodes, it splits the large log files on a\ndistributed file system and quickly processes them using MapReduce programming\nmodel. The cluster is created using an open source cloud infrastructure, which\nallows us to easily expand the computational power by adding new nodes. This\ngives us the ability to automatically resize the cluster according to the data\nanalysis requirements. We implemented MapReduce programs for basic log analysis\nneeds like frequency analysis, error detection, busy hour detection etc. as\nwell as more complex analyses which require running several jobs. The system\ncan automatically identify and analyze several web server log types such as\nApache, IIS, Squid etc. We use open source projects for creating the cloud\ninfrastructure and running MapReduce jobs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 13:42:09 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Aydin", "Galip", ""], ["Hallac", "Ibrahim Riza", ""]]}, {"id": "1802.03597", "submitter": "Ibrahim Riza Hallac", "authors": "Galip Aydin, Ibrahim Riza Hallac", "title": "Document Classification Using Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": "10.15224/978-1-63248-044-6-129", "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the performance and success rates of Na\\\"ive\nBayes Classification Algorithm for automatic classification of Turkish news\ninto predetermined categories like economy, life, health etc. We use Apache Big\nData technologies such as Hadoop, HDFS, Spark and Mahout, and apply these\ndistributed technologies to Machine Learning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 14:18:51 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Aydin", "Galip", ""], ["Hallac", "Ibrahim Riza", ""]]}, {"id": "1802.03603", "submitter": "Ibrahim Riza Hallac", "authors": "G\\\"ung\\\"or Yildirim, \\.Ibrahim R Hallac, Galip Aydin, Yetkin Tatar", "title": "Running genetic algorithms on Hadoop for solving high dimensional\n  optimization problems", "comments": null, "journal-ref": null, "doi": "10.1109/ICAICT.2015.7338506", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hadoop is a popular MapReduce framework for developing parallel applications\nin distributed environments. Several advantages of MapReduce such as\nprogramming ease and ability to use commodity hardware make the applicability\nof soft computing methods for parallel and distributed systems easier than\nbefore. In this paper, we present the results of an experimental study on\nrunning soft computing algorithms using Hadoop. This study shows how a simple\ngenetic algorithm running on Hadoop can be used to produce solutions for high\ndimensional optimization problems. In addition, a simple but effective\ntechnique, which did not need MapReduce chains, has been proposed.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 14:47:34 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Yildirim", "G\u00fcng\u00f6r", ""], ["Hallac", "\u0130brahim R", ""], ["Aydin", "Galip", ""], ["Tatar", "Yetkin", ""]]}, {"id": "1802.03604", "submitter": "Gong-Duo Zhang", "authors": "Gong-Duo Zhang, Shen-Yi Zhao, Hao Gao, Wu-Jun Li", "title": "Feature-Distributed SVRG for High-Dimensional Linear Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear classification has been widely used in many high-dimensional\napplications like text classification. To perform linear classification for\nlarge-scale tasks, we often need to design distributed learning methods on a\ncluster of multiple machines. In this paper, we propose a new distributed\nlearning method, called feature-distributed stochastic variance reduced\ngradient (FD-SVRG) for high-dimensional linear classification. Unlike most\nexisting distributed learning methods which are instance-distributed, FD-SVRG\nis feature-distributed. FD-SVRG has lower communication cost than other\ninstance-distributed methods when the data dimensionality is larger than the\nnumber of data instances. Experimental results on real data demonstrate that\nFD-SVRG can outperform other state-of-the-art distributed methods for\nhigh-dimensional linear classification in terms of both communication cost and\nwall-clock time, when the dimensionality is larger than the number of instances\nin training data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 14:53:57 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Zhang", "Gong-Duo", ""], ["Zhao", "Shen-Yi", ""], ["Gao", "Hao", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1802.03606", "submitter": "Ibrahim Riza Hallac", "authors": "Galip Aydin, Ibrahim Riza Hallac", "title": "Distributed NLP", "comments": "Presented at Third International Symposium on Innovative Technologies\n  in Engineering and Science 3-5 June, 2015, Valencia, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the performance of parallel text processing with Map\nReduce on a cloud platform. Scientific papers in Turkish language are processed\nusing Zemberek NLP library. Experiments were run on a Hadoop cluster and\ncompared with the single machines performance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 15:08:56 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Aydin", "Galip", ""], ["Hallac", "Ibrahim Riza", ""]]}, {"id": "1802.03641", "submitter": "Yehia Elkhatib PhD", "authors": "Yehia Elkhatib and Faiza Samreen and Gordon S. Blair", "title": "Same Same, but Different: A Descriptive Differentiation of Intra-cloud\n  Iaas Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of cloud computing are overwhelmed with choice, even within the\nservices offered by one provider. As such, many users select cloud services\nbased on description alone. We investigate the degree to which such strategy is\noptimal. In this quantitative study, we investigate the services of 2 of major\nIaaS providers. We use 2 representative applications to obtain longitudinal\nobservations over 7 days of the week and over different times of the day,\ntotalling over 14,000 executions. We give evidence of significant variations of\nperformance offered within IaaS services, calling for brokers to use automated\nand adaptive decision making processes with means for incorporating expressive\nuser constraints.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 19:00:35 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Elkhatib", "Yehia", ""], ["Samreen", "Faiza", ""], ["Blair", "Gordon S.", ""]]}, {"id": "1802.03671", "submitter": "Jason Li", "authors": "Bernhard Haeupler, Jason Li", "title": "Faster Distributed Shortest Path Approximations via Shortcuts", "comments": "To appear in DISC 2018; 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long series of recent results and breakthroughs have led to faster and\nbetter distributed approximation algorithms for single source shortest paths\n(SSSP) and related problems in the CONGEST model. The runtime of all these\nalgorithms, however, is $\\tilde{\\Omega}(\\sqrt{n})$, regardless of the network\ntopology, even on nice networks with a (poly)logarithmic network diameter $D$.\nWhile this is known to be necessary for some pathological networks, most\ntopologies of interest are arguably not of this type.\n  We give the first distributed approximation algorithms for shortest paths\nproblems that adjust to the topology they are run on, thus achieving\nsignificantly faster running times on many topologies of interest. The running\ntime of our algorithms depends on and is close to $Q$, where $Q$ is the quality\nof the best shortcut that exists for the given topology. While $Q =\n\\tilde{\\Theta}(\\sqrt{n} + D)$ for pathological worst-case topologies, many\ntopologies of interest have $Q = \\tilde{\\Theta}(D)$, which results in near\ninstance optimal running times for our algorithm, given the trivial $\\Omega(D)$\nlower bound.\n  The problems we consider are as follows: (1) an approximate shortest path\ntree and SSSP distances, (2) a polylogarithmic size distance label for every\nnode such that from the labels of any two nodes alone one can determine their\ndistance (approximately), and (3) an (approximately) optimal flow for the\ntransshipment problem.\n  Our algorithms have a tunable tradeoff between running time and approximation\nratio. Our fastest algorithms have an arbitrarily good polynomial approximation\nguarantee and an essentially optimal $\\tilde{O}(Q)$ running time. On the other\nend of the spectrum, we achieve polylogarithmic approximations in $\\tilde{O}(Q\n\\cdot n^{\\epsilon})$ rounds for any $\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 00:50:14 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 22:17:16 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 22:05:25 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Li", "Jason", ""]]}, {"id": "1802.03700", "submitter": "Vaneet Aggarwal", "authors": "Ruijiu Mao and Vaneet Aggarwal and Mung Chiang", "title": "Stochastic Non-preemptive Co-flow Scheduling with Time-Indexed\n  Relaxation", "comments": "Some of the results have been fixed, mainly involving the CoV. The\n  changes compared to the previous version are minor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-flows model a modern scheduling setting that is commonly found in a\nvariety of applications in distributed and cloud computing. A stochastic\nco-flow task contains a set of parallel flows with randomly distributed sizes.\nFurther, many applications require non-preemptive scheduling of co-flow tasks.\nThis paper gives an approximation algorithm for stochastic non-preemptive\nco-flow scheduling. The proposed approach uses a time-indexed linear\nrelaxation, and uses its solution to come up with a feasible schedule. This\nalgorithm is shown to achieve a competitive ratio of\n$(2\\log{m}+1)(1+\\sqrt{m}\\Delta)(1+m{\\Delta}){(3+\\Delta)}/{2}$ for zero-release\ntimes, and $(2\\log{m}+1)(1+\\sqrt{m}\\Delta)(1+m\\Delta)(2+\\Delta)$ for general\nrelease times, where $\\Delta$ represents the upper bound of squared coefficient\nof variation of processing times, and $m$ is the number of servers.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 06:31:59 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 16:09:34 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Mao", "Ruijiu", ""], ["Aggarwal", "Vaneet", ""], ["Chiang", "Mung", ""]]}, {"id": "1802.03749", "submitter": "Andr\\'as Attila Sulyok", "authors": "Andr\\'as Attila Sulyok, G\\'abor D\\'aniel Balogh, Istv\\'an Zolt\\'an\n  Reguly and Gihan R. Mudalige", "title": "Locality Optimized Unstructured Mesh Algorithms on GPUs", "comments": "Number of pages: 36 Number of figures: 21 Submitted to JPDC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured-mesh based numerical algorithms such as finite volume and finite\nelement algorithms form an important class of applications for many scientific\nand engineering domains. The key difficulty in achieving higher performance\nfrom these applications is the indirect accesses that lead to data-races when\nparallelized. Current methods for handling such data-races lead to reduced\nparallelism and suboptimal performance. Particularly on modern many-core\narchitectures, such as GPUs, that has increasing core/thread counts, reducing\ndata movement and exploiting memory locality is vital for gaining good\nperformance.\n  In this work we present novel locality-exploiting optimizations for the\nefficient execution of unstructured-mesh algorithms on GPUs. Building on a\ntwo-layered coloring strategy for handling data races, we introduce novel\nreordering and partitioning techniques to further improve efficient execution.\nThe new optimizations are then applied to several well established\nunstructured-mesh applications, investigating their performance on NVIDIA's\nlatest P100 and V100 GPUs. We demonstrate significant speedups\n($1.1\\text{--}1.75\\times$) compared to the state-of-the-art. A range of\nperformance metrics are benchmarked including runtime, memory transactions,\nachieved bandwidth performance, GPU occupancy and data reuse factors and are\nused to understand and explain the key factors impacting performance. The\noptimized algorithms are implemented as an open-source software library and we\nillustrate its use for improving performance of existing or new\nunstructured-mesh applications.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 14:59:49 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 19:54:41 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 20:37:58 GMT"}, {"version": "v4", "created": "Sat, 27 Jul 2019 12:40:28 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Sulyok", "Andr\u00e1s Attila", ""], ["Balogh", "G\u00e1bor D\u00e1niel", ""], ["Reguly", "Istv\u00e1n Zolt\u00e1n", ""], ["Mudalige", "Gihan R.", ""]]}, {"id": "1802.03760", "submitter": "Khaled Ammar", "authors": "Khaled Ammar, Frank McSherry, Semih Salihoglu, Manas Joglekar", "title": "Distributed Evaluation of Subgraph Queries Using Worstcase Optimal\n  LowMemory Dataflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding and monitoring fixed-size subgraphs in a\ncontinually changing large-scale graph. We present the first approach that (i)\nperforms worst-case optimal computation and communication, (ii) maintains a\ntotal memory footprint linear in the number of input edges, and (iii) scales\ndown per-worker computation, communication, and memory requirements linearly as\nthe number of workers increases, even on adversarially skewed inputs.\n  Our approach is based on worst-case optimal join algorithms, recast as a\ndata-parallel dataflow computation. We describe the general algorithm and\nmodifications that make it robust to skewed data, prove theoretical bounds on\nits resource requirements in the massively parallel computing model, and\nimplement and evaluate it on graphs containing as many as 64 billion edges. The\nunderlying algorithm and ideas generalize from finding and monitoring subgraphs\nto the more general problem of computing and maintaining relational equi-joins\nover dynamic relations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 16:08:47 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ammar", "Khaled", ""], ["McSherry", "Frank", ""], ["Salihoglu", "Semih", ""], ["Joglekar", "Manas", ""]]}, {"id": "1802.03821", "submitter": "Ibrahim Riza Hallac", "authors": "Betul Karakus, Ibrahim Riza Hallac, Galip Aydin", "title": "Distributed Readability Analysis Of Turkish Elementary School Textbooks", "comments": "Proceedings of International Conference on Information Technology and\n  Computer Science July 11-12, 2015, ISBN:9788193137307", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The readability assessment deals with estimating the level of difficulty in\nreading texts.Many readability tests, which do not indicate execution\nefficiency, have been applied on specific texts to measure the reading grade\nlevel in science textbooks. In this paper, we analyze the content covered in\nelementary school Turkish textbooks by employing a distributed parallel\nprocessing framework based on popular MapReduce paradigm. We outline the\narchitecture of a distributed Big Data processing system which uses Hadoop for\nfull-text readability analysis. The readability scores of the textbooks and\nsystem performance measurements are also given in the paper.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 21:18:45 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Karakus", "Betul", ""], ["Hallac", "Ibrahim Riza", ""], ["Aydin", "Galip", ""]]}, {"id": "1802.03844", "submitter": "Pankaj Khanchandani", "authors": "Pankaj Khanchandani and Roger Wattenhofer", "title": "Reducing Compare-and-Swap to Consensus Number One Primitives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consensus number of an object is the maximum number of processes among\nwhich binary consensus can be solved using any number of instances of the\nobject and read-write registers. Herlihy [6] showed in his seminal work that if\nan object has a consensus number of n, then there is a universal construction\nfor a wait-free and linearizable implementation of any non-trivial concurrent\nobject or data structure that is shared among n processes. Thus, a\nsynchronization object such as compare-and-swap with an infinite consensus\nnumber and the corresponding instruction can be viewed as \"strong\". On the\nother hand, a synchronization object such as fetch-and-add with consensus\nnumber two and the corresponding fetch-and-add instruction can be viewed as\n\"weak\".\n  Ellen et al. [2] observed recently that an object supporting two weak\ninstructions can also achieve infinite consensus number like an object that\nsupports one strong instruction. Using Herlihy's universal construction, this\nimplies that ignoring concerns about efficiency, one can design any concurrent\ndata structure or algorithm using only weak instructions. However, is it\npossible that a combination of weak instructions is really powerful enough to\nefficiently replace a strong instruction, like compare-and-swap, without\nincurring a large overhead in time or space? In this paper, we answer this\nquestion by giving an O(1) time wait-free and linearizable implementation of a\ncompare-and-swap register shared among n processes using read-write registers\nand O(1) registers that support two synchronization primitives half-max and\nmax-write, each having consensus number one. Thus, any algorithm that solves\nsome arbitrary synchronization problem using read-write and compare-and-swap\nregisters can be transformed into an algorithm that has the same asymptotic\ntime complexity and only uses consensus number one instructions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 00:00:06 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 19:08:36 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 10:01:20 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Khanchandani", "Pankaj", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1802.03870", "submitter": "Nicholas Woolsey", "authors": "Nicholas Woolsey, Rong-Rong Chen and Mingyue Ji", "title": "A New Combinatorial Design of Coded Distributed Computing", "comments": "Submitted to ISIT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded distributed computing introduced by Li et al. in 2015 is an efficient\napproach to trade computing power to reduce the communication load in general\ndistributed computing frameworks such as MapReduce. In particular, Li et al.\nshow that increasing the computation load in the Map phase by a factor of $r$\ncan create coded multicasting opportunities to reduce the communication load in\nthe Reduce phase by the same factor. However, there are two major limitations\nin practice. First, it requires an exponentially large number of input files\n(data batches) when the number of computing nodes gets large. Second, it forces\nevery $s$ computing nodes to compute one Map function, which leads to a large\nnumber of Map functions required to achieve the promised gain. In this paper,\nwe make an attempt to overcome these two limitations by proposing a novel coded\ndistributed computing approach based on a combinatorial design. We demonstrate\nthat when the number of computing nodes becomes large, 1) the proposed approach\nrequires an exponentially less number of input files; 2) the required number of\nMap functions is also reduced exponentially. Meanwhile, the resulting\ncomputation-communication trade-off maintains the multiplicative gain compared\nto conventional uncoded unicast and achieves the information theoretic lower\nbound asymmetrically for some system parameters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 03:23:51 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Woolsey", "Nicholas", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "1802.04112", "submitter": "Swaminathan Gopalswamy", "authors": "Swaminathan Gopalswamy, Sivakumar Rathinam", "title": "Infrastructure Enabled Autonomy: A Distributed Intelligence Architecture\n  for Autonomous Vehicles", "comments": "submitted to the IEEE Intelligent Vehicles Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple studies have illustrated the potential for dramatic societal,\nenvironmental and economic benefits from significant penetration of autonomous\ndriving. However, all the current approaches to autonomous driving require the\nautomotive manufacturers to shoulder the primary responsibility and liability\nassociated with replacing human perception and decision making with automation,\npotentially slowing the penetration of autonomous vehicles, and consequently\nslowing the realization of the societal benefits of autonomous vehicles. We\npropose here a new approach to autonomous driving that will re-balance the\nresponsibility and liabilities associated with autonomous driving between\ntraditional automotive manufacturers, infrastructure players, and third-party\nplayers. Our proposed distributed intelligence architecture leverages the\nsignificant advancements in connectivity and edge computing in the recent\ndecades to partition the driving functions between the vehicle, edge computers\non the road side, and specialized third-party computers that reside in the\nvehicle. Infrastructure becomes a critical enabler for autonomy. With this\nInfrastructure Enabled Autonomy (IEA) concept, the traditional automotive\nmanufacturers will only need to shoulder responsibility and liability\ncomparable to what they already do today, and the infrastructure and\nthird-party players will share the added responsibility and liabilities\nassociated with autonomous functionalities. We propose a Bayesian Network Model\nbased framework for assessing the risk benefits of such a distributed\nintelligence architecture. An additional benefit of the proposed architecture\nis that it enables \"autonomy as a service\" while still allowing for private\nownership of automobiles.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 23:33:53 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Gopalswamy", "Swaminathan", ""], ["Rathinam", "Sivakumar", ""]]}, {"id": "1802.04211", "submitter": "Claude Tadonki Dr. HDR", "authors": "Claude Tadonki", "title": "Basic Parallel and Distributed Computing Curriculum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of multi-core processors and their fast expansion, it is\nquite clear that {\\em parallel computing} is now a genuine requirement in\nComputer Science and Engineering (and related) curriculum. In addition to the\npervasiveness of parallel computing devices, we should take into account the\nfact that there are lot of existing softwares that are implemented in the\nsequential mode, and thus need to be adapted for a parallel execution.\nTherefore, it is required to the programmer to be able to design parallel\nprograms and also to have some skills in moving from a given sequential code to\nthe corresponding parallel code. In this paper, we present a basic educational\nscenario on how to give a consistent and efficient background in parallel\ncomputing to ordinary computer scientists and engineers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 17:55:24 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Tadonki", "Claude", ""]]}, {"id": "1802.04245", "submitter": "Augusto Amarilla", "authors": "Augusto Amarilla", "title": "Scalarization Methods for Many-Objective Virtual Machine Placement of\n  Elastic Infrastructures in Overbooked Cloud Computing Data Centers Under\n  Uncertainty", "comments": "71 pages, Informatics Engineering Final Work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cloud computing datacenters provide thousands to millions of virtual machines\n(VMs) on-demand in highly dynamic environments, requiring quick placement of\nrequested VMs into available physical machines (PMs). Due to the randomness of\ncustomer requests, the Virtual Machine Placement (VMP) should be formulated as\nan online optimization problem.\n  The first part of this work analyzes alternatives to solve the formulated\nproblem, an experimental comparison of five different online deterministic\nheuristics against an offline memetic algorithm with migration of VMs was\nperformed, considering several experimental workloads. Simulations indicate\nthat First-Fit Decreasing algorithm (A4) outperforms other evaluated heuristics\non average.\n  This work presents a two-phase schema formulation of a VMP problem\nconsidering the optimization of three objective functions in an IaaS\nenvironment with elasticity and overbooking capabilities. The two-phase schema\nformulation describes that the allocation of the VMs can be separated into two\nsub-problems, the incremental allocation (iVMP) and the reconfiguration of a\nplacement (VMPr).\n  To analyze alternatives to solve the formulated problem, an experimental\ncomparison of three different objective function scalarization methods as part\nof the iVMP and VMPr was performed considering several experimental workloads.\nSimulations indicate that the Euclidean distance to origin outperforms other\nevaluated scalarization methods on average.\n  In order to portray the dynamic nature of an IaaS environment a customizable\nworkload trace generator was developed to simulate uncertainty in the scenarios\nwith elasticity and overbooking of resources in VM requests.\n  Experimental results proved that the Euclidean distance is preferable over\nthe other scalarizatiom methods to improve the values of the power consumption\nobjective function.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:46:11 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Amarilla", "Augusto", ""]]}, {"id": "1802.04249", "submitter": "Kijung Shin", "authors": "Kijung Shin, Euiwoong Lee, Jinoh Oh, Mohammad Hammoud, Christos\n  Faloutsos", "title": "CoCoS: Fast and Accurate Distributed Triangle Counting in Graph Streams", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph stream, how can we estimate the number of triangles in it using\nmultiple machines with limited storage? Specifically, how should edges be\nprocessed and sampled across the machines for rapid and accurate estimation?\n  The count of triangles (i.e., cliques of size three) has proven useful in\nnumerous applications, including anomaly detection, community detection, and\nlink recommendation. For triangle counting in large and dynamic graphs, recent\nwork has focused largely on streaming algorithms and distributed algorithms but\nlittle on their combinations for \"the best of both worlds\".\n  In this work, we propose CoCoS, a fast and accurate distributed streaming\nalgorithm for estimating the counts of global triangles (i.e., all triangles)\nand local triangles incident to each node. Making one pass over the input\nstream, COCOS carefully processes and stores the edges across multiple machines\nso that the redundant use of computational and storage resources is minimized.\nCompared to baselines, CoCoS is (a) Accurate: giving up to 39X smaller\nestimation error, (b) Fast: up to 10.4X faster, scaling linearly with the size\nof the input stream, and (c) Theoretically sound: yielding unbiased estimates.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:57:57 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 06:28:17 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 22:05:29 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2020 07:48:15 GMT"}, {"version": "v5", "created": "Sat, 27 Feb 2021 06:23:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shin", "Kijung", ""], ["Lee", "Euiwoong", ""], ["Oh", "Jinoh", ""], ["Hammoud", "Mohammad", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1802.04434", "submitter": "Jeremy Bernstein", "authors": "Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, Anima\n  Anandkumar", "title": "signSGD: Compressed Optimisation for Non-Convex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large neural networks requires distributing learning across multiple\nworkers, where the cost of communicating gradients can be a significant\nbottleneck. signSGD alleviates this problem by transmitting just the sign of\neach minibatch stochastic gradient. We prove that it can get the best of both\nworlds: compressed gradients and SGD-level convergence rate. The relative\n$\\ell_1/\\ell_2$ geometry of gradients, noise and curvature informs whether\nsignSGD or SGD is theoretically better suited to a particular problem. On the\npractical side we find that the momentum counterpart of signSGD is able to\nmatch the accuracy and convergence speed of Adam on deep Imagenet models. We\nextend our theory to the distributed setting, where the parameter server uses\nmajority vote to aggregate gradient signs from each worker enabling 1-bit\ncompression of worker-server communication in both directions. Using a theorem\nby Gauss we prove that majority vote can achieve the same reduction in variance\nas full precision distributed SGD. Thus, there is great promise for sign-based\noptimisation schemes to achieve fast communication and fast convergence. Code\nto reproduce experiments is to be found at https://github.com/jxbz/signSGD .\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 02:14:35 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 18:01:27 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 18:55:19 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Bernstein", "Jeremy", ""], ["Wang", "Yu-Xiang", ""], ["Azizzadenesheli", "Kamyar", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1802.04450", "submitter": "Yu Jin", "authors": "Yu Jin, Joseph F. JaJa", "title": "A High Performance Implementation of Spectral Clustering on CPU-GPU\n  Platforms", "comments": "2016 IEEE International Parallel and Distributed Processing Symposium\n  Workshops (Parallel Computing and Optimization (PCO) workshop). Codes are\n  available on https://github.com/yuj-umd/fastsc", "journal-ref": null, "doi": "10.1109/IPDPSW.2016.79", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most popular graph clustering algorithms,\nwhich achieves the best performance for many scientific and engineering\napplications. However, existing implementations in commonly used software\nplatforms such as Matlab and Python do not scale well for many of the emerging\nBig Data applications. In this paper, we present a fast implementation of the\nspectral clustering algorithm on a CPU-GPU heterogeneous platform. Our\nimplementation takes advantage of the computational power of the multi-core CPU\nand the massive multithreading and SIMD capabilities of GPUs. Given the input\nas data points in high dimensional space, we propose a parallel scheme to build\na sparse similarity graph represented in a standard sparse representation\nformat. Then we compute the smallest $k$ eigenvectors of the Laplacian matrix\nby utilizing the reverse communication interfaces of ARPACK software and\ncuSPARSE library, where $k$ is typically very large. Moreover, we implement a\nvery fast parallelized $k$-means algorithm on GPUs. Our implementation is shown\nto be significantly faster compared to the best known Matlab and Python\nimplementations for each step. In addition, our algorithm scales to problems\nwith a very large number of clusters.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 03:08:22 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Jin", "Yu", ""], ["JaJa", "Joseph F.", ""]]}, {"id": "1802.04647", "submitter": "Niketan Pansare", "authors": "Niketan Pansare, Michael Dusenberry, Nakul Jindal, Matthias Boehm,\n  Berthold Reinwald, Prithviraj Sen", "title": "Deep Learning with Apache SystemML", "comments": "Accepted at SysML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enterprises operate large data lakes using Hadoop and Spark frameworks that\n(1) run a plethora of tools to automate powerful data\npreparation/transformation pipelines, (2) run on shared, large clusters to (3)\nperform many different analytics tasks ranging from model preparation,\nbuilding, evaluation, and tuning for both machine learning and deep learning.\nDeveloping machine/deep learning models on data in such shared environments is\nchallenging. Apache SystemML provides a unified framework for implementing\nmachine learning and deep learning algorithms in a variety of shared deployment\nscenarios. SystemML's novel compilation approach automatically generates\nruntime execution plans for machine/deep learning algorithms that are composed\nof single-node and distributed runtime operations depending on data and cluster\ncharacteristics such as data size, data sparsity, cluster size, and memory\nconfigurations, while still exploiting the capabilities of the underlying big\ndata frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 23:54:37 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Pansare", "Niketan", ""], ["Dusenberry", "Michael", ""], ["Jindal", "Nakul", ""], ["Boehm", "Matthias", ""], ["Reinwald", "Berthold", ""], ["Sen", "Prithviraj", ""]]}, {"id": "1802.04696", "submitter": "Damiano Carra", "authors": "Damiano Carra, Giovanni Neglia, Pietro Michiardi", "title": "Elastic Provisioning of Cloud Caches: a Cost-aware TTL Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider elastic resource provisioning in the cloud, focusing on in-memory\nkey-value stores used as caches. Our goal is to dynamically scale resources to\nthe traffic pattern minimizing the overall cost, which includes not only the\nstorage cost, but also the cost due to misses. In fact, a small variation on\nthe cache miss ratio may have a significant impact on user perceived\nperformance in modern web services, which in turn has an impact on the overall\nrevenues for the content provider that uses those services. We propose and\nstudy a dynamic algorithm for TTL caches, which is able to obtain\nclose-to-minimal costs. Since high-throughput caches require low complexity\noperations, we discuss a practical implementation of such a scheme requiring\nconstant overhead per request independently from the cache size. We evaluate\nour solution with real-world traces collected from Akamai, and show that we are\nable to obtain a 17% decrease in the overall cost compared to a baseline static\nconfiguration.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:09:02 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Carra", "Damiano", ""], ["Neglia", "Giovanni", ""], ["Michiardi", "Pietro", ""]]}, {"id": "1802.04766", "submitter": "Yueming Liu", "authors": "Peng Zhang, Yueming Liu and Meikang Qiu", "title": "SNC: A Cloud Service Platform for Symbolic-Numeric Computation using\n  Just-In-Time Compilation", "comments": "13 pages, 23 figures", "journal-ref": "IEEE Transactions on Cloud Computing, 2017", "doi": "10.1109/TCC.2017.2656088", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud services have been widely employed in IT industry and scientific\nresearch. By using Cloud services users can move computing tasks and data away\nfrom local computers to remote datacenters. By accessing Internet-based\nservices over lightweight and mobile devices, users deploy diversified Cloud\napplications on powerful machines. The key drivers towards this paradigm for\nthe scientific computing field include the substantial computing capacity,\non-demand provisioning and cross-platform interoperability. To fully harness\nthe Cloud services for scientific computing, however, we need to design an\napplication-specific platform to help the users efficiently migrate their\napplications. In this, we propose a Cloud service platform for symbolic-numeric\ncomputation - SNC. SNC allows the Cloud users to describe tasks as symbolic\nexpressions through C/C++, Python, Java APIs and SNC script. Just-In-Time (JIT)\ncompilation through using LLVM/JVM is used to compile the user code to the\nmachine code. We implemented the SNC design and tested a wide range of\nsymbolic-numeric computation applications (including nonlinear minimization,\nMonte Carlo integration, finite element assembly and multibody dynamics) on\nseveral popular cloud platforms (including the Google Compute Engine, Amazon\nEC2, Microsoft Azure, Rackspace, HP Helion and VMWare vCloud). These results\ndemonstrate that our approach can work across multiple cloud platforms, support\ndifferent languages and significantly improve the performance of\nsymbolic-numeric computation using cloud platforms. This offered a way to\nstimulate the need for using the cloud computing for the symbolic-numeric\ncomputation in the field of scientific research.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 20:20:14 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zhang", "Peng", ""], ["Liu", "Yueming", ""], ["Qiu", "Meikang", ""]]}, {"id": "1802.04780", "submitter": "David Dao", "authors": "David Dao, Dan Alistarh, Claudiu Musat, Ce Zhang", "title": "DataBright: Towards a Global Exchange for Decentralized Data Ownership\n  and Trusted Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is safe to assume that, for the foreseeable future, machine learning,\nespecially deep learning will remain both data- and computation-hungry. In this\npaper, we ask: Can we build a global exchange where everyone can contribute\ncomputation and data to train the next generation of machine learning\napplications?\n  We present an early, but running prototype of DataBright, a system that turns\nthe creation of training examples and the sharing of computation into an\ninvestment mechanism. Unlike most crowdsourcing platforms, where the\ncontributor gets paid when they submit their data, DataBright pays dividends\nwhenever a contributor's data or hardware is used by someone to train a machine\nlearning model. The contributor becomes a shareholder in the dataset they\ncreated. To enable the measurement of usage, a computation platform that\ncontributors can trust is also necessary. DataBright thus merges both a data\nmarket and a trusted computation market.\n  We illustrate that trusted computation can enable the creation of an AI\nmarket, where each data point has an exact value that should be paid to its\ncreator. DataBright allows data creators to retain ownership of their\ncontribution and attaches to it a measurable value. The value of the data is\ngiven by its utility in subsequent distributed computation done on the\nDataBright computation market. The computation market allocates tasks and\nsubsequent payments to pooled hardware. This leads to the creation of a\ndecentralized AI cloud. Our experiments show that trusted hardware such as\nIntel SGX can be added to the usual ML pipeline with no additional costs. We\nuse this setting to orchestrate distributed computation that enables the\ncreation of a computation market. DataBright is available for download at\nhttps://github.com/ds3lab/databright.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 18:20:07 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Dao", "David", ""], ["Alistarh", "Dan", ""], ["Musat", "Claudiu", ""], ["Zhang", "Ce", ""]]}, {"id": "1802.04789", "submitter": "Keren Censor-Hillel", "authors": "Keren Censor-Hillel, Dean Leitersdorf, Elia Turner", "title": "Sparse Matrix Multiplication and Triangle Listing in the Congested\n  Clique Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We multiply two $n \\times n$ matrices $S,T$ over semirings in the Congested\nClique model, where $n$ fully connected nodes communicate synchronously using\n$O(\\log n)$-bit messages, within $O(nz(S)^{1/3} nz(T)^{1/3}/n + 1)$ rounds of\ncommunication, where $nz(A)$ denotes the number of non-zero elements in a\nmatrix $A$. By leveraging the sparsity of the input matrices, our algorithm\ngreatly reduces communication compared with general algorithms [Censor-Hillel\net al., PODC 2015], improving upon the state-of-the-art for matrices with\n$o(n^2)$ non-zero elements. Our algorithm exhibits the additional strength of\nsurpassing previous solutions also when only one matrix is sparse. This allows\nefficiently raising a sparse matrix to a power greater than 2. As applications,\nwe speed up 4-cycle counting and APSP in sparse graphs.\n  Our algorithmic contribution is a new \\emph{deterministic} method of\nrestructuring the input matrices in a sparsity-aware manner, which assigns each\nnode with element-wise multiplication tasks that are not necessarily\nconsecutive but are balanced, yielding communication-efficient multiplication.\n  Moreover, this new deterministic method for restructuring matrices may be\nused to restructure the adjacency matrix of input graphs, enabling faster\nsolutions for graph related problems. As an example, we present a new\ndeterministic algorithm which solves the triangle listing problem in\n$O(m/n^{5/3} + 1)$ rounds, a complexity that was previously obtained by a\n\\emph{randomized} algorithm [Pandurangan et al., SPAA 2018] and matches the\nlower bound of $\\tilde{\\Omega}(n^{1/3})$ when $m=n^2$ of [Izumi and Le Gall,\nPODC 2017, Pandurangan et al., SPAA 2018].\n  Our triangle listing algorithm implies triangle counting with the same\ncomplexity of $O(m/n^{5/3} + 1)$ rounds, which is a \\emph{cubic} improvement\nover the previous $O(m^2/n^3)$-round algorithm [Dolev et al., DISC 2012].\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 18:47:15 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 06:02:53 GMT"}, {"version": "v3", "created": "Thu, 10 May 2018 17:35:36 GMT"}, {"version": "v4", "created": "Wed, 20 Mar 2019 19:05:40 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Leitersdorf", "Dean", ""], ["Turner", "Elia", ""]]}, {"id": "1802.04819", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Logan Stafman, Andrew Or, Michael J. Freedman", "title": "SLAQ: Quality-Driven Scheduling for Distributed Machine Learning", "comments": "Appeared in the 1st SysML Conference. Full paper published in ACM\n  SoCC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning (ML) models with large datasets can incur\nsignificant resource contention on shared clusters. This training typically\ninvolves many iterations that continually improve the quality of the model. Yet\nin exploratory settings, better models can be obtained faster by directing\nresources to jobs with the most potential for improvement. We describe SLAQ, a\ncluster scheduling system for approximate ML training jobs that aims to\nmaximize the overall job quality. When allocating cluster resources, SLAQ\nexplores the quality-runtime trade-offs across multiple jobs to maximize\nsystem-wide quality improvement. To do so, SLAQ leverages the iterative nature\nof ML training algorithms, by collecting quality and resource usage information\nfrom concurrent jobs, and then generating highly-tailored quality-improvement\npredictions for future iterations. Experiments show that SLAQ achieves an\naverage quality improvement of up to 73% and an average delay reduction of up\nto 44% on a large set of ML training jobs, compared to resource fairness\nschedulers.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 19:03:49 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Zhang", "Haoyu", ""], ["Stafman", "Logan", ""], ["Or", "Andrew", ""], ["Freedman", "Michael J.", ""]]}, {"id": "1802.04876", "submitter": "Weijie J. Su", "authors": "Weijie J. Su and Yuancheng Zhu", "title": "Uncertainty Quantification for Online Learning and Stochastic\n  Approximation via Hierarchical Incremental Gradient Descent", "comments": "Changed the title and polished writing. For more details, please\n  visit the HiGrad webpage http://stat.wharton.upenn.edu/~suw/higrad", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is an immensely popular approach for online\nlearning in settings where data arrives in a stream or data sizes are very\nlarge. However, despite an ever- increasing volume of work on SGD, much less is\nknown about the statistical inferential properties of SGD-based predictions.\nTaking a fully inferential viewpoint, this paper introduces a novel procedure\ntermed HiGrad to conduct statistical inference for online learning, without\nincurring additional computational cost compared with SGD. The HiGrad procedure\nbegins by performing SGD updates for a while and then splits the single thread\ninto several threads, and this procedure hierarchically operates in this\nfashion along each thread. With predictions provided by multiple threads in\nplace, a t-based confidence interval is constructed by decorrelating\npredictions using covariance structures given by a Donsker-style extension of\nthe Ruppert--Polyak averaging scheme, which is a technical contribution of\nindependent interest. Under certain regularity conditions, the HiGrad\nconfidence interval is shown to attain asymptotically exact coverage\nprobability. Finally, the performance of HiGrad is evaluated through extensive\nsimulation studies and a real data example. An R package higrad has been\ndeveloped to implement the method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:15:10 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 15:16:31 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Su", "Weijie J.", ""], ["Zhu", "Yuancheng", ""]]}, {"id": "1802.04924", "submitter": "Zhihao Jia", "authors": "Zhihao Jia, Sina Lin, Charles R. Qi, Alex Aiken", "title": "Exploring Hidden Dimensions in Parallelizing Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed growth in the computational requirements\nfor training deep convolutional neural networks. Current approaches parallelize\ntraining onto multiple devices by applying a single parallelization strategy\n(e.g., data or model parallelism) to all layers in a network. Although easy to\nreason about, these approaches result in suboptimal runtime performance in\nlarge-scale distributed training, since different layers in a network may\nprefer different parallelization strategies. In this paper, we propose\nlayer-wise parallelism that allows each layer in a network to use an individual\nparallelization strategy. We jointly optimize how each layer is parallelized by\nsolving a graph search problem. Our evaluation shows that layer-wise\nparallelism outperforms state-of-the-art approaches by increasing training\nthroughput, reducing communication costs, achieving better scalability to\nmultiple GPUs, while maintaining original network accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 02:00:40 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 16:19:03 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Jia", "Zhihao", ""], ["Lin", "Sina", ""], ["Qi", "Charles R.", ""], ["Aiken", "Alex", ""]]}, {"id": "1802.04949", "submitter": "Sheng Wang", "authors": "Sheng Wang, Tien Tuan Anh Dinh, Qian Lin, Zhongle Xie, Meihui Zhang,\n  Qingchao Cai, Gang Chen, Wanzeng Fu, Beng Chin Ooi, Pingcheng Ruan", "title": "ForkBase: An Efficient Storage Engine for Blockchain and Forkable\n  Applications", "comments": "15 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing data storage systems offer a wide range of functionalities to\naccommodate an equally diverse range of applications. However, new classes of\napplications have emerged, e.g., blockchain and collaborative analytics,\nfeaturing data versioning, fork semantics, tamper-evidence or any combination\nthereof. They present new opportunities for storage systems to efficiently\nsupport such applications by embedding the above requirements into the storage.\n  In this paper, we present ForkBase, a storage engine specifically designed to\nprovide efficient support for blockchain and forkable applications. By\nintegrating the core application properties into the storage, ForkBase not only\ndelivers high performance but also reduces development effort. Data in ForkBase\nis multi-versioned, and each version uniquely identifies the data content and\nits history. Two variants of fork semantics are supported in ForkBase to\nfacilitate any collaboration workflows. A novel index structure is introduced\nto efficiently identify and eliminate duplicate content across data objects.\nConsequently, ForkBase is not only efficient in performance, but also in space\nrequirement. We demonstrate the performance of ForkBase using three\napplications: a blockchain platform, a wiki engine and a collaborative\nanalytics application. We conduct extensive experimental evaluation of these\napplications against respective state-of-the-art system. The results show that\nForkBase achieves superior performance while significantly lowering the\ndevelopment cost.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 04:07:34 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Wang", "Sheng", ""], ["Dinh", "Tien Tuan Anh", ""], ["Lin", "Qian", ""], ["Xie", "Zhongle", ""], ["Zhang", "Meihui", ""], ["Cai", "Qingchao", ""], ["Chen", "Gang", ""], ["Fu", "Wanzeng", ""], ["Ooi", "Beng Chin", ""], ["Ruan", "Pingcheng", ""]]}, {"id": "1802.05055", "submitter": "Selen Gurbuz", "authors": "Selen Gurbuz, Galip Aydin", "title": "Classification of Scientific Papers With Big Data Technologies", "comments": "in Turkish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sizes that cannot be processed by conventional data storage and analysis\nsystems are named as Big Data.It also refers to nex technologies developed to\nstore, process and analyze large amounts of data. Automatic information\nretrieval about the contents of a large number of documents produced by\ndifferent sources, identifying research fields and topics, extraction of the\ndocument abstracts, or discovering patterns are some of the topics that have\nbeen studied in the field of big data.In this study, Naive Bayes classification\nalgorithm, which is run on a data set consisting of scientific articles, has\nbeen tried to automatically determine the classes to which these documents\nbelong. We have developed an efficient system that can analyze the Turkish\nscientific documents with the distributed document classification algorithm run\non the Cloud Computing infrastructure. The Apache Mahout library is used in the\nstudy. The servers required for classifying and clustering distributed\ndocuments are\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 12:03:35 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Gurbuz", "Selen", ""], ["Aydin", "Galip", ""]]}, {"id": "1802.05206", "submitter": "Christoph Dibak", "authors": "Christoph Dibak, Bernard Haasdonk, Andreas Schmidt, Frank D\\\"urr, Kurt\n  Rothermel", "title": "Enabling Interactive Mobile Simulations Through Distributed Reduced\n  Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.pmcj.2018.02.002", "report-no": null, "categories": "cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, various hardware and software companies are developing augmented\nreality devices, most prominently Microsoft with its Hololens. Besides gaming,\nsuch devices can be used for serious pervasive applications, like interactive\nmobile simulations to support engineers in the field. Interactive simulations\nhave high demands on resources, which the mobile device alone is unable to\nsatisfy. Therefore, we propose a framework to support mobile simulations by\ndistributing the computation between the mobile device and a remote server\nbased on the reduced basis method. Evaluations show that we can speed-up the\nnumerical computation by over 131 times while using 73 times less energy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 16:53:36 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Dibak", "Christoph", ""], ["Haasdonk", "Bernard", ""], ["Schmidt", "Andreas", ""], ["D\u00fcrr", "Frank", ""], ["Rothermel", "Kurt", ""]]}, {"id": "1802.05371", "submitter": "Philippe Tillet", "authors": "Philippe Tillet and David Cox", "title": "Input-Aware Auto-Tuning of Compute-Bound HPC Kernels", "comments": null, "journal-ref": "Proceedings of the International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2017", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient implementations of HPC applications for parallel architectures\ngenerally rely on external software packages (e.g., BLAS, LAPACK, CUDNN). While\nthese libraries provide highly optimized routines for certain characteristics\nof inputs (e.g., square matrices), they generally do not retain optimal\nperformance across the wide range of problems encountered in practice. In this\npaper, we present an input-aware auto-tuning framework for matrix\nmultiplications and convolutions, ISAAC, which uses predictive modeling\ntechniques to drive highly parameterized PTX code templates towards not only\nhardware-, but also application-specific kernels. Numerical experiments on the\nNVIDIA Maxwell and Pascal architectures show up to 3x performance gains over\nboth cuBLAS and cuDNN after only a few hours of auto-tuning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 00:40:16 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Tillet", "Philippe", ""], ["Cox", "David", ""]]}, {"id": "1802.05421", "submitter": "Kamalika Das", "authors": "Aniruddha Basak, Kamalika Das, Ole J. Mengshoel", "title": "CADDeLaG: Framework for distributed anomaly detection in large dense\n  graph sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random walk based distance measures for graphs such as commute-time distance\nare useful in a variety of graph algorithms, such as clustering, anomaly\ndetection, and creating low dimensional embeddings. Since such measures hinge\non the spectral decomposition of the graph, the computation becomes a\nbottleneck for large graphs and do not scale easily to graphs that cannot be\nloaded in memory. Most existing graph mining libraries for large graphs either\nresort to sampling or exploit the sparsity structure of such graphs for\nspectral analysis. However, such methods do not work for dense graphs\nconstructed for studying pairwise relationships among entities in a data set.\nExamples of such studies include analyzing pairwise locations in gridded\nclimate data for discovering long distance climate phenomena. These graphs\nrepresentations are fully connected by construction and cannot be sparsified\nwithout loss of meaningful information. In this paper we describe CADDeLaG, a\nframework for scalable computation of commute-time distance based anomaly\ndetection in large dense graphs without the need to load the entire graph in\nmemory. The framework relies on Apache Spark's memory-centric cluster-computing\ninfrastructure and consists of two building blocks: a decomposable algorithm\nfor commute time distance computation and a distributed linear system solver.\nWe illustrate the scalability of CADDeLaG and its dependency on various factors\nusing both synthetic and real world data sets. We demonstrate the usefulness of\nCADDeLaG in identifying anomalies in a climate graph sequence, that have been\nhistorically missed due to ad hoc graph sparsification and on an election\ndonation data set.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 07:18:41 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Basak", "Aniruddha", ""], ["Das", "Kamalika", ""], ["Mengshoel", "Ole J.", ""]]}, {"id": "1802.05443", "submitter": "Mauro Femminella", "authors": "Gianluca Reali, Mauro Femminella, Emilia Nunzi, Dario Valocchi", "title": "Genomics as a Service: a Joint Computing and Networking Perspective", "comments": "Journal publication", "journal-ref": "Computer Networks, Volume 145, 9 November 2018, Pages 27-51", "doi": "10.1016/j.comnet.2018.08.005", "report-no": null, "categories": "cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a global picture about the deployment of networked\nprocessing services for genomic data sets. Many current research make an\nextensive use genomic data, which are massive and rapidly increasing over time.\nThey are typically stored in remote databases, accessible by using Internet.\nFor this reason, a significant issue for effectively handling genomic data\nthrough data networks consists of the available network services. A first\ncontribution of this paper consists of identifying the still unexploited\nfeatures of genomic data that could allow optimizing their networked\nmanagement. The second and main contribution of this survey consists of a\nmethodological classification of computing and networking alternatives which\ncan be used to offer what we call the Genomic-as-a-Service (GaaS) paradigm. In\nmore detail, we analyze the main genomic processing applications, and classify\nnot only the main computing alternatives to run genomics workflows in either a\nlocal machine or a distributed cloud environment, but also the main software\ntechnologies available to develop genomic processing services. Since an\nanalysis encompassing only the computing aspects would provide only a partial\nview of the issues for deploying GaaS system, we present also the main\nnetworking technologies that are available to efficiently support a GaaS\nsolution. We first focus on existing service platforms, and analyze them in\nterms of service features, such as scalability, flexibility, and efficiency.\nThen, we present a taxonomy for both wide area and datacenter network\ntechnologies that may fit the GaaS requirements. It emerges that\nvirtualization, both in computing and networking, is the key for a successful\nlarge-scale exploitation of genomic data, by pushing ahead the adoption of the\nGaaS paradigm. Finally, the paper illustrates a short and long-term vision on\nfuture research challenges in the field.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 09:27:51 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 10:17:18 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Reali", "Gianluca", ""], ["Femminella", "Mauro", ""], ["Nunzi", "Emilia", ""], ["Valocchi", "Dario", ""]]}, {"id": "1802.05465", "submitter": "Laurens Versluis", "authors": "Alexandru Iosup, Alexandru Uta, Laurens Versluis, Georgios Andreadis,\n  Erwin van Eyk, Tim Hegeman, Sacheendra Talluri, Vincent van Beek, Lucian\n  Toader", "title": "Massivizing Computer Systems: a Vision to Understand, Design, and\n  Engineer Computer Ecosystems through and beyond Modern Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our society is digital: industry, science, governance, and individuals\ndepend, often transparently, on the inter-operation of large numbers of\ndistributed computer systems. Although the society takes them almost for\ngranted, these computer ecosystems are not available for all, may not be\naffordable for long, and raise numerous other research challenges. Inspired by\nthese challenges and by our experience with distributed computer systems, we\nenvision Massivizing Computer Systems, a domain of computer science focusing on\nunderstanding, controlling, and evolving successfully such ecosystems. Beyond\nestablishing and growing a body of knowledge about computer ecosystems and\ntheir constituent systems, the community in this domain should also aim to\neducate many about design and engineering for this domain, and all people about\nits principles. This is a call to the entire community: there is much to\ndiscover and achieve.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 10:25:14 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 09:17:18 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Iosup", "Alexandru", ""], ["Uta", "Alexandru", ""], ["Versluis", "Laurens", ""], ["Andreadis", "Georgios", ""], ["van Eyk", "Erwin", ""], ["Hegeman", "Tim", ""], ["Talluri", "Sacheendra", ""], ["van Beek", "Vincent", ""], ["Toader", "Lucian", ""]]}, {"id": "1802.05559", "submitter": "Peter Chini", "authors": "Peter Chini, Roland Meyer, Prakash Saivasan", "title": "Fine-Grained Complexity of Safety Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fine-grained complexity of Leader Contributor Reachability (LCR)\nand Bounded-Stage Reachability (BSR), two variants of the safety verification\nproblem for shared memory concurrent programs. For both problems, the memory is\na single variable over a finite data domain. Our contributions are new\nverification algorithms and lower bounds. The latter are based on the\nExponential Time Hypothesis (ETH), the problem Set Cover, and\ncross-compositions.\n  LCR is the question whether a designated leader thread can reach an unsafe\nstate when interacting with a certain number of equal contributor threads. We\nsuggest two parameterizations: (1) By the size of the data domain D and the\nsize of the leader L, and (2) by the size of the contributors C. We present\nalgorithms for both cases. The key techniques are compact witnesses and dynamic\nprogramming. The algorithms run in O*((L(D+1))^(LD) * D^D) and O*(2^C) time,\nshowing that both parameterizations are fixed-parameter tractable. We\ncomplement the upper bounds by (matching) lower bounds based on ETH and Set\nCover. Moreover, we prove the absence of polynomial kernels.\n  For BSR, we consider programs involving t different threads. We restrict the\nanalysis to computations where the write permission changes s times between the\nthreads. BSR asks whether a given configuration is reachable via such an\ns-stage computation. When parameterized by P, the maximum size of a thread, and\nt, the interesting observation is that the problem has a large number of\ndifficult instances. Formally, we show that there is no polynomial kernel, no\ncompression algorithm that reduces the size of the data domain D or the number\nof stages s to a polynomial dependence on P and t. This indicates that symbolic\nmethods may be harder to find for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 14:26:10 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 14:55:41 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 17:02:16 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Chini", "Peter", ""], ["Meyer", "Roland", ""], ["Saivasan", "Prakash", ""]]}, {"id": "1802.05582", "submitter": "Louis Esperet", "authors": "Pierre Aboulker, Marthe Bonamy, Nicolas Bousquet, Louis Esperet", "title": "Distributed coloring in sparse graphs with fewer colors", "comments": "16 pages, 4 figures - An extended abstract of this work was presented\n  at PODC'18 (ACM Symposium on Principles of Distributed Computing)", "journal-ref": "Electronic Journal of Combinatorics 26(4) (2019), P4.20", "doi": null, "report-no": null, "categories": "math.CO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with efficiently coloring sparse graphs in the\ndistributed setting with as few colors as possible. According to the celebrated\nFour Color Theorem, planar graphs can be colored with at most 4 colors, and the\nproof gives a (sequential) quadratic algorithm finding such a coloring. A\nnatural problem is to improve this complexity in the distributed setting. Using\nthe fact that planar graphs contain linearly many vertices of degree at most 6,\nGoldberg, Plotkin, and Shannon obtained a deterministic distributed algorithm\ncoloring $n$-vertex planar graphs with 7 colors in $O(\\log n)$ rounds. Here, we\nshow how to color planar graphs with 6 colors in $\\mbox{polylog}(n)$ rounds.\nOur algorithm indeed works more generally in the list-coloring setting and for\nsparse graphs (for such graphs we improve by at least one the number of colors\nresulting from an efficient algorithm of Barenboim and Elkin, at the expense of\na slightly worst complexity). Our bounds on the number of colors turn out to be\nquite sharp in general. Among other results, we show that no distributed\nalgorithm can color every $n$-vertex planar graph with 4 colors in $o(n)$\nrounds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 14:49:19 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 08:38:31 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 10:46:05 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 11:57:13 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Aboulker", "Pierre", ""], ["Bonamy", "Marthe", ""], ["Bousquet", "Nicolas", ""], ["Esperet", "Louis", ""]]}, {"id": "1802.05839", "submitter": "Michel M\\\"uller", "authors": "Michel M\\\"uller, Takayuki Aoki", "title": "New High Performance GPGPU Code Transformation Framework Applied to\n  Large Production Weather Prediction Code", "comments": "Preprint as accepted for ACM TOPC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce \"Hybrid Fortran\", a new approach that allows a high performance\nGPGPU port for structured grid Fortran codes. This technique only requires\nminimal changes for a CPU targeted codebase, which is a significant advancement\nin terms of productivity. It has been successfully applied to both dynamical\ncore and physical processes of ASUCA, a Japanese mesoscale weather prediction\nmodel with more than 150k lines of code. By means of a minimal weather\napplication that resembles ASUCA's code structure, Hybrid Fortran is compared\nto both a performance model as well as today's commonly used method, OpenACC.\nAs a result, the Hybrid Fortran implementation is shown to deliver the same or\nbetter performance than OpenACC and its performance agrees with the model both\non CPU and GPU. In a full scale production run, using an ASUCA grid with 1581 x\n1301 x 58 cells and real world weather data in 2km resolution, 24 NVIDIA Tesla\nP100 running the Hybrid Fortran based GPU port are shown to replace more than\n50 18-core Intel Xeon Broadwell E5-2695 v4 running the reference implementation\n- an achievement comparable to more invasive GPGPU rewrites of other weather\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 05:29:38 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["M\u00fcller", "Michel", ""], ["Aoki", "Takayuki", ""]]}, {"id": "1802.05872", "submitter": "Robert Palovics", "authors": "Andr\\'as A. Bencz\\'ur, Levente Kocsis and R\\'obert P\\'alovics", "title": "Online Machine Learning in Big Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of online machine learning in big data streams covers algorithms\nthat are (1) distributed and (2) work from data streams with only a limited\npossibility to store past data. The first requirement mostly concerns software\narchitectures and efficient algorithms. The second one also imposes nontrivial\ntheoretical restrictions on the modeling methods: In the data stream model,\nolder data is no longer available to revise earlier suboptimal modeling\ndecisions as the fresh data arrives.\n  In this article, we provide an overview of distributed software architectures\nand libraries as well as machine learning models for online learning. We\nhighlight the most important ideas for classification, regression,\nrecommendation, and unsupervised modeling from streaming data, and we show how\nthey are implemented in various distributed data stream processing systems.\n  This article is a reference material and not a survey. We do not attempt to\nbe comprehensive in describing all existing methods and solutions; rather, we\ngive pointers to the most important resources in the field. All related\nsub-fields, online algorithms, online learning, and distributed data processing\nare hugely dominant in current research and development with conceptually new\nresearch results and software components emerging at the time of writing. In\nthis article, we refer to several survey results, both for distributed data\nprocessing and for online machine learning. Compared to past surveys, our\narticle is different because we discuss recommender systems in extended detail.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 09:15:27 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Bencz\u00far", "Andr\u00e1s A.", ""], ["Kocsis", "Levente", ""], ["P\u00e1lovics", "R\u00f3bert", ""]]}, {"id": "1802.05969", "submitter": "Alvaro Garcia-Perez", "authors": "\\'Alvaro Garc\\'ia-P\\'erez, Alexey Gotsman, Yuri Meshman and Ilya\n  Sergey", "title": "Paxos Consensus, Deconstructed and Abstracted (Extended Version)", "comments": "Accepted for publication in the 27th European Symposium on\n  Programming (ESOP'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lamport's Paxos algorithm is a classic consensus protocol for state machine\nreplication in environments that admit crash failures. Many versions of Paxos\nexploit the protocol's intrinsic properties for the sake of gaining better\nrun-time performance, thus widening the gap between the original description of\nthe algorithm, which was proven correct, and its real-world implementations. In\nthis work, we address the challenge of specifying and verifying complex\nPaxos-based systems by (a) devising composable specifications for\nimplementations of Paxos's single-decree version, and (b) engineering\ndisciplines to reason about protocol-aware, semantics-preserving optimisations\nto single-decree Paxos. In a nutshell, our approach elaborates on the\ndeconstruction of single-decree Paxos by Boichat et al. We provide novel\nnon-deterministic specifications for each module in the deconstruction and\nprove that the implementations refine the corresponding specifications, such\nthat the proofs of the modules that remain unchanged can be reused across\ndifferent implementations. We further reuse this result and show how to obtain\na verified implementation of Multi-Paxos from a verified implementation of\nsingle-decree Paxos, by a series of novel protocol-aware transformations of the\nnetwork semantics, which we prove to be behaviour-preserving.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 15:05:48 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Garc\u00eda-P\u00e9rez", "\u00c1lvaro", ""], ["Gotsman", "Alexey", ""], ["Meshman", "Yuri", ""], ["Sergey", "Ilya", ""]]}, {"id": "1802.06270", "submitter": "Saurabh Bagchi", "authors": "Raghav Shankar, Benjamin Kobin, Saurabh Bagchi, Michael Kistler, Jan\n  Rellermeyer", "title": "MAVIS: Managing Datacenters using Smartphones", "comments": "ACM Classification (2012): Data center networks; System management;\n  Ubiquitous and mobile computing systems and tools", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed monitoring plays a crucial role in managing the activities of\ncloud-based datacenters. System administrators have long relied on monitoring\nsystems such as Nagios and Ganglia to obtain status alerts on their\ndesktop-class machines. However, the popularity of mobile devices is pushing\nthe community to develop datacenter monitoring solutions for smartphone-class\ndevices. Here we lay out desirable characteristics of such smartphone-based\nmonitoring and identify quantitatively the shortcomings from directly applying\nexisting solutions to this domain. Then we introduce a possible design that\naddresses some of these shortcomings and provide results from an early\nprototype, called MAVIS, using one month of monitoring data from approximately\n3,000 machines hosted by Purdue's central IT organization.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 18:15:53 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Shankar", "Raghav", ""], ["Kobin", "Benjamin", ""], ["Bagchi", "Saurabh", ""], ["Kistler", "Michael", ""], ["Rellermeyer", "Jan", ""]]}, {"id": "1802.06305", "submitter": "Mohammad Saeid Mahdavinejad", "authors": "Mohammad Saeid Mahdavinejad, Mohammadreza Rezvan, Mohammadamin\n  Barekatain, Peyman Adibi, Payam Barnaghi, Amit P. Sheth", "title": "Machine learning for Internet of Things data analysis: A survey", "comments": "Digital Communications and Networks (2017)", "journal-ref": null, "doi": "10.1016/j.dcan.2017.10.002", "report-no": null, "categories": "cs.LG cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid developments in hardware, software, and communication technologies have\nallowed the emergence of Internet-connected sensory devices that provide\nobservation and data measurement from the physical world. By 2020, it is\nestimated that the total number of Internet-connected devices being used will\nbe between 25 and 50 billion. As the numbers grow and technologies become more\nmature, the volume of data published will increase. Internet-connected devices\ntechnology, referred to as Internet of Things (IoT), continues to extend the\ncurrent Internet by providing connectivity and interaction between the physical\nand cyber worlds. In addition to increased volume, the IoT generates Big Data\ncharacterized by velocity in terms of time and location dependency, with a\nvariety of multiple modalities and varying data quality. Intelligent processing\nand analysis of this Big Data is the key to developing smart IoT applications.\nThis article assesses the different machine learning methods that deal with the\nchallenges in IoT data by considering smart cities as the main use case. The\nkey contribution of this study is presentation of a taxonomy of machine\nlearning algorithms explaining how different techniques are applied to the data\nin order to extract higher level information. The potential and challenges of\nmachine learning for IoT data analytics will also be discussed. A use case of\napplying Support Vector Machine (SVM) on Aarhus Smart City traffic data is\npresented for a more detailed exploration.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 22:37:17 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Mahdavinejad", "Mohammad Saeid", ""], ["Rezvan", "Mohammadreza", ""], ["Barekatain", "Mohammadamin", ""], ["Adibi", "Peyman", ""], ["Barnaghi", "Payam", ""], ["Sheth", "Amit P.", ""]]}, {"id": "1802.06466", "submitter": "Ying Shan", "authors": "Ying Shan and Jian Jiao and Jie Zhu and JC Mao", "title": "Recurrent Binary Embedding for GPU-Enabled Exhaustive Retrieval from\n  Billion-Scale Semantic Vectors", "comments": "15 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in GPU hardware and multiple areas of Deep Learning open up a\nnew opportunity for billion-scale information retrieval with exhaustive search.\nBuilding on top of the powerful concept of semantic learning, this paper\nproposes a Recurrent Binary Embedding (RBE) model that learns compact\nrepresentations for real-time retrieval. The model has the unique ability to\nrefine a base binary vector by progressively adding binary residual vectors to\nmeet the desired accuracy. The refined vector enables efficient implementation\nof exhaustive similarity computation with bit-wise operations, followed by a\nnear- lossless k-NN selection algorithm, also proposed in this paper. The\nproposed algorithms are integrated into an end-to-end multi-GPU system that\nretrieves thousands of top items from over a billion candidates in real-time.\nThe RBE model and the retrieval system were evaluated with data from a major\npaid search engine. When measured against the state-of-the-art model for binary\nrepresentation and the full precision model for semantic embedding, RBE\nsignificantly outperformed the former, and filled in over 80% of the AUC gap\nin-between. Experiments comparing with our production retrieval system also\ndemonstrated superior performance. While the primary focus of this paper is to\nbuild RBE based on a particular class of semantic models, generalizing to other\ntypes is straightforward, as exemplified by two different models at the end of\nthe paper.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 23:02:58 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Shan", "Ying", ""], ["Jiao", "Jian", ""], ["Zhu", "Jie", ""], ["Mao", "JC", ""]]}, {"id": "1802.06532", "submitter": "Takeharu Shiraga", "authors": "Takeharu Shiraga", "title": "Discrepancy Analysis of a New Randomized Diffusion Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an arbitrary initial configuration of discrete loads over vertices of a\ndistributed graph, we consider the problem of minimizing the {\\em discrepancy}\nbetween the maximum and minimum loads among all vertices. For this problem,\nthis paper is concerned with the ability of natural diffusion-based iterative\nalgorithms: at each discrete and synchronous time step on an algorithm, each\nvertex is allowed to distribute its loads to each neighbor (including itself)\nwithout occurring negative loads or using the information of previous time\nsteps.\n  In this setting, this paper presents a new {\\em randomized} diffusion\nalgorithm like multiple random walks. Our algorithm archives $O(\\sqrt{d \\log\nN})$ discrepancy for any $d$-regular graph with $N$ vertices with high\nprobability, while {\\em deterministic} diffusion algorithms have $\\Omega(d)$\nlower bound. Furthermore, we succeed in generalizing our algorithm to any\nsymmetric round matrix. This yields that $O(\\sqrt{ d_{\\max} \\log N})$\ndiscrepancy for arbitrary graphs without using the information of maximum\ndegree $d_{\\max}$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 07:10:15 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 07:15:16 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 10:23:04 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Shiraga", "Takeharu", ""]]}, {"id": "1802.06625", "submitter": "Jani Boutellier", "authors": "Jani Boutellier, Jiahao Wu, Heikki Huttunen, Shuvra S. Bhattacharyya", "title": "PRUNE: Dynamic and Decidable Dataflow for Signal Processing on\n  Heterogeneous Platforms", "comments": "This is the author's version of an article that has been published in\n  this journal. Changes were made to this version by the publisher prior to\n  publication", "journal-ref": "IEEE Transactions on Signal Processing, Year: 2018, Volume: 66,\n  Issue: 3, Pages: 654 - 665", "doi": "10.1109/TSP.2017.2773424", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of contemporary mobile devices and personal computers are based\non heterogeneous computing platforms that consist of a number of CPU cores and\none or more Graphics Processing Units (GPUs). Despite the high volume of these\ndevices, there are few existing programming frameworks that target full and\nsimultaneous utilization of all CPU and GPU devices of the platform.\n  This article presents a dataflow-flavored Model of Computation (MoC) that has\nbeen developed for deploying signal processing applications to heterogeneous\nplatforms. The presented MoC is dynamic and allows describing applications with\ndata dependent run-time behavior. On top of the MoC, formal design rules are\npresented that enable application descriptions to be simultaneously dynamic and\ndecidable. Decidability guarantees compile-time application analyzability for\ndeadlock freedom and bounded memory.\n  The presented MoC and the design rules are realized in a novel Open Source\nprogramming environment \"PRUNE\" and demonstrated with representative\napplication examples from the domains of image processing, computer vision and\nwireless communications. Experimental results show that the proposed approach\noutperforms the state-of-the-art in analyzability, flexibility and performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 13:44:33 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Boutellier", "Jani", ""], ["Wu", "Jiahao", ""], ["Huttunen", "Heikki", ""], ["Bhattacharyya", "Shuvra S.", ""]]}, {"id": "1802.06686", "submitter": "Yitong Yin", "authors": "Weiming Feng and Yitong Yin", "title": "On Local Distributed Sampling and Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classic distributed graph problems, each instance on a graph specifies a\nspace of feasible solutions (e.g. all proper ($\\Delta+1$)-list-colorings of the\ngraph), and the task of distributed algorithm is to construct a feasible\nsolution using local information.\n  We study distributed sampling and counting problems, in which each instance\nspecifies a joint distribution of feasible solutions. The task of distributed\nalgorithm is to sample from this joint distribution, or to locally measure the\nvolume of the probability space via the marginal probabilities. The latter task\nis also known as inference, which is a local counterpart of counting.\n  For self-reducible classes of instances, the following equivalences are\nestablished in the LOCAL model up to polylogarithmic factors:\n  $\\bullet$ For all joint distributions, approximate inference and approximate\nsampling are computationally equivalent.\n  $\\bullet$ For all joint distributions defined by local constraints, exact\nsampling is reducible to either one of the above tasks.\n  $\\bullet$ If further, sequentially constructing a feasible solution is\ntrivial locally, then all above tasks are easy if and only if the joint\ndistribution exhibits strong spatial mixing.\n  Combining with the state of the arts of strong spatial mixing, we obtain\nefficient sampling algorithms in the LOCAL model for various important sampling\nproblems, including: an $O(\\sqrt{\\Delta}\\log^3n)$-round algorithm for exact\nsampling matchings in graphs with maximum degree $\\Delta$, and an\n$O(\\log^3n)$-round algorithm for sampling according to the hardcore model\n(weighted independent sets) in the uniqueness regime, which along with the\n$\\Omega(\\mathrm{diam})$ lower bound in arXiv:1702.00142 for sampling according\nto the hardcore model in the non-uniqueness regime, gives the first\ncomputational phase transition for distributed sampling.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 16:06:32 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Feng", "Weiming", ""], ["Yin", "Yitong", ""]]}, {"id": "1802.06712", "submitter": "Martin Wilhelm", "authors": "Martin Wilhelm", "title": "Multithreading for the expression-dag-based number type Real_algebraic", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": "FIN-001-2018", "categories": "cs.DC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms, especially in the field of computational geometry, are based\non the premise that arithmetic operations are performed exactly. Real machines\nare based on inexact floating-point arithmetic. Various number types have been\ndeveloped to close this gap by providing exact computation or ensuring exact\ndecisions. In this report we describe the implementation of an extension to the\nexact-decisions number type Real_algebraic that enables us to take advantage of\nmultiple processing units.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 17:13:11 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 11:28:04 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wilhelm", "Martin", ""]]}, {"id": "1802.06742", "submitter": "Mika\\\"el Rabie", "authors": "Marthe Bonamy, Paul Ouvrard, Mika\\\"el Rabie, Jukka Suomela, Jara Uitto", "title": "Distributed Recoloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two colorings of a graph, we consider the following problem: can we\nrecolor the graph from one coloring to the other through a series of elementary\nchanges, such that the graph is properly colored after each step?\n  We introduce the notion of distributed recoloring: The input graph represents\na network of computers that needs to be recolored. Initially, each node is\naware of its own input color and target color. The nodes can exchange messages\nwith each other, and eventually each node has to stop and output its own\nrecoloring schedule, indicating when and how the node changes its color. The\nrecoloring schedules have to be globally consistent so that the graph remains\nproperly colored at each point, and we require that adjacent nodes do not\nchange their colors simultaneously.\n  We are interested in the following questions: How many communication rounds\nare needed (in the LOCAL model of distributed computing) to find a recoloring\nschedule? What is the length of the recoloring schedule? And how does the\npicture change if we can use extra colors to make recoloring easier?\n  The main contributions of this work are related to distributed recoloring\nwith one extra color in the following graph classes: trees, $3$-regular graphs,\nand toroidal grids.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 18:16:40 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 11:18:25 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Bonamy", "Marthe", ""], ["Ouvrard", "Paul", ""], ["Rabie", "Mika\u00ebl", ""], ["Suomela", "Jukka", ""], ["Uitto", "Jara", ""]]}, {"id": "1802.06867", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Leszek G\\k{a}sieniec, Grzegorz Stachowiak, Przemys{\\l}aw Uzna\\'nski", "title": "Almost logarithmic-time space optimal leader election in population\n  protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model of population protocols refers to a large collection of simple\nindistinguishable entities, frequently called {\\em agents}. The agents\ncommunicate and perform computation through pairwise interactions. We study\nfast and space efficient leader election in population of cardinality $n$\ngoverned by a random scheduler, where during each time step the scheduler\nuniformly at random selects for interaction exactly one pair of agents.\n  We propose the first $o(\\log^2 n)$-time leader election protocol. Our\nsolution operates in expected parallel time $O(\\log n\\log\\log n)$ which is\nequivalent to $O(n \\log n\\log\\log n)$ pairwise interactions. This is the\nfastest currently known leader election algorithm in which each agent utilises\nasymptotically optimal number of $O(\\log\\log n)$ states.\n  The new protocol incorporates and amalgamates successfully the power of\nassorted {\\em synthetic coins} with variable rate {\\em phase clocks}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 21:41:51 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 16:53:57 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["G\u0105sieniec", "Leszek", ""], ["Stachowiak", "Grzegorz", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1802.06872", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Adrian Kosowski, Przemys{\\l}aw Uzna\\'nski", "title": "Population Protocols Are Fast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A population protocol describes a set of state change rules for a population\nof $n$ indistinguishable finite-state agents (automata), undergoing random\npairwise interactions. Within this very basic framework, it is possible to\nresolve a number of fundamental tasks in distributed computing, including:\nleader election, aggregate and threshold functions on the population, such as\nmajority computation, and plurality consensus. For the first time, we show that\nsolutions to all of these problems can be obtained \\emph{quickly} using\nfinite-state protocols. For any input, the designed finite-state protocols\nconverge under a fair random scheduler to an output which is correct with high\nprobability in expected $O(\\mathrm{poly} \\log n)$ parallel time. In the same\nsetting, we also show protocols which always reach a valid solution, in\nexpected parallel time $O(n^\\varepsilon)$, where the number of states of the\ninteracting automata depends only on the choice of $\\varepsilon>0$. The stated\ntime bounds hold for \\emph{any} semi-linear predicate computable in the\npopulation protocol framework.\n  The key ingredient of our result is the decentralized design of a hierarchy\nof phase-clocks, which tick at different rates, with the rates of adjacent\nclocks separated by a factor of $\\Theta(\\log n)$. The construction of this\nclock hierarchy relies on a new protocol composition technique, combined with\nan adapted analysis of a self-organizing process of oscillatory dynamics. This\nclock hierarchy is used to provide nested synchronization primitives, which\nallow us to view the population in a global manner and design protocols using a\nhigh-level imperative programming language with a (limited) capacity for loops\nand branching instructions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 21:49:01 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 20:43:45 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Kosowski", "Adrian", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1802.06949", "submitter": "Amith Rajith Mamidala", "authors": "Amith R Mamidala", "title": "Efficient Embedding of MPI Collectives in MXNET DAGs for scaling Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability of high performance computing infrastructures such as clusters\nof GPUs and CPUs have fueled the growth of distributed learning systems. Deep\nLearning frameworks express neural nets as DAGs and execute these DAGs on\ncomputation resources such as GPUs. In this paper, we propose efficient designs\nof embedding MPI collective operations into data parallel DAGs. Incorrect\ndesigns can easily lead to deadlocks or program crashes. In particular, we\ndemonstrate three designs: Funneled, Concurrent communication and Dependency\nchaining of using MPI collectives with DAGs. These designs automatically enable\noverlap of computation with communication by allowing for concurrent execution\nwith the other tasks. We directly implement these designs into the KVStore API\nof the MXNET. This allows us to directly leverage the rest of the\ninfrastructure. Using ImageNet and CIFAR data sets, we show the potential of\nour designs. In particular, our designs scale to 256 GPUs with as low as 50\nseconds of epoch times for ImageNet 1K datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 03:36:20 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Mamidala", "Amith R", ""]]}, {"id": "1802.07000", "submitter": "Denis Rystsov", "authors": "Denis Rystsov", "title": "CASPaxos: Replicated State Machines without logs", "comments": "update style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  CASPaxos is a wait-free, linearizable, multi-writer multi-reader register in\nunreliable, asynchronous networks supporting arbitrary update operations\nincluding compare-and-set (CAS). The register acts as a replicated state\nmachine providing an interface for changing its value by applying an arbitrary\nuser-provided function (a command). Unlike Multi-Paxos and Raft which replicate\nthe log of commands, CASPaxos replicates state, thus avoiding associated\ncomplexity, reducing write amplification, increasing concurrency of disk\noperations and hardware utilization. The paper describes CASPaxos, proves its\nsafety properties and evaluates the characteristics of a CASPaxos-based\nprototype of key-value storage.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 08:09:38 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 07:22:28 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 18:11:53 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 15:58:22 GMT"}, {"version": "v5", "created": "Fri, 16 Mar 2018 16:42:19 GMT"}, {"version": "v6", "created": "Mon, 26 Mar 2018 03:41:03 GMT"}, {"version": "v7", "created": "Tue, 3 Apr 2018 16:02:27 GMT"}, {"version": "v8", "created": "Thu, 5 Apr 2018 17:32:34 GMT"}, {"version": "v9", "created": "Wed, 16 May 2018 16:52:08 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Rystsov", "Denis", ""]]}, {"id": "1802.07144", "submitter": "Alexander Noe", "authors": "Alexandra Henzinger, Alexander Noe, Christian Schulz", "title": "ILP-based Local Search for Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing high-quality graph partitions is a challenging problem with\nnumerous applications. In this paper, we present a novel meta-heuristic for the\nbalanced graph partitioning problem. Our approach is based on integer linear\nprograms that solve the partitioning problem to optimality. However, since\nthose programs typically do not scale to large inputs, we adapt them to\nheuristically improve a given partition. We do so by defining a much smaller\nmodel that allows us to use symmetry breaking and other techniques that make\nthe approach scalable. For example, in Walshaw's well-known benchmark tables we\nare able to improve roughly half of all entries when the number of blocks is\nhigh.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 15:11:23 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Henzinger", "Alexandra", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""]]}, {"id": "1802.07209", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim and Victor Khazanov", "title": "Distributed Symmetry-Breaking Algorithms for Congested Cliques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {Congested Clique} is a distributed-computing model for single-hop\nnetworks with restricted bandwidth that has been very intensively studied\nrecently. It models a network by an $n$-vertex graph in which any pair of\nvertices can communicate one with another by transmitting $O(\\log n )$ bits in\neach round. Various problems have been studied in this setting, but for some of\nthem the best-known results are those for general networks. In this paper we\ndevise significantly improved algorithms for various symmetry-breaking\nproblems, such as forests-decompositions, vertex-colorings, and maximal\nindependent set.\n  We analyze the running time of our algorithms as a function of the arboricity\n$a$ of a clique subgraph that is given as input. Our algorithms are especially\nefficient in Trees, planar graphs, graphs with constant genus, and many other\ngraphs that have bounded arboricity, but unbounded size. We obtain\n$O(a)$-forest-decomposition algorithm with $O(\\log a)$ time that improves the\npreviously-known $O(\\log n)$ time, $O(a^{2 + \\epsilon})$-coloring in $O(\\log^*\nn)$ time that improves upon an $O(\\log n)$-time algorithm, $O(a)$-coloring in\n$O(a^{\\epsilon})$-time that improves upon several previous algorithms, and a\nmaximal independent set algorithm with $O(\\sqrt a)$ time that improves at least\nquadratically upon the state-of-the-art for small and moderate values of $a$.\n  Those results are achieved using several techniques. First, we produce a\nforest decomposition with a helpful structure called {$H$-partition} within\n$O(\\log a)$ rounds. In general graphs this structure requires $\\Theta(\\log n)$\ntime, but in Congested Cliques we are able to compute it faster. We employ this\nstructure in conjunction with partitioning techniques that allow us to solve\nvarious symmetry-breaking problems efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 17:20:16 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Barenboim", "Leonid", ""], ["Khazanov", "Victor", ""]]}, {"id": "1802.07240", "submitter": "Ethan MacBrough", "authors": "Ethan MacBrough", "title": "Cobalt: BFT Governance in Open Networks", "comments": "49 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Cobalt, a novel atomic broadcast algorithm that works in networks\nwith non-uniform trust and no global agreement on participants, and is\nprobabilistically guaranteed to make forward progress even in the presence of\nmaximal faults and arbitrary asynchrony. The exact properties that Cobalt\nsatisfies makes it particularly applicable to designing an efficient\ndecentralized \"voting network\" that allows a public, open-entry group of nodes\nto agree on changes to some shared set of rules in a fair and consistent manner\nwhile tolerating some trusted nodes and arbitrarily many untrusted nodes\nbehaving maliciously. We also define a new set of properties which must be\nsatisfied by any safe decentralized governance algorithm, and all of which\nCobalt satisfies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 18:37:25 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["MacBrough", "Ethan", ""]]}, {"id": "1802.07242", "submitter": "Ethan MacBrough", "authors": "Brad Chase and Ethan MacBrough", "title": "Analysis of the XRP Ledger Consensus Protocol", "comments": "25 pages, 6 figures, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The XRP Ledger Consensus Protocol is a previously developed consensus\nprotocol powering the XRP Ledger. It is a low-latency Byzantine agreement\nprotocol, capable of reaching consensus without full agreement on which nodes\nare members of the network. We present a detailed explanation of the algorithm\nand derive conditions for its safety and liveness.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 18:39:04 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Chase", "Brad", ""], ["MacBrough", "Ethan", ""]]}, {"id": "1802.07389", "submitter": "Hyeontaek Lim", "authors": "Hyeontaek Lim and David G. Andersen and Michael Kaminsky", "title": "3LC: Lightweight and Effective Traffic Compression for Distributed\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance and efficiency of distributed machine learning (ML) depends\nsignificantly on how long it takes for nodes to exchange state changes.\nOverly-aggressive attempts to reduce communication often sacrifice final model\naccuracy and necessitate additional ML techniques to compensate for this loss,\nlimiting their generality. Some attempts to reduce communication incur high\ncomputation overhead, which makes their performance benefits visible only over\nslow networks.\n  We present 3LC, a lossy compression scheme for state change traffic that\nstrikes balance between multiple goals: traffic reduction, accuracy,\ncomputation overhead, and generality. It combines three new\ntechniques---3-value quantization with sparsity multiplication, quartic\nencoding, and zero-run encoding---to leverage strengths of quantization and\nsparsification techniques and avoid their drawbacks. It achieves a data\ncompression ratio of up to 39--107X, almost the same test accuracy of trained\nmodels, and high compression speed. Distributed ML frameworks can employ 3LC\nwithout modifications to existing ML algorithms. Our experiments show that 3LC\nreduces wall-clock training time of ResNet-110--based image classifiers for\nCIFAR-10 on a 10-GPU cluster by up to 16--23X compared to TensorFlow's baseline\ndesign.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 01:08:58 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Lim", "Hyeontaek", ""], ["Andersen", "David G.", ""], ["Kaminsky", "Michael", ""]]}, {"id": "1802.07504", "submitter": "Michael Feldmann M. Sc.", "authors": "Michael Feldmann, Christian Scheideler and Alexander Setzer", "title": "Skueue: A Scalable and Sequentially Consistent Distributed Queue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed protocol for a queue, called \\textsc{Skueue}, which\nspreads its data fairly onto multiple processes, avoiding bottlenecks in high\nthroughput scenarios. \\textsc{Skueue} can be used in highly dynamic\nenvironments, through the addition of join and leave requests to the standard\nqueue operations enqueue and dequeue. Furthermore \\textsc{Skueue} satisfies\nsequential consistency in the asynchronous message passing model. Scalability\nis achieved by aggregating multiple requests to a batch, which can then be\nprocessed in a distributed fashion without hurting the queue semantics.\nOperations in \\textsc{Skueue} need a logarithmic number of rounds w.h.p. until\nthey are processed, even under a high rate of incoming requests.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 10:40:50 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 08:46:43 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Feldmann", "Michael", ""], ["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""]]}, {"id": "1802.07647", "submitter": "Christian Konrad", "authors": "Christian Konrad", "title": "MIS in the Congested Clique Model in $O(\\log \\log \\Delta)$ Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a maximal independent set (MIS) algorithm that runs in $O(\\log \\log\n\\Delta)$ rounds in the congested clique model, where $\\Delta$ is the maximum\ndegree of the input graph. This improves upon the $O(\\frac{\\log(\\Delta) \\cdot\n\\log \\log \\Delta}{\\sqrt{\\log n}} + \\log \\log \\Delta )$ rounds algorithm of\n[Ghaffari, PODC '17], where $n$ is the number of vertices of the input graph.\n  In the first stage of our algorithm, we simulate the first\n$O(\\frac{n}{\\text{poly} \\log n})$ iterations of the sequential random order\nGreedy algorithm for MIS in the congested clique model in $O(\\log \\log \\Delta)$\nrounds. This thins out the input graph relatively quickly: After this stage,\nthe maximum degree of the residual graph is poly-logarithmic. In the second\nstage, we run the MIS algorithm of [Ghaffari, PODC '17] on the residual graph,\nwhich completes in $O(\\log \\log \\Delta)$ rounds on graphs of poly-logarithmic\ndegree.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 16:21:34 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Konrad", "Christian", ""]]}, {"id": "1802.07817", "submitter": "Chryssis Georgiou", "authors": "Antonio Fern\\'andez Anta and Chryssis Georgiou and Kishori Konwar and\n  Nicolas Nicolaou", "title": "Formalizing and Implementing Distributed Ledger Objects", "comments": "18 pages, 3 figures, preliminary version appears in Proceedings of\n  NETYS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the hype about blockchains and distributed ledgers, no formal\nabstraction of these objects has been proposed. To face this issue, in this\npaper we provide a proper formulation of a distributed ledger object. In brief,\nwe define a ledger object as a sequence of records, and we provide the\noperations and the properties that such an object should support.\nImplementation of a ledger object on top of multiple (possibly geographically\ndispersed) computing devices gives rise to the distributed ledger object. In\ncontrast to the centralized object, distribution allows operations to be\napplied concurrently on the ledger, introducing challenges on the consistency\nof the ledger in each participant. We provide the definitions of three well\nknown consistency guarantees in terms of the operations supported by the ledger\nobject: (1) atomic consistency (linearizability), (2) sequential consistency,\nand (3) eventual consistency. We then provide implementations of distributed\nledgers on asynchronous message passing crash-prone systems using an Atomic\nBroadcast service, and show that they provide eventual, sequential or atomic\nconsistency semantics. We conclude with a variation of the ledger - the\nvalidated ledger - which requires that each record in the ledger satisfies a\nparticular validation rule.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 21:32:04 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 07:30:14 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Anta", "Antonio Fern\u00e1ndez", ""], ["Georgiou", "Chryssis", ""], ["Konwar", "Kishori", ""], ["Nicolaou", "Nicolas", ""]]}, {"id": "1802.07834", "submitter": "El Mahdi El Mhamdi", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, Alexandre Maurer, Vladislav\n  Tempez", "title": "Learning to Gather without Communication", "comments": "Preliminary version, presented at the 5th Biological Distributed\n  Algorithms Workshop. Washington D.C, July 28th, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DC cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard belief on emerging collective behavior is that it emerges from\nsimple individual rules. Most of the mathematical research on such collective\nbehavior starts from imperative individual rules, like always go to the center.\nBut how could an (optimal) individual rule emerge during a short period within\nthe group lifetime, especially if communication is not available. We argue that\nsuch rules can actually emerge in a group in a short span of time via\ncollective (multi-agent) reinforcement learning, i.e learning via rewards and\npunishments. We consider the gathering problem: several agents (social animals,\nswarming robots...) must gather around a same position, which is not determined\nin advance. They must do so without communication on their planned decision,\njust by looking at the position of other agents. We present the first\nexperimental evidence that a gathering behavior can be learned without\ncommunication in a partially observable environment. The learned behavior has\nthe same properties as a self-stabilizing distributed algorithm, as processes\ncan gather from any initial state (and thus tolerate any transient failure).\nBesides, we show that it is possible to tolerate the brutal loss of up to 90\\%\nof agents without significant impact on the behavior.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 22:26:21 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Maurer", "Alexandre", ""], ["Tempez", "Vladislav", ""]]}, {"id": "1802.07923", "submitter": "Jianxiang Xi", "authors": "Jianxiang Xi, Cheng Wang, Hao Liu, Zhong Wang", "title": "Dynamic Output Feedback Guaranteed-Cost Synchronization for Multiagent\n  Networks with Given Cost Budgets", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper addresses the distributed guaranteed-cost synchronization\nproblems for general high-order linear multiagent networks. Existing works on\nthe guaranteed-cost synchronization usually require all state information of\nneighboring agents and cannot give the cost budget previously. For both\nleaderless and leader-following interaction topologies, the current paper\nfirstly proposes a dynamic output feedback synchronization protocol with\nguaranteed-cost constraints, which can realize the tradeoff design between the\nenergy consumption and the synchronization regulation performance with the\ngiven cost budget. Then, according to different structure features of\ninteraction topologies, leaderless and leader-following guaranteed-cost\nsynchronization analysis and design criteria are presented, respectively, and\nan algorithm is proposed to deal with the impacts of nonlinear terms by using\nboth synchronization analysis and design criteria. Especially, an explicit\nexpression of the synchronization function is shown for leaderless cases, which\nis independent of protocol states and the given cost budget. Finally, numerical\nexamples are presented to demonstrate theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:37:07 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Xi", "Jianxiang", ""], ["Wang", "Cheng", ""], ["Liu", "Hao", ""], ["Wang", "Zhong", ""]]}, {"id": "1802.07927", "submitter": "El Mahdi El Mhamdi", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, S\\'ebastien Rouault", "title": "The Hidden Vulnerability of Distributed Learning in Byzantium", "comments": "Accepted to ICML 2018 as a long talk", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While machine learning is going through an era of celebrated success,\nconcerns have been raised about the vulnerability of its backbone: stochastic\ngradient descent (SGD). Recent approaches have been proposed to ensure the\nrobustness of distributed SGD against adversarial (Byzantine) workers sending\npoisoned gradients during the training phase. Some of these approaches have\nbeen proven Byzantine-resilient: they ensure the convergence of SGD despite the\npresence of a minority of adversarial workers.\n  We show in this paper that convergence is not enough. In high dimension $d\n\\gg 1$, an adver\\-sary can build on the loss function's non-convexity to make\nSGD converge to ineffective models. More precisely, we bring to light that\nexisting Byzantine-resilient schemes leave a margin of poisoning of\n$\\Omega\\left(f(d)\\right)$, where $f(d)$ increases at least like $\\sqrt{d~}$.\nBased on this leeway, we build a simple attack, and experimentally show its\nstrong to utmost effectivity on CIFAR-10 and MNIST.\n  We introduce Bulyan, and prove it significantly reduces the attackers leeway\nto a narrow $O( \\frac{1}{\\sqrt{d~}})$ bound. We empirically show that Bulyan\ndoes not suffer the fragility of existing aggregation rules and, at a\nreasonable cost in terms of required batch size, achieves convergence as if\nonly non-Byzantine gradients had been used to update the model.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:42:00 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 18:10:23 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Rouault", "S\u00e9bastien", ""]]}, {"id": "1802.07928", "submitter": "El Mahdi El Mhamdi", "authors": "Georgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui, Rhicheek\n  Patra, Mahsa Taziki", "title": "Asynchronous Byzantine Machine Learning (the case of SGD)", "comments": "accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Asynchronous distributed machine learning solutions have proven very\neffective so far, but always assuming perfectly functioning workers. In\npractice, some of the workers can however exhibit Byzantine behavior, caused by\nhardware failures, software bugs, corrupt data, or even malicious attacks. We\nintroduce \\emph{Kardam}, the first distributed asynchronous stochastic gradient\ndescent (SGD) algorithm that copes with Byzantine workers. Kardam consists of\ntwo complementary components: a filtering and a dampening component. The first\nis scalar-based and ensures resilience against $\\frac{1}{3}$ Byzantine workers.\nEssentially, this filter leverages the Lipschitzness of cost functions and acts\nas a self-stabilizer against Byzantine workers that would attempt to corrupt\nthe progress of SGD. The dampening component bounds the convergence rate by\nadjusting to stale information through a generic gradient weighting scheme. We\nprove that Kardam guarantees almost sure convergence in the presence of\nasynchrony and Byzantine behavior, and we derive its convergence rate. We\nevaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead\nwith respect to non Byzantine-resilient solutions. We empirically show that\nKardam does not introduce additional noise to the learning procedure but does\ninduce a slowdown (the cost of Byzantine resilience) that we both theoretically\nand empirically show to be less than $f/n$, where $f$ is the number of\nByzantine failures tolerated and $n$ the total number of workers.\nInterestingly, we also empirically observe that the dampening component is\ninteresting in its own right for it enables to build an SGD algorithm that\noutperforms alternative staleness-aware asynchronous competitors in\nenvironments with honest workers.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:47:35 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 17:48:06 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Damaskinos", "Georgios", ""], ["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Patra", "Rhicheek", ""], ["Taziki", "Mahsa", ""]]}, {"id": "1802.08021", "submitter": "Cedric Renggli", "authors": "Cedric Renggli and Saleh Ashkboos and Mehdi Aghagolzadeh and Dan\n  Alistarh and Torsten Hoefler", "title": "SparCML: High-Performance Sparse Communication for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying machine learning techniques to the quickly growing data in science\nand industry requires highly-scalable algorithms. Large datasets are most\ncommonly processed \"data parallel\" distributed across many nodes. Each node's\ncontribution to the overall gradient is summed using a global allreduce. This\nallreduce is the single communication and thus scalability bottleneck for most\nmachine learning workloads. We observe that frequently, many gradient values\nare (close to) zero, leading to sparse of sparsifyable communications. To\nexploit this insight, we analyze, design, and implement a set of\ncommunication-efficient protocols for sparse input data, in conjunction with\nefficient machine learning algorithms which can leverage these primitives. Our\ncommunication protocols generalize standard collective operations, by allowing\nprocesses to contribute arbitrary sparse input data vectors. Our generic\ncommunication library, SparCML, extends MPI to support additional features,\nsuch as non-blocking (asynchronous) operations and low-precision data\nrepresentations. As such, SparCML and its techniques will form the basis of\nfuture highly-scalable machine learning frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 13:02:53 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 04:05:22 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 07:46:14 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Renggli", "Cedric", ""], ["Ashkboos", "Saleh", ""], ["Aghagolzadeh", "Mehdi", ""], ["Alistarh", "Dan", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1802.08102", "submitter": "Moo-Ryong Ra", "authors": "Moo-Ryong Ra (AT&T Labs Research)", "title": "Understanding the Performance of Ceph Block Storage for Hyper-Converged\n  Cloud with All Flash Storage", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-converged cloud refers to an architecture that an operator runs compute\nand storage services on the same set of physical servers. Although the\nhyper-converged design comes with a number of benefits, it makes crucial\noperational tasks, such as capacity planning and cost analysis, fairly\ncomplicated. The problem becomes more onerous if we consider a complex\ndistributed system, such as Ceph, for the cloud with the proliferation of SSD\ndrives. In this paper, we aim to answer some of these questions based on\ncomprehensive microbenchmarks, and consequently better understand the behavior\nof Ceph in a hyper-converged cloud with all-flash storage. We reported our\nfindings based on the study, devised a cost model and compared the cost of\nhyper-converged architecture with dedicated storage architecture. Additionally\nwe summarized our experience based on the interactions with many teams at AT&T\nin the past couple of years.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 15:38:31 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Ra", "Moo-Ryong", "", "AT&T Labs Research"]]}, {"id": "1802.08159", "submitter": "Lili Su", "authors": "Lili Su, Martin Zubeldia, Nancy Lynch", "title": "Collaboratively Learning the Best Option, Using Bounded Memory", "comments": "Authors's comments: This is a preliminary preprint of our work on\n  complete graphs. New aspects of our approach on general graphs have moved to:\n  Collaboratively Learning the Best Option on Graphs, Using Bounded Local\n  Memory, arXiv:1811.03968", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-armed bandit problems in social groups wherein each\nindividual has bounded memory and shares the common goal of learning the best\narm/option. We say an individual learns the best option if eventually (as $t\n\\to \\infty$) it pulls only the arm with the highest average reward. While this\ngoal is provably impossible for an isolated individual, we show that, in social\ngroups, this goal can be achieved easily with the aid of social persuasion,\ni.e., communication. Specifically, we study the learning dynamics wherein an\nindividual sequentially decides on which arm to pull next based on not only its\nprivate reward feedback but also the suggestions provided by randomly chosen\npeers. Our learning dynamics are hard to analyze via explicit probabilistic\ncalculations due to the stochastic dependency induced by social interaction.\nInstead, we employ the mean-field approximation method from statistical physics\nand we show:\n  (1) With probability $\\to 1$ as the social group size $N \\to \\infty $, every\nindividual in the social group learns the best option.\n  (2) Over an arbitrary finite time horizon $[0, T]$, with high probability (in\n$N$), the fraction of individuals that prefer the best option grows to 1\nexponentially fast as $t$ increases ($t\\in [0, T]$).\n  A major innovation of our mean-filed analysis is a simple yet powerful\ntechnique to deal with absorbing states in the interchange of limits $N \\to\n\\infty$ and $t \\to \\infty $. The mean-field approximation method allows us to\napproximate the probabilistic sample paths of our learning dynamics by a\ndeterministic and smooth trajectory that corresponds to the unique solution of\na well-behaved system of ordinary differential equations (ODEs). Such an\napproximation is desired because the analysis of a system of ODEs is relatively\neasier than that of the original stochastic system.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 16:47:56 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 03:20:45 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 02:21:20 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Su", "Lili", ""], ["Zubeldia", "Martin", ""], ["Lynch", "Nancy", ""]]}, {"id": "1802.08176", "submitter": "George K. Thiruvathukal", "authors": "Ahmed S. Kaseb, Bo Fu, Anup Mohan, Yung-Hsiang Lu, Amy Reibman, George\n  K. Thiruvathukal", "title": "Analyzing Real-Time Multimedia Content From Network Cameras Using CPUs\n  and GPUs in the Cloud", "comments": "Accepted at MIPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Millions of network cameras are streaming real-time multimedia content\n(images or videos) for various environments (e.g., highways and malls) and can\nbe used for a variety of applications. Analyzing the content from many network\ncameras requires significant amounts of computing resources. Cloud vendors\noffer resources in the form of cloud instances with different capabilities and\nhourly costs. Some instances include GPUs that can accelerate analysis\nprograms. Doing so incurs additional monetary cost because instances with GPUs\nare more expensive. It is a challenging problem to reduce the overall monetary\ncost of using the cloud to analyze the real-time multimedia content from\nnetwork cameras while meeting the desired analysis frame rates. This paper\ndescribes a cloud resource manager that solves this problem by estimating the\nresource requirements of executing analysis programs using CPU or GPU,\nformulating the resource allocation problem as a multiple-choice vector bin\npacking problem, and solving it using an existing algorithm. The experiments\nshow that the manager can reduce up to 61\\% of the cost compared with other\nallocation strategies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:46:27 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 14:30:56 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Kaseb", "Ahmed S.", ""], ["Fu", "Bo", ""], ["Mohan", "Anup", ""], ["Lu", "Yung-Hsiang", ""], ["Reibman", "Amy", ""], ["Thiruvathukal", "George K.", ""]]}, {"id": "1802.08233", "submitter": "Saurabh Hukerikar", "authors": "Rizwan A. Ashraf, Saurabh Hukerikar, Christian Engelmann", "title": "Pattern-based Modeling of Multiresilience Solutions for High-Performance\n  Computing", "comments": "2018 ACM/SPEC International Conference on Performance Engineering\n  (ICPE '18) Berlin, Germany", "journal-ref": null, "doi": "10.1145/3184407.3184421", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resiliency is the ability of large-scale high-performance computing (HPC)\napplications to gracefully handle errors, and recover from failures. In this\npaper, we propose a pattern-based approach to constructing resilience solutions\nthat handle multiple error modes. Using resilience patterns, we evaluate the\nperformance and reliability characteristics of detection, containment and\nmitigation techniques for transient errors that cause silent data corruptions\nand techniques for fail-stop errors that result in process failures. We\ndemonstrate the design and implementation of the multiresilience solution based\non patterns instantiated across multiple layers of the system stack. The\npatterns are integrated to work together to achieve resiliency to different\nerror types in a performance-efficient manner.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:45:00 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Ashraf", "Rizwan A.", ""], ["Hukerikar", "Saurabh", ""], ["Engelmann", "Christian", ""]]}, {"id": "1802.08236", "submitter": "Xin Jin", "authors": "Xin Jin, Xiaozhou Li, Haoyu Zhang, Nate Foster, Jeongkeun Lee, Robert\n  Soule, Changhoon Kim, Ion Stoica", "title": "NetChain: Scale-Free Sub-RTT Coordination (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination services are a fundamental building block of modern cloud\nsystems, providing critical functionalities like configuration management and\ndistributed locking. The major challenge is to achieve low latency and high\nthroughput while providing strong consistency and fault-tolerance. Traditional\nserver-based solutions require multiple round-trip times (RTTs) to process a\nquery. This paper presents NetChain, a new approach that provides scale-free\nsub-RTT coordination in datacenters. NetChain exploits recent advances in\nprogrammable switches to store data and process queries entirely in the network\ndata plane. This eliminates the query processing at coordination servers and\ncuts the end-to-end latency to as little as half of an RTT---clients only\nexperience processing delay from their own software stack plus network delay,\nwhich in a datacenter setting is typically much smaller. We design new\nprotocols and algorithms based on chain replication to guarantee strong\nconsistency and to efficiently handle switch failures. We implement a prototype\nwith four Barefoot Tofino switches and four commodity servers. Evaluation\nresults show that compared to traditional server-based solutions like\nZooKeeper, our prototype provides orders of magnitude higher throughput and\nlower latency, and handles failures gracefully.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:46:39 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Jin", "Xin", ""], ["Li", "Xiaozhou", ""], ["Zhang", "Haoyu", ""], ["Foster", "Nate", ""], ["Lee", "Jeongkeun", ""], ["Soule", "Robert", ""], ["Kim", "Changhoon", ""], ["Stoica", "Ion", ""]]}, {"id": "1802.08237", "submitter": "Slobodan Mitrovi\\'c", "authors": "Mohsen Ghaffari, Themis Gouleakis, Christian Konrad, Slobodan\n  Mitrovi\\'c, Ronitt Rubinfeld", "title": "Improved Massively Parallel Computation Algorithms for MIS, Matching,\n  and Vertex Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present $O(\\log\\log n)$-round algorithms in the Massively Parallel\nComputation (MPC) model, with $\\tilde{O}(n)$ memory per machine, that compute a\nmaximal independent set, a $1+\\epsilon$ approximation of maximum matching, and\na $2+\\epsilon$ approximation of minimum vertex cover, for any $n$-vertex graph\nand any constant $\\epsilon>0$. These improve the state of the art as follows:\n  - Our MIS algorithm leads to a simple $O(\\log\\log \\Delta)$-round MIS\nalgorithm in the Congested Clique model of distributed computing, which\nimproves on the $\\tilde{O}(\\sqrt{\\log \\Delta})$-round algorithm of Ghaffari\n[PODC'17].\n  - Our $O(\\log\\log n)$-round $(1+\\epsilon)$-approximate maximum matching\nalgorithm simplifies or improves on the following prior work: $O(\\log^2\\log\nn)$-round $(1+\\epsilon)$-approximation algorithm of Czumaj et al. [STOC'18] and\n$O(\\log\\log n)$-round $(1+\\epsilon)$-approximation algorithm of Assadi et al.\n[SODA'19].\n  - Our $O(\\log\\log n)$-round $(2+\\epsilon)$-approximate minimum vertex cover\nalgorithm improves on an $O(\\log\\log n)$-round $O(1)$-approximation of Assadi\net al. [arXiv'17].\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:48:48 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 18:41:27 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 17:35:25 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Gouleakis", "Themis", ""], ["Konrad", "Christian", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1802.08254", "submitter": "Wanling Gao", "authors": "Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Xu\n  Wen, Rui Ren, Chen Zheng, Xiwen He, Hainan Ye, Haoning Tang, Zheng Cao,\n  Shujie Zhang and Jiahui Dai", "title": "BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several fundamental changes in technology indicate domain-specific hardware\nand software co-design is the only path left. In this context, architecture,\nsystem, data management, and machine learning communities pay greater attention\nto innovative big data and AI algorithms, architecture, and systems.\nUnfortunately, complexity, diversity, frequently-changed workloads, and rapid\nevolution of big data and AI systems raise great challenges. First, the\ntraditional benchmarking methodology that creates a new benchmark or proxy for\nevery possible workload is not scalable, or even impossible for Big Data and AI\nbenchmarking. Second, it is prohibitively expensive to tailor the architecture\nto characteristics of one or more application or even a domain of applications.\nWe consider each big data and AI workload as a pipeline of one or more classes\nof units of computation performed on different initial or intermediate data\ninputs, each class of which we call a data motif. On the basis of our previous\nwork that identifies eight data motifs taking up most of the run time of a wide\nvariety of big data and AI workloads, we propose a scalable benchmarking\nmethodology that uses the combination of one or more data motifs---to represent\ndiversity of big data and AI workloads. Following this methodology, we present\na unified big data and AI benchmark suite---BigDataBench 4.0, publicly\navailable from~\\url{http://prof.ict.ac.cn/BigDataBench}. This unified benchmark\nsuite sheds new light on domain-specific hardware and software co-design:\ntailoring the system and architecture to characteristics of the unified eight\ndata motifs other than one or more application case by case. Also, for the\nfirst time, we comprehensively characterize the CPU pipeline efficiency using\nthe benchmarks of seven workload types in BigDataBench 4.0.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 01:28:44 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 05:52:50 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Gao", "Wanling", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Zheng", "Daoyi", ""], ["Wen", "Xu", ""], ["Ren", "Rui", ""], ["Zheng", "Chen", ""], ["He", "Xiwen", ""], ["Ye", "Hainan", ""], ["Tang", "Haoning", ""], ["Cao", "Zheng", ""], ["Zhang", "Shujie", ""], ["Dai", "Jiahui", ""]]}, {"id": "1802.08417", "submitter": "Yanjun Han", "authors": "Yanjun Han, Ayfer \\\"Ozg\\\"ur, Tsachy Weissman", "title": "Geometric Lower Bounds for Distributed Parameter Estimation under\n  Communication Constraints", "comments": "This version (v4) added a new corollary on logistic regression, as\n  well as more discussions on sparse Gaussian mean estimation, compared to v3", "journal-ref": "published in COLT 2018", "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parameter estimation in distributed networks, where each sensor\nin the network observes an independent sample from an underlying distribution\nand has $k$ bits to communicate its sample to a centralized processor which\ncomputes an estimate of a desired parameter. We develop lower bounds for the\nminimax risk of estimating the underlying parameter for a large class of losses\nand distributions. Our results show that under mild regularity conditions, the\ncommunication constraint reduces the effective sample size by a factor of $d$\nwhen $k$ is small, where $d$ is the dimension of the estimated parameter.\nFurthermore, this penalty reduces at most exponentially with increasing $k$,\nwhich is the case for some models, e.g., estimating high-dimensional\ndistributions. For other models however, we show that the sample size reduction\nis re-mediated only linearly with increasing $k$, e.g. when some sub-Gaussian\nstructure is available. We apply our results to the distributed setting with\nproduct Bernoulli model, multinomial model, Gaussian location models, and\nlogistic regression which recover or strengthen existing results.\n  Our approach significantly deviates from existing approaches for developing\ninformation-theoretic lower bounds for communication-efficient estimation. We\ncircumvent the need for strong data processing inequalities used in prior work\nand develop a geometric approach which builds on a new representation of the\ncommunication constraint. This approach allows us to strengthen and generalize\nexisting results with simpler and more transparent proofs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 07:34:50 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 23:22:17 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 08:53:17 GMT"}, {"version": "v4", "created": "Thu, 22 Jul 2021 09:55:24 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Han", "Yanjun", ""], ["\u00d6zg\u00fcr", "Ayfer", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1802.08469", "submitter": "Nicolas Markey", "authors": "A. R. Balasubramanian, Nathalie Bertrand, Nicolas Markey", "title": "Parameterized verification of synchronization in constrained\n  reconfigurable broadcast networks", "comments": "Accepted for publication in TACAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.FL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reconfigurable broadcast networks provide a convenient formalism for\nmodelling and reasoning about networks of mobile agents broadcasting messages\nto other agents following some (evolving) communication topology. The\nparameterized verification of such models aims at checking whether a given\nproperty holds irrespective of the initial configuration (number of agents,\ninitial states and initial communication topology). We focus here on the\nsynchronization property, asking whether all agents converge to a set of target\nstates after some execution. This problem is known to be decidable in\npolynomial time when no constraints are imposed on the evolution of the\ncommunication topology (while it is undecidable for static broadcast networks).\n  In this paper we investigate how various constraints on reconfigurations\naffect the decidability and complexity of the synchronization problem. In\nparticular, we show that when bounding the number of reconfigured links between\ntwo communications steps by a constant, synchronization becomes undecidable; on\nthe other hand, synchronization remains decidable in PTIME when the bound grows\nwith the number of agents.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 10:17:50 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Balasubramanian", "A. R.", ""], ["Bertrand", "Nathalie", ""], ["Markey", "Nicolas", ""]]}, {"id": "1802.08474", "submitter": "Valter Balegas", "authors": "Valter Balegas, Nuno Pregui\\c{c}a, S\\'ergio Duarte, Carla Ferreira,\n  Rodrigo Rodrigues", "title": "IPA: Invariant-preserving Applications for Weakly-consistent Replicated\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage systems based on Weak Consistency provide better availability and\nlower latency than systems that use Strong Consistency, especially in\ngeo-replicated settings. However, under Weak Consistency, it is harder to\nensure the correctness of applications, as the execution of uncoordinated\noperations may lead to invalid states.\n  In this paper we show how to modify an application to make it run correctly\nunder Weak Consistency. We developed an analysis that detects which operations\nneed to be corrected, and proposes possible modifications to operations to\nprevent inconsistencies. This analysis allows the programmer to choose the\npreferred semantics for each problematic execution, while preserving the\noriginal semantics of operations when no conflicts occur. The modified\napplication runs with small overhead when compared with its Weak Consistency\ncounterpart, which cannot preserve application correctness.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 10:38:51 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 09:54:39 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Balegas", "Valter", ""], ["Pregui\u00e7a", "Nuno", ""], ["Duarte", "S\u00e9rgio", ""], ["Ferreira", "Carla", ""], ["Rodrigues", "Rodrigo", ""]]}, {"id": "1802.08483", "submitter": "Johann Briffa", "authors": "Johann A. Briffa", "title": "GPU Implementation and Optimization of a Flexible MAP Decoder for\n  Synchronization Correction", "comments": null, "journal-ref": "J. A. Briffa, \"Graphics processing unit implementation and\n  optimization of a flexible maximum a-posteriori decoder for synchronisation\n  correction\", IET Journal of Engineering, Jun. 2014", "doi": "10.1049/joe.2014.0049", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an optimized parallel implementation of a flexible\nMAP decoder for synchronization error correcting codes, supporting a very wide\nrange of code sizes and channel conditions. On mid-range GPUs we demonstrate\ndecoding speedups of more than two orders of magnitude over a CPU\nimplementation of the same optimized algorithm, and more than an order of\nmagnitude over our earlier GPU implementation. The prominent challenge is to\nmaintain high parallelization efficiency over a wide range of code sizes and\nchannel conditions, and different execution hardware. We ensure this with a\ndynamic strategy for choosing parallel execution parameters at run-time. We\nalso present a variant that trades off some decoding speed for significantly\nreduced memory requirement, with no loss to the decoder's error correction\nperformance. The increased throughput of our implementation and its ability to\nwork with less memory allow us to analyse larger codes and poorer channel\nconditions, and makes practical use of such codes more feasible.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 11:12:07 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Briffa", "Johann A.", ""]]}, {"id": "1802.08557", "submitter": "Amit Gurung", "authors": "Amit Gurung and Rajarshi Ray", "title": "Simultaneous Solving of Batched Linear Programs on a GPU", "comments": "Around 13 figures and 24 pages. arXiv admin note: substantial text\n  overlap with arXiv:1609.08114", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Programs (LPs) appear in a large number of applications and offloading\nthem to a GPU is viable to gain performance. Existing work on offloading and\nsolving an LP on a GPU suggests that there is performance gain generally on\nlarge sized LPs (typically 500 constraints, 500 variables and above). In order\nto gain performance from a GPU, for applications involving small to medium\nsized LPs, we propose batched solving of a large number of LPs in parallel. In\nthis paper, we present the design and implementation of a batched LP solver in\nCUDA, keeping memory coalescent access, low CPU-GPU memory transfer latency and\nload balancing as the goals. The performance of the batched LP solver is\ncompared against sequential solving in the CPU using the open source solver\nGLPK (GNU Linear Programming Kit) and the CPLEX solver from IBM. The evaluation\non selected LP benchmarks from the Netlib repository displays a maximum\nspeed-up of 95x and 5x with respect to CPLEX and GLPK solver respectively, for\na batch of 1e5 LPs. We demonstrate the application of our batched LP solver to\nenhance performance in the domain of state-space exploration of mathematical\nmodels of control systems design.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:02:11 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Gurung", "Amit", ""], ["Ray", "Rajarshi", ""]]}, {"id": "1802.08603", "submitter": "Alexander Engelmann", "authors": "Alexander Engelmann, Yuning Jiang, Tillmann M\\\"uhlpfordt, Boris\n  Houska, Timm Faulwasser", "title": "Towards Distributed OPF using ALADIN", "comments": null, "journal-ref": null, "doi": "10.1109/TPWRS.2018.2867682", "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper discusses the application of the recently proposed\nAugmented Lagrangian Alternating Direction Inexact Newton (ALADIN) method to\nnon-convex AC Optimal Power Flow Problems (OPF) in a distributed fashion. In\ncontrast to the often used Alternating Direction of Multipliers Method (ADMM),\nALADIN guarantees locally quadratic convergence for AC OPF. Numerical results\nfor 5 to 300 bus test cases indicate that ALADIN is able to outperform ADMM and\nto reduce the number of iterations by about one order of magnitude. We compare\nALADIN to numerical results for ADMM documented in the literature. The improved\nconvergence speed comes at the cost of increasing the communication effort per\niteration. Therefore, we propose a variant of ALADIN that uses inexact Hessians\nto reduce communication. Additionally, we provide a detailed comparison of\nthese ALADIN variants to ADMM from an algorithmic and communication\nperspective. Moreover, we prove that ALADIN converges locally at quadratic rate\neven for the relevant case of suboptimally solved local NLPs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:37:31 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 11:34:06 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Engelmann", "Alexander", ""], ["Jiang", "Yuning", ""], ["M\u00fchlpfordt", "Tillmann", ""], ["Houska", "Boris", ""], ["Faulwasser", "Timm", ""]]}, {"id": "1802.08733", "submitter": "Nicholas V. Lewchenko", "authors": "Nicholas V. Lewchenko, Arjun Radhakrishna, Akash Gaonkar, Pavol\n  \\v{C}ern\\'y", "title": "Conflict-Aware Replicated Data Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Conflict-Aware Replicated Data Types (CARDs). CARDs are\nsignificantly more expressive than Conflict-free Replicated Data Types (CRDTs)\nas they support operations that can conflict with each other. Introducing\nconflicting operations typically brings the need to block an operation in at\nleast some executions, leading to difficulties in programming and reasoning\nabout correctness, as well as potential inefficiencies in implementation.\n  The salient aspect of CARDs is that they allow ease of programming and\nreasoning about programs comparable to CRDTs, while enabling algorithmic\ninference of conflicts so that an operation is blocked only when necessary. The\nkey idea is to have a language that allows associating with each operation a\ntwo-state predicate called {\\em consistency guard} that relates the state of\nthe replica on which the operation is executing to a global state (which is\nnever computed). The consistency guards bring three advantages. First, a\nprogrammer developing an operation needs only to choose a consistency guard\nthat states what the operation will rely on. In particular, they do not need to\nconsider the operation conflicts with other operation. This allows purely {\\em\nmodular reasoning}. Second, we show that consistency guard allow reducing the\ncomplexity of reasoning needed to prove invariants that hold as CARD operations\nare executing. The reason is that consistency guard allow reducing the\nreasoning about concurrency among operations to purely {\\em sequential\nreasoning}. Third, conflicts among operations can be algorithmically inferred\nby checking whether the effect of one operation preserves the consistency guard\nof another operation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 20:48:19 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 16:55:18 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Lewchenko", "Nicholas V.", ""], ["Radhakrishna", "Arjun", ""], ["Gaonkar", "Akash", ""], ["\u010cern\u00fd", "Pavol", ""]]}, {"id": "1802.08751", "submitter": "Lili Wang", "authors": "L. Wang, J. Liu, A. S. Morse, B. D. O. Anderson and D. Fullmer", "title": "A Generalized Discrete-Time Altafini Model", "comments": "7 pages, 3 figures, ECC paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discrete-time modulus consensus model is considered in which the\ninteraction among a family of networked agents is described by a time-dependent\ngain graph whose vertices correspond to agents and whose arcs are assigned\ncomplex numbers from a cyclic group. Limiting behavior of the model is studied\nusing a graphical approach. It is shown that, under appropriate connectedness,\na certain type of clustering will be reached exponentially fast for almost all\ninitial conditions if and only if the sequence of gain graphs is \"repeatedly\njointly structurally balanced\" corresponding to that type of clustering, where\nthe number of clusters is at most the order of a cyclic group. It is also shown\nthat the model will reach a consensus asymptotically at zero if the sequence of\ngain graphs is repeatedly jointly strongly connected and structurally\nunbalanced. In the special case when the cyclic group is of order two, the\nmodel simplifies to the so-called Altafini model whose gain graph is simply a\nsigned graph.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 22:27:47 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Wang", "L.", ""], ["Liu", "J.", ""], ["Morse", "A. S.", ""], ["Anderson", "B. D. O.", ""], ["Fullmer", "D.", ""]]}, {"id": "1802.08800", "submitter": "Florin Rusu", "authors": "Yujing Ma, Florin Rusu, Martin Torres", "title": "Stochastic Gradient Descent on Highly-Parallel Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increased interest in building data analytics frameworks with\nadvanced algebraic capabilities both in industry and academia. Many of these\nframeworks, e.g., TensorFlow and BIDMach, implement their compute-intensive\nprimitives in two flavors---as multi-thread routines for multi-core CPUs and as\nhighly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is\nthe most popular optimization method for model training implemented extensively\non modern data analytics platforms. While the data-intensive properties of SGD\nare well-known, there is an intense debate on which of the many SGD variants is\nbetter in practice. In this paper, we perform a comprehensive study of parallel\nSGD for training generalized linear models. We consider the impact of three\nfactors -- computing architecture (multi-core CPU or GPU), synchronous or\nasynchronous model updates, and data sparsity -- on three measures---hardware\nefficiency, statistical efficiency, and time to convergence. In the process, we\ndesign an optimized asynchronous SGD algorithm for GPU that leverages warp\nshuffling and cache coalescing for data and model access. We draw several\ninteresting findings from our extensive experiments with logistic regression\n(LR) and support vector machines (SVM) on five real datasets. For synchronous\nSGD, GPU always outperforms parallel CPU---they both outperform a sequential\nCPU solution by more than 400X. For asynchronous SGD, parallel CPU is the\nsafest choice while GPU with data replication is better in certain situations.\nThe choice between synchronous GPU and asynchronous CPU depends on the task and\nthe characteristics of the data. As a reference, our best implementation\noutperforms TensorFlow and BIDMach consistently. We hope that our insights\nprovide a useful guide for applying parallel SGD to generalized linear models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 05:27:04 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Ma", "Yujing", ""], ["Rusu", "Florin", ""], ["Torres", "Martin", ""]]}, {"id": "1802.09080", "submitter": "Mohammad Noormohammadpour", "authors": "Mohammad Noormohammadpour and Cauligi S. Raghavendra", "title": "Minimizing Flow Completion Times using Adaptive Routing over\n  Inter-Datacenter Wide Area Networks", "comments": "Accepted into IEEE INFOCOM 2018 Poster Sessions, Honolulu, HI, USA", "journal-ref": "IEEE INFOCOM 2018 - IEEE Conference on Computer Communications\n  Workshops (2018) 1046-1047", "doi": "10.1109/INFCOMW.2018.8406853", "report-no": null, "categories": "cs.NI cs.DC cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-datacenter networks connect dozens of geographically dispersed\ndatacenters and carry traffic flows with highly variable sizes and different\nclasses. Adaptive flow routing can improve efficiency and performance by\nassigning paths to new flows according to network status and flow properties. A\npopular approach widely used for traffic engineering is based on current\nbandwidth utilization of links. We propose an alternative that reduces\nbandwidth usage by up to at least 50% and flow completion times by up to at\nleast 40% across various scheduling policies and flow size distributions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 21:14:30 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Noormohammadpour", "Mohammad", ""], ["Raghavendra", "Cauligi S.", ""]]}, {"id": "1802.09113", "submitter": "Sudhir Kylasa", "authors": "Sudhir B. Kylasa, Farbod Roosta-Khorasani, Michael W. Mahoney and\n  Ananth Grama", "title": "GPU Accelerated Sub-Sampled Newton's Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First order methods, which solely rely on gradient information, are commonly\nused in diverse machine learning (ML) and data analysis (DA) applications. This\nis attributed to the simplicity of their implementations, as well as low\nper-iteration computational/storage costs. However, they suffer from\nsignificant disadvantages; most notably, their performance degrades with\nincreasing problem ill-conditioning. Furthermore, they often involve a large\nnumber of hyper-parameters, and are notoriously sensitive to parameters such as\nthe step-size. By incorporating additional information from the Hessian,\nsecond-order methods, have been shown to be resilient to many such adversarial\neffects. However, these advantages of using curvature information come at the\ncost of higher per-iteration costs, which in \\enquote{big data} regimes, can be\ncomputationally prohibitive.\n  In this paper, we show that, contrary to conventional belief, second-order\nmethods, when implemented appropriately, can be more efficient than first-order\nalternatives in many large-scale ML/ DA applications. In particular, in convex\nsettings, we consider variants of classical Newton\\textsf{'}s method in which\nthe Hessian and/or the gradient are randomly sub-sampled. We show that by\neffectively leveraging the power of GPUs, such randomized Newton-type\nalgorithms can be significantly accelerated, and can easily outperform state of\nthe art implementations of existing techniques in popular ML/ DA software\npackages such as TensorFlow. Additionally these randomized methods incur a\nsmall memory overhead compared to first-order methods. In particular, we show\nthat for million-dimensional problems, our GPU accelerated sub-sampled\nNewton\\textsf{'}s method achieves a higher test accuracy in milliseconds as\ncompared with tens of seconds for first order alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 00:32:31 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 01:51:02 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Kylasa", "Sudhir B.", ""], ["Roosta-Khorasani", "Farbod", ""], ["Mahoney", "Michael W.", ""], ["Grama", "Ananth", ""]]}, {"id": "1802.09180", "submitter": "Tomer Kaftan", "authors": "Tomer Kaftan, Magdalena Balazinska, Alvin Cheung, Johannes Gehrke", "title": "Cuttlefish: A Lightweight Primitive for Adaptive Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data processing applications execute increasingly sophisticated\nanalysis that requires operations beyond traditional relational algebra. As a\nresult, operators in query plans grow in diversity and complexity. Designing\nquery optimizer rules and cost models to choose physical operators for all of\nthese novel logical operators is impractical. To address this challenge, we\ndevelop Cuttlefish, a new primitive for adaptively processing online query\nplans that explores candidate physical operator instances during query\nexecution and exploits the fastest ones using multi-armed bandit reinforcement\nlearning techniques. We prototype Cuttlefish in Apache Spark and adaptively\nchoose operators for image convolution, regular expression matching, and\nrelational joins. Our experiments show Cuttlefish-based adaptive convolution\nand regular expression operators can reach 72-99% of the throughput of an\nall-knowing oracle that always selects the optimal algorithm, even when\nindividual physical operators are up to 105x slower than the optimal.\nAdditionally, Cuttlefish achieves join throughput improvements of up to 7.5x\ncompared with Spark SQL's query optimizer.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 06:50:43 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kaftan", "Tomer", ""], ["Balazinska", "Magdalena", ""], ["Cheung", "Alvin", ""], ["Gehrke", "Johannes", ""]]}, {"id": "1802.09205", "submitter": "Geppino Pucci", "authors": "Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci", "title": "Solving $k$-center Clustering (with Outliers) in MapReduce and\n  Streaming, almost as Accurately as Sequentially", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Center-based clustering is a fundamental primitive for data analysis and\nbecomes very challenging for large datasets. In this paper, we focus on the\npopular $k$-center variant which, given a set $S$ of points from some metric\nspace and a parameter $k<|S|$, requires to identify a subset of $k$ centers in\n$S$ minimizing the maximum distance of any point of $S$ from its closest\ncenter. A more general formulation, introduced to deal with noisy datasets,\nfeatures a further parameter $z$ and allows up to $z$ points of $S$ (outliers)\nto be disregarded when computing the maximum distance from the centers. We\npresent coreset-based 2-round MapReduce algorithms for the above two\nformulations of the problem, and a 1-pass Streaming algorithm for the case with\noutliers. For any fixed $\\epsilon>0$, the algorithms yield solutions whose\napproximation ratios are a mere additive term $\\epsilon$ away from those\nachievable by the best known polynomial-time sequential algorithms, a result\nthat substantially improves upon the state of the art. Our algorithms are\nrather simple and adapt to the intrinsic complexity of the dataset, captured by\nthe doubling dimension $D$ of the metric space. Specifically, our analysis\nshows that the algorithms become very space-efficient for the important case of\nsmall (constant) $D$. These theoretical results are complemented with a set of\nexperiments on real-world and synthetic datasets of up to over a billion\npoints, which show that our algorithms yield better quality solutions over the\nstate of the art while featuring excellent scalability, and that they also lend\nthemselves to sequential implementations much faster than existing ones.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 09:01:45 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 15:04:53 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 13:08:10 GMT"}, {"version": "v4", "created": "Fri, 12 Oct 2018 13:34:56 GMT"}, {"version": "v5", "created": "Wed, 16 Jan 2019 07:06:57 GMT"}, {"version": "v6", "created": "Tue, 1 Jun 2021 15:56:13 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""]]}, {"id": "1802.09207", "submitter": "Stig Bosmans", "authors": "Stig Bosmans, Siegfried Mercelis Peter Hellinckx and Joachim Denil", "title": "Towards evaluating emergent behavior of the Internet of Things using\n  large scale simulation techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in Internet of Things devices and more decentralized\narchitectures we see a new type of application gain importance, a type where\nlocal interactions between individual entities lead to a global emergent\nbehavior, Emergent-based IoT (EBI) Systems. In this position paper we explore\ntechniques to evaluate this emergent behavior in IoT applications. Because of\nthe required scale and diversity this is not an easy task. Therefore, we mainly\nfocus on a distributed simulation approach and provide an overview of possible\ntechniques that could optimize the overall simulation performance. Our focus is\nboth on modeling and simulation technology.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 09:05:08 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Bosmans", "Stig", ""], ["Hellinckx", "Siegfried Mercelis Peter", ""], ["Denil", "Joachim", ""]]}, {"id": "1802.09462", "submitter": "Xin Zhao", "authors": "Xin Zhao, Philipp Haller", "title": "Observable atomic consistency for CvRDTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of distributed systems requires developers to balance the\nneed for consistency, availability, and partition tolerance. Conflict-free\nreplicated data types (CRDTs) are widely used in eventually consistent systems\nto reduce concurrency control. However, due to the lack of consistent\nnon-monotonic operations, the usage of CRDTs can be difficult. In this paper,\nwe propose a new consistency protocol, the observable atomic consistency\nprotocol (OACP). OACP enables a principled relax- ation of strong consistency\nto improve performance in specific scenarios. OACP combines the advantages of\nmergeable data types, specifically, convergent replicated data types, and\nreliable total order broadcast to provide on-demand strong consistency. By\nproviding observable atomic consistency, OACP avoids the anomalies of related\nprotocols. We provide a distributed, cluster-enabled implementation of OACP\nbased on Akka, a widely-used actor-based middleware. Our experimental\nevaluation shows that OACP can reduce the coordination between replicas\ncompared to other protocols providing atomic consistency in several bench-\nmarks. Our results also suggest that OACP gains availability through mergeable\ndata types and provides acceptable latency for achieving strong consistency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 17:15:31 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 11:09:03 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhao", "Xin", ""], ["Haller", "Philipp", ""]]}, {"id": "1802.09478", "submitter": "Harald B\\\"ogeholz", "authors": "Harald B\\\"ogeholz, Michael Brand, Radu-Alexandru Todor", "title": "In-database connected component analysis", "comments": "major revision with new datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Big Data-practical, SQL-implementable algorithm for efficiently\ndetermining connected components for graph data stored in a Massively Parallel\nProcessing (MPP) relational database. The algorithm described is a\nlinear-space, randomised algorithm, always terminating with the correct answer\nbut subject to a stochastic running time, such that for any $\\epsilon>0$ and\nany input graph $G=\\langle V, E \\rangle$ the algorithm terminates after\n$\\mathop{\\text{O}}(\\log |V|)$ SQL queries with probability of at least\n$1-\\epsilon$, which we show empirically to translate to a quasi-linear runtime\nin practice.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 17:55:03 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 11:18:15 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["B\u00f6geholz", "Harald", ""], ["Brand", "Michael", ""], ["Todor", "Radu-Alexandru", ""]]}, {"id": "1802.09687", "submitter": "Saksham Chand", "authors": "Saksham Chand, Yanhong A. Liu", "title": "Simpler Specifications and Easier Proofs of Distributed Algorithms Using\n  History Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies specifications and proofs of distributed algorithms when\nonly message history variables are used, using the Basic Paxos and Multi-Paxos\nalgorithms for distributed consensus as precise case studies. We show that not\nusing and maintaining other state variables yields simpler specifications that\nare more declarative and easier to understand. It also allows easier proofs to\nbe developed by needing fewer invariants and facilitating proof derivations.\nFurthermore, the proofs are mechanically checked more efficiently.\n  We show that specifications in TLA+, Lamport's temporal logic of actions, and\nproofs in TLAPS, the TLA+ Proof System (TLAPS) are reduced by a quarter or more\nfor single-value Paxos and by about half or more for multi-value Paxos. Overall\nwe need about half as many manually written invariants and proof obligations.\nOur proof for Basic Paxos takes about 25% less time for TLAPS to check, and our\nproofs for Multi-Paxos are checked within 1.5 minutes whereas prior proofs fail\nto be checked by TLAPS.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 02:13:49 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 18:30:52 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 22:38:38 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Chand", "Saksham", ""], ["Liu", "Yanhong A.", ""]]}, {"id": "1802.09877", "submitter": "Maria Potop-Butucaru", "authors": "Emmanuelle Anceaume (CNRS, CIDRE), Antonella Del Pozzo (LIST), Romaric\n  Ludinard (IMT Atlantique, ADOPNET), Maria Potop-Butucaru (LINCS, NPA), Sara\n  Tucci-Piergiovanni (LIST)", "title": "Blockchain Abstract Data Type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented work continues the line of recent distributed computing\ncommunityefforts dedicated to the theoretical aspects of blockchains. This\npaper is the rst tospecify blockchains as a composition of abstract data types\nall together with a hierarchyof consistency criteria that formally\ncharacterizes the histories admissible for distributedprograms that use them.\nOur work is based on an original oracle-based constructionthat, along with new\nconsistency deffnitions, captures the eventual convergence processin blockchain\nsystems. The paper presents as well some results on implementability ofthe\npresented abstractions and a mapping of representative existing blockchains\nfromboth academia and industry in our framework.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 13:37:34 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 11:59:00 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Anceaume", "Emmanuelle", "", "CNRS, CIDRE"], ["Del Pozzo", "Antonella", "", "LIST"], ["Ludinard", "Romaric", "", "IMT Atlantique, ADOPNET"], ["Potop-Butucaru", "Maria", "", "LINCS, NPA"], ["Tucci-Piergiovanni", "Sara", "", "LIST"]]}, {"id": "1802.09941", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Torsten Hoefler", "title": "Demystifying Parallel and Distributed Deep Learning: An In-Depth\n  Concurrency Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are becoming an important tool in modern\ncomputing applications. Accelerating their training is a major challenge and\ntechniques range from distributed algorithms to low-level circuit design. In\nthis survey, we describe the problem from a theoretical perspective, followed\nby approaches for its parallelization. We present trends in DNN architectures\nand the resulting implications on parallelization strategies. We then review\nand model the different types of concurrency in DNNs: from the single operator,\nthrough parallelism in network inference and training, to distributed deep\nlearning. We discuss asynchronous stochastic optimization, distributed system\narchitectures, communication schemes, and neural architecture search. Based on\nthose approaches, we extrapolate potential directions for parallelism in deep\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 08:47:34 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 08:36:28 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1802.10116", "submitter": "Cong Xie", "authors": "Cong Xie, Oluwasanmi Koyejo, Indranil Gupta", "title": "Generalized Byzantine-tolerant SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three new robust aggregation rules for distributed synchronous\nStochastic Gradient Descent~(SGD) under a general Byzantine failure model. The\nattackers can arbitrarily manipulate the data transferred between the servers\nand the workers in the parameter server~(PS) architecture. We prove the\nByzantine resilience properties of these aggregation rules. Empirical analysis\nshows that the proposed techniques outperform current approaches for realistic\nuse cases and Byzantine attack scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 19:06:15 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 21:20:17 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 20:08:04 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Oluwasanmi", ""], ["Gupta", "Indranil", ""]]}, {"id": "1802.10199", "submitter": "Philipp Bamberger", "authors": "Philipp Bamberger, Fabian Kuhn, Yannic Maus", "title": "Local Distributed Algorithms in Highly Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper studies local distributed graph problems in highly dynamic\nnetworks. Communication and changes of the graph happen in synchronous rounds\nand our algorithms always, i.e., in every round, satisfy non-trivial\nguarantees, no matter how dynamic the network is.\n  We define a (in our view) natural generalization of static graph problems to\nthe dynamic graph setting. Throughout the execution of an algorithm we consider\na sliding window over the last $T$, e.g., polylogarithmic, rounds. Then, in\nsome round, the feasibility of an output only depends on the topology of the\ngraphs in the current sliding window and we call a feasible output a\n$T$-dynamic solution. The guarantees of a $T$-dynamic solution become stronger\nthe more stable the graph is during this sliding window and, in particular,\nthey coincide with the definition of the static graph problem if the graph is\nstatic throughout the window. We further present an abstract framework that\nallows to develop algorithms that output $T$-dynamic solutions in all rounds.\nThe resulting algorithms have another desirable property: If a constant\nneighborhood around some part of the graph is stable during an interval\n$[t_1,t_2]$, the algorithms compute a static solution for this part of the\ngraph throughout the interval $[t_1+T',t_2]$ for some (small) $T'>0$.\n  We demonstrate our generic framework with two sample problems that abstract\nbasic operations in dynamic networks, namely $\\textit{(degree+1)-vertex\ncoloring}$ and $\\textit{maximal independent set (MIS)}$. To illustrate the\ngiven guarantees consider the vertex coloring problem: The sliding window of\nour (randomized) algorithm is of length $T=O(\\log n)$ and any conflict between\ntwo nodes caused by a newly inserted edge is resolved within that time. During\nthis conflict resolving both nodes always output colors that are not in\nconflict with their respective 'old' neighbors.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 22:54:56 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 16:55:15 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 15:13:46 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Bamberger", "Philipp", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "1802.10280", "submitter": "Xuhao Chen", "authors": "Xuhao Chen", "title": "Escoin: Efficient Sparse Convolutional Neural Network Inference on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved remarkable accuracy in many artificial\nintelligence applications, e.g. computer vision, at the cost of a large number\nof parameters and high computational complexity. Weight pruning can compress\nDNN models by removing redundant parameters in the networks, but it brings\nsparsity in the weight matrix, and therefore makes the computation inefficient\non GPUs. Although pruning can remove more than 80% of the weights, it actually\nhurts inference performance (speed) when running models on GPUs.\n  Two major problems cause this unsatisfactory performance on GPUs. First,\nlowering convolution onto matrix multiplication reduces data reuse\nopportunities and wastes memory bandwidth. Second, the sparsity brought by\npruning makes the computation irregular, which leads to inefficiency when\nrunning on massively parallel GPUs. To overcome these two limitations, we\npropose Escort, an efficient sparse convolutional neural networks on GPUs.\nInstead of using the lowering method, we choose to compute the sparse\nconvolutions directly. We then orchestrate the parallelism and locality for the\ndirect sparse convolution kernel, and apply customized optimization techniques\nto further improve performance. Evaluation on NVIDIA GPUs show that Escort can\nimprove sparse convolution speed by 2.63x and 3.07x, and inference speed by\n1.43x and 1.69x, compared to CUBLAS and CUSPARSE respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 06:31:45 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 21:11:27 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Chen", "Xuhao", ""]]}, {"id": "1802.10297", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi", "title": "Semi-MapReduce Meets Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph problems are troublesome when it comes to MapReduce. Typically, to be\nable to design algorithms that make use of the advantages of MapReduce,\nassumptions beyond what the model imposes, such as the density of the input\ngraph, are required.\n  In a recent shift, a simple and robust model of MapReduce for graph problems,\nwhere the space per machine is set to be O(|V|), has attracted considerable\nattention. We term this model semi-MapReduce, or in short, semiMPC, and focus\non its computational power.\n  We show through a set of simulation methods that semiMPC is, perhaps\nsurprisingly, equivalent to the congested clique model of distributed\ncomputing. However, semiMPC, in addition to round complexity, incorporates\nanother practically important dimension to optimize: the number of machines.\nFurthermore, we show that algorithms in other distributed computing models,\nsuch as CONGEST, can be simulated to run in the same number of rounds of\nsemiMPC while also using an optimal number of machines. We later show the\nimplications of these simulation methods by obtaining improved algorithms for\nthese models using the recent algorithms that have been developed.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 07:50:04 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 22:48:13 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Derakhshan", "Mahsa", ""], ["Hajiaghayi", "MohammadTaghi", ""]]}, {"id": "1802.10375", "submitter": "Pablo Chico De Guzman", "authors": "Pablo Chico de Guzman, Felipe Gorostiaga, Cesar Sanchez", "title": "i2kit: A Tool for Immutable Infrastructure Deployments based on\n  Lightweight Virtual Machines specialized to run Containers", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Container technologies, like Docker, are becoming increasingly popular.\nContainers provide exceptional developer experience because containers offer\nlightweight isolation and ease of software distribution. Containers are also\nwidely used in production environments, where a different set of challenges\narise such as security, networking, service discovery and load balancing.\nContainer cluster management tools, such as Kubernetes, attempt to solve these\nproblems by introducing a new control layer with the container as the unit of\ndeployment. However, adding a new control layer is an extra configuration step\nand an additional potential source of runtime errors. The virtual machine\ntechnology offered by cloud providers is more mature and proven in terms of\nsecurity, networking, service discovery and load balancing. However, virtual\nmachines are heavier than containers for local development, are less flexible\nfor resource allocation, and suffer longer boot times. This paper presents an\nalternative to containers that enjoy the best features of both approaches: (1)\nthe use of mature, proven cloud vendor technology; (2) no need for a new\ncontrol layer; and (3) as lightweight as containers. Our solution is i2kit, a\ndeployment tool based on the immutable infrastructure pattern, where the\nvirtual machine is the unit of deployment. The i2kit tool accepts a simplified\nformat of Kubernetes Deployment Manifests in order to reuse Kubernetes' most\nsuccessful principles, but it creates a lightweight virtual machine for each\nPod using Linuxkit. Linuxkit alleviates the drawback in size that using virtual\nmachines would otherwise entail, because the footprint of Linuxkit is\napproximately 60MB. Finally, the attack surface of the system is reduced since\nLinuxkit only installs the minimum set of OS dependencies to run containers,\nand different Pods are isolated by hypervisor technology.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 11:51:21 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["de Guzman", "Pablo Chico", ""], ["Gorostiaga", "Felipe", ""], ["Sanchez", "Cesar", ""]]}, {"id": "1802.10376", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen, Georg von der Br\\\"uggen, and Niklas Ueter", "title": "Push Forward: Global Fixed-Priority Scheduling of Arbitrary-Deadline\n  Sporadic Task Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sporadic task model is often used to analyze recurrent execution of\nidentical tasks in real-time systems. A sporadic task defines an infinite\nsequence of task instances, also called jobs, that arrive under the minimum\ninter-arrival time constraint. To ensure the system safety, timeliness has to\nbe guaranteed in addition to functional correctness, i.e., all jobs of all\ntasks have to be finished before the job deadlines. We focus on analyzing\narbitrary-deadline task sets on a homogeneous (identical) multiprocessor system\nunder any given global fixed-priority scheduling approach and provide a series\nof schedulability tests with different tradeoffs between their time complexity\nand their accuracy. Under the arbitrary-deadline setting, the relative deadline\nof a task can be longer than the minimum inter-arrival time of the jobs of the\ntask. We show that global deadline-monotonic (DM) scheduling has a speedup\nbound of $3-1/M$ against any optimal scheduling algorithms, where $M$ is the\nnumber of identical processors, and prove that this bound is asymptotically\ntight.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 11:51:26 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Chen", "Jian-Jia", ""], ["von der Br\u00fcggen", "Georg", ""], ["Ueter", "Niklas", ""]]}, {"id": "1802.10530", "submitter": "Charitha Saumya Gusthinna Waduge", "authors": "Shreya Inamdar, Charitha Saumya, Nomchin Banga", "title": "Orion+: Automated Problem Diagnosis in Computing Systems by Mining\n  Metric Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the suspicious code at a finer granularity of call stack\nrather than code region, which was being returned by Orion. Call stack based\ncomparison returns call stacks that are most impacted by the bug and save\ndeveloper time to debug from scratch. This solution has polynomial complexity\nand hence can be implemented practically.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:44:50 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Inamdar", "Shreya", ""], ["Saumya", "Charitha", ""], ["Banga", "Nomchin", ""]]}]