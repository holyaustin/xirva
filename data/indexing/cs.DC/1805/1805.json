[{"id": "1805.00106", "submitter": "Nathaniel McVicar", "authors": "Nathaniel McVicar, Akina Hoshino, Anna La Torre, Thomas A. Reh, Walter\n  L. Ruzzo and Scott Hauck", "title": "FPGA Acceleration of Short Read Alignment", "comments": "15 pages, technical report version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aligning millions of short DNA or RNA reads, of 75 to 250 base pairs each, to\na reference genome is a significant computation problem in bioinformatics. We\npresent a flexible and fast FPGA-based short read alignment tool. Our aligner\nmakes use of the processing power of FPGAs in conjunction with the greater host\nmemory bandwidth and flexibility of software to improve performance and achieve\na high level of configurability. This flexible design supports a variety of\nreference genome sizes without the performance degradation suffered by other\nsoftware and FPGA-based aligners. It is also better able to support the\nfeatures of new alignment algorithms, which frequently crop up in the rapidly\nevolving field of bioinformatics. We demonstrate these advantages in a case\nstudy where we align RNA-Seq data from a hypothesized mouse / human xenograft.\nIn this case study, our aligner provides a speedup of 5.6x over BWA-SW with\nenergy savings of 21%, while also reducing incorrect short read classification\nby 29%. To demonstrate the flexibility of our system we show that the speedup\ncan be substantially improved while retaining most of the accuracy gains over\nBWA-SW. The speedup can be increased to 71.3x, while still enjoying a 28%\nincorrect classification improvement and 52% improvement in unaligned reads.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 21:26:41 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["McVicar", "Nathaniel", ""], ["Hoshino", "Akina", ""], ["La Torre", "Anna", ""], ["Reh", "Thomas A.", ""], ["Ruzzo", "Walter L.", ""], ["Hauck", "Scott", ""]]}, {"id": "1805.00154", "submitter": "Jongmin Lee", "authors": "Jongmin Lee, Cihan Tepedelenlioglu, Andreas Spanias", "title": "Consensus-based Distributed Quantile Estimation in Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantile is defined as a value below which random draws from a given\ndistribution falls with a given probability. In a centralized setting where the\ncumulative distribution function (CDF) is unknown, the empirical CDF (ECDF) can\nbe used to estimate such quantiles after aggregating the data. In a fully\ndistributed sensor network, however, it is challenging to estimate quantiles.\nThis is because each sensor node observes local measurement data with limited\nstorage and data transmission power which make it difficult to obtain the\nglobal ECDF. This paper proposes consensus-based quantile estimation for such a\ndistributed network. The states of the proposed algorithm are recursively\nupdated with two-steps at each iteration: one is a local update based on the\nmeasurement data and the current state, and the other is averaging the updated\nstates with neighboring nodes. We consider the realistic case of communication\nlinks between nodes being corrupted by independent random noise. It is shown\nthat the estimated state sequence is asymptotically unbiased and converges\ntoward the sample quantile in the mean-square sense. The two step-size\nsequences corresponding to the averaging and local update steps result in a\nmixed-time scale algorithm with proper decay rates in order to achieve\nconvergence. We also provide applications to distributed estimation of trimmed\nmean, computation of median, maximum, or minimum values and identification of\noutliers through simulation.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 02:22:24 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Lee", "Jongmin", ""], ["Tepedelenlioglu", "Cihan", ""], ["Spanias", "Andreas", ""]]}, {"id": "1805.00265", "submitter": "Sarod Yatawatta", "authors": "Sarod Yatawatta", "title": "Performance Analysis of Distributed Radio Interferometric Calibration", "comments": "Draft, to be published in the Proceedings of IEEE Sensor Array and\n  Multichannel Signal Processing Workshop (IEEE SAM 2018), published by IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed calibration based on consensus optimization is a computationally\nefficient method to calibrate large radio interferometers such as LOFAR and\nSKA. Calibrating along multiple directions in the sky and removing the bright\nforeground signal is a crucial step in many science cases in radio\ninterferometry. The residual data contain weak signals of huge scientific\ninterest and of particular concern is the effect of incomplete sky models used\nin calibration on the residual. In order to study this, we consider the mapping\nbetween the input uncalibrated data and the output residual data. We derive an\nanalytical relationship between the input and output probability density\nfunctions which can be used to study the performance of calibration.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 10:26:25 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Yatawatta", "Sarod", ""]]}, {"id": "1805.00280", "submitter": "Songjie Niu", "authors": "Dongyan Zhou, Songjie Niu, Shimin Chen", "title": "Efficient Graph Computation for Node2Vec", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node2Vec is a state-of-the-art general-purpose feature learning method for\nnetwork analysis. However, current solutions cannot run Node2Vec on large-scale\ngraphs with billions of vertices and edges, which are common in real-world\napplications. The existing distributed Node2Vec on Spark incurs significant\nspace and time overhead. It runs out of memory even for mid-sized graphs with\nmillions of vertices. Moreover, it considers at most 30 edges for every vertex\nin generating random walks, causing poor result quality. In this paper, we\npropose Fast-Node2Vec, a family of efficient Node2Vec random walk algorithms on\na Pregel-like graph computation framework. Fast-Node2Vec computes transition\nprobabilities during random walks to reduce memory space consumption and\ncomputation overhead for large-scale graphs. The Pregel-like scheme avoids\nspace and time overhead of Spark's read-only RDD structures and shuffle\noperations. Moreover, we propose a number of optimization techniques to further\nreduce the computation overhead for popular vertices with large degrees.\nEmpirical evaluation show that Fast-Node2Vec is capable of computing Node2Vec\non graphs with billions of vertices and edges on a mid-sized machine cluster.\nCompared to Spark-Node2Vec, Fast-Node2Vec achieves 7.7--122x speedups.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 12:01:10 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Zhou", "Dongyan", ""], ["Niu", "Songjie", ""], ["Chen", "Shimin", ""]]}, {"id": "1805.00290", "submitter": "Robert Kl\\\"ofkorn", "authors": "Andreas Dedner, Birane Kane, Robert Kl\\\"ofkorn, and Martin Nolte", "title": "Python Framework for HP Adaptive Discontinuous Galerkin Method for Two\n  Phase Flow in Porous Media", "comments": "Keywords: DG, hp-adaptivity, Two-phase flow, IMPES, Fully implicit,\n  Dune, Python, Porous media. 28 pages, 9 figures, various code snippets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a framework for solving two phase flow problems in\nporous media. The discretization is based on a Discontinuous Galerkin method\nand includes local grid adaptivity and local choice of polynomial degree. The\nmethod is implemented using the new Python frontend Dune-FemPy to the open\nsource framework Dune. The code used for the simulations is made available as\nJupyter notebook and can be used through a Docker container. We present a\nnumber of time stepping approaches ranging from a classical IMPES method to\nfully coupled implicit scheme. The implementation of the discretization is very\nflexible allowing for test different formulations of the two phase flow model\nand adaptation strategies.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 12:30:23 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Dedner", "Andreas", ""], ["Kane", "Birane", ""], ["Kl\u00f6fkorn", "Robert", ""], ["Nolte", "Martin", ""]]}, {"id": "1805.00556", "submitter": "Stefano Markidis Prof.", "authors": "Sai Narasimhamurthy, Nikita Danilov, Sining Wu, Ganesan Umanesan,\n  Stefano Markidis, Sergio Rivas-Gomez, Ivy Bo Peng, Erwin Laure, Dirk Pleiter,\n  Shaun de Witt", "title": "SAGE: Percipient Storage for Exascale Data Centric Computing", "comments": null, "journal-ref": "Parallel Computing, 23 March 2018", "doi": "10.1016/j.parco.2018.03.002", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to implement a Big Data/Extreme Computing (BDEC) capable system\ninfrastructure as we head towards the era of Exascale computing - termed SAGE\n(Percipient StorAGe for Exascale Data Centric Computing). The SAGE system will\nbe capable of storing and processing immense volumes of data at the Exascale\nregime, and provide the capability for Exascale class applications to use such\na storage infrastructure. SAGE addresses the increasing overlaps between Big\nData Analysis and HPC in an era of next-generation data centric computing that\nhas developed due to the proliferation of massive data sources, such as large,\ndispersed scientific instruments and sensors, whose data needs to be processed,\nanalyzed and integrated into simulations to derive scientific and innovative\ninsights. Indeed, Exascale I/O, as a problem that has not been sufficiently\ndealt with for simulation codes, is appropriately addressed by the SAGE\nplatform. The objective of this paper is to discuss the software architecture\nof the SAGE system and look at early results we have obtained employing some of\nits key methodologies, as the system continues to evolve.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 21:20:31 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Narasimhamurthy", "Sai", ""], ["Danilov", "Nikita", ""], ["Wu", "Sining", ""], ["Umanesan", "Ganesan", ""], ["Markidis", "Stefano", ""], ["Rivas-Gomez", "Sergio", ""], ["Peng", "Ivy Bo", ""], ["Laure", "Erwin", ""], ["Pleiter", "Dirk", ""], ["de Witt", "Shaun", ""]]}, {"id": "1805.00569", "submitter": "Yang You", "authors": "Yang You, James Demmel, Cho-Jui Hsieh, Richard Vuduc", "title": "Accurate, Fast and Scalable Kernel Ridge Regression on Parallel and\n  Distributed Systems", "comments": "This paper has been accepted by ACM International Conference on\n  Supercomputing (ICS) 2018", "journal-ref": null, "doi": "10.1145/3205289.3205290", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new methods to address the weak scaling problems of KRR: the\nBalanced KRR (BKRR) and K-means KRR (KKRR). These methods consider alternative\nways to partition the input dataset into p different parts, generating p\ndifferent models, and then selecting the best model among them. Compared to a\nconventional implementation, KKRR2 (optimized version of KKRR) improves the\nweak scaling efficiency from 0.32% to 38% and achieves a 591times speedup for\ngetting the same accuracy by using the same data and the same hardware (1536\nprocessors). BKRR2 (optimized version of BKRR) achieves a higher accuracy than\nthe current fastest method using less training time for a variety of datasets.\nFor the applications requiring only approximate solutions, BKRR2 improves the\nweak scaling efficiency to 92% and achieves 3505 times speedup (theoretical\nspeedup: 4096 times).\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 22:14:27 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["You", "Yang", ""], ["Demmel", "James", ""], ["Hsieh", "Cho-Jui", ""], ["Vuduc", "Richard", ""]]}, {"id": "1805.00626", "submitter": "Ellis Solaiman", "authors": "Carlos Molina-Jimenez, Ellis Solaiman, Ioannis Sfyrakis, Irene Ng, Jon\n  Crowcroft", "title": "On and Off-Blockchain Enforcement Of Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss how conventional business contracts can be converted\ninto smart contracts---their electronic equivalents that can be used to\nsystematically monitor and enforce contractual rights, obligations and\nprohibitions at run time. We explain that emerging blockchain technology is\ncertainly a promising platform for implementing smart contracts but argue that\nthere is a large class of applications, where blockchain is inadequate due to\nperformance, scalability, and consistency requirements, and also due to\nlanguage expressiveness and cost issues that are hard to solve. We explain that\nin some situations a centralised approach that does not rely on blockchain is a\nbetter alternative due to its simplicity, scalability, and performance. We\nsuggest that in applications where decentralisation and transparency are\nessential, developers can advantageously combine the two approaches into hybrid\nsolutions where some operations are enforced by enforcers deployed\non--blockchains and the rest by enforcers deployed on trusted third parties.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 05:05:52 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Molina-Jimenez", "Carlos", ""], ["Solaiman", "Ellis", ""], ["Sfyrakis", "Ioannis", ""], ["Ng", "Irene", ""], ["Crowcroft", "Jon", ""]]}, {"id": "1805.00658", "submitter": "Ivano Notarnicola", "authors": "Ivano Notarnicola, Ying Sun, Gesualdo Scutari, Giuseppe Notarstefano", "title": "Distributed Big-Data Optimization via Block-Iterative Convexification\n  and Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study distributed big-data nonconvex optimization in\nmulti-agent networks. We consider the (constrained) minimization of the sum of\na smooth (possibly) nonconvex function, i.e., the agents' sum-utility, plus a\nconvex (possibly) nonsmooth regularizer. Our interest is in big-data problems\nwherein there is a large number of variables to optimize. If treated by means\nof standard distributed optimization algorithms, these large-scale problems may\nbe intractable, due to the prohibitive local computation and communication\nburden at each node. We propose a novel distributed solution method whereby at\neach iteration agents optimize and then communicate (in an uncoordinated\nfashion) only a subset of their decision variables. To deal with non-convexity\nof the cost function, the novel scheme hinges on Successive Convex\nApproximation (SCA) techniques coupled with i) a tracking mechanism\ninstrumental to locally estimate gradient averages; and ii) a novel block-wise\nconsensus-based protocol to perform local block-averaging operations and\ngradient tacking. Asymptotic convergence to stationary solutions of the\nnonconvex problem is established. Finally, numerical results show the\neffectiveness of the proposed algorithm and highlight how the block dimension\nimpacts on the communication overhead and practical convergence speed.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 07:56:37 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Notarnicola", "Ivano", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1805.00680", "submitter": "Evangelos Psomakelis Mr", "authors": "Evangelos Psomakelis, Konstantinos Tserpes, Dimosthenis\n  Anagnostopoulos and Theodora Varvarigou", "title": "BUDAMAF: Data Management in Cloud Federations", "comments": "11 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data management has always been a multi-domain problem even in the simplest\ncases. It involves, quality of service, security, resource management, cost\nmanagement, incident identification, disaster avoidance and/or recovery, as\nwell as many other concerns. In our case, this situation gets ever more\ncomplicated because of the divergent nature of a cloud federation like BASMATI.\nIn this federation, the BASMATI Unified Data Management Framework (BUDaMaF),\ntries to create an automated uniform way of managing all the data transactions,\nas well as the data stores themselves, in a polyglot multi-cloud, consisting of\na plethora of different machines and data store systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 08:53:35 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Psomakelis", "Evangelos", ""], ["Tserpes", "Konstantinos", ""], ["Anagnostopoulos", "Dimosthenis", ""], ["Varvarigou", "Theodora", ""]]}, {"id": "1805.00745", "submitter": "Yuan Xu", "authors": "Yuan Xu, Zhiyuan Yan, Sa Wang, Cheng Yang, Qingsai Xiao and Yungang\n  Bao", "title": "Avalon: Building an Operating System for Robotcenter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper envisions a scenario that hundreds of heterogeneous robots form a\nrobotcenter which can be shared by multiple users and used like a single\npowerful robot to perform complex tasks. However, current multi-robot systems\nare either unable to manage heterogeneous robots or unable to support multiple\nconcurrent users. Inspired by the design of modern datacenter OSes, we propose\nAvalon, a robot operating system with two-level scheduling scheme which is\nwidely adopted in datacenters for Internet services and cloud computing.\nSpecifically, Avalon integrates three important features together: (1) Instead\nof allocating a whole robot, Avalon classifies fine-grained robot resources\ninto three categories to distinguish which fine-grained resources can be shared\nby multi-robot frameworks simultaneously. (2) Avalon adopts a location based\nresource allocation policy to substantially reduce scheduling overhead. (3)\nAvalon enables robots to offload computation intensive tasks to the clouds.We\nhave implemented and evaluated Avalon on robots on both simulated environments\nand real world.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 11:47:19 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Xu", "Yuan", ""], ["Yan", "Zhiyuan", ""], ["Wang", "Sa", ""], ["Yang", "Cheng", ""], ["Xiao", "Qingsai", ""], ["Bao", "Yungang", ""]]}, {"id": "1805.00774", "submitter": "Alexander Setzer", "authors": "Peter Robinson, Christian Scheideler, Alexander Setzer", "title": "Breaking the $\\tilde\\Omega(\\sqrt{n})$ Barrier: Fast Consensus under a\n  Late Adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the consensus problem in a synchronous distributed system of $n$\nnodes under an adaptive adversary that has a slightly outdated view of the\nsystem and can block all incoming and outgoing communication of a constant\nfraction of the nodes in each round. Motivated by a result of Ben-Or and\nBar-Joseph (1998), showing that any consensus algorithm that is resilient\nagainst a linear number of crash faults requires $\\tilde \\Omega(\\sqrt{n})$\nrounds in an $n$-node network against an adaptive adversary, we consider a late\nadaptive adversary, who has full knowledge of the network state at the\nbeginning of the previous round and unlimited computational power, but is\noblivious to the current state of the nodes.\n  Our main contributions are randomized distributed algorithms that achieve\nalmost-everywhere consensus w.h.p. against a late adaptive adversary who can\nblock up to $\\epsilon n$ nodes in each round, for a small constant $\\epsilon\n>0$. Our first protocol achieves binary and plurality consensus, and the second\none achieves multi-value consensus. Both of our algorithms succeed in $O(\\log\nn)$ rounds with high probability, thus showing an exponential gap to the\naforementioned lower bound for strongly adaptive crash-failure adversaries,\nwhich can be strengthened to $\\Omega(n)$ when allowing the adversary to block\nnodes instead of permanently crashing them. In our algorithms each node\ncontacts only an (amortized) constant number of peers in each communication\nround. We show that our algorithms are optimal up to constant (resp.\nsub-logarithmic) factors by proving that every almost-everywhere consensus\nprotocol takes $\\Omega(\\log_d n)$ rounds in the worst case, where $d$ is an\nupper bound on the number of communication requests initiated per node in each\nround. We complement our theoretical results with an experimental evaluation of\nthe first protocol revealing a short convergence time.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 12:54:40 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Robinson", "Peter", ""], ["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""]]}, {"id": "1805.00857", "submitter": "Mohammed Khatiri", "authors": "Nicolas Gast, Mohammed Khatiri, Denis Trystram and Frederic Wagner", "title": "A new analysis of Work Stealing with latency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the impact of communication latency on the classical\nWork Stealing load balancing algorithm. Our paper extends the reference model\nin which we introduce a latency parameter. By using a theoretical analysis and\nsimulation, we study the overall impact of this latency on the Makespan\n(maximum completion time). We derive a new expression of the expected running\ntime of a bag of tasks scheduled by Work Stealing. This expression enables us\nto predict under which conditions a given run will yield acceptable\nperformance. For instance, we can easily calibrate the maximal number of\nprocessors to use for a given work/platform combination. All our results are\nvalidated through simulation on a wide range of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 15:04:51 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Gast", "Nicolas", ""], ["Khatiri", "Mohammed", ""], ["Trystram", "Denis", ""], ["Wagner", "Frederic", ""]]}, {"id": "1805.00860", "submitter": "Doriane Perard", "authors": "Doriane Perard, J\\'er\\^ome Lacan, Yann Bachy and Jonathan Detchart", "title": "Erasure code-based low storage blockchain node", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of a decentralized ledger usually implies that each node of a\nblockchain network stores the entire blockchain. However, in the case of\npopular blockchains, which each weigh several hundreds of GB, the large amount\nof data to be stored can incite new or low-capacity nodes to run lightweight\nclients. Such nodes do not participate to the global storage effort and can\nresult in a centralization of the blockchain by very few nodes, which is\ncontrary to the basic concepts of a blockchain.\n  To avoid this problem, we propose new low storage nodes that store a reduced\namount of data generated from the blockchain by using erasure codes. The\nproperties of this technique ensure that any block of the chain can be easily\nrebuild from a small number of such nodes. This system should encourage low\nstorage nodes to contribute to the storage of the blockchain and to maintain\ndecentralization despite of a globally increasing size of the blockchain. This\nsystem paves the way to new types of blockchains which would only be managed by\nlow capacity nodes.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 15:13:32 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Perard", "Doriane", ""], ["Lacan", "J\u00e9r\u00f4me", ""], ["Bachy", "Yann", ""], ["Detchart", "Jonathan", ""]]}, {"id": "1805.00967", "submitter": "Line Pouchard", "authors": "Line Pouchard, Sterling Baldwin, Todd Elsethagen, Carlos Gamboa,\n  Shantenu Jha, Bibi Raju, Eric Stephan, Li Tang, Kerstin Kleese Van Dam", "title": "Use Cases of Computational Reproducibility for Scientific Workflows at\n  Exascale", "comments": "Presented at SC17, Denver, CO. Full version submitted to IJHPCA March\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an approach for improved reproducibility that includes capturing\nand relating provenance characteristics and performance metrics, in a hybrid\nqueriable system, the ProvEn server. The system capabilities are illustrated on\ntwo use cases: scientific reproducibility of results in the ACME climate\nsimulations and performance reproducibility in molecular dynamics workflows on\nHPC computing platforms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 16:03:13 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Pouchard", "Line", ""], ["Baldwin", "Sterling", ""], ["Elsethagen", "Todd", ""], ["Gamboa", "Carlos", ""], ["Jha", "Shantenu", ""], ["Raju", "Bibi", ""], ["Stephan", "Eric", ""], ["Tang", "Li", ""], ["Van Dam", "Kerstin Kleese", ""]]}, {"id": "1805.00998", "submitter": "Kyriakos Georgiou", "authors": "Vitor R. G. Silva, Alex Furtunato, Kyriakos Georgiou, Kerstin Eder,\n  Samuel Xavier-de-Souza", "title": "Energy-Optimal Configurations for Single-Node HPC Applications", "comments": "21 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": "LAPPS2018_003", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency is a growing concern for modern computing, especially for\nHPC due to operational costs and the environmental impact. We propose a\nmethodology to find energy-optimal frequency and number of active cores to run\nsingle-node HPC applications using an application-agnostic power model of the\narchitecture and an architecture-aware performance model of the application. We\ncharacterize the application performance using Support Vector Regression. The\npower consumption is estimated by modeling CMOS dynamic and static power\nwithout knowledge of the application. The energy-optimal configuration is\nestimated by minimizing the product of the power model and the performance\nmodel's outcomes. Results for four PARSEC applications with five different\ninputs show that the proposed approach used about 14X less energy when compared\nto the worst case of the default Linux DVFS governor. For the best case of the\nDVFS scheme, 23% savings were observed, with an overall average of 6% less\nenergy.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 19:54:54 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Silva", "Vitor R. G.", ""], ["Furtunato", "Alex", ""], ["Georgiou", "Kyriakos", ""], ["Eder", "Kerstin", ""], ["Xavier-de-Souza", "Samuel", ""]]}, {"id": "1805.01081", "submitter": "Qi Zhang", "authors": "Qi Zhang, Petr Novotny, Salman Baset, Donna Dillenberger, Artem\n  Barger, Yacov Manevich", "title": "LedgerGuard: Improving Blockchain Ledger Dependability", "comments": "8 pages. Appears in 2018 International Conference on Blockchain\n  (ICBC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of crypto-currencies has spawned great interest in their underlying\ntechnology, namely, Blockchain. The central component in a Blockchain is a\nshared distributed ledger. A ledger comprises series of blocks, which in turns\ncontains a series of transactions. An identical copy of the ledger is stored on\nall nodes in a blockchain network. Maintaining ledger integrity and security is\none of the crucial design aspects of any blockchain platform. Thus, there are\ntypically built-in validation mechanisms leveraging cryptography to ensure the\nvalidity of incoming blocks before committing them into the ledger. However, a\nblockchain node may run over an extended period of time, during which the\nblocks on the disk can may become corrupted due to software or hardware\nfailures, or due to malicious activity. This paper proposes LedgerGuard, a tool\nto maintain ledger integrity by detecting corrupted blocks and recovering these\nblocks by synchronizing with rest of the network. The experimental\nimplementation of LedgerGuard is based on Hyperledger Fabric, which is a\npopular open source permissioned blockchain platform.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 01:55:05 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Zhang", "Qi", ""], ["Novotny", "Petr", ""], ["Baset", "Salman", ""], ["Dillenberger", "Donna", ""], ["Barger", "Artem", ""], ["Manevich", "Yacov", ""]]}, {"id": "1805.01177", "submitter": "Venkatesh-Prasad Ranganath", "authors": "Venkatesh-Prasad Ranganath and Daniel Andresen", "title": "Why do Users Kill HPC Jobs?", "comments": "Minor formatting and content update based on reader feedback", "journal-ref": null, "doi": "10.1109/HiPC.2018.00039", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the cost of HPC clusters, making best use of them is crucial to improve\ninfrastructure ROI. Likewise, reducing failed HPC jobs and related waste in\nterms of user wait times is crucial to improve HPC user productivity (aka human\nROI). While most efforts (e.g.,debugging HPC programs) explore technical\naspects to improve ROI of HPC clusters, we hypothesize non-technical (human)\naspects are worth exploring to make non-trivial ROI gains; specifically,\nunderstanding non-technical aspects and how they contribute to the failure of\nHPC jobs.\n  In this regard, we conducted a case study in the context of Beocat cluster at\nKansas State University. The purpose of the study was to learn the reasons why\nusers terminate jobs and to quantify wasted computations in such jobs in terms\nof system utilization and user wait time. The data from the case study helped\nidentify interesting and actionable reasons why users terminate HPC jobs. It\nalso helped confirm that user terminated jobs may be associated with\nnon-trivial amount of wasted computation, which if reduced can help improve the\nROI of HPC clusters.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 09:16:13 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 00:02:58 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Ranganath", "Venkatesh-Prasad", ""], ["Andresen", "Daniel", ""]]}, {"id": "1805.01208", "submitter": "Moritz von Looz-Corswarem", "authors": "Moritz von Looz, Charilaos Tzovas, Henning Meyerhenke", "title": "Balanced k-means for Parallel Geometric Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh partitioning is an indispensable tool for efficient parallel numerical\nsimulations. Its goal is to minimize communication between the processes of a\nsimulation while achieving load balance. Established graph-based partitioning\ntools yield a high solution quality; however, their scalability is limited.\nGeometric approaches usually scale better, but their solution quality may be\nunsatisfactory for `non-trivial' mesh topologies.\n  In this paper, we present a scalable version of $k$-means that is adapted to\nyield balanced clusters. Balanced $k$-means constitutes the core of our new\npartitioning algorithm Geographer. Bootstrapping of initial centers is\nperformed with space-filling curves, leading to fast convergence of the\nsubsequent balanced k-means algorithm.\n  Our experiments with up to 16384 MPI processes on numerous benchmark meshes\nshow the following: (i) Geographer produces partitions with a lower\ncommunication volume than state-of-the-art geometric partitioners from the\nZoltan package; (ii) Geographer scales well on large inputs; (iii) a Delaunay\nmesh with a few billion vertices and edges can be partitioned in a few seconds.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 10:23:45 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["von Looz", "Moritz", ""], ["Tzovas", "Charilaos", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1805.01279", "submitter": "Marcelle Von-Wendland", "authors": "Marcelle von Wendland", "title": "Smart contracts that are smart and can function as legal contracts - A\n  Review of Semantic Blockchain and Distributed Ledger Technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain and Distributed ledger Technologies are increasingly becoming key\nenablers for vital innovation in financial services, manufacturing, government\nand other industries. One of the biggest challenges though is the level of\nsupport for semantics by most of the Block Chain and Distributed Ledger\ntechnologies. This paper reviews and categorises common block chain and DLT\napproaches and introduces a new approach to Blockchain / DLT promising to\nresolve the semantic problems inherent in other Blockchain / DLT approaches\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 21:03:56 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["von Wendland", "Marcelle", ""]]}, {"id": "1805.01319", "submitter": "Simon Collet", "authors": "Simon Collet (IRIF, CNRS, UPD7), Amos Korman (IRIF, CNRS, UPD7)", "title": "Intense Competition can Drive Selfish Explorers to Optimize Coverage", "comments": null, "journal-ref": "SPAA, Jul 2018, Vienna, Austria. 2018", "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a game-theoretic setting in which selfish individuals compete\nover resources of varying quality. The motivating example is a group of animals\nthat disperse over patches of food of different abundances. In such scenarios,\nindividuals are biased towards selecting the higher quality patches, while, at\nthe same time, aiming to avoid costly collisions or overlaps. Our goal is to\ninvestigate the impact of collision costs on the parallel coverage of resources\nby the whole group. Consider M sites, where a site x has value f(x). We think\nof f(x) as the reward associated with site x, and assume that if a single\nindividual visits x exclusively, it receives this exact reward. Typically, we\nassume that if > 1 individuals visit x then each receives at most f(x). In\nparticular, when competition costs are high, each individual might receive an\namount strictly less than f(x), which could even be negative. Conversely,\nmodeling cooperation at a site, we also consider cases where each one gets more\nthan f(x). There are k identical players that compete over the rewards. They\nindependently act in parallel, in a one-shot scenario, each specifying a single\nsite to visit, without knowing which sites are explored by others. The group\nperformance is evaluated by the expected coverage, defined as the sum of f(x)\nover all sites that are explored by at least one player. Since we assume that\nplayers cannot coordinate before choosing their site we focus on symmetric\nstrategies. The main takeaway message of this paper is that the optimal\nsymmetric coverage is expected to emerge when collision costs are relatively\nhigh, so that the following \"Judgment of Solomon\" type of rule holds: If a\nsingle player explores a site x then it gains its full reward f(x), but if\nseveral players explore it, then neither one receives any reward. Under this\npolicy, it turns out that there exists a unique symmetric Nash Equilibrium\nstrategy, which is, in fact, evolutionary stable. Moreover, this strategy\nyields the best possible coverage among all symmetric strategies. Viewing the\ncoverage measure as the social welfare, this policy thus enjoys a (Symmetric)\nPrice of Anarchy of precisely 1, whereas, in fact, any other congestion policy\nhas a price strictly greater than 1. Our model falls within the scope of\nmechanism design, and more precisely in the area of incentivizing exploration.\nIt finds relevance in evolutionary ecology, and further connects to studies on\nBayesian parallel search algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 14:18:11 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Collet", "Simon", "", "IRIF, CNRS, UPD7"], ["Korman", "Amos", "", "IRIF, CNRS, UPD7"]]}, {"id": "1805.01406", "submitter": "Emilio Cruciani", "authors": "Emilio Cruciani, Emanuele Natale, Giacomo Scornavacca", "title": "Distributed Community Detection via Metastability of the 2-Choices\n  Dynamics", "comments": "Full version of paper appeared in AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the behavior of a simple majority dynamics on networks of\nagents whose interaction topology exhibits a community structure. By leveraging\nrecent advancements in the analysis of dynamics, we prove that, when the states\nof the nodes are randomly initialized, the system rapidly and stably converges\nto a configuration in which the communities maintain internal consensus on\ndifferent states. This is the first analytical result on the behavior of\ndynamics for non-consensus problems on non-complete topologies, based on the\nfirst symmetry-breaking analysis in such setting. Our result has several\nimplications in different contexts in which dynamics are adopted for\ncomputational and biological modeling purposes. In the context of Label\nPropagation Algorithms, a class of widely used heuristics for community\ndetection, it represents the first theoretical result on the behavior of a\ndistributed label propagation algorithm with quasi-linear message complexity.\nIn the context of evolutionary biology, dynamics such as the Moran process have\nbeen used to model the spread of mutations in genetic populations [Lieberman,\nHauert, and Nowak 2005]; our result shows that, when the probability of\nadoption of a given mutation by a node of the evolutionary graph depends\nsuper-linearly on the frequency of the mutation in the neighborhood of the node\nand the underlying evolutionary graph exhibits a community structure, there is\na non-negligible probability for species differentiation to occur.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 16:21:57 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 12:18:38 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Cruciani", "Emilio", ""], ["Natale", "Emanuele", ""], ["Scornavacca", "Giacomo", ""]]}, {"id": "1805.01457", "submitter": "Hendrik Chang", "authors": "Eric Zhang, Hendrik C, Yang Liu, Archit Sharma, Jasper L", "title": "TrueChain: Highly Performant Decentralized Public Ledger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the initial design of Minerva consensus protocol for\nTruechain and other technical details. Currently, it is widely believed in the\nblockchain community that a public chain cannot simultaneously achieve high\nperformance, decentralization and security. This is true in the case of a\nNakamoto chain (low performance) or a delegated proof of stake chain (partially\ncentralized), which are the most popular block chain solutions at time of\nwriting. Our consensus design enjoys the same consistency, liveness,\ntransaction finality and security guarantee, a de-facto with the Hybrid\nConsensus. We go on to propose the idea of a new virtual machine on top of\nEthereum which adds permissioned-chain based transaction processing\ncapabilities in a permissionless setting. We also use the idea of data sharding\nand speculative transactions, and evaluation of smart contracts in a sharding\nfriendly virtual machine. Finally, we will briefly discuss our fundamentally\nASIC resistant mining algorithm, Truehash.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 17:59:25 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 19:21:20 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 15:55:21 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhang", "Eric", ""], ["C", "Hendrik", ""], ["Liu", "Yang", ""], ["Sharma", "Archit", ""], ["L", "Jasper", ""]]}, {"id": "1805.01548", "submitter": "Rafael Pereira Pires", "authors": "Rafael Pires, David Goltzsche, Sonia Ben Mokhtar, Sara Bouchenak,\n  Antoine Boutet, Pascal Felber, R\\\"udiger Kapitza, Marcelo Pasin and Valerio\n  Schiavoni", "title": "CYCLOSA: Decentralizing Private Web Search Through SGX-Based Browser\n  Extensions", "comments": null, "journal-ref": "38th IEEE International Conference on Distributed Computing\n  Systems (ICDCS 2018)", "doi": "10.1109/ICDCS.2018.00053", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By regularly querying Web search engines, users (unconsciously) disclose\nlarge amounts of their personal data as part of their search queries, among\nwhich some might reveal sensitive information (e.g. health issues, sexual,\npolitical or religious preferences). Several solutions exist to allow users\nquerying search engines while improving privacy protection. However, these\nsolutions suffer from a number of limitations: some are subject to user\nre-identification attacks, while others lack scalability or are unable to\nprovide accurate results. This paper presents CYCLOSA, a secure, scalable and\naccurate private Web search solution. CYCLOSA improves security by relying on\ntrusted execution environments (TEEs) as provided by Intel SGX. Further,\nCYCLOSA proposes a novel adaptive privacy protection solution that reduces the\nrisk of user re- identification. CYCLOSA sends fake queries to the search\nengine and dynamically adapts their count according to the sensitivity of the\nuser query. In addition, CYCLOSA meets scalability as it is fully\ndecentralized, spreading the load for distributing fake queries among other\nnodes. Finally, CYCLOSA achieves accuracy of Web search as it handles the real\nquery and the fake queries separately, in contrast to other existing solutions\nthat mix fake and real query results.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 21:34:07 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 09:07:54 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Pires", "Rafael", ""], ["Goltzsche", "David", ""], ["Mokhtar", "Sonia Ben", ""], ["Bouchenak", "Sara", ""], ["Boutet", "Antoine", ""], ["Felber", "Pascal", ""], ["Kapitza", "R\u00fcdiger", ""], ["Pasin", "Marcelo", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1805.01563", "submitter": "Rafael Pereira Pires", "authors": "Stefan Contiu, Rafael Pires, S\\'ebastien Vaucher, Marcelo Pasin,\n  Pascal Felber and Laurent R\\'eveill\\`ere", "title": "IBBE-SGX: Cryptographic Group Access Control using Trusted Execution\n  Environments", "comments": null, "journal-ref": "48th IEEE/IFIP International Conference on Dependable Systems and\n  Networks (DSN 2018)", "doi": "10.1109/DSN.2018.00032", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many cloud storage systems allow users to protect their data by making\nuse of encryption, only few support collaborative editing on that data. A major\nchallenge for enabling such collaboration is the need to enforce cryptographic\naccess control policies in a secure and efficient manner. In this paper, we\nintroduce IBBE-SGX, a new cryptographic access control extension that is\nefficient both in terms of computation and storage even when processing large\nand dynamic workloads of membership operations, while at the same time offering\nzero knowledge guarantees. IBBE-SGX builds upon Identity-Based Broadcasting\nEncryption (IBBE). We address IBBE's impracticality for cloud deployments by\nexploiting Intel Software Guard Extensions (SGX) to derive cuts in the\ncomputational complexity. Moreover, we propose a group partitioning mechanism\nsuch that the computational cost of membership update is bound to a fixed\nconstant partition size rather than the size of the whole group. We have\nimplemented and evaluated our new access control extension. Results highlight\nthat IBBE-SGX performs membership changes 1.2 orders of magnitude faster than\nthe traditional approach of Hybrid Encryption (HE), producing group metadata\nthat are 6 orders of magnitude smaller than HE, while at the same time offering\nzero knowledge guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 22:41:30 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 09:15:56 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Contiu", "Stefan", ""], ["Pires", "Rafael", ""], ["Vaucher", "S\u00e9bastien", ""], ["Pasin", "Marcelo", ""], ["Felber", "Pascal", ""], ["R\u00e9veill\u00e8re", "Laurent", ""]]}, {"id": "1805.01657", "submitter": "Adam Shimi", "authors": "Adam Shimi, Aur\\'elie Hurault and Philippe Qu\\'einnec", "title": "Characterizing Asynchronous Message-Passing Models Through Rounds", "comments": "20 pages; Full paper for the version accepted to OPODIS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Message-passing models of distributed computing vary along numerous\ndimensions: degree of synchrony, kind of faults, number of faults...\nUnfortunately, the sheer number of models and their subtle distinctions hinder\nour ability to design a general theory of message-passing models. One way out\nof this conundrum restricts communication to proceed by round. A great variety\nof message-passing models can then be captured in the Heard-Of model, through\npredicates on the messages sent in a round and received during or before this\nround. Then, the issue is to find the most accurate Heard-Of predicate to\ncapture a given model. This is straightforward in synchronous models, because\nwaiting for the upper bound on communication delay ensures that all available\nmessages are received, while not waiting forever. On the other hand, asynchrony\nallows unbounded message delays. Is there nonetheless a meaningful\ncharacterization of asynchronous models by a Heard-Of predicate?\n  We formalize this characterization by introducing Delivered collections: the\ncollections of all messages delivered at each round, whether late or not.\nPredicates on Delivered collections capture message-passing models. The\nquestion is to determine which Heard-Of predicates can be generated by a given\nDelivered predicate. We answer this by formalizing strategies for when to\nchange round. Thanks to a partial order on these strategies, we also find the\n\"best\" strategy for multiple models, where \"best\" intuitively means it waits\nfor as many messages as possible while not waiting forever. Finally, a strategy\nfor changing round that never blocks a process forever implements a Heard-Of\npredicate. This allows us to translate the order on strategies into an order on\nHeard-Of predicates. The characterizing predicate for a model is then the\ngreatest element for that order, if it exists.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 08:29:41 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 15:48:10 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 15:27:49 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Shimi", "Adam", ""], ["Hurault", "Aur\u00e9lie", ""], ["Qu\u00e9innec", "Philippe", ""]]}, {"id": "1805.01681", "submitter": "Ian Hayes", "authors": "Ian J. Hayes and Larissa A. Meinicke", "title": "Encoding fairness in a synchronous concurrent program algebra: extended\n  version with proofs", "comments": "Formal Methods 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent program refinement algebra provides a suitable basis for\nsupporting mechanised reasoning about shared-memory concurrent programs in a\ncompositional manner, for example, it supports the rely/guarantee approach of\nJones. The algebra makes use of a synchronous parallel operator motivated by\nAczel's trace model of concurrency and with similarities to Milner's SCCS. This\npaper looks at defining a form of fairness within the program algebra. The\nencoding allows one to reason about the fair execution of a single process in\nisolation as well as define fair-parallel in terms of a base parallel operator,\nof which no fairness properties are assumed. An algebraic theory to support\nfairness and fair-parallel is developed.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 09:35:41 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Hayes", "Ian J.", ""], ["Meinicke", "Larissa A.", ""]]}, {"id": "1805.01742", "submitter": "Rafael Pereira Pires", "authors": "Sonia Ben Mokhtar, Antoine Boutet, Pascal Felber, Marcelo Pasin,\n  Rafael Pires and Valerio Schiavoni", "title": "X-Search: Revisiting Private Web Search using Intel SGX", "comments": "Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference. Las\n  Vegas, NV, USA, December 11-15, 2017, 11 pages", "journal-ref": "2017 In Proceedings of Middleware '17", "doi": "10.1145/3135974.3135987", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploitation of user search queries by search engines is at the heart of\ntheir economic model. As consequence, offering private Web search\nfunctionalities is essential to the users who care about their privacy.\nNowadays, there exists no satisfactory approach to enable users to access\nsearch engines in a privacy-preserving way. Existing solutions are either too\ncostly due to the heavy use of cryptographic mechanisms (e.g., private\ninformation retrieval protocols), subject to attacks (e.g., Tor, TrackMeNot,\nGooPIR) or rely on weak adversarial models (e.g., PEAS). This paper introduces\nX-Search , a novel private Web search mechanism building on the disruptive\nSoftware Guard Extensions (SGX) proposed by Intel. We compare X-Search to its\nclosest competitors, Tor and PEAS, using a dataset of real web search queries.\nOur evaluation shows that: (1) X-Search offers stronger privacy guarantees than\nits competitors as it operates under a stronger adversarial model; (2) it\nbetter resists state-of-the-art re-identification attacks; and (3) from the\nperformance perspective, X-Search outperforms its competitors both in terms of\nlatency and throughput by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 12:47:49 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Mokhtar", "Sonia Ben", ""], ["Boutet", "Antoine", ""], ["Felber", "Pascal", ""], ["Pasin", "Marcelo", ""], ["Pires", "Rafael", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1805.01752", "submitter": "Rafael Pereira Pires", "authors": "Aur\\'elien Havet, Rafael Pires, Pascal Felber, Marcelo Pasin, Romain\n  Rouvoy and Valerio Schiavoni", "title": "SecureStreams: A Reactive Middleware Framework for Secure Data Stream\n  Processing", "comments": "Barcelona, Spain, June 19-23, 2017, 10 pages", "journal-ref": "In Proceedings of DEBS 2017", "doi": "10.1145/3093742.3093927", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing adoption of distributed data processing frameworks in a wide\ndiversity of application domains challenges end-to-end integration of\nproperties like security, in particular when considering deployments in the\ncontext of large-scale clusters or multi-tenant Cloud infrastructures. This\npaper therefore introduces SecureStreams, a reactive middleware framework to\ndeploy and process secure streams at scale. Its design combines the high-level\nreactive dataflow programming paradigm with Intel's low-level software guard\nextensions (SGX) in order to guarantee privacy and integrity of the processed\ndata. The experimental results of SecureStreams are promising: while offering a\nfluent scripting language based on Lua, our middleware delivers high processing\nthroughput, thus enabling developers to implement secure processing pipelines\nin just few lines of code.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 13:08:39 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Havet", "Aur\u00e9lien", ""], ["Pires", "Rafael", ""], ["Felber", "Pascal", ""], ["Pasin", "Marcelo", ""], ["Rouvoy", "Romain", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1805.01765", "submitter": "Sukhpal  Singh Gill", "authors": "Rajkumar Buyya and Sukhpal Singh Gill", "title": "Sustainable Cloud Computing: Foundations and Future Directions", "comments": "7 Pages, 4 Figures. arXiv admin note: substantial text overlap with\n  arXiv:1712.02899", "journal-ref": "Business Technology & Digital Transformation Strategies, Cutter\n  Consortium, EXECUTIVE UPDATE, Vol. 21, no. 6, Pages 1-9, 2018", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Major cloud providers such as Microsoft, Google, Facebook and Amazon rely\nheavily on datacenters to support the ever-increasing demand for their\ncomputational and application services. However, the financial and carbon\nfootprint related costs of running such large infrastructure negatively impacts\nthe sustainability of cloud services. Most of existing efforts primarily focus\non minimizing the energy consumption of servers. In this paper, we devise a\nconceptual model and practical design guidelines for holistic management of all\nresources (including servers, networks, storage, cooling systems) to improve\nthe energy efficiency and reduce carbon footprints in Cloud Data Centers\n(CDCs). Furthermore, we discuss the intertwined relationship between energy and\nreliability for sustainable cloud computing, where we highlight the associated\nresearch issues. Finally, we propose a set of future research directions in the\nfield and setup grounds for further practical developments.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 00:36:08 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Gill", "Sukhpal Singh", ""]]}, {"id": "1805.01768", "submitter": "Mohammed Khatiri", "authors": "Mohammed Khatiri, Denis Trystram, Frederic Wagner", "title": "Work Stealing with latency", "comments": "arXiv admin note: text overlap with arXiv:1805.00857", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the impact of communication latency on the classical\nWork Stealing load balancing algorithm. Our approach considers existing\nperformance models and the underlying algorithms. We introduce a latency\nparameter in the model and study its overall impact by careful observations of\nsimulation results. Using this method we are able to derive a new expression of\nthe expected running time of divisible load applications. This expression\nenables us to predict under which conditions a given run will yield acceptable\nperformance. For instance, we can easily calibrate the maximal number of\nprocessors one should use for a given work platform combination. We also\nconsider the impact of several algorithmic variants like simultaneous transfers\nof work or thresholds for avoiding useless transfers. All our results are\nvalidated through simulation on a wide range of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 08:12:30 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Khatiri", "Mohammed", ""], ["Trystram", "Denis", ""], ["Wagner", "Frederic", ""]]}, {"id": "1805.01772", "submitter": "Peter Hawkins", "authors": "Yuan Yu, Mart\\'in Abadi, Paul Barham, Eugene Brevdo, Mike Burrows,\n  Andy Davis, Jeff Dean, Sanjay Ghemawat, Tim Harley, Peter Hawkins, Michael\n  Isard, Manjunath Kudlur, Rajat Monga, Derek Murray, Xiaoqiang Zheng", "title": "Dynamic Control Flow in Large-Scale Machine Learning", "comments": "Appeared in EuroSys 2018. 14 pages, 16 figures", "journal-ref": "EuroSys 2018: Thirteenth EuroSys Conference, April 23-26, 2018,\n  Porto, Portugal. ACM, New York, NY, USA", "doi": "10.1145/3190508.3190551", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent machine learning models rely on fine-grained dynamic control flow\nfor training and inference. In particular, models based on recurrent neural\nnetworks and on reinforcement learning depend on recurrence relations,\ndata-dependent conditional execution, and other features that call for dynamic\ncontrol flow. These applications benefit from the ability to make rapid\ncontrol-flow decisions across a set of computing devices in a distributed\nsystem. For performance, scalability, and expressiveness, a machine learning\nsystem must support dynamic control flow in distributed and heterogeneous\nenvironments.\n  This paper presents a programming model for distributed machine learning that\nsupports dynamic control flow. We describe the design of the programming model,\nand its implementation in TensorFlow, a distributed machine learning system.\nOur approach extends the use of dataflow graphs to represent machine learning\nmodels, offering several distinctive features. First, the branches of\nconditionals and bodies of loops can be partitioned across many machines to run\non a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs.\nSecond, programs written in our model support automatic differentiation and\ndistributed gradient computations, which are necessary for training machine\nlearning models that use control flow. Third, our choice of non-strict\nsemantics enables multiple loop iterations to execute in parallel across\nmachines, and to overlap compute and I/O operations.\n  We have done our work in the context of TensorFlow, and it has been used\nextensively in research and production. We evaluate it using several real-world\napplications, and demonstrate its performance and scalability.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 13:40:07 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Yu", "Yuan", ""], ["Abadi", "Mart\u00edn", ""], ["Barham", "Paul", ""], ["Brevdo", "Eugene", ""], ["Burrows", "Mike", ""], ["Davis", "Andy", ""], ["Dean", "Jeff", ""], ["Ghemawat", "Sanjay", ""], ["Harley", "Tim", ""], ["Hawkins", "Peter", ""], ["Isard", "Michael", ""], ["Kudlur", "Manjunath", ""], ["Monga", "Rajat", ""], ["Murray", "Derek", ""], ["Zheng", "Xiaoqiang", ""]]}, {"id": "1805.01783", "submitter": "Rafael Pereira Pires", "authors": "Florian Kelbert, Franz Gregor, Rafael Pires, Stefan K\\\"opsell, Marcelo\n  Pasin, Aur\\'elien Havet, Valerio Schiavoni, Pascal Felber, Christof Fetzer,\n  Peter Pietzuch", "title": "SecureCloud: Secure Big Data Processing in Untrusted Clouds", "comments": "4 pages. Lausanne, Switzerland", "journal-ref": "Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2017", "doi": "10.23919/DATE.2017.7926999", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the SecureCloud EU Horizon 2020 project, whose goal is to enable\nnew big data applications that use sensitive data in the cloud without\ncompromising data security and privacy. For this, SecureCloud designs and\ndevelops a layered architecture that allows for (i) the secure creation and\ndeployment of secure micro-services; (ii) the secure integration of individual\nmicro-services to full-fledged big data applications; and (iii) the secure\nexecution of these applications within untrusted cloud environments. To provide\nsecurity guarantees, SecureCloud leverages novel security mechanisms present in\nrecent commodity CPUs, in particular, Intel's Software Guard Extensions (SGX).\nSecureCloud applies this architecture to big data applications in the context\nof smart grids. We describe the SecureCloud approach, initial results, and\nconsidered use cases.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 13:47:37 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Kelbert", "Florian", ""], ["Gregor", "Franz", ""], ["Pires", "Rafael", ""], ["K\u00f6psell", "Stefan", ""], ["Pasin", "Marcelo", ""], ["Havet", "Aur\u00e9lien", ""], ["Schiavoni", "Valerio", ""], ["Felber", "Pascal", ""], ["Fetzer", "Christof", ""], ["Pietzuch", "Peter", ""]]}, {"id": "1805.01786", "submitter": "Christina Delimitrou", "authors": "Justin Hu, Ariana Bruno, Drew Zagieboylo, Mark Zhao, Brian Ritchken,\n  Brendon Jackson, Joo Yeon Chae, Francois Mertil, Mateo Espinosa, Christina\n  Delimitrou", "title": "To Centralize or Not to Centralize: A Tale of Swarm Coordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large swarms of autonomous devices are increasing in size and importance.\nWhen it comes to controlling the devices of large-scale swarms there are two\nmain lines of thought. Centralized control, where all decisions - and often\ncompute - happen in a centralized back-end cloud system, and distributed\ncontrol, where edge devices are responsible for selecting and executing tasks\nwith minimal or zero help from a centralized entity. In this work we aim to\nquantify the trade-offs between the two approaches with respect to task\nassignment quality, latency, and reliability. We do so first on a local swarm\nof 12 programmable drones with a 10-server cluster as the backend cloud, and\nthen using a validated simulator to study the tail at scale effects of swarm\ncoordination control. We conclude that although centralized control almost\nalways outperforms distributed in the quality of its decisions, it faces\nsignificant scalability limitations, and we provide a list of system challenges\nthat need to be addressed for centralized control to scale.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 13:50:25 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Hu", "Justin", ""], ["Bruno", "Ariana", ""], ["Zagieboylo", "Drew", ""], ["Zhao", "Mark", ""], ["Ritchken", "Brian", ""], ["Jackson", "Brendon", ""], ["Chae", "Joo Yeon", ""], ["Mertil", "Francois", ""], ["Espinosa", "Mateo", ""], ["Delimitrou", "Christina", ""]]}, {"id": "1805.01790", "submitter": "Siavash Ghiasvand", "authors": "Siavash Ghiasvand and Florina M. Ciorba", "title": "Assessing Data Usefulness for Failure Analysis in Anonymized System Logs", "comments": "11 pages, 3 figures, submitted to 17th IEEE International Symposium\n  on Parallel and Distributed Computing", "journal-ref": null, "doi": "10.1109/ISPDC2018.2018.00031", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System logs are a valuable source of information for the analysis and\nunderstanding of systems behavior for the purpose of improving their\nperformance. Such logs contain various types of information, including\nsensitive information. Information deemed sensitive can either directly be\nextracted from system log entries by correlation of several log entries, or can\nbe inferred from the combination of the (non-sensitive) information contained\nwithin system logs with other logs and/or additional datasets. The analysis of\nsystem logs containing sensitive information compromises data privacy.\nTherefore, various anonymization techniques, such as generalization and\nsuppression have been employed, over the years, by data and computing centers\nto protect the privacy of their users, their data, and the system as a whole.\nPrivacy-preserving data resulting from anonymization via generalization and\nsuppression may lead to significantly decreased data usefulness, thus,\nhindering the intended analysis for understanding the system behavior.\nMaintaining a balance between data usefulness and privacy preservation,\ntherefore, remains an open and important challenge. Irreversible encoding of\nsystem logs using collision-resistant hashing algorithms, such as SHAKE-128, is\na novel approach previously introduced by the authors to mitigate data privacy\nconcerns. The present work describes a study of the applicability of the\nencoding approach from earlier work on the system logs of a production high\nperformance computing system. Moreover, a metric is introduced to assess the\ndata usefulness of the anonymized system logs to detect and identify the\nfailures encountered in the system.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 14:00:56 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ghiasvand", "Siavash", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1805.01993", "submitter": "Songze Li", "authors": "Songze Li, Mohammad Ali Maddah-Ali, A. Salman Avestimehr", "title": "Compressed Coded Distributed Computing", "comments": "A shorter version to appear in ISIT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication overhead is one of the major performance bottlenecks in\nlarge-scale distributed computing systems, in particular for machine learning\napplications. Conventionally, compression techniques are used to reduce the\nload of communication by combining intermediate results of the same computation\ntask as much as possible. Recently, via the development of coded distributed\ncomputing (CDC), it has been shown that it is possible to enable coding\nopportunities across intermediate results of different computation tasks to\nfurther reduce the communication load. We propose a new scheme, named\ncompressed coded distributed computing (in short, compressed CDC), which\njointly exploits the above two techniques (i.e., combining the intermediate\nresults of the same computation and coding across the intermediate results of\ndifferent computations) to significantly reduce the communication load for\ncomputations with linear aggregation (reduction) of intermediate results in the\nfinal stage that are prevalent in machine learning (e.g., distributed training\nalgorithms where partial gradients are computed distributedly and then averaged\nin the final stage). In particular, compressed CDC first compresses/combines\nseveral intermediate results for a single computation, and then utilizes\nmultiple such combined packets to create a coded multicast packet that is\nsimultaneously useful for multiple computations. We characterize the achievable\ncommunication load of compressed CDC and show that it substantially outperforms\nboth combining methods and CDC scheme.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 03:38:28 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Li", "Songze", ""], ["Maddah-Ali", "Mohammad Ali", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1805.02007", "submitter": "Mohammad Asadul Hoque", "authors": "Mohammad A Hoque, Xiaoyan Hong, Md Salman Ahmed", "title": "Parallel Closed-Loop Connected Vehicle Simulator for Large-Scale\n  Transportation Network Management: Challenges, Issues, and Solution\n  Approaches", "comments": "Accepted for publication in IEEE Intelligent Transportation Systems\n  Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The augmented scale and complexity of urban transportation networks have\nsignificantly increased the execution time and resource requirements of\nvehicular network simulations, exceeding the capabilities of sequential\nsimulators. The need for a parallel and distributed simulation environment is\ninevitable from a smart city perspective, especially when the entire city-wide\ninformation system is expected to be integrated with numerous services and ITS\napplications. In this paper, we present a conceptual model of an Integrated\nDistributed Connected Vehicle Simulator (IDCVS) that can emulate real-time\ntraffic in a large metro area by incorporating hardware-in-the-loop simulation\ntogether with the closed-loop coupling of SUMO and OMNET++. We also discuss the\nchallenges, issues, and solution approaches for implementing such a parallel\nclosed-loop transportation network simulator by addressing transportation\nnetwork partitioning problems, synchronization, and scalability issues. One\nunique feature of the envisioned integrated simulation tool is that it utilizes\nthe vehicle traces collected through multiple roadway sensors-DSRC onboard\nunit, magnetometer, loop detector, and video detector. Another major feature of\nthe proposed model is the incorporation of hybrid parallelism in both\ntransportation and communication simulation platforms. We identify the\nchallenges and issues involved in IDCVS to incorporate this multi-level\nparallelism. We also discuss the approaches for integrating\nhardware-in-the-loop simulation, addressing the steps involved in preprocessing\nsensor data, filtering, and extrapolating missing data, managing large\nreal-time traffic data, and handling different data formats.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 05:37:29 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Hoque", "Mohammad A", ""], ["Hong", "Xiaoyan", ""], ["Ahmed", "Md Salman", ""]]}, {"id": "1805.02010", "submitter": "Arjun Sanjeev", "authors": "Venkatesh Choppella, Kasturi Viswanath and Arjun Sanjeev", "title": "Generalised Dining Philosophers as Feedback Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the Generalised Dining Philosophers problem through the\nperspective of feedback control. The result is a modular development of the\nsolution using the notions of system and system composition (the latter due to\nTabuada) in a formal setting that employs simple equational reasoning. The\nmodular approach separates the solution architecture from the algorithmic\nminutiae and has the benefit of simplifying the design and correctness proofs.\n  Three variants of the problem are considered: N=1, and N>1 with centralised\nand distributed topology. The base case (N=1) reveals important insights into\nthe problem specification and the architecture of the solution. In each case,\nsolving the Generalised Dining Philosophers reduces to designing an appropriate\nfeedback controller.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 06:02:48 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Choppella", "Venkatesh", ""], ["Viswanath", "Kasturi", ""], ["Sanjeev", "Arjun", ""]]}, {"id": "1805.02105", "submitter": "Artem Barger", "authors": "Yacov Manevich, Artem Barger, Yoav Tock", "title": "Service Discovery for Hyperledger Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperledger Fabric (HLF) is a modular and extensible permissioned blockchain\nplatform released to open-source and hosted by the Linux Foundation. The\nplatform's design exhibits principles required by enterprise grade business\napplications like supply-chains, financial transactions, asset management, food\nsafety, and many more. For that end HLF introduces several innovations, two of\nwhich are smart contracts in general purpose languages (\\emph{chaincode} in\nHLF), and flexible endorsement policies, which govern whether a transaction is\nconsidered valid.\n  Typical blockchain applications are comprised of two tiers: the first tier\nfocuses on the modelling of the data schema and embedding of business rules\ninto the blockchain by means of smart contracts (\\emph{chaincode}) and\nendorsment policies; and the second tier uses the SDK (Software Development\nKit) provided by HLF to implement client side application logic.\n  However there is a gap between the two tiers that hinders the rapid adoption\nof changes in the chaincode and endorsement policies within the client SDK.\nCurrently, the chaincode location and endorsement policies are statically\nconfigured into the client SDK. This limits the reliability and availability of\nthe client in the event of changes in the platform, and makes the platform more\ndifficult to use. In this work we address and bridge the gap by describing the\ndesign and implementation of \\emph{Service Discovery}.\n  \\emph{Service Discovery} provides APIs which allow dynamic discovery of the\nconfiguration required for the client SDK to interact with the platform,\nalleviating the client from the burden of maintaining it. This enables the\nclient to rapidly adapt to changes in the platform, thus significantly\nimproving the reliability of the application layer. It also makes the HLF\nplatform more consumable, simplifying the job of creating blockchain\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 19:05:19 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Manevich", "Yacov", ""], ["Barger", "Artem", ""], ["Tock", "Yoav", ""]]}, {"id": "1805.02206", "submitter": "Arnold Filtser", "authors": "Arnold Filtser and Nimrod Talmon", "title": "Distributed Monitoring of Election Winners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed elections, where there is a center and $k$ sites. In\nsuch distributed elections, each voter has preferences over some set of\ncandidates, and each voter is assigned to exactly one site such that each site\nis aware only of the voters assigned to it. The center is able to directly\ncommunicate with all sites. We are interested in designing\ncommunication-efficient protocols, allowing the center to maintain a candidate\nwhich, with arbitrarily high probability, is guaranteed to be a winner, or at\nleast close to being a winner. We consider various single-winner voting rules,\nsuch as variants of Approval voting and scoring rules, tournament-based voting\nrules, and several round-based voting rules. For the voting rules we consider,\nwe show that, using communication which is logarithmic in the number of voters,\nit is possible for the center to maintain such approximate winners; that is,\nupon a query at any time the center can immediately return a candidate which is\nguaranteed to be an approximate winner with high probability. We complement our\nprotocols with lower bounds. Our results are theoretical in nature and relate\nto various scenarios, such as aggregating customer preferences in online\nshopping websites or supermarket chains and collecting votes from different\npolling stations of political elections.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 13:16:26 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 08:27:28 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Filtser", "Arnold", ""], ["Talmon", "Nimrod", ""]]}, {"id": "1805.02305", "submitter": "Shadi A. Noghabi", "authors": "Shadi A. Noghabi, Jack Kolb, Peter Bodik, Eduardo Cuervo", "title": "Unified Management and Optimization of Edge-Cloud IoT Applications", "comments": "accepted as NSDI 2018 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) applications have seen a phenomenal growth with\nestimates of growing to a 25 Billion dollar industry by 2020. With the scale of\nIoT applications growing and stricter requirements on latency, edge computing\nhas piqued the interest for such environments. However, the industry is still\nin its infancy with no proper support for applications running across the\nentire edge-cloud environment, and an array of manual tedious per-application\noptimizations. In this work, we propose Steel, a unified framework for\ndeveloping, deploying, and monitoring applications in the edge-cloud. Steel\nsupports dynamically adapting and easily moving services back and forth between\nthe edge and the cloud. Steel is extensible where common optimizations (but\ncrucial for the edge) can be built as pluggable and configurable modules. We\nhave added two very common optimizations: placement and adaptive communication,\nto cope with both short and long-term changes in the workload and environment.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 01:03:08 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Noghabi", "Shadi A.", ""], ["Kolb", "Jack", ""], ["Bodik", "Peter", ""], ["Cuervo", "Eduardo", ""]]}, {"id": "1805.02401", "submitter": "St\\'ephane Devismes", "authors": "Karine Altisen and St\\'ephane Devismes and Ana\\\"is Durand", "title": "Acyclic Strategy for Silent Self-Stabilization in Spanning Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formalize design patterns, commonly used in the\nself-stabilizing area, to obtain general statements regarding both correctness\nand time complexity guarantees. Precisely, we study a general class of\nalgorithms designed for networks endowed with a sense of direction describing a\nspanning forest (e.g., a directed tree or a network where a directed spanning\ntree is available) whose characterization is a simple (i.e., quasi-syntactic)\ncondition. We show that any algorithm of this class is (1) silent and\nself-stabilizing under the distributed unfair daemon, and (2) has a\nstabilization time which is polynomial in moves and asymptotically optimal in\nrounds. To illustrate the versatility of our method, we review several existing\nworks where our results apply.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 08:40:45 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Altisen", "Karine", ""], ["Devismes", "St\u00e9phane", ""], ["Durand", "Ana\u00efs", ""]]}, {"id": "1805.02430", "submitter": "Sateeshkrishna Dhuli", "authors": "S. Kouachi, Sateeshkrishna Dhuli, and Y. N. Singh", "title": "Convergence Rate Analysis for Periodic Gossip Algorithms in Wireless\n  Sensor Networks", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1109/JSEN.2020.3003623", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periodic gossip algorithms have generated a lot of interest due to their\nability to compute the global statistics by using local pairwise communications\namong nodes. Simple execution, robustness to topology changes, and distributed\nnature make these algorithms quite suitable for wireless sensor networks (WSN).\nHowever, these algorithms converge to the global statistics after certain\nrounds of pair-wise communications. A significant challenge for periodic gossip\nalgorithms is difficult to predict the convergence rate for large-scale\nnetworks. To facilitate the convergence rate evaluation, we study a\none-dimensional lattice network model. In this scenario, to derive the explicit\nformula for convergence rate, we have to obtain a closed form expression for\nsecond largest eigenvalue of perturbed pentadiagonal matrices. In our approach,\nwe derive the explicit expressions of eigenvalues by exploiting the theory of\nrecurrent sequences. Unlike the existing methods in the literature, this is a\ndirect method which avoids the theory of orthogonal polynomials [18]. Finally,\nwe derive the explicit expressions for convergence rate of the average periodic\ngossip algorithm in one-dimensional WSNs. We analyze the convergence rate by\nconsidering the linear weight updating approach and investigate the impact of\ngossip weights on the convergence rates for a different number of nodes.\nFurther, we also study the effect of link failures on the convergence rate for\naverage periodic gossip algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 10:22:24 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 18:17:36 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 13:13:52 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kouachi", "S.", ""], ["Dhuli", "Sateeshkrishna", ""], ["Singh", "Y. N.", ""]]}, {"id": "1805.02498", "submitter": "Nandita Vijaykumar", "authors": "Nandita Vijaykumar, Kevin Hsieh, Gennady Pekhimenko, Samira Khan,\n  Ashish Shrestha, Saugata Ghose, Adwait Jog, Phillip B. Gibbons, Onur Mutlu", "title": "Decoupling GPU Programming Models from Resource Management for Enhanced\n  Programming Ease, Portability, and Performance", "comments": "arXiv admin note: substantial text overlap with arXiv:1802.02573", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application resource specification--a static specification of several\nparameters such as the number of threads and the scratchpad memory usage per\nthread block--forms a critical component of modern GPU programming models. This\nspecification determines the parallelism, and hence performance, of the\napplication during execution because the corresponding on-chip hardware\nresources are allocated and managed based on this specification. This\ntight-coupling between the software-provided resource specification and\nresource management in hardware leads to significant challenges in programming\nease, portability, and performance. Zorua is a new resource virtualization\nframework, that decouples the programmer-specified resource usage of a GPU\napplication from the actual allocation in the on-chip hardware resources. Zorua\nenables this decoupling by virtualizing each resource transparently to the\nprogrammer.\n  We demonstrate that by providing the illusion of more resources than\nphysically available via controlled and coordinated virtualization, Zorua\noffers several important benefits: (i) Programming Ease. Zorua eases the burden\non the programmer to provide code that is tuned to efficiently utilize the\nphysically available on-chip resources. (ii) Portability. Zorua alleviates the\nnecessity of re-tuning an application's resource usage when porting the\napplication across GPU generations. (iii) Performance. By dynamically\nallocating resources and carefully oversubscribing them when necessary, Zorua\nimproves or retains the performance of applications that are already highly\ntuned to best utilize the resources.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 23:22:46 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Vijaykumar", "Nandita", ""], ["Hsieh", "Kevin", ""], ["Pekhimenko", "Gennady", ""], ["Khan", "Samira", ""], ["Shrestha", "Ashish", ""], ["Ghose", "Saugata", ""], ["Jog", "Adwait", ""], ["Gibbons", "Phillip B.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.02566", "submitter": "Hyoukjun Kwon", "authors": "Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman\n  Parashar, Vivek Sarkar, Tushar Krishna", "title": "Understanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A\n  Data-Centric Approach Using MAESTRO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data partitioning and scheduling strategies used by DNN accelerators to\nleverage reuse and perform staging are known as dataflow, and they directly\nimpact the performance and energy efficiency of DNN accelerator designs. An\naccelerator microarchitecture dictates the dataflow(s) that can be employed to\nexecute a layer or network. Selecting an optimal dataflow for a layer shape can\nhave a large impact on utilization and energy efficiency, but there is a lack\nof understanding on the choices and consequences of dataflows, and of tools and\nmethodologies to help architects explore the co-optimization design space. In\nthis work, we first introduce a set of data-centric directives to concisely\nspecify the space of DNN dataflows in a compilerfriendly form. We then show how\nthese directives can be analyzed to infer various forms of reuse and to exploit\nthem using hardware capabilities. We codify this analysis into an analytical\ncost model, MAESTRO (Modeling Accelerator Efficiency via Spatio-Temporal Reuse\nand Occupancy), that estimates various cost-benefit tradeoffs of a dataflow\nincluding execution time and energy efficiency for a DNN model and hardware\nconfiguration. We demonstrate the use of MAESTRO to drive a hardware design\nspace exploration (DSE) experiment, which searches across 480M designs to\nidentify 2.5M valid designs at an average rate of 0.17M designs per second,\nincluding Pareto-optimal throughput- and energy-optimized design points.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 15:36:44 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 17:09:00 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 17:53:17 GMT"}, {"version": "v4", "created": "Sun, 8 Sep 2019 01:02:40 GMT"}, {"version": "v5", "created": "Tue, 1 Oct 2019 23:41:20 GMT"}, {"version": "v6", "created": "Mon, 11 May 2020 17:08:01 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kwon", "Hyoukjun", ""], ["Chatarasi", "Prasanth", ""], ["Pellauer", "Michael", ""], ["Parashar", "Angshuman", ""], ["Sarkar", "Vivek", ""], ["Krishna", "Tushar", ""]]}, {"id": "1805.02755", "submitter": "Ra\\'ul Nozal", "authors": "Ra\\'ul Nozal, Jose Luis Bosque and Ram\\'on Beivide (Universidad de\n  Cantabria)", "title": "EngineCL: Usability and Performance in Heterogeneous Computing", "comments": "18 pages, 13 figures, 3 tables, 2 listings Published in Future\n  Generation Computer Systems: Received 31 May 2019, Revised 9 November 2019,\n  Accepted 5 February 2020, Available online 8 February 2020", "journal-ref": "Future Generation Computer Systems Volume 107, June 2020, Pages\n  522-537", "doi": "10.1016/j.future.2020.02.016", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous systems have become one of the most common architectures today,\nthanks to their excellent performance and energy consumption. However, due to\ntheir heterogeneity they are very complex to program and even more to achieve\nperformance portability on different devices. This paper presents EngineCL, a\nnew OpenCL-based runtime system that outstandingly simplifies the co-execution\nof a single massive data-parallel kernel on all the devices of a heterogeneous\nsystem. It performs a set of low level tasks regarding the management of\ndevices, their disjoint memory spaces and scheduling the workload between the\nsystem devices while providing a layered API. EngineCL has been validated in\ntwo compute nodes (HPC and commodity system), that combine six devices with\ndifferent architectures. Experimental results show that it has excellent\nusability compared with OpenCL; a maximum 2.8% of overhead compared to the\nnative version under loads of less than a second of execution and a tendency\ntowards zero for longer execution times; and it can reach an average efficiency\nof 0.89 when balancing the load.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 21:37:03 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 09:48:13 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Nozal", "Ra\u00fal", "", "Universidad de\n  Cantabria"], ["Bosque", "Jose Luis", "", "Universidad de\n  Cantabria"], ["Beivide", "Ram\u00f3n", "", "Universidad de\n  Cantabria"]]}, {"id": "1805.02790", "submitter": "Amy Tai", "authors": "Amy Tai, Andrew Kryczka, Shobhit Kanaujia, Chris Petersen, Mikhail\n  Antonov, Muhammad Waliji, Kyle Jamieson, Michael J. Freedman, Asaf Cidon", "title": "Live Recovery of Bit Corruptions in Datacenter Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its high performance and decreasing cost per bit, flash is becoming\nthe main storage medium in datacenters for hot data. However, flash endurance\nis a perpetual problem, and due to technology trends, subsequent generations of\nflash devices exhibit progressively shorter lifetimes before they experience\nuncorrectable bit errors.\n  In this paper we propose extending flash lifetime by allowing devices to\nexpose higher bit error rates. To do so, we present DIRECT, a novel set of\npolicies that leverages latent redundancy in distributed storage systems to\nrecover from bit corruption errors with minimal performance and recovery\noverhead. In doing so, DIRECT can significantly extend the lifetime of flash\ndevices by effectively utilizing these devices even after they begin exposing\nbit errors.\n  We implemented DIRECT on two real-world storage systems: ZippyDB, a\ndistributed key-value store backed by RocksDB, and HDFS, a distributed file\nsystem. When tested on production traces at Facebook, DIRECT reduces\napplication-visible error rates in ZippyDB by more than 10^2 and recovery time\nby more than 10^4. DIRECT also allows HDFS to tolerate a 10^4--10^5 higher bit\nerror rate without experiencing application-visible errors.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 00:58:10 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 00:34:44 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Tai", "Amy", ""], ["Kryczka", "Andrew", ""], ["Kanaujia", "Shobhit", ""], ["Petersen", "Chris", ""], ["Antonov", "Mikhail", ""], ["Waliji", "Muhammad", ""], ["Jamieson", "Kyle", ""], ["Freedman", "Michael J.", ""], ["Cidon", "Asaf", ""]]}, {"id": "1805.02818", "submitter": "Gowri Sankar Ramachandran Dr", "authors": "Gowri Sankar Ramachandran, Bhaskar Krishnamachari", "title": "Blockchain for the IoT: Opportunities and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology has been transforming the financial industry and has\ncreated a new crypto-economy in the last decade. The foundational concepts such\nas decentralized trust and distributed ledger are promising for distributed,\nand large-scale Internet of Things (IoT) applications. However, the\napplications of Blockchain beyond cryptocurrencies in this domain are few and\nfar between because of the lack of understanding and inherent architectural\nchallenges. In this paper, we describe the opportunities for applications of\nblockchain for the IoT and examine the challenges involved in architecting\nBlockchain-based IoT applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 03:20:48 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Ramachandran", "Gowri Sankar", ""], ["Krishnamachari", "Bhaskar", ""]]}, {"id": "1805.02974", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Xiaorui Sun, Omri Weinstein", "title": "Massively Parallel Algorithms for Finding Well-Connected Components in\n  Sparse Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question that shrouds the emergence of massively parallel\ncomputing (MPC) platforms is how can the additional power of the MPC paradigm\nbe leveraged to achieve faster algorithms compared to classical parallel models\nsuch as PRAM?\n  Previous research has identified the sparse graph connectivity problem as a\nmajor obstacle to such improvement: While classical logarithmic-round PRAM\nalgorithms for finding connected components in any $n$-vertex graph have been\nknown for more than three decades, no $o(\\log{n})$-round MPC algorithms are\nknown for this task with truly sublinear in $n$ memory per machine. This\nproblem arises when processing massive yet sparse graphs with $O(n)$ edges, for\nwhich the interesting setting of parameters is $n^{1-\\Omega(1)}$ memory per\nmachine. It is conjectured that achieving an $o(\\log{n})$-round algorithm for\nconnectivity on general sparse graphs with $n^{1-\\Omega(1)}$ per-machine memory\nmay not be possible, and this conjecture also forms the basis for multiple\nconditional hardness results on the round complexity of other problems in the\nMPC model.\n  We take an opportunistic approach towards the sparse graph connectivity\nproblem, by designing an algorithm with improved performance guarantees in\nterms of the connectivity structure of the input graph. Formally, we design an\nalgorithm that finds all connected components with spectral gap at least\n$\\lambda$ in a graph in $O(\\log\\log{n} + \\log{(1/\\lambda)})$ MPC rounds and\n$n^{\\Omega(1)}$ memory per machine. As such, this algorithm achieves an\nexponential round reduction on sparse \"well-connected\" components (i.e.,\n$\\lambda \\geq 1/\\text{polylog}{(n)}$) using only $n^{\\Omega(1)}$ memory per\nmachine and $\\widetilde{O}(n)$ total memory, and still operates in $o(\\log n)$\nrounds even when $\\lambda = 1/n^{o(1)}$.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 12:29:21 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Assadi", "Sepehr", ""], ["Sun", "Xiaorui", ""], ["Weinstein", "Omri", ""]]}, {"id": "1805.03055", "submitter": "Peilin Zhong", "authors": "Alexandr Andoni, Clifford Stein, Zhao Song, Zhengyu Wang, Peilin Zhong", "title": "Parallel Graph Connectivity in Log Diameter Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graph connectivity problem in MPC model. On an undirected graph with\n$n$ nodes and $m$ edges, $O(\\log n)$ round connectivity algorithms have been\nknown for over 35 years. However, no algorithms with better complexity bounds\nwere known. In this work, we give fully scalable, faster algorithms for the\nconnectivity problem, by parameterizing the time complexity as a function of\nthe diameter of the graph. Our main result is a $O(\\log D \\log\\log_{m/n} n)$\ntime connectivity algorithm for diameter-$D$ graphs, using $\\Theta(m)$ total\nmemory. If our algorithm can use more memory, it can terminate in fewer rounds,\nand there is no lower bound on the memory per processor.\n  We extend our results to related graph problems such as spanning forest,\nfinding a DFS sequence, exact/approximate minimum spanning forest, and\nbottleneck spanning forest. We also show that achieving similar bounds for\nreachability in directed graphs would imply faster boolean matrix\nmultiplication algorithms.\n  We introduce several new algorithmic ideas. We describe a general technique\ncalled double exponential speed problem size reduction which roughly means that\nif we can use total memory $N$ to reduce a problem from size $n$ to $n/k$, for\n$k=(N/n)^{\\Theta(1)}$ in one phase, then we can solve the problem in\n$O(\\log\\log_{N/n} n)$ phases. In order to achieve this fast reduction for graph\nconnectivity, we use a multistep algorithm. One key step is a carefully\nconstructed truncated broadcasting scheme where each node broadcasts neighbor\nsets to its neighbors in a way that limits the size of the resulting neighbor\nsets. Another key step is random leader contraction, where we choose a smaller\nset of leaders than many previous works do.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 14:33:34 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Andoni", "Alexandr", ""], ["Stein", "Clifford", ""], ["Song", "Zhao", ""], ["Wang", "Zhengyu", ""], ["Zhong", "Peilin", ""]]}, {"id": "1805.03141", "submitter": "Ji Liu", "authors": "Ji Liu and Noel Moreno Lemus and Esther Pacitti and Fabio Porto and\n  Patrick Valduriez", "title": "Parallel Computation of PDFs on Big Spatial Data Using Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider big spatial data, which is typically produced in scientific areas\nsuch as geological or seismic interpretation. The spatial data can be produced\nby observation (e.g. using sensors or soil instrument) or numerical simulation\nprograms and correspond to points that represent a 3D soil cube area. However,\nerrors in signal processing and modeling create some uncertainty, and thus a\nlack of accuracy in identifying geological or seismic phenomenons. Such\nuncertainty must be carefully analyzed. To analyze uncertainty, the main\nsolution is to compute a Probability Density Function (PDF) of each point in\nthe spatial cube area. However, computing PDFs on big spatial data can be very\ntime consuming (from several hours to even months on a parallel computer). In\nthis paper, we propose a new solution to efficiently compute such PDFs in\nparallel using Spark, with three methods: data grouping, machine learning\nprediction and sampling. We evaluate our solution by extensive experiments on\ndifferent computer clusters using big data ranging from hundreds of GB to\nseveral TB. The experimental results show that our solution scales up very well\nand can reduce the execution time by a factor of 33 (in the order of seconds or\nminutes) compared with a baseline method.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 16:22:25 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Liu", "Ji", ""], ["Lemus", "Noel Moreno", ""], ["Pacitti", "Esther", ""], ["Porto", "Fabio", ""], ["Valduriez", "Patrick", ""]]}, {"id": "1805.03241", "submitter": "Ruslan Rezin", "authors": "Konstantin Danilov, Ruslan Rezin, Alexander Kolotov and Ilya Afanasyev", "title": "Towards blockchain-based robonomics: autonomous agents behavior\n  validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decentralized trading market approach, where both autonomous agents and\npeople can consume and produce services expanding own opportunities to reach\ngoals, looks very promising as a part of the Fourth Industrial revolution. The\nkey component of the approach is a blockchain platform that allows an\ninteraction between agents via liability smart contracts. Reliability of a\nservice provider is usually determined by a reputation model. However, this\nsolution only warns future customers about an extent of trust to the service\nprovider in case it could not execute any previous liabilities correctly. From\nthe other hand a blockchain consensus protocol can additionally include a\nvalidation procedure that detects incorrect liability executions in order to\nsuspend payment transactions to questionable service providers. The paper\npresents the validation methodology of a liability execution for agent-based\nservice providers in a decentralized trading market, using the Model Checking\nmethod based on the mathematical model of finite state automata and Temporal\nLogic properties of interest. To demonstrate this concept, we implemented the\nmethodology in the Duckietown application, moving an autonomous mobile robot to\nachieve a mission goal with the following behavior validation at the end of a\ncompleted scenario.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 19:24:59 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Danilov", "Konstantin", ""], ["Rezin", "Ruslan", ""], ["Kolotov", "Alexander", ""], ["Afanasyev", "Ilya", ""]]}, {"id": "1805.03357", "submitter": "Chaodong Zheng", "authors": "Fabian Kuhn, Chaodong Zheng", "title": "Efficient Distributed Computation of MIS and Generalized MIS in Linear\n  Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph, a maximal independent set (MIS) is a maximal subset of\npairwise non-adjacent vertices. Finding an MIS is a fundamental problem in\ndistributed computing. Although the problem is extensively studied and well\nunderstood in simple graphs, our knowledge is still quite limited when solving\nit in hypergraphs, especially in the distributed CONGEST model. In this paper,\nwe focus on linear hypergraphs---a class of hypergraphs in which any two\nhyperedges overlap on at most one node.\n  We first present a randomized algorithm for computing an MIS in linear\nhypergraphs. It has poly-logarithmic runtime and it works in the CONGEST model.\nThe algorithm uses a network decomposition to achieve fast parallel processing.\nWithin each cluster of the decomposition, we run a distributed variant of a\nparallel hypergraph MIS algorithm by Luczak and Szymanska.\n  We then propose the concept of a generalized maximal independent set (GMIS)\nas an extension to the classical MIS in hypergraphs. More specifically, in a\nGMIS, for each hyperedge $e$ in a hypergraph $\\mathcal{H}$, we associate an\ninteger threshold $t_e$ in the range $[1, |e|-1]$, and the goal is to find a\nmaximal subset $\\mathcal{I}$ of vertices that do not violate any threshold\nconstraints: $\\forall e \\in E(\\mathcal{H}), |e \\cap \\mathcal{I}| \\leq t_e$. We\nhope that GMIS might capture a broader class of real-world problems than MIS;\nwe also believe that GMIS is an interesting and challenging symmetry breaking\nproblem on its own.\n  Our second upper bound result is a distributed algorithm for computing a GMIS\nin linear hypergraphs, subject to the constraint that the maximum hyperedge\nsize is bounded by some constant. Again, the algorithm has poly-logarithmic\nruntime and it works in the CONGEST model. It is obtained by generalizing our\nprevious (linear) hypergraph MIS algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 03:04:45 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Kuhn", "Fabian", ""], ["Zheng", "Chaodong", ""]]}, {"id": "1805.03391", "submitter": "T-H. Hubert Chan", "authors": "Ittai Abraham, T-H. Hubert Chan, Danny Dolev, Kartik Nayak, Rafael\n  Pass, Ling Ren, Elaine Shi", "title": "Communication Complexity of Byzantine Agreement, Revisited", "comments": "The conference version of this paper appeared in PODC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Byzantine Agreement (BA) protocols find application in large-scale\ndecentralized cryptocurrencies, an increasingly important problem is to design\nBA protocols with improved communication complexity. A few existing works have\nshown how to achieve subquadratic BA under an {\\it adaptive} adversary.\nIntriguingly, they all make a common relaxation about the adaptivity of the\nattacker, that is, if an honest node sends a message and then gets corrupted in\nsome round, the adversary {\\it cannot erase the message that was already sent}\n--- henceforth we say that such an adversary cannot perform \"after-the-fact\nremoval\". By contrast, many (super-)quadratic BA protocols in the literature\ncan tolerate after-the-fact removal. In this paper, we first prove that\ndisallowing after-the-fact removal is necessary for achieving\nsubquadratic-communication BA.\n  Next, we show new subquadratic binary BA constructions (of course, assuming\nno after-the-fact removal) that achieves near-optimal resilience and expected\nconstant rounds under standard cryptographic assumptions and a public-key\ninfrastructure (PKI) in both synchronous and partially synchronous settings. In\ncomparison, all known subquadratic protocols make additional strong assumptions\nsuch as random oracles or the ability of honest nodes to erase secrets from\nmemory, and even with these strong assumptions, no prior work can achieve the\nabove properties. Lastly, we show that some setup assumption is necessary for\nachieving subquadratic multicast-based BA.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 07:17:31 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 03:21:14 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 07:12:03 GMT"}, {"version": "v4", "created": "Sat, 15 Feb 2020 11:16:39 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Abraham", "Ittai", ""], ["Chan", "T-H. Hubert", ""], ["Dolev", "Danny", ""], ["Nayak", "Kartik", ""], ["Pass", "Rafael", ""], ["Ren", "Ling", ""], ["Shi", "Elaine", ""]]}, {"id": "1805.03441", "submitter": "Zheng Wang", "authors": "Zheng Wang and Michael O'Boyle", "title": "Machine Learning in Compiler Optimisation", "comments": "Accepted to be published at Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last decade, machine learning based compilation has moved from an an\nobscure research niche to a mainstream activity. In this article, we describe\nthe relationship between machine learning and compiler optimisation and\nintroduce the main concepts of features, models, training and deployment. We\nthen provide a comprehensive survey and provide a road map for the wide variety\nof different research areas. We conclude with a discussion on open issues in\nthe area and potential research directions. This paper provides both an\naccessible introduction to the fast moving area of machine learning based\ncompilation and a detailed bibliography of its main achievements.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 10:04:28 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Wang", "Zheng", ""], ["O'Boyle", "Michael", ""]]}, {"id": "1805.03472", "submitter": "Michael Feldmann M. Sc.", "authors": "Michael Feldmann and Christian Scheideler", "title": "Skeap & Seap: Scalable Distributed Priority Queues for Constant and\n  Arbitrary Priorities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two protocols for distributed priority queues (for simplicity\ndenoted 'heap') called SKEAP and SEAP. SKEAP realizes a distributed heap for a\nconstant amount of priorities and SEAP one for an arbitrary amount. Both\nprotocols build on an overlay, which induces an aggregation tree on top of\nwhich heap operations are aggregated in batches, ensuring that our protocols\nscale even for a high rate of incoming requests. As part of SEAP we provide a\nnovel distributed protocol for the $k$-selection problem that runs in $O(\\log\nn)$ rounds w.h.p. SKEAP guarantees sequential consistency for its heap\noperations, while SEAP guarantees serializability. SKEAP and SEAP provide\nlogarithmic runtimes w.h.p. on all their operations with SEAP having to use\nonly $O(\\log n)$ bit messages.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 12:16:04 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 11:09:09 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 11:46:06 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2019 09:26:22 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Feldmann", "Michael", ""], ["Scheideler", "Christian", ""]]}, {"id": "1805.03490", "submitter": "Stefano De Angelis", "authors": "Stefano De Angelis", "title": "Assessing Security and Performances of Consensus algorithms for\n  Permissioned Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is a novel technology that is rising a lot of interest in the\nindustrial and re- search sectors because its properties of decentralisation,\nimmutability and data integrity. Initially, the underlying consensus mechanism\nhas been designed for permissionless block- chain on trustless network model\nthrough the proof-of-work, i.e. a mathematical challenge which requires high\ncomputational power. This solution suffers of poor performances, hence\nalternative consensus algorithms as the proof-of-stake have been proposed.\nConversely, for permissioned blockchain, where participants are known and\nauthenti- cated, variants of distributed consensus algorithms have been\nemployed. However, most of them comes out without formal expression of security\nanalysis and trust assumptions because the absence of an established knowledge.\nTherefore the lack of adequate analysis on these algorithms hinders any\ncautious evaluation of their effectiveness in a real-world setting where\nsystems are deployed over trustless networks, i.e. Internet ...\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 12:53:05 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["De Angelis", "Stefano", ""]]}, {"id": "1805.03691", "submitter": "Frederik Mallmann-Trenn", "authors": "Anna Dornhaus, Nancy Lynch, Frederik Mallmann-Trenn, Dominik Pajak and\n  Tsvetomira Radeva", "title": "Self-Stabilizing Task Allocation In Spite of Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributed task allocation inspired by the behavior\nof social insects, which perform task allocation in a setting of limited\ncapabilities and noisy environment feedback. We assume that each task has a\ndemand that should be satisfied but not exceeded, i.e., there is an optimal\nnumber of ants that should be working on this task at a given time. The goal is\nto assign a near-optimal number of workers to each task in a distributed manner\nand without explicit access to the values of the demands nor the number of ants\nworking on the task.\n  We seek to answer the question of how the quality of task allocation depends\non the accuracy of assessing whether too many (overload) or not enough (lack)\nants are currently working on a given task. Concretely, we address the open\nquestion of solving task allocation in the model where each ant receives\nfeedback that depends on the deficit defined as the (possibly negative)\ndifference between the optimal demand and the current number of workers in the\ntask. The feedback is modeled as a random variable that takes value lack or\noverload with probability given by a sigmoid of the deficit. Each ants receives\nthe feedback independently, but the higher the overload or lack of workers for\na task, the more likely it is that all the ants will receive the same, correct\nfeedback from this task; the closer the deficit is to zero, the less reliable\nthe feedback becomes. We measure the performance of task allocation algorithms\nusing the notion of regret, defined as the absolute value of the deficit summed\nover all tasks and summed over time.\n  We propose a simple, constant-memory, self-stabilizing, distributed algorithm\nthat quickly converges from any initial distribution to a near-optimal\nassignment. We also show that our algorithm works not only under stochastic\nnoise but also in an adversarial noise setting.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 18:40:20 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 00:36:13 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Dornhaus", "Anna", ""], ["Lynch", "Nancy", ""], ["Mallmann-Trenn", "Frederik", ""], ["Pajak", "Dominik", ""], ["Radeva", "Tsvetomira", ""]]}, {"id": "1805.03721", "submitter": "Natanael Arndt", "authors": "Natanael Arndt, Patrick Naumann, Norman Radtke, Michael Martin, Edgard\n  Marx", "title": "Decentralized Collaborative Knowledge Management using Git", "comments": "Special Issue on Managing the Evolution and Preservation of the Data\n  Web", "journal-ref": null, "doi": "10.1016/j.websem.2018.08.002", "report-no": null, "categories": "cs.DB cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web and the Semantic Web are designed as a network of\ndistributed services and datasets. The distributed character of the Web brings\nmanifold collaborative possibilities to interchange data. The commonly adopted\ncollaborative solutions for RDF data are centralized (e.g. SPARQL endpoints and\nwiki systems). But to support distributed collaboration, a system is needed,\nthat supports divergence of datasets, brings the possibility to conflate\ndiverged states, and allows distributed datasets to be synchronized. In this\npaper, we present Quit Store, it was inspired by and it builds upon the\nsuccessful Git system. The approach is based on a formal expression of\nevolution and consolidation of distributed datasets. During the collaborative\ncuration process, the system automatically versions the RDF dataset and tracks\nprovenance information. It also provides support to branch, merge, and\nsynchronize distributed RDF datasets. The merging process is guarded by\nspecific merge strategies for RDF data. Finally, we use our reference\nimplementation to show overall good performance and demonstrate the practical\nusability of the system.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 20:24:20 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 11:48:38 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 07:53:05 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Arndt", "Natanael", ""], ["Naumann", "Patrick", ""], ["Radtke", "Norman", ""], ["Martin", "Michael", ""], ["Marx", "Edgard", ""]]}, {"id": "1805.03727", "submitter": "Nicolas Nicolaou", "authors": "Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi,\n  Kishori M. Konwar, Nancy Lynch, Muriel Medard", "title": "ARES: Adaptive, Reconfigurable, Erasure coded, atomic Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Atomicity or strong consistency is one of the fundamental, most intuitive,\nand hardest to provide primitives in distributed shared memory emulations. To\nensure survivability, scalability, and availability of a storage service in the\npresence of failures, traditional approaches for atomic memory emulation, in\nmessage passing environments, replicate the objects across multiple servers.\nCompared to replication based algorithms, erasure code-based atomic memory\nalgorithms has much lower storage and communication costs, but usually, they\nare harder to design. The difficulty of designing atomic memory algorithms\nfurther grows, when the set of servers may be changed to ensure survivability\nof the service over software and hardware upgrades, while avoiding service\ninterruptions. Atomic memory algorithms for performing server reconfiguration,\nin the replicated systems, are very few, complex, and are still part of an\nactive area of research; reconfigurations of erasure-code based algorithms are\nnon-existent.\n  In this work, we present ARES, an algorithmic framework that allows\nreconfiguration of the underlying servers, and is particularly suitable for\nerasure-code based algorithms emulating atomic objects. ARES introduces new\nconfigurations while keeping the service available. To use with ARES we also\npropose a new, and to our knowledge, the first two-round erasure code based\nalgorithm TREAS, for emulating multi-writer, multi-reader (MWMR) atomic objects\nin asynchronous, message-passing environments, with near-optimal communication\nand storage costs. Our algorithms can tolerate crash failures of any client and\nsome fraction of servers, and yet, guarantee safety and liveness property.\nMoreover, by bringing together the advantages of ARES and TREAS, we propose an\noptimized algorithm where new configurations can be installed without the\nobjects values passing through the reconfiguration clients.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 20:44:02 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 15:59:14 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Nicolaou", "Nicolas", ""], ["Cadambe", "Viveck", ""], ["Prakash", "N.", ""], ["Trigeorgi", "Andria", ""], ["Konwar", "Kishori M.", ""], ["Lynch", "Nancy", ""], ["Medard", "Muriel", ""]]}, {"id": "1805.03812", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Qiang Wang, Xiaowen Chu and Bo Li", "title": "A DAG Model of Synchronous Stochastic Gradient Descent in Distributed\n  Deep Learning", "comments": "8 pages. Accepted by ICPADS'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With huge amounts of training data, deep learning has made great\nbreakthroughs in many artificial intelligence (AI) applications. However, such\nlarge-scale data sets present computational challenges, requiring training to\nbe distributed on a cluster equipped with accelerators like GPUs. With the fast\nincrease of GPU computing power, the data communications among GPUs have become\na potential bottleneck on the overall training performance. In this paper, we\nfirst propose a general directed acyclic graph (DAG) model to describe the\ndistributed synchronous stochastic gradient descent (S-SGD) algorithm, which\nhas been widely used in distributed deep learning frameworks. To understand the\npractical impact of data communications on training performance, we conduct\nextensive empirical studies on four state-of-the-art distributed deep learning\nframeworks (i.e., Caffe-MPI, CNTK, MXNet and TensorFlow) over multi-GPU and\nmulti-node environments with different data communication techniques, including\nPCIe, NVLink, 10GbE, and InfiniBand. Through both analytical and experimental\nstudies, we identify the potential bottlenecks and overheads that could be\nfurther optimized. At last, we make the data set of our experimental traces\npublicly available, which could be used to support simulation-based studies.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 04:28:49 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 07:14:35 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 17:28:04 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Shi", "Shaohuai", ""], ["Wang", "Qiang", ""], ["Chu", "Xiaowen", ""], ["Li", "Bo", ""]]}, {"id": "1805.03841", "submitter": "Beau Johnston", "authors": "Beau Johnston and Josh Milthorpe", "title": "Dwarfs on Accelerators: Enhancing OpenCL Benchmarking for Heterogeneous\n  Computing Architectures", "comments": "10 pages, 5 figures", "journal-ref": "ICPP 2018 Proceedings of the 47th International Conference on\n  Parallel Processing Companion", "doi": "10.1145/3229710.3229729", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For reasons of both performance and energy efficiency, high-performance\ncomputing (HPC) hardware is becoming increasingly heterogeneous. The OpenCL\nframework supports portable programming across a wide range of computing\ndevices and is gaining influence in programming next-generation accelerators.\nTo characterize the performance of these devices across a range of applications\nrequires a diverse, portable and configurable benchmark suite, and OpenCL is an\nattractive programming model for this purpose. We present an extended and\nenhanced version of the OpenDwarfs OpenCL benchmark suite, with a strong focus\nplaced on the robustness of applications, curation of additional benchmarks\nwith an increased emphasis on correctness of results and choice of problem\nsize. Preliminary results and analysis are reported for eight benchmark codes\non a diverse set of architectures -- three Intel CPUs, five Nvidia GPUs, six\nAMD GPUs and a Xeon Phi.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 06:31:13 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Johnston", "Beau", ""], ["Milthorpe", "Josh", ""]]}, {"id": "1805.03870", "submitter": "Fan Long", "authors": "Chenxing Li, Peilun Li, Dong Zhou, Wei Xu, Fan Long, Andrew Yao", "title": "Scaling Nakamoto Consensus to Thousands of Transactions per Second", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Conflux, a fast, scalable and decentralized blockchain\nsystem that optimistically process concurrent blocks without discarding any as\nforks. The Conflux consensus protocol represents relationships between blocks\nas a direct acyclic graph and achieves consensus on a total order of the\nblocks. Conflux then, from the block order, deterministically derives a\ntransaction total order as the blockchain ledger. We evaluated Conflux on\nAmazon EC2 clusters with up to 20k full nodes. Conflux achieves a transaction\nthroughput of 5.76GB/h while confirming transactions in 4.5-7.4 minutes. The\nthroughput is equivalent to 6400 transactions per second for typical Bitcoin\ntransactions. Our results also indicate that when running Conflux, the\nconsensus protocol is no longer the throughput bottleneck. The bottleneck is\ninstead at the processing capability of individual nodes.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 07:54:02 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 16:49:16 GMT"}, {"version": "v3", "created": "Sun, 12 Aug 2018 18:48:06 GMT"}, {"version": "v4", "created": "Fri, 31 Aug 2018 19:15:22 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Chenxing", ""], ["Li", "Peilun", ""], ["Zhou", "Dong", ""], ["Xu", "Wei", ""], ["Long", "Fan", ""], ["Yao", "Andrew", ""]]}, {"id": "1805.03887", "submitter": "Luca Venturini", "authors": "Luca Venturini, Elena Baralis, Paolo Garza", "title": "Scaling associative classification for very large datasets", "comments": null, "journal-ref": "J Big Data (2017) 4: 44", "doi": "10.1186/s40537-017-0107-2", "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised learning algorithms are nowadays successfully scaling up to\ndatasets that are very large in volume, leveraging the potential of in-memory\ncluster-computing Big Data frameworks. Still, massive datasets with a number of\nlarge-domain categorical features are a difficult challenge for any classifier.\nMost off-the-shelf solutions cannot cope with this problem. In this work we\nintroduce DAC, a Distributed Associative Classifier. DAC exploits ensemble\nlearning to distribute the training of an associative classifier among parallel\nworkers and improve the final quality of the model. Furthermore, it adopts\nseveral novel techniques to reach high scalability without sacrificing quality,\namong which a preventive pruning of classification rules in the extraction\nphase based on Gini impurity. We ran experiments on Apache Spark, on a real\nlarge-scale dataset with more than 4 billion records and 800 million distinct\ncategories. The results showed that DAC improves on a state-of-the-art solution\nin both prediction quality and execution time. Since the generated model is\nhuman-readable, it can not only classify new records, but also allow\nunderstanding both the logic behind the prediction and the properties of the\nmodel, becoming a useful aid for decision makers.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 08:41:55 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Venturini", "Luca", ""], ["Baralis", "Elena", ""], ["Garza", "Paolo", ""]]}, {"id": "1805.03949", "submitter": "Guillaume Houzeaux", "authors": "Marta Garcia-Gasulla, Guillaume Houzeaux, Roger Ferrer, Antoni\n  Artigues, Victor L\\'opez, Jes\\'us Labarta and Mariano V\\'azquez", "title": "MPI+X: task-based parallelization and dynamic load balance of finite\n  element assembly", "comments": null, "journal-ref": null, "doi": "10.1080/10618562.2019.1617856", "report-no": null, "categories": "cs.MS cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main computing tasks of a finite element code(FE) for solving partial\ndifferential equations (PDE's) are the algebraic system assembly and the\niterative solver. This work focuses on the first task, in the context of a\nhybrid MPI+X paradigm. Although we will describe algorithms in the FE context,\na similar strategy can be straightforwardly applied to other discretization\nmethods, like the finite volume method. The matrix assembly consists of a loop\nover the elements of the MPI partition to compute element matrices and\nright-hand sides and their assemblies in the local system to each MPI\npartition. In a MPI+X hybrid parallelism context, X has consisted traditionally\nof loop parallelism using OpenMP. Several strategies have been proposed in the\nliterature to implement this loop parallelism, like coloring or substructuring\ntechniques to circumvent the race condition that appears when assembling the\nelement system into the local system. The main drawback of the first technique\nis the decrease of the IPC due to bad spatial locality. The second technique\navoids this issue but requires extensive changes in the implementation, which\ncan be cumbersome when several element loops should be treated. We propose an\nalternative, based on the task parallelism of the element loop using some\nextensions to the OpenMP programming model. The taskification of the assembly\nsolves both aforementioned problems. In addition, dynamic load balance will be\napplied using the DLB library, especially efficient in the presence of hybrid\nmeshes, where the relative costs of the different elements is impossible to\nestimate a priori. This paper presents the proposed methodology, its\nimplementation and its validation through the solution of large computational\nmechanics problems up to 16k cores.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 16:01:01 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Garcia-Gasulla", "Marta", ""], ["Houzeaux", "Guillaume", ""], ["Ferrer", "Roger", ""], ["Artigues", "Antoni", ""], ["L\u00f3pez", "Victor", ""], ["Labarta", "Jes\u00fas", ""], ["V\u00e1zquez", "Mariano", ""]]}, {"id": "1805.03965", "submitter": "Fukuhito Ooshita", "authors": "Fukuhito Ooshita and S\\'ebastien Tixeuil", "title": "Ring Exploration with Myopic Luminous Robots", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate exploration algorithms for autonomous mobile robots evolving\nin uniform ring-shaped networks. Different from the usual Look-Compute-Move\n(LCM) model, we consider two characteristics: myopia and luminosity. Myopia\nmeans each robot has a limited visibility. We consider the weakest assumption\nfor myopia: each robot can only observe its neighboring nodes. Luminosity means\neach robot maintains a non-volatile visible light. We consider the weakest\nassumption for luminosity: each robot can use only two colors for its light.\nThe main interest of this paper is to clarify the impact of luminosity on\nexploration with myopic robots. As a main contribution, we prove that 1) in the\nfully synchronous model, two and three robots are necessary and sufficient to\nachieve perpetual and terminating exploration, respectively, and 2) in the\nsemi-synchronous and asynchronous models, three and four robots are necessary\nand sufficient to achieve perpetual and terminating exploration, respectively.\nThese results clarify the power of lights for myopic robots since, without\nlights, five robots are necessary and sufficient to achieve terminating\nexploration in the fully synchronous model, and no terminating exploration\nalgorithm exists in the semi-synchronous and asynchronous models. We also show\nthat, in the fully synchronous model (resp., the semi-synchronous and\nasynchronous models), the proposed perpetual exploration algorithm is\nuniversal, that is, the algorithm solves perpetual exploration from any\nsolvable initial configuration with two (resp., three) robots and two colors.\nOn the other hand, we show that, in the fully synchronous model (resp., the\nsemi-synchronous and asynchronous models), no universal algorithm exists for\nterminating exploration, that is, no algorithm may solve terminating\nexploration from any solvable initial configuration with three (resp., four)\nrobots and two colors.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 13:29:06 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Ooshita", "Fukuhito", ""], ["Tixeuil", "S\u00e9bastien", ""]]}, {"id": "1805.03992", "submitter": "Yotam Feldman", "authors": "Yotam M. Y. Feldman and Constantin Enea and Adam Morrison and Noam\n  Rinetzky and Sharon Shoham", "title": "Order out of Chaos: Proving Linearizability Using Local Views", "comments": "Full version of the DISC'18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proving the linearizability of highly concurrent data structures, such as\nthose using optimistic concurrency control, is a challenging task. The main\ndifficulty is in reasoning about the view of the memory obtained by the\nthreads, because as they execute, threads observe different fragments of memory\nfrom different points in time. Until today, every linearizability proof has\ntackled this challenge from scratch.\n  We present a unifying proof argument for the correctness of unsynchronized\ntraversals, and apply it to prove the linearizability of several highly\nconcurrent search data structures, including an optimistic self-balancing\nbinary search tree, the Lazy List and a lock-free skip list. Our framework\nharnesses {\\em sequential reasoning} about the view of a thread, considering\nthe thread as if it traverses the data structure without interference from\nother operations. Our key contribution is showing that properties of\nreachability along search paths can be deduced for concurrent traversals from\nsuch interference-free traversals, when certain intuitive conditions are met.\nBasing the correctness of traversals on such \\emph{local view arguments}\ngreatly simplifies linearizability proofs.\n  To apply our framework, the user proves that the data structure satisfies two\nconditions: (1) acyclicity of the order on memory, even when it is considered\nacross intermediate memory states, and (2) preservation of search paths to\nlocations modified by interfering writes. Establishing the conditions, as well\nas the full linearizability proof utilizing our proof argument, reduces to\nsimple concurrent reasoning. The result is a clear and comprehensible\ncorrectness proof, and elucidates common patterns underlying several existing\ndata structures.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 14:19:16 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 12:11:04 GMT"}, {"version": "v3", "created": "Sun, 5 Aug 2018 14:59:30 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Feldman", "Yotam M. Y.", ""], ["Enea", "Constantin", ""], ["Morrison", "Adam", ""], ["Rinetzky", "Noam", ""], ["Shoham", "Sharon", ""]]}, {"id": "1805.04005", "submitter": "Enis Afgan", "authors": "Enis Afgan, Andrew Lonie, James Taylor, Nuwan Goonasekera", "title": "CloudLaunch: Discover and Deploy Cloud Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a common platform for delivering software to end users.\nHowever, the process of making complex-to-deploy applications available across\ndifferent cloud providers requires isolated and uncoordinated\napplication-specific solutions, often locking-in developers to a particular\ncloud provider. Here, we present the CloudLaunch application as a uniform\nplatform for discovering and deploying applications for different cloud\nproviders. CloudLaunch allows arbitrary applications to be added to a catalog\nwith each application having its own customizable user interface and control\nover the launch process, while preserving cloud-agnosticism so that authors can\neasily make their applications available on multiple clouds with minimal\neffort. It then provides a uniform interface for launching available\napplications by end users across different cloud providers. Architecture\ndetails are presented along with examples of different deployable applications\nthat highlight architectural features.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 14:34:01 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 02:19:53 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Afgan", "Enis", ""], ["Lonie", "Andrew", ""], ["Taylor", "James", ""], ["Goonasekera", "Nuwan", ""]]}, {"id": "1805.04007", "submitter": "Yinhao Li", "authors": "Yinhao Li, Awa Alqahtani, Ellis Solaiman, Charith Perera, Prem Prakash\n  Jayaraman, Boualem Benatallah, and Rajiv Ranjan", "title": "A Unified Knowledge Representation and Context-aware Recommender System\n  in Internet of Things", "comments": "This paper is an incomplete draft. Therefore, I would like to\n  withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the rapidly developing Internet of Things (IoT), numerous and diverse\nphysical devices, Edge devices, Cloud infrastructure, and their quality of\nservice requirements (QoS), need to be represented within a unified\nspecification in order to enable rapid IoT application development, monitoring,\nand dynamic reconfiguration. But heterogeneities among different configuration\nknowledge representation models pose limitations for acquisition, discovery and\ncuration of configuration knowledge for coordinated IoT applications. This\npaper proposes a unified data model to represent IoT resource configuration\nknowledge artifacts. It also proposes IoT-CANE (Context-Aware recommendatioN\nsystEm) to facilitate incremental knowledge acquisition and declarative context\ndriven knowledge recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 14:42:00 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 13:59:41 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Li", "Yinhao", ""], ["Alqahtani", "Awa", ""], ["Solaiman", "Ellis", ""], ["Perera", "Charith", ""], ["Jayaraman", "Prem Prakash", ""], ["Benatallah", "Boualem", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1805.04071", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang", "title": "Energy Complexity of Distance Computation in Multi-hop Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency is a critical issue for wireless devices operated under\nstringent power constraint (e.g., battery). Following prior works, we measure\nthe energy cost of a device by its transceiver usage, and define the energy\ncomplexity of an algorithm as the maximum number of time slots a device\ntransmits or listens, over all devices. In a recent paper of Chang et al. (PODC\n2018), it was shown that broadcasting in a multi-hop network of unknown\ntopology can be done in $\\text{poly} \\log n$ energy. In this paper, we continue\nthis line of research, and investigate the energy complexity of other\nfundamental graph problems in multi-hop networks. Our results are summarized as\nfollows.\n  1. To avoid spending $\\Omega(D)$ energy, the broadcasting protocols of Chang\net al. (PODC 2018) do not send the message along a BFS tree, and it is open\nwhether BFS could be computed in $o(D)$ energy, for sufficiently large $D$. In\nthis paper we devise an algorithm that attains $\\tilde{O}(\\sqrt{n})$ energy\ncost.\n  2. We show that the framework of the ${\\Omega}(n)$ round lower bound proof\nfor computing diameter in CONGEST of Abboud et al. (DISC 2017) can be adapted\nto give an $\\tilde{\\Omega}(n)$ energy lower bound in the wireless network model\n(with no message size constraint), and this lower bound applies to $O(\\log\nn)$-arboricity graphs. From the upper bound side, we show that the energy\ncomplexity of $\\tilde{O}(\\sqrt{n})$ can be attained for bounded-genus graphs\n(which includes planar graphs).\n  3. Our upper bounds for computing diameter can be extended to other graph\nproblems. We show that exact global minimum cut or approximate $s$--$t$ minimum\ncut can be computed in $\\tilde{O}(\\sqrt{n})$ energy for bounded-genus graphs.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 17:25:23 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 12:54:17 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Chang", "Yi-Jun", ""]]}, {"id": "1805.04165", "submitter": "Ellis Hershkowitz", "authors": "Keren Censor-Hillel and Bernhard Haeupler and D Ellis Hershkowitz and\n  Goran Zuzic", "title": "Erasure Correction for Noisy Radio Networks", "comments": "We gave significantly more high level intuition of our results in a\n  new section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The radio network model is a well-studied model of wireless, multi-hop\nnetworks. However, radio networks make the strong assumption that messages are\ndelivered deterministically. The recently introduced noisy radio network model\nrelaxes this assumption by dropping messages independently at random.\n  In this work we quantify the relative computational power of noisy radio\nnetworks and classic radio networks. In particular, given a non-adaptive\nprotocol for a fixed radio network we show how to reliably simulate this\nprotocol if noise is introduced with a multiplicative cost of\n$\\mathrm{poly}(\\log \\Delta, \\log \\log n)$ rounds where $n$ is the number nodes\nin the network and $\\Delta$ is the max degree. Moreover, we demonstrate that,\neven if the simulated protocol is not non-adaptive, it can be simulated with a\nmultiplicative $O(\\Delta \\log ^2 \\Delta)$ cost in the number of rounds. Lastly,\nwe argue that simulations with a multiplicative overhead of $o(\\log \\Delta)$\nare unlikely to exist by proving that an $\\Omega(\\log \\Delta)$ multiplicative\nround overhead is necessary under certain natural assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:29:03 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 00:42:05 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 18:48:26 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Haeupler", "Bernhard", ""], ["Hershkowitz", "D Ellis", ""], ["Zuzic", "Goran", ""]]}, {"id": "1805.04170", "submitter": "Minjie Wang", "authors": "Minjie Wang, Chien-chin Huang and Jinyang Li", "title": "Unifying Data, Model and Hybrid Parallelism in Deep Learning via Tensor\n  Tiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning systems have become vital tools across many fields, but the\nincreasing model sizes mean that training must be accelerated to maintain such\nsystems' utility. Current systems like Tensorflow and MXNet focus on one\nspecific parallelization strategy, data parallelism, which requires large\ntraining batch sizes in order to scale. We cast the problem of finding the best\nparallelization strategy as the problem of finding the best tiling to partition\ntensors with the least overall communication. We propose an algorithm that can\nfind the optimal tiling. Our resulting parallelization solution is a hybrid of\ndata parallelism and model parallelism. We build the SoyBean system that\nperforms automatic parallelization. SoyBean automatically transforms a serial\ndataflow graph captured by an existing deep learning system frontend into a\nparallel dataflow graph based on the optimal tiling it has found. Our\nevaluations show that SoyBean is 1.5x-4x faster than pure data parallelism for\nAlexNet and VGG. We present this automatic tiling in a new system, SoyBean,\nthat can act as a backend for Tensorflow, MXNet, and others.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:38:56 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Wang", "Minjie", ""], ["Huang", "Chien-chin", ""], ["Li", "Jinyang", ""]]}, {"id": "1805.04263", "submitter": "Martin Kleppmann", "authors": "Martin Kleppmann, Victor B. F. Gomes, Dominic P. Mulligan, Alastair R.\n  Beresford", "title": "OpSets: Sequential Specifications for Replicated Datatypes (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce OpSets, an executable framework for specifying and reasoning\nabout the semantics of replicated datatypes that provide eventual consistency\nin a distributed system, and for mechanically verifying algorithms that\nimplement these datatypes. Our approach is simple but expressive, allowing us\nto succinctly specify a variety of abstract datatypes, including maps, sets,\nlists, text, graphs, trees, and registers. Our datatypes are also composable,\nenabling the construction of complex data structures. To demonstrate the\nutility of OpSets for analysing replication algorithms, we highlight an\nimportant correctness property for collaborative text editing that has\ntraditionally been overlooked; algorithms that do not satisfy this property can\nexhibit awkward interleaving of text. We use OpSets to specify this correctness\nproperty and prove that although one existing replication algorithm satisfies\nthis property, several other published algorithms do not. We also show how\nOpSets can be used to develop new replicated datatypes: we provide a simple\nspecification of an atomic move operation for trees, an operation that had\npreviously been thought to be impossible to implement without locking. We use\nthe Isabelle/HOL proof assistant to formalise the OpSets approach and produce\nmechanised proofs of correctness of the main claims in this paper, thereby\neliminating the ambiguity of previous informal approaches, and ruling out\nreasoning errors that could occur in handwritten proofs.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 07:36:33 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 10:07:05 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Kleppmann", "Martin", ""], ["Gomes", "Victor B. F.", ""], ["Mulligan", "Dominic P.", ""], ["Beresford", "Alastair R.", ""]]}, {"id": "1805.04319", "submitter": "D\\'aniel Ber\\'enyi", "authors": "D\\'aniel Ber\\'enyi and Andr\\'as Leitereg and G\\'abor Lehel", "title": "Towards scalable pattern-based optimization for dense linear algebra", "comments": "23 pages, 6 figures, was presented on the Lambda Days 2018 Conference", "journal-ref": "Concurrency and Computation: Practice and Experience 30:e4696\n  (2018)", "doi": "10.1002/cpe.4696", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear algebraic expressions are the essence of many computationally\nintensive problems, including scientific simulations and machine learning\napplications. However, translating high-level formulations of these expressions\nto efficient machine-level representations is far from trivial: developers\nshould be assisted by automatic optimization tools so that they can focus their\nattention on high-level problems, rather than low-level details. The\ntractability of these optimizations is highly dependent on the choice of the\nprimitive constructs in terms of which the computations are to be expressed. In\nthis work we propose to describe operations on multi-dimensional arrays using a\nselection of higher-order functions, inspired by functional programming, and we\npresent rewrite rules for these such that they can be automatically optimized\nfor modern hierarchical and heterogeneous architectures. Using this formalism\nwe systematically construct and analyse different subdivisions and permutations\nof the dense matrix multiplication problem.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 10:54:41 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Ber\u00e9nyi", "D\u00e1niel", ""], ["Leitereg", "Andr\u00e1s", ""], ["Lehel", "G\u00e1bor", ""]]}, {"id": "1805.04337", "submitter": "Ramy Ali", "authors": "Ramy E. Ali, Viveck Cadambe, Jaime Llorca and Antonia Tulino", "title": "Fundamental Limits of Erasure-Coded Key-Value Stores with Side\n  Information", "comments": "Extended version of the ISIT 2018 paper that generalizes the code\n  constructions and converse for arbitrary graphs, and a case study based on\n  Amazon web services pricing and latency information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of distributed storage systems to modern key-value stores,\nthe stored data is highly dynamic due to frequent updates. The multi-version\ncoding problem was formulated to study the cost of storing dynamic data in\ndistributed storage systems. Previous work on multi-version coding considered a\ncompletely decentralized and asynchronous system assuming that the servers are\nnot aware of which versions of the data are received by the other servers. In\nthis paper, we relax this assumption and study a system where a server may\nacquire side information of the data versions propagated to some other servers\nbased on the network topology. Specifically, we study a storage system with $n$\nservers over a directed graph that store $\\nu$ totally ordered versions of a\nmessage. Each server receives a subset of these $\\nu$ versions. A server is\naware of which versions have been received by its neighbors in the network\ngraph. We show that the side information can result in a better storage cost as\ncompared with the case where there is no side information for some regimes at\nthe expense of the additional latency associated with exchanging the side\ninformation. Through an information-theoretic converse, we identify surprising\nscenarios where the side information may not help in improving the worst-case\nstorage cost beyond the case where servers have no side information. Finally,\nwe present a case study over Amazon web services (AWS) that demonstrates the\npotential cost reductions that may be obtained by our constructions.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 11:36:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 18:48:51 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 23:59:58 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Ali", "Ramy E.", ""], ["Cadambe", "Viveck", ""], ["Llorca", "Jaime", ""], ["Tulino", "Antonia", ""]]}, {"id": "1805.04449", "submitter": "Mansour Khelghatdoust", "authors": "Mansour Khelghatdoust and Vincent Gramoli", "title": "Peacock: Probe-Based Scheduling of Jobs by Rotating Between Elastic\n  Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose Peacock, a new distributed probe-based scheduler\nwhich handles heterogeneous workloads in data analytics frameworks with low\nlatency. Peacock mitigates the \\emph{Head-of-Line blocking} problem, i.e.,\nshorter tasks are enqueued behind the longer tasks, better than the\nstate-of-the-art. To this end, we introduce a novel probe rotation technique.\nWorkers form a ring overlay network and rotate probes using elastic queues. It\nis augmented by a novel probe reordering algorithm executed in workers. We\nevaluate the performance of Peacock against two state-of-the-art probe-based\nsolutions through both trace-driven simulation and distributed experiment in\nSpark under various loads and cluster sizes. Our large-scale performance\nresults indicate that Peacock outperforms the state-of-the-art in all cluster\nsizes and loads. Our distributed experiments confirm our simulation results.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 15:28:40 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Khelghatdoust", "Mansour", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1805.04548", "submitter": "Mahnush Movahedi", "authors": "Timo Hanke, Mahnush Movahedi, Dominic Williams", "title": "DFINITY Technology Overview Series, Consensus System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DFINITY blockchain computer provides a secure, performant and flexible\nconsensus mechanism. At its core, DFINITY contains a decentralized randomness\nbeacon which acts as a verifiable random function (VRF) that produces a stream\nof outputs over time. The novel technique behind the beacon relies on the\nexistence of a unique-deterministic, non-interactive, DKG-friendly threshold\nsignatures scheme. The only known examples of such a scheme are pairing-based\nand derived from BLS.\n  The DFINITY blockchain is layered on top of the DFINITY beacon and uses the\nbeacon as its source of randomness for leader selection and leader ranking. A\n\"weight\" is attributed to a chain based on the ranks of the leaders who propose\nthe blocks in the chain, and that weight is used to select between competing\nchains. The DFINITY blockchain is layered on top of the DFINITY beacon and uses\nthe beacon as its source of randomness for leader selection and leader ranking\nblockchain is further hardened by a notarization process which dramatically\nimproves the time to finality and eliminates the nothing-at-stake and selfish\nmining attacks.\n  DFINITY consensus algorithm is made to scale through continuous quorum\nselections driven by the random beacon. In practice, DFINITY achieves block\ntimes of a few seconds and transaction finality after only two confirmations.\nThe system gracefully handles temporary losses of network synchrony including\nnetwork splits, while it is provably secure under synchrony.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 18:35:09 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Hanke", "Timo", ""], ["Movahedi", "Mahnush", ""], ["Williams", "Dominic", ""]]}, {"id": "1805.04586", "submitter": "Peter Kling", "authors": "Petra Berenbrink, Robert Els\\\"asser, Tom Friedetzky, Dominik Kaaser,\n  Peter Kling, Tomasz Radzik", "title": "Time-space Trade-offs in Population Protocols for the Majority Problem", "comments": null, "journal-ref": null, "doi": "10.1007/s00446-020-00385-0", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a model for distributed computing that is focused on\nsimplicity and robustness. A system of $n$ identical agents (finite state\nmachines) performs a global task like electing a unique leader or determining\nthe majority opinion when each agent has one of two opinions. Agents\ncommunicate in pairwise interactions with randomly assigned communication\npartners. Quality is measured in two ways: the number of interactions to\ncomplete the task and the number of states per agent. We present protocols for\nthe majority problem that allow for a trade-off between these two measures.\nCompared to the only other trade-off result [Alistarh, Gelashvili, Vojnovic;\nPODC'15], we improve the number of interactions by almost a linear factor.\nFurthermore, our protocols can be made uniform (working correctly without any\ninformation on the population size $n$), yielding the first uniform majority\nprotocols that stabilize in a subquadratic number of interactions.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 20:43:15 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 06:10:21 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 15:43:24 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Berenbrink", "Petra", ""], ["Els\u00e4sser", "Robert", ""], ["Friedetzky", "Tom", ""], ["Kaaser", "Dominik", ""], ["Kling", "Peter", ""], ["Radzik", "Tomasz", ""]]}, {"id": "1805.04599", "submitter": "Joshua Daymude", "authors": "Sarah Cannon and Joshua J. Daymude and Cem Gokmen and Dana Randall and\n  Andr\\'ea W. Richa", "title": "A Local Stochastic Algorithm for Separation in Heterogeneous\n  Self-Organizing Particle Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.ET math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and rigorously analyze the behavior of a distributed, stochastic\nalgorithm for separation and integration in self-organizing particle systems,\nan abstraction of programmable matter. Such systems are composed of individual\ncomputational particles with limited memory, strictly local communication\nabilities, and modest computational power. We consider heterogeneous particle\nsystems of two different colors and prove that these systems can collectively\nseparate into different color classes or integrate, indifferent to color. We\naccomplish both behaviors with the same fully distributed, local, stochastic\nalgorithm. Achieving separation or integration depends only on a single global\nparameter determining whether particles prefer to be next to other particles of\nthe same color or not; this parameter is meant to represent external,\nenvironmental influences on the particle system. The algorithm is a\ngeneralization of a previous distributed, stochastic algorithm for compression\n(PODC '16), which can be viewed as a special case of separation where all\nparticles have the same color. It is significantly more challenging to prove\nthat the desired behavior is achieved in the heterogeneous setting, however,\neven in the bichromatic case we focus on. This requires combining several new\ntechniques, including the cluster expansion from statistical physics, a new\nvariant of the bridging argument of Miracle, Pascoe and Randall (RANDOM '11),\nthe high-temperature expansion of the Ising model, and careful probabilistic\narguments.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 21:46:59 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 04:41:19 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Cannon", "Sarah", ""], ["Daymude", "Joshua J.", ""], ["Gokmen", "Cem", ""], ["Randall", "Dana", ""], ["Richa", "Andr\u00e9a W.", ""]]}, {"id": "1805.04657", "submitter": "Magnus Westerlund", "authors": "Magnus Westerlund and Nane Kratzke", "title": "Towards Distributed Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This review focuses on the evolution of cloud computing and distributed\nledger technologies (blockchains) over the last decade. Cloud computing relies\nmainly on a conceptually centralized service provisioning model, while\nblockchain technologies originate from a peer-to-peer and a completely\ndistributed approach. Still, noteworthy commonalities between both approaches\nare often overlooked by researchers. Therefore, to the best of the authors\nknowledge, this paper reviews both domains in parallel for the first time. We\nconclude that both approaches have advantages and disadvantages. The advantages\nof centralized service provisioning approaches are often the disadvantages of\ndistributed ledger approaches and vice versa. It is obviously an interesting\nquestion whether both approaches could be combined in a way that the advantages\ncan be added while the disadvantages could be avoided. We derive a software\nstack that could build the foundation unifying the best of these two worlds and\nthat would avoid existing shortcomings like vendor lock-in, some security\nproblems, and inherent platform dependencies.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 05:22:06 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Westerlund", "Magnus", ""], ["Kratzke", "Nane", ""]]}, {"id": "1805.04764", "submitter": "Jason Li", "authors": "Mohsen Ghaffari, Jason Li", "title": "New Distributed Algorithms in Almost Mixing Time via Transformations\n  from Parallel Algorithms", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that many classical optimization problems --- such as\n$(1\\pm\\epsilon)$-approximate maximum flow, shortest path, and transshipment ---\ncan be computed in $\\newcommand{\\tmix}{{\\tau_{\\text{mix}}}}\\tmix(G)\\cdot\nn^{o(1)}$ rounds of distributed message passing, where $\\tmix(G)$ is the mixing\ntime of the network graph $G$. This extends the result of Ghaffari et al.\\\n[PODC'17], whose main result is a distributed MST algorithm in $\\tmix(G)\\cdot\n2^{O(\\sqrt{\\log n \\log\\log n})}$ rounds in the CONGEST model, to a much wider\nclass of optimization problems. For many practical networks of interest, e.g.,\npeer-to-peer or overlay network structures, the mixing time $\\tmix(G)$ is\nsmall, e.g., polylogarithmic. On these networks, our algorithms bypass the\n$\\tilde\\Omega(\\sqrt n+D)$ lower bound of Das Sarma et al.\\ [STOC'11], which\napplies for worst-case graphs and applies to all of the above optimization\nproblems. For all of the problems except MST, this is the first distributed\nalgorithm which takes $o(\\sqrt n)$ rounds on a (nontrivial) restricted class of\nnetwork graphs.\n  Towards deriving these improved distributed algorithms, our main contribution\nis a general transformation that simulates any work-efficient PRAM algorithm\nrunning in $T$ parallel rounds via a distributed algorithm running in $T\\cdot\n\\tmix(G)\\cdot 2^{O(\\sqrt{\\log n})}$ rounds. Work- and time-efficient parallel\nalgorithms for all of the aforementioned problems follow by combining the work\nof Sherman [FOCS'13, SODA'17] and Peng and Spielman [STOC'14]. Thus, simulating\nthese parallel algorithms using our transformation framework produces the\ndesired distributed algorithms.\n  The core technical component of our transformation is the algorithmic problem\nof solving \\emph{multi-commodity routing}---that is, roughly, routing $n$\npackets each from a given source to a given destination---in random graphs. For\nthis problem, we obtain a...\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 18:58:10 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Li", "Jason", ""]]}, {"id": "1805.04776", "submitter": "Sebastian Brandt", "authors": "Alkida Balliu, Sebastian Brandt, Dennis Olivetti, Jukka Suomela", "title": "Almost Global Problems in the LOCAL Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The landscape of the distributed time complexity is nowadays well-understood\nfor subpolynomial complexities. When we look at deterministic algorithms in the\nLOCAL model and locally checkable problems (LCLs) in bounded-degree graphs, the\nfollowing picture emerges:\n  - There are lots of problems with time complexities of $\\Theta(\\log^* n)$ or\n$\\Theta(\\log n)$.\n  - It is not possible to have a problem with complexity between $\\omega(\\log^*\nn)$ and $o(\\log n)$.\n  - In general graphs, we can construct LCL problems with infinitely many\ncomplexities between $\\omega(\\log n)$ and $n^{o(1)}$.\n  - In trees, problems with such complexities do not exist.\n  However, the high end of the complexity spectrum was left open by prior work.\nIn general graphs there are LCL problems with complexities of the form\n$\\Theta(n^\\alpha)$ for any rational $0 < \\alpha \\le 1/2$, while for trees only\ncomplexities of the form $\\Theta(n^{1/k})$ are known. No LCL problem with\ncomplexity between $\\omega(\\sqrt{n})$ and $o(n)$ is known, and neither are\nthere results that would show that such problems do not exist. We show that:\n  - In general graphs, we can construct LCL problems with infinitely many\ncomplexities between $\\omega(\\sqrt{n})$ and $o(n)$.\n  - In trees, problems with such complexities do not exist.\n  Put otherwise, we show that any LCL with a complexity $o(n)$ can be solved in\ntime $O(\\sqrt{n})$ in trees, while the same is not true in general graphs.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 20:25:59 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 15:58:54 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 15:48:11 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Olivetti", "Dennis", ""], ["Suomela", "Jukka", ""]]}, {"id": "1805.04778", "submitter": "Assaf Yifrach", "authors": "Assaf Yifrach, Yishay Mansour", "title": "Fair Leader Election for Rational Agents in Asynchronous Rings and\n  Networks", "comments": "48 pages, PODC 2018", "journal-ref": null, "doi": "10.1145/3212734.3212767", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a game theoretic model where a coalition of processors might collude\nto bias the outcome of the protocol, where we assume that the processors always\nprefer any legitimate outcome over a non-legitimate one. We show that the\nproblems of Fair Leader Election and Fair Coin Toss are equivalent, and focus\non Fair Leader Election.\n  Our main focus is on a directed asynchronous ring of $n$ processors, where we\ninvestigate the protocol proposed by Abraham et al.\n\\cite{abraham2013distributed} and studied in Afek et al.\n\\cite{afek2014distributed}. We show that in general the protocol is resilient\nonly to sub-linear size coalitions. Specifically, we show that\n$\\Omega(\\sqrt{n\\log n})$ randomly located processors or $\\Omega(\\sqrt[3]{n})$\nadversarially located processors can force any outcome. We complement this by\nshowing that the protocol is resilient to any adversarial coalition of size\n$O(\\sqrt[4]{n})$.\n  We propose a modification to the protocol, and show that it is resilient to\nevery coalition of size $\\Theta(\\sqrt{n})$, by exhibiting both an attack and a\nresilience result. For every $k \\geq 1$, we define a family of graphs\n${\\mathcal{G}}_{k}$ that can be simulated by trees where each node in the tree\nsimulates at most $k$ processors. We show that for every graph in\n${\\mathcal{G}}_{k}$, there is no fair leader election protocol that is\nresilient to coalitions of size $k$. Our result generalizes a previous result\nof Abraham et al. \\cite{abraham2013distributed} that states that for every\ngraph, there is no fair leader election protocol which is resilient to\ncoalitions of size $\\lceil \\frac{n}{2} \\rceil$.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 20:40:21 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 13:39:35 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yifrach", "Assaf", ""], ["Mansour", "Yishay", ""]]}, {"id": "1805.04779", "submitter": "Panagiota Fatourou", "authors": "Panagiota Fatourou, Eric Ruppert", "title": "Persistent Non-Blocking Binary Search Trees Supporting Wait-Free Range\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": "FORTH ICS TR 470, May 2018", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the first implementation of a search tree data structure\nin an asynchronous shared-memory system that provides a wait-free algorithm for\nexecuting range queries on the tree, in addition to non-blocking algorithms for\nInsert, Delete and Find, using single-word Compare-and-Swap (CAS). The\nimplementation is linearizable and tolerates any number of crash failures.\nInsert and Delete operations that operate on different parts of the tree run\nfully in parallel (without any interference with one another). We employ a\nlightweight helping mechanism, where each Insert, Delete and Find operation\nhelps only update operations that affect the local neighbourhood of the leaf it\narrives at. Similarly, a Scan helps only those updates taking place on nodes of\nthe part of the tree it traverses, and therefore Scans operating on different\nparts of the tree do not interfere with one another. Our implementation works\nin a dynamic system where the number of processes may change over time.\n  The implementation builds upon the non-blocking binary search tree\nimplementation presented by Ellen et al. (in PODC 2010) by applying a simple\nmechanism to make the tree persistent.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 20:58:12 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Fatourou", "Panagiota", ""], ["Ruppert", "Eric", ""]]}, {"id": "1805.04781", "submitter": "Andrea Zonca", "authors": "Andrea Zonca, Robert S. Sinkovits", "title": "Deploying Jupyter Notebooks at scale on XSEDE resources for Science\n  Gateways and workshops", "comments": "7 pages, 3 figures, PEARC '18: Practice and Experience in Advanced\n  Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": null, "doi": "10.1145/3219104.3219122", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jupyter Notebooks have become a mainstream tool for interactive computing in\nevery field of science. Jupyter Notebooks are suitable as companion\napplications for Science Gateways, providing more flexibility and\npost-processing capability to the users. Moreover they are often used in\ntraining events and workshops to provide immediate access to a pre-configured\ninteractive computing environment. The Jupyter team released the JupyterHub web\napplication to provide a platform where multiple users can login and access a\nJupyter Notebook environment. When the number of users and memory requirements\nare low, it is easy to setup JupyterHub on a single server. However, setup\nbecomes more complicated when we need to serve Jupyter Notebooks at scale to\ntens or hundreds of users. In this paper we will present three strategies for\ndeploying JupyterHub at scale on XSEDE resources. All options share the\ndeployment of JupyterHub on a Virtual Machine on XSEDE Jetstream. In the first\nscenario, JupyterHub connects to a supercomputer and launches a single node job\non behalf of each user and proxies back the Notebook from the computing node\nback to the user's browser. In the second scenario, implemented in the context\nof a XSEDE consultation for the IRIS consortium for Seismology, we deploy\nDocker in Swarm mode to coordinate many XSEDE Jetstream virtual machines to\nprovide Notebooks with persistent storage and quota. In the last scenario we\ninstall the Kubernetes containers orchestration framework on Jetstream to\nprovide a fault-tolerant JupyterHub deployment with a distributed filesystem\nand capability to scale to thousands of users. In the conclusion section we\nprovide a link to step-by-step tutorials complete with all the necessary\ncommands and configuration files to replicate these deployments.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 21:03:37 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 13:29:51 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 10:51:43 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zonca", "Andrea", ""], ["Sinkovits", "Robert S.", ""]]}, {"id": "1805.04819", "submitter": "Shreyas Gokhale", "authors": "Shreyas Gokhale, Neeraj Mittal", "title": "Fast and Scalable Group Mutual Exclusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The group mutual exclusion (GME) problem is a generalization of the classical\nmutual exclusion problem in which every critical section is associated with a\ntype or session. Critical sections belonging to the same session can execute\nconcurrently, whereas critical sections belonging to different sessions must be\nexecuted serially. The well-known read-write mutual exclusion problem is a\nspecial case of the group mutual exclusion problem. In this work, we present a\nnovel GME algorithm for an asynchronous shared-memory system that, in addition\nto satisfying lockout freedom, bounded exit and concurrent entering properties,\nhas O(1) step-complexity when the system contains no conflicting requests as\nwell as O(1) space-complexity per GME object when the system contains\nsufficient number of GME objects. To the best of our knowledge, no existing GME\nalgorithm has O(1) step-complexity for concurrent entering. Moreover, most\nexisting GME algorithms have {\\Omega}(n) space complexity per GME object, where\nn denotes the number of processes in the system. We also show that our GME\nalgorithm can be easily modified to use bounded space variables.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 04:48:42 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 19:59:21 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 17:53:49 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Gokhale", "Shreyas", ""], ["Mittal", "Neeraj", ""]]}, {"id": "1805.04832", "submitter": "David Doty", "authors": "David Doty, Mahsa Eftekhari, Othon Michail, Paul G. Spirakis, and\n  Michail Theofilatos", "title": "Exact size counting in uniform population protocols in nearly\n  logarithmic time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study population protocols: networks of anonymous agents that interact\nunder a scheduler that picks pairs of agents uniformly at random. The _size\ncounting problem_ is that of calculating the exact number $n$ of agents in the\npopulation, assuming no leader (each agent starts in the same state). We give\nthe first protocol that solves this problem in sublinear time. The protocol\nconverges in $O(\\log n \\log \\log n)$ time and uses $O(n^{60})$ states ($O(1) +\n60 \\log n$ bits of memory per agent) with probability $1-O(\\frac{\\log \\log\nn}{n})$. The time complexity is also $O(\\log n \\log \\log n)$ in expectation.\nThe time to converge is also $O(\\log n \\log \\log n)$ in expectation. Crucially,\nunlike most published protocols with $\\omega(1)$ states, our protocol is\n_uniform_: it uses the same transition algorithm for any population size, so\ndoes not need an estimate of the population size to be embedded into the\nalgorithm. A sub-protocol is the first uniform sublinear-time leader election\npopulation protocol, taking $O(\\log n \\log \\log n)$ time and $O(n^{18})$\nstates. The state complexity of both the counting and leader election protocols\ncan be reduced to $O(n^{30})$ and $O(n^{9})$ respectively, while increasing the\ntime to $O(\\log^2 n)$.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 06:55:51 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Doty", "David", ""], ["Eftekhari", "Mahsa", ""], ["Michail", "Othon", ""], ["Spirakis", "Paul G.", ""], ["Theofilatos", "Michail", ""]]}, {"id": "1805.04838", "submitter": "Peter Davies", "authors": "Artur Czumaj, Peter Davies", "title": "Deterministic Blind Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ad-hoc radio networks and multiple access channels are classical and\nwell-studied models of distributed systems, with a large body of literature on\ndeterministic algorithms for fundamental communications primitives such as\nbroadcasting and wake-up. However, almost all of these algorithms assume\nknowledge of the number of participating nodes and the range of possible IDs,\nand often make the further assumption that the latter is linear in the former.\nThese are very strong assumptions for models which were designed to capture\nnetworks of weak devices organized in an ad-hoc manner. It was believed that\nwithout this knowledge, deterministic algorithms must necessarily be much less\nefficient.\n  In this paper we address this fundamental question and show that this is not\nthe case. We present \\emph{deterministic} algorithms for \\emph{blind} networks\n(in which nodes know only their own IDs), which match or nearly match the\nrunning times of the fastest algorithms which assume network knowledge (and\neven surpass the previous fastest algorithms which assume parameter knowledge\nbut not small labels).\n  Specifically, in multiple access channels with $k$ participating nodes and\nIDs up to $L$, we give a wake-up algorithm requiring $O(\\frac{k\\log L \\log k\n}{\\log\\log k})$ time, improving dramatically over the $O(L^3 \\log^3 L)$ time\nalgorithm of De Marco et al. (2007), and a broadcasting algorithm requiring\n\\sloppy{$O(k\\log L \\log\\log k)$ }time, improving over the $O(L)$ time algorithm\nof Gasieniec et al. (2001) in most circumstances. Furthermore, we show how\nthese same algorithms apply directly to multi-hop radio networks, achieving\neven larger running time improvements.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 08:08:24 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""]]}, {"id": "1805.04840", "submitter": "Aryaz Eghbali", "authors": "Aryaz Eghbali and Philipp Woelfel", "title": "An Almost Tight RMR Lower Bound for Abortable Test-And-Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a lower bound of Omega(log n/loglog n) for the remote memory\nreference (RMR) complexity of abortable test-and-set (leader election) in the\ncache-coherent (CC) and the distributed shared memory (DSM) model.\n  This separates the complexities of abortable and non-abortable test-and-set,\nas the latter has constant RMR complexity (Golab, Hendler, Woelfel, SIAM\nJournal of Computing Vol. 39, 2010).\n  Golab, Hendler, Hadzilacos and Woelfel (Distributed Computing Vol. 25, 2012)\nshowed that compare-and-swap can be implemented from registers and TAS objects\nwith constant RMR complexity.\n  We observe that a small modification to that implementation is abortable,\nprovided that the used TAS objects are abortable.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 08:13:05 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Eghbali", "Aryaz", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1805.04842", "submitter": "Peter Davies", "authors": "Artur Czumaj, Peter Davies", "title": "Randomized Communication Without Network Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio networks are a long-studied model for distributed system of devices\nwhich communicate wirelessly. When these devices are mobile or have limited\ncapabilities, the system is often best modeled by the ad-hoc variant, in which\nthe devices do not know the structure of the network. A large body of work has\nbeen devoted to designing algorithms for the ad-hoc model, particularly for\nfundamental communications tasks such as broadcasting. Most of these\nalgorithms, however, assume that devices have some network knowledge (usually\nbounds on the number of nodes in the network $n$, and the diameter $D$), which\nmay not always be realistic in systems with weak devices or gradual deployment.\nVery little is known about what can be done when this information is not\navailable.\n  This is the issue we address in this work, by presenting the first\n\\emph{randomized} broadcasting algorithms for \\emph{blind} networks in which\nnodes have no prior knowledge whatsoever. We demonstrate that lack of parameter\nknowledge can be overcome at only a small increase in running time.\nSpecifically, we show that in networks without collision detection, broadcast\ncan be achieved in $O(D\\log\\frac nD\\log^2\\log\\frac nD + \\log^2 n)$ time, almost\nreaching the $\\Omega(D\\log\\frac nD + \\log^2 n)$ lower bound. We also give an\nalgorithm for directed networks with collision detection, which requires only\n$O(D\\log\\frac nD\\log\\log\\log\\frac nD + \\log^2 n)$ time.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 08:18:59 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""]]}, {"id": "1805.04864", "submitter": "Aikaterini Nikolidaki", "authors": "Aikaterini Nikolidaki", "title": "SPAIDS and OAMS Models in Wireless Ad Hoc Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two randomized distributed algorithms in wireless\nad hoc networks. We consider that the network is structured into pairs of nodes\n(sender, receiver) in a decay space. We take into account the following: Each\nnode has its own power assignment and the distance between them does not follow\nthe symmetry property. Then, we consider a non-uniform network or a realistic\nwireless network, which is beyond the geometry. Our model is based on the\nSignal to Interference plus Noise Ratio (SINR) model. In this work, the main\nproblem is to solve the scheduling task aiming the successful transmission of\nmessages in a realistic environment. Therefore, we propose the first randomized\nscheduling and power selection algorithm in a decay space and is called as\nSPAIDS. In order to solve the problem in this non-uniform network, we introduce\na new way to study the affectance (the interference) among the links, which is\ndefined as Weighted Average Affectance (WAFF). Moreover, we study the online\nbroadcast problem in a metric space, in which the nodes are activated in case\nthat they receive packets. We propose an online algorithm in a metric space\nwhich is denoted as OAMS. Our aim is to obtain the maximum subset of nodes that\nreceive the message from a sender node with enough energy supplies. Finally, we\ncompare the performance of OAMS to the optimal.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 11:33:38 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 18:09:16 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Nikolidaki", "Aikaterini", ""]]}, {"id": "1805.04886", "submitter": "Nikolay Malitsky", "authors": "Nikolay Malitsky, Aashish Chaudhary, Sebastien Jourdain, Matt Cowan,\n  Patrick O'Leary, Marcus Hanwell, and Kerstin Kleese Van Dam", "title": "Building Near-Real-Time Processing Pipelines with the Spark-MPI Platform", "comments": "New York Scientific Data Summit, August 6-9, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in detectors and computational technologies provide new\nopportunities for applied research and the fundamental sciences. Concurrently,\ndramatic increases in the three Vs (Volume, Velocity, and Variety) of\nexperimental data and the scale of computational tasks produced the demand for\nnew real-time processing systems at experimental facilities. Recently, this\ndemand was addressed by the Spark-MPI approach connecting the Spark\ndata-intensive platform with the MPI high-performance framework. In contrast\nwith existing data management and analytics systems, Spark introduced a new\nmiddleware based on resilient distributed datasets (RDDs), which decoupled\nvarious data sources from high-level processing algorithms. The RDD middleware\nsignificantly advanced the scope of data-intensive applications, spreading from\nSQL queries to machine learning to graph processing. Spark-MPI further extended\nthe Spark ecosystem with the MPI applications using the Process Management\nInterface. The paper explores this integrated platform within the context of\nonline ptychographic and tomographic reconstruction pipelines.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 13:49:39 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Malitsky", "Nikolay", ""], ["Chaudhary", "Aashish", ""], ["Jourdain", "Sebastien", ""], ["Cowan", "Matt", ""], ["O'Leary", "Patrick", ""], ["Hanwell", "Marcus", ""], ["Van Dam", "Kerstin Kleese", ""]]}, {"id": "1805.04923", "submitter": "Thomas Nowak", "authors": "Matthias F\\\"ugger and Thomas Nowak", "title": "Fast Multidimensional Asymptotic and Approximate Consensus", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problems of asymptotic and approximate consensus in which agents\nhave to get their values arbitrarily close to each others' inside the convex\nhull of initial values, either without or with an explicit decision by the\nagents. In particular, we are concerned with the case of multidimensional data,\ni.e., the agents' values are $d$-dimensional vectors. We introduce two new\nalgorithms for dynamic networks, subsuming classical failure models like\nasynchronous message passing systems with Byzantine agents. The algorithms are\nthe first to have a contraction rate and time complexity independent of the\ndimension $d$. In particular, we improve the time complexity from the\npreviously fastest approximate consensus algorithm in asynchronous message\npassing systems with Byzantine faults by Mendes et al. [Distrib. Comput. 28]\nfrom $\\Omega\\!\\left( d \\log\\frac{d\\Delta}{\\varepsilon} \\right)$ to $O\\!\\left(\n\\log\\frac{\\Delta}{\\varepsilon} \\right)$, where $\\Delta$ is the initial and\n$\\varepsilon$ is the terminal diameter of the set of vectors of correct agents.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 18:20:51 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["F\u00fcgger", "Matthias", ""], ["Nowak", "Thomas", ""]]}, {"id": "1805.05004", "submitter": "Guillaume Jourjon", "authors": "Parinya Ekparinya, Vincent Gramoli, and Guillaume Jourjon", "title": "Double-Spending Risk Quantification in Private, Consortium and Public\n  Ethereum Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several works conjectured the vulnerabilities of mainstream\nblockchains under several network attacks. All these attacks translate into\nshowing that the assumptions of these blockchains can be violated in theory or\nunder simulation at best. Unfortunately, previous results typically omit both\nthe nature of the network under which the blockchain code runs and whether\nblockchains are private, consortium or public. In this paper, we study the\npublic Ethereum blockchain as well as a consortium and private blockchains and\nquantify the feasibility of man-in-the-middle and double spending attacks\nagainst them. To this end, we list important properties of the Ethereum public\nblockchain topology, we deploy VMs with constrained CPU quantum to mimic the\ntop-10 mining pools of Ethereum and we develop full-fledged attacks, that first\npartition the network through BGP hijacking or ARP spoofing before issuing a\nBalance Attack to steal coins. Our results demonstrate that attacking Ethereum\nis remarkably devastating in a consortium or private context as the adversary\ncan multiply her digital assets by 200, 000x in 10 hours through BGP hijacking\nwhereas it would be almost impossible in a public context.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 03:53:25 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Ekparinya", "Parinya", ""], ["Gramoli", "Vincent", ""], ["Jourjon", "Guillaume", ""]]}, {"id": "1805.05137", "submitter": "Marjorie Bournat", "authors": "Marjorie Bournat (DELYS), Swan Dubois (DELYS), Franck Petit (DELYS)", "title": "Gracefully Degrading Gathering in Dynamic Rings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gracefully degrading algorithms [Biely \\etal, TCS 2018] are designed to\ncircumvent impossibility results in dynamic systems by adapting themselves to\nthe dynamics. Indeed, such an algorithm solves a given problem under some\ndynamics and, moreover, guarantees that a weaker (but related) problem is\nsolved under a higher dynamics under which the original problem is impossible\nto solve. The underlying intuition is to solve the problem whenever possible\nbut to provide some kind of quality of service if the dynamics become\n(unpredictably) higher.In this paper, we apply for the first time this approach\nto robot networks. We focus on the fundamental problem of gathering a squad of\nautonomous robots on an unknown location of a dynamic ring. In this goal, we\nintroduce a set of weaker variants of this problem. Motivated by a set of\nimpossibility results related to the dynamics of the ring, we propose a\ngracefully degrading gathering algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 12:17:17 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 09:06:16 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Bournat", "Marjorie", "", "DELYS"], ["Dubois", "Swan", "", "DELYS"], ["Petit", "Franck", "", "DELYS"]]}, {"id": "1805.05152", "submitter": "Eduardo Alchieri", "authors": "Eduardo Alchieri and Fernando Dotti and Fernando Pedone", "title": "Early Scheduling in Parallel State Machine Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State machine replication is standard approach to fault tolerance. One of the\nkey assumptions of state machine replication is that replicas must execute\noperations deterministically and thus serially. To benefit from multi-core\nservers, some techniques allow concurrent execution of operations in state\nmachine replication. Invariably, these techniques exploit the fact that\nindependent operations (those that do not share any common state or do not\nupdate shared state) can execute concurrently. A promising category of\nsolutions trades scheduling freedom for simplicity. This paper generalizes this\ncategory of scheduling solutions. In doing so, it proposes an automated\nmechanism to schedule operations on worker threads at replicas. We integrate\nour contributions to a popular state machine replication framework and\nexperimentally compare the resulting system to more classic approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 13:01:49 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Alchieri", "Eduardo", ""], ["Dotti", "Fernando", ""], ["Pedone", "Fernando", ""]]}, {"id": "1805.05157", "submitter": "Tomasz Radzik", "authors": "Petra Berenbrink, Robert Els\\\"asser, Tom Friedetzky, Dominik Kaaser,\n  Peter Kling, Tomasz Radzik", "title": "A population protocol for exact majority with $O(\\log^{5/3} n)$\n  stabilization time and asymptotically optimal number of states", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A population protocol can be viewed as a sequence of pairwise interactions of\n$n$ agents (nodes). During one interaction, two agents selected uniformly at\nrandom update their states by applying a specified deterministic transition\nfunction. In a long run, the whole system should stabilize at the correct\noutput property. The main performance objectives in designing population\nprotocols are small number of states per agent and fast stabilization time.\n  We present a fast population protocol for the exact-majority problem which\nuses $\\Theta(\\log n)$ states (per agent) and stabilizes in $O(\\log^{5/3} n)$\nparallel time (i.e., $O(n\\log^{5/3} n)$ interactions) in expectation and with\nhigh probability. Alistarh et al. [SODA 2018] showed that any exact-majority\nprotocol which stabilizes in expected $O(n^{1-\\epsilon})$ parallel time, for\nany constant $\\epsilon > 0$, requires $\\Omega(\\log n)$ states. They also showed\nan $O(\\log^2 n)$-time protocol with $O(\\log n)$ states, the currently fastest\nexact-majority protocol with polylogarithmic number of states. The standard\ndesign framework for majority protocols is based on $O(\\log n)$ phases and\nrequires that all nodes are well synchronized within each phase, leading\nnaturally to upper bounds of the order of at least $\\log^2 n$ because of\n$\\Theta(\\log n)$ synchronization time per phase. We show how this framework can\nbe tightened with {\\em weak synchronization} to break the $O(\\log^2 n)$ upper\nbound of previous protocols.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 13:16:51 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 22:58:35 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Berenbrink", "Petra", ""], ["Els\u00e4sser", "Robert", ""], ["Friedetzky", "Tom", ""], ["Kaaser", "Dominik", ""], ["Kling", "Peter", ""], ["Radzik", "Tomasz", ""]]}, {"id": "1805.05197", "submitter": "Yun Zeng", "authors": "Yun Zeng, Jian Tan, Cathy H. Xia", "title": "Fork and Join Queueing Networks with Heavy Tails: Scaling Dimension and\n  Throughput Limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel and distributed computing systems are foundational to the success of\ncloud computing and big data analytics. These systems process computational\nworkflows in a way that can be mathematically modeled by a fork-and-join\nqueueing network with blocking (FJQN/B). While engineering solutions have long\nbeen made to build and scale such systems, it is challenging to rigorously\ncharacterize their throughput performance at scale theoretically. What further\ncomplicates the study is the presence of heavy-tailed delays that have been\nwidely documented therein. To this end, we introduce two fundamental concepts\nfor networks of arbitrary topology (scaling dimension and extended metric\ndimension) and utilize an infinite sequence of growing FJQN/Bs to study the\nthroughput limit. The throughput is said to be scalable if the throughput limit\ninfimum of the sequence is strictly positive as the network size grows to\ninfinity. We investigate throughput scalability by focusing on heavy-tailed\nservice times that are regularly varying (with index $\\alpha>1$) and featuring\nthe network topology described by the two aforementioned dimensions. In\nparticular, we show that an infinite sequence of FJQN/Bs is throughput scalable\nif the extended metric dimension $<\\alpha-1$ and only if the scaling dimension\n$\\le\\alpha-1$. These theoretical results provide new insights on the\nscalability of a rich class of FJQN/Bs with various structures, including\ntandem, lattice, hexagon, pyramid, tree, and fractals.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 14:37:51 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zeng", "Yun", ""], ["Tan", "Jian", ""], ["Xia", "Cathy H.", ""]]}, {"id": "1805.05201", "submitter": "Brice N\\'edelec", "authors": "Brice N\\'edelec, Pascal Molli, Achour Most\\'efaoui", "title": "Breaking the Scalability Barrier of Causal Broadcast for Large and\n  Dynamic Systems", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed protocols and applications rely on causal broadcast to\nensure consistency criteria. However, none of causality tracking\nstate-of-the-art approaches scale in large and dynamic systems. This paper\npresents a new non-blocking causal broadcast protocol suited for dynamic\nsystems. The proposed protocol outperforms state-of-the-art in size of\nmessages, execution time complexity, and local space complexity. Most\nimportantly, messages piggyback control information the size of which is\nconstant. We prove that for both static and dynamic systems. Consequently,\nlarge and dynamic systems can finally afford causal broadcast.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 08:28:11 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["N\u00e9delec", "Brice", ""], ["Molli", "Pascal", ""], ["Most\u00e9faoui", "Achour", ""]]}, {"id": "1805.05208", "submitter": "Laxman Dhulipala", "authors": "Laxman Dhulipala, Guy E. Blelloch, Julian Shun", "title": "Theoretically Efficient Parallel Graph Algorithms Can Be Fast and\n  Scalable", "comments": "This is the full version of the paper appearing in the ACM Symposium\n  on Parallelism in Algorithms and Architectures (SPAA), 2018", "journal-ref": null, "doi": "10.1145/3210377.3210414", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent interest in parallel graph processing due\nto the need to quickly analyze the large graphs available today. Many graph\ncodes have been designed for distributed memory or external memory. However,\ntoday even the largest publicly-available real-world graph (the Hyperlink Web\ngraph with over 3.5 billion vertices and 128 billion edges) can fit in the\nmemory of a single commodity multicore server. Nevertheless, most experimental\nwork in the literature report results on much smaller graphs, and the ones for\nthe Hyperlink graph use distributed or external memory. Therefore, it is\nnatural to ask whether we can efficiently solve a broad class of graph problems\non this graph in memory.\n  This paper shows that theoretically-efficient parallel graph algorithms can\nscale to the largest publicly-available graphs using a single machine with a\nterabyte of RAM, processing them in minutes. We give implementations of\ntheoretically-efficient parallel algorithms for 20 important graph problems. We\nalso present the optimizations and techniques that we used in our\nimplementations, which were crucial in enabling us to process these large\ngraphs quickly. We show that the running times of our implementations\noutperform existing state-of-the-art implementations on the largest real-world\ngraphs. For many of the problems that we consider, this is the first time they\nhave been solved on graphs at this scale. We have made the implementations\ndeveloped in this work publicly-available as the Graph-Based Benchmark Suite\n(GBBS).\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 14:58:56 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 19:18:55 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 01:57:24 GMT"}, {"version": "v4", "created": "Wed, 21 Aug 2019 03:40:34 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Blelloch", "Guy E.", ""], ["Shun", "Julian", ""]]}, {"id": "1805.05278", "submitter": "Grey Ballard", "authors": "Grey Ballard, James Demmel, Laura Grigori, Mathias Jacquelin, Nicholas\n  Knight", "title": "A 3D Parallel Algorithm for QR Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interprocessor communication often dominates the runtime of large matrix\ncomputations. We present a parallel algorithm for computing QR decompositions\nwhose bandwidth cost (communication volume) can be decreased at the cost of\nincreasing its latency cost (number of messages). By varying a parameter to\nnavigate the bandwidth/latency tradeoff, we can tune this algorithm for\nmachines with different communication costs.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 16:45:16 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Ballard", "Grey", ""], ["Demmel", "James", ""], ["Grigori", "Laura", ""], ["Jacquelin", "Mathias", ""], ["Knight", "Nicholas", ""]]}, {"id": "1805.05580", "submitter": "Charles McGuffey", "authors": "Guy E. Blelloch, Phillip B. Gibbons, Yan Gu, Charles McGuffey, Julian\n  Shun", "title": "The Parallel Persistent Memory Model", "comments": "This paper is the full version of a paper at SPAA 2018 with the same\n  name", "journal-ref": null, "doi": "10.1145/3210377.3210381", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a parallel computational model that consists of $P$ processors,\neach with a fast local ephemeral memory of limited size, and sharing a large\npersistent memory. The model allows for each processor to fault with bounded\nprobability, and possibly restart. On faulting all processor state and local\nephemeral memory are lost, but the persistent memory remains. This model is\nmotivated by upcoming non-volatile memories that are as fast as existing random\naccess memory, are accessible at the granularity of cache lines, and have the\ncapability of surviving power outages. It is further motivated by the\nobservation that in large parallel systems, failure of processors and their\ncaches is not unusual.\n  Within the model we develop a framework for developing locality efficient\nparallel algorithms that are resilient to failures. There are several\nchallenges, including the need to recover from failures, the desire to do this\nin an asynchronous setting (i.e., not blocking other processors when one\nfails), and the need for synchronization primitives that are robust to\nfailures. We describe approaches to solve these challenges based on breaking\ncomputations into what we call capsules, which have certain properties, and\ndeveloping a work-stealing scheduler that functions properly within the context\nof failures. The scheduler guarantees a time bound of $O(W/P_A + D(P/P_A)\n\\lceil\\log_{1/f} W\\rceil)$ in expectation, where $W$ and $D$ are the work and\ndepth of the computation (in the absence of failures), $P_A$ is the average\nnumber of processors available during the computation, and $f \\le 1/2$ is the\nprobability that a capsule fails. Within the model and using the proposed\nmethods, we develop efficient algorithms for parallel sorting and other\nprimitives.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 06:11:56 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 18:49:57 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Gibbons", "Phillip B.", ""], ["Gu", "Yan", ""], ["McGuffey", "Charles", ""], ["Shun", "Julian", ""]]}, {"id": "1805.05660", "submitter": "Yuval Emek", "authors": "Yehuda Afek, Yuval Emek, Noa Kolikant", "title": "Selecting a Leader in a Network of Finite State Machines", "comments": "To appear in DISC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a variant of the \\emph{leader election} problem under the\n\\emph{stone age} model (Emek and Wattenhofer, PODC 2013) that considers a\nnetwork of $n$ randomized finite automata with very weak communication\ncapabilities (a multi-frequency asynchronous generalization of the\n\\emph{beeping} model's communication scheme). Since solving the classic leader\nelection problem is impossible even in more powerful models, we consider a\nrelaxed variant, referred to as \\emph{$k$-leader selection}, in which a leader\nshould be selected out of at most $k$ initial candidates. Our main contribution\nis an algorithm that solves $k$-leader selection for bounded $k$ in the\naforementioned stone age model. On (general topology) graphs of diameter $D$,\nthis algorithm runs in $\\tilde{O}(D)$ time and succeeds with high probability.\nThe assumption that $k$ is bounded turns out to be unavoidable: we prove that\nif $k = \\omega (1)$, then no algorithm in this model can solve $k$-leader\nselection with a (positive) constant probability.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 09:28:37 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 20:01:47 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Afek", "Yehuda", ""], ["Emek", "Yuval", ""], ["Kolikant", "Noa", ""]]}, {"id": "1805.05674", "submitter": "Zhenyu Wen", "authors": "Zhenyu Wen, Do Le Quoc, Pramod Bhatotia, Ruichuan Chen and Myungjin\n  Lee", "title": "Approximate Edge Analytics for the IoT Ecosystem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT-enabled devices continue to generate a massive amount of data.\nTransforming this continuously arriving raw data into timely insights is\ncritical for many modern online services. For such settings, the traditional\nform of data analytics over the entire dataset would be prohibitively limiting\nand expensive for supporting real-time stream analytics. In this work, we make\na case for approximate computing for data analytics in IoT settings.\nApproximate computing aims for efficient execution of workflows where an\napproximate output is sufficient instead of the exact output. The idea behind\napproximate computing is to compute over a representative sample instead of the\nentire input dataset. Thus, approximate computing - based on the chosen sample\nsize - can make a systematic trade-off between the output accuracy and\ncomputation efficiency. This motivated the design of APPROXIOT - a data\nanalytics system for approximate computing in IoT. To realize this idea, we\ndesigned an online hierarchical stratified reservoir sampling algorithm that\nuses edge computing resources to produce approximate output with rigorous error\nbounds. To showcase the effectiveness of our algorithm, we implemented\nAPPROXIOT based on Apache Kafka and evaluated its effectiveness using a set of\nmicrobenchmarks and real-world case studies. Our results show that APPROXIOT\nachieves a speedup 1.3X-9.9X with varying sampling fraction of 80% to 10%\ncompared to simple random sampling.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 09:56:41 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Wen", "Zhenyu", ""], ["Quoc", "Do Le", ""], ["Bhatotia", "Pramod", ""], ["Chen", "Ruichuan", ""], ["Lee", "Myungjin", ""]]}, {"id": "1805.05787", "submitter": "Wei Quan Lim", "authors": "Kunal Agrawal, Seth Gilbert, Wei Quan Lim", "title": "Parallel Working-Set Search Structures", "comments": "Authors' version of a paper accepted to SPAA 2018", "journal-ref": null, "doi": "10.1145/3210377.3210390", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two versions of a parallel working-set map on p\nprocessors that supports searches, insertions and deletions. In both versions,\nthe total work of all operations when the map has size at least p is bounded by\nthe working-set bound, i.e., the cost of an item depends on how recently it was\naccessed (for some linearization): accessing an item in the map with recency r\ntakes O(1+log r) work. In the simpler version each map operation has O((log\np)^2+log n) span (where n is the maximum size of the map). In the pipelined\nversion each map operation on an item with recency r has O((log p)^2+log r)\nspan. (Operations in parallel may have overlapping span; span is additive only\nfor operations in sequence.)\n  Both data structures are designed to be used by a dynamic multithreading\nparallel program that at each step executes a unit-time instruction or makes a\ndata structure call. To achieve the stated bounds, the pipelined data structure\nrequires a weak-priority scheduler, which supports a limited form of 2-level\nprioritization. At the end we explain how the results translate to practical\nimplementations using work-stealing schedulers.\n  To the best of our knowledge, this is the first parallel implementation of a\nself-adjusting search structure where the cost of an operation adapts to the\naccess sequence. A corollary of the working-set bound is that it achieves work\nstatic optimality: the total work is bounded by the access costs in an optimal\nstatic search tree.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 14:14:20 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 17:17:06 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 11:27:04 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Agrawal", "Kunal", ""], ["Gilbert", "Seth", ""], ["Lim", "Wei Quan", ""]]}, {"id": "1805.05836", "submitter": "Vahid Maleki Raee", "authors": "Vahid Maleki Raee, Diala Naboulsi, Roch Glitho", "title": "Energy Efficient Task Assignment in Virtualized Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Sensor Networks (WSNs) are being used extensively today in various\ndomains. However, they are traditionally deployed with applications embedded in\nthem which precludes their re-use for new applications. Nowadays,\nvirtualization enables several applications on a same WSN by abstracting the\nphysical resources (i.e. sensing capabilities) into logical ones. However, this\ncomes at a cost, including an energy cost. It is therefore critical to ensure\nthe efficient allocation of these resources. In this paper, we study the\nproblem of assigning application sensing tasks to sensor devices, in\nvirtualized WSNs. Our goal is to minimize the overall energy consumption\nresulting from the assignment. We focus on the static version of the problem\nand formulate it using Integer Linear Programming (ILP), while accounting for\nsensor nodes' available energy and virtualization overhead. We solve the\nproblem over different scenarios and compare the obtained solution to the case\nof a traditional WSN, i.e. one with no support for virtualization. Our results\nshow that significant energy can be saved when tasks are appropriately assigned\nin a WSN that supports virtualization.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 20:15:32 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Raee", "Vahid Maleki", ""], ["Naboulsi", "Diala", ""], ["Glitho", "Roch", ""]]}, {"id": "1805.05847", "submitter": "S\\'ebastien Vaucher", "authors": "S\\'ebastien Vaucher, Rafael Pires, Pascal Felber, Marcelo Pasin,\n  Valerio Schiavoni and Christof Fetzer", "title": "SGX-Aware Container Orchestration for Heterogeneous Clusters", "comments": "Presented in the 38th IEEE International Conference on Distributed\n  Computing Systems (ICDCS 2018)", "journal-ref": "2018 IEEE 38th International Conference on Distributed Computing\n  Systems (ICDCS), Vienna, Austria, 2018, pp. 730-741", "doi": "10.1109/ICDCS.2018.00076", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containers are becoming the de facto standard to package and deploy\napplications and micro-services in the cloud. Several cloud providers (e.g.,\nAmazon, Google, Microsoft) begin to offer native support on their\ninfrastructure by integrating container orchestration tools within their cloud\noffering. At the same time, the security guarantees that containers offer to\napplications remain questionable. Customers still need to trust their cloud\nprovider with respect to data and code integrity. The recent introduction by\nIntel of Software Guard Extensions (SGX) into the mass market offers an\nalternative to developers, who can now execute their code in a hardware-secured\nenvironment without trusting the cloud provider.\n  This paper provides insights regarding the support of SGX inside Kubernetes,\nan industry-standard container orchestrator. We present our contributions\nacross the whole stack supporting execution of SGX-enabled containers. We\nprovide details regarding the architecture of the scheduler and its monitoring\nframework, the underlying operating system support and the required kernel\ndriver extensions. We evaluate our complete implementation on a private cluster\nusing the real-world Google Borg traces. Our experiments highlight the\nperformance trade-offs that will be encountered when deploying SGX-enabled\nmicro-services in the cloud.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 15:31:15 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 13:13:19 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Vaucher", "S\u00e9bastien", ""], ["Pires", "Rafael", ""], ["Felber", "Pascal", ""], ["Pasin", "Marcelo", ""], ["Schiavoni", "Valerio", ""], ["Fetzer", "Christof", ""]]}, {"id": "1805.05874", "submitter": "Do Le Quoc", "authors": "Do Le Quoc, Istemi Ekin Akkus, Pramod Bhatotia, Spyros Blanas,\n  Ruichuan Chen, Christof Fetzer, Thorsten Strufe", "title": "Approximate Distributed Joins in Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The join operation is a fundamental building block of parallel data\nprocessing. Unfortunately, it is very resource-intensive to compute an\nequi-join across massive datasets. The approximate computing paradigm allows\nusers to trade accuracy and latency for expensive data processing operations.\nThe equi-join operator is thus a natural candidate for optimization using\napproximation techniques. Although sampling-based approaches are widely used\nfor approximation, sampling over joins is a compelling but challenging task\nregarding the output quality. Naive approaches, which perform joins over\ndataset samples, would not preserve statistical properties of the join output.\n  To realize this potential, we interweave Bloom filter sketching and\nstratified sampling with the join computation in a new operator, ApproxJoin,\nthat preserves the statistical properties of the join output. ApproxJoin\nleverages a Bloom filter to avoid shuffling non-joinable data items around the\nnetwork and then applies stratified sampling to obtain a representative sample\nof the join output.\n  Our analysis shows that ApproxJoin scales well and significantly reduces data\nmovement, without sacrificing tight error bounds on the accuracy of the final\nresults. We implemented ApproxJoin in Apache Spark and evaluated ApproxJoin\nusing microbenchmarks and real-world case studies. The evaluation shows that\nApproxJoin achieves a speedup of 6-9x over unmodified Spark-based joins with\nthe same sampling rate. Furthermore, the speedup is accompanied by a\nsignificant reduction in the shuffled data volume, which is 5-82x less than\nunmodified Spark-based joins.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 16:07:53 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Quoc", "Do Le", ""], ["Akkus", "Istemi Ekin", ""], ["Bhatotia", "Pramod", ""], ["Blanas", "Spyros", ""], ["Chen", "Ruichuan", ""], ["Fetzer", "Christof", ""], ["Strufe", "Thorsten", ""]]}, {"id": "1805.05988", "submitter": "EPTCS", "authors": "Fabrizio Genovese (Statebox), Jelle Herold (Statebox)", "title": "Executions in (Semi-)Integer Petri Nets are Compact Closed Categories", "comments": "In Proceedings QPL 2018, arXiv:1901.09476", "journal-ref": "EPTCS 287, 2019, pp. 127-144", "doi": "10.4204/EPTCS.287.7", "report-no": null, "categories": "math.CT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyse Petri nets where places are allowed to have a\nnegative number of tokens. For each net we build its correspondent category of\nexecutions, which is compact closed, and prove that this procedure is\nfunctorial. We moreover exhibit a procedure to recover the original net from\nits category of executions, show that it is again functorial, and that this\ngives rise to an adjoint pair. Finally, we use compact closeness to infer that\nallowing negative tokens in a Petri net makes the causal relations between\ntransition firings non-trivial, and we use this to model interesting phenomena\nin economics and computer science.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 18:37:05 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 10:02:13 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 05:36:51 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Genovese", "Fabrizio", "", "Statebox"], ["Herold", "Jelle", "", "Statebox"]]}, {"id": "1805.06124", "submitter": "Scott Field", "authors": "Harbir Antil, Dangxing Chen, Scott E. Field", "title": "A Note on QR-Based Model Reduction: Algorithm, Software, and\n  Gravitational Wave Applications", "comments": "22 pages, 4 figures. This preprint is an expanded, more technical\n  version of the manuscript to be published in IEEE's Computing in Science &\n  Engineering", "journal-ref": "Computing in Science & Engineering ( Volume: 20 , Issue: 4 ,\n  Jul./Aug. 2018 )", "doi": "10.1109/MCSE.2018.042781323", "report-no": null, "categories": "cs.DC gr-qc math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the proper orthogonal decomposition (POD) is optimal under certain\nnorms it's also expensive to compute. For large matrix sizes, it is well known\nthat the QR decomposition provides a tractable alternative. Under the\nassumption that it is rank--revealing QR (RRQR), the approximation error\nincurred is similar to the POD error and, furthermore, we show the existence of\nan RRQR with exactly same error estimate as POD. To numerically realize an RRQR\ndecomposition, we will discuss the (iterative) modified Gram Schmidt with\npivoting (MGS) and reduced basis method by employing a greedy strategy. We show\nthat these two, seemingly different approaches from linear algebra and\napproximation theory communities are in fact equivalent. Finally, we describe\nan MPI/OpenMP parallel code that implements one of the QR-based model reduction\nalgorithms we analyze. This code was developed with model reduction in mind,\nand includes functionality for tasks that go beyond what is required for\nstandard QR decompositions. We document the code's scalability and show it to\nbe capable of tackling large problems. In particular, we apply our code to a\nmodel reduction problem motivated by gravitational waves emitted from binary\nblack hole mergers and demonstrate excellent weak scalability on the\nsupercomputer Blue Waters up to 32,768 cores and for complex, dense matrices as\nlarge as 10,000-by-3,276,800 (about half a terabyte in size).\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 04:29:33 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Antil", "Harbir", ""], ["Chen", "Dangxing", ""], ["Field", "Scott E.", ""]]}, {"id": "1805.06125", "submitter": "Preethika Kasu", "authors": "Preethika Kasu, Taeuk Kim, Youngjae Kim, Jung-Ho Um, Kyongseok Park,\n  Scott Atchley", "title": "FT-LADS: Fault-Tolerant Object-Logging based Big Data Transfer System\n  using Layout-Aware Data Scheduling", "comments": "35 pages,10 figures", "journal-ref": "IEEE Access 7 (2019) 37448-37462", "doi": "10.1109/ACCESS.2019.2905158", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layout-Aware Data Scheduler (LADS) data transfer tool, identifies and\naddresses the issues that lead to congestion on the path of an end-to-end data\ntransfer in the terabit network environments. It exploits the underlying\nstorage layout at each endpoint to maximize throughput without negatively\nimpacting the performance of shared storage resources for other users. LADS can\navoid congested storage elements within the shared storage resource, improving\ninput/output bandwidth, and hence the data transfer rates across the high speed\nnetworks. However, absence of FT (fault tolerance) support in LADS results in\ndata re-transmission overhead along with the possible integrity issues upon\nerrors. In this paper, we propose object based logging methods to avoid\ntransmitting the objects which are successfully written to Parallel File System\n(PFS) at the sink end. Depending on the number of logger files created, for the\nwhole dataset, we classified our fault tolerance mechanisms into three\ndifferent categories: File logger, Transaction logger and Universal logger.\nAlso, to address space overhead of these object based logging mechanisms, we\nhave proposed different methods of populating logger files with the information\nof the completed objects. We have evaluated the data transfer performance and\nrecovery time overhead of the proposed object based logging fault tolerant\nmechanisms on LADS data transfer tool. Our experimental results show that, LADS\nin conjunction with proposed object based fault tolerance mechanisms exhibit an\noverhead of less than 1% with respect to data transfer time and total recovery\ntime overhead is around 10% of total data transfer time at any fault point.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 04:30:04 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Kasu", "Preethika", ""], ["Kim", "Taeuk", ""], ["Kim", "Youngjae", ""], ["Um", "Jung-Ho", ""], ["Park", "Kyongseok", ""], ["Atchley", "Scott", ""]]}, {"id": "1805.06149", "submitter": "Joshua Daymude", "authors": "Joshua J. Daymude, Robert Gmyr, Kristian Hinnenthal, Irina Kostitsyna,\n  Christian Scheideler, Andr\\'ea W. Richa", "title": "Convex Hull Formation for Programmable Matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We envision programmable matter as a system of nano-scale agents (called\nparticles) with very limited computational capabilities that move and compute\ncollectively to achieve a desired goal. We use the geometric amoebot model as\nour computational framework, which assumes particles move on the triangular\nlattice. Motivated by the problem of sealing an object using minimal resources,\nwe show how a particle system can self-organize to form an object's convex\nhull. We give a distributed, local algorithm for convex hull formation and\nprove that it runs in $\\mathcal{O}(B)$ asynchronous rounds, where $B$ is the\nlength of the object's boundary. Within the same asymptotic runtime, this\nalgorithm can be extended to also form the object's (weak) $\\mathcal{O}$-hull,\nwhich uses the same number of particles but minimizes the area enclosed by the\nhull. Our algorithms are the first to compute convex hulls with distributed\nentities that have strictly local sensing, constant-size memory, and no shared\nsense of orientation or coordinates. Ours is also the first distributed\napproach to computing restricted-orientation convex hulls. This approach\ninvolves coordinating particles as distributed memory; thus, as a supporting\nbut independent result, we present and analyze an algorithm for organizing\nparticles with constant-size memory as distributed binary counters that\nefficiently support increments, decrements, and zero-tests --- even as the\nparticles move.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 06:28:47 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 21:19:08 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Daymude", "Joshua J.", ""], ["Gmyr", "Robert", ""], ["Hinnenthal", "Kristian", ""], ["Kostitsyna", "Irina", ""], ["Scheideler", "Christian", ""], ["Richa", "Andr\u00e9a W.", ""]]}, {"id": "1805.06151", "submitter": "Tsvi Kopelowitz", "authors": "Tsvi Kopelowitz, Ely Porat, Yair Rosenmutter", "title": "Improved Worst-Case Deterministic Parallel Dynamic Minimum Spanning\n  Forest", "comments": "Full version of a paper accepted to SPAA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a new deterministic algorithm for the dynamic Minimum\nSpanning Forest (MSF) problem in the EREW PRAM model, where the goal is to\nmaintain a MSF of a weighted graph with $n$ vertices and $m$ edges while\nsupporting edge insertions and deletions. We show that one can solve the\ndynamic MSF problem using $O(\\sqrt n)$ processors and $O(\\log n)$ worst-case\nupdate time, for a total of $O(\\sqrt n \\log n)$ work. This improves on the work\nof Ferragina [IPPS 1995] which costs $O(\\log n)$ worst-case update time and\n$O(n^{2/3} \\log{\\frac{m}{n}})$ work.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 06:32:44 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""], ["Rosenmutter", "Yair", ""]]}, {"id": "1805.06156", "submitter": "Neda Tavakoli", "authors": "Neda Tavakoli, Dong Dai, Yong Chen", "title": "Client-side Straggler-Aware I/O Scheduler for Object-based Parallel File\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-based parallel file systems have emerged as promising storage\nsolutions for high-performance computing (HPC) systems. Despite the fact that\nobject storage provides a flexible interface, scheduling highly concurrent I/O\nrequests that access a large number of objects still remains as a challenging\nproblem, especially in the case when stragglers (storage servers that are\nsignificantly slower than others) exist in the system. An efficient I/O\nscheduler needs to avoid possible stragglers to achieve low latency and high\nthroughput. In this paper, we introduce a log-assisted straggler-aware I/O\nscheduling to mitigate the impact of storage server stragglers. The\ncontribution of this study is threefold. First, we introduce a client-side,\nlog-assisted, straggler-aware I/O scheduler architecture to tackle the storage\nstraggler issue in HPC systems. Second, we present three scheduling algorithms\nthat can make efficient decision for scheduling I/Os while avoiding stragglers\nbased on such an architecture. Third, we evaluate the proposed I/O scheduler\nusing simulations, and the simulation results have confirmed the promise of the\nnewly introduced straggler-aware I/O scheduler.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 06:59:06 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Tavakoli", "Neda", ""], ["Dai", "Dong", ""], ["Chen", "Yong", ""]]}, {"id": "1805.06161", "submitter": "Neda Tavakoli", "authors": "Neda Tavakoli, Dong Dai, John Jenkins, Philip Carns, Robert Ross, Yong\n  Chen", "title": "A Software-Defined Approach for QoS Control in High-Performance\n  Computing Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance computing (HPC) storage systems become increasingly critical\nto scientific applications given the data-driven discovery paradigm shift. As a\nstorage solution for large-scale HPC systems, dozens of applications share the\nsame storage system, and will compete and can interfere with each other.\nApplication interference can dramatically degrade the overall storage system\nperformance. Therefore, developing a flexible and effective storage solution to\nassure a certain level of resources per application, i.e. the\nQuality-of-Service (QoS) support, is critical. One of the common solution to\nachieve QoS assurance for storage systems is using provisioning\ntechnique~\\cite{3}. Provisioning refers to the ability of providing certain\namount of resources for applications and expected workloads. However,\nprovisioning has limitations such as requiring the detailed knowledge of the\nexpected workloads. In addition, the storage workloads are transient hence\nexpensive to be satisfied. Due to these limitations, providing QoS storage\nsystems through provisioning is challenging.\n  In this research, a software-defined approach~\\cite{0} is proposed as a\nflexible solution to achieve QoS guarantee for storage systems. The driving\nforce of using a software-defined approach instead of the traditional\napproaches, is that it has the ability to enable a more flexible, scalable, and\nefficient platform. For example, if any changes occurred in the system, it does\nnot necessarily need to re-configure thousands of devices; instead, with\nre-configuring a logically centralized component, other devices will be\nautomatically notified.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 07:07:58 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Tavakoli", "Neda", ""], ["Dai", "Dong", ""], ["Jenkins", "John", ""], ["Carns", "Philip", ""], ["Ross", "Robert", ""], ["Chen", "Yong", ""]]}, {"id": "1805.06167", "submitter": "Neda Tavakoli", "authors": "Dong Dai, Robert Ross, Dounia Khaldi, Yonghong Yan, Matthieu Dorier,\n  Neda Tavakoli, Yong Chen", "title": "A Cross-Layer Solution in Scientific Workflow System for Tackling Data\n  Movement Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications in HPC environment are more com-plex and more\ndata-intensive nowadays. Scientists usually rely on workflow system to manage\nthe complexity: simply define multiple processing steps into a single script\nand let the work-flow systems compile it and schedule all tasks accordingly.\nNumerous workflow systems have been proposed and widely used, like Galaxy,\nPegasus, Taverna, Kepler, Swift, AWE, etc., to name a few examples.\n  Traditionally, scientific workflow systems work with parallel file systems,\nlike Lustre, PVFS, Ceph, or other forms of remote shared storage systems. As\nsuch, the data (including the intermediate data generated during workflow\nexecution) need to be transferred back and forth between compute nodes and\nstorage systems, which introduces a significant performance bottleneck on I/O\noperations. Along with the enlarging perfor-mance gap between CPU and storage\ndevices, this bottleneck is expected to be worse.\n  Recently, we have introduced a new concept of Compute-on-Data-Path to allow\ntasks and data binding to be more efficient to reduce the data movement cost.\nTo workflow systems, the key is to exploit the data locality in HPC storage\nhierarchy: if the datasets are stored in compute nodes, near the workflow\ntasks, then the task can directly access them with better performance with less\nnetwork usage. Several recent studies have been done regarding building such a\nshared storage system, utilizing compute node resources, to serve HPC workflows\nwith locality, such as Hercules [1] and WOSS [2] etc. In this research, we\nfurther argue that providing a compute-node side storage system is not\nsufficient to fully exploit data locality. A cross-layer solution combining\nstorage system, compiler, and runtime is necessary. We take Swift/T [3], a\nworkflow system for data-intensive applications, as a prototype platform to\ndemonstrate such a cross-layer solution\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 07:39:06 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Dai", "Dong", ""], ["Ross", "Robert", ""], ["Khaldi", "Dounia", ""], ["Yan", "Yonghong", ""], ["Dorier", "Matthieu", ""], ["Tavakoli", "Neda", ""], ["Chen", "Yong", ""]]}, {"id": "1805.06169", "submitter": "Neda Tavakoli", "authors": "Neda Tavakoli, Yong Chen", "title": "A Software-Defined QoS Provisioning Framework for HPC Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of large-scale data-intensive high-performance\napplications, new I/O challenges appear in the efficient management of\npetabytes of information in High-Performance Computing (HPC) environments. Data\nmanagement environments must meet the performance needs of such applications,\nrepresented by various Quality-of-Service (QoS) metrics such as desired\nbandwidth, response time guarantee, and resource utilization. Traditional\nhigh-performance management platforms are facing considerable challenges\nregarding flexibility, as well as the need to address a variety of QoS metrics\nand constraints. To tackle these challenges, a Software-Defined approach is\nconsidered promising, and various prototypes have already been deployed in\nCloud-based data centers. In this paper, we investigate the idea of utilizing a\nsoftware-defined approach to provide I/O QoS provisioning for HPC applications.\nWe identify the key challenges towards the high degree of concurrency and\nvariation in HPC platforms, and propose a series of novel designs into the\ngeneral software-defined approach in order to deliver our goal. Specifically,\nwe introduced a borrowing-based strategy and a new M-LWDF algorithm based on\ntraditional token-bucket algorithms to assure a fair and efficient utilization\nof resources for HPC applications. Due to the lack of software-defined\nframeworks in current HPC platform, we evaluated our framework through\nsimulation. The experimental results show that our strategies make a\nsignificant improvement upon the general HPC frameworks and lead to clear\nperformance gain for HPC applications.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 07:43:25 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Tavakoli", "Neda", ""], ["Chen", "Yong", ""]]}, {"id": "1805.06180", "submitter": "Marco Capuccini", "authors": "Marco Capuccini, Anders Larsson, Matteo Carone, Jon Ander Novella,\n  Noureddin Sadawi, Jianliang Gao, Salman Toor, and Ola Spjuth", "title": "On-Demand Virtual Research Environments using Microservices", "comments": null, "journal-ref": null, "doi": "10.7717/peerj-cs.232", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational demands for scientific applications are continuously\nincreasing. The emergence of cloud computing has enabled on-demand resource\nallocation. However, relying solely on infrastructure as a service does not\nachieve the degree of flexibility required by the scientific community. Here we\npresent a microservice-oriented methodology, where scientific applications run\nin a distributed orchestration platform as software containers, referred to as\non-demand, virtual research environments. The methodology is vendor agnostic\nand we provide an open source implementation that supports the major cloud\nproviders, offering scalable management of scientific pipelines. We demonstrate\napplicability and scalability of our methodology in life science applications,\nbut the methodology is general and can be applied to other scientific domains.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 07:59:12 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 06:54:15 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 10:46:47 GMT"}, {"version": "v4", "created": "Fri, 10 May 2019 12:22:00 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Capuccini", "Marco", ""], ["Larsson", "Anders", ""], ["Carone", "Matteo", ""], ["Novella", "Jon Ander", ""], ["Sadawi", "Noureddin", ""], ["Gao", "Jianliang", ""], ["Toor", "Salman", ""], ["Spjuth", "Ola", ""]]}, {"id": "1805.06238", "submitter": "Fabian Reiter", "authors": "Fabian Reiter", "title": "Distributed Automata and Logic", "comments": "PhD thesis, 116 pages. http://www.theses.fr/2017USPCC034", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed automata are finite-state machines that operate on finite\ndirected graphs. Acting as synchronous distributed algorithms, they use their\ninput graph as a network in which identical processors communicate for a\npossibly infinite number of synchronous rounds. For the local variant of those\nautomata, where the number of rounds is bounded by a constant, Hella et al.\n(2012, 2015) have established a logical characterization in terms of basic\nmodal logic. In this thesis, we provide similar logical characterizations for\ntwo more expressive classes of distributed automata.\n  The first class extends local automata with a global acceptance condition and\nthe ability to alternate between nondeterministic and parallel computations. We\nshow that it is equivalent to monadic second-order logic on graphs. By\nrestricting transitions to be nondeterministic or deterministic, we also obtain\ntwo strictly weaker variants for which the emptiness problem is decidable.\n  Our second class transfers the standard notion of asynchronous algorithm to\nthe setting of nonlocal distributed automata. The resulting machines are shown\nto be equivalent to a small fragment of least fixpoint logic, and more\nspecifically, to a restricted variant of the modal {\\mu}-calculus that allows\nleast fixpoints but forbids greatest fixpoints. Exploiting the connection with\nlogic, we additionally prove that the expressive power of those asynchronous\nautomata is independent of whether or not messages can be lost.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 10:42:41 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Reiter", "Fabian", ""]]}, {"id": "1805.06265", "submitter": "Alon Berger", "authors": "Alon Berger, Idit Keidar, and Alexander Spiegelman", "title": "Integrated Bounds for Disintegrated Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We point out a somewhat surprising similarity between non-authenticated\nByzantine storage, coded storage, and certain emulations of shared registers\nfrom smaller ones. A common characteristic in all of these is the inability of\nreads to safely return a value obtained in a single atomic access to shared\nstorage. We collectively refer to such systems as disintegrated storage, and\nshow integrated space lower bounds for asynchronous regular wait-free\nemulations in all of them. In a nutshell, if readers are invisible, then the\nstorage cost of such systems is inherently exponential in the size of written\nvalues; otherwise, it is at least linear in the number of readers. Our bounds\nare asymptotically tight to known algorithms, and thus justify their high\ncosts.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 12:15:10 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 12:23:49 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Berger", "Alon", ""], ["Keidar", "Idit", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "1805.06358", "submitter": "Marc Shapiro", "authors": "Nuno Pregui\\c{c}a, Carlos Baquero, Marc Shapiro (Regal)", "title": "Conflict-free Replicated Data Types (CRDTs)", "comments": null, "journal-ref": "Sakr, Sherif and Zomaya, Albert. Encyclopedia of Big Data\n  Technologies, Springer International Publishing, 2018, Encyclopedia of Big\n  Data Technologies, 978-3-319-63962-8", "doi": "10.1007/978-3-319-63962-8\\_185-1", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conflict-free replicated data type (CRDT) is an abstract data type, with a\nwell defined interface, designed to be replicated at multiple processes and\nexhibiting the following properties: (1) any replica can be modified without\ncoordinating with another replicas; (2) when any two replicas have received the\nsame set of updates, they reach the same state, deterministically, by adopting\nmathematically sound rules to guarantee state convergence.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 15:00:36 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Pregui\u00e7a", "Nuno", "", "Regal"], ["Baquero", "Carlos", "", "Regal"], ["Shapiro", "Marc", "", "Regal"]]}, {"id": "1805.06383", "submitter": "Arturo Argueta", "authors": "Arturo Argueta, David Chiang", "title": "Composing Finite State Transducers on GPUs", "comments": "ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted finite-state transducers (FSTs) are frequently used in language\nprocessing to handle tasks such as part-of-speech tagging and speech\nrecognition. There has been previous work using multiple CPU cores to\naccelerate finite state algorithms, but limited attention has been given to\nparallel graphics processing unit (GPU) implementations. In this paper, we\nintroduce the first (to our knowledge) GPU implementation of the FST\ncomposition operation, and we also discuss the optimizations used to achieve\nthe best performance on this architecture. We show that our approach obtains\nspeedups of up to 6x over our serial implementation and 4.5x over OpenFST.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 15:56:17 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Argueta", "Arturo", ""], ["Chiang", "David", ""]]}, {"id": "1805.06425", "submitter": "Allan Santos", "authors": "Allan Santos and Hermano Lustosa and Fabio Porto and Bruno Schulze", "title": "Towards In-transit Analysis on Supercomputing Environments", "comments": "1 page, remove institute index (\\instnum) since there is only one 1\n  page, write institute name in English", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The drive towards exascale computing is opening an enormous opportunity for\nmore realistic and precise simulations of natural phenomena. The process of\nsimulation, however, involves not only the numerical computation of predictions\nbut also the analysis of results both to evaluate the simulation quality and\ninterpret the simulated phenomenon. In this context, one may consider the\nduality between transaction and analytical processing to be repositioned in\nthis new context. The co-habitation of simulation computation and analysis has\nbeen named after in situ analysis, whereas the separation in different systems\nconsidered as in-transit analysis. In this paper we focus in the latter model\nand study the impact of transferring varying block size data from the\nsimulation system to the analytical one. We use the Remote Direct Memory Access\nprotocol (RDMA) that reduces the interference on performance caused by data\ncopies and context switching. It adopts an in-memory data transfer strategy\ncombined with TCP, using the BSD sockets API and the Linux splice(2) syscall.\nWe present a performance evaluation with our work and traditional utilities.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 17:01:42 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 14:43:24 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Santos", "Allan", ""], ["Lustosa", "Hermano", ""], ["Porto", "Fabio", ""], ["Schulze", "Bruno", ""]]}, {"id": "1805.06728", "submitter": "Volker Turau", "authors": "Volker Turau", "title": "A Distributed Algorithm for Finding Hamiltonian Cycles in Random Graphs\n  in O(log n) Time", "comments": "17 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known for some time that a random graph $G(n,p)$ contains w.h.p. a\nHamiltonian cycle if $p$ is larger than the critical value $p_{crit}= (\\log n +\n\\log \\log n + \\omega_n)/n$. The determination of a concrete Hamiltonian cycle\nis even for values much larger than $p_{crit}$ a nontrivial task. In this paper\nwe consider random graphs $G(n,p)$ with $p$ in $\\tilde{\\Omega}(1/\\sqrt{n})$,\nwhere $\\tilde{\\Omega}$ hides poly-logarithmic factors in $n$. For this range of\n$p$ we present a distributed algorithm ${\\cal A}_{HC}$ that finds w.h.p. a\nHamiltonian cycle in $O(\\log n)$ rounds. The algorithm works in the synchronous\nmodel and uses messages of size $O(\\log n)$ and $O(\\log n)$ memory per node.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 12:38:15 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Turau", "Volker", ""]]}, {"id": "1805.06801", "submitter": "K. R. Jayaram", "authors": "Scott Boag, Parijat Dube, Kaoutar El Maghraoui, Benjamin Herta,\n  Waldemar Hummer, K. R. Jayaram, Rania Khalaf, Vinod Muthusamy, Michael\n  Kalantar, Archit Verma", "title": "Dependability in a Multi-tenant Multi-framework Deep Learning\n  as-a-Service Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL), a form of machine learning, is becoming increasingly\npopular in several application domains. As a result, cloud-based Deep Learning\nas a Service (DLaaS) platforms have become an essential infrastructure in many\norganizations. These systems accept, schedule, manage and execute DL training\njobs at scale.\n  This paper explores dependability in the context of a DLaaS platform used in\nIBM. We begin by explaining how DL training workloads are different, and what\nfeatures ensure dependability in this context. We then describe the\narchitecture, design and implementation of a cloud-based orchestration system\nfor DL training. We show how this system has been architected with\ndependability in mind while also being horizontally scalable, elastic, flexible\nand efficient. We also present an initial empirical evaluation of the overheads\nintroduced by our platform, and discuss tradeoffs between efficiency and\ndependability.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 14:32:02 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Boag", "Scott", ""], ["Dube", "Parijat", ""], ["Maghraoui", "Kaoutar El", ""], ["Herta", "Benjamin", ""], ["Hummer", "Waldemar", ""], ["Jayaram", "K. R.", ""], ["Khalaf", "Rania", ""], ["Muthusamy", "Vinod", ""], ["Kalantar", "Michael", ""], ["Verma", "Archit", ""]]}, {"id": "1805.06963", "submitter": "Gesualdo Scutari", "authors": "Gesualdo Scutari and Ying Sun", "title": "Parallel and Distributed Successive Convex Approximation Methods for\n  Big-Data Optimization", "comments": null, "journal-ref": "Lecture Notes in Mathematics, C.I.M.E, Springer Verlag series,\n  2018", "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a surge of interest in parallel and distributed\noptimization methods for large-scale systems. In particular, nonconvex\nlarge-scale optimization problems have found a wide range of applications in\nseveral engineering fields. The design and the analysis of such complex,\nlarge-scale, systems pose several challenges and call for the development of\nnew optimization models and algorithms. The major contribution of this paper is\nto put forth a general, unified, algorithmic framework, based on Successive\nConvex Approximation (SCA) techniques, for the parallel and distributed\nsolution of a general class of non-convex constrained (non-separable,\nnetworked) problems. The presented framework unifies and generalizes several\nexisting SCA methods, making them appealing for a parallel/distributed\nimplementation while offering a flexible selection of function approximants,\nstep size schedules, and control of the computation/communication efficiency.\nThis paper is organized according to the lectures that one of the authors\ndelivered at the CIME Summer School on Centralized and Distributed Multi-agent\nOptimization Models and Algorithms, held in Cetraro, Italy, June 23--27, 2014.\nThese lectures are: I) Successive Convex Approximation Methods: Basics; II)\nParallel Successive Convex Approximation Methods; and III) Distributed\nSuccessive Convex Approximation Methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 20:44:36 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Scutari", "Gesualdo", ""], ["Sun", "Ying", ""]]}, {"id": "1805.06989", "submitter": "Jo\\~ao Leit\\~ao", "authors": "Jo\\~ao Leit\\~ao and Pedro \\'Akos Costa and Maria Cec\\'ilia Gomes and\n  Nuno Pregui\\c{c}a", "title": "Towards Enabling Novel Edge-Enabled Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing has emerged as a distributed computing paradigm to overcome\npractical scalability limits of cloud computing. The main principle of edge\ncomputing is to leverage on computational resources outside of the cloud for\nperforming computations closer to data sources, avoiding unnecessary data\ntransfers to the cloud and enabling faster responses for clients.\n  While this paradigm has been successfully employed to improve response times\nin some contexts, mostly by having clients perform pre-processing and/or\nfiltering of data, or by leveraging on distributed caching infrastructures, we\nargue that the combination of edge and cloud computing has the potential to\nenable novel applications. However, to do so, some significant research\nchallenges have to be tackled by the computer science community. In this paper,\nwe discuss different edge resources and their potential use, motivated by\nenvisioned use cases. We then discuss concrete research challenges that are in\nthe critical path towards realizing our edge vision. We conclude by proposing a\nresearch agenda to allow the full exploitation of the potential for the\nemerging hybrid cloud/edge paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 23:12:40 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 07:39:13 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Leit\u00e3o", "Jo\u00e3o", ""], ["Costa", "Pedro \u00c1kos", ""], ["Gomes", "Maria Cec\u00edlia", ""], ["Pregui\u00e7a", "Nuno", ""]]}, {"id": "1805.07047", "submitter": "Wyatt Meldman-Floch", "authors": "Wyatt Meldman-Floch", "title": "Blockchain Cohomology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We follow existing distributed systems frameworks employing methods from\nalgebraic topology to formally define primitives of blockchain technology. We\ndefine the notion of cross chain liquidity, sharding and probability spaces\nbetween and within blockchain protocols. We incorporate recent advancements in\nsynthetic homology to show that this topological framework can be implemented\nwithin a type system. We use recursion schemes to define kernels admitting\nsmooth manifolds across protocol complexes, leading to the formal definition of\na Poincare protocol.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 05:06:33 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Meldman-Floch", "Wyatt", ""]]}, {"id": "1805.07209", "submitter": "Simon Weidner", "authors": "Fabian Kuhn, Yannic Maus, Simon Weidner", "title": "Deterministic Distributed Ruling Sets of Line Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(\\alpha,\\beta)$-ruling set of a graph $G=(V,E)$ is a set $R\\subseteq V$\nsuch that for any node $v\\in V$ there is a node $u\\in R$ in distance at most\n$\\beta$ from $v$ and such that any two nodes in $R$ are at distance at least\n$\\alpha$ from each other. The concept of ruling sets can naturally be extended\nto edges, i.e., a subset $F\\subseteq E$ is an $(\\alpha,\\beta)$-ruling edge set\nof a graph $G=(V,E)$ if the corresponding nodes form an $(\\alpha,\\beta)$-ruling\nset in the line graph of $G$. This paper presents a simple deterministic,\ndistributed algorithm, in the $\\mathsf{CONGEST}$ model, for computing\n$(2,2)$-ruling edge sets in $O(\\log^* n)$ rounds. Furthermore, we extend the\nalgorithm to compute ruling sets of graphs with bounded diversity. Roughly\nspeaking, the diversity of a graph is the maximum number of maximal cliques a\nvertex belongs to. We devise $(2,O(\\mathcal{D}))$-ruling sets on graphs with\ndiversity $\\mathcal{D}$ in $O(\\mathcal{D}+\\log^* n)$ rounds. This also implies\na fast, deterministic $(2,O(\\ell))$-ruling edge set algorithm for hypergraphs\nwith rank at most $\\ell$.\n  Furthermore, we provide a ruling set algorithm for general graphs that for\nany $B\\geq 2$ computes an $\\big(\\alpha, \\alpha \\lceil \\log_B n \\rceil\n\\big)$-ruling set in $O(\\alpha \\cdot B \\cdot \\log_B n)$ rounds in the\n$\\mathsf{CONGEST}$ model. The algorithm can be modified to compute a $\\big(2,\n\\beta \\big)$-ruling set in $O(\\beta \\Delta^{2/\\beta} + \\log^* n)$ rounds in the\n$\\mathsf{CONGEST}$~ model, which matches the currently best known such\nalgorithm in the more general $\\mathsf{LOCAL}$ model.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 13:52:42 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Weidner", "Simon", ""]]}, {"id": "1805.07269", "submitter": "Mohammad Bakhshalipour", "authors": "Mohammad Bakhshalipour and Hamid Sarbazi-Azad", "title": "Parallelizing Bisection Root-Finding: A Case for Accelerating Serial\n  Algorithms in Multicore Substrates", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicore architectures dominate today's processor market. Even though the\nnumber of cores and threads are pretty high and continues to grow, inherently\nserial algorithms do not benefit from the abundance of cores and threads. In\nthis paper, we propose Runahead Computing, a technique which uses idle threads\nin a multi-threaded architecture for accelerating the execution time of serial\nalgorithms. Through detailed evaluations targeting both CPU and GPU platforms\nand a specific serial algorithm, our approach reduces the execution latency up\nto 9x in our experiments.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 03:47:08 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Bakhshalipour", "Mohammad", ""], ["Sarbazi-Azad", "Hamid", ""]]}, {"id": "1805.07294", "submitter": "Kristian Hinnenthal", "authors": "John Augustine, Mohsen Ghaffari, Robert Gmyr, Kristian Hinnenthal,\n  Fabian Kuhn, Jason Li, Christian Scheideler", "title": "Distributed Computation in Node-Capacitated Networks", "comments": "This is the full version of a paper that appears at SPAA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study distributed graph algorithms in networks in which the\nnodes have a limited communication capacity. Many distributed systems are built\non top of an underlying networking infrastructure, for example by using a\nvirtual communication topology known as an overlay network. Although this\nunderlying network might allow each node to directly communicate with a large\nnumber of other nodes, the amount of communication that a node can perform in a\nfixed amount of time is typically much more limited. We introduce the\nNode-Capacitated Clique model as an abstract communication model, which allows\nus to study the effect of nodes having limited communication capacity on the\ncomplexity of distributed graph computations. In this model, the $n$ nodes of a\nnetwork are connected as a clique and communicate in synchronous rounds. In\neach round, every node can exchange messages of $O(\\log n)$ bits with at most\n$O(\\log n)$ other nodes. When solving a graph problem, the input graph $G$ is\ndefined on the same set of $n$ nodes, where each node knows which other nodes\nare its neighbors in $G$. To initiate research on the Node-Capacitated Clique\nmodel, we present distributed algorithms for the Minimum Spanning Tree (MST),\nBFS Tree, Maximal Independent Set, Maximal Matching, and Vertex Coloring\nproblems. We show that even with only $O(\\log n)$ concurrent interactions per\nnode, the MST problem can still be solved in polylogarithmic time. In all other\ncases, the runtime of our algorithms depends linearly on the arboricity of $G$,\nwhich is a constant for many important graph families such as planar graphs.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:38:47 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 13:36:38 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Augustine", "John", ""], ["Ghaffari", "Mohsen", ""], ["Gmyr", "Robert", ""], ["Hinnenthal", "Kristian", ""], ["Kuhn", "Fabian", ""], ["Li", "Jason", ""], ["Scheideler", "Christian", ""]]}, {"id": "1805.07339", "submitter": "Alex Poms", "authors": "Alex Poms, Will Crichton, Pat Hanrahan, Kayvon Fatahalian", "title": "Scanner: Efficient Video Analysis at Scale", "comments": "14 pages, 14 figuers", "journal-ref": null, "doi": "10.1145/3197517.3201394", "report-no": null, "categories": "cs.CV cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of visual computing applications depend on the analysis of\nlarge video collections. The challenge is that scaling applications to operate\non these datasets requires efficient systems for pixel data access and parallel\nprocessing across large numbers of machines. Few programmers have the\ncapability to operate efficiently at these scales, limiting the field's ability\nto explore new applications that leverage big video data. In response, we have\ncreated Scanner, a system for productive and efficient video analysis at scale.\nScanner organizes video collections as tables in a data store optimized for\nsampling frames from compressed video, and executes pixel processing\ncomputations, expressed as dataflow graphs, on these frames. Scanner schedules\nvideo analysis applications expressed using these abstractions onto\nheterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and\nmedia processing ASICs, for high-throughput pixel processing. We demonstrate\nthe productivity of Scanner by authoring a variety of video processing\napplications including the synthesis of stereo VR video streams from\nmulti-camera rigs, markerless 3D human pose reconstruction from video, and\ndata-mining big video datasets such as hundreds of feature-length films or over\n70,000 hours of TV news. These applications achieve near-expert performance on\na single machine and scale efficiently to hundreds of machines, enabling\nformerly long-running big video data analysis tasks to be carried out in\nminutes to hours.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 17:43:55 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Poms", "Alex", ""], ["Crichton", "Will", ""], ["Hanrahan", "Pat", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "1805.07356", "submitter": "Snehanshu Saha", "authors": "Bidisha Goswami, Jyotirmoy Sarkar, Snehanshu Saha, Saibal Kar and\n  Poulami Sarkar", "title": "ALVEC: Auto-scaling by Lotka Volterra Elastic Cloud: A QoS aware Non\n  Linear Dynamical Allocation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elasticity in resource allocation is still a relevant problem in cloud\ncomputing. There are many academic and white papers which have investigated the\nproblem and offered solutions.\\textbf{Unfortunately, there are scant evidence\nof determining scaling number dynamically.} Elasticity is defined as the\nability to adapt with the changing workloads by provisioning and\nde-provisioning Cloud resources. We propose ALVEC, a novel model of resource\nallocation in Cloud data centers, inspired by population dynamics and\nMathematical Biology, which addresses dynamic allocation by auto-tuning model\nparameters. The proposed model, governed by a coupled differential equation\nknown as Lotka Volterra (LV), fares better in Service level agreement (SLA)\nmanagement and Quality of Services (QoS). We show evidence of true elasticity,\nin theory and empirical comparisons. Additionally, ALVEC is able to predict the\nfuture load and allocate VM's accordingly. The proposed model, ALVEC is the\nfirst example of unsupervised resource allocation scheme.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 04:18:20 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Goswami", "Bidisha", ""], ["Sarkar", "Jyotirmoy", ""], ["Saha", "Snehanshu", ""], ["Kar", "Saibal", ""], ["Sarkar", "Poulami", ""]]}, {"id": "1805.07384", "submitter": "Dingwen Tao", "authors": "Dingwen Tao, Sheng Di, Xin Liang, Zizhong Chen, Franck Cappello", "title": "Fixed-PSNR Lossy Compression for Scientific Data", "comments": "5 pages, 2 figures, 2 tables, accepted by IEEE Cluster'18. arXiv\n  admin note: text overlap with arXiv:1806.08901", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error-controlled lossy compression has been studied for years because of\nextremely large volumes of data being produced by today's scientific\nsimulations. None of existing lossy compressors, however, allow users to fix\nthe peak signal-to-noise ratio (PSNR) during compression, although PSNR has\nbeen considered as one of the most significant indicators to assess compression\nquality. In this paper, we propose a novel technique providing a fixed-PSNR\nlossy compression for scientific data sets. We implement our proposed method\nbased on the SZ lossy compression framework and release the code as an\nopen-source toolkit. We evaluate our fixed-PSNR compressor on three real-world\nhigh-performance computing data sets. Experiments show that our solution has a\nhigh accuracy in controlling PSNR, with an average deviation of 0.1 ~ 5.0 dB on\nthe tested data sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 17:05:48 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 19:10:07 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 14:57:56 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Tao", "Dingwen", ""], ["Di", "Sheng", ""], ["Liang", "Xin", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "1805.07440", "submitter": "Linnan Wang", "authors": "Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca", "title": "Neural Architecture Search using Deep Neural Networks and Monte Carlo\n  Tree Search", "comments": "To appear in the Thirty-Fourth AAAI conference on Artificial\n  Intelligence (AAAI-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has shown great success in automating the\ndesign of neural networks, but the prohibitive amount of computations behind\ncurrent NAS methods requires further investigations in improving the sample\nefficiency and the network evaluation cost to get better results in a shorter\ntime. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS)\nbased NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the\nsearch efficiency by adaptively balancing the exploration and exploitation at\nthe state level, and by a Meta-Deep Neural Network (DNN) to predict network\naccuracies for biasing the search toward a promising region. To amortize the\nnetwork evaluation cost, AlphaX accelerates MCTS rollouts with a distributed\ndesign and reduces the number of epochs in evaluating a network by transfer\nlearning, which is guided with the tree structure in MCTS. In 12 GPU days and\n1000 samples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy\non CIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods\nin both the accuracy and sampling efficiency. Particularly, we also evaluate\nAlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more\nsample efficient than Random Search and Regularized Evolution in finding the\nglobal optimum. Finally, we show the searched architecture improves a variety\nof vision applications from Neural Style Transfer, to Image Captioning and\nObject Detection.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 20:57:41 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 07:47:47 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 06:50:28 GMT"}, {"version": "v4", "created": "Wed, 2 Oct 2019 01:04:05 GMT"}, {"version": "v5", "created": "Thu, 21 Nov 2019 17:45:31 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wang", "Linnan", ""], ["Zhao", "Yiyang", ""], ["Jinnai", "Yuu", ""], ["Tian", "Yuandong", ""], ["Fonseca", "Rodrigo", ""]]}, {"id": "1805.07491", "submitter": "Assaf Kfoury", "authors": "Assaf Kfoury", "title": "A Compositional Approach to Network Algorithms", "comments": "44 pages, 12 figures, 47 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present elements of a typing theory for flow networks, where \"types\",\n\"typings\", and \"type inference\" are formulated in terms of familiar notions\nfrom polyhedral analysis and convex optimization. Based on this typing theory,\nwe develop an alternative approach to the design and analysis of network\nalgorithms, which we illustrate by applying it to the max-flow problem in\nmultiple-source, multiple-sink, capacited directed planar graphs.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 02:21:31 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Kfoury", "Assaf", ""]]}, {"id": "1805.07565", "submitter": "Saeid Pourroostaei Ardakani", "authors": "Saeid Pourroostaei Ardakani", "title": "ACR: a cluster-based routing protocol for VANET", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.5121/ijwmn.2018.10204", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a technique used in network routing to enhance the performance\nand conserve the network resources. This paper presents a cluster-based routing\nprotocol for VANET utilizing a new addressing scheme in which each node gets an\naddress according to its mobility pattern. Hamming distance technique is used\nthen to partition the network in an address-centric manner. The simulation\nresults show that this protocol enhances routing reachability, whereas reduces\nrouting end-to-end delay and traffic received comparing with two benchmarks\nnamely AODV and DSDV.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 10:13:21 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Ardakani", "Saeid Pourroostaei", ""]]}, {"id": "1805.07568", "submitter": "Chen Wu", "authors": "Chen Wu, Andreas Wicenec, Rodrigo Tobar", "title": "Partitioning SKA Dataflows for Optimal Graph Execution", "comments": "Accepted in HPDC ScienceCloud 2018 Workshop", "journal-ref": null, "doi": "10.1145/3217880.3217886", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing data-intensive workflow execution is essential to many modern\nscientific projects such as the Square Kilometre Array (SKA), which will be the\nlargest radio telescope in the world, collecting terabytes of data per second\nfor the next few decades. At the core of the SKA Science Data Processor is the\ngraph execution engine, scheduling tens of thousands of algorithmic components\nto ingest and transform millions of parallel data chunks in order to solve a\nseries of large-scale inverse problems within the power budget. To tackle this\nchallenge, we have developed the Data Activated Liu Graph Engine (DALiuGE) to\nmanage data processing pipelines for several SKA pathfinder projects. In this\npaper, we discuss the DALiuGE graph scheduling sub-system. By extending\nprevious studies on graph scheduling and partitioning, we lay the foundation on\nwhich we can develop polynomial time optimization methods that minimize both\nworkflow execution time and resource footprint while satisfying resource\nconstraints imposed by individual algorithms. We show preliminary results\nobtained from three radio astronomy data pipelines.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 11:09:07 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Wu", "Chen", ""], ["Wicenec", "Andreas", ""], ["Tobar", "Rodrigo", ""]]}, {"id": "1805.07764", "submitter": "Michal Dory", "authors": "Michal Dory", "title": "Distributed Approximation of Minimum $k$-edge-connected Spanning\n  Subgraphs", "comments": "To appear in PODC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the minimum $k$-edge-connected spanning subgraph ($k$-ECSS) problem the\ngoal is to find the minimum weight subgraph resistant to up to $k-1$ edge\nfailures. This is a central problem in network design, and a natural\ngeneralization of the minimum spanning tree (MST) problem. While the MST\nproblem has been studied extensively by the distributed computing community,\nfor $k \\geq 2$ less is known in the distributed setting.\n  In this paper, we present fast randomized distributed approximation\nalgorithms for $k$-ECSS in the CONGEST model. Our first contribution is an\n$\\widetilde{O}(D + \\sqrt{n})$-round $O(\\log{n})$-approximation for 2-ECSS, for\na graph with $n$ vertices and diameter $D$. The time complexity of our\nalgorithm is almost tight and almost matches the time complexity of the MST\nproblem. For larger constant values of $k$ we give an $\\widetilde{O}(n)$-round\n$O(\\log{n})$-approximation. Additionally, in the special case of unweighted\n3-ECSS we show how to improve the time complexity to $O(D \\log^3{n})$ rounds.\nAll our results significantly improve the time complexity of previous\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 12:59:17 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Dory", "Michal", ""]]}, {"id": "1805.07841", "submitter": "Yan  Li", "authors": "Yan Li, Chao Qu, Huan Xu", "title": "Communication-Efficient Projection-Free Algorithm for Distributed\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization has gained a surge of interest in recent years. In\nthis paper we propose a distributed projection free algorithm named Distributed\nConditional Gradient Sliding(DCGS). Compared to the state-of-the-art\ndistributed Frank-Wolfe algorithm, our algorithm attains the same communication\ncomplexity under much more realistic assumptions. In contrast to the consensus\nbased algorithm, DCGS is based on the primal-dual algorithm, yielding a modular\nanalysis that can be exploited to improve linear oracle complexity whenever\ncentralized Frank-Wolfe can be improved. We demonstrate this advantage and show\nthat the linear oracle complexity can be reduced to almost the same order of\nmagnitude as the communication complexity, when the feasible set is polyhedral.\nFinally we present experimental results on Lasso and matrix completion,\ndemonstrating significant performance improvement compared to the existing\ndistributed Frank-Wolfe algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 23:36:44 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Li", "Yan", ""], ["Qu", "Chao", ""], ["Xu", "Huan", ""]]}, {"id": "1805.07891", "submitter": "Liang Luo", "authors": "Liang Luo, Jacob Nelson, Luis Ceze, Amar Phanishayee, Arvind\n  Krishnamurthy", "title": "Parameter Hub: a Rack-Scale Parameter Server for Distributed Deep Neural\n  Network Training", "comments": null, "journal-ref": null, "doi": "10.1145/3267809.3267840", "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed deep neural network (DDNN) training constitutes an increasingly\nimportant workload that frequently runs in the cloud. Larger DNN models and\nfaster compute engines are shifting DDNN training bottlenecks from computation\nto communication. This paper characterizes DDNN training to precisely pinpoint\nthese bottlenecks. We found that timely training requires high performance\nparameter servers (PSs) with optimized network stacks and gradient processing\npipelines, as well as server and network hardware with balanced computation and\ncommunication resources. We therefore propose PHub, a high performance\nmulti-tenant, rack-scale PS design. PHub co-designs the PS software and\nhardware to accelerate rack-level and hierarchical cross-rack parameter\nexchange, with an API compatible with many DDNN training frameworks. PHub\nprovides a performance improvement of up to 2.7x compared to state-of-the-art\ndistributed training techniques for cloud-based ImageNet workloads, with 25%\nbetter throughput per dollar.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 04:55:04 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 21:18:51 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Luo", "Liang", ""], ["Nelson", "Jacob", ""], ["Ceze", "Luis", ""], ["Phanishayee", "Amar", ""], ["Krishnamurthy", "Arvind", ""]]}, {"id": "1805.07954", "submitter": "Guy Goren", "authors": "Guy Goren and Yoram Moses", "title": "Silence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of communication is a substantial factor affecting the scalability\nof many distributed applications. Every message sent can incur a cost in\nstorage, computation, energy and bandwidth. Consequently, reducing the\ncommunication costs of distributed applications is highly desirable. The best\nway to reduce message costs is by communicating without sending any messages\nwhatsoever. This paper initiates a rigorous investigation into the use of\nsilence in synchronous settings, in which processes can fail. We formalize\nsufficient conditions for information transfer using silence, as well as\nnecessary conditions for particular cases of interest. This allows us to\nidentify message patterns that enable communication through silence. In\nparticular, a pattern called a {\\em silent choir} is identified, and shown to\nbe central to information transfer via silence in failure-prone systems. The\npower of the new framework is demonstrated on the {\\em atomic commitment}\nproblem (AC). A complete characterization of the tradeoff between message\ncomplexity and round complexity in the synchronous model with crash failures is\nprovided, in terms of lower bounds and matching protocols. In particular, a new\nmessage-optimal AC protocol is designed using silence, in which processes\ndecide in~3 rounds in the common case. This significantly improves on the best\npreviously known message-optimal AC protocol, in which decisions were performed\nin $\\Theta(n)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 09:13:12 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Goren", "Guy", ""], ["Moses", "Yoram", ""]]}, {"id": "1805.07998", "submitter": "Ali Mohammed", "authors": "Ali Mohammed, Ahmed Eleliemy, and Florina M. Ciorba", "title": "Performance Reproduction and Prediction of Selected Dynamic Loop\n  Scheduling Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications are complex, large, and often exhibit irregular and\nstochastic behavior. The use of efficient loop scheduling techniques in\ncomputationally-intensive applications is crucial for improving their\nperformance on high-performance computing (HPC) platforms. A number of dynamic\nloop scheduling (DLS) techniques have been proposed between the late 1980s and\nearly 2000s, and efficiently used in scientific applications. In most cases,\nthe computing systems on which they have been tested and validated are no\nlonger available. This work is concerned with the minimization of the sources\nof uncertainty in the implementation of DLS techniques to avoid unnecessary\ninfluences on the performance of scientific applications. Therefore, it is\nimportant to ensure that the DLS techniques employed in scientific applications\ntoday adhere to their original design goals and specifications. The goal of\nthis work is to attain and increase the trust in the implementation of DLS\ntechniques in present studies. To achieve this goal, the performance of a\nselection of scheduling experiments from the 1992 original work that introduced\nfactoring is reproduced and predicted via both, simulative and native\nexperimentation. The experiments show that the simulation reproduces the\nperformance achieved on the past computing platform and accurately predicts the\nperformance achieved on the present computing platform. The performance\nreproduction and prediction confirm that the present implementation of the DLS\ntechniques considered both, in simulation and natively, adheres to their\noriginal description. The results confirm the hypothesis that reproducing\nexperiments of identical scheduling scenarios on past and modern hardware leads\nto an entirely different behavior from expected.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 11:46:49 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 16:02:32 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Mohammed", "Ali", ""], ["Eleliemy", "Ahmed", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1805.08025", "submitter": "Edmond Nurellari", "authors": "Daniel Bonilla Licea, Edmond Nurellari, Mounir Ghogho", "title": "Energy balancing for robotic aided clustered wireless sensor networks\n  using mobility diversity algorithms", "comments": "5 pages, 26th European Signal Processing Conference (EUSIPCO 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of energy balancing in a clustered wireless sensor\nnetwork (WSN) deployed randomly in a large field and aided by a mobile robot\n(MR). The sensor nodes (SNs) are tasked to monitor a region of interest (ROI)\nand report their test statistics to the cluster heads (CHs), which subsequently\nreport to the fusion center (FC) over a wireless fading channel. To maximize\nthe lifetime of the WSN, the MR is deployed to act as an adaptive relay between\na subset of the CHs and the FC. To achieve this we develop a multiple-link\nmobility diversity algorithm (MDA) executed by the MR that will allow to\ncompensate simultaneously for the small-scale fading at the established\nwireless links (i.e., the MR-to-FC as well as various CH-to-MR communication\nlinks). Simulation results show that the proposed MR aided technique is able to\nsignificantly reduce the transmission power required and thus extend the\noperational lifetime of the WSN. We also show how the effect of small-scale\nfading at various wireless links is mitigated by using the proposed\nmultiple-link MDA executed by a MR equipped with a single antenna.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 12:56:55 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 12:40:22 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Licea", "Daniel Bonilla", ""], ["Nurellari", "Edmond", ""], ["Ghogho", "Mounir", ""]]}, {"id": "1805.08124", "submitter": "Matteo Pontecorvi", "authors": "Matteo Pontecorvi and Vijaya Ramachandran", "title": "Distributed Algorithms for Directed Betweenness Centrality and All Pairs\n  Shortest Paths", "comments": "The new algorithms for APSP and BC in Section 4 of this manuscript\n  will appear as the theoretical component in the following paper, which will\n  appear in Proc. ACM PPoPP, February 2019: Loc Hoang, Matteo Pontecorvi,\n  Roshan Dathathri, Gurbinder Gill, Bozhi You, Keshav Pingali, and Vijaya\n  Ramachandran - A round-efficient distributed Betweenness Centrality algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The betweenness centrality (BC) of a node in a network (or graph) is a\nmeasure of its importance in the network. BC is widely used in a large number\nof environments such as social networks, transport networks, security/mobile\nnetworks and more. We present an O(n)-round distributed algorithm for computing\nBC of every vertex as well as all pairs shortest paths (APSP) in a directed\nunweighted network, where n is the number of vertices and m is the number of\nedges. We also present O(n)-round distributed algorithms for computing APSP and\nBC in a weighted directed acyclic graph (dag). Our algorithms are in the\nCongest model and our weighted dag algorithms appear to be the first nontrivial\ndistributed algorithms for both APSP and BC. All our algorithms pay careful\nattention to the constant factors in the number of rounds and number of\nmessages sent, and for unweighted graphs they improve on one or both of these\nmeasures by at least a constant factor over previous results for both directed\nand undirected APSP and BC.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:31:23 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 20:08:23 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Pontecorvi", "Matteo", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1805.08288", "submitter": "Johannes de Fine Licht", "authors": "Johannes de Fine Licht, Maciej Besta, Simon Meierhans, Torsten Hoefler", "title": "Transformations of High-Level Synthesis Codes for High-Performance\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing architectures promise a major stride in performance and\nenergy efficiency over the traditional load/store devices currently employed in\nlarge scale computing systems. The adoption of high-level synthesis (HLS) from\nlanguages such as C++ and OpenCL has greatly increased programmer productivity\nwhen designing for such platforms. While this has enabled a wider audience to\ntarget spatial computing architectures, the optimization principles known from\ntraditional software design are no longer sufficient to implement\nhigh-performance codes, due to fundamentally distinct aspects of hardware\ndesign, such as programming for deep pipelines, distributed memory resources,\nand scalable routing. To alleviate this, we present a collection of optimizing\ntransformations for HLS, targeting scalable and efficient architectures for\nhigh-performance computing (HPC) applications. We systematically identify\nclasses of transformations (pipelining, scalability, and memory), the\ncharacteristics of their effect on the HLS code and the resulting hardware\n(e.g., increasing data reuse or resource consumption), and the objectives that\neach transformation can target (e.g., resolve interface contention, or increase\nparallelism). We show how these can be used to efficiently exploit pipelining,\non-chip distributed fast memory, and on-chip dataflow, allowing for massively\nparallel architectures. To quantify the effect of various transformations, we\ncover the optimization process of a sample set of HPC kernels, provided as open\nsource reference codes. We aim to establish a common toolbox to guide both\nperformance engineers and compiler engineers in tapping into the performance\npotential offered by spatial computing architectures using HLS.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 20:55:09 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 00:58:25 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 12:25:43 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2019 12:11:04 GMT"}, {"version": "v5", "created": "Tue, 29 Oct 2019 09:19:39 GMT"}, {"version": "v6", "created": "Mon, 23 Nov 2020 14:10:19 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Licht", "Johannes de Fine", ""], ["Besta", "Maciej", ""], ["Meierhans", "Simon", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1805.08309", "submitter": "Xin He", "authors": "Xin He, Liu Ke, Wenyan Lu, Guihai Yan, Xuan Zhang", "title": "AxTrain: Hardware-Oriented Neural Network Training for Approximate\n  Inference", "comments": "In International Symposium on Low Power Electronics and Design\n  (ISLPED) 2018", "journal-ref": null, "doi": "10.1145/3218603.3218643", "report-no": null, "categories": "cs.LG cs.DC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intrinsic error tolerance of neural network (NN) makes approximate\ncomputing a promising technique to improve the energy efficiency of NN\ninference. Conventional approximate computing focuses on balancing the\nefficiency-accuracy trade-off for existing pre-trained networks, which can lead\nto suboptimal solutions. In this paper, we propose AxTrain, a hardware-oriented\ntraining framework to facilitate approximate computing for NN inference.\nSpecifically, AxTrain leverages the synergy between two orthogonal\nmethods---one actively searches for a network parameters distribution with high\nerror tolerance, and the other passively learns resilient weights by\nnumerically incorporating the noise distributions of the approximate hardware\nin the forward pass during the training phase. Experimental results from\nvarious datasets with near-threshold computing and approximation multiplication\nstrategies demonstrate AxTrain's ability to obtain resilient neural network\nparameters and system energy efficiency improvement.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 22:27:14 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["He", "Xin", ""], ["Ke", "Liu", ""], ["Lu", "Wenyan", ""], ["Yan", "Guihai", ""], ["Zhang", "Xuan", ""]]}, {"id": "1805.08327", "submitter": "Arya Mazumdar", "authors": "Raj Kumar Maity, Ankit Singh Rawat and Arya Mazumdar", "title": "Robust Gradient Descent via Moment Encoding with LDPC Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of implementing large-scale gradient descent\nalgorithms in a distributed computing setting in the presence of {\\em\nstraggling} processors. To mitigate the effect of the stragglers, it has been\npreviously proposed to encode the data with an erasure-correcting code and\ndecode at the master server at the end of the computation. We, instead, propose\nto encode the second-moment of the data with a low density parity-check (LDPC)\ncode. The iterative decoding algorithms for LDPC codes have very low\ncomputational overhead and the number of decoding iterations can be made to\nautomatically adjust with the number of stragglers in the system. We show that\nfor a random model for stragglers, the proposed moment encoding based gradient\ndescent method can be viewed as the stochastic gradient descent method. This\nallows us to obtain convergence guarantees for the proposed solution.\nFurthermore, the proposed moment encoding based method is shown to outperform\nthe existing schemes in a real distributed computing setup.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 00:02:47 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 02:47:58 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Maity", "Raj Kumar", ""], ["Rawat", "Ankit Singh", ""], ["Mazumdar", "Arya", ""]]}, {"id": "1805.08332", "submitter": "Hosein Mohammadi Makrani", "authors": "Hosein Mohammadi Makrani", "title": "Storage and Memory Characterization of Data Intensive Workloads for Bare\n  Metal Cloud", "comments": "8 pages, research draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the cost-per-byte of storage systems dramatically decreases, SSDs are\nfinding their ways in emerging cloud infrastructure. Similar trend is happening\nfor main memory subsystem, as advanced DRAM technologies with higher capacity,\nfrequency and number of channels are deploying for cloud-scale solutions\nspecially for non-virtualized environment where cloud subscribers can exactly\nspecify the configuration of underling hardware. Given the performance\nsensitivity of standard workloads to the memory hierarchy parameters, it is\nimportant to understand the role of memory and storage for data intensive\nworkloads. In this paper, we investigate how the choice of DRAM (high-end vs\nlow-end) impacts the performance of Hadoop, Spark, and MPI based Big Data\nworkloads in the presence of different storage types on bare metal cloud.\nThrough a methodical experimental setup, we have analyzed the impact of DRAM\ncapacity, operating frequency, the number of channels, storage type, and\nscale-out factors on the performance of these popular frameworks. Based on\nmicro-architectural analysis, we classified data-intensive workloads into three\ngroups namely I/O bound, compute bound, and memory bound. The characterization\nresults show that neither DRAM capacity, frequency, nor the number of channels\nplay a significant role on the performance of all studied Hadoop workloads as\nthey are mostly I/O bound. On the other hand, our results reveal that iterative\ntasks (e.g. machine learning) in Spark and MPI are benefiting from a high-end\nDRAM in particular high frequency and large number of channels, as they are\nmemory or compute bound. Our results show that using SSD PCIe cannot shift the\nbottleneck from storage to memory, while it can change the workload behavior\nfrom I/O bound to compute bound.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 00:31:10 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 17:41:24 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Makrani", "Hosein Mohammadi", ""]]}, {"id": "1805.08373", "submitter": "Peng Sun", "authors": "Zhenzhen Hui, Peng Sun, Yonggang Wen", "title": "Speeding-up Age Estimation in Intelligent Demographics System via\n  Network Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age estimation is a difficult task which requires the automatic detection and\ninterpretation of facial features. Recently, Convolutional Neural Networks\n(CNNs) have made remarkable improvement on learning age patterns from benchmark\ndatasets. However, for a face \"in the wild\" (from a video frame or Internet),\nthe existing algorithms are not as accurate as for a frontal and neutral face.\nIn addition, with the increasing number of in-the-wild aging data, the\ncomputation speed of existing deep learning platforms becomes another crucial\nissue. In this paper, we propose a high-efficient age estimation system with\njoint optimization of age estimation algorithm and deep learning system.\nCooperated with the city surveillance network, this system can provide age\ngroup analysis for intelligent demographics. First, we build a three-tier fog\ncomputing architecture including an edge, a fog and a cloud layer, which\ndirectly processes age estimation from raw videos. Second, we optimize the age\nestimation algorithm based on CNNs with label distribution and K-L divergence\ndistance embedded in the fog layer and evaluate the model on the latest wild\naging dataset. Experimental results demonstrate that: 1. our system collects\nthe demographics data dynamically at far-distance without contact, and makes\nthe city population analysis automatically; and 2. the age model training has\nbeen speed-up without losing training progress or model quality. To our best\nknowledge, this is the first intelligent demographics system which has\npotential applications in improving the efficiency of smart cities and urban\nliving.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 03:13:18 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Hui", "Zhenzhen", ""], ["Sun", "Peng", ""], ["Wen", "Yonggang", ""]]}, {"id": "1805.08429", "submitter": "Yackolley Amoussou-Guenou", "authors": "Yackolley Amoussou-Guenou (CEA, NPA, LIP6), Antonella Del Pozzo (CEA),\n  Maria Potop-Butucaru (NPA, LIP6, LINCS), Sara Tucci-Piergiovanni (DILS, CEA)", "title": "Correctness and Fairness of Tendermint-core Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tendermint-core blockchains (e.g. Cosmos) are considered today one of the\nmost viable alternatives for the highly energy consuming proof-of-work\nblockchains such as Bitcoin and Ethereum. Their particularity is that they aim\nat offering strong consistency (no forks) in an open system combining two\ningredients (i) a set of validators that generate blocks via a variant of\nPractical Byzantine Fault Tolerant (PBFT) consensus protocol and (ii) a\nselection strategy that dynamically selects nodes to be validators for the next\nblock via a proof-of-stake mechanism. However,the exact assumptions on the\nsystem model under which Tendermint underlying algorithms are correct and the\nexact properties Tendermint verifies have never been formally analyzed. The\ncontribution of this paper is two-fold. First, while formalizing Tendermint\nalgorithms we precisely characterize the system model and the exact problem\nsolved by Tendermint. We prove that in eventual synchronous systems a modified\nversion of Tendermint solves (i) under additional assumptions, a variant of\none-shot consensus for the validation of one single block and (ii) a variant of\nthe repeated consensus problem for multiple blocks. These results hold even if\nthe set of validators is hit by Byzantine failures, provided that for each\none-shot consensus instance less than one third of the validators is Byzantine.\nOur second contribution relates to the fairness of the rewarding mechanism. It\nis common knowledge that in permisionless blockchain systems the main threat is\nthe tragedy of commons that may yield the system to collapse if the rewarding\nmechanism is not adequate. Ad minimum the rewarding mechanism must be fair,\ni.e.distributing the rewards in proportion to the merit of participants. We\nprove, for the first time in blockchain systems, that in repeated-consensus\nbased blockchains there exists an (eventual) fair rewarding mechanism if and\nonly if the system is (eventual) synchronous. We also show that the original\nTendermint rewarding is not fair, however, a modification of the original\nprotocol makes it eventually fair.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 07:39:28 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 10:05:51 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 11:53:04 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Amoussou-Guenou", "Yackolley", "", "CEA, NPA, LIP6"], ["Del Pozzo", "Antonella", "", "CEA"], ["Potop-Butucaru", "Maria", "", "NPA, LIP6, LINCS"], ["Tucci-Piergiovanni", "Sara", "", "DILS, CEA"]]}, {"id": "1805.08430", "submitter": "Ming Wu", "authors": "Jilong Xue, Youshan Miao, Cheng Chen, Ming Wu, Lintao Zhang, Lidong\n  Zhou", "title": "RPC Considered Harmful: Fast Distributed Deep Learning on RDMA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning emerges as an important new resource-intensive workload and has\nbeen successfully applied in computer vision, speech, natural language\nprocessing, and so on. Distributed deep learning is becoming a necessity to\ncope with growing data and model sizes. Its computation is typically\ncharacterized by a simple tensor data abstraction to model multi-dimensional\nmatrices, a data-flow graph to model computation, and iterative executions with\nrelatively frequent synchronizations, thereby making it substantially different\nfrom Map/Reduce style distributed big data computation.\n  RPC, commonly used as the communication primitive, has been adopted by\npopular deep learning frameworks such as TensorFlow, which uses gRPC. We show\nthat RPC is sub-optimal for distributed deep learning computation, especially\non an RDMA-capable network. The tensor abstraction and data-flow graph, coupled\nwith an RDMA network, offers the opportunity to reduce the unnecessary overhead\n(e.g., memory copy) without sacrificing programmability and generality. In\nparticular, from a data access point of view, a remote machine is abstracted\njust as a \"device\" on an RDMA channel, with a simple memory interface for\nallocating, reading, and writing memory regions. Our graph analyzer looks at\nboth the data flow graph and the tensors to optimize memory allocation and\nremote data access using this interface. The result is up to 25 times speedup\nin representative deep learning benchmarks against the standard gRPC in\nTensorFlow and up to 169% improvement even against an RPC implementation\noptimized for RDMA, leading to faster convergence in the training process.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 07:42:33 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Xue", "Jilong", ""], ["Miao", "Youshan", ""], ["Chen", "Cheng", ""], ["Wu", "Ming", ""], ["Zhang", "Lintao", ""], ["Zhou", "Lidong", ""]]}, {"id": "1805.08469", "submitter": "Gilles Louppe", "authors": "Joeri Hermans, Gilles Louppe", "title": "Gradient Energy Matching for Distributed Asynchronous Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed asynchronous SGD has become widely used for deep learning in\nlarge-scale systems, but remains notorious for its instability when increasing\nthe number of workers. In this work, we study the dynamics of distributed\nasynchronous SGD under the lens of Lagrangian mechanics. Using this\ndescription, we introduce the concept of energy to describe the optimization\nprocess and derive a sufficient condition ensuring its stability as long as the\ncollective energy induced by the active workers remains below the energy of a\ntarget synchronous process. Making use of this criterion, we derive a stable\ndistributed asynchronous optimization procedure, GEM, that estimates and\nmaintains the energy of the asynchronous system below or equal to the energy of\nsequential SGD with momentum. Experimental results highlight the stability and\nspeedup of GEM compared to existing schemes, even when scaling to one hundred\nasynchronous workers. Results also indicate better generalization compared to\nthe targeted SGD with momentum.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 09:32:21 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Hermans", "Joeri", ""], ["Louppe", "Gilles", ""]]}, {"id": "1805.08531", "submitter": "Raphael Berthier", "authors": "Rapha\\\"el Berthier (SIERRA), Francis Bach (SIERRA), Pierre Gaillard\n  (SIERRA)", "title": "Accelerated Gossip in Networks of Given Dimension using Jacobi\n  Polynomial Iterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a network of agents connected by communication links, where each\nagent holds a real value. The gossip problem consists in estimating the average\nof the values diffused in the network in a distributed manner. We develop a\nmethod solving the gossip problem that depends only on the spectral dimension\nof the network, that is, in the communication network set-up, the dimension of\nthe space in which the agents live. This contrasts with previous work that\nrequired the spectral gap of the network as a parameter, or suffered from slow\nmixing. Our method shows an important improvement over existing algorithms in\nthe non-asymptotic regime, i.e., when the values are far from being fully mixed\nin the network. Our approach stems from a polynomial-based point of view on\ngossip algorithms, as well as an approximation of the spectral measure of the\ngraphs with a Jacobi measure. We show the power of the approach with\nsimulations on various graphs, and with performance guarantees on graphs of\nknown spectral dimension, such as grids and random percolation bonds. An\nextension of this work to distributed Laplacian solvers is discussed. As a side\nresult, we also use the polynomial-based point of view to show the convergence\nof the message passing algorithm for gossip of Moallemi \\& Van Roy on regular\ngraphs. The explicit computation of the rate of the convergence shows that\nmessage passing has a slow rate of convergence on graphs with small spectral\ngap.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 12:02:26 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 14:23:15 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 07:41:05 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 09:04:43 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Berthier", "Rapha\u00ebl", "", "SIERRA"], ["Bach", "Francis", "", "SIERRA"], ["Gaillard", "Pierre", "", "SIERRA"]]}, {"id": "1805.08541", "submitter": "Marcus Brandenburger", "authors": "Marcus Brandenburger, Christian Cachin, R\\\"udiger Kapitza, Alessandro\n  Sorniotti", "title": "Blockchain and Trusted Computing: Problems, Pitfalls, and a Solution for\n  Hyperledger Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A smart contract on a blockchain cannot keep a secret because its data is\nreplicated on all nodes in a network. To remedy this problem, it has been\nsuggested to combine blockchains with trusted execution environments (TEEs),\nsuch as Intel SGX, for executing applications that demand privacy. Untrusted\nblockchain nodes cannot get access to the data and computations inside the TEE.\n  This paper first explores some pitfalls that arise from the combination of\nTEEs with blockchains. Since TEEs are, in principle, stateless they are\nsusceptible to rollback attacks, which should be prevented to maintain privacy\nfor the application. However, in blockchains with non-final consensus\nprotocols, such as the proof-of-work in Ethereum and others, the contract\nexecution must handle rollbacks by design. This implies that TEEs for securing\nblockchain execution cannot be directly used for such blockchains; this\napproach works only when the consensus decisions are final.\n  Second, this work introduces an architecture and a prototype for\nsmart-contract execution within Intel SGX technology for Hyperledger Fabric, a\nprominent platform for enterprise blockchain applications. Our system resolves\ndifficulties posed by the execute-order-validate architecture of Fabric and\nprevents rollback attacks on TEE-based execution as far as possible. For\nincreasing security, our design encapsulates each application on the blockchain\nwithin its own enclave that shields it from the host system. An evaluation\nshows that the overhead moving execution into SGX is within 10%-20% for a\nsealed-bid auction application.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 12:34:15 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Brandenburger", "Marcus", ""], ["Cachin", "Christian", ""], ["Kapitza", "R\u00fcdiger", ""], ["Sorniotti", "Alessandro", ""]]}, {"id": "1805.08598", "submitter": "Ying Mao", "authors": "Ying Mao, Jenna Oak, Anthony Pompili, Daniel Beer, Tao Han, Peizhao Hu", "title": "DRAPS: Dynamic and Resource-Aware Placement Scheme for Docker Containers\n  in a Heterogeneous Cluster", "comments": "The 36th IEEE International Performance Computing and Communications\n  Conference(IPCCC'2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization is a promising technology that has facilitated cloud computing\nto become the next wave of the Internet revolution. Adopted by data centers,\nmillions of applications that are powered by various virtual machines improve\nthe quality of services. Although virtual machines are well-isolated among each\nother, they suffer from redundant boot volumes and slow provisioning time. To\naddress limitations, containers were born to deploy and run distributed\napplications without launching entire virtual machines. As a dominant player,\nDocker is an open-source implementation of container technology. When managing\na cluster of Docker containers, the management tool, Swarmkit, does not take\nthe heterogeneities in both physical nodes and virtualized containers into\nconsideration. The heterogeneity lies in the fact that different nodes in the\ncluster may have various configurations, concerning resource types and\navailabilities, etc., and the demands generated by services are varied, such as\nCPU-intensive (e.g. Clustering services) as well as memory-intensive (e.g. Web\nservices). In this paper, we target on investigating the Docker container\ncluster and developed, DRAPS, a resource-aware placement scheme to boost the\nsystem performance in a heterogeneous cluster.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:18:46 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Mao", "Ying", ""], ["Oak", "Jenna", ""], ["Pompili", "Anthony", ""], ["Beer", "Daniel", ""], ["Han", "Tao", ""], ["Hu", "Peizhao", ""]]}, {"id": "1805.08639", "submitter": "Manuel P\\\"oter MSc", "authors": "Manuel P\\\"oter, Jesper Larsson Tr\\\"aff", "title": "Stamp-it: A more Thread-efficient, Concurrent Memory Reclamation Scheme\n  in the C++ Memory Model", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.06134", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Stamp-it, a new, concurrent, lock-less memory reclamation scheme\nwith amortized, constant-time (thread-count independent) reclamation overhead.\nStamp-it has been implemented and proved correct in the C++ memory model using\nas weak memory-consistency assumptions as possible. We have likewise\n(re)implemented six other comparable reclamation schemes. We give a detailed\nperformance comparison, showing that Stamp-it performs favorably (sometimes\nbetter, at least as good as) than most of these other schemes while being able\nto reclaim free memory nodes earlier.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 17:05:37 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 21:35:55 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["P\u00f6ter", "Manuel", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1805.08645", "submitter": "Deniz Ozsoyeller", "authors": "Deniz Ozsoyeller", "title": "Multi-robot Symmetric Rendezvous Search on the Line with an Unknown\n  Initial Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the symmetric rendezvous search problem on the line\nwith n > 2 robots that are unaware of their locations and the initial distances\nbetween them. In the symmetric version of this problem, the robots execute the\nsame strategy. The multi-robot symmetric rendezvous algorithm, MSR presented in\nthis paper is an extension our symmetric rendezvous algorithm, SR presented in\n[23]. We study both the synchronous and asynchronous cases of the problem. The\nasynchronous version of MSR algorithm is called MASR algorithm. We consider\nthat robots start executing MASR at different times. We perform the theoretical\nanalysis of MSR and MASR, and show that their competitive ratios are\n$O(n^{0.67})$ and $O(n^{1.5})$, respectively. Finally, we confirm our\ntheoretical results through simulations.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 08:05:24 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ozsoyeller", "Deniz", ""]]}, {"id": "1805.08650", "submitter": "Damiano Carra", "authors": "Pietro Michiardi, Damiano Carra, Sara Migliorini", "title": "Cache-based Multi-query Optimization for Data-intensive Scalable\n  Computing Frameworks", "comments": "12 pages + references, extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern large-scale distributed systems, analytics jobs submitted by\nvarious users often share similar work, for example scanning and processing the\nsame subset of data. Instead of optimizing jobs independently, which may result\nin redundant and wasteful processing, multi-query optimization techniques can\nbe employed to save a considerable amount of cluster resources. In this work,\nwe introduce a novel method combining in-memory cache primitives and\nmulti-query optimization, to improve the efficiency of data-intensive, scalable\ncomputing frameworks. By careful selection and exploitation of common\n(sub)expressions, while satisfying memory constraints, our method transforms a\nbatch of queries into a new, more efficient one which avoids unnecessary\nrecomputations. To find feasible and efficient execution plans, our method uses\na cost-based optimization formulation akin to the multiple-choice knapsack\nproblem. Extensive experiments on a prototype implementation of our system show\nsignificant benefits of worksharing for both TPC-DS workloads and detailed\nmicro-benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:59:02 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Michiardi", "Pietro", ""], ["Carra", "Damiano", ""], ["Migliorini", "Sara", ""]]}, {"id": "1805.08755", "submitter": "Alexandros A. Voudouris", "authors": "Adelina Madhja, Sotiris Nikoletseas, Alexandros A. Voudouris", "title": "Energy-aware tree network formation among computationally weak nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of distributed network formation among\nmobile agents of limited computational power that aim to achieve energy balance\nby wirelessly transmitting and receiving energy in a peer-to-peer manner.\nSpecifically, we design simple distributed protocols consisting of a small\nnumber of states and interaction rules for the formation of arbitrary and k-ary\ntree networks. Furthermore, we evaluate (theoretically and also using computer\nsimulations) a plethora of energy redistribution protocols that exploit\ndifferent levels of knowledge in order to achieve desired energy distributions\namong the agents which require that every agent has exactly or at least twice\nthe energy of the agents of higher depth, according to the structure of the\nnetwork. Our study shows that without using any knowledge about the network\nstructure, such energy distributions cannot be achieved in a timely manner,\nmeaning that there might be high energy loss during the redistribution process.\nOn the other hand, only a few extra bits of information seem to be enough to\nguarantee quick convergence to energy distributions that satisfy particular\nproperties, yielding low energy loss.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:35:47 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 16:39:35 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Madhja", "Adelina", ""], ["Nikoletseas", "Sotiris", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "1805.08768", "submitter": "Felix Sattler", "authors": "Felix Sattler, Simon Wiedemann, Klaus-Robert M\\\"uller, Wojciech Samek", "title": "Sparse Binary Compression: Towards Distributed Deep Learning with\n  minimal Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, progressively larger deep neural networks are trained on ever\ngrowing data corpora. As this trend is only going to increase in the future,\ndistributed training schemes are becoming increasingly relevant. A major issue\nin distributed training is the limited communication bandwidth between\ncontributing nodes or prohibitive communication cost in general. These\nchallenges become even more pressing, as the number of computation nodes\nincreases. To counteract this development we propose sparse binary compression\n(SBC), a compression framework that allows for a drastic reduction of\ncommunication cost for distributed training. SBC combines existing techniques\nof communication delay and gradient sparsification with a novel binarization\nmethod and optimal weight update encoding to push compression gains to new\nlimits. By doing so, our method also allows us to smoothly trade-off gradient\nsparsity and temporal sparsity to adapt to the requirements of the learning\ntask. Our experiments show, that SBC can reduce the upstream communication on a\nvariety of convolutional and recurrent neural network architectures by more\nthan four orders of magnitude without significantly harming the convergence\nspeed in terms of forward-backward passes. For instance, we can train ResNet50\non ImageNet in the same number of iterations to the baseline accuracy, using\n$\\times 3531$ less bits or train it to a $1\\%$ lower accuracy using $\\times\n37208$ less bits. In the latter case, the total upstream communication required\nis cut from 125 terabytes to 3.35 gigabytes for every participating client.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:54:13 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Sattler", "Felix", ""], ["Wiedemann", "Simon", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1805.08804", "submitter": "Muhammad Samir Khan", "authors": "Russell L. Jones, Muhammad S. Khan, and Nitin H. Vaidya", "title": "Optimal Record and Replay under Causal Consistency", "comments": "Added a new RnR model and results for that model. Also added some\n  text for better reading and some references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the minimum record needed to replay executions of processes\nthat share causally consistent memory. For a version of causal consistency, we\nidentify optimal records under both offline and online recording setting. Under\nthe offline setting, a central authority has information about every process'\nview of the execution and can decide what information to record for each\nprocess. Under the online setting, each process has to decide on the record at\nruntime as the operations are observed.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 18:16:08 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 19:48:55 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Jones", "Russell L.", ""], ["Khan", "Muhammad S.", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1805.08893", "submitter": "Michael Kenzel", "authors": "Michael Kenzel, Bernhard Kerbl, Wolfgang Tatzgern, Elena Ivanchenko,\n  Dieter Schmalstieg, Markus Steinberger", "title": "On-the-fly Vertex Reuse for Massively-Parallel Software Geometry\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compute-mode rendering is becoming more and more attractive for non-standard\nrendering applications, due to the high flexibility of compute-mode execution.\nThese newly designed pipelines often include streaming vertex and geometry\nprocessing stages. In typical triangle meshes, the same transformed vertex is\non average required six times during rendering. To avoid redundant computation,\na post-transform cache is traditionally suggested to enable reuse of vertex\nprocessing results. However, traditional caching neither scales well as the\nhardware becomes more parallel, nor can be efficiently implemented in a\nsoftware design. We investigate alternative strategies to reusing vertex\nshading results on-the-fly for massively parallel software geometry processing.\nForming static and dynamic batching on the data input stream, we analyze the\neffectiveness of identifying potential local reuse based on sorting, hashing,\nand efficient intra-thread-group communication. Altogether, we present four\nvertex reuse strategies, tailored to modern parallel architectures. Our\nsimulations showcase that our batch-based strategies significantly outperform\nparallel caches in terms of reuse. On actual GPU hardware, our evaluation shows\nthat our strategies not only lead to good reuse of processing results, but also\nboost performance by $2-3\\times$ compared to na\\\"ively ignoring reuse in a\nvariety of practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 22:40:07 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Kenzel", "Michael", ""], ["Kerbl", "Bernhard", ""], ["Tatzgern", "Wolfgang", ""], ["Ivanchenko", "Elena", ""], ["Schmalstieg", "Dieter", ""], ["Steinberger", "Markus", ""]]}, {"id": "1805.08984", "submitter": "Sok Kyongjin", "authors": "Won-je Kim, Song-il Cha, Kyong-Jin Sok", "title": "On the Formal Model for IEC 61499 Composite Function Blocks", "comments": "14pags", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The applications for IEC 61499 that is standard architecture for developing\nthe applications of distributed control and measurement in factory automation,\nhave the connected structure of the graphical elements called BFB(basic\nfunction block), SIFB(service interface function block) and CFB(composite\nfunction block). The research on the composite function block has been regarded\nas important issues in implementing hierarchy, multi-functionality and\nsimplicity of software. Nowadays many researchers have been investigated\nIEC61499 in the fields of the software modeling composed of basic function\nblock and service interface function block, the transformation from IEC61131 to\nIEC61499 and syntactic extension of ECC of basic function block. However, work\nrelated to the mathematical modeling for IEC61499 composite function block\nusing in designing software with hierarchical structure is still lacking. This\npaper presents the mathematical model for the structure and execution analysis\nof IEC 61499 composite function blocks by using notation of the set theory.\nAlso a subaplication configuration algorithm is suggested for the\nsubapplication corresponding to the composite function block. Then its\neffectiveness through the computation experiment of several distributed control\napplications is shown. The proposed model can be used effectively as a basis\nfor analyzing a runtime environment of a software tool for designing and\ndeveloping the applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 07:14:51 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Kim", "Won-je", ""], ["Cha", "Song-il", ""], ["Sok", "Kyong-Jin", ""]]}, {"id": "1805.09018", "submitter": "Yehia Elkhatib PhD", "authors": "Abdessalam Elhabbash, Faiza Samreen, James Hadley, Yehia Elkhatib", "title": "Cloud Brokerage: A Systematic Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The proliferation of cloud providers and provisioning levels has\nopened a space for cloud brokerage services. Brokers intermediate between cloud\ncustomers and providers to assist the customer in selecting the most suitable\ncloud service, helping to manage the dimensionality, heterogeneity, and\nuncertainty associated with cloud services. Objective: This paper identifies\nand classifies approaches to realise cloud brokerage. By doing so, this paper\npresents an understanding of the state of the art and a novel taxonomy to\ncharacterise cloud brokers. Method: We conducted a systematic literature survey\nto compile studies related to cloud brokerage and explore how cloud brokers are\nengineered. We analysed the studies from multiple perspectives, such as\nmotivation, functionality, engineering approach, and evaluation methodology.\nResults: The survey resulted in a knowledge base of current proposals for\nrealising cloud brokers. The survey identified surprising differences between\nthe studies' implementations, with engineering efforts directed at combinations\nof market-based solutions, middlewares, toolkits, algorithms, semantic\nframeworks, and conceptual frameworks. Conclusion: Our comprehensive\nmeta-analysis shows that cloud brokerage is still a formative field. There is\nno doubt that progress has been achieved in the field but considerable\nchallenges remain to be addressed. This survey identifies such challenges and\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 09:15:57 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Elhabbash", "Abdessalam", ""], ["Samreen", "Faiza", ""], ["Hadley", "James", ""], ["Elkhatib", "Yehia", ""]]}, {"id": "1805.09246", "submitter": "Jie Xu", "authors": "Jie Xu", "title": "Memory efficient distributed sliding super point cardinality estimation\n  by GPU", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.11036", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super point is a kind of special host in the network which contacts with huge\nof other hosts. Estimating its cardinality, the number of other hosts\ncontacting with it, plays important roles in network management. But all of\nexisting works focus on discrete time window super point cardinality estimation\nwhich has great latency and ignores many measuring periods. Sliding time window\nmeasures super point cardinality in a finer granularity than that of discrete\ntime window but also more complex. This paper firstly introduces an algorithm\nto estimate super point cardinality under sliding time window from distributed\nedge routers. This algorithm's ability of sliding super point cardinality\nestimating comes from a novel method proposed in this paper which can record\nthe time that a host appears. Based on this method, two sliding cardinality\nestimators, sliding rough estimator and sliding linear estimator, are devised\nfor super points detection and their cardinalities estimation separately. When\nusing these two estimators together, the algorithm consumes the smallest memory\nwith the highest accuracy. This sliding super point cardinality algorithm can\nbe deployed in distributed environment and acquire the global super points'\ncardinality by merging estimators of distributed nodes. Both of these\nestimators could process packets parallel which makes it becom possible to deal\nwith high speed network in real time by GPU. Experiments on a real world\ntraffic show that this algorithm have the highest accuracy and the smallest\nmemory comparing with others when running under discrete time window. Under\nsliding time window, this algorithm also has the same performance as under\ndiscrete time window.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 07:38:55 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Xu", "Jie", ""]]}, {"id": "1805.09266", "submitter": "Trong Nghia Hoang", "authors": "Trong Nghia Hoang, Quang Minh Hoang, Kian Hsiang Low and Jonathan How", "title": "Collective Online Learning of Gaussian Processes in Massive Multi-Agent\n  Systems", "comments": "Extended version with proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning (ML) is a modern computation paradigm that\ndivides its workload into independent tasks that can be simultaneously achieved\nby multiple machines (i.e., agents) for better scalability. However, a typical\ndistributed system is usually implemented with a central server that collects\ndata statistics from multiple independent machines operating on different\nsubsets of data to build a global analytic model. This centralized\ncommunication architecture however exposes a single choke point for operational\nfailure and places severe bottlenecks on the server's communication and\ncomputation capacities as it has to process a growing volume of communication\nfrom a crowd of learning agents. To mitigate these bottlenecks, this paper\nintroduces a novel Collective Online Learning Gaussian Process framework for\nmassive distributed systems that allows each agent to build its local model,\nwhich can be exchanged and combined efficiently with others via peer-to-peer\ncommunication to converge on a global model of higher quality. Finally, our\nempirical results consistently demonstrate the efficiency of our framework on\nboth synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 16:26:41 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 20:08:05 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hoang", "Trong Nghia", ""], ["Hoang", "Quang Minh", ""], ["Low", "Kian Hsiang", ""], ["How", "Jonathan", ""]]}, {"id": "1805.09470", "submitter": "Xin Zhang", "authors": "Xin Zhang, Jia Liu, Zhengyuan Zhu", "title": "Taming Convergence for Asynchronous Stochastic Gradient Descent with\n  Unbounded Delay in Non-Convex Learning", "comments": "2020 IEEE 59th Conference on Decision and Control (CDC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the convergence performance of asynchronous stochastic gradient\ndescent method (Async-SGD) has received increasing attention in recent years\ndue to their foundational role in machine learning. To date, however, most of\nthe existing works are restricted to either bounded gradient delays or convex\nsettings. In this paper, we focus on Async-SGD and its variant Async-SGDI\n(which uses increasing batch size) for non-convex optimization problems with\nunbounded gradient delays. We prove $o(1/\\sqrt{k})$ convergence rate for\nAsync-SGD and $o(1/k)$ for Async-SGDI. Also, a unifying sufficient condition\nfor Async-SGD's convergence is established, which includes two major gradient\ndelay models in the literature as special cases and yields a new delay model\nnot considered thus far.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 01:10:36 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 17:57:22 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zhang", "Xin", ""], ["Liu", "Jia", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1805.09495", "submitter": "Jiecao Chen", "authors": "Jiecao Chen, Erfan Sadeqi Azer, Qin Zhang", "title": "A Practical Algorithm for Distributed Clustering and Outlier Detection", "comments": "Accepted to NIPS 2018. 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic $k$-means/median clustering, which are fundamental\nproblems in unsupervised learning, in the setting where data are partitioned\nacross multiple sites, and where we are allowed to discard a small portion of\nthe data by labeling them as outliers. We propose a simple approach based on\nconstructing small summary for the original dataset. The proposed method is\ntime and communication efficient, has good approximation guarantees, and can\nidentify the global outliers effectively. To the best of our knowledge, this is\nthe first practical algorithm with theoretical guarantees for distributed\nclustering with outliers. Our experiments on both real and synthetic data have\ndemonstrated the clear superiority of our algorithm against all the baseline\nalgorithms in almost all metrics.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 03:00:55 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 00:56:10 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Chen", "Jiecao", ""], ["Azer", "Erfan Sadeqi", ""], ["Zhang", "Qin", ""]]}, {"id": "1805.09675", "submitter": "Jeremy Kepner", "authors": "Siddharth Samsi, Vijay Gadepally, Michael Hurley, Michael Jones,\n  Edward Kao, Sanjeev Mohindra, Paul Monticciolo, Albert Reuther, Steven Smith,\n  William Song, Diane Staheli, Jeremy Kepner", "title": "GraphChallenge.org: Raising the Bar on Graph Analytic Performance", "comments": "7 pages, 6 figures; submitted to IEEE HPEC Graph Challenge. arXiv\n  admin note: text overlap with arXiv:1708.06866", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547527", "report-no": null, "categories": "cs.DC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of graph analytic systems has created a need for new ways to measure\nand compare the capabilities of graph processing systems. The MIT/Amazon/IEEE\nGraph Challenge has been developed to provide a well-defined community venue\nfor stimulating research and highlighting innovations in graph analysis\nsoftware, hardware, algorithms, and systems. GraphChallenge.org provides a wide\nrange of pre-parsed graph data sets, graph generators, mathematically defined\ngraph algorithms, example serial implementations in a variety of languages, and\nspecific metrics for measuring performance. Graph Challenge 2017 received 22\nsubmissions by 111 authors from 36 organizations. The submissions highlighted\ngraph analytic innovations in hardware, software, algorithms, systems, and\nvisualization. These submissions produced many comparable performance\nmeasurements that can be used for assessing the current state of the art of the\nfield. There were numerous submissions that implemented the triangle counting\nchallenge and resulted in over 350 distinct measurements. Analysis of these\nsubmissions show that their execution time is a strong function of the number\nof edges in the graph, $N_e$, and is typically proportional to $N_e^{4/3}$ for\nlarge values of $N_e$. Combining the model fits of the submissions presents a\npicture of the current state of the art of graph analysis, which is typically\n$10^8$ edges processed per second for graphs with $10^8$ edges. These results\nare $30$ times faster than serial implementations commonly used by many graph\nanalysts and underscore the importance of making these performance benefits\navailable to the broader community. Graph Challenge provides a clear picture of\ncurrent graph analysis systems and underscores the need for new innovations to\nachieve high performance on very large graphs.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 01:18:37 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Samsi", "Siddharth", ""], ["Gadepally", "Vijay", ""], ["Hurley", "Michael", ""], ["Jones", "Michael", ""], ["Kao", "Edward", ""], ["Mohindra", "Sanjeev", ""], ["Monticciolo", "Paul", ""], ["Reuther", "Albert", ""], ["Smith", "Steven", ""], ["Song", "William", ""], ["Staheli", "Diane", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1805.09682", "submitter": "Cong Xie", "authors": "Cong Xie, Oluwasanmi Koyejo, Indranil Gupta", "title": "Phocas: dimensional Byzantine-resilient stochastic gradient descent", "comments": "Submitted to NIPS 2018. arXiv admin note: substantial text overlap\n  with arXiv:1802.10116", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel robust aggregation rule for distributed synchronous\nStochastic Gradient Descent~(SGD) under a general Byzantine failure model. The\nattackers can arbitrarily manipulate the data transferred between the servers\nand the workers in the parameter server~(PS) architecture. We prove the\nByzantine resilience of the proposed aggregation rules. Empirical analysis\nshows that the proposed techniques outperform current approaches for realistic\nuse cases and Byzantine attack scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 09:25:48 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Oluwasanmi", ""], ["Gupta", "Indranil", ""]]}, {"id": "1805.09687", "submitter": "Michele Dolfi", "authors": "Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas", "title": "Corpus Conversion Service: A machine learning platform to ingest\n  documents at scale [Poster abstract]", "comments": "Accepted in SysML 2018 (www.sysml.cc)", "journal-ref": null, "doi": "10.13140/RG.2.2.10858.82888", "report-no": null, "categories": "cs.DL cs.CL cs.CV cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make their\ncontent discoverable. Unfortunately, both the format of these documents (e.g.\nthe PDF format or bitmap images) as well as the presentation of the data (e.g.\ncomplex tables) make the extraction of qualitative and quantitive data\nextremely challenging. We present a platform to ingest documents at scale which\nis powered by Machine Learning techniques and allows the user to train custom\nmodels on document collections. We show precision/recall results greater than\n97% with regard to conversion to structured formats, as well as scaling\nevidence for each of the microservices constituting the platform.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 07:05:52 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Staar", "Peter W J", ""], ["Dolfi", "Michele", ""], ["Auer", "Christoph", ""], ["Bekas", "Costas", ""]]}, {"id": "1805.09767", "submitter": "Sebastian U. Stich", "authors": "Sebastian U. Stich", "title": "Local SGD Converges Fast and Communicates Little", "comments": "to appear at ICLR 2019, 19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mini-batch stochastic gradient descent (SGD) is state of the art in large\nscale distributed training. The scheme can reach a linear speedup with respect\nto the number of workers, but this is rarely seen in practice as the scheme\noften suffers from large network delays and bandwidth limits. To overcome this\ncommunication bottleneck recent works propose to reduce the communication\nfrequency. An algorithm of this type is local SGD that runs SGD independently\nin parallel on different workers and averages the sequences only once in a\nwhile.\n  This scheme shows promising results in practice, but eluded thorough\ntheoretical analysis. We prove concise convergence rates for local SGD on\nconvex problems and show that it converges at the same rate as mini-batch SGD\nin terms of number of evaluated gradients, that is, the scheme achieves linear\nspeedup in the number of workers and mini-batch size. The number of\ncommunication rounds can be reduced up to a factor of T^{1/2}---where T denotes\nthe number of total steps---compared to mini-batch SGD. This also holds for\nasynchronous implementations. Local SGD can also be used for large scale\ntraining of deep learning models.\n  The results shown here aim serving as a guideline to further explore the\ntheoretical and practical aspects of local SGD in these applications.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 16:38:51 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 08:19:47 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 12:58:04 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Stich", "Sebastian U.", ""]]}, {"id": "1805.09861", "submitter": "Yaniv Tzur", "authors": "Leonid Barenboim and Yaniv Tzur", "title": "Distributed Symmetry-Breaking with Improved Vertex-Averaged Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distributed message-passing model in which a communication\nnetwork is represented by a graph G=(V,E). Usually, the measure of complexity\nthat is considered in this model is the worst-case complexity, which is the\nlargest number of rounds performed by a vertex v\\in V. While often this is a\nreasonable measure, in some occasions it does not express sufficiently well the\nactual performance of the algorithm. For example, an execution in which one\nprocessor performs r rounds, and all the rest perform significantly less rounds\nthan r, has the same running time as an execution in which all processors\nperform the same number of rounds r. On the other hand, the latter execution is\nless efficient in several respects, such as energy efficiency, task execution\nefficiency, local-neighborhood efficiency and simulation efficiency.\nConsequently, a more appropriate measure is required in these cases. Recently,\nthe vertex-averaged complexity was proposed by \\cite{Feuilloley2017}. In this\nmeasure, the running time is the worst-case sum of rounds of communication\nperformed by all of the graph's vertices, averaged over the number of vertices.\nFeuilloley \\cite{Feuilloley2017} showed that leader-election admits an\nalgorithm with a vertex-averaged complexity significantly better than its\nworst-case complexity. On the other hand, for O(1)-coloring of rings, the\nworst-case and vertex-averaged complexities are the same. This complexity is\nO\\left(\\log^{*}n\\right) [12]. It remained open whether the vertex-averaged\ncomplexity of symmetry-breaking in general graphs can be better than the\nworst-case complexity. We answer this question in the affirmative, by showing a\nnumber of results with improved vertex-averaged complexity.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 19:28:08 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 22:32:25 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Barenboim", "Leonid", ""], ["Tzur", "Yaniv", ""]]}, {"id": "1805.09891", "submitter": "Haewon Jeong", "authors": "Haewon Jeong, Tze Meng Low, and Pulkit Grover", "title": "Coded FFT and Its Communication Overhead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a coded computing strategy and examine communication costs of\ncoded computing algorithms to make distributed Fast Fourier Transform (FFT)\nresilient to errors during the computation. We apply maximum distance separable\n(MDS) codes to a widely used \"Transpose\" algorithm for parallel FFT. In the\nuncoded distributed FFT algorithm, the most expensive step is a single\n\"all-to-all\" communication. We compare this with communication overhead of\ncoding in our coded FFT algorithm. We show that by using a systematic MDS code,\nthe communication overhead of coding is negligible in comparison with the\ncommunication costs inherent in an uncoded FFT implementation if the number of\nparity nodes is at most $o(\\log K)$, where $K$ is the number of systematic\nnodes.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 20:46:09 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Jeong", "Haewon", ""], ["Low", "Tze Meng", ""], ["Grover", "Pulkit", ""]]}, {"id": "1805.09934", "submitter": "Songze Li", "authors": "Songze Li, Seyed Mohammadreza Mousavi Kalan, Qian Yu, Mahdi\n  Soltanolkotabi, A. Salman Avestimehr", "title": "Polynomially Coded Regression: Optimal Straggler Mitigation via Data\n  Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training a least-squares regression model on a\nlarge dataset using gradient descent. The computation is carried out on a\ndistributed system consisting of a master node and multiple worker nodes. Such\ndistributed systems are significantly slowed down due to the presence of\nslow-running machines (stragglers) as well as various communication\nbottlenecks. We propose \"polynomially coded regression\" (PCR) that\nsubstantially reduces the effect of stragglers and lessens the communication\nburden in such systems. The key idea of PCR is to encode the partial data\nstored at each worker, such that the computations at the workers can be viewed\nas evaluating a polynomial at distinct points. This allows the master to\ncompute the final gradient by interpolating this polynomial. PCR significantly\nreduces the recovery threshold, defined as the number of workers the master has\nto wait for prior to computing the gradient. In particular, PCR requires a\nrecovery threshold that scales inversely proportionally with the amount of\ncomputation/storage available at each worker. In comparison, state-of-the-art\nstraggler-mitigation schemes require a much higher recovery threshold that only\ndecreases linearly in the per worker computation/storage load. We prove that\nPCR's recovery threshold is near minimal and within a factor two of the best\npossible scheme. Our experiments over Amazon EC2 demonstrate that compared with\nstate-of-the-art schemes, PCR improves the run-time by 1.50x ~ 2.36x with\nnaturally occurring stragglers, and by as much as 2.58x ~ 4.29x with artificial\nstragglers.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 23:59:20 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Li", "Songze", ""], ["Kalan", "Seyed Mohammadreza Mousavi", ""], ["Yu", "Qian", ""], ["Soltanolkotabi", "Mahdi", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1805.09965", "submitter": "Tianyi Chen", "authors": "Tianyi Chen, Georgios B. Giannakis, Tao Sun, Wotao Yin", "title": "LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed\n  Learning", "comments": "Fix a typo in equation (11)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new class of gradient methods for distributed machine\nlearning that adaptively skip the gradient calculations to learn with reduced\ncommunication and computation. Simple rules are designed to detect\nslowly-varying gradients and, therefore, trigger the reuse of outdated\ngradients. The resultant gradient-based algorithms are termed Lazily Aggregated\nGradient --- justifying our acronym LAG used henceforth. Theoretically, the\nmerits of this contribution are: i) the convergence rate is the same as batch\ngradient descent in strongly-convex, convex, and nonconvex smooth cases; and,\nii) if the distributed datasets are heterogeneous (quantified by certain\nmeasurable constants), the communication rounds needed to achieve a targeted\naccuracy are reduced thanks to the adaptive reuse of lagged gradients.\nNumerical experiments on both synthetic and real data corroborate a significant\ncommunication reduction compared to alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 03:36:14 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 07:02:44 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Chen", "Tianyi", ""], ["Giannakis", "Georgios B.", ""], ["Sun", "Tao", ""], ["Yin", "Wotao", ""]]}, {"id": "1805.10032", "submitter": "Cong Xie", "authors": "Cong Xie, Oluwasanmi Koyejo, Indranil Gupta", "title": "Zeno: Distributed Stochastic Gradient Descent with Suspicion-based\n  Fault-tolerance", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Zeno, a technique to make distributed machine learning,\nparticularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number\nof faulty workers. Zeno generalizes previous results that assumed a majority of\nnon-faulty nodes; we need assume only one non-faulty worker. Our key idea is to\nsuspect workers that are potentially defective. Since this is likely to lead to\nfalse positives, we use a ranking-based preference mechanism. We prove the\nconvergence of SGD for non-convex problems under these scenarios. Experimental\nresults show that Zeno outperforms existing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:33:24 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 08:20:57 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 00:52:06 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Oluwasanmi", ""], ["Gupta", "Indranil", ""]]}, {"id": "1805.10041", "submitter": "Adrian Jackson", "authors": "Adrian Jackson, Michele Weiland, Mark Parsons, Bernhard Homoelle", "title": "Architectures for High Performance Computing and Data Systems using\n  Byte-Addressable Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile, byte addressable, memory technology with performance close to\nmain memory promises to revolutionise computing systems in the near future.\nSuch memory technology provides the potential for extremely large memory\nregions (i.e. > 3TB per server), very high performance I/O, and new ways of\nstoring and sharing data for applications and workflows. This paper outlines an\narchitecture that has been designed to exploit such memory for High Performance\nComputing and High Performance Data Analytics systems, along with descriptions\nof how applications could benefit from such hardware.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:53:05 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Jackson", "Adrian", ""], ["Weiland", "Michele", ""], ["Parsons", "Mark", ""], ["Homoelle", "Bernhard", ""]]}, {"id": "1805.10167", "submitter": "Dominik Bartuschat", "authors": "Nils Kohl, Dominik Th\\\"onnes, Daniel Drzisga, Dominik Bartuschat, and\n  Ulrich R\\\"ude", "title": "A Scalable and Modular Software Architecture for Finite Elements on\n  Hierarchical Hybrid Grids", "comments": "Preprint of an article submitted to International Journal of\n  Parallel, Emergent and Distributed Systems (Taylor & Francis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a new generic higher-order finite-element framework for\nmassively parallel simulations is presented. The modular software architecture\nis carefully designed to exploit the resources of modern and future\nsupercomputers. Combining an unstructured topology with structured grid\nrefinement facilitates high geometric adaptability and matrix-free multigrid\nimplementations with excellent performance. Different abstraction levels and\nfully distributed data structures additionally ensure high flexibility,\nextensibility, and scalability. The software concepts support sophisticated\nload balancing and flexibly combining finite element spaces. Example scenarios\nwith coupled systems of PDEs show the applicability of the concepts to\nperforming geophysical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 14:10:59 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Kohl", "Nils", ""], ["Th\u00f6nnes", "Dominik", ""], ["Drzisga", "Daniel", ""], ["Bartuschat", "Dominik", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1805.10351", "submitter": "Christina Delimitrou", "authors": "Yu Gan and Christina Delimitrou", "title": "The Architectural Implications of Microservices in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud services have recently undergone a shift from monolithic applications\nto microservices, with hundreds or thousands of loosely-coupled microservices\ncomprising the end-to-end application. Microservices present both opportunities\nand challenges when optimizing for quality of service (QoS) and cloud\nutilization. In this paper we explore the implications cloud microservices have\non system bottlenecks, and datacenter server design. We first present and\ncharacterize an end-to-end application built using tens of popular open-source\nmicroservices that implements a movie renting and streaming service, and is\nmodular and extensible. We then use the end-to-end service to study the\nscalability and performance bottlenecks of microservices, and highlight\nimplications they have on the design of datacenter hardware. Specifically, we\nrevisit the long-standing debate of brawny versus wimpy cores in the context of\nmicroservices, we quantify the I-cache pressure they introduce, and measure the\ntime spent in computation versus communication between microservices over RPCs.\nAs more cloud applications switch to this new programming model, it is\nincreasingly important to revisit the assumptions we have previously used to\nbuild and manage cloud systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 20:19:50 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Gan", "Yu", ""], ["Delimitrou", "Christina", ""]]}, {"id": "1805.10378", "submitter": "Zachary Charles", "authors": "Zachary Charles, Dimitris Papailiopoulos", "title": "Gradient Coding via the Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent and its many variants, including mini-batch stochastic\ngradient descent, form the algorithmic foundation of modern large-scale machine\nlearning. Due to the size and scale of modern data, gradient computations are\noften distributed across multiple compute nodes. Unfortunately, such\ndistributed implementations can face significant delays caused by straggler\nnodes, i.e., nodes that are much slower than average. Gradient coding is a new\ntechnique for mitigating the effect of stragglers via algorithmic redundancy.\nWhile effective, previously proposed gradient codes can be computationally\nexpensive to construct, inaccurate, or susceptible to adversarial stragglers.\nIn this work, we present the stochastic block code (SBC), a gradient code based\non the stochastic block model. We show that SBCs are efficient, accurate, and\nthat under certain settings, adversarial straggler selection becomes as hard as\ndetecting a community structure in the multiple community, block stochastic\ngraph model.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 22:02:30 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Charles", "Zachary", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "1805.10499", "submitter": "Tevfik Kosar", "authors": "Dengpan Yin and Tevfik Kosar", "title": "Data-Aware Approximate Workflow Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of data placement in complex scientific workflows has become\nvery crucial since the large amounts of data generated by these workflows\nsignificantly increases the turnaround time of the end-to-end application. It\nis almost impossible to make an optimal scheduling for the end-to-end workflow\nwithout considering the intermediate data movement. In order to reduce the\ncomplexity of the workflow-scheduling problem, most of the existing work\nconstrains the problem space by some unrealistic assumptions, which result in\nnon-optimal scheduling in practice. In this study, we propose a genetic\ndata-aware algorithm for the end-to-end workflow scheduling problem. Distinct\nfrom the past research, we develop a novel data-aware evaluation function for\neach chromosome, a common augmenting crossover operator and a simple but\neffective mutation operator. Our experiments on different workflow structures\nshow that the proposed GA based approach gives a scheduling close to the\noptimal one.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 15:41:55 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Yin", "Dengpan", ""], ["Kosar", "Tevfik", ""]]}, {"id": "1805.10654", "submitter": "Ivano Notarnicola", "authors": "Ivano Notarnicola, Ying Sun, Gesualdo Scutari, Giuseppe Notarstefano", "title": "Distributed Big-Data Optimization via Block Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed multi-agent large-scale optimization problems, wherein\nthe cost function is composed of a smooth possibly nonconvex sum-utility plus a\nDC (Difference-of-Convex) regularizer. We consider the scenario where the\ndimension of the optimization variables is so large that optimizing and/or\ntransmitting the entire set of variables could cause unaffordable computation\nand communication overhead. To address this issue, we propose the first\ndistributed algorithm whereby agents optimize and communicate only a portion of\ntheir local variables. The scheme hinges on successive convex approximation\n(SCA) to handle the nonconvexity of the objective function, coupled with a\nnovel block-signal tracking scheme, aiming at locally estimating the average of\nthe agents' gradients. Asymptotic convergence to stationary solutions of the\nnonconvex problem is established. Numerical results on a sparse regression\nproblem show the effectiveness of the proposed algorithm and the impact of the\nblock size on its practical convergence speed and communication cost.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 17:02:26 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Notarnicola", "Ivano", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1805.10657", "submitter": "Moti Medina", "authors": "Reut Levi, Moti Medina, and Dana Ron", "title": "Property Testing of Planarity in the CONGEST model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a distributed algorithm in the {\\sf CONGEST} model for property\ntesting of planarity with one-sided error in general (unbounded-degree) graphs.\nFollowing Censor-Hillel et al. (DISC 2016), who recently initiated the study of\nproperty testing in the distributed setting, our algorithm gives the following\nguarantee: For a graph $G = (V,E)$ and a distance parameter $\\epsilon$, if $G$\nis planar, then every node outputs {\\sf accept\\/}, and if $G$ is $\\epsilon$-far\nfrom being planar (i.e., more than $\\epsilon\\cdot |E|$ edges need to be removed\nin order to make $G$ planar), then with probability $1-1/{\\rm poly}(n)$ at\nleast one node outputs {\\sf reject}. The algorithm runs in $O(\\log|V|\\cdot{\\rm\npoly}(1/\\epsilon))$ rounds, and we show that this result is tight in terms of\nthe dependence on $|V|$.\n  Our algorithm combines several techniques of graph partitioning and local\nverification of planar embeddings. Furthermore, we show how a main subroutine\nin our algorithm can be applied to derive additional results for property\ntesting of cycle-freeness and bipartiteness, as well as the construction of\nspanners, in minor-free (unweighted) graphs.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 17:23:48 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 12:30:10 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Levi", "Reut", ""], ["Medina", "Moti", ""], ["Ron", "Dana", ""]]}, {"id": "1805.10780", "submitter": "Rajkumar Buyya", "authors": "Rajkumar Buyya and Jungmin Son", "title": "Software-Defined Multi-Cloud Computing: A Vision, Architectural\n  Elements, and Future Directions", "comments": "16 pages, 1 figure, 2 tables, Proceedings of the 18th International\n  Conference on Computational Science and Applications (ICCSA 2018, LNCS,\n  Springer, Germany), Melbourne, Australia, July 2-5, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has been emerged in the last decade to enable utility-based\ncomputing resource management without purchasing hardware equipment. Cloud\nproviders run multiple data centers in various locations to manage and\nprovision the Cloud resources to their customers. More recently, the\nintroduction of Software-Defined Networking (SDN) and Network Function\nVirtualization (NFV) opens more opportunities in Clouds which enables dynamic\nand autonomic configuration and provisioning of the resources in Cloud data\ncenters. This paper proposes architectural framework and principles for\nProgrammable Network Clouds hosting SDNs and NFVs for geographically\ndistributed Multi-Cloud computing environments. Cost and SLA-aware resource\nprovisioning and scheduling that minimizes the operating cost without violating\nthe negotiated SLAs are investigated and discussed in regards of techniques for\nautonomic and timely VNF composition, deployment and management across multiple\nClouds. We also discuss open challenges and directions for creating\nauto-scaling solutions for performance optimization of VNFs using analytics and\nmonitoring techniques, algorithms for SDN controller for scalable traffic and\ndeployment management. The simulation platform and the proof-of-concept\nprototype are presented with initial evaluation results.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 05:59:00 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Son", "Jungmin", ""]]}, {"id": "1805.10904", "submitter": "Richard Forster", "authors": "Richard Forster", "title": "Parallel Louvain Community Detection Optimized for GPUs", "comments": "Submitting for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection now is an important operation in numerous graph based\napplications. It is used to reveal groups that exist within real world networks\nwithout imposing prior size or cardinality constraints on the set of\ncommunities. Despite its potential, the support for parallel computers is\nrather limited. The cause is largely the irregularity of the algorithm and the\nunderlying heuristics imply a sequential nature. In this paper a GPU based\nparallelized version of the Louvain method is presented. The Louvain method is\na multi-phase, iterative heuristic for modularity optimization. It was\noriginally developed by Blondel et al. (2008), the method has become\nincreasingly popular owing to its ability to detect high modularity community\npartitions in a fast and memory-efficient manner. The parallel heuristics used,\nwere first introduced by Hao Lu et al. (2015). As the Louvain method is\ninherently sequential, it limits the possibility of scalable usage. Thanks to\nthe proposed parallel heuristics, I observe how this method can behave on GPUs.\nFor evaluation I implemented the heuristics using CUDA on a GeForce GTX 980 GPU\nand for testing Ive used organization landscapes from the CERN developed\nCollaboration Spotting project that involves patents and publications to\nvisualize the connections in technologies among its collaborators. Compared to\nthe parallel Louvain implementation running on 8 threads on the same machine\nthat has the used GPU, the CUDA implementation is able to produce community\noutputs comparable to the CPU generated results, while providing absolute\nspeedups of up to 30 using the GeForce GTX 980 consumer grade GPU.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 13:11:31 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Forster", "Richard", ""]]}, {"id": "1805.11029", "submitter": "Chunlei Liu", "authors": "Chunlei Liu", "title": "Subadditive stake systems", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.11136", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stake systems which issue stakes as well as coins are proposed. Two\nsubadditive stake systems are studied: one is the radical stake system, the\nother is the logarithmic stake system. Securities of both systems are analysed.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 12:31:02 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 16:00:06 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 00:58:44 GMT"}, {"version": "v4", "created": "Mon, 11 Jun 2018 04:23:23 GMT"}, {"version": "v5", "created": "Tue, 12 Jun 2018 14:54:19 GMT"}, {"version": "v6", "created": "Fri, 15 Jun 2018 07:15:49 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Liu", "Chunlei", ""]]}, {"id": "1805.11390", "submitter": "Senthil Nathan N", "authors": "Parth Thakkar, Senthil Nathan, Balaji Vishwanathan", "title": "Performance Benchmarking and Optimizing Hyperledger Fabric Blockchain\n  Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise in popularity of permissioned blockchain platforms in recent time is\nsignificant. Hyperledger Fabric is one such permissioned blockchain platform\nand one of the Hyperledger projects hosted by the Linux Foundation. The Fabric\ncomprises various components such as smart-contracts, endorsers, committers,\nvalidators, and orderers. As the performance of blockchain platform is a major\nconcern for enterprise applications, in this work, we perform a comprehensive\nempirical study to characterize the performance of Hyperledger Fabric and\nidentify potential performance bottlenecks to gain a better understanding of\nthe system. We follow a two-phased approach. In the first phase, our goal is to\nunderstand the impact of various configuration parameters such as block size,\nendorsement policy, channels, resource allocation, state database choice on the\ntransaction throughput & latency to provide various guidelines on configuring\nthese parameters. In addition, we also aim to identify performance bottlenecks\nand hotspots. We observed that (1) endorsement policy verification, (2)\nsequential policy validation of transactions in a block, and (3) state\nvalidation and commit (with CouchDB) were the three major bottlenecks. In the\nsecond phase, we focus on optimizing Hyperledger Fabric v1.0 based on our\nobservations. We introduced and studied various simple optimizations such as\naggressive caching for endorsement policy verification in the cryptography\ncomponent (3x improvement in the performance) and parallelizing endorsement\npolicy verification (7x improvement). Further, we enhanced and measured the\neffect of an existing bulk read/write optimization for CouchDB during state\nvalidation & commit phase (2.5x improvement). By combining all three\noptimizations1, we improved the overall throughput by 16x (i.e., from 140 tps\nto 2250 tps).\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 12:30:37 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Thakkar", "Parth", ""], ["Nathan", "Senthil", ""], ["Vishwanathan", "Balaji", ""]]}, {"id": "1805.11453", "submitter": "Duong Nguyen", "authors": "Duong Nguyen, Aleksey Charapko, Sandeep Kulkarni, Murat Demirbas", "title": "Technical Report: Optimistic Execution in Key-Value Store", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.07319", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limitations of the CAP theorem imply that if availability is desired in the\npresence of network partitions, one must sacrifice sequential consistency, a\nconsistency model that is more natural for system design. We focus on the\nproblem of what a designer should do if he/she has an algorithm that works\ncorrectly with sequential consistency but is faced with an underlying key-value\nstore that provides a weaker (e.g., eventual or causal) consistency. We propose\na detect-rollback based approach: The designer identifies a correctness\npredicate, say $P$, and continues to run the protocol, as our system monitors\n$P$. If $P$ is violated (because the underlying key-value store provides a\nweaker consistency), the system rolls back and resumes the computation at a\nstate where $P$ holds.\n  We evaluate this approach with practical graph applications running on the\nVoldemort key-value store. Our experiments with deployment on Amazon AWS EC2\ninstances shows that using eventual consistency with monitoring can provide a\n$50-80\\%$ increase in throughput when compared with sequential consistency. We\nalso show that the overhead of the monitoring itself is low (typically less\nthan 4\\%) and the latency of detecting violations is small. In particular, more\nthan $99.9\\%$ of violations are detected in less than $50$ milliseconds in\nregional AWS networks, and in less than $5$ seconds in global AWS networks.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 23:19:24 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 00:48:20 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 03:37:01 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Nguyen", "Duong", ""], ["Charapko", "Aleksey", ""], ["Kulkarni", "Sandeep", ""], ["Demirbas", "Murat", ""]]}, {"id": "1805.11454", "submitter": "Shi Pu", "authors": "Shi Pu and Angelia Nedi\\'c", "title": "Distributed Stochastic Gradient Tracking Methods", "comments": "Accepted in Mathematical Programming. This article draws heavily from\n  arXiv:1803.07741 (conference submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of distributed multi-agent optimization\nover a network, where each agent possesses a local cost function that is smooth\nand strongly convex. The global objective is to find a common solution that\nminimizes the average of all cost functions. Assuming agents only have access\nto unbiased estimates of the gradients of their local cost functions, we\nconsider a distributed stochastic gradient tracking method (DSGT) and a\ngossip-like stochastic gradient tracking method (GSGT). We show that, in\nexpectation, the iterates generated by each agent are attracted to a\nneighborhood of the optimal solution, where they accumulate exponentially fast\n(under a constant stepsize choice). Under DSGT, the limiting (expected) error\nbounds on the distance of the iterates from the optimal solution decrease with\nthe network size $n$, which is a comparable performance to a centralized\nstochastic gradient algorithm. Moreover, we show that when the network is\nwell-connected, GSGT incurs lower communication cost than DSGT while\nmaintaining a similar computational cost. Numerical example further\ndemonstrates the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 23:34:48 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 16:53:45 GMT"}, {"version": "v3", "created": "Sat, 8 Sep 2018 20:04:52 GMT"}, {"version": "v4", "created": "Sun, 4 Aug 2019 15:36:47 GMT"}, {"version": "v5", "created": "Tue, 10 Mar 2020 13:15:33 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Pu", "Shi", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1805.11477", "submitter": "Nicolas Kourtellis Ph.D.", "authors": "Nicolas Kourtellis, Gianmarco De Francisci Morales, and Albert Bifet", "title": "Large-Scale Learning from Data Streams with Apache SAMOA", "comments": "31 pages, 7 Tables, 16 Figures, 26 References. arXiv admin note:\n  substantial text overlap with arXiv:1607.08325", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache SAMOA (Scalable Advanced Massive Online Analysis) is an open-source\nplatform for mining big data streams. Big data is defined as datasets whose\nsize is beyond the ability of typical software tools to capture, store, manage,\nand analyze, due to the time and memory complexity. Apache SAMOA provides a\ncollection of distributed streaming algorithms for the most common data mining\nand machine learning tasks such as classification, clustering, and regression,\nas well as programming abstractions to develop new algorithms. It features a\npluggable architecture that allows it to run on several distributed stream\nprocessing engines such as Apache Flink, Apache Storm, and Apache Samza. Apache\nSAMOA is written in Java and is available at https://samoa.incubator.apache.org\nunder the Apache Software License version 2.0.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 13:42:04 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Kourtellis", "Nicolas", ""], ["Morales", "Gianmarco De Francisci", ""], ["Bifet", "Albert", ""]]}, {"id": "1805.11510", "submitter": "Xiaoyi Li", "authors": "Nate Zou, Eric Li, Henry Zhang", "title": "ALZA: An Efficient Hybrid Decentralized Payment System", "comments": "Overlap wording with Hadoop file systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of decentralized book systems like Bitcoin and Ethereum has\nalways been a challenge. It is usually measured by three major factors:\nscalability, throughput, and latency. Scalability refers to how the system\ncapacity is increased by adding more physical resources. Throughput measures\nthe volume of transactions for a given period of time, where most current\nsolutions attempt to improve such as NEO, EOS, etc. Latency measures the\nprocessing time of any single transaction. In current blockchain based systems,\nthe block generation rate is the main latency bottleneck. Off-chain processes\nsuch as state channels are the most recent work that can integrate partial\ninbound transactions, reducing latency. Unfortunately, the state channel\nintroduces more issues at the same time, such as cross-channel synchronization,\nwhich makes the state channel unavailable for full adoption of current\nblockchain solutions.\n  In order to solve the efficiency problem, we proposed an end-to-end solution\ncalled ALZA, which links the dedicated high-throughput blockchain with\nself-organizing payment fields. This mechanism allows arbitrary set of users to\ncreate payment fields that process extremely low latency transactions within\neach field. Therefore, users can make transactions almost immediately. Since\nall transactions are conducted within fields, transaction costs will be reduced\nby several orders of magnitude. In addition, ALZA distributes main ledger to\neach client through an innovative replication mechanism. Therefore, the system\nwill be significantly more robust to blockchain system failures. In theory,\nALZA can complete millions of transactions in one second, which naturally\nsupports high-frequency trading.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 19:25:02 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 18:24:50 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 07:40:42 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 06:15:47 GMT"}, {"version": "v5", "created": "Sat, 30 Jun 2018 06:42:28 GMT"}, {"version": "v6", "created": "Wed, 4 Jul 2018 19:37:39 GMT"}, {"version": "v7", "created": "Sat, 14 Jul 2018 01:53:40 GMT"}, {"version": "v8", "created": "Sat, 20 Oct 2018 02:38:12 GMT"}, {"version": "v9", "created": "Thu, 7 Mar 2019 22:03:41 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Zou", "Nate", ""], ["Li", "Eric", ""], ["Zhang", "Henry", ""]]}, {"id": "1805.11775", "submitter": "Abdullah-Al Mamun", "authors": "Abdullah-Al Mamun, Zigeng Wang, Xingyu Cai, Nalini Ravishanker, and\n  Sanguthevar Rajasekaran", "title": "Efficient Sequential and Parallel Algorithms for Estimating Higher Order\n  Spectra", "comments": "12 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polyspectral estimation is a problem of great importance in the analysis of\nnonlinear time series that has applications in biomedical signal processing,\ncommunications, geophysics, image, radar, sonar and speech processing, etc.\nHigher order spectra (HOS) have been used in unsupervised and supervised\nclustering in big data scenarios, in testing for Gaussianity, to suppress\nGaussian noise, to characterize nonlinearities in time series data, and so on .\n  Any algorithm for computing the $k$th order spectra of a time series of\nlength $n$ needs $\\Omega(n^{k-1})$ time since the output size will be\n$\\Omega(n^{k-1})$ as well. Given that we live in an era of big data, $n$ could\nbe very large. In this case, sequential algorithms might take unacceptable\namounts of time. Thus it is essential to develop parallel algorithms. There is\nalso room for improving existing sequential algorithms. In addition, parallel\nalgorithms in the literature are nongeneric. In this paper we offer generic\nsequential algorithms for computing higher order spectra that are\nasymptotically faster than any published algorithm for HOS. Further, we offer\nmemory efficient algorithms. We also present optimal parallel implementations\nof these algorithms on parallel computing models such as the PRAM and the mesh.\nWe provide experimental results on our sequential and parallel algorithms. Our\nparallel implementation achieves very good speedups.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 02:09:03 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Mamun", "Abdullah-Al", ""], ["Wang", "Zigeng", ""], ["Cai", "Xingyu", ""], ["Ravishanker", "Nalini", ""], ["Rajasekaran", "Sanguthevar", ""]]}, {"id": "1805.11800", "submitter": "Kai Rothauge", "authors": "Alex Gittens, Kai Rothauge, Shusen Wang, Michael W. Mahoney, Lisa\n  Gerhardt, Prabhat, Jey Kottalam, Michael Ringenburg, Kristyn Maschhoff", "title": "Accelerating Large-Scale Data Analysis by Offloading to High-Performance\n  Computing Libraries using Alchemist", "comments": "Accepted for publication in Proceedings of the 24th ACM SIGKDD\n  International Conference on Knowledge Discovery and Data Mining, London, UK,\n  2018", "journal-ref": null, "doi": "10.1145/3219819.3219927", "report-no": null, "categories": "cs.DC cs.DB physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Spark is a popular system aimed at the analysis of large data sets,\nbut recent studies have shown that certain computations---in particular, many\nlinear algebra computations that are the basis for solving common machine\nlearning problems---are significantly slower in Spark than when done using\nlibraries written in a high-performance computing framework such as the\nMessage-Passing Interface (MPI).\n  To remedy this, we introduce Alchemist, a system designed to call MPI-based\nlibraries from Apache Spark. Using Alchemist with Spark helps accelerate linear\nalgebra, machine learning, and related computations, while still retaining the\nbenefits of working within the Spark environment. We discuss the motivation\nbehind the development of Alchemist, and we provide a brief overview of its\ndesign and implementation.\n  We also compare the performances of pure Spark implementations with those of\nSpark implementations that leverage MPI-based codes via Alchemist. To do so, we\nuse data science case studies: a large-scale application of the conjugate\ngradient method to solve very large linear systems arising in a speech\nclassification problem, where we see an improvement of an order of magnitude;\nand the truncated singular value decomposition (SVD) of a 400GB\nthree-dimensional ocean temperature data set, where we see a speedup of up to\n7.9x. We also illustrate that the truncated SVD computation is easily scalable\nto terabyte-sized data by applying it to data sets of sizes up to 17.6TB.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:23:41 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Gittens", "Alex", ""], ["Rothauge", "Kai", ""], ["Wang", "Shusen", ""], ["Mahoney", "Michael W.", ""], ["Gerhardt", "Lisa", ""], ["Prabhat", "", ""], ["Kottalam", "Jey", ""], ["Ringenburg", "Michael", ""], ["Maschhoff", "Kristyn", ""]]}, {"id": "1805.11877", "submitter": "Carl Witt", "authors": "Carl Witt, Marc Bux, Wladislaw Gusew, Ulf Leser", "title": "Predictive Performance Modeling for Distributed Computing using\n  Black-Box Monitoring and Machine Learning", "comments": "19 pages, 3 figures, 5 tables", "journal-ref": "Information Systems 2019", "doi": "10.1016/j.is.2019.01.006", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains, the previous decade was characterized by increasing data\nvolumes and growing complexity of computational workloads, creating new demands\nfor highly data-parallel computing in distributed systems. Effective operation\nof these systems is challenging when facing uncertainties about the performance\nof jobs and tasks under varying resource configurations, e.g., for scheduling\nand resource allocation. We survey predictive performance modeling (PPM)\napproaches to estimate performance metrics such as execution duration, required\nmemory or wait times of future jobs and tasks based on past performance\nobservations. We focus on non-intrusive methods, i.e., methods that can be\napplied to any workload without modification, since the workload is usually a\nblack-box from the perspective of the systems managing the computational\ninfrastructure. We classify and compare sources of performance variation,\npredicted performance metrics, required training data, use cases, and the\nunderlying prediction techniques. We conclude by identifying several open\nproblems and pressing research needs in the field.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 09:24:08 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Witt", "Carl", ""], ["Bux", "Marc", ""], ["Gusew", "Wladislaw", ""], ["Leser", "Ulf", ""]]}, {"id": "1805.11900", "submitter": "Christian Mayer", "authors": "Christian Mayer, Ruben Mayer, Jonas Grunert, Kurt Rothermel, and\n  Muhammad Adnan Tariq", "title": "Q-Graph: Preserving Query Locality in Multi-Query Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arising user-centric graph applications such as route planning and\npersonalized social network analysis have initiated a shift of paradigms in\nmodern graph processing systems towards multi-query analysis, i.e., processing\nmultiple graph queries in parallel on a shared graph. These applications\ngenerate a dynamic number of localized queries around query hotspots such as\npopular urban areas. However, existing graph processing systems are not yet\ntailored towards these properties: The employed methods for graph partitioning\nand synchronization management disregard query locality and dynamism which\nleads to high query latency. To this end, we propose the system Q-Graph for\nmulti-query graph analysis that considers query locality on three levels. (i)\nThe query-aware graph partitioning algorithm Q-cut maximizes query locality to\nreduce communication overhead. (ii) The method for synchronization management,\ncalled hybrid barrier synchronization, allows for full exploitation of local\nqueries spanning only a subset of partitions. (iii) Both methods adapt at\nruntime to changing query workloads in order to maintain and exploit locality.\nOur experiments show that Q-cut reduces average query latency by up to 57\npercent compared to static query-agnostic partitioning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 11:14:30 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Mayer", "Christian", ""], ["Mayer", "Ruben", ""], ["Grunert", "Jonas", ""], ["Rothermel", "Kurt", ""], ["Tariq", "Muhammad Adnan", ""]]}, {"id": "1805.12041", "submitter": "Corey Tessler", "authors": "Corey Tessler, Nathan Fisher", "title": "BUNDLEP: Prioritizing Conflict Free Regions in Multi-Threaded Programs\n  to Improve Cache Reuse -- Extended Results and Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In BUNDLE: Real-Time Multi-Threaded Scheduling to Reduce Cache Contention,\nTessler and Fisher propose a scheduling mechanism and combined worst-case\nexecution time calculation method that treats the instruction cache as a\nbeneficial resource shared between threads. Object analysis produces a\nworst-case execution time bound and separates code segments into regions.\nThreads are dynamically placed in bundles as- sociated with regions at run time\nby the BUNDLE scheduling algorithm where they benefit from shared cache values.\n  In the evaluation of the previous work, tasks were created with a\npredetermined worst-case execution time path through the control flow graph.\nApriori knowledge of the worst-case path is an impractical restriction on any\nanalysis. At the time, the only other solution available was an all-paths\nsearch of the graph, which is an equally impractical approach due to its\ncomplexity.\n  The primary focus of this work is to build upon BUNDLE, expanding its\napplicability beyond a proof of concept. We present a complete a worst-case\nexecution time calculation method that includes thread level context switch\ncosts, operating on real programs, with representative architecture parameters,\nand compare our results to those produced by Heptane's state of the art method.\nTo these ends, we propose a modification to the BUNDLE scheduling algorithm\ncalled BUNDLEP. Bundles are assigned priorities that enforce an ordered flow of\nthreads through the control flow graph -- avoiding the need for multiple\nall-paths searches through the graph. In many cases, our evaluation shows a\nrun-time and analytical benefit for BUNLDEP compared to serialized thread\nexecution and state of the art WCET analysis.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 15:42:05 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Tessler", "Corey", ""], ["Fisher", "Nathan", ""]]}, {"id": "1805.12120", "submitter": "Aditya Balu", "authors": "Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar", "title": "On Consensus-Optimality Trade-offs in Collaborative Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed machine learning, where agents collaboratively learn from\ndiverse private data sets, there is a fundamental tension between consensus and\noptimality. In this paper, we build on recent algorithmic progresses in\ndistributed deep learning to explore various consensus-optimality trade-offs\nover a fixed communication topology. First, we propose the incremental\nconsensus-based distributed SGD (i-CDSGD) algorithm, which involves multiple\nconsensus steps (where each agent communicates information with its neighbors)\nwithin each SGD iteration. Second, we propose the generalized consensus-based\ndistributed SGD (g-CDSGD) algorithm that enables us to navigate the full\nspectrum from complete consensus (all agents agree) to complete disagreement\n(each agent converges to individual model parameters). We analytically\nestablish convergence of the proposed algorithms for strongly convex and\nnonconvex objective functions; we also analyze the momentum variants of the\nalgorithms for the strongly convex case. We support our algorithms via\nnumerical experiments, and demonstrate significant improvements over existing\nmethods for collaborative deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 17:59:24 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Jiang", "Zhanhong", ""], ["Balu", "Aditya", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1805.12212", "submitter": "Timothy Duff", "authors": "Nathan Bliss, Timothy Duff, Anton Leykin, Jeff Sommars", "title": "Monodromy Solver: Sequential and Parallel", "comments": "19 pages, accepted to ISSAC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe, study, and experiment with an algorithm for finding all\nsolutions of systems of polynomial equations using homotopy continuation and\nmonodromy. This algorithm follows a framework developed in previous work and\ncan operate in the presence of a large number of failures of the homotopy\ncontinuation subroutine. We give special attention to parallelization and\nprobabilistic analysis of a model adapted to parallelization and failures.\nApart from theoretical results, we developed a simulator that allows us to run\na large number of experiments without recomputing the outcomes of the\ncontinuation subroutine.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 20:23:11 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Bliss", "Nathan", ""], ["Duff", "Timothy", ""], ["Leykin", "Anton", ""], ["Sommars", "Jeff", ""]]}, {"id": "1805.12242", "submitter": "Ajay Kshemkalyani", "authors": "Ajay D. Kshemkalyani and Faizan Ali", "title": "Efficient Dispersion of Mobile Robots on Graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dispersion problem on graphs requires $k$ robots placed arbitrarily at\nthe $n$ nodes of an anonymous graph, where $k \\leq n$, to coordinate with each\nother to reach a final configuration in which each robot is at a distinct node\nof the graph. The dispersion problem is important due to its relationship to\ngraph exploration by mobile robots, scattering on a graph, and load balancing\non a graph. In addition, an intrinsic application of dispersion has been shown\nto be the relocation of self-driven electric cars (robots) to recharge stations\n(nodes). We propose three efficient algorithms to solve dispersion on graphs.\nOur algorithms require $O(k \\log \\Delta)$ bits at each robot, and $O(m)$ steps\nrunning time, where $m$ is the number of edges and $\\Delta$ is the degree of\nthe graph. The algorithms differ in whether they address the synchronous or the\nasynchronous system model, and in what, where, and how data structures are\nmaintained.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 22:04:57 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Kshemkalyani", "Ajay D.", ""], ["Ali", "Faizan", ""]]}, {"id": "1805.12258", "submitter": "Haomiao Wang Mr.", "authors": "Haomiao Wang, Prabu Thiagaraj, and Oliver Sinnen", "title": "Harmonic-summing Module of SKA on FPGA--Optimising the Irregular Memory\n  Accesses", "comments": "14 pages, 12 figures, 7 tables, 30 references", "journal-ref": null, "doi": "10.1109/TVLSI.2018.2882238", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Square Kilometre Array (SKA), which will be the world's largest radio\ntelescope, will enhance and boost a large number of science projects, including\nthe search for pulsars. The frequency domain acceleration search is an\nefficient approach to search for binary pulsars. A significant part of it is\nthe harmonic-summing module, which is the research subject of this paper. Most\nof the operations in the harmonic-summing module are relatively cheap\noperations for FPGAs. The main challenge is the large number of point accesses\nto off-chip memory which are not consecutive but irregular. Although\nharmonic-summing alone might not be targeted for FPGA acceleration, it is a\npart of the pulsar search pipeline that contains many other compute-intensive\nmodules, which are efficiently executed on FPGA. Hence having the\nharmonic-summing also on the FPGA will avoid off-board communication, which\ncould destroy other acceleration benefits. Two types of harmonic-summing\napproaches are investigated in this paper: 1) storing intermediate data in\noff-chip memory and 2) processing the input signals directly without storing.\nFor the second type, two approaches of caching data are proposed and evaluated:\n1) preloading points that are frequently touched 2) preloading all necessary\npoints that are used to generate a chunk of output points. OpenCL is adopted to\nimplement the proposed approaches. In an extensive experimental evaluation, the\nsame OpenCL kernel codes are evaluated on FPGA boards and GPU cards. Regarding\nthe proposed preloading methods, preloading all necessary points method while\nreordering the input signals is faster than all the other methods. While in raw\nperformance a single FPGA board cannot compete with a GPU, in terms of energy\ndissipation, GPU costs up to 2.6x times more energy than that of FPGAs in\nexecuting the same NDRange kernels.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 23:15:52 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 00:11:31 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Wang", "Haomiao", ""], ["Thiagaraj", "Prabu", ""], ["Sinnen", "Oliver", ""]]}, {"id": "1805.12280", "submitter": "Haomiao Wang Mr.", "authors": "Haomiao Wang, Prabu Thiagaraj and Oliver Sinnen", "title": "FPGA-based Acceleration of FT Convolution for Pulsar Search Using OpenCL", "comments": "25 page, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Square Kilometre Array (SKA) project will be the world largest radio\ntelescope array. With its large number of antennas, the number of signals that\nneed to be processed is dramatic. One important element of the SKA's Central\nSignal Processor package is pulsar search. This paper focuses on the FPGA-based\nacceleration of the Frequency-Domain Acceleration Search module, which is a\npart of SKA pulsar search engine. In this module, the frequency-domain input\nsignals have to be processed by 85 Finite Impulse response (FIR) filters within\na short period of limitation and for thousands of input arrays. Because of the\nlarge scale of the input length and FIR filter size, even high-end FPGA devices\ncannot parallelise the task completely. We start by investigating both\ntime-domain FIR filter (TDFIR) and frequency-domain FIR filter (FDFIR) to\ntackle this task. We applied the overlap-add algorithm to split the coefficient\narray of TDFIR and the overlap-save algorithm to split the input signals of\nFDFIR. To achieve fast prototyping design, we employed OpenCL, which is a\nhigh-level FPGA development technique. The performance and power consumption\nare evaluated using multiple FPGA devices simultaneously and compared with GPU\nresults, which is achieved by porting FPGA-based OpenCL kernels. The\nexperimental evaluation shows that the FDFIR solution is very competitive in\nterms of performance, with a clear energy consumption advantage over the GPU\nsolution.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 01:18:35 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 00:17:54 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Wang", "Haomiao", ""], ["Thiagaraj", "Prabu", ""], ["Sinnen", "Oliver", ""]]}, {"id": "1805.12305", "submitter": "Qinzhe Wu", "authors": "Shuang Song and Xu Liu and Qinzhe Wu and Andreas Gerstlauer and Tao Li\n  and Lizy K. John", "title": "Start Late or Finish Early: A Distributed Graph Processing System with\n  Redundancy Reduction", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph processing systems are important in the big data domain. However,\nprocessing graphs in parallel often introduces redundant computations in\nexisting algorithms and models. Prior work has proposed techniques to optimize\nredundancies for the out-of-core graph systems, rather than the distributed\ngraph systems. In this paper, we study various state-of-the-art distributed\ngraph systems and observe root causes for these pervasively existing\nredundancies. To reduce redundancies without sacrificing parallelism, we\nfurther propose SLFE, a distributed graph processing system, designed with the\nprinciple of \"start late or finish early\". SLFE employs a novel preprocessing\nstage to obtain a graph's topological knowledge with negligible overhead.\nSLFE's redundancy-aware vertex-centric computation model can then utilize such\nknowledge to reduce the redundant computations at runtime. SLFE also provides a\nset of APIs to improve the programmability. Our experiments on an 8-node\nhigh-performance cluster show that SLFE outperforms all well-known distributed\ngraph processing systems on real-world graphs (yielding up to 74.8x speedup).\nSLFE's redundancy-reduction schemes are generally applicable to other\nvertex-centric graph processing systems.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 03:23:55 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Song", "Shuang", ""], ["Liu", "Xu", ""], ["Wu", "Qinzhe", ""], ["Gerstlauer", "Andreas", ""], ["Li", "Tao", ""], ["John", "Lizy K.", ""]]}]