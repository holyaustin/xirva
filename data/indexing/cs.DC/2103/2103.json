[{"id": "2103.00033", "submitter": "Sebastian Burckhardt", "authors": "Sebastian Burckhardt, Chris Gillum, David Justo, Konstantinos Kallas,\n  Connor McMahon, Christopher S. Meiklejohn", "title": "Serverless Workflows with Durable Functions and Netherite", "comments": "This paper was written in September 2020, and the content has not\n  been edited after October 10, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serverless is an increasingly popular choice for service architects because\nit can provide elasticity and load-based billing with minimal developer effort.\nA common and important use case is to compose serverless functions and cloud\nstorage into reliable workflows. However, existing solutions for authoring\nworkflows provide a rudimentary experience compared to writing standard code in\na modern programming language. Furthermore, executing workflows reliably in an\nelastic serverless environment poses significant performance challenges.\n  To address these, we propose Durable Functions, a programming model for\nserverless workflows, and Netherite, a distributed execution engine to execute\nthem efficiently. Workflows in Durable Functions are expressed as task-parallel\ncode in a host language of choice. Internally, the workflows are translated to\nfine-grained stateful communicating processes, which are load-balanced over an\nelastic cluster. The main challenge is to minimize the cost of reliably\npersisting progress to storage while supporting elastic scale. Netherite solves\nthis by introducing partitioning, recovery logs, asynchronous snapshots, and\nspeculative communication.\n  Our results show that Durable Functions simplifies the expression of complex\nworkflows, and that Netherite achieves lower latency and higher throughput than\nthe prevailing approaches for serverless workflows in Azure and AWS, by orders\nof magnitude in some cases.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 19:51:58 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Burckhardt", "Sebastian", ""], ["Gillum", "Chris", ""], ["Justo", "David", ""], ["Kallas", "Konstantinos", ""], ["McMahon", "Connor", ""], ["Meiklejohn", "Christopher S.", ""]]}, {"id": "2103.00081", "submitter": "Krishna Kumar", "authors": "Xiang Sun, Kenichi Soga, Alp Cinar, Zhenxiang Su, Kecheng Chen,\n  Krishna Kumar, Patrick F. Dobson, Peter S. Nico", "title": "An HPC-Based Hydrothermal Finite Element Simulator for Modeling\n  Underground Response to Community-Scale Geothermal Energy Production", "comments": "46th Workshop on Geothermal Reservoir Engineering Stanford\n  University, Stanford, California, February 15-17, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geothermal heat, as renewable energy, shows great advantage with respect to\nits environmental impact due to its significantly lower CO2 emissions than\nconventional fossil fuel. Open and closed-loop geothermal heat pumps, which\nutilize shallow geothermal systems, are an efficient technology for cooling and\nheating buildings, especially in urban areas. Integrated use of geothermal\nenergy technologies for district heating, cooling, and thermal energy storage\ncan be applied to optimize the subsurface for communities to provide them with\nmultiple sustainable energy and community resilience benefits. The utilization\nof the subsurface resources may lead to a variation in the underground\nenvironment, which might further impact local environmental conditions.\nHowever, very few simulators can handle such a highly complex set of coupled\ncomputations on a regional or city scale. We have developed high-performance\ncomputing (HPC) based hydrothermal finite element (FE) simulator that can\nsimulate the subsurface and its hydrothermal conditions at a scale of tens of\nkm. The HPC simulator enables us to investigate the subsurface thermal and\nhydrologic response to the built underground environment (such as basements and\nsubways) at the community scale. In this study, a coupled hydrothermal\nsimulator is developed based on the open-source finite element library deal.II.\nThe HPC simulator was validated by comparing the results of a benchmark case\nstudy against COMSOL Multiphysics, in which Aquifer Thermal Energy Storage\n(ATES) is modeled and a process of heat injection into ATES is simulated. The\nuse of an energy pile system at the Treasure Island redevelopment site (San\nFrancisco, CA, USA) was selected as a case study to demonstrate the HPC\ncapability of the developed simulator. The simulator is capable of modeling\nmultiple city-scale geothermal scenarios in a reasonable amount of time.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 23:06:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sun", "Xiang", ""], ["Soga", "Kenichi", ""], ["Cinar", "Alp", ""], ["Su", "Zhenxiang", ""], ["Chen", "Kecheng", ""], ["Kumar", "Krishna", ""], ["Dobson", "Patrick F.", ""], ["Nico", "Peter S.", ""]]}, {"id": "2103.00091", "submitter": "Matteo Turilli", "authors": "Andre Merzky, Matteo Turilli, Mikhail Titov, Aymen Al-Saadi, Shantenu\n  Jha", "title": "Design and Performance Characterization of RADICAL-Pilot on\n  Leadership-class Platforms", "comments": "arXiv admin note: text overlap with arXiv:1801.01843", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many extreme scale scientific applications have workloads comprised of a\nlarge number of individual high-performance tasks. The Pilot abstraction\ndecouples workload specification, resource management, and task execution via\njob placeholders and late-binding. As such, suitable implementations of the\nPilot abstraction can support the collective execution of large number of tasks\non supercomputers. We introduce RADICAL-Pilot (RP) as a portable, modular and\nextensible Pilot enabled runtime system. We describe RP's design, architecture\nand implementation. We characterize its performance and show its ability to\nscalably execute workloads comprised of tens of thousands heterogeneous tasks\non DOE and NSF leadership-class HPC platforms. Specifically, we investigate\nRP's weak/strong scaling with CPU/GPU, single/multi core, (non)MPI tasks and\npython functions when using most of ORNL Summit and TACC Frontera.\nRADICAL-Pilot can be used stand-alone, as well as the runtime for third-party\nworkflow systems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 23:57:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Merzky", "Andre", ""], ["Turilli", "Matteo", ""], ["Titov", "Mikhail", ""], ["Al-Saadi", "Aymen", ""], ["Jha", "Shantenu", ""]]}, {"id": "2103.00130", "submitter": "Sihuan Li", "authors": "Sihuan Li, Jianyu Huang, Ping Tak Peter Tang, Daya Khudia, Jongsoo\n  Park, Harish Dattatraya Dixit, Zizhong Chen", "title": "Efficient Soft-Error Detection for Low-precision Deep Learning\n  Recommendation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Soft error, namely silent corruption of signal or datum in a computer system,\ncannot be caverlierly ignored as compute and communication density grow\nexponentially. Soft error detection has been studied in the context of\nenterprise computing, high-performance computing and more recently in\nconvolutional neural networks related to autonomous driving. Deep learning\nrecommendation systems (DLRMs) have by now become ubiquitous and serve billions\nof users per day. Nevertheless, DLRM-specific soft error detection methods are\nhitherto missing. To fill the gap, this paper presents the first set of\nsoft-error detection methods for low-precision quantized-arithmetic operators\nin DLRM including general matrix multiplication (GEMM) and EmbeddingBag. A\npractical method must detect error and do so with low overhead lest reduced\ninference speed degrades user experience. Exploiting the characteristics of\nboth quantized arithmetic and the operators, we achieved more than 95%\ndetection accuracy for GEMM with an overhead below 20%. For EmbeddingBag, we\nachieved 99% effectiveness in significant-bit-flips detection with less than\n10% of false positives, while keeping overhead below 26%.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 05:07:20 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Sihuan", ""], ["Huang", "Jianyu", ""], ["Tang", "Ping Tak Peter", ""], ["Khudia", "Daya", ""], ["Park", "Jongsoo", ""], ["Dixit", "Harish Dattatraya", ""], ["Chen", "Zizhong", ""]]}, {"id": "2103.00148", "submitter": "Andreas Hellander", "authors": "Morgan Ekmefjord, Addi Ait-Mlouk, Sadi Alawadi, Mattias {\\AA}kesson,\n  Desislava Stoyanova, Ola Spjuth, Salman Toor, Andreas Hellander", "title": "Scalable federated machine learning with FEDn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated machine learning has great promise to overcome the input privacy\nchallenge in machine learning. The appearance of several projects capable of\nsimulating federated learning has led to a corresponding rapid progress on\nalgorithmic aspects of the problem. However, there is still a lack of federated\nmachine learning frameworks that focus on fundamental aspects such as\nscalability, robustness, security, and performance in a geographically\ndistributed setting. To bridge this gap we have designed and developed the FEDn\nframework. A main feature of FEDn is to support both cross-device and\ncross-silo training settings. This makes FEDn a powerful tool for researching a\nwide range of machine learning applications in a realistic setting.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 07:30:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ekmefjord", "Morgan", ""], ["Ait-Mlouk", "Addi", ""], ["Alawadi", "Sadi", ""], ["\u00c5kesson", "Mattias", ""], ["Stoyanova", "Desislava", ""], ["Spjuth", "Ola", ""], ["Toor", "Salman", ""], ["Hellander", "Andreas", ""]]}, {"id": "2103.00154", "submitter": "Dinuka De Zoysa", "authors": "B.D.M. De Zoysa, Y.A.M.M.A. Ali, M.D.I. Maduranga, Indika Perera,\n  Saliya Ekanayake, Anil Vullikanti", "title": "Parallel Algorithms for Densest Subgraph Discovery Using Shared Memory\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of finding dense components of a graph is a widely explored area\nin data analysis, with diverse applications in fields and branches of study\nincluding community mining, spam detection, computer security and\nbioinformatics. This research project explores previously available algorithms\nin order to study them and identify potential modifications that could result\nin an improved version with considerable performance and efficiency leap.\nFurthermore, efforts were also steered towards devising a novel algorithm for\nthe problem of densest subgraph discovery. This paper presents an improved\nimplementation of a widely used densest subgraph discovery algorithm and a\nnovel parallel algorithm which produces better results than a 2-approximation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 08:07:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["De Zoysa", "B. D. M.", ""], ["Ali", "Y. A. M. M. A.", ""], ["Maduranga", "M. D. I.", ""], ["Perera", "Indika", ""], ["Ekanayake", "Saliya", ""], ["Vullikanti", "Anil", ""]]}, {"id": "2103.00167", "submitter": "Dirk Fahland", "authors": "Dirk Fahland, Vadim Denisov, Wil. M.P. van der Aalst", "title": "Inferring Unobserved Events in Systems With Shared Resources and Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.FL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the causes of performance problems or to predict process\nbehavior, it is essential to have correct and complete event data. This is\nparticularly important for distributed systems with shared resources, e.g., one\ncase can block another case competing for the same machine, leading to\ninter-case dependencies in performance. However, due to a variety of reasons,\nreal-life systems often record only a subset of all events taking place. For\nexample, to reduce costs, the number of sensors is minimized or parts of the\nsystem are not connected. To understand and analyze the behavior of processes\nwith shared resources, we aim to reconstruct bounds for timestamps of events\nthat must have happened but were not recorded. We present a novel approach that\ndecomposes system runs into entity traces of cases and resources that may need\nto synchronize in the presence of many-to-many relationships. Such\nrelationships occur, for example, in warehouses where packages for N incoming\norders are not handled in a single delivery but in M different deliveries. We\nuse linear programming over entity traces to derive the timestamps of\nunobserved events in an efficient manner. This helps to complete the event logs\nand facilitates analysis. We focus on material handling systems like baggage\nhandling systems in airports to illustrate our approach. However, the approach\ncan be applied to other settings where recording is incomplete. The ideas have\nbeen implemented in ProM and were evaluated using both synthetic and real-life\nevent logs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 09:34:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fahland", "Dirk", ""], ["Denisov", "Vadim", ""], ["van der Aalst", "Wil. M. P.", ""]]}, {"id": "2103.00176", "submitter": "Mayank Gokarna", "authors": "Mayank Gokarna", "title": "Reasons behind growing adoption of Cloud after Covid-19 Pandemic and\n  Challenges ahead", "comments": "10 pages including 27 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There are many sectors which have moved to Cloud and are planning\naggressively to move their workloads to Cloud since the world entered Covid-19\npandemic. There are various reasons why Cloud is an essential irresistible\ntechnology and serves as an ultimate solution to access IT software and\nsystems. It has become a new essential catalyst for Enterprise Organisations\nwhich are looking for Digital Transformation. Remote working is a common\nphenomenon now across all the IT companies making the services available all\nthe time. Covid-19 has made cloud adoption an immediate priority for\nOrganisation rather than a slowly approached future transformation. The\nbenefits of Cloud lies in the fact that employees rather engineers of an\nenterprise are no more dependent on the closed hardware-based IT infrastructure\nand hence eliminates the necessity of working from the networked office\npremises. This has raised a huge demand for skilled Cloud specialist who can\nmanage and support the systems running on cloud across different regions of the\nworld. In this research, the reasons for growing Cloud adoption after pandemic\nCovid-19 has been described and the challenges which Organization will face is\nalso explained. This study also details the most used cloud services during the\npandemic considering Amazon Web Services as the cloud provider.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 10:32:10 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gokarna", "Mayank", ""]]}, {"id": "2103.00179", "submitter": "Kostas Kolomvatsos", "authors": "Kostas Kolomvatsos, Christos Anagnostopoulos", "title": "A Soft Method for Outliers Detection at the Edge of the Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of the Internet of Things and the Edge Computing gives many\nopportunities to support innovative applications close to end users. Numerous\ndevices present in both infrastructures can collect data upon which various\nprocessing activities can be performed. However, the quality of the outcomes\nmay be jeopardized by the presence of outliers. In this paper, we argue on a\nnovel model for outliers detection by elaborating on a `soft' approach. Our\nmechanism is built upon the concepts of candidate and confirmed outliers. Any\ndata object that deviates from the population is confirmed as an outlier only\nafter the study of its sequence of magnitude values as new data are\nincorporated into our decision making model. We adopt the combination of a\nsliding with a landmark window model when a candidate outlier is detected to\nexpand the sequence of data objects taken into consideration. The proposed\nmodel is fast and efficient as exposed by our experimental evaluation while a\ncomparative assessment reveals its pros and cons.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 10:36:24 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kolomvatsos", "Kostas", ""], ["Anagnostopoulos", "Christos", ""]]}, {"id": "2103.00266", "submitter": "Nimish Shah", "authors": "Nimish Shah, Laura I. Galindez Olascoaga, Wannes Meert and Marian\n  Verhelst", "title": "Acceleration of probabilistic reasoning through custom processor\n  architecture", "comments": null, "journal-ref": "Design, Automation & Test in Europe Conference & Exhibition (DATE)\n  2020", "doi": "10.23919/DATE48585.2020.9116326", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic reasoning is an essential tool for robust decision-making\nsystems because of its ability to explicitly handle real-world uncertainty,\nconstraints and causal relations. Consequently, researchers are developing\nhybrid models by combining Deep Learning with probabilistic reasoning for\nsafety-critical applications like self-driving vehicles, autonomous drones,\netc. However, probabilistic reasoning kernels do not execute efficiently on\nCPUs or GPUs. This paper, therefore, proposes a custom programmable processor\nto accelerate sum-product networks, an important probabilistic reasoning\nexecution kernel. The processor has an optimized datapath architecture and\nmemory hierarchy optimized for sum-product networks execution. Experimental\nresults show that the processor, while requiring fewer computational and memory\nunits, achieves a 12x throughput benefit over the Nvidia Jetson TX2 embedded\nGPU platform.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 16:57:20 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shah", "Nimish", ""], ["Olascoaga", "Laura I. Galindez", ""], ["Meert", "Wannes", ""], ["Verhelst", "Marian", ""]]}, {"id": "2103.00373", "submitter": "Xingcai Zhou", "authors": "Xingcai Zhou, Le Chang, Pengfei Xu and Shaogao Lv", "title": "Communication-efficient Byzantine-robust distributed learning with\n  statistical guarantee", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication efficiency and robustness are two major issues in modern\ndistributed learning framework. This is due to the practical situations where\nsome computing nodes may have limited communication power or may behave\nadversarial behaviors. To address the two issues simultaneously, this paper\ndevelops two communication-efficient and robust distributed learning algorithms\nfor convex problems. Our motivation is based on surrogate likelihood framework\nand the median and trimmed mean operations. Particularly, the proposed\nalgorithms are provably robust against Byzantine failures, and also achieve\noptimal statistical rates for strong convex losses and convex (non-smooth)\npenalties. For typical statistical models such as generalized linear models,\nour results show that statistical errors dominate optimization errors in finite\niterations. Simulated and real data experiments are conducted to demonstrate\nthe numerical performance of our algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:38:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhou", "Xingcai", ""], ["Chang", "Le", ""], ["Xu", "Pengfei", ""], ["Lv", "Shaogao", ""]]}, {"id": "2103.00490", "submitter": "David Yuan", "authors": "Yiannis Gkoufas (1) and David Yu Yuan (2) ((1) IBM Research - Ireland,\n  (2) Technology and Science Integration, European Bioinformatics Institute,\n  European Molecular Biology Laboratory)", "title": "Dataset Lifecycle Framework and its applications in Bioinformatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bioinformatics pipelines depend on shared POSIX filesystems for its input,\noutput and intermediate data storage. Containerization makes it more difficult\nfor the workloads to access the shared file systems. In our previous study, we\nwere able to run both ML and non-ML pipelines on Kubeflow successfully.\nHowever, the storage solutions were complex and less optimal. This is because\nthere are no established resource types to represent the concept of data source\non Kubernetes. More and more applications are running on Kubernetes for batch\nprocessing. End users are burdened with configuring and optimising the data\naccess, which is what we have experienced before.\n  In this article, we are introducing a new concept of Dataset and its\ncorresponding resource as a native Kubernetes object. We have leveraged the\nDataset Lifecycle Framework which takes care of all the low-level details about\ndata access in Kubernetes pods. Its pluggable architecture is designed for the\ndevelopment of caching, scheduling and governance plugins. Together, they\nmanage the entire lifecycle of the custom resource Dataset.\n  We use Dataset Lifecycle Framework to serve data from object stores to both\nML and non-ML pipelines running on Kubeflow. With DLF, we make training data\nfed into ML models directly without being downloaded to the local disks, which\nmakes the input scalable. We have enhanced the durability of training metadata\nby storing it into a dataset, which also simplifies the set up of the\nTensorboard, separated from the notebook server. For the non-ML pipeline, we\nhave simplified the 1000 Genome Project pipeline with datasets injected into\nthe pipeline dynamically. In addition, our preliminary results indicate that\nthe pluggable caching mechanism can improve the performance significantly.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 21:54:42 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gkoufas", "Yiannis", ""], ["Yuan", "David Yu", ""]]}, {"id": "2103.00523", "submitter": "Wen Guan", "authors": "Wen Guan, Tadashi Maeno, Brian Paul Bockelman, Torre Wenaus, Fahui\n  Lin, Siarhei Padolski, Rui Zhang, Aleksandr Alekseev", "title": "An intelligent Data Delivery Service for and beyond the ATLAS experiment", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC hep-ex", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The intelligent Data Delivery Service (iDDS) has been developed to cope with\nthe huge increase of computing and storage resource usage in the coming LHC\ndata taking. iDDS has been designed to intelligently orchestrate workflow and\ndata management systems, decoupling data pre-processing, delivery, and main\nprocessing in various workflows. It is an experiment-agnostic service around a\nworkflow-oriented structure to work with existing and emerging use cases in\nATLAS and other experiments. Here we will present the motivation for iDDS, its\ndesign schema and architecture, use cases and current status, and plans for the\nfuture.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 14:37:20 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Guan", "Wen", ""], ["Maeno", "Tadashi", ""], ["Bockelman", "Brian Paul", ""], ["Wenaus", "Torre", ""], ["Lin", "Fahui", ""], ["Padolski", "Siarhei", ""], ["Zhang", "Rui", ""], ["Alekseev", "Aleksandr", ""]]}, {"id": "2103.00543", "submitter": "Saurabh Agarwal", "authors": "Saurabh Agarwal, Hongyi Wang, Shivaram Venkataraman, Dimitris\n  Papailiopoulos", "title": "On the Utility of Gradient Compression in Distributed Training Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rich body of prior work has highlighted the existence of communication\nbottlenecks in synchronous data-parallel training. To alleviate these\nbottlenecks, a long line of recent work proposes gradient and model compression\nmethods. In this work, we evaluate the efficacy of gradient compression methods\nand compare their scalability with optimized implementations of synchronous\ndata-parallel SGD across more than 200 different setups. Surprisingly, we\nobserve that only in 6 cases out of more than 200, gradient compression methods\nprovide speedup over optimized synchronous data-parallel training in the\ntypical data-center setting. We conduct an extensive investigation to identify\nthe root causes of this phenomenon, and offer a performance model that can be\nused to identify the benefits of gradient compression for a variety of system\nsetups. Based on our analysis, we propose a list of desirable properties that\ngradient compression methods should satisfy, in order for them to provide a\nmeaningful end-to-end speedup.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 15:58:45 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 21:30:45 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 23:55:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Agarwal", "Saurabh", ""], ["Wang", "Hongyi", ""], ["Venkataraman", "Shivaram", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "2103.00571", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi, Fabio Checconi, Douglas Doerfler, Fabrizio Petrini", "title": "Performance Optimization of SU3_Bench on Xeon and Programmable\n  Integrated Unified Memory Architecture", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SU3\\_Bench is a microbenchmark developed to explore performance portability\nacross multiple programming models/methodologies using a simple, but\nnontrivial, mathematical kernel. This kernel has been derived from the MILC\nlattice quantum chromodynamics (LQCD) code. SU3\\_Bench is bandwidth bound and\ngenerates regular compute and data access patterns. Therefore, on most\ntraditional CPU and GPU-based systems, its performance is mainly determined by\nthe achievable memory bandwidth. Although SU3\\_Bench is a simple kernel,\nexperience says its subtleties require a certain amount of tweaking to achieve\npeak performance for a given programming model and hardware, making performance\nportability challenging. In this paper, we share some of the challenges in\nobtaining the peak performance for SU3\\_Bench on a state-of-the-art Intel Xeon\nmachine, due to the nuances of variable definition, the nature of\ncompiler-provided default constructors, how memory is accessed at object\ncreation time, and the NUMA effects on the machine. We discuss how to tackle\nthose challenges to improve SU3\\_Bench's performance by \\(2\\times\\) compared to\nthe original OpenMP implementation available at Github. This provides a\nvaluable lesson for other similar kernels.\n  Expanding on the performance portability aspects, we also show early results\nobtained porting SU3\\_Bench to the new Intel Programmable Integrated Unified\nMemory Architecture (PIUMA), characterized by a more balanced flops-to-byte\nratio. This paper shows that it is not the usual bandwidth or flops, rather the\npipeline throughput, that determines SU3\\_Bench's performance on PIUMA.\nFinally, we show how to improve performance on PIUMA and how that compares with\nthe performance on Xeon, which has around one order of magnitude more\nflops-per-byte.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 17:33:26 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 18:15:07 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Checconi", "Fabio", ""], ["Doerfler", "Douglas", ""], ["Petrini", "Fabrizio", ""]]}, {"id": "2103.00710", "submitter": "Alysa Ziying Tan", "authors": "Alysa Ziying Tan, Han Yu, Lizhen Cui, Qiang Yang", "title": "Towards Personalized Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As artificial intelligence (AI)-empowered applications become widespread,\nthere is growing awareness and concern for user privacy and data\nconfidentiality. This has contributed to the popularity of federated learning\n(FL). FL applications often face data distribution and device capability\nheterogeneity across data owners. This has stimulated the rapid development of\nPersonalized FL (PFL). In this paper, we complement existing surveys, which\nlargely focus on the methods and applications of FL, with a review of recent\nadvances in PFL. We discuss hurdles to PFL under the current FL settings, and\npresent a unique taxonomy dividing PFL techniques into data-based and\nmodel-based approaches. We highlight their key ideas, and envision promising\nfuture trajectories of research towards new PFL architectural design, realistic\nPFL benchmarking, and trustworthy PFL approaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 02:45:19 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tan", "Alysa Ziying", ""], ["Yu", "Han", ""], ["Cui", "Lizhen", ""], ["Yang", "Qiang", ""]]}, {"id": "2103.00777", "submitter": "Fangyu Gai", "authors": "Fangyu Gai, Ali Farahbakhsh, Jianyu Niu, Chen Feng, Ivan Beschastnikh,\n  Hao Duan", "title": "Dissecting the Performance of Chained-BFT", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Permissioned blockchains employ Byzantine fault-tolerant (BFT) state machine\nreplication (SMR) to reach agreement on an ever-growing, linearly ordered log\nof transactions. A new paradigm, combined with decades of research in BFT SMR\nand blockchain (namely chained-BFT, or cBFT), has emerged for directly\nconstructing blockchain protocols. Chained-BFT protocols have a unifying\npropose-vote scheme instead of multiple different voting phases with a set of\nvoting and commit rules to guarantee safety and liveness. However, distinct\nvoting and commit rules impose varying impacts on performance under different\nworkloads, network conditions, and Byzantine attacks. Therefore, a fair\ncomparison of the proposed protocols poses a challenge that has not yet been\naddressed by existing work.\n  We fill this gap by studying a family of cBFT protocols with a two-pronged\nsystematic approach. First, we present an evaluation framework, Bamboo, for\nquick prototyping of cBFT protocols and that includes helpful benchmarking\nfacilities. To validate Bamboo, we introduce an analytic model using queuing\ntheory which also offers a back-of-the-envelope guide for dissecting these\nprotocols. We build multiple cBFT protocols using Bamboo and we are the first\nto fairly compare three representatives (i.e., HotStuff, two-chain HotStuff,\nand Streamlet). We evaluated these protocols under various parameters and\nscenarios, including two Byzantine attacks that have not been widely discussed\nin the literature. Our findings reveal interesting trade-offs (e.g.,\nresponsiveness vs. forking-resilience) between different cBFT protocols and\ntheir design choices, which provide developers and researchers with insights\ninto the design and implementation of this protocol family.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:01:08 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gai", "Fangyu", ""], ["Farahbakhsh", "Ali", ""], ["Niu", "Jianyu", ""], ["Feng", "Chen", ""], ["Beschastnikh", "Ivan", ""], ["Duan", "Hao", ""]]}, {"id": "2103.01170", "submitter": "Philipp Wiesner", "authors": "Philipp Wiesner and Lauritz Thamsen", "title": "LEAF: Simulating Large Energy-Aware Fog Computing Environments", "comments": "To appear in the Proceedings of the 5th IEEE International Conference\n  on Fog and Edge Computing 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite constant improvements in efficiency, today's data centers and\nnetworks consume enormous amounts of energy and this demand is expected to rise\neven further. An important research question is whether and how fog computing\ncan curb this trend. As real-life deployments of fog infrastructure are still\nrare, a significant part of research relies on simulations. However, existing\npower models usually only target particular components such as compute nodes or\nbattery-constrained edge devices.\n  Combining analytical and discrete-event modeling, we develop a holistic but\ngranular energy consumption model that can determine the power usage of compute\nnodes as well as network traffic and applications over time. Simulations can\nincorporate thousands of devices that execute complex application graphs on a\ndistributed, heterogeneous, and resource-constrained infrastructure. We\nevaluated our publicly available prototype LEAF within a smart city traffic\nscenario, demonstrating that it enables research on energy-conserving fog\ncomputing architectures and can be used to assess dynamic task placement\nstrategies and other energy-saving mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:08:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wiesner", "Philipp", ""], ["Thamsen", "Lauritz", ""]]}, {"id": "2103.01206", "submitter": "Baturalp Buyukates", "authors": "Baturalp Buyukates and Emre Ozfatura and Sennur Ulukus and Deniz\n  Gunduz", "title": "Gradient Coding with Dynamic Clustering for Straggler-Tolerant\n  Distributed Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.01922", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed implementations are crucial in speeding up large scale machine\nlearning applications. Distributed gradient descent (GD) is widely employed to\nparallelize the learning task by distributing the dataset across multiple\nworkers. A significant performance bottleneck for the per-iteration completion\ntime in distributed synchronous GD is $straggling$ workers. Coded distributed\ncomputation techniques have been introduced recently to mitigate stragglers and\nto speed up GD iterations by assigning redundant computations to workers. In\nthis paper, we consider gradient coding (GC), and propose a novel dynamic GC\nscheme, which assigns redundant data to workers to acquire the flexibility to\ndynamically choose from among a set of possible codes depending on the past\nstraggling behavior. In particular, we consider GC with clustering, and\nregulate the number of stragglers in each cluster by dynamically forming the\nclusters at each iteration; hence, the proposed scheme is called $GC$ $with$\n$dynamic$ $clustering$ (GC-DC). Under a time-correlated straggling behavior,\nGC-DC gains from adapting to the straggling behavior over time such that, at\neach iteration, GC-DC aims at distributing the stragglers across clusters as\nuniformly as possible based on the past straggler behavior. For both\nhomogeneous and heterogeneous worker models, we numerically show that GC-DC\nprovides significant improvements in the average per-iteration completion time\nwithout an increase in the communication load compared to the original GC\nscheme.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:51:29 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Buyukates", "Baturalp", ""], ["Ozfatura", "Emre", ""], ["Ulukus", "Sennur", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2103.01216", "submitter": "Yan Gu", "authors": "Yan Gu, Omar Obeya, Julian Shun", "title": "Parallel In-Place Algorithms: Theory and Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many parallel algorithms use at least linear auxiliary space in the size of\nthe input to enable computations to be done independently without conflicts.\nUnfortunately, this extra space can be prohibitive for memory-limited machines,\npreventing large inputs from being processed. Therefore, it is desirable to\ndesign parallel in-place algorithms that use sublinear (or even\npolylogarithmic) auxiliary space.\n  In this paper, we bridge the gap between theory and practice for parallel\nin-place (PIP) algorithms. We first define two computational models based on\nfork-join parallelism, which reflect modern parallel programming environments.\nWe then introduce a variety of new parallel in-place algorithms that are simple\nand efficient, both in theory and in practice. Our algorithmic highlight is the\nDecomposable Property introduced in this paper, which enables existing\nnon-in-place but highly-optimized parallel algorithms to be converted into\nparallel in-place algorithms. Using this property, we obtain algorithms for\nrandom permutation, list contraction, tree contraction, and merging that take\nlinear work, $O(n^{1-\\epsilon})$ auxiliary space, and\n$O(n^\\epsilon\\cdot\\text{polylog}(n))$ span for $0<\\epsilon<1$. We also present\nnew parallel in-place algorithms for scan, filter, merge, connectivity,\nbiconnectivity, and minimum spanning forest using other techniques.\n  In addition to theoretical results, we present experimental results for\nimplementations of many of our parallel in-place algorithms. We show that on a\n72-core machine with two-way hyper-threading, the parallel in-place algorithms\nusually outperform existing parallel algorithms for the same problems that use\nlinear auxiliary space, indicating that the theory developed in this paper\nindeed leads to practical benefits in terms of both space usage and running\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:59:05 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gu", "Yan", ""], ["Obeya", "Omar", ""], ["Shun", "Julian", ""]]}, {"id": "2103.01304", "submitter": "Edward Hutter", "authors": "Edward Hutter and Edgar Solomonik", "title": "Accelerating Distributed-Memory Autotuning via Statistical Analysis of\n  Execution Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prohibitive expense of automatic performance tuning at scale has largely\nlimited the use of autotuning to libraries for shared-memory and GPU\narchitectures. We introduce a framework for approximate autotuning that\nachieves a desired confidence in each algorithm configuration's performance by\nconstructing confidence intervals to describe the performance of individual\nkernels (subroutines of benchmarked programs). Once a kernel's performance is\ndeemed sufficiently predictable for a set of inputs, subsequent invocations are\navoided and replaced with a predictive model of the execution time. We then\nleverage online execution path analysis to coordinate selective kernel\nexecution and propagate each kernel's statistical profile. This strategy is\neffective in the presence of frequently-recurring computation and communication\nkernels, which is characteristic to algorithms in numerical linear algebra. We\nencapsulate this framework as part of a new profiling tool, Critter, that\nautomates kernel execution decisions and propagates statistical profiles along\ncritical paths of execution. We evaluate performance prediction accuracy\nobtained by our selective execution methods using state-of-the-art\ndistributed-memory implementations of Cholesky and QR factorization on\nStampede2, and demonstrate speed-ups of up to 7.1x with 98% prediction\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:51:05 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hutter", "Edward", ""], ["Solomonik", "Edgar", ""]]}, {"id": "2103.01447", "submitter": "Zhize Li", "authors": "Zhize Li, Peter Richt\\'arik", "title": "ZeroSARAH: Efficient Nonconvex Finite-Sum Optimization with Zero Full\n  Gradient Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ZeroSARAH -- a novel variant of the variance-reduced method SARAH\n(Nguyen et al., 2017) -- for minimizing the average of a large number of\nnonconvex functions $\\frac{1}{n}\\sum_{i=1}^{n}f_i(x)$. To the best of our\nknowledge, in this nonconvex finite-sum regime, all existing variance-reduced\nmethods, including SARAH, SVRG, SAGA and their variants, need to compute the\nfull gradient over all $n$ data samples at the initial point $x^0$, and then\nperiodically compute the full gradient once every few iterations (for SVRG,\nSARAH and their variants). Moreover, SVRG, SAGA and their variants typically\nachieve weaker convergence results than variants of SARAH: $n^{2/3}/\\epsilon^2$\nvs. $n^{1/2}/\\epsilon^2$. ZeroSARAH is the first variance-reduced method which\ndoes not require any full gradient computations, not even for the initial\npoint. Moreover, ZeroSARAH obtains new state-of-the-art convergence results,\nwhich can improve the previous best-known result (given by e.g., SPIDER,\nSpiderBoost, SARAH, SSRGD and PAGE) in certain regimes. Avoiding any full\ngradient computations (which is a time-consuming step) is important in many\napplications as the number of data samples $n$ usually is very large.\nEspecially in the distributed setting, periodic computation of full gradient\nover all data samples needs to periodically synchronize all machines/devices,\nwhich may be impossible or very hard to achieve. Thus, we expect that ZeroSARAH\nwill have a practical impact in distributed and federated learning where full\ndevice participation is impractical.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:31:06 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Li", "Zhize", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2103.01487", "submitter": "Maarten Van Steen", "authors": "Maarten van Steen, Andrew Chien, Patrick Eugster", "title": "The Difficulty in Scaling Blockchains: A Simple Explanation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blockchains have become immensely popular and are high on the list of\nnational and international research and innovation agenda's. This is partly\ncaused by the numerous interesting applications, combined with the promise of\nfull decentralization and high scalability (among others). Keeping that\npromise, however, is technically extremely demanding and has already required\nsubstantial scientific efforts, which so far have not been overwhelmingly\nsuccessful. In this paper, we provide a laymen's description of what may turn\nout to be a fundamental hurdle in attaining blockchains that combine\nscalability, high transaction processing capabilities, and are indeed fully\ndecentralized.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 05:55:24 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["van Steen", "Maarten", ""], ["Chien", "Andrew", ""], ["Eugster", "Patrick", ""]]}, {"id": "2103.01503", "submitter": "Mahdi Soleymani", "authors": "Mahdi Soleymani, Mohammad Vahid Jamali, Hessam Mahdavifar", "title": "Coded Computing via Binary Linear Codes: Designs and Performance Limits", "comments": "arXiv admin note: substantial text overlap with arXiv:1906.10105", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of coded distributed computing where a large linear\ncomputational job, such as a matrix multiplication, is divided into k smaller\ntasks, encoded using an (n,k) linear code, and performed over n distributed\nnodes. The goal is to reduce the average execution time of the computational\njob. We provide a connection between the problem of characterizing the average\nexecution time of a coded distributed computing system and the problem of\nanalyzing the error probability of codes of length n used over erasure\nchannels. Accordingly, we present closed-form expressions for the execution\ntime using binary random linear codes and the best execution time any\nlinear-coded distributed computing system can achieve. It is also shown that\nthere exist good binary linear codes that not only attain (asymptotically) the\nbest performance that any linear code (not necessarily binary) can achieve but\nalso are numerically stable against the inevitable rounding errors in practice.\nWe then develop a low-complexity algorithm for decoding Reed-Muller (RM) codes\nover erasure channels. Our decoder only involves additions and subtractions and\nenables coded computation over real-valued data. Extensive numerical analysis\nof the fundamental results as well as RM- and polar-coded computing schemes\ndemonstrate the excellence of the RM-coded computation in achieving\nclose-to-optimal performance while having a low-complexity decoding and\nexplicit construction. The proposed framework in this paper enables efficient\ndesigns of distributed computing systems given the rich literature in the\nchannel coding theory.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:45:13 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Soleymani", "Mahdi", ""], ["Jamali", "Mohammad Vahid", ""], ["Mahdavifar", "Hessam", ""]]}, {"id": "2103.01538", "submitter": "Sahil Dhoked", "authors": "Sahil Dhoked and Neeraj Mittal", "title": "Memory Reclamation for Recoverable Mutual Exclusion", "comments": "arXiv admin note: text overlap with arXiv:2006.07086", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual exclusion (ME) is a commonly used technique to handle conflicts in\nconcurrent systems. With recent advancements in non-volatile memory technology,\nthere is an increased focus on the problem of recoverable mutual exclusion\n(RME), a special case of ME where processes can fail and recover. However, in\norder to ensure that the problem of RME is also of practical interest, and not\njust a theoretical one, memory reclamation poses as a major obstacle in several\nRME algorithms. Often RME algorithms need to allocate memory dynamically, which\nincreases the memory footprint of the algorithm over time. These algorithms are\ntypically not equipped with suitable garbage collection due to concurrency and\nfailures.\n  In this work, we present the first \"general\" recoverable algorithm for memory\nreclamation in the context of recoverable mutual exclusion. Our algorithm can\nbe plugged into any RME algorithm very easily and preserves all correctness\nproperty and most desirable properties of the algorithm. The space overhead of\nour algorithm is $\\mathcal{O}(n^2 * sizeof(node)\\ )$, where $n$ is the total\nnumber of processes in the system. In terms of remote memory references (RMRs),\nour algorithm is RMR-optimal, i.e, it has a constant RMR overhead per passage.\nOur RMR and space complexities are applicable to both $CC$ and $DSM$ memory\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:40:22 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Dhoked", "Sahil", ""], ["Mittal", "Neeraj", ""]]}, {"id": "2103.01548", "submitter": "Bingyan Liu", "authors": "Bingyan Liu, Yao Guo, Xiangqun Chen", "title": "PFA: Privacy-preserving Federated Adaptation for Effective Model\n  Personalization", "comments": "This paper has been accepted by WWW2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has become a prevalent distributed machine learning\nparadigm with improved privacy. After learning, the resulting federated model\nshould be further personalized to each different client. While several methods\nhave been proposed to achieve personalization, they are typically limited to a\nsingle local device, which may incur bias or overfitting since data in a single\ndevice is extremely limited. In this paper, we attempt to realize\npersonalization beyond a single client. The motivation is that during FL, there\nmay exist many clients with similar data distribution, and thus the\npersonalization performance could be significantly boosted if these similar\nclients can cooperate with each other. Inspired by this, this paper introduces\na new concept called federated adaptation, targeting at adapting the trained\nmodel in a federated manner to achieve better personalization results. However,\nthe key challenge for federated adaptation is that we could not outsource any\nraw data from the client during adaptation, due to privacy concerns. In this\npaper, we propose PFA, a framework to accomplish Privacy-preserving Federated\nAdaptation. PFA leverages the sparsity property of neural networks to generate\nprivacy-preserving representations and uses them to efficiently identify\nclients with similar data distributions. Based on the grouping results, PFA\nconducts an FL process in a group-wise way on the federated model to accomplish\nthe adaptation. For evaluation, we manually construct several practical FL\ndatasets based on public datasets in order to simulate both the class-imbalance\nand background-difference conditions. Extensive experiments on these datasets\nand popular model architectures demonstrate the effectiveness of PFA,\noutperforming other state-of-the-art methods by a large margin while ensuring\nuser privacy. We will release our code at: https://github.com/lebyni/PFA.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:07:34 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 16:11:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Bingyan", ""], ["Guo", "Yao", ""], ["Chen", "Xiangqun", ""]]}, {"id": "2103.01597", "submitter": "Johannes Pekkil\\\"a", "authors": "Johannes Pekkil\\\"a, Miikka S. V\\\"ais\\\"al\\\"a, Maarit J. K\\\"apyl\\\"a,\n  Matthias Rheinhardt, Oskar Lappi", "title": "Scalable communication for high-order stencil computations using\n  CUDA-aware MPI", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph physics.flu-dyn", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern compute nodes in high-performance computing provide a tremendous level\nof parallelism and processing power. However, as arithmetic performance has\nbeen observed to increase at a faster rate relative to memory and network\nbandwidths, optimizing data movement has become critical for achieving strong\nscaling in many communication-heavy applications. This performance gap has been\nfurther accentuated with the introduction of graphics processing units, which\ncan provide by multiple factors higher throughput in data-parallel tasks than\ncentral processing units. In this work, we explore the computational aspects of\niterative stencil loops and implement a generic communication scheme using\nCUDA-aware MPI, which we use to accelerate magnetohydrodynamics simulations\nbased on high-order finite differences and third-order Runge-Kutta integration.\nWe put particular focus on improving intra-node locality of workloads. In\ncomparison to a theoretical performance model, our implementation exhibits\nstrong scaling from one to $64$ devices at $50\\%$--$87\\%$ efficiency in\nsixth-order stencil computations when the problem domain consists of\n$256^3$--$1024^3$ cells.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:44:42 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Pekkil\u00e4", "Johannes", ""], ["V\u00e4is\u00e4l\u00e4", "Miikka S.", ""], ["K\u00e4pyl\u00e4", "Maarit J.", ""], ["Rheinhardt", "Matthias", ""], ["Lappi", "Oskar", ""]]}, {"id": "2103.01858", "submitter": "Cristina Abad", "authors": "Cristina L. Abad and Alexandru Iosup and Edwin F. Boza and Eduardo\n  Ortiz-Holguin", "title": "An Analysis of Distributed Systems Syllabi With a Focus on\n  Performance-Related Topics", "comments": "Accepted for publication at WEPPE 2021, to be held in conjunction\n  with ACM/SPEC ICPE 2021: https://doi.org/10.1145/3447545.3451197 This article\n  is a follow-up of our prior ACM SIGCSE publication, arXiv:2012.00552", "journal-ref": null, "doi": "10.1145/3447545.3451197", "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a dataset of 51 current (2019-2020) Distributed Systems syllabi\nfrom top Computer Science programs, focusing on finding the prevalence and\ncontext in which topics related to performance are being taught in these\ncourses. We also study the scale of the infrastructure mentioned in DS courses,\nfrom small client-server systems to cloud-scale, peer-to-peer, global-scale\nsystems. We make eight main findings, covering goals such as performance, and\nscalability and its variant elasticity; activities such as performance\nbenchmarking and monitoring; eight selected performance-enhancing techniques\n(replication, caching, sharding, load balancing, scheduling, streaming,\nmigrating, and offloading); and control issues such as trade-offs that include\nperformance and performance variability.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:49:09 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Abad", "Cristina L.", ""], ["Iosup", "Alexandru", ""], ["Boza", "Edwin F.", ""], ["Ortiz-Holguin", "Eduardo", ""]]}, {"id": "2103.01869", "submitter": "Amin Totounferoush", "authors": "Amin Totounferoush, Neda Ebrahimi Pour, Sabine Roller and Miriam Mehl", "title": "Parallel Machine Learning of Partial Differential Equations", "comments": "Submitted to PDSEC workshop, IPDPS conference 2021. We will replace\n  with the final version as soon as we have the DOI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a parallel scheme for machine learning of partial\ndifferential equations. The scheme is based on the decomposition of the\ntraining data corresponding to spatial subdomains, where an individual neural\nnetwork is assigned to each data subset. Message Passing Interface (MPI) is\nused for parallelization and data communication. We use convolutional neural\nnetwork layers (CNN) to account for spatial connectivity. We showcase the\nlearning of the linearized Euler equations to assess the accuracy of the\npredictions and the efficiency of the proposed scheme. These equations are of\nparticular interest for aeroacoustic problems. A first investigation\ndemonstrated a very good agreement of the predicted results with the simulation\nresults. In addition, we observe an excellent reduction of the training time\ncompared to the sequential version, providing an almost perfect scalability up\nto 64 CPU cores.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:06:47 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Totounferoush", "Amin", ""], ["Pour", "Neda Ebrahimi", ""], ["Roller", "Sabine", ""], ["Mehl", "Miriam", ""]]}, {"id": "2103.01871", "submitter": "Oksana Shadura", "authors": "Matous Adamec (1), Garhan Attebury (1), Kenneth Bloom (1), Brian\n  Bockelman (2), Carl Lundstedt (1), Oksana Shadura (1) and John Thiltges (1)\n  ((1) University of Nebraska-Lincoln, (2) Morgridge Institute for Research)", "title": "Coffea-casa: an analysis facility prototype", "comments": "Submitted as proceedings fo 25th International Conference on\n  Computing in High-Energy and Nuclear Physics\n  (https://indico.cern.ch/event/948465/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis in HEP has often relied on batch systems and event loops; users\nare given a non-interactive interface to computing resources and consider data\nevent-by-event. The \"Coffea-casa\" prototype analysis facility is an effort to\nprovide users with alternate mechanisms to access computing resources and\nenable new programming paradigms. Instead of the command-line interface and\nasynchronous batch access, a notebook-based web interface and interactive\ncomputing is provided. Instead of writing event loops, the column-based Coffea\nlibrary is used. In this paper, we describe the architectural components of the\nfacility, the services offered to end-users, and how it integrates into a\nlarger ecosystem for data access and authentication.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:08:22 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 12:34:53 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Adamec", "Matous", "", "University of Nebraska-Lincoln"], ["Attebury", "Garhan", "", "University of Nebraska-Lincoln"], ["Bloom", "Kenneth", "", "University of Nebraska-Lincoln"], ["Bockelman", "Brian", "", "Morgridge Institute for Research"], ["Lundstedt", "Carl", "", "University of Nebraska-Lincoln"], ["Shadura", "Oksana", "", "University of Nebraska-Lincoln"], ["Thiltges", "John", "", "University of Nebraska-Lincoln"]]}, {"id": "2103.02051", "submitter": "Yasaman Esfandiari", "authors": "Yasaman Esfandiari, Sin Yong Tan, Zhanhong Jiang, Aditya Balu, Ethan\n  Herron, Chinmay Hegde, Soumik Sarkar", "title": "Cross-Gradient Aggregation for Decentralized Learning from Non-IID data", "comments": "ICML 2021 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized learning enables a group of collaborative agents to learn\nmodels using a distributed dataset without the need for a central parameter\nserver. Recently, decentralized learning algorithms have demonstrated\nstate-of-the-art results on benchmark data sets, comparable with centralized\nalgorithms. However, the key assumption to achieve competitive performance is\nthat the data is independently and identically distributed (IID) among the\nagents which, in real-life applications, is often not applicable. Inspired by\nideas from continual learning, we propose Cross-Gradient Aggregation (CGA), a\nnovel decentralized learning algorithm where (i) each agent aggregates\ncross-gradient information, i.e., derivatives of its model with respect to its\nneighbors' datasets, and (ii) updates its model using a projected gradient\nbased on quadratic programming (QP). We theoretically analyze the convergence\ncharacteristics of CGA and demonstrate its efficiency on non-IID data\ndistributions sampled from the MNIST and CIFAR-10 datasets. Our empirical\ncomparisons show superior learning performance of CGA over existing\nstate-of-the-art decentralized learning algorithms, as well as maintaining the\nimproved performance under information compression to reduce peer-to-peer\ncommunication overhead. The code is available here on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 21:58:12 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 20:11:17 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Esfandiari", "Yasaman", ""], ["Tan", "Sin Yong", ""], ["Jiang", "Zhanhong", ""], ["Balu", "Aditya", ""], ["Herron", "Ethan", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""]]}, {"id": "2103.02060", "submitter": "Georgios Andreadis", "authors": "Georgios Andreadis, Fabian Mastenbroek, Vincent van Beek, Alexandru\n  Iosup", "title": "Capelin: Data-Driven Capacity Procurement for Cloud Datacenters using\n  Portfolios of Scenarios -- Extended Technical Report", "comments": "Technical Report on the TPDS homonym article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud datacenters provide a backbone to our digital society. Inaccurate\ncapacity procurement for cloud datacenters can lead to significant performance\ndegradation, denser targets for failure, and unsustainable energy consumption.\nAlthough this activity is core to improving cloud infrastructure, relatively\nfew comprehensive approaches and support tools exist for mid-tier operators,\nleaving many planners with merely rule-of-thumb judgement. We derive\nrequirements from a unique survey of experts in charge of diverse datacenters\nin several countries. We propose Capelin, a data-driven, scenario-based\ncapacity planning system for mid-tier cloud datacenters. Capelin introduces the\nnotion of portfolios of scenarios, which it leverages in its probing for\nalternative capacity-plans. At the core of the system, a trace-based,\ndiscrete-event simulator enables the exploration of different possible\ntopologies, with support for scaling the volume, variety, and velocity of\nresources, and for horizontal (scale-out) and vertical (scale-up) scaling.\nCapelin compares alternative topologies and for each gives detailed\nquantitative operational information, which could facilitate human decisions of\ncapacity planning. We implement and open-source Capelin, and show through\ncomprehensive trace-based experiments it can aid practitioners. The results\ngive evidence that reasonable choices can be worse by a factor of 1.5-2.0 than\nthe best, in terms of performance degradation or energy consumption.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 22:18:12 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Andreadis", "Georgios", ""], ["Mastenbroek", "Fabian", ""], ["van Beek", "Vincent", ""], ["Iosup", "Alexandru", ""]]}, {"id": "2103.02131", "submitter": "Bogdan Nicolae", "authors": "Bogdan Nicolae and Adam Moody and Gregory Kosinovsky and Kathryn\n  Mohror and Franck Cappello", "title": "VELOC: VEry Low Overhead Checkpointing in the Age of Exascale", "comments": null, "journal-ref": "SuperCheck'21: First International Symposium on Checkpointing for\n  Supercomputing, 2021", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Checkpointing large amounts of related data concurrently to stable storage is\na common I/O pattern of many HPC applications. However, such a pattern\nfrequently leads to I/O bottlenecks that lead to poor scalability and\nperformance. As modern HPC infrastructures continue to evolve, there is a\ngrowing gap between compute capacity vs. I/O capabilities. Furthermore, the\nstorage hierarchy is becoming increasingly heterogeneous: in addition to\nparallel file systems, it comprises burst buffers, key-value stores, deep\nmemory hierarchies at node level, etc. In this context, state of art is\ninsufficient to deal with the diversity of vendor APIs, performance and\npersistency characteristics. This extended abstract presents an overview of\nVeloC (Very Low Overhead Checkpointing System), a checkpointing runtime\nspecifically design to address these challenges for the next generation\nExascale HPC applications and systems. VeloC offers a simple API at user level,\nwhile employing an advanced multi-level resilience strategy that transparently\noptimizes the performance and scalability of checkpointing by leveraging\nheterogeneous storage.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:24:05 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Nicolae", "Bogdan", ""], ["Moody", "Adam", ""], ["Kosinovsky", "Gregory", ""], ["Mohror", "Kathryn", ""], ["Cappello", "Franck", ""]]}, {"id": "2103.02160", "submitter": "Jieyi Long", "authors": "Jieyi Long and Ribao Wei", "title": "Off-Chain Micropayment Pool for High-ThroughputBandwidth Sharing Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a layer-2 micropayment pool design which supports\nhigh-throughput blockchain off-chain payment, designed specifically for the use\ncase of rewarding users who share redundant bandwidth to assist video stream\ndelivery. We analyze the validity and effectiveness of the proposed\nmicropayment pool. Our analysis results demonstrate that the proposed\nmicropayment pool design is better suited for the bandwidth reward use case\ncompared to existing off-chain payment channel solution.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:52:05 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Long", "Jieyi", ""], ["Wei", "Ribao", ""]]}, {"id": "2103.02182", "submitter": "Matthew Feickert", "authors": "Matthew Feickert, Lukas Heinrich, Giordon Stark, Ben Galewsky", "title": "Distributed statistical inference with pyhf enabled through funcX", "comments": "9 pages, 1 figure, 2 listings, 1 table, submitted to the 25th\n  International Conference on Computing in High Energy & Nuclear Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In High Energy Physics facilities that provide High Performance Computing\nenvironments provide an opportunity to efficiently perform the statistical\ninference required for analysis of data from the Large Hadron Collider, but can\npose problems with orchestration and efficient scheduling. The compute\narchitectures at these facilities do not easily support the Python compute\nmodel, and the configuration scheduling of batch jobs for physics often\nrequires expertise in multiple job scheduling services. The combination of the\npure-Python libraries pyhf and funcX reduces the common problem in HEP analyses\nof performing statistical inference with binned models, that would\ntraditionally take multiple hours and bespoke scheduling, to an on-demand\n(fitting) \"function as a service\" that can scalably execute across workers in\njust a few minutes, offering reduced time to insight and inference. We\ndemonstrate execution of a scalable workflow using funcX to simultaneously fit\n125 signal hypotheses from a published ATLAS search for new physics using pyhf\nwith a wall time of under 3 minutes. We additionally show performance\ncomparisons for other physics analyses with openly published probability models\nand argue for a blueprint of fitting as a service systems at HPC centers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:04:16 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Feickert", "Matthew", ""], ["Heinrich", "Lukas", ""], ["Stark", "Giordon", ""], ["Galewsky", "Ben", ""]]}, {"id": "2103.02260", "submitter": "David Fischer", "authors": "Jiali Xing, David Fischer, Nitya Labh, Ryan Piersma, Benjamin C. Lee,\n  Yu Amy Xia, Tuhin Sahai, Vahid Tarokh", "title": "Talaria: A Framework for Simulation of Permissioned Blockchains for\n  Logistics and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Talaria, a novel permissioned blockchain simulator\nthat supports numerous protocols and use cases, most notably in supply chain\nmanagement. Talaria extends the capability of BlockSim, an existing blockchain\nsimulator, to include permissioned blockchains and serves as a foundation for\nfurther private blockchain assessment. Talaria is designed with both practical\nByzantine Fault Tolerance (pBFT) and simplified version of Proof-of-Authority\nconsensus protocols, but can be revised to include other permissioned protocols\nwithin its modular framework. Moreover, Talaria is able to simulate different\ntypes of malicious authorities and a variable daily transaction load at each\nnode. In using Talaria, business practitioners and policy planners have an\nopportunity to measure, evaluate, and adapt a range of blockchain solutions for\ncommercial operations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:43:30 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 00:22:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Xing", "Jiali", ""], ["Fischer", "David", ""], ["Labh", "Nitya", ""], ["Piersma", "Ryan", ""], ["Lee", "Benjamin C.", ""], ["Xia", "Yu Amy", ""], ["Sahai", "Tuhin", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2103.02351", "submitter": "Amirkeivan Mohtashami", "authors": "Sebastian U. Stich, Amirkeivan Mohtashami, Martin Jaggi", "title": "Critical Parameters for Scalable Distributed Learning with Large Batches\n  and Asynchronous Updates", "comments": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been experimentally observed that the efficiency of distributed\ntraining with stochastic gradient (SGD) depends decisively on the batch size\nand -- in asynchronous implementations -- on the gradient staleness.\nEspecially, it has been observed that the speedup saturates beyond a certain\nbatch size and/or when the delays grow too large. We identify a data-dependent\nparameter that explains the speedup saturation in both these settings. Our\ncomprehensive theoretical analysis, for strongly convex, convex and non-convex\nsettings, unifies and generalized prior work directions that often focused on\nonly one of these two aspects. In particular, our approach allows us to derive\nimproved speedup results under frequently considered sparsity assumptions. Our\ninsights give rise to theoretically based guidelines on how the learning rates\ncan be adjusted in practice. We show that our results are tight and illustrate\nkey findings in numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:08:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Stich", "Sebastian U.", ""], ["Mohtashami", "Amirkeivan", ""], ["Jaggi", "Martin", ""]]}, {"id": "2103.02825", "submitter": "Lishan Yang", "authors": "Lishan Yang, Bin Nie, Adwait Jog, Evgenia Smirni", "title": "Enabling Software Resilience in GPGPU Applications via Partial Thread\n  Protection", "comments": "Accepted to the 43rd International Conference on Software Engineering\n  (ICSE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Processing Units (GPUs) are widely used by various applications in a\nbroad variety of fields to accelerate their computation but remain susceptible\nto transient hardware faults (soft errors) that can easily compromise\napplication output. By taking advantage of a general purpose GPU application\nhierarchical organization in threads, warps, and cooperative thread arrays, we\npropose a methodology that identifies the resilience of threads and aims to map\nthreads with the same resilience characteristics to the same warp. This allows\nengaging partial replication mechanisms for error detection/correction at the\nwarp level. By exploring 12 benchmarks (17 kernels) from 4 benchmark suites, we\nillustrate that threads can be remapped into reliable or unreliable warps with\nonly 1.63% introduced overhead (on average), and then enable selective\nprotection via replication to those groups of threads that truly need it.\nFurthermore, we show that thread remapping to different warps does not\nsacrifice application performance. We show how this remapping facilitates warp\nreplication for error detection and/or correction and achieves an average\nreduction of 20.61% and 27.15% execution cycles, respectively comparing to\nstandard duplication/triplication.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 04:04:34 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:02:24 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Lishan", ""], ["Nie", "Bin", ""], ["Jog", "Adwait", ""], ["Smirni", "Evgenia", ""]]}, {"id": "2103.02843", "submitter": "Agastya Bhati", "authors": "Agastya P. Bhati, Shunzhou Wan, Dario Alf\\`e, Austin R. Clyde, Mathis\n  Bode, Li Tan, Mikhail Titov, Andre Merzky, Matteo Turilli, Shantenu Jha,\n  Roger R. Highfield, Walter Rocchia, Nicola Scafuri, Sauro Succi, Dieter\n  Kranzlm\\\"uller, Gerald Mathias, David Wifling, Yann Donon, Alberto Di Meglio,\n  Sofia Vallecorsa, Heng Ma, Anda Trifan, Arvind Ramanathan, Tom Brettin,\n  Alexander Partin, Fangfang Xia, Xiaotan Duan, Rick Stevens, Peter V. Coveney", "title": "Pandemic Drugs at Pandemic Speed: Accelerating COVID-19 Drug Discovery\n  with Hybrid Machine Learning- and Physics-based Simulations on High\n  Performance Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.LG physics.bio-ph q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The race to meet the challenges of the global pandemic has served as a\nreminder that the existing drug discovery process is expensive, inefficient and\nslow. There is a major bottleneck screening the vast number of potential small\nmolecules to shortlist lead compounds for antiviral drug development. New\nopportunities to accelerate drug discovery lie at the interface between machine\nlearning methods, in this case developed for linear accelerators, and\nphysics-based methods. The two in silico methods, each have their own\nadvantages and limitations which, interestingly, complement each other. Here,\nwe present an innovative method that combines both approaches to accelerate\ndrug discovery. The scale of the resulting workflow is such that it is\ndependent on high performance computing. We have demonstrated the applicability\nof this workflow on four COVID-19 target proteins and our ability to perform\nthe required large-scale calculations to identify lead compounds on a variety\nof supercomputers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 05:43:18 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Bhati", "Agastya P.", ""], ["Wan", "Shunzhou", ""], ["Alf\u00e8", "Dario", ""], ["Clyde", "Austin R.", ""], ["Bode", "Mathis", ""], ["Tan", "Li", ""], ["Titov", "Mikhail", ""], ["Merzky", "Andre", ""], ["Turilli", "Matteo", ""], ["Jha", "Shantenu", ""], ["Highfield", "Roger R.", ""], ["Rocchia", "Walter", ""], ["Scafuri", "Nicola", ""], ["Succi", "Sauro", ""], ["Kranzlm\u00fcller", "Dieter", ""], ["Mathias", "Gerald", ""], ["Wifling", "David", ""], ["Donon", "Yann", ""], ["Di Meglio", "Alberto", ""], ["Vallecorsa", "Sofia", ""], ["Ma", "Heng", ""], ["Trifan", "Anda", ""], ["Ramanathan", "Arvind", ""], ["Brettin", "Tom", ""], ["Partin", "Alexander", ""], ["Xia", "Fangfang", ""], ["Duan", "Xiaotan", ""], ["Stevens", "Rick", ""], ["Coveney", "Peter V.", ""]]}, {"id": "2103.02916", "submitter": "Jakob Notland", "authors": "Henrik Knudsen, Jakob Svennevik Notland, Peter Halland Haro, Truls\n  Bakkejord R{\\ae}der, Jingyue Li", "title": "Consensus in Blockchain Systems with Low Network Throughput: A\n  Systematic Mapping Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain technologies originate from cryptocurrencies. Thus, most\nblockchain technologies assume an environment with a fast and stable network.\nHowever, in some blockchain-based systems, e.g., supply chain management (SCM)\nsystems, some Internet of Things (IOT) nodes can only rely on the low-quality\nnetwork sometimes to achieve consensus. Thus, it is critical to understand the\napplicability of existing consensus algorithms in such environments. We\nperformed a systematic mapping study to evaluate and compare existing consensus\nmechanisms' capability to provide integrity and security with varying network\nproperties. Our study identified 25 state-of-the-art consensus algorithms from\npublished and preprint literature. We categorized and compared the consensus\nalgorithms qualitatively based on established performance and integrity metrics\nand well-known blockchain security issues. Results show that consensus\nalgorithms rely on the synchronous network for correctness cannot provide the\nexpected integrity. Such consensus algorithms may also be vulnerable to\ndistributed-denial-of-service (DDOS) and routing attacks, given limited network\nthroughput. Conversely, asynchronous consensus algorithms, e.g.,\nHoney-BadgerBFT, are deemed more robust against many of these attacks and may\nprovide high integrity in asynchrony events.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 09:43:13 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Knudsen", "Henrik", ""], ["Notland", "Jakob Svennevik", ""], ["Haro", "Peter Halland", ""], ["R\u00e6der", "Truls Bakkejord", ""], ["Li", "Jingyue", ""]]}, {"id": "2103.02928", "submitter": "Eduin Hernandez", "authors": "Busra Tegin, Eduin. E. Hernandez, Stefano Rini, Tolga M. Duman", "title": "Straggler Mitigation through Unequal Error Protection for Distributed\n  Approximate Matrix Multiplication", "comments": "16 pages. arXiv admin note: text overlap with arXiv:2011.02749", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale machine learning and data mining methods routinely distribute\ncomputations across multiple agents to parallelize processing. The time\nrequired for the computations at the agents is affected by the availability of\nlocal resources and/or poor channel conditions giving rise to the \"straggler\nproblem\". As a remedy to this problem, we employ Unequal Error Protection (UEP)\ncodes to obtain an approximation of the matrix product in the distributed\ncomputation setting to provide higher protection for the blocks with higher\neffect on the final result. We characterize the performance of the proposed\napproach from a theoretical perspective by bounding the expected reconstruction\nerror for matrices with uncorrelated entries. We also apply the proposed coding\nstrategy to the computation of the back-propagation step in the training of a\nDeep Neural Network (DNN) for an image classification task in the evaluation of\nthe gradients. Our numerical experiments show that it is indeed possible to\nobtain significant improvements in the overall time required to achieve the DNN\ntraining convergence by producing approximation of matrix products using UEP\ncodes in the presence of stragglers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:19:59 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 12:59:18 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Tegin", "Busra", ""], ["Hernandez", "Eduin. E.", ""], ["Rini", "Stefano", ""], ["Duman", "Tolga M.", ""]]}, {"id": "2103.02958", "submitter": "Yuncheng Wu", "authors": "Yuncheng Wu, Tien Tuan Anh Dinh, Guoyu Hu, Meihui Zhang, Yeow Meng\n  Chee, Beng Chin Ooi", "title": "Serverless Model Serving for Data Science", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) is an important part of modern data science\napplications. Data scientists today have to manage the end-to-end ML life cycle\nthat includes both model training and model serving, the latter of which is\nessential, as it makes their works available to end-users. Systems for model\nserving require high performance, low cost, and ease of management. Cloud\nproviders are already offering model serving options, including managed\nservices and self-rented servers. Recently, serverless computing, whose\nadvantages include high elasticity and fine-grained cost model, brings another\npossibility for model serving.\n  In this paper, we study the viability of serverless as a mainstream model\nserving platform for data science applications. We conduct a comprehensive\nevaluation of the performance and cost of serverless against other model\nserving systems on two clouds: Amazon Web Service (AWS) and Google Cloud\nPlatform (GCP). We find that serverless outperforms many cloud-based\nalternatives with respect to cost and performance. More interestingly, under\nsome circumstances, it can even outperform GPU-based systems for both average\nlatency and cost. These results are different from previous works' claim that\nserverless is not suitable for model serving, and are contrary to the\nconventional wisdom that GPU-based systems are better for ML workloads than\nCPU-based systems. Other findings include a large gap in cold start time\nbetween AWS and GCP serverless functions, and serverless' low sensitivity to\nchanges in workloads or models. Our evaluation results indicate that serverless\nis a viable option for model serving. Finally, we present several practical\nrecommendations for data scientists on how to use serverless for scalable and\ncost-effective model serving.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:23:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wu", "Yuncheng", ""], ["Dinh", "Tien Tuan Anh", ""], ["Hu", "Guoyu", ""], ["Zhang", "Meihui", ""], ["Chee", "Yeow Meng", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2103.02979", "submitter": "Krishnasuri Narayanam", "authors": "Krishnasuri Narayanam, Seep Goel, Abhishek Singh, Yedendra\n  Shrinivasan, Parameswaram Selvam", "title": "Blockchain Based Accounts Payable Platform for Goods Trade", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goods trade is a supply chain transaction that involves shippers buying goods\nfrom suppliers and carriers providing goods transportation. Shippers are issued\ninvoices from suppliers and carriers. Shippers carry out goods receiving and\ninvoice processing before payment processing of bills for suppliers and\ncarriers, where invoice processing includes tasks like processing claims and\nadjusting the bill payments. Goods receiving involves verification of received\ngoods by the Shipper's receiving team. Invoice processing is carried out by the\nShipper's accounts payable team, which in turn is verified by the accounts\nreceivable teams of suppliers and carriers. This paper presents a\nblockchain-based accounts payable system that generates claims for the\ndeficiency in the goods received and accordingly adjusts the payment in the\nbills for suppliers and carriers. Primary motivations for these supply chain\norganizations to adopt blockchain-based accounts payable systems are to\neliminate the process redundancies (accounts payable vs. accounts receivable),\nto reduce the number of disputes among the transacting participants, and to\naccelerate the accounts payable processes via optimizations in the claims\ngeneration and blockchain-based dispute reconciliation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:57:24 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Narayanam", "Krishnasuri", ""], ["Goel", "Seep", ""], ["Singh", "Abhishek", ""], ["Shrinivasan", "Yedendra", ""], ["Selvam", "Parameswaram", ""]]}, {"id": "2103.03013", "submitter": "Georg Hager", "authors": "Christie Alappat and Nils Meyer and Jan Laukemann and Thomas Gruber\n  and Georg Hager and Gerhard Wellein and Tilo Wettig", "title": "ECM modeling and performance tuning of SpMV and Lattice QCD on A64FX", "comments": "31 pages, 24 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The A64FX CPU is arguably the most powerful Arm-based processor design to\ndate. Although it is a traditional cache-based multicore processor, its peak\nperformance and memory bandwidth rival accelerator devices. A good\nunderstanding of its performance features is of paramount importance for\ndevelopers who wish to leverage its full potential. We present an architectural\nanalysis of the A64FX used in the Fujitsu FX1000 supercomputer at a level of\ndetail that allows for the construction of Execution-Cache-Memory (ECM)\nperformance models for steady-state loops. In the process we identify\narchitectural peculiarities that point to viable generic optimization\nstrategies. After validating the model using simple streaming loops we apply\nthe insight gained to sparse matrix-vector multiplication (SpMV) and the domain\nwall (DW) kernel from quantum chromodynamics (QCD). For SpMV we show why the\nCRS matrix storage format is not a good practical choice on this architecture\nand how the SELL-$C$-$\\sigma$ format can achieve bandwidth saturation. For the\nDW kernel we provide a cache-reuse analysis and show how an appropriate choice\nof data layout for complex arrays can realize memory-bandwidth saturation in\nthis case as well. A comparison with state-of-the-art high-end Intel Cascade\nLake AP and Nvidia V100 systems puts the capabilities of the A64FX into\nperspective. We also explore the potential for power optimizations using the\ntuning knobs provided by the Fugaku system, achieving energy savings of about\n31% for SpMV and 18% for DW.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:21:38 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Alappat", "Christie", ""], ["Meyer", "Nils", ""], ["Laukemann", "Jan", ""], ["Gruber", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Wettig", "Tilo", ""]]}, {"id": "2103.03032", "submitter": "Hans van Ditmarsch", "authors": "Hans van Ditmarsch", "title": "Wanted Dead or Alive : Epistemic logic for impure simplicial complexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a logic of knowledge for impure simplicial complexes. Impure\nsimplicial complexes represent distributed systems under uncertainty over which\nprocesses are still active (are alive) and which processes have failed or\ncrashed (are dead). Our work generalizes the logic of knowledge for pure\nsimplicial complexes, where all processes are alive, by Goubault et al. Our\nlogical semantics has a satisfaction relation defined simultaneously with a\ndefinability relation. The latter restricts which formulas are allowed to have\na truth value: dead processes cannot know or be ignorant of any proposition,\nand live processes cannot know or be ignorant of propositions involving\nprocesses they know to be dead. The logic satisfies some but not all axioms and\nrules of the modal logic S5. Impure simplicial complexes correspond to Kripke\nmodels where each agent's accessibility relation is an equivalence relation on\na subset of the domain only, and otherwise empty, and where each propositional\nvariable is known by an agent. We also propose a notion of bisimulation for\nimpure simplexes and show bisimulation correspondence on certain finitary\nsimplexes. % Dynamic aspects of our semantics, such as how to formalize\npossibly incomplete tasks and algorithms in distributed computing, is left for\nfuture research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:47:35 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["van Ditmarsch", "Hans", ""]]}, {"id": "2103.03044", "submitter": "Ramon Canal", "authors": "Giovanni Agosta, William Fornaciari, David Atienza, Ramon Canal,\n  Alessandro Cilardo, Jos\\'e Flich Cardo, Carles Hernandez Luz, Michal\n  Kulczewski, Giuseppe Massari, Rafael Tornero Gavil\\'a, Marina Zapater", "title": "The RECIPE Approach to Challenges in Deeply Heterogeneous High\n  Performance Systems", "comments": null, "journal-ref": "Microprocessors and Microsystems, Volume 77, 2020", "doi": "10.1016/j.micpro.2020.103185", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  RECIPE (REliable power and time-ConstraInts-aware Predictive management of\nheterogeneous Exascale systems) is a recently started project funded within the\nH2020 FETHPC programme, which is expressly targeted at exploring new\nHigh-Performance Computing (HPC) technologies. RECIPE aims at introducing a\nhierarchical runtime resource management infrastructure to optimize energy\nefficiency and minimize the occurrence of thermal hotspots, while enforcing the\ntime constraints imposed by the applications and ensuring reliability for both\ntime-critical and throughput-oriented computation that run on deeply\nheterogeneous accelerator-based systems. This paper presents a detailed\noverview of RECIPE, identifying the fundamental challenges as well as the key\ninnovations addressed by the project. In particular, the need for predictive\nreliability approaches to maximize hardware lifetime and guarantee application\nperformance is identified as the key concern for RECIPE, and is addressed via\nhierarchical resource management of the heterogeneous architectural components\nof the system, driven by estimates of the application latency and hardware\nreliability obtained respectively through timing analysis and modelling thermal\nproperties, mean-time-to-failure of subsystems. We show the impact of\nprediction accuracy on the overheads imposed by the checkpointing policy, as\nwell as a possible application to a weather forecasting use case.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:06:06 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Agosta", "Giovanni", ""], ["Fornaciari", "William", ""], ["Atienza", "David", ""], ["Canal", "Ramon", ""], ["Cilardo", "Alessandro", ""], ["Cardo", "Jos\u00e9 Flich", ""], ["Luz", "Carles Hernandez", ""], ["Kulczewski", "Michal", ""], ["Massari", "Giuseppe", ""], ["Gavil\u00e1", "Rafael Tornero", ""], ["Zapater", "Marina", ""]]}, {"id": "2103.03175", "submitter": "Georg Hager", "authors": "Ayesha Afzal and Georg Hager and Gerhard Wellein", "title": "Analytic Modeling of Idle Waves in Parallel Programs: Communication,\n  Cluster Topology, and Noise Impact", "comments": "19 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most distributed-memory bulk-synchronous parallel programs in HPC assume that\ncompute resources are available continuously and homogeneously across the\nallocated set of compute nodes. However, long one-off delays on individual\nprocesses can cause global disturbances, so-called idle waves, by rippling\nthrough the system. This process is mainly governed by the communication\ntopology of the underlying parallel code. This paper makes significant\ncontributions to the understanding of idle wave dynamics. We study the\npropagation mechanisms of idle waves across the ranks of MPI-parallel programs.\nWe present a validated analytic model for their propagation velocity with\nrespect to communication parameters and topology, with a special emphasis on\nsparse communication patterns. We study the interaction of idle waves with MPI\ncollectives and show that, depending on the implementation, a collective may be\ntransparent to the wave. Finally we analyze two mechanisms of idle wave decay:\ntopological decay, which is rooted in differences in communication\ncharacteristics among parts of the system, and noise-induced decay, which is\ncaused by system or application noise. We show that noise-induced decay is\nlargely independent of noise characteristics but depends only on the overall\nnoise power. An analytic expression for idle wave decay rate with respect to\nnoise power is derived. For model validation we use microbenchmarks and stencil\nalgorithms on three different supercomputing platforms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:36:18 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Afzal", "Ayesha", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "2103.03180", "submitter": "Pramod Mane", "authors": "Pramod C. Mane, Kapil Ahuja, Pradeep Singh", "title": "A Critical Note on Social Cloud", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of a social cloud has emerged as a resource sharing paradigm in a\nsocial network context. Undoubtedly, state-of-the-art social cloud systems\ndemonstrate the potential of the social cloud acting as complementary to other\ncomputing paradigms such as the cloud, grid, peer-to-peer and volunteer\ncomputing. However, in this note, we have done a critical survey of the social\ncloud literature and come to the conclusion that these initial efforts fail to\noffer a general framework of the social cloud, also, to show the uniqueness of\nthe social cloud. This short note reveals that there are significant\ndifferences regarding the concept of social cloud, resource definition,\nresource sharing and allocation mechanism, and its application and\nstakeholders. This study is an attempt to express a need for a general\nframework of the social cloud, which can incorporate various views and resource\nsharing setups discussed in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 19:12:08 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Mane", "Pramod C.", ""], ["Ahuja", "Kapil", ""], ["Singh", "Pradeep", ""]]}, {"id": "2103.03181", "submitter": "Zhuolun Xiang", "authors": "Rati Gelashvili, Lefteris Kokoris-Kogias, Alexander Spiegelman,\n  Zhuolun Xiang", "title": "Be Prepared When Network Goes Bad: An Asynchronous View-Change Protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of permissioned blockchain systems demands BFT SMR protocols\nthat are efficient under good network conditions (synchrony) and robust under\nbad network conditions (asynchrony). The state-of-the-art partially synchronous\nBFT SMR protocols provide optimal linear communication cost per decision under\nsynchrony and good leaders, but lose liveness under asynchrony. On the other\nhand, the state-of-the-art asynchronous BFT SMR protocols are live even under\nasynchrony, but always pay quadratic cost even under synchrony. In this paper,\nwe propose a BFT SMR protocol that achieves the best of both worlds -- optimal\nlinear cost per decision under good networks and leaders, optimal quadratic\ncost per decision under bad networks, and remains always live.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:40:33 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Gelashvili", "Rati", ""], ["Kokoris-Kogias", "Lefteris", ""], ["Spiegelman", "Alexander", ""], ["Xiang", "Zhuolun", ""]]}, {"id": "2103.03239", "submitter": "Max Ryabinin", "authors": "Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, Gennady\n  Pekhimenko", "title": "Moshpit SGD: Communication-Efficient Decentralized Training on\n  Heterogeneous Unreliable Devices", "comments": "41 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks on large datasets can often be accelerated by\nusing multiple compute nodes. This approach, known as distributed training, can\nutilize hundreds of computers via specialized message-passing protocols such as\nRing All-Reduce. However, running these protocols at scale requires reliable\nhigh-speed networking that is only available in dedicated clusters. In\ncontrast, many real-world applications, such as federated learning and\ncloud-based distributed training, operate on unreliable devices with unstable\nnetwork bandwidth. As a result, these applications are restricted to using\nparameter servers or gossip-based averaging protocols. In this work, we lift\nthat restriction by proposing Moshpit All-Reduce -- an iterative averaging\nprotocol that exponentially converges to the global average. We demonstrate the\nefficiency of our protocol for distributed optimization with strong theoretical\nguarantees. The experiments show 1.3x speedup for ResNet-50 training on\nImageNet compared to competitive gossip-based strategies and 1.5x speedup when\ntraining ALBERT-large from scratch using preemptible compute nodes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:58:05 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ryabinin", "Max", ""], ["Gorbunov", "Eduard", ""], ["Plokhotnyuk", "Vsevolod", ""], ["Pekhimenko", "Gennady", ""]]}, {"id": "2103.03247", "submitter": "Mateusz Iwo Dubaniowski", "authors": "Mateusz Iwo Dubaniowski, Hans Rudolf Heinimann", "title": "Time granularity impact on propagation of disruptions in a\n  system-of-systems simulation of infrastructure and business networks", "comments": "26 pages, 11 figures, 2 tables, Submitted to International Journal of\n  Environmental Research and Public Health: Special Issue on Cascading Disaster\n  Modelling and Prevention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  System-of-systems (SoS) approach is often used for simulating disruptions to\nbusiness and infrastructure system networks allowing for integration of several\nmodels into one simulation. However, the integration is frequently challenging\nas each system is designed individually with different characteristics, such as\ntime granularity. Understanding the impact of time granularity on propagation\nof disruptions between businesses and infrastructure systems and finding the\nappropriate granularity for the SoS simulation remain as major challenges. To\ntackle these, we explore how time granularity, recovery time, and disruption\nsize affect the propagation of disruptions between constituent systems of an\nSoS simulation. To address this issue, we developed a High Level Architecture\n(HLA) simulation of 3 networks and performed a series of simulation\nexperiments. Our results revealed that time granularity and especially recovery\ntime have huge impact on propagation of disruptions. Consequently, we developed\na model for selecting an appropriate time granularity for an SoS simulation\nbased on expected recovery time. Our simulation experiments show that time\ngranularity should be less than 1.13 of expected recovery time. We identified\nsome areas for future research centered around extending the experimental\nfactors space.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:39:37 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Dubaniowski", "Mateusz Iwo", ""], ["Heinimann", "Hans Rudolf", ""]]}, {"id": "2103.03311", "submitter": "Twinkle Jain", "authors": "Twinkle Jain, Jie Wang", "title": "Checkpointing SPAdes for Metagenome Assembly: Transparency versus\n  Performance in Production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SPAdes assembler for metagenome assembly is a long-running application\ncommonly used at the NERSC supercomputing site. However, NERSC, like many other\nsites, has a 48-hour limit on resource allocations. The solution is to chain\ntogether multiple resource allocations in a single run, using\ncheckpoint-restart. This case study provides insights into the \"pain points\" in\napplying a well-known checkpointing package (DMTCP: Distributed MultiThreaded\nCheckPointing) to long-running production workloads of SPAdes. This work has\nexposed several bugs and limitations of DMTCP, which were fixed to support the\nlarge memory and fragmented intermediate files of SPAdes. But perhaps more\ninteresting for other applications, this work reveals a tension between the\ntransparency goals of DMTCP and performance concerns due to an I/O bottleneck\nduring the checkpointing process when supporting large memory and many files.\nSuggestions are made for overcoming this I/O bottleneck, which provides\nimportant \"lessons learned\" for similar applications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:31:00 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Jain", "Twinkle", ""], ["Wang", "Jie", ""]]}, {"id": "2103.03452", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran-Dinh, Nhan H. Pham, Dzung T. Phan, and Lam M. Nguyen", "title": "FedDR -- Randomized Douglas-Rachford Splitting Algorithms for Nonconvex\n  Federated Composite Optimization", "comments": "38 pages, and 12 figures", "journal-ref": null, "doi": null, "report-no": "UNC-STOR-June 2021", "categories": "stat.ML cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop two new algorithms, called, FedDR and asyncFedDR, for solving a\nfundamental nonconvex composite optimization problem in federated learning. Our\nalgorithms rely on a novel combination between a nonconvex Douglas-Rachford\nsplitting method, randomized block-coordinate strategies, and asynchronous\nimplementation. They can also handle convex regularizers. Unlike recent methods\nin the literature, e.g., FedSplit and FedPD, our algorithms update only a\nsubset of users at each communication round, and possibly in an asynchronous\nmanner, making them more practical. These new algorithms also achieve\ncommunication efficiency and more importantly can handle statistical and system\nheterogeneity, which are the two main challenges in federated learning. Our\nconvergence analysis shows that the new algorithms match the communication\ncomplexity lower bound up to a constant factor under standard assumptions. Our\nnumerical experiments illustrate the advantages of our methods compared to\nexisting ones on several datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:24:04 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 20:44:07 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tran-Dinh", "Quoc", ""], ["Pham", "Nhan H.", ""], ["Phan", "Dzung T.", ""], ["Nguyen", "Lam M.", ""]]}, {"id": "2103.03653", "submitter": "Maciej Besta", "authors": "Maciej Besta, Zur Vonarburg-Shmaria, Yannick Schaffner, Leonardo\n  Schwarz, Grzegorz Kwasniewski, Lukas Gianinazzi, Jakub Beranek, Kacper Janda,\n  Tobias Holenstein, Sebastian Leisinger, Peter Tatkowski, Esref Ozdemir,\n  Adrian Balla, Marcin Copik, Philipp Lindenberger, Pavel Kalvoda, Marek\n  Konieczny, Onur Mutlu, Torsten Hoefler", "title": "GraphMineSuite: Enabling High-Performance and Programmable Graph Mining\n  Algorithms with Set Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.DS cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose GraphMineSuite (GMS): the first benchmarking suite for graph\nmining that facilitates evaluating and constructing high-performance graph\nmining algorithms. First, GMS comes with a benchmark specification based on\nextensive literature review, prescribing representative problems, algorithms,\nand datasets. Second, GMS offers a carefully designed software platform for\nseamless testing of different fine-grained elements of graph mining algorithms,\nsuch as graph representations or algorithm subroutines. The platform includes\nparallel implementations of more than 40 considered baselines, and it\nfacilitates developing complex and fast mining algorithms. High modularity is\npossible by harnessing set algebra operations such as set intersection and\ndifference, which enables breaking complex graph mining algorithms into simple\nbuilding blocks that can be separately experimented with. GMS is supported with\na broad concurrency analysis for portability in performance insights, and a\nnovel performance metric to assess the throughput of graph mining algorithms,\nenabling more insightful evaluation. As use cases, we harness GMS to rapidly\nredesign and accelerate state-of-the-art baselines of core graph mining\nproblems: degeneracy reordering (by up to >2x), maximal clique listing (by up\nto >9x), k-clique listing (by 1.1x), and subgraph isomorphism (by up to 2.5x),\nalso obtaining better theoretical performance bounds.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:26:18 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Besta", "Maciej", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Schaffner", "Yannick", ""], ["Schwarz", "Leonardo", ""], ["Kwasniewski", "Grzegorz", ""], ["Gianinazzi", "Lukas", ""], ["Beranek", "Jakub", ""], ["Janda", "Kacper", ""], ["Holenstein", "Tobias", ""], ["Leisinger", "Sebastian", ""], ["Tatkowski", "Peter", ""], ["Ozdemir", "Esref", ""], ["Balla", "Adrian", ""], ["Copik", "Marcin", ""], ["Lindenberger", "Philipp", ""], ["Kalvoda", "Pavel", ""], ["Konieczny", "Marek", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2103.03667", "submitter": "Abhishek Singh", "authors": "Abhishek Narain Singh", "title": "SasCsvToolkit -- A versatile parallel 'bag-of-tasks' job submission\n  application on heterogeneous and homogeneous platforms for Big Data Analytics\n  such as for Biomedical Informatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: The need for big data analysis requires being able to process\nlarge data which are being held fine-tuned for usage by corporate. It is only\nvery recently that the need for big data has caught attention for low budget\ncorporate groups and academia who typically do not have money and resources to\nbuy expensive licenses of big data analysis platforms such as SAS. The\ncorporate continue to work on SAS data format largely because of systemic\norganizational history and that the prior codes have been built on them. The\ndata-providers continue to thus provide data in SAS formats. Acute sudden need\nhas arisen because of this gap of data being in SAS format and the coders not\nhaving a SAS expertise or training background as the economic and inertial\nforces acting of having shaped these two class of people have been different.\nMethod: We analyze the differences and thus the need for SasCsvToolkit which\nhelps to generate a CSV file for a SAS format data so that the data scientist\ncan then make use of his skills in other tools that can process CSVs such as R,\nSPSS, or even Microsoft Excel. At the same time, it also provides conversion of\nCSV files to SAS format. Apart from this, a SAS database programmer always\nstruggles in finding the right method to do a database search, exact match,\nsubstring match, except condition, filters, unique values, table joins and data\nmining for which the toolbox also provides template scripts to modify and use\nfrom command line. Results: The toolkit has been implemented on SLURM scheduler\nplatform as a `bag-of-tasks` algorithm for parallel and distributed workflow\nthough serial version has also been incorporated.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:48:03 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Singh", "Abhishek Narain", ""]]}, {"id": "2103.03918", "submitter": "Runhua Xu", "authors": "Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, James Joshi, Heiko\n  Ludwig", "title": "FedV: Privacy-Preserving Federated Learning over Vertically Partitioned\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning (FL) has been proposed to allow collaborative training of\nmachine learning (ML) models among multiple parties where each party can keep\nits data private. In this paradigm, only model updates, such as model weights\nor gradients, are shared. Many existing approaches have focused on horizontal\nFL, where each party has the entire feature set and labels in the training data\nset. However, many real scenarios follow a vertically-partitioned FL setup,\nwhere a complete feature set is formed only when all the datasets from the\nparties are combined, and the labels are only available to a single party.\nPrivacy-preserving vertical FL is challenging because complete sets of labels\nand features are not owned by one entity. Existing approaches for vertical FL\nrequire multiple peer-to-peer communications among parties, leading to lengthy\ntraining times, and are restricted to (approximated) linear models and just two\nparties. To close this gap, we propose FedV, a framework for secure gradient\ncomputation in vertical settings for several widely used ML models such as\nlinear models, logistic regression, and support vector machines. FedV removes\nthe need for peer-to-peer communication among parties by using functional\nencryption schemes; this allows FedV to achieve faster training times. It also\nworks for larger and changing sets of parties. We empirically demonstrate the\napplicability for multiple types of ML models and show a reduction of 10%-70%\nof training time and 80% to 90% in data transfer with respect to the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 19:59:29 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 20:31:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xu", "Runhua", ""], ["Baracaldo", "Nathalie", ""], ["Zhou", "Yi", ""], ["Anwar", "Ali", ""], ["Joshi", "James", ""], ["Ludwig", "Heiko", ""]]}, {"id": "2103.04014", "submitter": "Chuan-Zheng Lee", "authors": "Chuan-Zheng Lee, Leighton Pate Barnes and Ayfer Ozgur", "title": "Over-the-Air Statistical Estimation", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study schemes and lower bounds for distributed minimax statistical\nestimation over a Gaussian multiple-access channel (MAC) under squared error\nloss, in a framework combining statistical estimation and wireless\ncommunication. First, we develop \"analog\" joint estimation-communication\nschemes that exploit the superposition property of the Gaussian MAC and we\ncharacterize their risk in terms of the number of nodes and dimension of the\nparameter space. Then, we derive information-theoretic lower bounds on the\nminimax risk of any estimation scheme restricted to communicate the samples\nover a given number of uses of the channel and show that the risk achieved by\nour proposed schemes is within a logarithmic factor of these lower bounds. We\ncompare both achievability and lower bound results to previous \"digital\" lower\nbounds, where nodes transmit errorless bits at the Shannon capacity of the MAC,\nshowing that estimation schemes that leverage the physical layer offer a\ndrastic reduction in estimation error over digital schemes relying on a\nphysical-layer abstraction.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 03:07:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lee", "Chuan-Zheng", ""], ["Barnes", "Leighton Pate", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "2103.04119", "submitter": "Parham Hadikhani", "authors": "Meysam Yari, Parham Hadikhani, Mohammad Yaghoubi, Raza Nowrozy, Zohreh\n  Asgharzadeh", "title": "An Energy Efficient Routing Algorithm for Wireless Sensor Networks Using\n  Mobile Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing usage of wireless sensor networks in human life is an\nindication of the high importance of this technology. Wireless sensor networks\nhave a vast majority of applications in monitoring and care which are known as\ntarget tracking. In this application, the moving targets are monitored and\ntracked in the environment. One of the most important challenges in this area\nis the limited energy of the sensors. In this paper, we proposed a new\nalgorithm to reduce energy consumption by increasing the load balancing in the\nnetwork. The proposed algorithm consists of four phases. In the first phase,\nwhich is the hole prevention phase, in each cluster, it is checked by the\ncluster heads that if the energy level in an area of the cluster is less than\nthe threshold, a mobile node is sent to that area. The second phase is the\nupdate phase. In this phase, the parameters required to detect a hole are\nupdated. In the third phase, the hole in the cluster is detected, and in the\nfourth phase, the hole is covered by static or moving nodes. A comparison of\nsimulation results with the well-known and successful routing method in\nwireless sensor networks show that the proposed method is suitable and working\nproperly.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 14:25:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yari", "Meysam", ""], ["Hadikhani", "Parham", ""], ["Yaghoubi", "Mohammad", ""], ["Nowrozy", "Raza", ""], ["Asgharzadeh", "Zohreh", ""]]}, {"id": "2103.04185", "submitter": "Christian Pilato", "authors": "Christian Pilato, Stanislav Bohm, Fabien Brocheton, Jeronimo\n  Castrillon, Riccardo Cevasco, Vojtech Cima, Radim Cmar, Dionysios\n  Diamantopoulos, Fabrizio Ferrandi, Jan Martinovic, Gianluca Palermo, Michele\n  Paolino, Antonio Parodi, Lorenzo Pittaluga, Daniel Raho, Francesco Regazzoni,\n  Katerina Slaninova, Christoph Hagleitner", "title": "EVEREST: A design environment for extreme-scale big data analytics on\n  heterogeneous platforms", "comments": "Paper accepted for presentation at the IEEE/EDAC/ACM Design,\n  Automation and Test in Europe Conference and Exhibition (DATE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-Performance Big Data Analytics (HPDA) applications are characterized by\nhuge volumes of distributed and heterogeneous data that require efficient\ncomputation for knowledge extraction and decision making. Designers are moving\ntowards a tight integration of computing systems combining HPC, Cloud, and IoT\nsolutions with artificial intelligence (AI). Matching the application and data\nrequirements with the characteristics of the underlying hardware is a key\nelement to improve the predictions thanks to high performance and better use of\nresources.\n  We present EVEREST, a novel H2020 project started on October 1st, 2020 that\naims at developing a holistic environment for the co-design of HPDA\napplications on heterogeneous, distributed, and secure platforms. EVEREST\nfocuses on programmability issues through a data-driven design approach, the\nuse of hardware-accelerated AI, and an efficient runtime monitoring with\nvirtualization support. In the different stages, EVEREST combines\nstate-of-the-art programming models, emerging communication standards, and\nnovel domain-specific extensions. We describe the EVEREST approach and the use\ncases that drive our research.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 20:01:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Pilato", "Christian", ""], ["Bohm", "Stanislav", ""], ["Brocheton", "Fabien", ""], ["Castrillon", "Jeronimo", ""], ["Cevasco", "Riccardo", ""], ["Cima", "Vojtech", ""], ["Cmar", "Radim", ""], ["Diamantopoulos", "Dionysios", ""], ["Ferrandi", "Fabrizio", ""], ["Martinovic", "Jan", ""], ["Palermo", "Gianluca", ""], ["Paolino", "Michele", ""], ["Parodi", "Antonio", ""], ["Pittaluga", "Lorenzo", ""], ["Raho", "Daniel", ""], ["Regazzoni", "Francesco", ""], ["Slaninova", "Katerina", ""], ["Hagleitner", "Christoph", ""]]}, {"id": "2103.04195", "submitter": "Rafael Ferreira da Silva", "authors": "Ewa Deelman (1), Anirban Mandal (2), Angela P. Murillo (3), Jarek\n  Nabrzyski (5), Valerio Pascucci (4), Robert Ricci (4), Ilya Baldin (2), Susan\n  Sons (3), Laura Christopherson (2), Charles Vardeman (5), Rafael Ferreira da\n  Silva (1), Jane Wyngaard (5), Steve Petruzza (4), Mats Rynge (1), Karan Vahi\n  (1), Wendy R. Whitcup (1), Josh Drake (3), Erik Scott (2) ((1) University of\n  Southern California, (2) University of North Carolina Chapel Hill, (3)\n  Indiana University, (4) University of Utah, (5) University of Notre Dame)", "title": "Blueprint: Cyberinfrastructure Center of Excellence", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.4587866", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In 2018, NSF funded an effort to pilot a Cyberinfrastructure Center of\nExcellence (CI CoE or Center) that would serve the cyberinfrastructure (CI)\nneeds of the NSF Major Facilities (MFs) and large projects with advanced CI\narchitectures. The goal of the CI CoE Pilot project (Pilot) effort was to\ndevelop a model and a blueprint for such a CoE by engaging with the MFs,\nunderstanding their CI needs, understanding the contributions the MFs are\nmaking to the CI community, and exploring opportunities for building a broader\nCI community. This document summarizes the results of community engagements\nconducted during the first two years of the project and describes the\nidentified CI needs of the MFs. To better understand MFs' CI, the Pilot has\ndeveloped and validated a model of the MF data lifecycle that follows the data\ngeneration and management within a facility and gained an understanding of how\nthis model captures the fundamental stages that the facilities' data passes\nthrough from the scientific instruments to the principal investigators and\ntheir teams, to the broader collaborations and the public. The Pilot also aimed\nto understand what CI workforce development challenges the MFs face while\ndesigning, constructing, and operating their CI and what solutions they are\nexploring and adopting within their projects. Based on the needs of the MFs in\nthe data lifecycle and workforce development areas, this document outlines a\nblueprint for a CI CoE that will learn about and share the CI solutions\ndesigned, developed, and/or adopted by the MFs, provide expertise to the\nlargest NSF projects with advanced and complex CI architectures, and foster a\ncommunity of CI practitioners and researchers.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 20:52:13 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Deelman", "Ewa", ""], ["Mandal", "Anirban", ""], ["Murillo", "Angela P.", ""], ["Nabrzyski", "Jarek", ""], ["Pascucci", "Valerio", ""], ["Ricci", "Robert", ""], ["Baldin", "Ilya", ""], ["Sons", "Susan", ""], ["Christopherson", "Laura", ""], ["Vardeman", "Charles", ""], ["da Silva", "Rafael Ferreira", ""], ["Wyngaard", "Jane", ""], ["Petruzza", "Steve", ""], ["Rynge", "Mats", ""], ["Vahi", "Karan", ""], ["Whitcup", "Wendy R.", ""], ["Drake", "Josh", ""], ["Scott", "Erik", ""]]}, {"id": "2103.04234", "submitter": "Salem Alqahtani", "authors": "Salem Alqahtani and Murat Demirbas", "title": "Bottlenecks in Blockchain Consensus Protocols", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the Blockchain permissioned systems employ Byzantine fault-tolerance\n(BFT) consensus protocols to ensure that honest validators agree on the order\nfor appending entries to their ledgers. In this paper, we study the performance\nand the scalability of prominent consensus protocols, namely PBFT, Tendermint,\nHotStuff, and Streamlet, both analytically via load formulas and practically\nvia implementation and evaluation. Under identical conditions, we identify the\nbottlenecks of these consensus protocols and show that these protocols do not\nscale well as the number of validators increases. Our investigation points to\nthe communication complexity as the culprit. Even when there is enough network\nbandwidth, the CPU cost of serialization and deserialization of the messages\nlimits the throughput and increases the latency of the protocols. To alleviate\nthe bottlenecks, the most useful techniques include reducing the communication\ncomplexity, rotating the hotspot of communications, and pipelining across\nconsensus instances.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 02:17:58 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 17:53:19 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 00:02:23 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Alqahtani", "Salem", ""], ["Demirbas", "Murat", ""]]}, {"id": "2103.04247", "submitter": "Elahe Vedadi", "authors": "Elahe Vedadi and Hulya Seferoglu", "title": "Adaptive Coding for Matrix Multiplication at Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is emerging as a new paradigm to allow processing data at the\nedge of the network, where data is typically generated and collected, by\nexploiting multiple devices at the edge collectively. However, exploiting the\npotential of edge computing is challenging mainly due to the heterogeneous and\ntime-varying nature of edge devices. Coded computation, which advocates mixing\ndata in sub-tasks by employing erasure codes and offloading these sub-tasks to\nother devices for computation, is recently gaining interest, thanks to its\nhigher reliability, smaller delay, and lower communication cost. In this paper,\nour focus is on characterizing the cost-benefit trade-offs of coded computation\nfor practical edge computing systems, and develop an adaptive coded computation\nframework. In particular, we focus on matrix multiplication as a\ncomputationally intensive task, and develop an adaptive coding for matrix\nmultiplication (ACM^2) algorithm by taking into account the heterogeneous and\ntime varying nature of edge devices. ACM^2 dynamically selects the best coding\npolicy by taking into account the computing time, storage requirements as well\nas successful decoding probability. We show that ACM^2 improves the task\ncompletion delay significantly as compared to existing coded matrix\nmultiplication algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 03:22:45 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 05:40:55 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Vedadi", "Elahe", ""], ["Seferoglu", "Hulya", ""]]}, {"id": "2103.04267", "submitter": "Rajeev Jain", "authors": "Rajeev Jain, Klaus Weide, Saurabh Chawdhary, Thomas Klostermann", "title": "Checkpoint/Restart for Lagrangian particle mesh with AMR in community\n  code FLASH-X", "comments": "chkpt symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this work we present the design decisions and advantages for accomplishing\ncross mesh format checkpoint-restart in community code FLASH-X. AMReX and\nParamesh are the two AMR mesh formats developed and supported by FLASH-X. We\nalso highlight strong and weak scaling study of existing HDF5 I/O checkpoint\nwriting along with new ideas and results (presented during talk) for utilizing\nheterogeneous compute architectures for improved I/O performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 04:48:02 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Jain", "Rajeev", ""], ["Weide", "Klaus", ""], ["Chawdhary", "Saurabh", ""], ["Klostermann", "Thomas", ""]]}, {"id": "2103.04626", "submitter": "Luidnel Maignan", "authors": "Tien Nguyen (LACL), Luidnel Maignan (LACL)", "title": "Millions of 5-State n^3 Sequence Generators via Local Mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we come back on the notion of local simulation allowing to\ntransform a cellular automaton into a closely related one with different local\nencoding of information. In a previous paper, we applied it to the Firing Squad\nSynchronization Problem. In this paper, we show that the approach is not tied\nto this problem by applying it to the class of Real-Time Sequence Generation\nproblems. We improve in particular on the generation of n 3 sequence by using\nlocal mappings to obtain millions of 5state solution, one of them using 58\ntransitions. It is based on the solution of Kamikawa and Umeo that uses 6\nstates and 74 transitions. Then, we explain in which sense even bigger classes\nof problems can be considered.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:24:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Nguyen", "Tien", "", "LACL"], ["Maignan", "Luidnel", "", "LACL"]]}, {"id": "2103.04681", "submitter": "Jeeta Ann Chacko", "authors": "Jeeta Ann Chacko, Ruben Mayer, Hans-Arno Jacobsen", "title": "Why Do My Blockchain Transactions Fail? A Study of Hyperledger Fabric\n  (Extended version)*", "comments": "This is an extended version of an upcoming publication at ACM SIGMOD\n  2021. Please cite the original SIGMOD version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned blockchain systems promise to provide both decentralized trust\nand privacy. Hyperledger Fabric is currently one of the most wide-spread\npermissioned blockchain systems and is heavily promoted both in industry and\nacademia. Due to its optimistic concurrency model, the transaction failure\nrates in Fabric can become a bottleneck. While there is active research to\nreduce failures, there is a lack of understanding on their root cause and,\nconsequently, a lack of guidelines on how to configure Fabric optimally for\ndifferent scenarios. To close this gap, in this paper, we first introduce a\nformal definition of the different types of transaction failures in Fabric.\nThen, we develop a comprehensive testbed and benchmarking system,\nHyperLedgerLab, along with four different chaincodes that represent realistic\nuse cases and a chaincode/workload generator. Using HyperLedgerLab, we conduct\nexhaustive experiments to analyze the impact of different parameters of Fabric\nsuch as block size, endorsement policies, and others, on transaction failures.\nWe further analyze three recently proposed optimizations from the literature,\nFabric++, Streamchain and FabricSharp, and evaluate under which conditions they\nreduce the failure rates. Finally, based on our results, we provide\nrecommendations for Fabric practitioners on how to configure the system and\nalso propose new research directions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:42:32 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chacko", "Jeeta Ann", ""], ["Mayer", "Ruben", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "2103.04828", "submitter": "Sreeja Nair", "authors": "Sreeja Nair (DELYS), Filipe Meirim (NOVA-LINCS), M\\'ario Pereira\n  (NOVA-LINCS), Carla Ferreira (NOVA-LINCS), Marc Shapiro (DELYS)", "title": "A coordination-free, convergent, and safe replicated tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tree is an essential data structure in many applications. In a\ndistributed application, such as a distributed file system, the tree is\nreplicated.To improve performance and availability, different clients should be\nable to update their replicas concurrently and without coordination. Such\nconcurrent updates converge if the effects commute, but nonetheless, concurrent\nmoves can lead to incorrect states and even data loss. Such a severe issue\ncannot be ignored; ultimately, only one of the conflicting moves may be allowed\nto take effect. However, as it is rare, a solution should be lightweight.\nPrevious approaches would require preventative cross-replica coordination, or\ntotally order move operations after-the-fact, requiring roll-back and\ncompensation operations. In this paper, we present a novel replicated tree that\nsupports coordination-free concurrent atomic moves, and provably maintains the\ntree invariant. Our analysis identifies cases where concurrent moves are\ninherently safe, and we devise a lightweight, coordination-free, rollback-free\nalgorithm for the remaining cases, such that a maximal safe subset of moves\ntakes effect. We present a detailed analysis of the concurrency issues with\ntrees, justifying our replicated tree data structure. We provide mechanized\nproof that the data structure is convergent and maintains the tree invariant.\nFinally, we compare the response time and availability of our design against\nthe literature.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:31:59 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Nair", "Sreeja", "", "DELYS"], ["Meirim", "Filipe", "", "NOVA-LINCS"], ["Pereira", "M\u00e1rio", "", "NOVA-LINCS"], ["Ferreira", "Carla", "", "NOVA-LINCS"], ["Shapiro", "Marc", "", "DELYS"]]}, {"id": "2103.04916", "submitter": "Twinkle Jain", "authors": "David Hou, Jun Gan, Yue Li, Younes El Idrissi Yazami, Twinkle Jain", "title": "Transparent Checkpointing for OpenGL Applications on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents transparent checkpointing of OpenGL applications, refining\nthe split-process technique[1] for application in GPU-based 3D graphics. The\nsplit-process technique was earlier applied to checkpointing MPI and CUDA\nprograms, enabling reinitialization of driver libraries.\n  The presented design targets practical, checkpoint-package agnostic\ncheckpointing of OpenGL applications. An early prototype is demonstrated on\nAutodesk Maya. Maya is a complex proprietary media-creation software suite used\nwith large-scale rendering hardware for CGI (Computer-Generated Animation).\nTransparent checkpointing of Maya provides critically-needed fault tolerance,\nsince Maya is prone to crash when artists use some of its bleeding-edge\ncomponents. Artists then lose hours of work in re-creating their complex\nenvironment.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:27:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hou", "David", ""], ["Gan", "Jun", ""], ["Li", "Yue", ""], ["Yazami", "Younes El Idrissi", ""], ["Jain", "Twinkle", ""]]}, {"id": "2103.04930", "submitter": "Carlos Rea\\~no", "authors": "Jason Kennedy, Blesson Varghese and Carlos Rea\\~no", "title": "AVEC: Accelerator Virtualization in Cloud-Edge Computing for Deep\n  Learning Libraries", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge computing offers the distinct advantage of harnessing compute\ncapabilities on resources located at the edge of the network to run workloads\nof relatively weak user devices. This is achieved by offloading computationally\nintensive workloads, such as deep learning from user devices to the edge. Using\nthe edge reduces the overall communication latency of applications as workloads\ncan be processed closer to where data is generated on user devices rather than\nsending them to geographically distant clouds. Specialised hardware\naccelerators, such as Graphics Processing Units (GPUs) available in the\ncloud-edge network can enhance the performance of computationally intensive\nworkloads that are offloaded from devices on to the edge. The underlying\napproach required to facilitate this is virtualization of GPUs. This paper\ntherefore sets out to investigate the potential of GPU accelerator\nvirtualization to improve the performance of deep learning workloads in a\ncloud-edge environment. The AVEC accelerator virtualization framework is\nproposed that incurs minimum overheads and requires no source-code modification\nof the workload. AVEC intercepts local calls to a GPU on a device and forwards\nthem to an edge resource seamlessly. The feasibility of AVEC is demonstrated on\na real-world application, namely OpenPose using the Caffe deep learning\nlibrary. It is observed that on a lab-based experimental test-bed AVEC delivers\nup to 7.48x speedup despite communication overheads incurred due to data\ntransfers.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:43:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kennedy", "Jason", ""], ["Varghese", "Blesson", ""], ["Rea\u00f1o", "Carlos", ""]]}, {"id": "2103.04955", "submitter": "Evangelos Kipouridis", "authors": "Evangelos Kipouridis, Paul G. Spirakis, Kostas Tsichlas", "title": "Threshold-based Network Structural Dynamics", "comments": "29 pages, extension of the Post-print containing all proofs, to\n  appear in SIROCCO 2021", "journal-ref": null, "doi": "10.1007/978-3-030-79527-6_8", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in dynamic processes on networks is steadily rising in recent\nyears. In this paper, we consider the $(\\alpha,\\beta)$-Thresholded Network\nDynamics ($(\\alpha,\\beta)$-Dynamics), where $\\alpha\\leq \\beta$, in which only\nstructural dynamics (dynamics of the network) are allowed, guided by local\nthresholding rules executed in each node. In particular, in each discrete round\n$t$, each pair of nodes $u$ and $v$ that are allowed to communicate by the\nscheduler, computes a value $\\mathcal{E}(u,v)$ (the potential of the pair) as a\nfunction of the local structure of the network at round $t$ around the two\nnodes. If $\\mathcal{E}(u,v) < \\alpha$ then the link (if it exists) between $u$\nand $v$ is removed; if $\\alpha \\leq \\mathcal{E}(u,v) < \\beta$ then an existing\nlink among $u$ and $v$ is maintained; if $\\beta \\leq \\mathcal{E}(u,v)$ then a\nlink between $u$ and $v$ is established if not already present.\n  The microscopic structure of $(\\alpha,\\beta)$-Dynamics appears to be simple,\nso that we are able to rigorously argue about it, but still flexible, so that\nwe are able to design meaningful microscopic local rules that give rise to\ninteresting macroscopic behaviors. Our goals are the following: a) to\ninvestigate the properties of the $(\\alpha,\\beta)$-Thresholded Network Dynamics\nand b) to show that $(\\alpha,\\beta)$-Dynamics is expressive enough to solve\ncomplex problems on networks.\n  Our contribution in these directions is twofold. We rigorously exhibit the\nclaim about the expressiveness of $(\\alpha,\\beta)$-Dynamics, both by designing\na simple protocol that provably computes the $k$-core of the network as well as\nby showing that $(\\alpha,\\beta)$-Dynamics is in fact Turing-Complete. Second\nand most important, we construct general tools for proving stabilization that\nwork for a subclass of $(\\alpha,\\beta)$-Dynamics and prove speed of convergence\nin a restricted setting.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:23:11 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 14:15:49 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 06:51:47 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kipouridis", "Evangelos", ""], ["Spirakis", "Paul G.", ""], ["Tsichlas", "Kostas", ""]]}, {"id": "2103.05032", "submitter": "Zachary Charles", "authors": "Zachary Charles, Jakub Kone\\v{c}n\\'y", "title": "Convergence and Accuracy Trade-Offs in Federated Learning and\n  Meta-Learning", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021. PMLR: Volume 130", "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of algorithms, which we refer to as local update methods,\ngeneralizing many federated and meta-learning algorithms. We prove that for\nquadratic models, local update methods are equivalent to first-order\noptimization on a surrogate loss we exactly characterize. Moreover, fundamental\nalgorithmic choices (such as learning rates) explicitly govern a trade-off\nbetween the condition number of the surrogate loss and its alignment with the\ntrue loss. We derive novel convergence rates showcasing these trade-offs and\nhighlight their importance in communication-limited settings. Using these\ninsights, we are able to compare local update methods based on their\nconvergence/accuracy trade-off, not just their convergence to critical points\nof the empirical loss. Our results shed new light on a broad range of\nphenomena, including the efficacy of server momentum in federated learning and\nthe impact of proximal client updates.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 19:40:32 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Charles", "Zachary", ""], ["Kone\u010dn\u00fd", "Jakub", ""]]}, {"id": "2103.05162", "submitter": "Andrey Prokopenko", "authors": "Andrey Prokopenko and Damien Lebrun-Grandie and Daniel Arndt", "title": "Fast tree-based algorithms for DBSCAN on GPUs", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DBSCAN is a well-known density-based clustering algorithm to discover\nclusters of arbitrary shape. The efforts to parallelize the algorithm on GPUs\noften suffer from high thread execution divergence (for example, due to\nasynchronous calls to range queries). In this paper, we propose a new general\nframework for DBSCAN on GPUs, and propose two tree-based algorithms within that\nframework. Both algorithms fuse neighbor search with updating clustering\ninformation, and differ in their treatment of dense regions of the data. We\nshow that the cost of computing clusters is at most twice the cost of neighbor\ndetermination in parallel. We compare the proposed algorithms with existing GPU\nimplementations, and demonstrate their competitiveness and excellent\nperformance in the presence of a fast traversal structure (bounding volume\nhierarchy). In addition, we show that the memory usage can be reduced by\nprocessing the neighbors of an object on the fly without storing them.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:15:37 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Prokopenko", "Andrey", ""], ["Lebrun-Grandie", "Damien", ""], ["Arndt", "Daniel", ""]]}, {"id": "2103.05185", "submitter": "Chao Chen", "authors": "Chao Chen, Greg Eisenhauer, and Santosh Pande", "title": "Near-zero Downtime Recovery from Transient-error-induced Crashes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Due to the system scaling, transient errors caused by external noises, e.g.,\nheat fluxes and particle strikes, have become a growing concern for the current\nand upcoming extreme-scale high-performance-computing (HPC) systems. However,\nsince such errors are still quite rare as compared to no-fault cases, desirable\nsolutions call for low/no-overhead systems that do not compromise the\nperformance under no-fault conditions and also allow very fast fault recovery\nto minimize downtime. In this paper, we present IterPro, a light-weight\ncompiler-assisted resilience technique to quickly and accurately recover\nprocesses from transient-error-induced crashes. IterPro repairs the corrupted\nprocess states on-the-fly upon occurrences of errors, enabling applications to\ncontinue their executions instead of being terminated. IterPro also exploits\nside effects introduced by induction variable based code optimization\ntechniques to improve its recovery capability. To this end, two new code\ntransformation passes are introduced to expose the side effects for resilience\npurposes. We evaluated IterPro with 4 scientific workloads as well as the NPB\nbenchmarks suite. During their normal execution, IterPro incurs almost zero\nruntime overhead and a small, fixed 27MB memory overhead. Meanwhile, IterPro\ncan recover on an average 83.55% of crash-causing errors within dozens of\nmilliseconds with negligible downtime. With such an effective recovery\nmechanism, IterPro could tremendously mitigate the overheads and resource\nrequirements of the resilience subsystem in future extreme-scale systems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:32:14 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chen", "Chao", ""], ["Eisenhauer", "Greg", ""], ["Pande", "Santosh", ""]]}, {"id": "2103.05245", "submitter": "Dominik Scheinert", "authors": "Dominik Scheinert, Alexander Acker, Lauritz Thamsen, Morgan K.\n  Geldenhuys, Odej Kao", "title": "Learning Dependencies in Distributed Cloud Applications to Identify and\n  Localize Anomalies", "comments": "6 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operation and maintenance of large distributed cloud applications can quickly\nbecome unmanageably complex, putting human operators under immense stress when\nproblems occur. Utilizing machine learning for identification and localization\nof anomalies in such systems supports human experts and enables fast\nmitigation. However, due to the various inter-dependencies of system\ncomponents, anomalies do not only affect their origin but propagate through the\ndistributed system. Taking this into account, we present Arvalus and its\nvariant D-Arvalus, a neural graph transformation method that models system\ncomponents as nodes and their dependencies and placement as edges to improve\nthe identification and localization of anomalies. Given a series of metric\nKPIs, our method predicts the most likely system state - either normal or an\nanomaly class - and performs localization when an anomaly is detected. During\nour experiments, we simulate a distributed cloud application deployment and\nsynthetically inject anomalies. The evaluation shows the generally good\nprediction performance of Arvalus and reveals the advantage of D-Arvalus which\nincorporates information about system component dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:34:05 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Scheinert", "Dominik", ""], ["Acker", "Alexander", ""], ["Thamsen", "Lauritz", ""], ["Geldenhuys", "Morgan K.", ""], ["Kao", "Odej", ""]]}, {"id": "2103.05288", "submitter": "Zhen Zheng", "authors": "Kai Zhu, Wenyi Zhao, Zhen Zheng, Tianyou Guo, Pengzhan Zhao, Junjie\n  Bai, Jun Yang, Xiaoyong Liu, Lansong Diao, Wei Lin", "title": "DISC: A Dynamic Shape Compiler for Machine Learning Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many recent machine learning models show dynamic shape characteristics.\nHowever, existing AI compiler optimization systems suffer a lot from problems\nbrought by dynamic shape models, including compilation overhead, memory usage,\noptimization pipeline and deployment complexity. This paper provides a compiler\nsystem to natively support optimization for dynamic shape workloads, named\nDISC. DISC enriches a set of IR to form a fully dynamic shape representation.\nIt generates the runtime flow at compile time to support processing dynamic\nshape based logic, which avoids the interpretation overhead at runtime and\nenlarges the opportunity of host-device co-optimization. It addresses the\nkernel fusion problem of dynamic shapes with shape propagation and constraints\ncollecting methods. This is the first work to demonstrate how to build an\nend-to-end dynamic shape compiler based on MLIR infrastructure. Experiments\nshow that DISC achieves up to 3.3x speedup than TensorFlow/PyTorch, and 1.8x\nthan Nimble.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:27:48 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhu", "Kai", ""], ["Zhao", "Wenyi", ""], ["Zheng", "Zhen", ""], ["Guo", "Tianyou", ""], ["Zhao", "Pengzhan", ""], ["Bai", "Junjie", ""], ["Yang", "Jun", ""], ["Liu", "Xiaoyong", ""], ["Diao", "Lansong", ""], ["Lin", "Wei", ""]]}, {"id": "2103.05394", "submitter": "Fatih Ta\\c{s}yaran", "authors": "Fatih Ta\\c{s}yaran, Berkay Demireller, Kamer Kaya, Bora U\\c{c}ar", "title": "Streaming Hypergraph Partitioning Algorithms on Limited Memory\n  Environments", "comments": "8 pages, 6 algorithms, 2 figures, 4 tables, submitted to HPCS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many well-known, real-world problems involve dynamic data which describe the\nrelationship among the entities. Hypergraphs are powerful combinatorial\nstructures that are frequently used to model such data. For many of today's\ndata-centric applications, this data is streaming; new items arrive\ncontinuously, and the data grows with time. With paradigms such as Internet of\nThings and Edge Computing, such applications become more natural and more\npractical. In this work, we assume a streaming model where the data is modeled\nas a hypergraph, which is generated at the edge. This data then partitioned and\nsent to remote nodes via an algorithm running on a memory-restricted device\nsuch as a single board computer. Such a partitioning is usually performed by\ntaking a connectivity metric into account to minimize the communication cost of\nlater analyses that will be performed in a distributed fashion. Although there\nare many offline tools that can partition static hypergraphs excellently,\nalgorithms for the streaming settings are rare. We analyze a well-known\nalgorithm from the literature and significantly improve its running time by\naltering its inner data structure. For instance, on a medium-scale hypergraph,\nthe new algorithm reduces the runtime from 17800 seconds to 10 seconds. We then\npropose sketch- and hash-based algorithms, as well as ones that can leverage\nextra memory to store a small portion of the data to enable the refinement of\npartitioning when possible. We experimentally analyze the performance of these\nalgorithms and report their run times, connectivity metric scores, and memory\nuses on a high-end server and four different single-board computer\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:33:57 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ta\u015fyaran", "Fatih", ""], ["Demireller", "Berkay", ""], ["Kaya", "Kamer", ""], ["U\u00e7ar", "Bora", ""]]}, {"id": "2103.05436", "submitter": "Pavlos Aimoniotis", "authors": "Pavlos Aimoniotis, Maria Rafaela Gkeka and Nikolaos Bellas", "title": "MapVisual: A Visualization Tool for Memory Access Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memory bandwidth is strongly correlated to the complexity of the memory\naccess pattern of a running application. To improve memory performance of\napplications with irregular and/or unpredictable memory patterns, we need tools\nto analyze these patterns during application development. In this work, we\npresent a software tool for the analysis and visualization of memory access\npatterns. We perform memory tracing and profiling, we do data processing and\nfiltering, and we use visualization algorithms to produce three dimensional\ngraphs that describe the patterns both in space and in time. Finally, we\nevaluate our toolflow on a variety of applications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:55:10 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Aimoniotis", "Pavlos", ""], ["Gkeka", "Maria Rafaela", ""], ["Bellas", "Nikolaos", ""]]}, {"id": "2103.05444", "submitter": "Vyacheslav Kungurtsev", "authors": "Alexander Kolesov and Vyacheslav Kungurtsev", "title": "Decentralized Langevin Dynamics over a Directed Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prevalence of technologies in the space of the Internet of Things and use\nof multi-processing computing platforms to aid in the computation required to\nperform learning and inference from large volumes of data has necessitated the\nextensive study of algorithms on decentralized platforms. In these settings,\ncomputing nodes send and receive data across graph-structured communication\nlinks, and using a combination of local computation and consensus-seeking\ncommunication, cooperately solve a problem of interest. Recently, Langevin\ndynamics as a tool for high dimensional sampling and posterior Bayesian\ninference has been studied in the context of a decentralized operation.\nHowever, this work has been limited to undirected graphs, wherein all\ncommunication is two-sided, i.e., if node A can send data to node B, then node\nB can also send data to node A. We extend the state of the art in considering\nLangevin dynamics on directed graphs.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 09:20:08 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Kolesov", "Alexander", ""], ["Kungurtsev", "Vyacheslav", ""]]}, {"id": "2103.05694", "submitter": "Ian Henriksen", "authors": "Ian Henriksen, Bozhi You, Keshav Pingali", "title": "Exploiting Asynchronous Priority Scheduling in Parallel Eikonal Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerical solutions to the Eikonal equation are computed using variants of\nthe fast marching method, the fast sweeping method, and the fast iterative\nmethod. In this paper, we provide a unified view of these algorithms that\nhighlights their similarities and suggests a wider class of Eikonal solvers. We\nthen use this framework to justify applying concurrent priority scheduling\ntechniques to Eikonal solvers. We demonstrate that doing so results in good\nparallel performance for a problem from seismology. We explain why existing\nEikonal solvers may produce different results despite using the same\ndifferencing scheme and demonstrate techniques to address these discrepancies.\nThese techniques allow us to obtain deterministic output from our asynchronous\nfine-grained parallel Eikonal solver.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:58:10 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Henriksen", "Ian", ""], ["You", "Bozhi", ""], ["Pingali", "Keshav", ""]]}, {"id": "2103.05809", "submitter": "Ahmed Eleliemy", "authors": "Ahmed Eleliemy and Florina M. Ciorba", "title": "A Resourceful Coordination Approach for Multilevel Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HPC users aim to improve their execution times without particular regard for\nincreasing system utilization. On the contrary, HPC operators favor increasing\nthe number of executed applications per time unit and increasing system\nutilization. This difference in the preferences promotes the following\noperational model. Applications execute on exclusively-allocated computing\nresources for a specific time and applications are assumed to utilize the\nallocated resources efficiently. In many cases, this operational model is\ninefficient, i.e., applications may not fully utilize their allocated\nresources. This inefficiency results in increasing application execution time\nand decreasing system utilization. In this work, we propose a resourceful\ncoordination approach (RCA) that enables the cooperation between, currently\nindependent, batch- and application-level schedulers. RCA enables application\nschedulers to share their allocated but idle computing resources with other\napplications through the batch system. The effective system performance (ESP)\nbenchmark is used to assess the proposed approach. The results show that RCA\nincreased system utilization up to 12.6% and decreased system makespan by the\nsame percent without affecting applications' performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 01:34:05 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Eleliemy", "Ahmed", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "2103.05875", "submitter": "Michael Stengel", "authors": "Michael Stengel, Zander Majercik, Benjamin Boudaoud, Morgan McGuire", "title": "A Distributed, Decoupled System for Losslessly Streaming Dynamic Light\n  Probes to Thin Clients", "comments": "12 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a networked, high performance graphics system that combines\ndynamic, high quality, ray traced global illumination computed on a server with\ndirect illumination and primary visibility computed on a client. This approach\nprovides many of the image quality benefits of real-time ray tracing on\nlow-power and legacy hardware, while maintaining a low latency response and\nmobile form factor. Our system distributes the graphic pipeline over a network\nby computing diffuse global illumination on a remote machine. Global\nillumination is computed using a recent irradiance volume representation\ncombined with a novel, lossless, HEVC-based, hardware-accelerated encoding, and\na perceptually-motivated update scheme. Our experimental implementation streams\nthousands of irradiance probes per second and requires less than 50 Mbps of\nthroughput, reducing the consumed bandwidth by 99.4% when streaming at 60 Hz\ncompared to traditional lossless texture compression. This bandwidth reduction\nallows higher quality and lower latency graphics than state-of-the-art remote\nrendering via video streaming. In addition, our split-rendering solution\ndecouples remote computation from local rendering and so does not limit local\ndisplay update rate or resolution.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 05:21:03 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Stengel", "Michael", ""], ["Majercik", "Zander", ""], ["Boudaoud", "Benjamin", ""], ["McGuire", "Morgan", ""]]}, {"id": "2103.06046", "submitter": "Hao Wang", "authors": "Qing Yang and Hao Wang", "title": "Exploring Blockchain for The Coordination of Distributed Energy\n  Resources", "comments": "55th Annual Conference on Information Sciences and Systems (CISS),\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The fast growth of distributed energy resources (DERs), such as distributed\nrenewables (e.g., rooftop PV panels), energy storage systems, electric\nvehicles, and controllable appliances, drives the power system toward a\ndecentralized system with bidirectional power flow. The coordination of DERs\nthrough an aggregator, such as a utility, system operator, or a third-party\ncoordinator, emerges as a promising paradigm. However, it is not well\nunderstood how to enable trust between the aggregator and DERs to integrate\nDERs efficiently. In this paper, we develop a trustable and distributed\ncoordination system for DERs using blockchain technology. We model various DERs\nand formulate a cost minimization problem for DERs to optimize their energy\ntrading, scheduling, and demand response. We use the alternating direction\nmethod of multipliers (ADMM) to solve the problem in a distributed fashion. To\nimplement the distributed algorithm in a trustable way, we design a smart\ncontract to update multipliers and communicate with DERs in a blockchain\nnetwork. We validate our design by experiments using real-world data, and the\nsimulation results demonstrate the effectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:32:33 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Qing", ""], ["Wang", "Hao", ""]]}, {"id": "2103.06145", "submitter": "Abhishek Singh", "authors": "Abhishek Narain Singh", "title": "GraphBreak: Tool for Network Community based Regulatory Medicine, Gene\n  co-expression, Linkage Disequilibrium analysis, functional annotation and\n  more", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph network science is becoming increasingly popular, notably in big-data\nperspective where understanding individual entities for individual functional\nroles is complex and time consuming. It is likely when a set of genes are\nregulated by a set of genetic variants, the genes set is recruited for a common\nor related functional purpose. Grouping and extracting communities from network\nof associations becomes critical to understand system complexity, thus\nprioritizing genes for dis-ease and functional associations. Workload is\nreduced when studying entities one at a time. For this, we present GraphBreak,\na suite of tools for community detection application, such as for gene\nco-expression, protein interaction, regulation network, etc.Although developed\nfor use case of eQTLs regulatory genomic net-work community study -- results\nshown with our analysis with sample eQTL data. Graphbreak can be deployed for\nother studies if input data has been fed in requisite format, including but not\nlimited to gene co-expression networks, protein-protein interaction network,\nsignaling pathway and metabolic network. Graph-Break showed critical use case\nvalue in its downstream analysis for disease association of communities\ndetected. If all independent steps of community detection and analysis are a\nstep-by-step sub-part of the algorithm, GraphBreak can be considered a new\nalgorithm for community based functional characterization. Combination of\nvarious algorithmic implementation modules into a single script for this\npurpose illustrates GraphBreak novelty. Compared to other similar tools, with\nGraphBreak we can better detect communities with over-representation of its\nmember genes for statistical association with diseases, therefore target genes\nwhich can be prioritized for drug-positioning or drug-re-positioning as the\ncase be.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:16:38 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Singh", "Abhishek Narain", ""]]}, {"id": "2103.06264", "submitter": "Vijay Garg", "authors": "Vijay K. Garg", "title": "A Lattice Linear Predicate Parallel Algorithm for the Dynamic\n  Programming Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been shown that the parallel Lattice Linear Predicate (LLP) algorithm\nsolves many combinatorial optimization problems such as the shortest path\nproblem, the stable marriage problem and the market clearing price problem. In\nthis paper, we give the parallel LLP algorithm for many dynamic programming\nproblems. In particular, we show that the LLP algorithm solves the longest\nsubsequence problem, the optimal binary search tree problem, and the knapsack\nproblem. Furthermore, the algorithm can be used to solve the constrained\nversions of these problems so long as the constraints are lattice linear. The\nparallel LLP algorithm requires only read-write atomicity and no higher-level\natomic instructions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:54:04 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Garg", "Vijay K.", ""]]}, {"id": "2103.06381", "submitter": "Ranesh Kumar Naha", "authors": "Ranesh Kumar Naha, Saurabh Garg, Muhammad Bilal Amin, and Rajiv Ranjan", "title": "Fuzzy Logic-based Robust Failure Handling Mechanism for Fog Computing", "comments": "12 Pages,12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fog computing is an emerging computing paradigm which is mainly suitable for\ntime-sensitive and real-time Internet of Things (IoT) applications. Academia\nand industries are focusing on the exploration of various aspects of Fog\ncomputing for market adoption. The key idea of the Fog computing paradigm is to\nuse idle computation resources of various handheld, mobile, stationery and\nnetwork devices around us, to serve the application requests in the Fog-IoT\nenvironment. The devices in the Fog environment are autonomous and not\nexclusively dedicated to Fog application processing. Due to that, the\nprobability of device failure in the Fog environment is high compared with\nother distributed computing paradigms. Solving failure issues in Fog is crucial\nbecause successful application execution can only be ensured if failure can be\nhandled carefully. To handle failure, there are several techniques available in\nthe literature, such as checkpointing and task migration, each of which works\nwell in cloud based enterprise applications that mostly deals with static or\ntransactional data. These failure handling methods are not applicable to highly\ndynamic Fog environment. In contrast, this work focuses on solving the problem\nof managing application failure in the Fog environment by proposing a composite\nsolution (combining fuzzy logic-based task checkpointing and task migration\ntechniques with task replication) for failure handling and generating a robust\nschedule. We evaluated the proposed methods using real failure traces in terms\nof application execution time, delay and cost. Average delay and total\nprocessing time improved by 56% and 48% respectively, on an average for the\nproposed solution, compared with the existing failure handling approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:03:48 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Naha", "Ranesh Kumar", ""], ["Garg", "Saurabh", ""], ["Amin", "Muhammad Bilal", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "2103.06385", "submitter": "Ranesh Kumar Naha", "authors": "Ranesh Kumar Naha, Saurabh Garg, Sudheer Kumar Battula, Muhammad Bilal\n  Amin, and Dimitrios Georgakopoulos", "title": "Multiple Linear Regression-Based Energy-Aware Resource Allocation in the\n  Fog Computing Environment", "comments": "8 Pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fog computing is a promising computing paradigm for time-sensitive Internet\nof Things (IoT) applications. It helps to process data close to the users, in\norder to deliver faster processing outcomes than the Cloud; it also helps to\nreduce network traffic. The computation environment in the Fog computing is\nhighly dynamic and most of the Fog devices are battery powered hence the\nchances of application failure is high which leads to delaying the application\noutcome. On the other hand, if we rerun the application in other devices after\nthe failure it will not comply with time-sensitiveness. To solve this problem,\nwe need to run applications in an energy-efficient manner which is a\nchallenging task due to the dynamic nature of Fog computing environment. It is\nrequired to schedule application in such a way that the application should not\nfail due to the unavailability of energy. In this paper, we propose a multiple\nlinear, regression-based resource allocation mechanism to run applications in\nan energy-aware manner in the Fog computing environment to minimise failures\ndue to energy constraint. Prior works lack of energy-aware application\nexecution considering dynamism of Fog environment. Hence, we propose A multiple\nlinear regression-based approach which can achieve such objectives. We present\na sustainable energy-aware framework and algorithm which execute applications\nin Fog environment in an energy-aware manner. The trade-off between\nenergy-efficient allocation and application execution time has been\ninvestigated and shown to have a minimum negative impact on the system for\nenergy-aware allocation. We compared our proposed method with existing\napproaches. Our proposed approach minimises the delay and processing by 20%,\nand 17% compared with the existing one. Furthermore, SLA violation decrease by\n57% for the proposed energy-aware allocation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:26:01 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Naha", "Ranesh Kumar", ""], ["Garg", "Saurabh", ""], ["Battula", "Sudheer Kumar", ""], ["Amin", "Muhammad Bilal", ""], ["Georgakopoulos", "Dimitrios", ""]]}, {"id": "2103.06406", "submitter": "Waheed Bajwa", "authors": "Bingqing Xiang, Arpita Gang, and Waheed U. Bajwa", "title": "Distributed Principal Subspace Analysis for Partitioned Big Data:\n  Algorithms, Analysis, and Implementation", "comments": "15 pages; 14 figures; 8 tables; preprint of a journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Subspace Analysis (PSA) is one of the most popular approaches for\ndimensionality reduction in signal processing and machine learning. But\ncentralized PSA solutions are fast becoming irrelevant in the modern era of big\ndata, in which the number of samples and/or the dimensionality of samples often\nexceed the storage and/or computational capabilities of individual machines.\nThis has led to study of distributed PSA solutions, in which the data are\npartitioned across multiple machines and an estimate of the principal subspace\nis obtained through collaboration among the machines. It is in this vein that\nthis paper revisits the problem of distributed PSA under the general framework\nof an arbitrarily connected network of machines that lacks a central server.\nThe main contributions of the paper in this regard are threefold. First, two\nalgorithms are proposed in the paper that can be used for distributed PSA in\nthe case of data that are partitioned across either samples or (raw) features.\nSecond, in the case of sample-wise partitioned data, the proposed algorithm and\na variant of it are analyzed, and their convergence to the true subspace at\nlinear rates is established. Third, extensive experiments on both synthetic and\nreal-world data are carried out to validate the usefulness of the proposed\nalgorithms. In particular, in the case of sample-wise partitioned data, an\nMPI-based distributed implementation is carried out to study the interplay\nbetween network topology and communications cost as well as to study of effect\nof straggler machines on the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 01:33:38 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Xiang", "Bingqing", ""], ["Gang", "Arpita", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2103.06513", "submitter": "Ali Hamdi", "authors": "Ali Hamdi, Flora D. Salim, Du Yong Kim, Azadeh Ghari Neiat, Athman\n  Bouguettaya", "title": "Drone-as-a-Service Composition Under Uncertainty", "comments": "20 pages, 20 figures, Accepted for publication at IEEE Transactions\n  on Services Computing", "journal-ref": "IEEE Transactions on Services Computing (TSC), 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose an uncertainty-aware service approach to provide drone-based\ndelivery services called Drone-as-a-Service (DaaS) effectively. Specifically,\nwe propose a service model of DaaS based on the dynamic spatiotemporal features\nof drones and their in-flight contexts. The proposed DaaS service approach\nconsists of three components: scheduling, route-planning, and composition.\nFirst, we develop a DaaS scheduling model to generate DaaS itineraries through\na Skyway network. Second, we propose an uncertainty-aware DaaS route-planning\nalgorithm that selects the optimal Skyways under weather uncertainties. Third,\nwe develop two DaaS composition techniques to select an optimal DaaS\ncomposition at each station of the planned route. A spatiotemporal DaaS\ncomposer first selects the optimal DaaSs based on their spatiotemporal\navailability and drone capabilities. A predictive DaaS composer then utilises\nthe outcome of the first composer to enable fast and accurate DaaS composition\nusing several Machine Learning classification methods. We train the classifiers\nusing a new set of spatiotemporal features which are in addition to other DaaS\nQoS properties. Our experiments results show the effectiveness and efficiency\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 07:29:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hamdi", "Ali", ""], ["Salim", "Flora D.", ""], ["Kim", "Du Yong", ""], ["Neiat", "Azadeh Ghari", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2103.06647", "submitter": "Chris Porter", "authors": "Girish Mururu, Chao Chen, Chris Porter, Santosh Pande, Ada Gavrilovska", "title": "Compiler-Guided Throughput Scheduling for Many-core Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern ARM-based servers such as ThunderX and ThunderX2 offer a tremendous\namount of parallelism by providing dozens or even hundreds of processors.\nHowever, exploiting these computing resources for reuse-heavy, data dependent\nworkloads is a big challenge because of shared cache resources. In particular,\nschedulers have to conservatively co-locate processes to avoid cache conflicts\nsince miss penalties are detrimental and conservative co-location decisions\nlead to lower resource utilization.\n  To address these challenges, in this paper we explore the utility of\npredictive analysis of applications' execution to dynamically forecast\nresource-heavy workload regions, and to improve the efficiency of resource\nmanagement through the use of new proactive methods. Our approach relies on the\ncompiler to insert \"beacons\" in the application at strategic program points to\nperiodically produce and/or update the attributes of anticipated\nresource-intense program region(s). The compiler classifies loops in programs\nbased on predictability of their execution time and inserts different types of\nbeacons at their entry/exit points. The precision of the information carried by\nbeacons varies as per the analyzability of the loops, and the scheduler uses\nperformance counters to fine tune co-location decisions. The information\nproduced by beacons in multiple processes is aggregated and analyzed by the\nproactive scheduler to respond to the anticipated workload requirements. For\nthroughput environments, we develop a framework that demonstrates high-quality\npredictions and improvements in throughput over CFS by 1.4x on an average and\nup to 4.7x on ThunderX and 1.9x on an average and up to 5.2x on ThunderX2\nservers on consolidated workloads across 45 benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 12:59:10 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Mururu", "Girish", ""], ["Chen", "Chao", ""], ["Porter", "Chris", ""], ["Pande", "Santosh", ""], ["Gavrilovska", "Ada", ""]]}, {"id": "2103.06972", "submitter": "Zebang Shen", "authors": "Zebang Shen, Hamed Hassani, Satyen Kale, Amin Karbasi", "title": "Federated Functional Gradient Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate a study of functional minimization in Federated\nLearning. First, in the semi-heterogeneous setting, when the marginal\ndistributions of the feature vectors on client machines are identical, we\ndevelop the federated functional gradient boosting (FFGB) method that provably\nconverges to the global minimum. Subsequently, we extend our results to the\nfully-heterogeneous setting (where marginal distributions of feature vectors\nmay differ) by designing an efficient variant of FFGB called FFGB.C, with\nprovable convergence to a neighborhood of the global minimum within a radius\nthat depends on the total variation distances between the client feature\ndistributions. For the special case of square loss, but still in the fully\nheterogeneous setting, we design the FFGB.L method that also enjoys provable\nconvergence to a neighborhood of the global minimum but within a radius\ndepending on the much tighter Wasserstein-1 distances. For both FFGB.C and\nFFGB.L, the radii of convergence shrink to zero as the feature distributions\nbecome more homogeneous. Finally, we conduct proof-of-concept experiments to\ndemonstrate the benefits of our approach against natural baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 21:49:19 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Shen", "Zebang", ""], ["Hassani", "Hamed", ""], ["Kale", "Satyen", ""], ["Karbasi", "Amin", ""]]}, {"id": "2103.06980", "submitter": "Xijun Li", "authors": "Jinhong Luo, Xijun Li, Mingxuan Yuan, Jianguo Yao and Jia Zeng", "title": "Learning to Optimize DAG Scheduling in Heterogeneous Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Directed Acyclic Graph (DAG) scheduling in a heterogeneous environment is\naimed at assigning the on-the-fly jobs to a cluster of heterogeneous computing\nexecutors in order to minimize the makespan while meeting all requirements of\nscheduling. The problem gets more attention than ever since the rapid\ndevelopment of heterogeneous cloud computing. A little reduction of makespan of\nDAG scheduling could both bring huge profits to the service providers and\nincrease the level of service of users. Although DAG scheduling plays an\nimportant role in cloud computing industries, existing solutions still have\nhuge room for improvement, especially in making use of topological dependencies\nbetween jobs. In this paper, we propose a task-duplication based learning\nalgorithm, called \\textit{Lachesis}, for the distributed DAG scheduling\nproblem. In our approach, it first perceives the topological dependencies\nbetween jobs using a specially designed graph convolutional network (GCN) to\nselect the most likely task to be executed. Then the task is assigned to a\nspecific executor with the consideration of duplicating all its precedent tasks\naccording to a sophisticated heuristic method. We have conducted extensive\nexperiments over standard workload data to evaluate our solution. The\nexperimental results suggest that the proposed algorithm can achieve at most\n26.7\\% reduction of makespan and 35.2\\% improvement of speedup ratio over seven\nstrong baseline algorithms, including state-of-the-art heuristics methods and a\nvariety of deep reinforcement learning based algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:15:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Luo", "Jinhong", ""], ["Li", "Xijun", ""], ["Yuan", "Mingxuan", ""], ["Yao", "Jianguo", ""], ["Zeng", "Jia", ""]]}, {"id": "2103.07027", "submitter": "Peter Robinson", "authors": "Christian Konrad, Peter Robinson, Viktor Zamaraev", "title": "Robust Lower Bounds for Graph Problems in the Blackboard Model of\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give lower bounds on the communication complexity of graph problems in the\nmulti-party blackboard model. In this model, the edges of an $n$-vertex input\ngraph are partitioned among $k$ parties, who communicate solely by writing\nmessages on a shared blackboard that is visible to every party. We show that\nany non-trivial graph problem on $n$-vertex graphs has blackboard communication\ncomplexity $\\Omega(n)$ bits, even if the edges of the input graph are randomly\nassigned to the $k$ parties. We say that a graph problem is non-trivial if the\noutput cannot be computed in a model where every party holds at most one edge\nand no communication is allowed. Our lower bound thus holds for essentially all\nkey graph problems relevant to distributed computing, including Maximal\nIndependent Set (MIS), Maximal Matching, ($\\Delta+1$)-coloring, and Dominating\nSet. In many cases, e.g., MIS, Maximal Matching, and $(\\Delta+1)$-coloring, our\nlower bounds are optimal, up to poly-logarithmic factors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 01:27:12 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Konrad", "Christian", ""], ["Robinson", "Peter", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "2103.07050", "submitter": "Chenhao Xu", "authors": "Chenhao Xu, Yong Li, Yao Deng, Jiaqi Ge, Longxiang Gao, Mengshi Zhang,\n  Yong Xiang, Xi Zheng", "title": "SCEI: A Smart-Contract Driven Edge Intelligence Framework for IoT\n  Systems", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) utilizes edge computing devices to collaboratively\ntrain a shared model while each device can fully control its local data access.\nGenerally, FL techniques focus on learning model on independent and identically\ndistributed (iid) dataset and cannot achieve satisfiable performance on non-iid\ndatasets (e.g. learning a multi-class classifier but each client only has a\nsingle class dataset). Some personalized approaches have been proposed to\nmitigate non-iid issues. However, such approaches cannot handle underlying data\ndistribution shift, namely data distribution skew, which is quite common in\nreal scenarios (e.g. recommendation systems learn user behaviors which change\nover time). In this work, we provide a solution to the challenge by leveraging\nsmart-contract with federated learning to build optimized, personalized deep\nlearning models. Specifically, our approach utilizes smart contract to reach\nconsensus among distributed trainers on the optimal weights of personalized\nmodels. We conduct experiments across multiple models (CNN and MLP) and\nmultiple datasets (MNIST and CIFAR-10). The experimental results demonstrate\nthat our personalized learning models can achieve better accuracy and faster\nconvergence compared to classic federated and personalized learning. Compared\nwith the model given by baseline FedAvg algorithm, the average accuracy of our\npersonalized learning models is improved by 2% to 20%, and the convergence rate\nis about 2$\\times$ faster. Moreover, we also illustrate that our approach is\nsecure against recent attack on distributed learning.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 02:57:05 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Xu", "Chenhao", ""], ["Li", "Yong", ""], ["Deng", "Yao", ""], ["Ge", "Jiaqi", ""], ["Gao", "Longxiang", ""], ["Zhang", "Mengshi", ""], ["Xiang", "Yong", ""], ["Zheng", "Xi", ""]]}, {"id": "2103.07092", "submitter": "Joel Mandebi Mbongue", "authors": "Joel Mandebi Mbongue, Danielle Tchuinkou Kwadjo, Christophe Bobda", "title": "Performance Exploration of Virtualization Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtualization has gained astonishing popularity in recent decades. It is\napplied in several application domains, including mainframes, personal\ncomputers, data centers, and embedded systems. While the benefits of\nvirtualization are no longer to be demonstrated, it often comes at the price of\nperformance degradation compared to native execution. In this work, we conduct\na comparative study on the performance outcome of VMWare, KVM, and Docker\nagainst compute-intensive, IO-intensive, and system benchmarks. The experiments\nreveal that containers are the way-to-go for the fast execution of\napplications. It also shows that VMWare and KVM perform similarly on most of\nthe benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 05:14:46 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Mbongue", "Joel Mandebi", ""], ["Kwadjo", "Danielle Tchuinkou", ""], ["Bobda", "Christophe", ""]]}, {"id": "2103.07133", "submitter": "Chalee Boonprasop", "authors": "Chalee Boonprasop, Yuhui Lin, Adam Barker", "title": "A Risk-taking Broker Model to Optimise User Requests placement on\n  On-demand and Contract VMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud providers offer end-users various pricing schemes to allow them to\ntailor VMs to their needs, e.g., a pay-as-you-go billing scheme, called\n\\textit{on-demand}, and a discounted contract scheme, called \\textit{reserved\ninstances}. This paper presents a cloud broker which offers users both the\nflexibility of on-demand instances and some level of discounts found in\nreserved instances. The broker employs a buy-low-and-sell-high strategy that\nplaces user requests into a resource pool of pre-purchased discounted cloud\nresources. By analysing user request time-series data, the broker takes a\nrisk-oriented approach to dynamically adjust the resource pool.\n  This approach does not require a training process which is useful at\nprocessing the large data stream. The broker is evaluated with high-frequency\nreal cloud datasets from Alibaba. The results show that the overall profit of\nthe broker is close to the theoretical optimal scenario where user requests can\nbe perfectly predicted.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 08:11:19 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Boonprasop", "Chalee", ""], ["Lin", "Yuhui", ""], ["Barker", "Adam", ""]]}, {"id": "2103.07237", "submitter": "Mohammad Reza Besharati", "authors": "Mohammad Reza Besharati, Ali Sepehri Khameneh", "title": "Modeling of Resource Allocation Mechanisms in Distributed Computing\n  Systems using Petri Nets and Stochastic Activity Networks (SAN): a Review and\n  Reo-based Suggestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resource allocation is crucial in the distributed systems. It is a key step\nin designing the mechanisms of systems for determining the resource allocation\nmechanism, it is important for obtaining the desired efficiency in the system,\nplus it is vital for predicting and preventing Deadlocks. Various models of\nPetri Net (Stochastic PN, Colored PN, Generalized PN, etc.) are used for\nmodeling, simulation, execution, and solving the problems of resource\nallocation. SAN models are used for modeling the problems pertinent to resource\nallocation. First, we shall address the basic concepts pertinent to these\nmodels and the resource allocation problem (introduction chapter), then, some\napplications of the Petri Net and SAN models in the distributed computational\nsystems or systems based on them shall be studied. Finally, the issues and\nfindings will be concluded.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:30:12 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Besharati", "Mohammad Reza", ""], ["Khameneh", "Ali Sepehri", ""]]}, {"id": "2103.07329", "submitter": "Boris Krasnopolsky Dr.", "authors": "Boris Krasnopolsky and Alexey Medvedev", "title": "XAMG: A library for solving linear systems with multiple right-hand side\n  vectors", "comments": null, "journal-ref": null, "doi": "10.1016/j.softx.2021.100695", "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the XAMG library for solving large sparse systems of\nlinear algebraic equations with multiple right-hand side vectors. The library\nspecializes but is not limited to the solution of linear systems obtained from\nthe discretization of elliptic differential equations. A corresponding set of\nnumerical methods includes Krylov subspace, algebraic multigrid, Jacobi,\nGauss-Seidel, and Chebyshev iterative methods. The parallelization is\nimplemented with MPI+POSIX shared memory hybrid programming model, which\nintroduces a three-level hierarchical decomposition using the corresponding\nper-level synchronization and communication primitives. The code contains a\nnumber of optimizations, including the multilevel data segmentation,\ncompression of indices, mixed-precision floating-point calculations, vector\nstatus flags, and others. The XAMG library uses the program code of the\nwell-known hypre library to construct the multigrid matrix hierarchy. The\nXAMG's own implementation for the solve phase of the iterative methods provides\nup to a twofold speedup compared to hypre for the tests performed.\nAdditionally, XAMG provides extended functionality to solve systems with\nmultiple right-hand side vectors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:54:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Krasnopolsky", "Boris", ""], ["Medvedev", "Alexey", ""]]}, {"id": "2103.07450", "submitter": "Joel Rybicki", "authors": "Victoria Andaur, Janna Burman, Matthias F\\\"ugger, Manish Kushwaha,\n  Bilal Manssouri, Thomas Nowak, Joel Rybicki", "title": "Reaching Agreement in Competitive Microbial Systems", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider distributed agreement tasks in microbial\ndistributed systems under stochastic population dynamics and competitive\ninteractions. We examine how competitive exclusion can be used to solve\ndistributed agreement tasks in the microbial setting. To this end, we develop a\nnew technique for analyzing the time to reach competitive exclusion in systems\nwith two competing species under biologically realistic population dynamics. We\nuse this technique to analyze a protocol that exploits competitive interactions\nto solve approximate majority consensus efficiently in microbial systems. To\ncorroborate our analytical results, we use computer simulations to show that\nthese consensus dynamics occur within practical time scales.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:23:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Andaur", "Victoria", ""], ["Burman", "Janna", ""], ["F\u00fcgger", "Matthias", ""], ["Kushwaha", "Manish", ""], ["Manssouri", "Bilal", ""], ["Nowak", "Thomas", ""], ["Rybicki", "Joel", ""]]}, {"id": "2103.07454", "submitter": "Soumyadip Ghosh", "authors": "Soumyadip Ghosh, Bernardo Aquino, Vijay Gupta", "title": "EventGraD: Event-Triggered Communication in Parallel Machine Learning", "comments": "Preprint submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication in parallel systems imposes significant overhead which often\nturns out to be a bottleneck in parallel machine learning. To relieve some of\nthis overhead, in this paper, we present EventGraD - an algorithm with\nevent-triggered communication for stochastic gradient descent in parallel\nmachine learning. The main idea of this algorithm is to modify the requirement\nof communication at every iteration in standard implementations of stochastic\ngradient descent in parallel machine learning to communicating only when\nnecessary at certain iterations. We provide theoretical analysis of convergence\nof our proposed algorithm. We also implement the proposed algorithm for\ndata-parallel training of a popular residual neural network used for training\nthe CIFAR-10 dataset and show that EventGraD can reduce the communication load\nby up to 60% while retaining the same level of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:28:50 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ghosh", "Soumyadip", ""], ["Aquino", "Bernardo", ""], ["Gupta", "Vijay", ""]]}, {"id": "2103.07704", "submitter": "Hye Young Paik", "authors": "Nicholas Malecki and Hye-young Paik and Aleksandar Ignjatovic and Alan\n  Blair and Elisa Bertino", "title": "Simeon -- Secure Federated Machine Learning Through Iterative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables a global machine learning model to be trained\ncollaboratively by distributed, mutually non-trusting learning agents who\ndesire to maintain the privacy of their training data and their hardware. A\nglobal model is distributed to clients, who perform training, and submit their\nnewly-trained model to be aggregated into a superior model. However, federated\nlearning systems are vulnerable to interference from malicious learning agents\nwho may desire to prevent training or induce targeted misclassification in the\nresulting global model. A class of Byzantine-tolerant aggregation algorithms\nhas emerged, offering varying degrees of robustness against these attacks,\noften with the caveat that the number of attackers is bounded by some quantity\nknown prior to training. This paper presents Simeon: a novel approach to\naggregation that applies a reputation-based iterative filtering technique to\nachieve robustness even in the presence of attackers who can exhibit arbitrary\nbehaviour. We compare Simeon to state-of-the-art aggregation techniques and\nfind that Simeon achieves comparable or superior robustness to a variety of\nattacks. Notably, we show that Simeon is tolerant to sybil attacks, where other\nalgorithms are not, presenting a key advantage of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 12:17:47 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Malecki", "Nicholas", ""], ["Paik", "Hye-young", ""], ["Ignjatovic", "Aleksandar", ""], ["Blair", "Alan", ""], ["Bertino", "Elisa", ""]]}, {"id": "2103.07974", "submitter": "Cheng Luo", "authors": "Cheng Luo, Lei Qu, Youshan Miao, Peng Cheng, Yongqiang Xiong", "title": "CrossoverScheduler: Overlapping Multiple Distributed Training\n  Applications in a Crossover Manner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed deep learning workloads include throughput-intensive training\ntasks on the GPU clusters, where the Distributed Stochastic Gradient Descent\n(SGD) incurs significant communication delays after backward propagation,\nforces workers to wait for the gradient synchronization via a centralized\nparameter server or directly in decentralized workers. We present\nCrossoverScheduler, an algorithm that enables communication cycles of a\ndistributed training application to be filled by other applications through\npipelining communication and computation. With CrossoverScheduler, the running\nperformance of distributed training can be significantly improved without\nsacrificing convergence rate and network accuracy. We achieve so by introducing\nCrossover Synchronization which allows multiple distributed deep learning\napplications to time-share the same GPU alternately. The prototype of\nCrossoverScheduler is built and integrated with Horovod. Experiments on a\nvariety of distributed tasks show that CrossoverScheduler achieves 20% \\times\nspeedup for image classification tasks on ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 17:01:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Luo", "Cheng", ""], ["Qu", "Lei", ""], ["Miao", "Youshan", ""], ["Cheng", "Peng", ""], ["Xiong", "Yongqiang", ""]]}, {"id": "2103.07977", "submitter": "Raveesh Garg", "authors": "Raveesh Garg, Eric Qin, Francisco Mu\\~noz Mart\\'inez, Robert Guirado,\n  Akshay Jain, Sergi Abadal, Jos\\'e L. Abell\\'an, Manuel E. Acacio, Eduard\n  Alarc\\'on, Sivasankaran Rajamanickam, Tushar Krishna", "title": "A Taxonomy for Classification and Comparison of Dataflows for GNN\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Graph Neural Networks (GNNs) have received a lot of interest\nbecause of their success in learning representations from graph structured\ndata. However, GNNs exhibit different compute and memory characteristics\ncompared to traditional Deep Neural Networks (DNNs). Graph convolutions require\nfeature aggregations from neighboring nodes (known as the aggregation phase),\nwhich leads to highly irregular data accesses. GNNs also have a very regular\ncompute phase that can be broken down to matrix multiplications (known as the\ncombination phase). All recently proposed GNN accelerators utilize different\ndataflows and microarchitecture optimizations for these two phases. Different\ncommunication strategies between the two phases have been also used. However,\nas more custom GNN accelerators are proposed, the harder it is to qualitatively\nclassify them and quantitatively contrast them. In this work, we present a\ntaxonomy to describe several diverse dataflows for running GNN inference on\naccelerators. This provides a structured way to describe and compare the\ndesign-space of GNN accelerators.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 17:14:13 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Garg", "Raveesh", ""], ["Qin", "Eric", ""], ["Mart\u00ednez", "Francisco Mu\u00f1oz", ""], ["Guirado", "Robert", ""], ["Jain", "Akshay", ""], ["Abadal", "Sergi", ""], ["Abell\u00e1n", "Jos\u00e9 L.", ""], ["Acacio", "Manuel E.", ""], ["Alarc\u00f3n", "Eduard", ""], ["Rajamanickam", "Sivasankaran", ""], ["Krishna", "Tushar", ""]]}, {"id": "2103.08053", "submitter": "Santosh Pandey", "authors": "Santosh Pandey, Zhibin Wang, Sheng Zhong, Chen Tian, Bolong Zheng,\n  Xiaoye Li, Lingda Li, Adolfy Hoisie, Caiwen Ding, Dong Li, Hang Liu", "title": "TRUST: Triangle Counting Reloaded on GPUs", "comments": null, "journal-ref": null, "doi": "10.1109/TPDS.2021.3064892", "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Triangle counting is a building block for a wide range of graph applications.\nTraditional wisdom suggests that i) hashing is not suitable for triangle\ncounting, ii) edge-centric triangle counting beats vertex-centric design, and\niii) communication-free and workload balanced graph partitioning is a grand\nchallenge for triangle counting. On the contrary, we advocate that i) hashing\ncan help the key operations for scalable triangle counting on Graphics\nProcessing Units (GPUs), i.e., list intersection and graph partitioning,\nii)vertex-centric design reduces both hash table construction cost and memory\nconsumption, which is limited on GPUs. In addition, iii) we exploit graph and\nworkload collaborative, and hashing-based 2D partitioning to scale\nvertex-centric triangle counting over 1,000 GPUswith sustained scalability. In\nthis work, we present TRUST which performs triangle counting with the hash\noperation and vertex-centric mechanism at the core. To the best of our\nknowledge, TRUSTis the first work that achieves over one trillion Traversed\nEdges Per Second (TEPS) rate for triangle counting.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 22:14:35 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Pandey", "Santosh", ""], ["Wang", "Zhibin", ""], ["Zhong", "Sheng", ""], ["Tian", "Chen", ""], ["Zheng", "Bolong", ""], ["Li", "Xiaoye", ""], ["Li", "Lingda", ""], ["Hoisie", "Adolfy", ""], ["Ding", "Caiwen", ""], ["Li", "Dong", ""], ["Liu", "Hang", ""]]}, {"id": "2103.08061", "submitter": "Sharareh Alipour", "authors": "Sharareh Alipour and Mohammadhadi Salari", "title": "On distributed algorithms for minimum dominating set problem and beyond", "comments": "arXiv admin note: text overlap with arXiv:2012.04883", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the minimum dominating set (MDS) problem and the\nminimum total dominating set MTDS) problem which have many applications in real\nworld. We propose a new idea to compute approximate MDS and MTDS. Next, we give\nan upper bound on the size of MDS of a graph. We also present a distributed\nrandomized algorithm that produces a (total) dominating subset of a given graph\nwhose expected size equals the upper bound. Next, we give fast distributed\nalgorithms for computing approximated solutions for the MDS and MTDS problems\nusing our theoretical results.\n  The MDS problem arises in diverse areas, for example in social networks,\nwireless networks, robotics, and etc. Most often, we need to compute MDS in a\ndistributed or parallel model. So we implement our algorithm on massive\nnetworks and compare our results with the state of the art algorithms to show\nthe efficiency of our proposed algorithms in practice. We also show how to\nextend our idea to propose algorithms for solving $k$-dominating set problem\nand set cover problem. Our algorithms can also handle the case where the\nnetwork is dynamic or in the case where we have constraints in choosing the\nelements of MDS.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 23:07:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Alipour", "Sharareh", ""], ["Salari", "Mohammadhadi", ""]]}, {"id": "2103.08191", "submitter": "Saurabh Kadekodi", "authors": "Saurabh Kadekodi, Francisco Maturana, Suhas Jayaram Subramanya,\n  Juncheng Yang, K. V. Rashmi, Gregory R. Ganger", "title": "PACEMAKER: Avoiding HeART attacks in storage clusters with disk-adaptive\n  redundancy", "comments": "Published in USENIX Symposium on Operating Systems Design and\n  Implementation (OSDI) 2020", "journal-ref": "14th USENIX Symposium on Operating Systems Design and\n  Implementation (OSDI), 2020, (pp. 369-385)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data redundancy provides resilience in large-scale storage clusters, but\nimposes significant cost overhead. Substantial space-savings can be realized by\ntuning redundancy schemes to observed disk failure rates. However, prior design\nproposals for such tuning are unusable in real-world clusters, because the IO\nload of transitions between schemes overwhelms the storage infrastructure\n(termed transition overload).\n  This paper analyzes traces for millions of disks from production systems at\nGoogle, NetApp, and Backblaze to expose and understand transition overload as a\nroadblock to disk-adaptive redundancy: transition IO under existing approaches\ncan consume 100% cluster IO continuously for several weeks. Building on the\ninsights drawn, we present PACEMAKER, a low-overhead disk-adaptive redundancy\norchestrator. PACEMAKER mitigates transition overload by (1) proactively\norganizing data layouts to make future transitions efficient, and (2)\ninitiating transitions proactively in a manner that avoids urgency while not\ncompromising on space-savings. Evaluation of PACEMAKER with traces from four\nlarge (110K-450K disks) production clusters show that the transition IO\nrequirement decreases to never needing more than 5% cluster IO bandwidth\n(0.2-0.4% on average). PACEMAKER achieves this while providing overall\nspace-savings of 14-20% and never leaving data under-protected. We also\ndescribe and experiment with an integration of PACEMAKER into HDFS.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 07:57:16 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kadekodi", "Saurabh", ""], ["Maturana", "Francisco", ""], ["Subramanya", "Suhas Jayaram", ""], ["Yang", "Juncheng", ""], ["Rashmi", "K. V.", ""], ["Ganger", "Gregory R.", ""]]}, {"id": "2103.08295", "submitter": "Haoyu Ren", "authors": "Haoyu Ren, Darko Anicic and Thomas Runkler", "title": "TinyOL: TinyML with Online-Learning on Microcontrollers", "comments": "Accepted by The International Joint Conference on Neural Network\n  (IJCNN) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tiny machine learning (TinyML) is a fast-growing research area committed to\ndemocratizing deep learning for all-pervasive microcontrollers (MCUs).\nChallenged by the constraints on power, memory, and computation, TinyML has\nachieved significant advancement in the last few years. However, the current\nTinyML solutions are based on batch/offline settings and support only the\nneural network's inference on MCUs. The neural network is first trained using a\nlarge amount of pre-collected data on a powerful machine and then flashed to\nMCUs. This results in a static model, hard to adapt to new data, and impossible\nto adjust for different scenarios, which impedes the flexibility of the\nInternet of Things (IoT). To address these problems, we propose a novel system\ncalled TinyOL (TinyML with Online-Learning), which enables incremental\non-device training on streaming data. TinyOL is based on the concept of online\nlearning and is suitable for constrained IoT devices. We experiment TinyOL\nunder supervised and unsupervised setups using an autoencoder neural network.\nFinally, we report the performance of the proposed solution and show its\neffectiveness and feasibility.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:39:41 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 08:47:33 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 23:06:28 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ren", "Haoyu", ""], ["Anicic", "Darko", ""], ["Runkler", "Thomas", ""]]}, {"id": "2103.08304", "submitter": "Michael Kuperberg", "authors": "Michael Kuperberg", "title": "Scaling a Blockchain-based Railway Control System Prototype for Mainline\n  Railways: a Progress Report", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Railway operations require control systems to ensure safety and efficiency,\nand to coordinate infrastructure elements such as switches, signals and train\nprotection. To compete with the traditional approaches to these systems, a\nblockchain-based approach has been proposed, with the intent to build a more\nresilient, integrated and cost-efficient system. Additionally, the developed\nblockchain-based architecture enables to run safety-relevant and\nsecurity-focused business logic on off-the-shelf platforms such as cloud,\nrather than on specialized (and expensive) secure hardware. After implementing\na prototype of the blockchain-based railway control system, scaling the\napproach to real-world mainline and branch operations required a thorough\nvalidation of the design choices. In this technical report, we show how\nperformance calculations, long-term technology perspectives and law-mandated\nnorms have impacted the architecture, the technology choices, and the\nmake-buy-reuse decisions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 12:33:35 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kuperberg", "Michael", ""]]}, {"id": "2103.08413", "submitter": "Meghana Thiyyakat", "authors": "Meghana Thiyyakat, Subramaniam Kalambur and Dinkar Sitaram", "title": "Megha: Decentralized Global Fair Scheduling for Federated Clusters", "comments": "10 pages, 12 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Increasing scale and heterogeneity in data centers have led to the\ndevelopment of federated clusters such as KubeFed, Hydra, and Pigeon, that\nfederate individual data center clusters. In our work, we introduce Megha, a\nnovel decentralized resource management framework for such federated clusters.\nMegha employs flexible logical partitioning of clusters to distribute its\nscheduling load, ensuring that the requirements of the workload are satisfied\nwith very low scheduling overheads. It uses a distributed global scheduler that\ndoes not rely on a centralized data store but, instead, works with eventual\nconsistency, unlike other schedulers that use a tiered architecture or rely on\ncentralized databases. Our experiments with Megha show that it can schedule\ntasks taking into account fairness and placement constraints with low resource\nallocation times - in the order of tens of milliseconds.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 14:39:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Thiyyakat", "Meghana", ""], ["Kalambur", "Subramaniam", ""], ["Sitaram", "Dinkar", ""]]}, {"id": "2103.08442", "submitter": "Anca Jurcut Dr.", "authors": "Promise Agbedanu and Anca Delia Jurcut", "title": "BLOFF: A Blockchain based Forensic Model in IoT", "comments": null, "journal-ref": "Revolutionary Applications of Blockchain-Enabled Privacy and\n  Access Control, Editors Surjit Singh (Thapar Institute of Engineering and\n  Technology, India) and Anca Delia Jurcut (University College Dublin,\n  Ireland), IGI Global, April 2021", "doi": "10.4018/978-1-7998-7589-5.ch003", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this era of explosive growth in technology, the internet of things (IoT)\nhas become the game changer when we consider technologies like smart homes and\ncities, smart energy, security and surveillance, and healthcare. The numerous\nbenefits provided by IoT have become attractive technologies for users and\ncybercriminals. Cybercriminals of today have the tools and the technology to\ndeploy millions of sophisticated attacks. These attacks need to be\ninvestigated; this is where digital forensics comes into play. However, it is\nnot easy to conduct a forensic investigation in IoT systems because of the\nheterogeneous nature of the IoT environment. Additionally, forensic\ninvestigators mostly rely on evidence from service providers, a situation that\ncan lead to evidence contamination. To solve this problem, the authors proposed\na blockchain-based IoT forensic model that prevents the admissibility of\ntampered logs into evidence.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:11:07 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Agbedanu", "Promise", ""], ["Jurcut", "Anca Delia", ""]]}, {"id": "2103.08546", "submitter": "Prashant Singh Chouhan", "authors": "Prashant Singh Chouhan (1), Harsh Khetawat (2), Neil Resnik (1),\n  Twinkle Jain (1), Rohan Garg (3), Gene Cooperman (1), Rebecca Hartman-Baker\n  (4) and Zhengji Zhao (4) ((1) Northeastern University, (2) North Carolina\n  State University, (3) Nutanix Inc., (4) Lawrence Berkeley National Lab.)", "title": "Improving scalability and reliability of MPI-agnostic transparent\n  checkpointing for production workloads at NERSC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Checkpoint/restart (C/R) provides fault-tolerant computing capability,\nenables long running applications, and provides scheduling flexibility for\ncomputing centers to support diverse workloads with different priority. It is\ntherefore vital to get transparent C/R capability working at NERSC. MANA, by\nGarg et. al., is a transparent checkpointing tool that has been selected due to\nits MPI-agnostic and network-agnostic approach. However, originally written as\na proof-of-concept code, MANA was not ready to use with NERSC's diverse\nproduction workloads, which are dominated by MPI and hybrid MPI+OpenMP\napplications. In this talk, we present ongoing work at NERSC to enable MANA for\nNERSC's production workloads, including fixing bugs that were exposed by the\ntop applications at NERSC, adding new features to address system changes,\nevaluating C/R overhead at scale, etc. The lessons learned from making MANA\nproduction-ready for HPC applications will be useful for C/R tool developers,\nsupercomputing centers and HPC end-users alike.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:13:56 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 14:12:34 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chouhan", "Prashant Singh", ""], ["Khetawat", "Harsh", ""], ["Resnik", "Neil", ""], ["Jain", "Twinkle", ""], ["Garg", "Rohan", ""], ["Cooperman", "Gene", ""], ["Hartman-Baker", "Rebecca", ""], ["Zhao", "Zhengji", ""]]}, {"id": "2103.08675", "submitter": "Daniel Ritter", "authors": "Daniel Ritter", "title": "Cost-aware Integration Process Modeling in Multiclouds", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration as a service (INTaaS) is the centrepiece of current corporate,\ncloud and device integration processes. Thereby, compositions of integration\npatterns denote the required integration logic as integration processes,\ncurrently running in single-clouds. While multicloud settings gain importance,\ntheir promised freedom of selecting the best option for a specific problem is\ncurrently not realized as well as security constraints are handled in a\ncost-intensive manner for the INTaaS vendors, leading to security vs. costs\ngoal conflicts, and intransparent to the process modeler.\n  In this work, we propose a design-time placement for processes in multiclouds\nthat is cost-optimal for INTaaS problem sizes, and respects configurable\nsecurity constraints of their customers. To make the solution tractable for\nlarger, productive INTaaS processes, it is relaxed by using a local search\nheuristic, and complemented by correctness-preserving model decomposition. This\nallows for a novel perspective on cost-aware process modeling from a process\nmodeler's perspective.\n  The multicloud process placement is evaluated on real-world integration\nprocesses with respect to cost- and runtime-efficiency, and discusses\ninteresting trade-offs. The process modeler's perspective is investigated based\non a new cost-aware modeling process, featuring the interaction between the\nuser and the INTaaS vendor through ad-hoc multicloud cost calculation and\ncorrectness-preserving, process cost reduction proposals.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 19:33:27 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ritter", "Daniel", ""]]}, {"id": "2103.08716", "submitter": "Jacob Odg{\\aa}rd T{\\o}rring", "authors": "Jacob Odg{\\aa}rd T{\\o}rring, Jan Christian Meyer, Anne C. Elster", "title": "Autotuning Benchmarking Techniques: A Roofline Model Case Study", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peak performance metrics published by vendors often do not correspond to what\ncan be achieved in practice. It is therefore of great interest to do extensive\nbenchmarking on core applications and library routines. Since DGEMM is one of\nthe most used in compute-intensive numerical codes, it is typically highly\nvendor optimized and of great interest for empirical benchmarks. In this paper\nwe show how to build a novel tool that autotunes the benchmarking process for\nthe Roofline model. Our novel approach can efficiently and reliably find\noptimal configurations for any target hardware. Results of our tool on a range\nof hardware architectures and comparisons to theoretical peak performance are\nincluded.\n  Our tool autotunes the benchmarks for the target architecture by deciding the\noptimal parameters through state space reductions and exhaustive search. Our\ncore idea includes calculating the confidence interval using the variance and\nmean and comparing it against the current optimum solution. We can then\nterminate the evaluation process early if the confidence interval's maximum is\nlower than the current optimum solution. This dynamic approach yields a search\ntime improvement of up to 116.33x for the DGEMM benchmarking process compared\nto a traditional fixed sample-size methodology. Our tool produces the same\nbenchmarking result with an error of less than 2% for each of the optimization\ntechniques we apply, while providing a great reduction in search time. We\ncompare these results against hand-tuned benchmarking parameters. Results from\nthe memory-intensive TRIAD benchmark, and some ideas for future directions are\nalso included.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:00:10 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 13:48:14 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["T\u00f8rring", "Jacob Odg\u00e5rd", ""], ["Meyer", "Jan Christian", ""], ["Elster", "Anne C.", ""]]}, {"id": "2103.08825", "submitter": "Kun Li", "authors": "Kun Li, Liang Yuan, Yunquan Zhang, Yue Yue, Hang Cao, Pengqi Lu", "title": "An Efficient Vectorization Scheme for Stencil Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stencil computation is one of the most important kernels in various\nscientific and engineering applications. A variety of work has focused on\nvectorization and tiling techniques, aiming at exploiting the in-core data\nparallelism and data locality respectively. In this paper, the downsides of\nexisting vectorization schemes are analyzed. Briefly, they either incur data\nalignment conflicts or hurt the data locality when integrated with tiling. Then\nwe propose a novel transpose layout to preserve the data locality for tiling\nand reduce the data reorganization overhead for vectorization simultaneously.\nTo further improve the data reuse at the register level, a time loop\nunroll-and-jam strategy is designed to perform multistep stencil computation\nalong the time dimension. Experimental results on the AVX-2 and AVX-512 CPUs\nshow that our approach obtains a competitive performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:22:52 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 02:56:58 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Li", "Kun", ""], ["Yuan", "Liang", ""], ["Zhang", "Yunquan", ""], ["Yue", "Yue", ""], ["Cao", "Hang", ""], ["Lu", "Pengqi", ""]]}, {"id": "2103.08828", "submitter": "Sudeep Pasricha", "authors": "Febin Sunny, Asif Mirza, Ishan Thakkar, Mahdi Nikdast, and Sudeep\n  Pasricha", "title": "ARXON: A Framework for Approximate Communication over Photonic\n  Networks-on-Chip", "comments": "arXiv admin note: text overlap with arXiv:2002.11289", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The approximate computing paradigm advocates for relaxing accuracy goals in\napplications to improve energy-efficiency and performance. Recently, this\nparadigm has been explored to improve the energy-efficiency of silicon photonic\nnetworks-on-chip (PNoCs). Silicon photonic interconnects suffer from high power\ndissipation because of laser sources, which generate carrier wavelengths, and\ntuning power required for regulating photonic devices under different\nuncertainties. In this paper, we propose a framework called ARXON to reduce\nsuch power dissipation overhead by enabling intelligent and aggressive\napproximation during communication over silicon photonic links in PNoCs. Our\nframework reduces laser and tuning-power overhead while intelligently\napproximating communication, such that application output quality is not\ndistorted beyond an acceptable limit. Simulation results show that our\nframework can achieve up to 56.4% lower laser power consumption and up to 23.8%\nbetter energy-efficiency than the best-known prior work on approximate\ncommunication with silicon photonic interconnects and for the same application\noutput quality.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:25:04 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sunny", "Febin", ""], ["Mirza", "Asif", ""], ["Thakkar", "Ishan", ""], ["Nikdast", "Mahdi", ""], ["Pasricha", "Sudeep", ""]]}, {"id": "2103.08870", "submitter": "Lusine Abrahamyan", "authors": "Lusine Abrahamyan, Yiming Chen, Giannis Bekoulis and Nikos Deligiannis", "title": "Learned Gradient Compression for Distributed Deep Learning", "comments": "15 pages 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks on large datasets containing high-dimensional\ndata requires a large amount of computation. A solution to this problem is\ndata-parallel distributed training, where a model is replicated into several\ncomputational nodes that have access to different chunks of the data. This\napproach, however, entails high communication rates and latency because of the\ncomputed gradients that need to be shared among nodes at every iteration. The\nproblem becomes more pronounced in the case that there is wireless\ncommunication between the nodes (i.e. due to the limited network bandwidth). To\naddress this problem, various compression methods have been proposed including\nsparsification, quantization, and entropy encoding of the gradients. Existing\nmethods leverage the intra-node information redundancy, that is, they compress\ngradients at each node independently. In contrast, we advocate that the\ngradients across the nodes are correlated and propose methods to leverage this\ninter-node redundancy to improve compression efficiency. Depending on the node\ncommunication protocol (parameter server or ring-allreduce), we propose two\ninstances of the LGC approach that we coin Learned Gradient Compression (LGC).\nOur methods exploit an autoencoder (i.e. trained during the first stages of the\ndistributed training) to capture the common information that exists in the\ngradients of the distributed nodes. We have tested our LGC methods on the image\nclassification and semantic segmentation tasks using different convolutional\nneural networks (ResNet50, ResNet101, PSPNet) and multiple datasets (ImageNet,\nCifar10, CamVid). The ResNet101 model trained for image classification on\nCifar10 achieved an accuracy of 93.57%, which is lower than the baseline\ndistributed training with uncompressed gradients only by 0.18%.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 06:42:36 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 05:55:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Abrahamyan", "Lusine", ""], ["Chen", "Yiming", ""], ["Bekoulis", "Giannis", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "2103.08894", "submitter": "Medha Atre", "authors": "Medha Atre and Birendra Jha and Ashwini Rao", "title": "Distributed Deep Learning Using Volunteer Computing-Like Paradigm", "comments": null, "journal-ref": "ScaDL workshop at IEEE International Parallel and Distributed\n  Processing Symposium 2021", "doi": "10.1109/IPDPSW52791.2021.00144", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of Deep Learning (DL) in commercial applications such as image\nclassification, sentiment analysis and speech recognition is increasing. When\ntraining DL models with large number of parameters and/or large datasets, cost\nand speed of training can become prohibitive. Distributed DL training solutions\nthat split a training job into subtasks and execute them over multiple nodes\ncan decrease training time. However, the cost of current solutions, built\npredominantly for cluster computing systems, can still be an issue. In contrast\nto cluster computing systems, Volunteer Computing (VC) systems can lower the\ncost of computing, but applications running on VC systems have to handle fault\ntolerance, variable network latency and heterogeneity of compute nodes, and the\ncurrent solutions are not designed to do so. We design a distributed solution\nthat can run DL training on a VC system by using a data parallel approach. We\nimplement a novel asynchronous SGD scheme called VC-ASGD suited for VC systems.\nIn contrast to traditional VC systems that lower cost by using untrustworthy\nvolunteer devices, we lower cost by leveraging preemptible computing instances\non commercial cloud platforms. By using preemptible instances that require\napplications to be fault tolerant, we lower cost by 70-90% and improve data\nsecurity.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:32:58 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 12:50:05 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 06:41:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Atre", "Medha", ""], ["Jha", "Birendra", ""], ["Rao", "Ashwini", ""]]}, {"id": "2103.08936", "submitter": "Antonio Fern\\'andez Anta", "authors": "Vicent Cholvi, Antonio Fern\\'andez Anta, Chryssis Georgiou, Nicolas\n  Nicolaou, Michel Raynal, Antonio Russo", "title": "Byzantine-tolerant Distributed Grow-only Sets: Specification and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to formalize Distributed Ledger Technologies and their\ninterconnections, a recent line of research work has formulated the notion of\nDistributed Ledger Object (DLO), which is a concurrent object that maintains a\ntotally ordered sequence of records, abstracting blockchains and distributed\nledgers. Through DLO, the Atomic Appends problem, intended as the need of a\nprimitive able to append multiple records to distinct ledgers in an atomic way,\nis studied as a basic interconnection problem among ledgers.\n  In this work, we propose the Distributed Grow-only Set object (DSO), which\ninstead of maintaining a sequence of records, as in a DLO, maintains a set of\nrecords in an immutable way: only Add and Get operations are provided. This\nobject is inspired by the Grow-only Set (G-Set) data type which is part of the\nConflict-free Replicated Data Types. We formally specify the object and we\nprovide a consensus-free Byzantine-tolerant implementation that guarantees\neventual consistency. We then use our Byzantine-tolerant DSO (BDSO)\nimplementation to provide consensus-free algorithmic solutions to the Atomic\nAppends and Atomic Adds (the analogous problem of atomic appends applied on\nG-Sets) problems, as well as to construct consensus-free Single-Writer BDLOs.\nWe believe that the BDSO has applications beyond the above-mentioned problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 09:40:15 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Cholvi", "Vicent", ""], ["Anta", "Antonio Fern\u00e1ndez", ""], ["Georgiou", "Chryssis", ""], ["Nicolaou", "Nicolas", ""], ["Raynal", "Michel", ""], ["Russo", "Antonio", ""]]}, {"id": "2103.08949", "submitter": "Joel Rybicki", "authors": "Dan Alistarh and Faith Ellen and Joel Rybicki", "title": "Wait-free approximate agreement on graphs", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate agreement is one of the few variants of consensus that can be\nsolved in a wait-free manner in asynchronous systems where processes\ncommunicate by reading and writing to shared memory. In this work, we consider\na natural generalisation of approximate agreement on arbitrary undirected\nconnected graphs. Each process is given a vertex of the graph as input and, if\nnon-faulty, must output a vertex such that\n  - all the outputs are within distance 1 of one another, and\n  - each output value lies on a shortest path between two input values.\n  From prior work, it is known that there is no wait-free algorithm among $n\n\\ge 3$ processes for this problem on any cycle of length $c \\ge 4$, by\nreduction from 2-set agreement (Casta\\~neda et al., 2018).\n  In this work, we investigate the solvability and complexity of this task on\ngeneral graphs. We give a new, direct proof of the impossibility of approximate\nagreement on cycles of length $c \\ge 4$, via a generalisation of Sperner's\nLemma to convex polygons. We also extend the reduction from 2-set agreement to\na larger class of graphs, showing that approximate agreement on on these graphs\nis unsolvable. Furthermore, we show that combinatorial arguments, used by both\nexisting proofs, are necessary, by showing that the impossibility of a\nwait-free algorithm in the nonuniform iterated snapshot model cannot be proved\nvia an extension-based proof. On the positive side, we present a wait-free\nalgorithm for a class of graphs that properly contains the class of chordal\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:15:58 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Alistarh", "Dan", ""], ["Ellen", "Faith", ""], ["Rybicki", "Joel", ""]]}, {"id": "2103.08983", "submitter": "Michel Gokan Khan", "authors": "Michel Gokan Khan, Javid Taheri, Auday Al-Dulaimy, Andreas Kassler", "title": "PerfSim: A Performance Simulator for Cloud Native Computing", "comments": "for the dataset used for evaluation, see\n  https://ieee-dataport.org/documents/experiments-data-used-evaluating-perfsim-simulation-accuracy-based-sfc-stress-workloads\n  and https://ui.neptune.ai/o/kau/org/PerfSim/experiments. Source code will be\n  published upon acceptance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud native computing paradigm allows microservice-based applications to\ntake advantage of cloud infrastructure in a scalable, reusable, and\ninteroperable way. However, in a cloud native system, the vast number of\nconfiguration parameters and highly granular resource allocation policies can\nsignificantly impact the performance and deployment cost of such applications.\nFor understanding and analyzing these implications in an easy, quick, and\ncost-effective way, we present PerfSim, a discrete-event simulator for\napproximating and predicting the performance of cloud native service chains in\nuser-defined scenarios. To this end, we proposed a systematic approach for\nmodeling the performance of microservices endpoint functions by collecting and\nanalyzing their performance and network traces. With a combination of the\nextracted models and user-defined scenarios, PerfSim can simulate the\nperformance behavior of service chains over a given period and provides an\napproximation for system KPIs, such as requests' average response time. Using\nthe processing power of a single laptop, we evaluated both simulation accuracy\nand speed of PerfSim in 104 prevalent scenarios and compared the simulation\nresults with the identical deployment in a real Kubernetes cluster. We achieved\n~81-99% simulation accuracy in approximating the average response time of\nincoming requests and ~16-1200 times speed-up factor for the simulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:21:04 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Khan", "Michel Gokan", ""], ["Taheri", "Javid", ""], ["Al-Dulaimy", "Auday", ""], ["Kassler", "Andreas", ""]]}, {"id": "2103.09019", "submitter": "Felippe Vieira Zacarias", "authors": "Felippe V. Zacarias (1, 2 and 3), Vinicius Petrucci (1 and 5), Rajiv\n  Nishtala (4), Paul Carpenter (3) and Daniel Moss\\'e (5) ((1) Universidade\n  Federal da Bahia, (2) Universitat Polit\\`ecnica de Catalunya, (3) Barcelona\n  Supercomputing Center, (4) Coop, Norway/Norwegian University of Science and\n  Technology, Norway, (5) University of Pittsburgh)", "title": "Intelligent colocation of HPC workloads", "comments": "Submitted to Journal of Parallel and Distributed Computing", "journal-ref": "Volume 151, May 2021, Pages 125-137", "doi": "10.1016/j.jpdc.2021.02.010", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many HPC applications suffer from a bottleneck in the shared caches,\ninstruction execution units, I/O or memory bandwidth, even though the remaining\nresources may be underutilized. It is hard for developers and runtime systems\nto ensure that all critical resources are fully exploited by a single\napplication, so an attractive technique for increasing HPC system utilization\nis to colocate multiple applications on the same server. When applications\nshare critical resources, however, contention on shared resources may lead to\nreduced application performance.\n  In this paper, we show that server efficiency can be improved by first\nmodeling the expected performance degradation of colocated applications based\non measured hardware performance counters, and then exploiting the model to\ndetermine an optimized mix of colocated applications. This paper presents a new\nintelligent resource manager and makes the following contributions: (1) a new\nmachine learning model to predict the performance degradation of colocated\napplications based on hardware counters and (2) an intelligent scheduling\nscheme deployed on an existing resource manager to enable application\nco-scheduling with minimum performance degradation. Our results show that our\napproach achieves performance improvements of 7% (avg) and 12% (max) compared\nto the standard policy commonly used by existing job managers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:35:35 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zacarias", "Felippe V.", "", "1, 2 and 3"], ["Petrucci", "Vinicius", "", "1 and 5"], ["Nishtala", "Rajiv", ""], ["Carpenter", "Paul", ""], ["Moss\u00e9", "Daniel", ""]]}, {"id": "2103.09181", "submitter": "Rafael Ferreira da Silva", "authors": "Rafael Ferreira da Silva, Henri Casanova, Kyle Chard, Dan Laney, Dong\n  Ahn, Shantenu Jha, Carole Goble, Lavanya Ramakrishnan, Luc Peterson, Bjoern\n  Enders, Douglas Thain, Ilkay Altintas, Yadu Babuji, Rosa M. Badia, Vivien\n  Bonazzi, Taina Coleman, Michael Crusoe, Ewa Deelman, Frank Di Natale, Paolo\n  Di Tommaso, Thomas Fahringer, Rosa Filgueira, Grigori Fursin, Alex Ganose,\n  Bjorn Gruning, Daniel S. Katz, Olga Kuchar, Ana Kupresanin, Bertram\n  Ludascher, Ketan Maheshwari, Marta Mattoso, Kshitij Mehta, Todd Munson,\n  Jonathan Ozik, Tom Peterka, Loic Pottier, Tim Randles, Stian Soiland-Reyes,\n  Benjamin Tovar, Matteo Turilli, Thomas Uram, Karan Vahi, Michael Wilde,\n  Matthew Wolf, Justin Wozniak", "title": "Workflows Community Summit: Bringing the Scientific Workflows Community\n  Together", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.4606958", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Scientific workflows have been used almost universally across scientific\ndomains, and have underpinned some of the most significant discoveries of the\npast several decades. Many of these workflows have high computational, storage,\nand/or communication demands, and thus must execute on a wide range of\nlarge-scale platforms, from large clouds to upcoming exascale high-performance\ncomputing (HPC) platforms. These executions must be managed using some software\ninfrastructure. Due to the popularity of workflows, workflow management systems\n(WMSs) have been developed to provide abstractions for creating and executing\nworkflows conveniently, efficiently, and portably. While these efforts are all\nworthwhile, there are now hundreds of independent WMSs, many of which are\nmoribund. As a result, the WMS landscape is segmented and presents significant\nbarriers to entry due to the hundreds of seemingly comparable, yet\nincompatible, systems that exist. As a result, many teams, small and large,\nstill elect to build their own custom workflow solution rather than adopt, or\nbuild upon, existing WMSs. This current state of the WMS landscape negatively\nimpacts workflow users, developers, and researchers. The \"Workflows Community\nSummit\" was held online on January 13, 2021. The overarching goal of the summit\nwas to develop a view of the state of the art and identify crucial research\nchallenges in the workflow community. Prior to the summit, a survey sent to\nstakeholders in the workflow community (including both developers of WMSs and\nusers of workflows) helped to identify key challenges in this community that\nwere translated into 6 broad themes for the summit, each of them being the\nobject of a focused discussion led by a volunteer member of the community. This\nreport documents and organizes the wealth of information provided by the\nparticipants before, during, and after the summit.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:29:33 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["da Silva", "Rafael Ferreira", ""], ["Casanova", "Henri", ""], ["Chard", "Kyle", ""], ["Laney", "Dan", ""], ["Ahn", "Dong", ""], ["Jha", "Shantenu", ""], ["Goble", "Carole", ""], ["Ramakrishnan", "Lavanya", ""], ["Peterson", "Luc", ""], ["Enders", "Bjoern", ""], ["Thain", "Douglas", ""], ["Altintas", "Ilkay", ""], ["Babuji", "Yadu", ""], ["Badia", "Rosa M.", ""], ["Bonazzi", "Vivien", ""], ["Coleman", "Taina", ""], ["Crusoe", "Michael", ""], ["Deelman", "Ewa", ""], ["Di Natale", "Frank", ""], ["Di Tommaso", "Paolo", ""], ["Fahringer", "Thomas", ""], ["Filgueira", "Rosa", ""], ["Fursin", "Grigori", ""], ["Ganose", "Alex", ""], ["Gruning", "Bjorn", ""], ["Katz", "Daniel S.", ""], ["Kuchar", "Olga", ""], ["Kupresanin", "Ana", ""], ["Ludascher", "Bertram", ""], ["Maheshwari", "Ketan", ""], ["Mattoso", "Marta", ""], ["Mehta", "Kshitij", ""], ["Munson", "Todd", ""], ["Ozik", "Jonathan", ""], ["Peterka", "Tom", ""], ["Pottier", "Loic", ""], ["Randles", "Tim", ""], ["Soiland-Reyes", "Stian", ""], ["Tovar", "Benjamin", ""], ["Turilli", "Matteo", ""], ["Uram", "Thomas", ""], ["Vahi", "Karan", ""], ["Wilde", "Michael", ""], ["Wolf", "Matthew", ""], ["Wozniak", "Justin", ""]]}, {"id": "2103.09235", "submitter": "Kun Li", "authors": "Kun Li, Liang Yuan, Yunquan Zhang, Yue Yue, Hang Cao, Pengqi Lu", "title": "Reducing Redundancy in Data Organization and Arithmetic Calculation for\n  Stencil Computations", "comments": "arXiv admin note: substantial text overlap with arXiv:2103.08825", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stencil computation is one of the most important kernels in various\nscientific and engineering applications. A variety of work has focused on\nvectorization techniques, aiming at exploiting the in-core data parallelism.\nBriefly, they either incur data alignment conflicts or hurt the data locality\nwhen integrated with tiling. In this paper, a novel transpose layout is devised\nto preserve the data locality for tiling in the data space and reduce the data\nreorganization overhead for vectorization simultaneously. We then propose an\napproach of temporal computation folding designed to further reduce the\nredundancy of arithmetic calculations by exploiting the register reuse,\nalleviating the increased register pressure, and deducing generalization with a\nlinear regression model. Experimental results on the AVX-2 and AVX-512 CPUs\nshow that our approach obtains a competitive performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:23:16 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Li", "Kun", ""], ["Yuan", "Liang", ""], ["Zhang", "Yunquan", ""], ["Yue", "Yue", ""], ["Cao", "Hang", ""], ["Lu", "Pengqi", ""]]}, {"id": "2103.09389", "submitter": "Romit Maulik", "authors": "Romit Maulik, Dimitrios Fytanidis, Bethany Lusch, Venkatram\n  Vishwanath, Saumil Patel", "title": "PythonFOAM: In-situ data analyses with OpenFOAM and Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we outline the development of a general-purpose Python-based\ndata analysis tool for OpenFOAM 8. Our implementation relies on the\nconstruction of OpenFOAM applications that have bindings to data analysis\nlibraries in Python. Double precision data in OpenFOAM is cast to a NumPy array\nusing the NumPy C-API and Python modules may then be used for arbitrary data\nanalysis and manipulation on flow-field information. This document highlights\nhow the proposed framework may be used for an in-situ online singular value\ndecomposition (SVD) implemented in Python and accessed from the OpenFOAM solver\nPimpleFOAM. Here, `in-situ' refers to a programming paradigm that allows for a\nconcurrent computation of the data analysis on the same computational resources\nutilized for the partial differential equation solver. In addition, to\ndemonstrate data-parallel analyses, we deploy a distributed SVD, which collects\nsnapshot data across the ranks of a distributed simulation to compute the\nglobal left singular vectors. Crucially, both OpenFOAM and Python share the\nsame message passing interface (MPI) communicator for this deployment which\nallows Python objects and functions to exchange NumPy arrays across ranks. Our\nexperiments also demonstrate how customized data science libraries such as\nTensorFlow may be leveraged through these bindings. Subsequently, we provide\nscaling assessments of our framework and the selected algorithms on multiple\nnodes of Intel Broadwell and KNL architectures for canonical test cases such as\nthe large eddy simulations of a backward facing step and a channel flow at\nfriction Reynolds number of 395.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 01:34:41 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Maulik", "Romit", ""], ["Fytanidis", "Dimitrios", ""], ["Lusch", "Bethany", ""], ["Vishwanath", "Venkatram", ""], ["Patel", "Saumil", ""]]}, {"id": "2103.09424", "submitter": "Raj Kumar Maity", "authors": "Avishek Ghosh, Raj Kumar Maity, Arya Mazumdar, Kannan Ramchandran", "title": "Escaping Saddle Points in Distributed Newton's Method with Communication\n  efficiency and Byzantine Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the problem of optimizing a non-convex loss function (with saddle\npoints) in a distributed framework in the presence of Byzantine machines. We\nconsider a standard distributed setting with one central machine (parameter\nserver) communicating with many worker machines. Our proposed algorithm is a\nvariant of the celebrated cubic-regularized Newton method of Nesterov and\nPolyak \\cite{nest}, which avoids saddle points efficiently and converges to\nlocal minima. Furthermore, our algorithm resists the presence of Byzantine\nmachines, which may create \\emph{fake local minima} near the saddle points of\nthe loss function, also known as saddle-point attack. We robustify the\ncubic-regularized Newton algorithm such that it avoids the saddle points and\nthe fake local minimas efficiently. Furthermore, being a second order\nalgorithm, the iteration complexity is much lower than its first order\ncounterparts, and thus our algorithm communicates little with the parameter\nserver. We obtain theoretical guarantees for our proposed scheme under several\nsettings including approximate (sub-sampled) gradients and Hessians. Moreover,\nwe validate our theoretical findings with experiments using standard datasets\nand several types of Byzantine attacks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 03:53:58 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ghosh", "Avishek", ""], ["Maity", "Raj Kumar", ""], ["Mazumdar", "Arya", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "2103.09425", "submitter": "Zhenliang Lu", "authors": "Yuan Lu and Zhenliang Lu and Qiang Tang", "title": "Bolt-Dumbo Transformer: Asynchronous Consensus As Fast As Pipelined BFT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimistic asynchronous atomic broadcast was proposed to improve the\nperformance of asynchronous protocols while maintaining their liveness in\nunstable networks (Kursawe-Shoup, 2002; Ramasamy-Cachin, 2005). They used a\nfaster deterministic protocol in the optimistic case when the network condition\nremains good, and can safely fallback to a pessimistic path running\nasynchronous atomic broadcast once the fast path fails to proceed.\nUnfortunately, besides that the pessimistic path is slow, existing fallback\nmechanisms directly use a heavy tool of asynchronous multi-valued validated\nByzantine agreement (MVBA). When deployed on the open Internet, which could be\nfluctuating, the inefficient fallback may happen frequently thus the benefits\nof adding the optimistic path are eliminated.\n  We give a generic framework for practical optimistic asynchronous atomic\nbroadcast. A new abstraction of the optimistic case protocols, which can be\ninstantiated easily, is presented. More importantly, it enables us to design a\nhighly efficient fallback mechanism to handle the fast path failures. The\nresulting fallback replaces the cumbersome MVBA by a variant of simple binary\nagreement only. Besides a detailed security analysis, we also give concrete\ninstantiations of our framework and implement them. Extensive experiments show\nthat our new fallback mechanism adds minimal overhead, demonstrating that our\nframework can enjoy both the low latency of deterministic protocols and robust\nliveness of randomized asynchronous protocols in practice.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 03:57:47 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 07:10:45 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lu", "Yuan", ""], ["Lu", "Zhenliang", ""], ["Tang", "Qiang", ""]]}, {"id": "2103.09506", "submitter": "Ying Cui", "authors": "Chencheng Ye, Ying Cui", "title": "Sample-based Federated Learning via Mini-batch SSCA", "comments": "to be published in ICC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate unconstrained and constrained sample-based\nfederated optimization, respectively. For each problem, we propose a privacy\npreserving algorithm using stochastic successive convex approximation (SSCA)\ntechniques, and show that it can converge to a Karush-Kuhn-Tucker (KKT) point.\nTo the best of our knowledge, SSCA has not been used for solving federated\noptimization, and federated optimization with nonconvex constraints has not\nbeen investigated. Next, we customize the two proposed SSCA-based algorithms to\ntwo application examples, and provide closed-form solutions for the respective\napproximate convex problems at each iteration of SSCA. Finally, numerical\nexperiments demonstrate inherent advantages of the proposed algorithms in terms\nof convergence speed, communication cost and model specification.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 08:38:03 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ye", "Chencheng", ""], ["Cui", "Ying", ""]]}, {"id": "2103.09636", "submitter": "Luidnel Maignan", "authors": "Alexandre Fernandez (LACL), Luidnel Maignan (LACL), Antoine Spicher\n  (LACL)", "title": "Accretive Computation of Global Transformations of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA math.CT nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of global transformations aims at describing synchronous\nrewriting systems on a given data structure. In this work we focus on the data\nstructure of graphs. Global transformations of graphs are defined and a local\ncriterion is given for a rule system to extend to a graph global\ntransformation. Finally we present an algorithm, with its correction, which\ncomputes online the global transformation of a finite graph in an accretive\nmanner.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:24:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Fernandez", "Alexandre", "", "LACL"], ["Maignan", "Luidnel", "", "LACL"], ["Spicher", "Antoine", "", "LACL"]]}, {"id": "2103.09655", "submitter": "Stefano Markidis Prof.", "authors": "Stefano Markidis", "title": "The Old and the New: Can Physics-Informed Deep-Learning Replace\n  Traditional Linear Solvers?", "comments": "Preprint - submitted to Frontiers", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-Informed Neural Networks (PINN) are neural networks encoding the\nproblem governing equations, such as Partial Differential Equations (PDE), as a\npart of the neural network. PINNs have emerged as a new essential tool to solve\nvarious challenging problems, including computing linear systems arising from\nPDEs, a task for which several traditional methods exist. In this work, we\nfocus first on evaluating the potential of PINNs as linear solvers in the case\nof the Poisson equation, an omnipresent equation in scientific computing. We\ncharacterize PINN linear solvers in terms of accuracy and performance under\ndifferent network configurations (depth, activation functions, input data set\ndistribution). We highlight the critical role of transfer learning. Our results\nshow that low-frequency components of the solution converge quickly as an\neffect of the F-principle. In contrast, an accurate solution of the high\nfrequencies requires an exceedingly long time. To address this limitation, we\npropose integrating PINNs into traditional linear solvers. We show that this\nintegration leads to the development of new solvers whose performance is on par\nwith other high-performance solvers, such as PETSc conjugate gradient linear\nsolvers, in terms of performance and accuracy. Overall, while the accuracy and\ncomputational performance are still a limiting factor for the direct use of\nPINN linear solvers, hybrid strategies combining old traditional linear solver\napproaches with new emerging deep-learning techniques are among the most\npromising methods for developing a new class of linear solvers.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:26:34 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 19:33:45 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Markidis", "Stefano", ""]]}, {"id": "2103.09683", "submitter": "Felix Liu", "authors": "Felix Liu, Niclas Jansson, Artur Podobas, Albin Fredriksson and\n  Stefano Markidis", "title": "Accelerating Radiation Therapy Dose Calculation with Nvidia GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiation Treatment Planning (RTP) is the process of planning the appropriate\nexternal beam radiotherapy to combat cancer in human patients. RTP is a complex\nand compute-intensive task, which often takes a long time (several hours) to\ncompute. Reducing this time allows for higher productivity at clinics and more\nsophisticated treatment planning, which can materialize in better treatments.\nThe state-of-the-art in medical facilities uses general-purpose processors\n(CPUs) to perform many steps in the RTP process. In this paper, we explore the\nuse of accelerators to reduce RTP calculating time. We focus on the step that\ncalculates the dose using the Graphics Processing Unit (GPU), which we believe\nis an excellent candidate for this computation type. Next, we create a highly\noptimized implementation for a custom Sparse Matrix-Vector Multiplication\n(SpMV) that operates on numerical formats unavailable in state-of-the-art SpMV\nlibraries (e.g., Ginkgo and cuSPARSE). We show that our implementation is\nseveral times faster than the baseline (up-to 4x) and has a higher operational\nintensity than similar (but different) versions such as Ginkgo and cuSPARSE.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:30:17 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Felix", ""], ["Jansson", "Niclas", ""], ["Podobas", "Artur", ""], ["Fredriksson", "Albin", ""], ["Markidis", "Stefano", ""]]}, {"id": "2103.09801", "submitter": "Zaid Hussain", "authors": "Hesham AlMansouri and Zaid Hussain", "title": "Real-Time Fault-Tolerance Node-to-Node Disjoint Paths Algorithm for\n  Symmetric Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disjoint paths are defined as paths between the source and destination nodes\nwhere the intermediate nodes in any two paths are disjoint. They are helpful in\nfault-tolerance routing and securing message distribution in the network.\nSeveral research papers were proposed to solve the problem of finding disjoint\npaths for variety of interconnection networks such as Hypercube, Generalized\nHypercube, Mesh, Torus, Gaussian, Eisenstein-Jacobi, and many other topologies.\nIn this research, we have developed a general real-time fault-tolerance\nalgorithm that constructs all node-to-node disjoint paths for symmetric\nnetworks where all paths are shortest or close to shortest. In addition, we\nhave simulated the proposed algorithm on different networks. The solution of\nunsolved problem in Cube-Connected-Cycles is given in the simulation results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:39:42 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["AlMansouri", "Hesham", ""], ["Hussain", "Zaid", ""]]}, {"id": "2103.09876", "submitter": "Vaikkunth Mugunthan", "authors": "Vaikkunth Mugunthan, Vignesh Gokul, Lalana Kagal, Shlomo Dubnov", "title": "Bias-Free FedGAN: A Federated Approach to Generate Bias-Free Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated Generative Adversarial Network (FedGAN) is a\ncommunication-efficient approach to train a GAN across distributed clients\nwithout clients having to share their sensitive training data. In this paper,\nwe experimentally show that FedGAN generates biased data points under\nnon-independent-and-identically-distributed (non-iid) settings. Also, we\npropose Bias-Free FedGAN, an approach to generate bias-free synthetic datasets\nusing FedGAN. Our approach generates metadata at the aggregator using the\nmodels received from clients and retrains the federated model to achieve\nbias-free results for image synthesis. Bias-Free FedGAN has the same\ncommunication cost as that of FedGAN. Experimental results on image datasets\n(MNIST and FashionMNIST) validate our claims.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 19:27:08 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 03:58:13 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mugunthan", "Vaikkunth", ""], ["Gokul", "Vignesh", ""], ["Kagal", "Lalana", ""], ["Dubnov", "Shlomo", ""]]}, {"id": "2103.10114", "submitter": "Hang Cao", "authors": "Hang Cao, Liang Yuan, He Zhang, and Yunquan Zhang", "title": "Enhanced AGCM3D: A Highly Scalable Dynamical Core of Atmospheric General\n  Circulation Model Based on Leap-Format", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finite-difference dynamical core based on the equal-interval\nlatitude-longitude mesh has been widely used for numerical simulations of the\nAtmospheric General Circulation Model (AGCM). Previous work utilizes different\nfiltering schemes to alleviate the instability problem incurred by the unequal\nphysical spacing at different latitudes, but they all incur high communication\nand computation overhead and become a scaling bottleneck. This paper proposes a\nhighly scalable 3d dynamical core based on a new leap-format finite-difference\ncomputing scheme. It generalizes the usual finite-difference format with\nadaptive wider intervals and is able to maintain the computational stability in\nthe grid updating. Therefore, the costly filtering scheme is eliminated. The\nnew scheme is parallelized with a shifting communication method and implemented\nwith fine communication optimizations based on a 3D decomposition. With the\nproposed leap-format computation scheme, the communication overhead of the AGCM\nis significantly reduced and good load balance is exhibited. The simulation\nresults verify the correctness of the new leap-format scheme. The new\nleap-format dynamical core scales up to 196,608 CPU cores and achieves the\nspeed of 7.4 simulation-year-per-day (SYPD) and 2.0x speedup on average over\nthe latest implementation at a high resolution of 25KM.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:41:15 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Cao", "Hang", ""], ["Yuan", "Liang", ""], ["Zhang", "He", ""], ["Zhang", "Yunquan", ""]]}, {"id": "2103.10116", "submitter": "Terry Cojean", "authors": "Yuhsiang M. Tsai and Terry Cojean and Hartwig Anzt", "title": "Porting a sparse linear algebra math library to Intel GPUs", "comments": "preprint, not submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the announcement that the Aurora Supercomputer will be composed of\ngeneral purpose Intel CPUs complemented by discrete high performance Intel\nGPUs, and the deployment of the oneAPI ecosystem, Intel has committed to enter\nthe arena of discrete high performance GPUs. A central requirement for the\nscientific computing community is the availability of production-ready software\nstacks and a glimpse of the performance they can expect to see on Intel high\nperformance GPUs. In this paper, we present the first platform-portable open\nsource math library supporting Intel GPUs via the DPC++ programming\nenvironment. We also benchmark some of the developed sparse linear algebra\nfunctionality on different Intel GPUs to assess the efficiency of the DPC++\nprogramming ecosystem to translate raw performance into application\nperformance. Aside from quantifying the efficiency within the hardware-specific\nroofline model, we also compare against routines providing the same\nfunctionality that ship with Intel's oneMKL vendor library.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:44:17 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Tsai", "Yuhsiang M.", ""], ["Cojean", "Terry", ""], ["Anzt", "Hartwig", ""]]}, {"id": "2103.10169", "submitter": "Asterios Katsifodimos", "authors": "Can Gencer, Marko Topolnik, Viliam \\v{D}urina, Emin Demirci, Ensar B.\n  Kahveci, Ali G\\\"urb\\\"uz Ond\\v{r}ej Luk\\'a\\v{s}, J\\'ozsef Bart\\'ok, Grzegorz\n  Gierlach, Franti\\v{s}ek Hartman, Ufuk Y{\\i}lmaz, Mehmet Do\\u{g}an, Mohamed\n  Mandouh, Marios Fragkoulis and Asterios Katsifodimos", "title": "Hazelcast Jet: Low-latency Stream Processing at the 99.99th Percentile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Jet is an open-source, high-performance, distributed stream processor built\nat Hazelcast during the last five years. Jet was engineered with millisecond\nlatency on the 99.99th percentile as its primary design goal. Originally Jet's\npurpose was to be an execution engine that performs complex business logic on\ntop of streams generated by Hazelcast's In-memory Data Grid (IMDG): a set of\nhigh-performance, in-memory, partitioned and replicated data structures. With\ntime, Jet evolved into a full-fledged, scale-out stream processor that can\nhandle out-of-order streams and exactly-once processing guarantees. Jet's\nend-to-end latency lies in the order of milliseconds, and its throughput in the\norder of millions of events per CPU-core. This paper presents main design\ndecisions we made in order to maximize the performance per CPU-core, alongside\nlessons learned, and an empirical performance evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:06:20 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Gencer", "Can", ""], ["Topolnik", "Marko", ""], ["\u010eurina", "Viliam", ""], ["Demirci", "Emin", ""], ["Kahveci", "Ensar B.", ""], ["Luk\u00e1\u0161", "Ali G\u00fcrb\u00fcz Ond\u0159ej", ""], ["Bart\u00f3k", "J\u00f3zsef", ""], ["Gierlach", "Grzegorz", ""], ["Hartman", "Franti\u0161ek", ""], ["Y\u0131lmaz", "Ufuk", ""], ["Do\u011fan", "Mehmet", ""], ["Mandouh", "Mohamed", ""], ["Fragkoulis", "Marios", ""], ["Katsifodimos", "Asterios", ""]]}, {"id": "2103.10280", "submitter": "Christoph Welzel", "authors": "Javier Esparza, Mikhail Raskin, Christoph Welzel", "title": "Computing Parameterized Invariants of Parameterized Petri Nets", "comments": "Extended version; accepted at Petri nets'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fundamental advantage of Petri net models is the possibility to\nautomatically compute useful system invariants from the syntax of the net.\nClassical techniques used for this are place invariants, P-components, siphons\nor traps. Recently, Bozga et al. have presented a novel technique for the\n\\emph{parameterized} verification of safety properties of systems with a ring\nor array architecture. They show that the statement \\enquote{for every instance\nof the parameterized Petri net, all markings satisfying the linear invariants\nassociated to all the P-components, siphons and traps of the instance are safe}\ncan be encoded in \\acs{WS1S} and checked using tools like MONA. However, while\nthe technique certifies that this infinite set of linear invariants extracted\nfrom P-components, siphons or traps are strong enough to prove safety, it does\nnot return an explanation of this fact understandable by humans. We present a\nCEGAR loop that constructs a \\emph{finite} set of \\emph{parameterized}\nP-components, siphons or traps, whose infinitely many instances are strong\nenough to prove safety. For this we design parameterization procedures for\ndifferent architectures.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:22:42 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 21:40:53 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Esparza", "Javier", ""], ["Raskin", "Mikhail", ""], ["Welzel", "Christoph", ""]]}, {"id": "2103.10346", "submitter": "Stefano Savazzi", "authors": "Stefano Savazzi, Sanaz Kianoush, Vittorio Rampa, Mehdi Bennis", "title": "A Framework for Energy and Carbon Footprint Analysis of Distributed and\n  Federated Edge Learning", "comments": "The work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in distributed learning raise environmental concerns due to\nthe large energy needed to train and move data to/from data centers. Novel\nparadigms, such as federated learning (FL), are suitable for decentralized\nmodel training across devices or silos that simultaneously act as both data\nproducers and learners. Unlike centralized learning (CL) techniques, relying on\nbig-data fusion and analytics located in energy hungry data centers, in FL\nscenarios devices collaboratively train their models without sharing their\nprivate data. This article breaks down and analyzes the main factors that\ninfluence the environmental footprint of FL policies compared with classical\nCL/Big-Data algorithms running in data centers. The proposed analytical\nframework takes into account both learning and communication energy costs, as\nwell as the carbon equivalent emissions; in addition, it models both vanilla\nand decentralized FL policies driven by consensus. The framework is evaluated\nin an industrial setting assuming a real-world robotized workplace. Results\nshow that FL allows remarkable end-to-end energy savings (30%-40%) for wireless\nsystems characterized by low bit/Joule efficiency (50 kbit/Joule or lower).\nConsensus-driven FL does not require the parameter server and further reduces\nemissions in mesh networks (200 kbit/Joule). On the other hand, all FL policies\nare slower to converge when local data are unevenly distributed (often 2x\nslower than CL). Energy footprint and learning loss can be traded off to\noptimize efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:04:42 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Savazzi", "Stefano", ""], ["Kianoush", "Sanaz", ""], ["Rampa", "Vittorio", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2103.10366", "submitter": "Felix Biermeier", "authors": "Gregor Bankhamer, Petra Berenbrink, Felix Biermeier, Robert\n  Els\\\"asser, Hamed Hosseinpour, Dominik Kaaser, Peter Kling", "title": "Fast Consensus via the Unconstrained Undecided State Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the plurality consensus problem among $n$ agents. Initially, each\nagent has one of $k$ different opinions. Agents choose random interaction\npartners and revise their state according to a fixed transition function,\ndepending on their own state and the state of the interaction partners. The\ngoal is to reach a consensus configuration in which all agents agree on the\nsame opinion, and if there is initially a sufficiently large bias towards one\nopinion, that opinion should prevail.\n  We analyze a synchronized variant of the undecided state dynamics defined as\nfollows. The agents act in phases, consisting of a decision and a boosting\npart. In the decision part, any agent that encounters an agent with a different\nopinion becomes undecided. In the boosting part, undecided agents adopt the\nfirst opinion they encounter.\n  We consider this dynamics in the population model and the gossip model. For\nthe population model, our protocol reaches consensus (w.h.p.) in $O(\\log^2 n)$\nparallel time, providing the first polylogarithmic result for $k > 2$ (w.h.p.)\nin this model. Without any assumption on the bias, fast consensus has only been\nshown for $k = 2$ for the unsynchronized version of the undecided state\ndynamics [Clementi et al., MFCS'18]. We show that the synchronized variant of\nthe undecided state dynamics reaches consensus (w.h.p.) in time $O(\\log^2 n)$,\nindependently of the initial number, bias, or distribution of opinions. In both\nmodels, we guarantee that if there is an initial bias of $\\Omega(\\sqrt{n \\log\nn})$, then (w.h.p.) that opinion wins.\n  A simple extension of our protocol in the gossip model yields a dynamics that\ndoes not depend on $n$ or $k$, is anonymous, and has (w.h.p.) runtime $O(\\log^2\nn)$. This solves an open problem formulated by Becchetti et al.~[Distributed\nComputing,~2017].\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:39:34 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Bankhamer", "Gregor", ""], ["Berenbrink", "Petra", ""], ["Biermeier", "Felix", ""], ["Els\u00e4sser", "Robert", ""], ["Hosseinpour", "Hamed", ""], ["Kaaser", "Dominik", ""], ["Kling", "Peter", ""]]}, {"id": "2103.10452", "submitter": "Eric Qin", "authors": "Eric Qin, Geonhwa Jeong, William Won, Sheng-Chun Kao, Hyoukjun Kwon,\n  Sudarshan Srinivasan, Dipankar Das, Gordon E. Moon, Sivasankaran\n  Rajamanickam, Tushar Krishna", "title": "Extending Sparse Tensor Accelerators to Support Multiple Compression\n  Formats", "comments": "Accepted for publication at the 35th IEEE International Parallel &\n  Distributed Processing Symposium (IPDPS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity, which occurs in both scientific applications and Deep Learning (DL)\nmodels, has been a key target of optimization within recent ASIC accelerators\ndue to the potential memory and compute savings. These applications use data\nstored in a variety of compression formats. We demonstrate that both the\ncompactness of different compression formats and the compute efficiency of the\nalgorithms enabled by them vary across tensor dimensions and amount of\nsparsity. Since DL and scientific workloads span across all sparsity regions,\nthere can be numerous format combinations for optimizing memory and compute\nefficiency. Unfortunately, many proposed accelerators operate on one or two\nfixed format combinations. This work proposes hardware extensions to\naccelerators for supporting numerous format combinations seamlessly and\ndemonstrates ~4X speedup over performing format conversions in software.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:08:56 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Qin", "Eric", ""], ["Jeong", "Geonhwa", ""], ["Won", "William", ""], ["Kao", "Sheng-Chun", ""], ["Kwon", "Hyoukjun", ""], ["Srinivasan", "Sudarshan", ""], ["Das", "Dipankar", ""], ["Moon", "Gordon E.", ""], ["Rajamanickam", "Sivasankaran", ""], ["Krishna", "Tushar", ""]]}, {"id": "2103.10481", "submitter": "Frank Lin", "authors": "Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam,\n  Christopher G. Brinton, Nicolo Michelusi", "title": "Two Timescale Hybrid Federated Learning with Cooperative D2D Local Model\n  Aggregations", "comments": "This paper is currently under review for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as a popular technique for distributing\nmachine learning (ML) model training across the wireless edge. In this paper,\nwe propose two timescale hybrid federated learning (TT-HF), which is a hybrid\nbetween the device-to-server communication paradigm in federated learning and\ndevice-to-device (D2D) communications for model training. In TT-HF, during each\nglobal aggregation interval, devices (i) perform multiple stochastic gradient\ndescent iterations on their individual datasets, and (ii) aperiodically engage\nin consensus formation of their model parameters through cooperative,\ndistributed D2D communications within local clusters. With a new general\ndefinition of gradient diversity, we formally study the convergence behavior of\nTT-HF, resulting in new convergence bounds for distributed ML. We leverage our\nconvergence bounds to develop an adaptive control algorithm that tunes the step\nsize, D2D communication rounds, and global aggregation period of TT-HF over\ntime to target a sublinear convergence rate of O(1/t) while minimizing network\nresource utilization. Our subsequent experiments demonstrate that TT-HF\nsignificantly outperforms the current art in federated learning in terms of\nmodel accuracy and/or network energy consumption in different scenarios where\nlocal device datasets exhibit statistical heterogeneity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:58:45 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Lin", "Frank Po-Chen", ""], ["Hosseinalipour", "Seyyedali", ""], ["Azam", "Sheikh Shams", ""], ["Brinton", "Christopher G.", ""], ["Michelusi", "Nicolo", ""]]}, {"id": "2103.10500", "submitter": "Zaid Hussain", "authors": "Mohammad Awadh and Zaid Hussain", "title": "Panconnectivity Algorithm for Eisenstein-Jacobi Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eisenstein-Jacobi (EJ) networks were proposed as an efficient interconnection\nnetwork topology for parallel and distributed systems. They are based on\nEisenstein-Jacobi integers modulo $a = a+b\\rho$, where $0 \\leq a \\leq b$, and\nthey are 6-regular symmetric networks and considered as a generalization of\nhexagonal networks. Most of the interconnection networks are modeled as graphs\nwhere the applications and functions of graph theory could be applied to. The\ncycles in a graph are one type of communications in interconnection networks\nthat are considered as a factor to measure the efficiency and reliability of\nthe networks' topology. The network is said to be panconnected if there are\ncycles of length $l$ for all $l = D(u, v), D(u, v)+1, D(u, v)+2, \\dots, n-1$\nwhere $D(u, v)$ is the shortest distance between nodes $u$ and $v$ in a given\nnetwork. In this paper, we investigate the panconnectivity problem in\nEisenstein-Jacobi networks. The proposed algorithm constructs and proves the\npanconnectivity of a given Eisenstein-Jacobi network and its complexity is\n$O(n^4)$. Simulation results are given to support the correctness of this work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 19:56:16 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Awadh", "Mohammad", ""], ["Hussain", "Zaid", ""]]}, {"id": "2103.10573", "submitter": "Ramon Nepomuceno", "authors": "R. Nepomuceno, R. Sterle, G. Valarini, M. Pereira, H. Yviquel, G.\n  Araujo", "title": "Enabling OpenMP Task Parallelism on Multi-FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  FPGA-based hardware accelerators have received increasing attention mainly\ndue to their ability to accelerate deep pipelined applications, thus resulting\nin higher computational performance and energy efficiency. Nevertheless, the\namount of resources available on even the most powerful FPGA is still not\nenough to speed up very large modern workloads. To achieve that, FPGAs need to\nbe interconnected in a Multi-FPGA architecture capable of accelerating a single\napplication. However, programming such architecture is a challenging endeavor\nthat still requires additional research. This paper extends the OpenMP\ntask-based computation offloading model to enable a number of FPGAs to work\ntogether as a single Multi-FPGA architecture. Experimental results for a set of\nOpenMP stencil applications running on a Multi-FPGA platform consisting of 6\nXilinx VC709 boards interconnected through fiber-optic links have shown close\nto linear speedups as the number of FPGAs and IP-cores per FPGA increase.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 00:10:38 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 02:20:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Nepomuceno", "R.", ""], ["Sterle", "R.", ""], ["Valarini", "G.", ""], ["Pereira", "M.", ""], ["Yviquel", "H.", ""], ["Araujo", "G.", ""]]}, {"id": "2103.10635", "submitter": "Atanu Barai", "authors": "Atanu Barai, Gopinath Chennupati, Nandakishore Santhi, Abdel-Hameed\n  Badawy, Yehia Arafa, Stephan Eidenbenz", "title": "PPT-SASMM: Scalable Analytical Shared Memory Model: Predicting the\n  Performance of Multicore Caches from a Single-Threaded Execution Trace", "comments": "11 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1907.12666", "journal-ref": null, "doi": "10.1145/3422575.3422806", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Performance modeling of parallel applications on multicore processors remains\na challenge in computational co-design due to multicore processors' complex\ndesign. Multicores include complex private and shared memory hierarchies. We\npresent a Scalable Analytical Shared Memory Model (SASMM). SASMM can predict\nthe performance of parallel applications running on a multicore. SASMM uses a\nprobabilistic and computationally-efficient method to predict the reuse\ndistance profiles of caches in multicores. SASMM relies on a stochastic, static\nbasic block-level analysis of reuse profiles. The profiles are calculated from\nthe memory traces of applications that run sequentially rather than using\nmulti-threaded traces. The experiments show that our model can predict private\nL1 cache hit rates with 2.12% and shared L2 cache hit rates with about 1.50%\nerror rate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 05:10:39 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Barai", "Atanu", ""], ["Chennupati", "Gopinath", ""], ["Santhi", "Nandakishore", ""], ["Badawy", "Abdel-Hameed", ""], ["Arafa", "Yehia", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "2103.10778", "submitter": "Md Rubel Ahmed", "authors": "Md Rubel Ahmed, Hao Zheng", "title": "Tools and Algorithms for SoC Communication Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study seven well-known trace analysis techniques both from\nthe hardware and software domain and discuss their performance on\ncommunication-centric system-on-chip (SoC) traces. SoC traces are usually huge\nin size and concurrent in nature, therefore mining SoC traces poses additional\nchallenges. We provide a hands-on discussion of the selected tools/algorithms\nin terms of the input, output, and analysis methods they employ. Hardware\ntraces also varies in nature when observed in different level, this work can\nhelp developers/academicians to pick up the right techniques for their work. We\ntake advantage of a synthetic trace generator to find the interestingness of\nthe mined outcomes for each tool as well as we work with a realistic GEM5 set\nup to find the performance of these tools on more realistic SoC traces.\nComprehensive analysis of the tool's performance and a benchmark trace dataset\nare also presented.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 04:05:06 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ahmed", "Md Rubel", ""], ["Zheng", "Hao", ""]]}, {"id": "2103.10779", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Aravinda Prasad, Smruti R. Sarangi, Sreenivas\n  Subramoney", "title": "Page Table Management for Heterogeneous Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern enterprise servers are increasingly embracing tiered memory systems\nwith a combination of low latency DRAMs and large capacity but high latency\nnon-volatile main memories (NVMMs) such as Intel's Optane DC PMM. Prior works\nhave focused on efficient placement and migration of data on a tiered memory\nsystem, but have not studied the optimal placement of page tables.\n  Explicit and efficient placement of page tables is crucial for large memory\nfootprint applications with high TLB miss rates because they incur dramatically\nhigher page walk latency when page table pages are placed in NVMM. We show that\n(i) page table pages can end up on NVMM even when enough DRAM memory is\navailable and (ii) page table pages that spill over to NVMM due to DRAM memory\npressure are not migrated back later when memory is available in DRAM.\n  We study the performance impact of page table placement in a tiered memory\nsystem and propose an efficient and transparent page table management technique\nthat (i) applies different placement policies for data and page table pages,\n(ii) introduces a differentiating policy for page table pages by placing a\nsmall but critical part of the page table in DRAM, and (iii) dynamically and\njudiciously manages the rest of the page table by transparently migrating the\npage table pages between DRAM and NVMM. Our implementation on a real system\nequipped with Intel's Optane NVMM running Linux reduces the page table walk\ncycles by 12% and total cycles by 20% on an average. This improves the runtime\nby 20% on an average for a set of synthetic and real-world large memory\nfootprint applications when compared with various default Linux kernel\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:46:59 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Kumar", "Sandeep", ""], ["Prasad", "Aravinda", ""], ["Sarangi", "Smruti R.", ""], ["Subramoney", "Sreenivas", ""]]}, {"id": "2103.10816", "submitter": "Emmanuel Godard", "authors": "Emmanuel Godard and Eloi Perdereau", "title": "Back to the Coordinated Attack Problem", "comments": "Mathematical Structures in Computer Science, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider the well known Coordinated Attack Problem, where two generals\nhave to decide on a common attack, when their messengers can be captured by the\nenemy. Informally, this problem represents the difficulties to agree in the\npresence of communication faults. We consider here only omission faults (loss\nof message), but contrary to previous studies, we do not to restrict the way\nmessages can be lost, i.e. we make no specific assumption, we use no specific\nfailure metric. In the large subclass of message adversaries where the double\nsimultaneous omission can never happen, we characterize which ones are\nobstructions for the Coordinated Attack Problem. We give two proofs of this\nresult. One is combinatorial and uses the classical bivalency technique for the\nnecessary condition. The second is topological and uses simplicial complexes to\nprove the necessary condition. We also present two different Consensus\nalgorithms that are combinatorial (resp. topological) in essence. Finally, we\nanalyze the two proofs and illustrate the relationship between the\ncombinatorial approach and the topological approach in the very general case of\nmessage adversaries. We show that the topological characterization gives a\nclearer explanation of why some message adversaries are obstructions or not.\nThis result is a convincing illustration of the power of topological tools for\ndistributed computability.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 14:02:16 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Godard", "Emmanuel", ""], ["Perdereau", "Eloi", ""]]}, {"id": "2103.10891", "submitter": "Shabnam Daghaghi", "authors": "Shabnam Daghaghi, Nicholas Meisburger, Mengnan Zhao, Yong Wu, Sameh\n  Gobriel, Charlie Tai, Anshumali Shrivastava", "title": "Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization,\n  Quantizations, Memory Optimizations, and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning implementations on CPUs (Central Processing Units) are gaining\nmore traction. Enhanced AI capabilities on commodity x86 architectures are\ncommercially appealing due to the reuse of existing hardware and virtualization\nease. A notable work in this direction is the SLIDE system. SLIDE is a C++\nimplementation of a sparse hash table based back-propagation, which was shown\nto be significantly faster than GPUs in training hundreds of million parameter\nneural models. In this paper, we argue that SLIDE's current implementation is\nsub-optimal and does not exploit several opportunities available in modern\nCPUs. In particular, we show how SLIDE's computations allow for a unique\npossibility of vectorization via AVX (Advanced Vector Extensions)-512.\nFurthermore, we highlight opportunities for different kinds of memory\noptimization and quantizations. Combining all of them, we obtain up to 7x\nspeedup in the computations on the same hardware. Our experiments are focused\non large (hundreds of millions of parameters) recommendation and NLP models.\nOur work highlights several novel perspectives and opportunities for\nimplementing randomized algorithms for deep learning on modern CPUs. We provide\nthe code and benchmark scripts at https://github.com/RUSH-LAB/SLIDE\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 02:13:43 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Daghaghi", "Shabnam", ""], ["Meisburger", "Nicholas", ""], ["Zhao", "Mengnan", ""], ["Wu", "Yong", ""], ["Gobriel", "Sameh", ""], ["Tai", "Charlie", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2103.10911", "submitter": "Kaoutar El Maghraoui", "authors": "Kauotar El Maghraoui and Lorraine M. Herger and Chekuri Choudary and\n  Kim Tran and Todd Deshane and David Hanson", "title": "Performance Analysis of Deep Learning Workloads on a Composable System", "comments": "Submitted to IPDPS ScaDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A composable infrastructure is defined as resources, such as compute,\nstorage, accelerators and networking, that are shared in a pool and that can be\ngrouped in various configurations to meet application requirements. This\nfreedom to 'mix and match' resources dynamically allows for experimentation\nearly in the design cycle, prior to the final architectural design or hardware\nimplementation of a system. This design provides flexibility to serve a variety\nof workloads and provides a dynamic co-design platform that allows experiments\nand measurements in a controlled manner. For instance, key performance\nbottlenecks can be revealed early on in the experimentation phase thus avoiding\ncostly and time consuming mistakes. Additionally, various system-level\ntopologies can be evaluated when experimenting with new System on Chip (SoCs)\nand new accelerator types. This paper details the design of an enterprise\ncomposable infrastructure that we have implemented and made available to our\npartners in the IBM Research AI Hardware Center (AIHC). Our experimental\nevaluations on the composable system give insights into how the system works\nand evaluates the impact of various resource aggregations and reconfigurations\non representative deep learning benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:15:42 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Maghraoui", "Kauotar El", ""], ["Herger", "Lorraine M.", ""], ["Choudary", "Chekuri", ""], ["Tran", "Kim", ""], ["Deshane", "Todd", ""], ["Hanson", "David", ""]]}, {"id": "2103.11187", "submitter": "Enis Karaarslan Dr.", "authors": "Ercan I\\c{s}{\\i}k, Melih Birim, Enis Karaarslan", "title": "Tubu-io Decentralized Application Development & Test Workbench", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized services are increasingly being developed and their proper\nusage in different areas is being experimented with. Autonomous codes, which\nare also called smart contracts, can be developed with Integrated Development\nEnvironments (IDE). However, these tools lack live environment tests. The\nunderlying blockchain technologies are also evolving and it is not easy to\ncatch all the developments. There is a need for an easy-to-use interface by\nwhich the developers can see the results of their codes. Tubu-io decentralized\napplication development workbench is developed to serve as an efficient way for\nthe programmers to deploy smart contracts on the blockchain networks and\ninteract with them easily. It can also be used for teaching decentralized\napplication programming for junior blockchain developers on blockchain\ntestbeds. Finally, it will have an effect in decreasing the development time\nand the costs of developing decentralized application projects.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 14:20:53 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["I\u015f\u0131k", "Ercan", ""], ["Birim", "Melih", ""], ["Karaarslan", "Enis", ""]]}, {"id": "2103.11445", "submitter": "Buse Yilmaz", "authors": "Buse Yilmaz (Istinye University)", "title": "Graph Transformation and Specialized Code Generation For Sparse\n  Triangular Solve (SpTRSV)", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sparse Triangular Solve (SpTRSV) is an important computational kernel used in\nthe solution of sparse linear algebra systems in many scientific and\nengineering applications. It is diffcult to parallelize SpTRSV in today's\narchitectures. The limited parallelism due to the dependencies between\ncalculations and the irregular nature of the computations require an effective\nload balancing and synchronization mechanism approach. In this work, we present\na novel graph transformation method where the equation representing a row is\nrewritten to break the dependencies. Using this approach, we propose a\ndependency graph transformation and code generation framework that increases\nthe parallelism of the parts of a sparse matrix where it is scarce, reducing\nthe need for synchronization points. In addition, the proposed framework\ngenerates specialized code for the transformed dependency graph on CPUs using\ndomain-specific optimizations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 17:27:07 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yilmaz", "Buse", "", "Istinye University"]]}, {"id": "2103.11619", "submitter": "George Pu", "authors": "George Pu, Yanlin Zhou, Dapeng Wu, Xiaolin Li", "title": "Server Averaging for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning allows distributed devices to collectively train a model\nwithout sharing or disclosing the local dataset with a central server. The\nglobal model is optimized by training and averaging the model parameters of all\nlocal participants. However, the improved privacy of federated learning also\nintroduces challenges including higher computation and communication costs. In\nparticular, federated learning converges slower than centralized training. We\npropose the server averaging algorithm to accelerate convergence. Sever\naveraging constructs the shared global model by periodically averaging a set of\nprevious global models. Our experiments indicate that server averaging not only\nconverges faster, to a target accuracy, than federated averaging (FedAvg), but\nalso reduces the computation costs on the client-level through epoch decay.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:07:00 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pu", "George", ""], ["Zhou", "Yanlin", ""], ["Wu", "Dapeng", ""], ["Li", "Xiaolin", ""]]}, {"id": "2103.11765", "submitter": "Andrea Merlina", "authors": "Andrea Merlina, Roman Vitenberg, Vinay Setty", "title": "A General and Configurable Framework for Blockchain-based Marketplaces", "comments": "27 pages, 2 figures, 7 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The first generation of blockchain focused on digital currencies and secure\nstorage, management and transfer of tokenized values. Thereafter, the focus has\nbeen shifting from currencies to a broader application space. In this paper, we\nsystematically explore marketplace types and properties, and consider the\nmechanisms required to support those properties through blockchain. We propose\na generic and configurable framework for blockchain-based marketplaces, and\ndescribe how popular marketplace types, price discovery policies, and other\nconfiguration parameters are implemented within the framework by presenting\nconcrete event-based algorithms. Finally, we consider three use cases with\nwidely diverging properties and show how the proposed framework supports them.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:28:53 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Merlina", "Andrea", ""], ["Vitenberg", "Roman", ""], ["Setty", "Vinay", ""]]}, {"id": "2103.11879", "submitter": "Hongyi Zhang", "authors": "Hongyi Zhang, Jan Bosch, Helena Holmstr\\\"om Olsson", "title": "Real-time End-to-End Federated Learning: An Automotive Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development and the increasing interests in ML/DL fields, companies\nare eager to utilize these methods to improve their service quality and user\nexperience. Federated Learning has been introduced as an efficient model\ntraining approach to distribute and speed up time-consuming model training and\npreserve user data privacy. However, common Federated Learning methods apply a\nsynchronized protocol to perform model aggregation, which turns out to be\ninflexible and unable to adapt to rapidly evolving environments and\nheterogeneous hardware settings in real-world systems. In this paper, we\nintroduce an approach to real-time end-to-end Federated Learning combined with\na novel asynchronous model aggregation protocol. We validate our approach in an\nindustrial use case in the automotive domain focusing on steering wheel angle\nprediction for autonomous driving. Our results show that asynchronous Federated\nLearning can significantly improve the prediction performance of local edge\nmodels and reach the same accuracy level as the centralized machine learning\nmethod. Moreover, the approach can reduce the communication overhead,\naccelerate model training speed and consume real-time streaming data by\nutilizing a sliding training window, which proves high efficiency when\ndeploying ML/DL components to heterogeneous real-world embedded systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:16:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Hongyi", ""], ["Bosch", "Jan", ""], ["Olsson", "Helena Holmstr\u00f6m", ""]]}, {"id": "2103.11926", "submitter": "Shucheng Chi", "authors": "David Y. C. Chan, Shucheng Chi, Vassos Hadzilacos, Sam Toueg", "title": "Differentiated nonblocking: a new progress condition and a matching\n  queue algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first propose a new liveness requirement for shared objects\nand data structures, we then give a shared queue algorithm that satisfies this\nrequirement and we prove its correctness. We also implement this algorithm and\ncompare it to a well-known shared queue algorithm that is used in practice. In\naddition to having a stronger worst-case progress guarantee, our experimental\nresults suggest that, at the cost of a marginal decrease in throughput, our\nalgorithm is significantly fairer, by a natural definition of fairness that we\nintroduce here.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:10:39 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chan", "David Y. C.", ""], ["Chi", "Shucheng", ""], ["Hadzilacos", "Vassos", ""], ["Toueg", "Sam", ""]]}, {"id": "2103.12010", "submitter": "Samuel Yen-Chi Chen", "authors": "Samuel Yen-Chi Chen, Shinjae Yoo", "title": "Federated Quantum Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training across several quantum computers could significantly\nimprove the training time and if we could share the learned model, not the\ndata, it could potentially improve the data privacy as the training would\nhappen where the data is located. However, to the best of our knowledge, no\nwork has been done in quantum machine learning (QML) in federation setting yet.\nIn this work, we present the federated training on hybrid quantum-classical\nmachine learning models although our framework could be generalized to pure\nquantum machine learning model. Specifically, we consider the quantum neural\nnetwork (QNN) coupled with classical pre-trained convolutional model. Our\ndistributed federated learning scheme demonstrated almost the same level of\ntrained model accuracies and yet significantly faster distributed training. It\ndemonstrates a promising future research direction for scaling and privacy\naspects.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:00:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Samuel Yen-Chi", ""], ["Yoo", "Shinjae", ""]]}, {"id": "2103.12012", "submitter": "Sangeeta Yadav", "authors": "Sangeeta Yadav, Asif Khan", "title": "SP Async:Single Source Shortest Path in Asynchronous Mode on MPI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding single source shortest path is a very ubiquitous problem. But with\nthe increasing size of large datasets in important application like social\nnetwork data-mining, network topology determination-efficient parallelization\nof these techniques is needed to match the need of really large graphs. We\npresent a new Inter node-bellman cum Intra node Dijkstra technique implemented\nin MPI to solve SSSP problem. We have used a triangle based edge pruning for\nidle processes, and two different techniques for termination detection. Within\neach node the algorithm works as Dijkstra and for outer communication it\nbehaves as inter node bellman ford. First termination detection technique is\nbased on the token ring and counter. Second is a heuristic based technique, in\nwhich the timeout is calculated from the number of inter-edges and number of\npartitions. In this project asynchronous mode of message passing is used.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:02:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yadav", "Sangeeta", ""], ["Khan", "Asif", ""]]}, {"id": "2103.12031", "submitter": "Jonathan Kerridge", "authors": "Jon Kerridge and Neil Urquhart", "title": "Groovy Parallel Patterns: A Process oriented Parallelization Library", "comments": "34 pages, 14 Figures, 10 Tables, 21 Code Listings, 52 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A novel parallel patterns library, Groovy Parallel Patterns, is presented\nwhich, from the outset, has been designed to exploit more general process\nparallelism than the usual data and task parallel architectures. The library\nexecutes on a standard Java Virtual Machine. The library provides a collection\nof processes that can be plugged together to form a variety of parallel\narchitectures and is intrinsically its own DSL. A network of processes is\nguaranteed to be deadlock and livelock free and terminate correctly and this is\nproved by the use of formal methods. Error capture and a basic logging\nmechanism have been incorporated. The library enables effective refinement of\nsolutions between process networks which can be checked also using formal\nmethods. A library user is only required to create the required methods as\npieces of sequential code, typically taken from extant sequential solutions,\nwhich can then be invoked by the processes as required. The utility of the\nlibrary is demonstrated by several examples including; Monte Carlo Methods,\nConcordance, Jacobi solutions, N-body problems and Mandelbrot, which is\nimplemented on both a multicore processor and a workstation cluster. The\nexamples are analysed for speedup and efficiency, which show good and\nconsistent performance improvement up to the number of available processor\ncores and workstations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:33:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kerridge", "Jon", ""], ["Urquhart", "Neil", ""]]}, {"id": "2103.12071", "submitter": "Fatima Ezzahraa Ben Bouazza", "authors": "Fatima Ezzahraa Ben Bouazza, Youn\\`es Bennani", "title": "Selective information exchange in collaborative clustering using\n  regularized Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative learning has recently achieved very significant results. It\nstill suffers, however, from several issues, including the type of information\nthat needs to be exchanged, the criteria for stopping and how to choose the\nright collaborators. We aim in this paper to improve the quality of the\ncollaboration and to resolve these issues via a novel approach inspired by\nOptimal Transport theory. More specifically, the objective function for the\nexchange of information is based on the Wasserstein distance, with a\nbidirectional transport of information between collaborators. This formulation\nallows to learns a stopping criterion and provide a criterion to choose the\nbest collaborators. Extensive experiments are conducted on multiple data-sets\nto evaluate the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:28:50 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 20:04:11 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bouazza", "Fatima Ezzahraa Ben", ""], ["Bennani", "Youn\u00e8s", ""]]}, {"id": "2103.12112", "submitter": "Ray Neiheiser", "authors": "Ray Neiheiser, Miguel Matos, Lu\\'is Rodrigues", "title": "The quest for scaling BFT Consensus through Tree-Based Vote Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing commercial interest in blockchain, permissioned\nimplementations have received increasing attention. Unfortunately, existing BFT\nconsensus protocols that are the backbone of permissioned blockchains, either\nscale poorly or offer limited throughput. Most of these algorithms require at\nleast one process to receive and validate the votes from all other processes\nand then broadcast the result, which is inherently non-scalable. Some\nalgorithms avoid this bottleneck by using aggregation trees to collect and\nvalidate votes. However, to the best of our knowledge, such algorithms offer\nlimited throughput and degrade quickly in the presence of faults. In this paper\nwe propose \\thesystem, the first BFT communication abstraction that organizes\nparticipants in a tree to perform scalable vote aggregation and that, in faulty\nruns, is able to terminate the protocol within an optimal number of\nreconfigurations ($f+1$). We define precisely which aggregation trees allow for\noptimal reconfiguration and show that, unlike previous protocols, when using\nthese configurations, \\thesystem scales to large number of processes and\noutperforms HotStuff's throughput by up to 38x.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:15:04 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Neiheiser", "Ray", ""], ["Matos", "Miguel", ""], ["Rodrigues", "Lu\u00eds", ""]]}, {"id": "2103.12116", "submitter": "Diego Davila", "authors": "Edgar Fajardo, Aashay Arora, Diego Davila, Richard Gao, Frank\n  W\\\"urthwein, Brian Bockelman", "title": "Systematic benchmarking of HTTPS third party copy on 100Gbps links using\n  XRootD", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The High Luminosity Large Hadron Collider provides a data challenge. The\namount of data recorded from the experiments and transported to hundreds of\nsites will see a thirty fold increase in annual data volume. A systematic\napproach to contrast the performance of different Third Party Copy(TPC)\ntransfer protocols arises. Two contenders, XRootD-HTTPS and the GridFTP are\nevaluated in their performance for transferring files from one server to\nan-other over 100Gbps interfaces. The benchmarking is done by scheduling pods\non the Pacific Research Platform Kubernetes cluster to ensure reproducible and\nrepeatable results. This opens a future pathway for network testing of any TPC\ntransfer protocol.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:19:47 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Fajardo", "Edgar", ""], ["Arora", "Aashay", ""], ["Davila", "Diego", ""], ["Gao", "Richard", ""], ["W\u00fcrthwein", "Frank", ""], ["Bockelman", "Brian", ""]]}, {"id": "2103.12131", "submitter": "James Kempf", "authors": "Oleg Berzin, Rafael Ansay, James Kempf, Imam Sheikh, and Doron Hendel", "title": "The IoT Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IoT ecosystem suffers from a variety of problems around security,\nidentity, access control, data flow and data storage that introduce friction\ninto interactions between various parties. In many respects, the situation is\nsimilar to the early days of the Internet, where, prior to the establishment of\nInternet Exchanges, routing between different BGP autonomous systems was often\npoint to point. We propose a similar solution, the IoT Exchange, where IoT\ndevice owners can register their devices and offer data for sale or can upload\ndata into the IoT services of any of the big hyperscale cloud platforms for\nfurther processing. The goal of the IoT Exchange is to break down the silos\nwithin which device wireless connectivity types and cloud provider IoT systems\nconstrain users to operate. In addition, if the device owner needs to maintain\nthe data close to the edge to reduce access latency, the MillenniumDB service\nrunning in an edge data center with minimal latency to the edge device,\nprovides a database with a variety of schema engines (SQL, noSQL, etc). The IoT\nexchange uses decentralized identifiers for identity management and verifiable\ncredentials for authorizing software updates and to control access to the\ndevices, to avoid dependence on certificate authorities and other centralized\nidentity and authorization management systems. In addition, verifiable\ncredentials provide a way whereby privacy preserving processing can be applied\nto traffic between a device and an end data or control customer, if some risk\nof privacy compromise exists.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:51:47 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Berzin", "Oleg", ""], ["Ansay", "Rafael", ""], ["Kempf", "James", ""], ["Sheikh", "Imam", ""], ["Hendel", "Doron", ""]]}, {"id": "2103.12144", "submitter": "Vahideh Hayyolalam", "authors": "Vahideh Hayyolalam, Moayad Aloqaily, Oznur Ozkasap, Mohsen Guizani", "title": "Edge Intelligence for Empowering IoT-based Healthcare Systems", "comments": "This paper has been accepted in IEEE Wireless Communication Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for real-time, affordable, and efficient smart healthcare services\nis increasing exponentially due to the technological revolution and burst of\npopulation. To meet the increasing demands on this critical infrastructure,\nthere is a need for intelligent methods to cope with the existing obstacles in\nthis area. In this regard, edge computing technology can reduce latency and\nenergy consumption by moving processes closer to the data sources in comparison\nto the traditional centralized cloud and IoT-based healthcare systems. In\naddition, by bringing automated insights into the smart healthcare systems,\nartificial intelligence (AI) provides the possibility of detecting and\npredicting high-risk diseases in advance, decreasing medical costs for\npatients, and offering efficient treatments. The objective of this article is\nto highlight the benefits of the adoption of edge intelligent technology, along\nwith AI in smart healthcare systems. Moreover, a novel smart healthcare model\nis proposed to boost the utilization of AI and edge technology in smart\nhealthcare systems. Additionally, the paper discusses issues and research\ndirections arising when integrating these different technologies together.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 19:35:06 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Hayyolalam", "Vahideh", ""], ["Aloqaily", "Moayad", ""], ["Ozkasap", "Oznur", ""], ["Guizani", "Mohsen", ""]]}, {"id": "2103.12594", "submitter": "Ruben Mayer", "authors": "Ruben Mayer and Hans-Arno Jacobsen", "title": "Hybrid Edge Partitioner: Partitioning Large Power-Law Graphs under\n  Memory Constraints", "comments": "SIGMOD 2021, 14 pages", "journal-ref": null, "doi": "10.1145/3448016.3457300", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems that manage and process graph-structured data internally\nsolve a graph partitioning problem to minimize their communication overhead and\nquery run-time. Besides computational complexity -- optimal graph partitioning\nis NP-hard -- another important consideration is the memory overhead.\nReal-world graphs often have an immense size, such that loading the complete\ngraph into memory for partitioning is not economical or feasible. Currently,\nthe common approach to reduce memory overhead is to rely on streaming\npartitioning algorithms. While the latest streaming algorithms lead to\nreasonable partitioning quality on some graphs, they are still not completely\ncompetitive to in-memory partitioners. In this paper, we propose a new system,\nHybrid Edge Partitioner (HEP), that can partition graphs that fit partly into\nmemory while yielding a high partitioning quality. HEP can flexibly adapt its\nmemory overhead by separating the edge set of the graph into two sub-sets. One\nsub-set is partitioned by NE++, a novel, efficient in-memory algorithm, while\nthe other sub-set is partitioned by a streaming approach. Our evaluations on\nlarge real-world graphs show that in many cases, HEP outperforms both in-memory\npartitioning and streaming partitioning at the same time. Hence, HEP is an\nattractive alternative to existing solutions that cannot fine-tune their memory\noverheads. Finally, we show that using HEP, we achieve a significant speedup of\ndistributed graph processing jobs on Spark/GraphX compared to state-of-the-art\npartitioning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:47:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Mayer", "Ruben", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "2103.12837", "submitter": "Ferhat Khendek Prof.", "authors": "Mina Nabi, Ferhat Khendek, Maria Toeroe", "title": "An Approach for the Automation of IaaS Cloud Upgrade", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Infrastructure as a Service (IaaS) cloud provider is committed to each\ntenant by a service level agreement (SLA) which indicates the terms of\ncommitment, e.g. the level of availability of the IaaS cloud service.The\ndifferent resources providing this IaaS cloud service may need to be upgraded\nseveral times throughout their life-cycle; and these upgrades may affect the\nservice delivered by the IaaS layer. This may violate the SLAs towards the\ntenants and result in penalty as they impact the tenant services relying on the\nIaaS.Therefore, it is important to handle upgrades properly with respect to the\nSLAs.The upgrade of IaaS cloud systems inherits all the challenges of clustered\nsystems and faces other, cloud specific challenges, such as size and dynamicity\ndue to elasticity.In this paper, we propose a novel approach to automatically\nupgrade an IaaS cloud system under SLA constraints such as availability and\nelasticity.In this approach, the upgrade methods and actions appropriate for\neach upgrade request are identified, scheduled, and applied automatically in an\niterative manner based on the vendors descriptions of the infrastructure\ncomponents, the tenant SLAs, and the status of the system. The proposed\napproach allows new upgrade requests during ongoing upgrades, which makes it\nsuitable for continuous delivery.In addition, it also handles failures of\nupgrade actions through localized retry and undo operations automatically.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:58:17 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nabi", "Mina", ""], ["Khendek", "Ferhat", ""], ["Toeroe", "Maria", ""]]}, {"id": "2103.13130", "submitter": "Eun-Sung Jung", "authors": "Sam Nickolay, Eun-Sung Jung, Rajkumar Kettimuthu, Ian Foster", "title": "Towards Accommodating Real-time Jobs on HPC Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Increasing data volumes in scientific experiments necessitate the use of\nhigh-performance computing (HPC) resources for data analysis. In many\nscientific fields, the data generated from scientific instruments and\nsupercomputer simulations must be analyzed rapidly. In fact, the requirement\nfor quasi-instant feedback is growing. Scientists want to use results from one\nexperiment to guide the selection of the next or even to improve the course of\na single experiment. Current HPC systems are typically batch-scheduled under\npolicies in which an arriving job is run immediately only if enough resources\nare available; otherwise, it is queued. It is hard for these systems to support\nreal-time jobs. Real-time jobs, in order to meet their requirements, should\nsometimes have to preempt batch jobs and/or be scheduled ahead of batch jobs\nthat were submitted earlier. Accommodating real-time jobs may negatively impact\nsystem utilization also, especially when preemption/restart of batch jobs is\ninvolved. We first explore several existing scheduling strategies to make\nreal-time jobs more likely to be scheduled in due time. We then rigorously\nformulate the problem as a mixed-integer linear programming for offline\nscheduling and develop novel scheduling heuristics for online scheduling. We\nperform simulation studies using trace logs of Mira, the IBM BG/Q system at\nArgonne National Laboratory, to quantify the impact of real-time jobs on batch\njob performance for various percentages of real-time jobs in the workload. We\npresent new insights gained from grouping jobs into different categories based\non runtime and the number of nodes used and studying the performance of each\ncategory. Our results show that with 10% real-time job percentages,\njust-in-time checkpointing combined with our heuristic can improve the\nslowdowns of real-time jobs by 35% while limiting the increase of the slowdowns\nof batch jobs to 10%.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:13:04 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nickolay", "Sam", ""], ["Jung", "Eun-Sung", ""], ["Kettimuthu", "Rajkumar", ""], ["Foster", "Ian", ""]]}, {"id": "2103.13138", "submitter": "Thanasis Vergoulis", "authors": "Thanasis Vergoulis, Konstantinos Zagganas, Loukas Kavouras, Martin\n  Reczko, Stelios Sartzetakis, Theodore Dalamagas", "title": "SCHeMa: Scheduling Scientific Containers on a Cluster of Heterogeneous\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the era of data-driven science, conducting computational experiments that\ninvolve analysing large datasets using heterogeneous computational clusters, is\npart of the everyday routine for many scientists. Moreover, to ensure the\ncredibility of their results, it is very important for these analyses to be\neasily reproducible by other researchers. Although various technologies, that\ncould facilitate the work of scientists in this direction, have been introduced\nin the recent years, there is still a lack of open source platforms that\ncombine them to this end. In this work, we describe and demonstrate SCHeMa, an\nopen-source platform that facilitates the execution and reproducibility of\ncomputational analysis on heterogeneous clusters, leveraging containerization,\nexperiment packaging, workflow management, and machine learning technologies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:28:57 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Vergoulis", "Thanasis", ""], ["Zagganas", "Konstantinos", ""], ["Kavouras", "Loukas", ""], ["Reczko", "Martin", ""], ["Sartzetakis", "Stelios", ""], ["Dalamagas", "Theodore", ""]]}, {"id": "2103.13226", "submitter": "Yongli Mou", "authors": "Yongli Mou, Sascha Welten, Yeliz Ucer Yediel, Toralf Kirsten, Oya\n  Deniz Beyan", "title": "Distributed Learning for Melanoma Classification using Personal Health\n  Train", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skin cancer is the most common cancer type. Usually, patients with suspicion\nof cancer are treated by doctors without any aided visual inspection. At this\npoint, dermoscopy has become a suitable tool to support physicians in their\ndecision-making. However, clinicians need years of expertise to classify\npossibly malicious skin lesions correctly. Therefore, research has applied\nimage processing and analysis tools to improve the treatment process. In order\nto perform image analysis and train a model on dermoscopic images data needs to\nbe centralized. Nevertheless, data centralization does not often comply with\nlocal data protection regulations due to its sensitive nature and due to the\nloss of sovereignty if data providers allow unlimited access to the data. A\nmethod to circumvent all privacy-related challenges of data centralization is\nDistributed Analytics (DA) approaches, which bring the analysis to the data\ninstead of vice versa. This paradigm shift enables data analyses - in our case,\nimage analysis - with data remaining inside institutional borders, i.e., the\norigin. In this documentation, we describe a straightforward use case including\na model training for skin lesion classification based on decentralised data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:37:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Mou", "Yongli", ""], ["Welten", "Sascha", ""], ["Yediel", "Yeliz Ucer", ""], ["Kirsten", "Toralf", ""], ["Beyan", "Oya Deniz", ""]]}, {"id": "2103.13262", "submitter": "Jiaao He", "authors": "Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, Jie Tang", "title": "FastMoE: A Fast Mixture-of-Expert Training System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of\nlanguage model to trillions of parameters. However, training trillion-scale MoE\nrequires algorithm and system co-design for a well-tuned high performance\ndistributed training system. Unfortunately, the only existing platform that\nmeets the requirements strongly depends on Google's hardware (TPU) and software\n(Mesh Tensorflow) stack, and is not open and available to the public,\nespecially GPU and PyTorch communities.\n  In this paper, we present FastMoE, a distributed MoE training system based on\nPyTorch with common accelerators. The system provides a hierarchical interface\nfor both flexible model design and easy adaption to different applications,\nsuch as Transformer-XL and Megatron-LM. Different from direct implementation of\nMoE models using PyTorch, the training speed is highly optimized in FastMoE by\nsophisticated high-performance acceleration skills. The system supports placing\ndifferent experts on multiple GPUs across multiple nodes, enabling enlarging\nthe number of experts linearly against the number of GPUs. The source of\nFastMoE is available at https://github.com/laekov/fastmoe under Apache-2\nlicense.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:27:15 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["He", "Jiaao", ""], ["Qiu", "Jiezhong", ""], ["Zeng", "Aohan", ""], ["Yang", "Zhilin", ""], ["Zhai", "Jidong", ""], ["Tang", "Jie", ""]]}, {"id": "2103.13266", "submitter": "Sang Su Lee", "authors": "Sangsu Lee, Xi Zheng, Jie Hua, Haris Vikalo, Christine Julien", "title": "Opportunistic Federated Learning: An Exploration of Egocentric\n  Collaboration for Pervasive Computing Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pervasive computing applications commonly involve user's personal smartphones\ncollecting data to influence application behavior. Applications are often\nbacked by models that learn from the user's experiences to provide personalized\nand responsive behavior. While models are often pre-trained on massive\ndatasets, federated learning has gained attention for its ability to train\nglobally shared models on users' private data without requiring the users to\nshare their data directly. However, federated learning requires devices to\ncollaborate via a central server, under the assumption that all users desire to\nlearn the same model. We define a new approach, opportunistic federated\nlearning, in which individual devices belonging to different users seek to\nlearn robust models that are personalized to their user's own experiences.\nHowever, instead of learning in isolation, these models opportunistically\nincorporate the learned experiences of other devices they encounter\nopportunistically. In this paper, we explore the feasibility and limits of such\nan approach, culminating in a framework that supports encounter-based pairwise\ncollaborative learning. The use of our opportunistic encounter-based learning\namplifies the performance of personalized learning while resisting overfitting\nto encountered data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:30:21 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lee", "Sangsu", ""], ["Zheng", "Xi", ""], ["Hua", "Jie", ""], ["Vikalo", "Haris", ""], ["Julien", "Christine", ""]]}, {"id": "2103.13289", "submitter": "Jonas Vogt", "authors": "Horst Wieker, Bechir Allani, Thomas Baum, Manuel F\\\"unfrocken, Arno\n  Hinsberger, Jonas Vogt, Sebastian Weber", "title": "Automatisierte Verwaltung von ITS Roadside Stations f\\\"ur den simTD\n  Feldversuch", "comments": "in German. Presented at Automatisierungs-, Assistenzsysteme und\n  eingebettete Systeme f\\\"ur Transportmittel - AAET, Braunschweig, Germany,\n  10-11 February 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The simTD project is the first large-scale field trial for vehicle-to-vehicle\nand vehicle-to-infrastructure communication in Europe. It consists of up to 400\nvehicles and over 100 infrastructure-side communication units, so-called ITS\nRoadside Stations (IRS). With the large number of remote units, a powerful\nmanagement system is needed to ensure that all necessary administrative tasks\nfor the IRS can be performed: from basic configuration, to installation and\nmanagement of applications, to handling and troubleshooting of the IRS\nthemselves. Furthermore, a graphical interface for administration will be\ncreated, an encrypted communication channel will be implemented, and a\nframework for third-party applications will be developed. Due to the importance\nof management for the entire project, the management system must be highly\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 10:39:09 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wieker", "Horst", ""], ["Allani", "Bechir", ""], ["Baum", "Thomas", ""], ["F\u00fcnfrocken", "Manuel", ""], ["Hinsberger", "Arno", ""], ["Vogt", "Jonas", ""], ["Weber", "Sebastian", ""]]}, {"id": "2103.13308", "submitter": "Bokan Chen", "authors": "Ana Radovanovic, Bokan Chen, Saurav Talukdar, Binz Roy, Alexandre\n  Duarte, and Mahya Shahbazi", "title": "Power Modeling for Effective Datacenter Planning and Compute Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datacenter power demand has been continuously growing and is the key driver\nof its cost. An accurate mapping of compute resources (CPU, RAM, etc.) and\nhardware types (servers, accelerators, etc.) to power consumption has emerged\nas a critical requirement for major Web and cloud service providers. With the\nglobal growth in datacenter capacity and associated power consumption, such\nmodels are essential for important decisions around datacenter design and\noperation. In this paper, we discuss two classes of statistical power models\ndesigned and validated to be accurate, simple, interpretable and applicable to\nall hardware configurations and workloads across hyperscale datacenters of\nGoogle fleet. To the best of our knowledge, this is the largest scale power\nmodeling study of this kind, in both the scope of diverse datacenter planning\nand real-time management use cases, as well as the variety of hardware\nconfigurations and workload types used for modeling and validation. We\ndemonstrate that the proposed statistical modeling techniques, while simple and\nscalable, predict power with less than 5% Mean Absolute Percent Error (MAPE)\nfor more than 95% diverse Power Distribution Units (more than 2000) using only\n4 features. This performance matches the reported accuracy of the previous\nstarted-of-the-art methods, while using significantly less features and\ncovering a wider range of use cases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 21:22:51 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 20:59:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Radovanovic", "Ana", ""], ["Chen", "Bokan", ""], ["Talukdar", "Saurav", ""], ["Roy", "Binz", ""], ["Duarte", "Alexandre", ""], ["Shahbazi", "Mahya", ""]]}, {"id": "2103.13333", "submitter": "Chao Zheng", "authors": "Chao Zheng, Qinghui Zhuang, Fei Guo", "title": "A Multi-Tenant Framework for Cloud Container Services", "comments": "ICDCS 21 industry track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Container technologies have been evolving rapidly in the cloud-native era.\nKubernetes, as a production-grade container orchestration platform, has been\nproven to be successful at managing containerized applications in on-premises\ndatacenters. However, Kubernetes lacks sufficient multi-tenant supports by\ndesign, meaning in cloud environments, dedicated clusters are required to serve\nmultiple users, i.e., tenants. This limitation significantly diminishes the\nbenefits of cloud computing, and makes it difficult to build multi-tenant\nsoftware as a service (SaaS) products using Kubernetes. In this paper, we\npropose Virtual-Cluster, a new multi-tenant framework that extends Kubernetes\nwith adequate multi-tenant supports. Basically, VirtualCluster provides both\ncontrol plane and data plane isolations while sharing the underlying compute\nresources among tenants. The new framework preserves the API compatibility by\navoiding modifying the Kubernetes core components. Hence, it can be easily\nintegrated with existing Kubernetes use cases. Our experimental results show\nthat the overheads introduced by VirtualCluster, in terms of latency and\nthroughput, is moderate.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:40:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zheng", "Chao", ""], ["Zhuang", "Qinghui", ""], ["Guo", "Fei", ""]]}, {"id": "2103.13351", "submitter": "Waleed Reda", "authors": "Waleed Reda, Marco Canini, Dejan Kosti\\'c, Simon Peter", "title": "RDMA is Turing complete, we just did not know it yet!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly popular for distributed systems to exploit\nnetwork offload to alleviate load on the CPU. Remote Direct Memory Access\n(RDMA) NICs (RNICs) are one such device, allowing applications to offload\nremote memory accesses. However, RDMA still requires CPU intervention for\ncomplex offloads, beyond simple remote memory access. As such, the offload\npotential for RNICs is limited and RDMA-based systems usually have to work\naround such limitations.\n  We present RedN, a principled, practical approach to implementing complex\nRNIC offloads, without requiring any hardware modifications. Using\nself-modifying RDMA chains, we lift the existing RDMA verbs interface to a\nTuring complete set of programming abstractions. We explore what is possible in\nterms of offload complexity and performance with just a commodity RNIC. Through\na key-value store use case study, we show how to integrate complex RNIC\noffloads into existing applications. RedN can outperform one and two-sided RDMA\nimplementations by up to 3x and 7.8x for key-value get operations and\nperformance isolation, respectively, and provide applications with failure\nresiliency to OS and process crashes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:17:40 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Reda", "Waleed", ""], ["Canini", "Marco", ""], ["Kosti\u0107", "Dejan", ""], ["Peter", "Simon", ""]]}, {"id": "2103.13577", "submitter": "Oded Green", "authors": "Oded Green", "title": "ButterFly BFS -- An Efficient Communication Pattern for Multi Node\n  Traversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breadth-First Search (BFS) is a building block used in a wide array of graph\nanalytics and is used in various network analysis domains: social, road,\ntransportation, communication, and much more. Over the last two decades,\nnetwork sizes have continued to grow. The popularity of BFS has brought with it\na need for significantly faster traversals. Thus, BFS algorithms have been\ndesigned to exploit shared-memory and shared-nothing systems -- this includes\nalgorithms for accelerators such as the GPU. GPUs offer extremely fast\ntraversals at the cost of processing smaller graphs due to their limited memory\nsize. In contrast, CPU shared-memory systems can scale to graphs with several\nbillion edges but do not have enough compute resources needed for fast\ntraversals. This paper introduces ButterFly BFS, a multi-GPU traversal\nalgorithm that allows analyzing significantly larger networks at high rates.\nButterFly BFS scales to the similar-sized graphs processed by shared-memory\nsystems while improving performance by more than 10X compared to CPUs. We\nevaluate our new algorithm on an NVIDIA DGX-2 server with 16 V100 GPUS and show\nthat our algorithm scales with an increase in the number of GPUS. We show that\nwe can achieve a roughly $70\\%$ performance linear speedup, which is\nnon-trivial for BFS. For a scale 29 Kronecker graph and edge factor of 8, our\nnew algorithm traverses the graph at a rate of over 300 GTEP/s. That is a high\ntraversal rate for a single server.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:22:01 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Green", "Oded", ""]]}, {"id": "2103.13748", "submitter": "Yiwei Liao", "authors": "Yiwei Liao, Zhuorui Li, Kun Huang, and Shi Pu", "title": "Compressed Gradient Tracking Methods for Decentralized Optimization with\n  Linear Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication compression techniques are of growing interests for solving the\ndecentralized optimization problem under limited communication, where the\nglobal objective is to minimize the average of local cost functions over a\nmulti-agent network using only local computation and peer-to-peer\ncommunication. In this paper, we first propose a novel compressed gradient\ntracking algorithm (C-GT) that combines gradient tracking technique with\ncommunication compression. In particular, C-GT is compatible with a general\nclass of compression operators that unifies both unbiased and biased\ncompressors. We show that C-GT inherits the advantages of gradient\ntracking-based algorithms and achieves linear convergence rate for strongly\nconvex and smooth objective functions. In the second part of this paper, we\npropose an error feedback based compressed gradient tracking algorithm\n(EF-C-GT) to further improve the algorithm efficiency for biased compression\noperators. Numerical examples complement the theoretical findings and\ndemonstrate the efficiency and flexibility of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 11:00:49 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 11:30:45 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 06:27:09 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Liao", "Yiwei", ""], ["Li", "Zhuorui", ""], ["Huang", "Kun", ""], ["Pu", "Shi", ""]]}, {"id": "2103.13822", "submitter": "Minxue Tang", "authors": "Minxue Tang, Xuefei Ning, Yitu Wang, Yu Wang and Yiran Chen", "title": "FedGP: Correlation-Based Active Client Selection for Heterogeneous\n  Federated Learning", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Client-wise heterogeneity is one of the major issues that hinder effective\ntraining in federated learning (FL). Since the data distribution on each client\nmay differ dramatically, the client selection strategy can largely influence\nthe convergence rate of the FL process. Several recent studies adopt active\nclient selection strategies. However, they neglect the loss correlations\nbetween the clients and achieve marginal improvement compared to the uniform\nselection strategy. In this work, we propose FedGP -- a federated learning\nframework built on a correlation-based client selection strategy, to boost the\nconvergence rate of FL. Specifically, we first model the loss correlations\nbetween the clients with a Gaussian Process (GP). To make the GP training\nfeasible in the communication-bounded FL process, we develop a GP training\nmethod utilizing the historical samples efficiently to reduce the communication\ncost. Finally, based on the correlations we learned, we derive the client\nselection with an enlarged reduction of expected global loss in each round. Our\nexperimental results show that compared to the latest active client selection\nstrategy, FedGP can improve the convergence rates by $1.3\\sim2.3\\times$ and\n$1.2\\sim1.4\\times$ on FMNIST and CIFAR-10, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:25:14 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Tang", "Minxue", ""], ["Ning", "Xuefei", ""], ["Wang", "Yitu", ""], ["Wang", "Yu", ""], ["Chen", "Yiran", ""]]}, {"id": "2103.14132", "submitter": "Charilaos Akasiadis PhD", "authors": "Charilaos Akasiadis, Miguel Ponce-de-Leon, Arnau Montagud, Evangelos\n  Michelioudakis, Alexia Atsidakou, Elias Alevizos, Alexander Artikis, Alfonso\n  Valencia and Georgios Paliouras", "title": "Parallel Model Exploration for Tumor Treatment Simulations", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DC q-bio.TO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational systems and methods are being applied to solve biological\nproblems for many years. Incorporating methods of this kind in the research for\ncancer treatment and related drug discovery in particular, is shown to be\nchallenging due to the complexity and the dynamic nature of the related\nfactors. Usually, there are two objectives in such settings; first to calibrate\nthe simulators so as to reproduce real-world cases, and second, to search for\nspecific values of the parameter space concerning effective drug treatments. We\ncombine a multi-scale simulator for tumor cell growth and a Genetic Algorithm\n(GA) as a heuristic search method for finding good parameter configurations in\nreasonable time. The two modules are integrated into a single workflow that can\nbe executed in a parallel manner on high performance computing infrastructures,\nsince large-scale computational and storage capabilities are necessary in this\ndomain. After using the GA for calibration, our goal is to explore different\ndrug delivery schemes. Among these schemes, we aim to find those that minimize\ntumor cell size and the probability of emergence of drug resistant cells in the\nfuture. Results from experiments on high performance computing infrastructure\nillustrate the effectiveness and timeliness of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 20:58:44 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Akasiadis", "Charilaos", ""], ["Ponce-de-Leon", "Miguel", ""], ["Montagud", "Arnau", ""], ["Michelioudakis", "Evangelos", ""], ["Atsidakou", "Alexia", ""], ["Alevizos", "Elias", ""], ["Artikis", "Alexander", ""], ["Valencia", "Alfonso", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2103.14225", "submitter": "Shih-Chun Lin", "authors": "Shih-Chun Lin, Kwang-Cheng Chen, and Ali Karimoddini", "title": "SD-VEC: Software-Defined Vehicular Edge Computing with Ultra-Low Latency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New paradigm shifts and 6G technological revolution in vehicular services\nhave emerged toward unmanned driving, automated transportation, and\nself-driving vehicles. As the technology for autonomous vehicles becomes\nmature, real challenges come from reliable, safe, real-time connected\ntransportation operations to achieve ubiquitous and prompt information\nexchanges with massive connected and autonomous vehicles. This article aims at\nintroducing novel wireless distributed architectures that embed the edge\ncomputing capability inside software-defined vehicular networking\ninfrastructure. Such edge networks consist of open-loop grant-free\ncommunications and computing-based control frameworks, which enable dynamic\neco-routing with ultra-low latency and mobile data-driven orchestration. Thus,\nthis work advances the frontiers of machine learning potentials and\nnext-generation mobile system realization in vehicular networking applications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:25:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lin", "Shih-Chun", ""], ["Chen", "Kwang-Cheng", ""], ["Karimoddini", "Ali", ""]]}, {"id": "2103.14409", "submitter": "Lars Bjertnes", "authors": "Lars Bjertnes, Jacob O. T{\\o}rring, Anne C. Elster", "title": "LS-CAT: A Large-Scale CUDA AutoTuning Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of Machine Learning (ML) methods depend on access to large\nsuitable datasets. In this article, we present how we build the LS-CAT\n(Large-Scale CUDA AutoTuning) dataset sourced from GitHub for the purpose of\ntraining NLP-based ML models. Our dataset includes 19 683 CUDA kernels focused\non linear algebra. In addition to the CUDA codes, our LS-CAT dataset contains 5\n028 536 associated runtimes, with different combinations of kernels, block\nsizes and matrix sizes. The runtime are GPU benchmarks on both Nvidia GTX 980\nand Nvidia T4 systems. This information creates a foundation upon which\nNLP-based models can find correlations between source-code features and optimal\nchoice of thread block sizes.\n  There are several results that can be drawn out of our LS-CAT database. E.g.,\nour experimental results show that an optimal choice in thread block size can\ngain an average of 6% for the average case. We thus also analyze how much\nperformance increase can be achieved in general, finding that in 10% of the\ncases more than 20% performance increase can be achieved by using the optimal\nblock. A description of current and future work is also included.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:33:48 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bjertnes", "Lars", ""], ["T\u00f8rring", "Jacob O.", ""], ["Elster", "Anne C.", ""]]}, {"id": "2103.14576", "submitter": "Saswata Paul", "authors": "Saswata Paul, Gul A. Agha, Stacy Patterson, Carlos A. Varela", "title": "Verification of Eventual Consensus in Synod Using a Failure-Aware Actor\n  Model", "comments": "This technical report is an extended version of the NASA Formal\n  Methods Symposium 2021 proceedings paper with the same name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Successfully attaining consensus in the absence of a centralized coordinator\nis a fundamental problem in distributed multi-agent systems. We analyze\nprogress in the Synod consensus protocol -- which does not assume a unique\nleader -- under the assumptions of asynchronous communication and potential\nagent failures. We identify a set of sufficient conditions under which it is\npossible to guarantee that a set of agents will eventually attain consensus.\nFirst, a subset of the agents must behave correctly and not permanently fail\nuntil consensus is reached, and second, at least one proposal must be\neventually uninterrupted by higher-numbered proposals. To formally reason about\nagent failures, we introduce a failure-aware actor model (FAM). Using FAM, we\nmodel the identified conditions and provide a formal proof of eventual progress\nin Synod. Our proof has been mechanically verified using the Athena proof\nassistant and, to the best of our knowledge, it is the first machine-checked\nproof of eventual progress in Synod.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:42:15 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Paul", "Saswata", ""], ["Agha", "Gul A.", ""], ["Patterson", "Stacy", ""], ["Varela", "Carlos A.", ""]]}, {"id": "2103.14649", "submitter": "Elad Michael Schiller (PhD)", "authors": "Chryssis Georgiou and Ioannis Marcoullis and Michel Raynal and Elad\n  Michael Schiller", "title": "Loosely-self-stabilizing Byzantine-tolerant Binary Consensus for\n  Signature-free Message-passing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  At PODC 2014, A. Most\\'efaoui, H. Moumen, and M. Raynal presented a new and\nsimple randomized signature-free binary consensus algorithm (denoted here MMR)\nthat copes with the net effect of asynchrony Byzantine behaviors. Assuming\nmessage scheduling is fair and independent from random numbers MMR is optimal\nin several respects: it deals with up to t Byzantine processes where t < n/3\nand n is the number of processes, O(n\\^2) messages and O(1) expected time. The\npresent article presents a non-trivial extension of MMR to an even more\nfault-prone context, namely, in addition to Byzantine processes, it considers\nalso that the system can experience transient failures. To this end it\nconsiders self-stabilization techniques to cope with communication failures and\narbitrary transient faults (such faults represent any violation of the\nassumptions according to which the system was designed to operate).\n  The proposed algorithm is the first loosely-self-stabilizing Byzantine\nfault-tolerant binary consensus algorithm suited to asynchronous\nmessage-passing systems. This is achieved via an instructive transformation of\nMMR to a self-stabilizing solution that can violate safety requirements with\nprobability Pr= O(1/(2\\^M)), where M is a predefined constant that can be set\nto any positive integer at the cost of 3 M n + log M bits of local memory. In\naddition to making MMR resilient to transient faults, the obtained\nself-stabilizing algorithm preserves its properties of optimal resilience and\ntermination, (i.e., t < n/3, and O(1) expected time). Furthermore, it only\nrequires a bounded amount of memory.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:48:31 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 15:22:15 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Georgiou", "Chryssis", ""], ["Marcoullis", "Ioannis", ""], ["Raynal", "Michel", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "2103.14701", "submitter": "Vasilis Gavrielatos", "authors": "Vasilis Gavrielatos, Antonios Katsarakis, Vijay Nagarajan", "title": "Extending Classic Paxos for High-performance Read-Modify-Write Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we provide a detailed specification of how we extended and\nimplemented Classic Paxos (CP) to execute Read-Modify-Writes. In addition, we\nalso specify how we implemented All-aboard Paxos over CP and how we use\ncarstamps, to also add ABD reads and writes, to accelerate the common case,\nwhere RMWs are not needed. Our specification targets a Key-Value-Store that is\ndeployed within the datacenter, is replicated across 3 to 7 machines and\nsupports reads, writes and RMWs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:22:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gavrielatos", "Vasilis", ""], ["Katsarakis", "Antonios", ""], ["Nagarajan", "Vijay", ""]]}, {"id": "2103.14737", "submitter": "Charles Leggett", "authors": "Zhihua Dong, Heather Gray, Charles Leggett, Meifeng Lin, Vincent R.\n  Pascuzzi, Kwangmin Yu", "title": "Porting HEP Parameterized Calorimeter Simulation Code to GPUs", "comments": "15 pages, 1 figure, 8 tables, 2 listings, submitted to Frontiers in\n  Big Data (Big Data in AI and High Energy Physics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The High Energy Physics (HEP) experiments, such as those at the Large Hadron\nCollider (LHC), traditionally consume large amounts of CPU cycles for detector\nsimulations and data analysis, but rarely use compute accelerators such as\nGPUs. As the LHC is upgraded to allow for higher luminosity, resulting in much\nhigher data rates, purely relying on CPUs may not provide enough computing\npower to support the simulation and data analysis needs. As a proof of concept,\nwe investigate the feasibility of porting a HEP parameterized calorimeter\nsimulation code to GPUs. We have chosen to use FastCaloSim, the ATLAS fast\nparametrized calorimeter simulation. While FastCaloSim is sufficiently fast\nsuch that it does not impose a bottleneck in detector simulations overall,\nsignificant speed-ups in the processing of large samples can be achieved from\nGPU parallelization at both the particle (intra-event) and event levels; this\nis especially beneficial in conditions expected at the high-luminosity LHC,\nwhere extremely high per-event particle multiplicities will result from the\nmany simultaneous proton-proton collisions. We report our experience with\nporting FastCaloSim to NVIDIA GPUs using CUDA. A preliminary Kokkos\nimplementation of FastCaloSim for portability to other parallel architectures\nis also described.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 21:21:57 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 18:35:16 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dong", "Zhihua", ""], ["Gray", "Heather", ""], ["Leggett", "Charles", ""], ["Lin", "Meifeng", ""], ["Pascuzzi", "Vincent R.", ""], ["Yu", "Kwangmin", ""]]}, {"id": "2103.14831", "submitter": "Aman Goel", "authors": "Aman Goel, Karem A. Sakallah", "title": "On Symmetry and Quantification: A New Approach to Verify Distributed\n  Protocols", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-76384-8_9", "report-no": null, "categories": "cs.LO cs.DC cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proving that an unbounded distributed protocol satisfies a given safety\nproperty amounts to finding a quantified inductive invariant that implies the\nproperty for all possible instance sizes of the protocol. Existing methods for\nsolving this problem can be described as search procedures for an invariant\nwhose quantification prefix fits a particular template. We propose an\nalternative constructive approach that does not prescribe, a priori, a specific\nquantifier prefix. Instead, the required prefix is automatically inferred\nwithout any search by carefully analyzing the structural symmetries of the\nprotocol. The key insight underlying this approach is that symmetry and\nquantification are closely related concepts that express protocol invariance\nunder different re-arrangements of its components. We propose symmetric\nincremental induction, an extension of the finite-domain IC3/PDR algorithm,\nthat automatically derives the required quantified inductive invariant by\nexploiting the connection between symmetry and quantification. While various\nattempts have been made to exploit symmetry in verification applications, to\nour knowledge, this is the first demonstration of a direct link between\nsymmetry and quantification in the context of clause learning during\nincremental induction. We also describe a procedure to automatically find a\nminimal finite size, the cutoff, that yields a quantified invariant proving\nsafety for any size.\n  Our approach is implemented in IC3PO, a new verifier for distributed\nprotocols that significantly outperforms the state-of-the-art, scales orders of\nmagnitude faster, and robustly derives compact inductive invariants fully\nautomatically.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 07:36:39 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Goel", "Aman", ""], ["Sakallah", "Karem A.", ""]]}, {"id": "2103.14915", "submitter": "Shengliang Lu", "authors": "Shengliang Lu, Shixuan Sun, Johns Paul, Yuchen Li, Bingsheng He", "title": "Cache-Efficient Fork-Processing Patterns on Large Graphs", "comments": "in SIGMOD 2021", "journal-ref": null, "doi": "10.1145/3448016.3457253", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As large graph processing emerges, we observe a costly fork-processing\npattern (FPP) that is common in many graph algorithms. The unique feature of\nthe FPP is that it launches many independent queries from different source\nvertices on the same graph. For example, an algorithm in analyzing the network\ncommunity profile can execute Personalized PageRanks that start from tens of\nthousands of source vertices at the same time. We study the efficiency of\nhandling FPPs in state-of-the-art graph processing systems on multi-core\narchitectures. We find that those systems suffer from severe cache miss penalty\nbecause of the irregular and uncoordinated memory accesses in processing FPPs.\n  In this paper, we propose ForkGraph, a cache-efficient FPP processing system\non multi-core architectures. To improve the cache reuse, we divide the graph\ninto partitions each sized of LLC capacity, and the queries in an FPP are\nbuffered and executed on the partition basis. We further develop efficient\nintra- and inter-partition execution strategies for efficiency. For\nintra-partition processing, since the graph partition fits into LLC, we propose\nto execute each graph query with efficient sequential algorithms (in contrast\nwith parallel algorithms in existing parallel graph processing systems) and\npresent an atomic-free query processing by consolidating contending operations\nto cache-resident graph partition. For inter-partition processing, we propose\nyielding and priority-based scheduling, to reduce redundant work in processing.\nBesides, we theoretically prove that ForkGraph performs the same amount of\nwork, to within a constant factor, as the fastest known sequential algorithms\nin FPP queries processing, which is work efficient. Our evaluations on\nreal-world graphs show that ForkGraph significantly outperforms\nstate-of-the-art graph processing systems with two orders of magnitude\nspeedups.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 14:29:04 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 01:05:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lu", "Shengliang", ""], ["Sun", "Shixuan", ""], ["Paul", "Johns", ""], ["Li", "Yuchen", ""], ["He", "Bingsheng", ""]]}, {"id": "2103.14990", "submitter": "Carmen Amo Alonso", "authors": "Carmen Amo Alonso and Shih-Hao Tseng", "title": "Effective GPU Parallelization of Distributed and Localized Model\n  Predictive Control", "comments": "Submitted to 2021 Control and Decision Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To effectively control large-scale distributed systems online, model\npredictive control (MPC) has to swiftly solve the underlying high-dimensional\noptimization. There are multiple techniques applied to accelerate the solving\nprocess in the literature, mainly attributed to software-based algorithmic\nadvancements and hardware-assisted computation enhancements. However, those\nmethods focus on arithmetic accelerations and overlook the benefits of the\nunderlying system's structure. In particular, the existing decoupled\nsoftware-hardware algorithm design that naively parallelizes the arithmetic\noperations by the hardware does not tackle the hardware overheads such as\nCPU-GPU and thread-to-thread communications in a principled manner. Also, the\nadvantages of parallelizable subproblem decomposition in distributed MPC are\nnot well recognized and exploited. As a result, we have not reached the full\npotential of hardware acceleration for MPC. In this paper, we explore those\nopportunities by leveraging GPU to parallelize the distributed and localized\nMPC (DLMPC) algorithm. We exploit the locality constraints embedded in the\nDLMPC formulation to reduce the hardware-intrinsic communication overheads. Our\nparallel implementation achieves up to 50x faster runtime than its CPU\ncounterparts under various parameters. Furthermore, we find that the\nlocality-aware GPU parallelization could halve the optimization runtime\ncomparing to the naive acceleration. Overall, our results demonstrate the\nperformance gains brought by software-hardware co-design with the information\nexchange structure in mind.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:34:55 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Alonso", "Carmen Amo", ""], ["Tseng", "Shih-Hao", ""]]}, {"id": "2103.15024", "submitter": "Xinbiao Gan", "authors": "Xinbiao Gan and Wen Tan", "title": "MT-lib: A Topology-aware Message Transfer Library for Graph500 on\n  Supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present MT-lib, an efficient message transfer library for messages gather\nand scatter in benchmarks like Graph500 for Supercomputers. Our library\nincludes MST version as well as new-MST version. The MT-lib is deliberately\nkept light-weight, efficient and friendly interfaces for massive graph\ntraverse. MST provides (1) a novel non-blocking communication scheme with\nsending and receiving messages asynchronously to overlap calculation and\ncommunication;(2) merging messages according to the target process for reducing\ncommunication overhead;(3) a new communication mode of gathering intra-group\nmessages before forwarding between groups for reducing communication traffic.\nIn MT-lib, there are (1) one-sided message; (2) two-sided messages; and (3)\ntwo-sided messages with buffer, in which dynamic buffer expansion is built for\nmessages delivery. We experimented with MST and then testing Graph500 with MST\non Tianhe supercomputers. Experimental results show high communication\nefficiency and high throughputs for both BFS and SSSP communication operations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 00:32:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gan", "Xinbiao", ""], ["Tan", "Wen", ""]]}, {"id": "2103.15195", "submitter": "Zhuang Wang", "authors": "Zhuang Wang, Xinyu Wu, T.S. Eugene Ng", "title": "MergeComp: A Compression Scheduler for Scalable Communication-Efficient\n  Distributed Training", "comments": "8 papes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Large-scale distributed training is increasingly becoming communication\nbound. Many gradient compression algorithms have been proposed to reduce the\ncommunication overhead and improve scalability. However, it has been observed\nthat in some cases gradient compression may even harm the performance of\ndistributed training.\n  In this paper, we propose MergeComp, a compression scheduler to optimize the\nscalability of communication-efficient distributed training. It automatically\nschedules the compression operations to optimize the performance of compression\nalgorithms without the knowledge of model architectures or system parameters.\nWe have applied MergeComp to nine popular compression algorithms. Our\nevaluations show that MergeComp can improve the performance of compression\nalgorithms by up to 3.83x without losing accuracy. It can even achieve a\nscaling factor of distributed training up to 99% over high-speed networks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 18:26:55 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Zhuang", ""], ["Wu", "Xinyu", ""], ["Ng", "T. S. Eugene", ""]]}, {"id": "2103.15217", "submitter": "Adam Polak", "authors": "Adam Polak, Adrian Siwiec, Micha{\\l} Stobierski", "title": "Euler Meets GPU: Practical Graph Algorithms with Theoretical Guarantees", "comments": "IPDPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euler tour technique is a classical tool for designing parallel graph\nalgorithms, originally proposed for the PRAM model. We ask whether it can be\nadapted to run efficiently on GPU. We focus on two established applications of\nthe technique: (1) the problem of finding lowest common ancestors (LCA) of\npairs of nodes in trees, and (2) the problem of finding bridges in undirected\ngraphs. In our experiments, we compare theoretically optimal algorithms using\nthe Euler tour technique against simpler heuristics supposed to perform\nparticularly well on typical instances. We show that the Euler tour-based\nalgorithms not only fulfill their theoretical promises and outperform practical\nheuristics on hard instances, but also perform on par with them on easy\ninstances.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:02:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Polak", "Adam", ""], ["Siwiec", "Adrian", ""], ["Stobierski", "Micha\u0142", ""]]}, {"id": "2103.15285", "submitter": "Junya Nakamura", "authors": "Junya Nakamura and Yonghwan Kim and Yoshiaki Katayama and Toshimitsu\n  Masuzawa", "title": "A cooperative partial snapshot algorithm for checkpoint-rollback\n  recovery of large-scale and dynamic distributed systems and experimental\n  evaluations", "comments": null, "journal-ref": "Concurrency Computat Pract Exper. 2020;e5647", "doi": "10.1002/cpe.5647", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed system consisting of a huge number of computational entities is\nprone to faults, because faults in a few nodes cause the entire system to fail.\nConsequently, fault tolerance of distributed systems is a critical issue.\nCheckpoint-rollback recovery is a universal and representative technique for\nfault tolerance; it periodically records the entire system state\n(configuration) to non-volatile storage, and the system restores itself using\nthe recorded configuration when the system fails. To record a configuration of\na distributed system, a specific algorithm known as a snapshot algorithm is\nrequired. However, many snapshot algorithms require coordination among all\nnodes in the system; thus, frequent executions of snapshot algorithms require\nunacceptable communication cost, especially if the systems are large. As a\nsophisticated snapshot algorithm, a partial snapshot algorithm has been\nintroduced that takes a partial snapshot (instead of a global snapshot).\nHowever, if two or more partial snapshot algorithms are concurrently executed,\nand their snapshot domains overlap, they should coordinate, so that the partial\nsnapshots (taken by the algorithms) are consistent. In this paper, we propose a\nnew efficient partial snapshot algorithm with the aim of reducing communication\nfor the coordination. In a simulation, we show that the proposed algorithm\ndrastically outperforms the existing partial snapshot algorithm, in terms of\nmessage and time complexity.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:39:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Nakamura", "Junya", ""], ["Kim", "Yonghwan", ""], ["Katayama", "Yoshiaki", ""], ["Masuzawa", "Toshimitsu", ""]]}, {"id": "2103.15386", "submitter": "Hui Wang", "authors": "Hui Wang, Wan-Lei Zhao and Xiangxiang Zeng", "title": "Large-Scale Approximate k-NN Graph Construction on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-nearest neighbor graph is a key data structure in many disciplines such as\nmanifold learning, machine learning and information retrieval, etc. NN-Descent\nwas proposed as an effective solution for the graph construction problem.\nHowever, it cannot be directly transplanted to GPU due to the intensive memory\naccesses required in the approach. In this paper, NN-Descent has been\nredesigned to adapt to the GPU architecture. In particular, the number of\nmemory accesses has been reduced significantly. The redesign fully exploits the\nparallelism of the GPU hardware. In the meantime, the genericness as well as\nthe simplicity of NN-Descent are well-preserved. In addition, a simple but\neffective k-NN graph merge approach is presented. It allows two graphs to be\nmerged efficiently on GPUs. More importantly, it makes the construction of\nhigh-quality k-NN graphs for out-of-GPU-memory datasets tractable. The results\nshow that our approach is 100-250x faster than single-thread NN-Descent and is\n2.5-5x faster than existing GPU-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:24:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Hui", ""], ["Zhao", "Wan-Lei", ""], ["Zeng", "Xiangxiang", ""]]}, {"id": "2103.15753", "submitter": "Pavlos Papadopoulos", "authors": "Pavlos Papadopoulos, Will Abramson, Adam J. Hall, Nikolaos Pitropakis\n  and William J. Buchanan", "title": "Privacy and Trust Redefined in Federated Machine Learning", "comments": "MDPI Mach. Learn. Knowl. Extr. 2021, 3(2), 333-356;\n  https://doi.org/10.3390/make3020017", "journal-ref": "Mach. Learn. Knowl. Extr. 2021, 3(2), 333-356", "doi": "10.3390/make3020017", "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common privacy issue in traditional machine learning is that data needs to\nbe disclosed for the training procedures. In situations with highly sensitive\ndata such as healthcare records, accessing this information is challenging and\noften prohibited. Luckily, privacy-preserving technologies have been developed\nto overcome this hurdle by distributing the computation of the training and\nensuring the data privacy to their owners. The distribution of the computation\nto multiple participating entities introduces new privacy complications and\nrisks. In this paper, we present a privacy-preserving decentralised workflow\nthat facilitates trusted federated learning among participants. Our\nproof-of-concept defines a trust framework instantiated using decentralised\nidentity technologies being developed under Hyperledger projects\nAries/Indy/Ursa. Only entities in possession of Verifiable Credentials issued\nfrom the appropriate authorities are able to establish secure, authenticated\ncommunication channels authorised to participate in a federated learning\nworkflow related to mental health data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:47:01 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:07:01 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Papadopoulos", "Pavlos", ""], ["Abramson", "Will", ""], ["Hall", "Adam J.", ""], ["Pitropakis", "Nikolaos", ""], ["Buchanan", "William J.", ""]]}, {"id": "2103.15860", "submitter": "J\\\"ames M\\'en\\'etrey", "authors": "J\\\"ames M\\'en\\'etrey, Marcelo Pasin, Pascal Felber, Valerio Schiavoni", "title": "Twine: An Embedded Trusted Runtime for WebAssembly", "comments": "12 pages. This is the author's version of the work. The definitive\n  version will be published in the proceedings of the 37th IEEE International\n  Conference on Data Engineering (ICDE'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WebAssembly is an increasingly popular lightweight binary instruction format,\nwhich can be efficiently embedded and sandboxed. Languages like C, C++, Rust,\nGo, and many others can be compiled into WebAssembly. This paper describes\nTwine, a WebAssembly trusted runtime designed to execute unmodified,\nlanguage-independent applications. We leverage Intel SGX to build the runtime\nenvironment without dealing with language-specific, complex APIs. While SGX\nhardware provides secure execution within the processor, Twine provides a\nsecure, sandboxed software runtime nested within an SGX enclave, featuring a\nWebAssembly system interface (WASI) for compatibility with unmodified\nWebAssembly applications. We evaluate Twine with a large set of general-purpose\nbenchmarks and real-world applications. In particular, we used Twine to\nimplement a secure, trusted version of SQLite, a well-known full-fledged\nembeddable database. We believe that such a trusted database would be a\nreasonable component to build many larger application services. Our evaluation\nshows that SQLite can be fully executed inside an SGX enclave via WebAssembly\nand existing system interface, with similar average performance overheads. We\nestimate that the performance penalties measured are largely compensated by the\nadditional security guarantees and its full compatibility with standard\nWebAssembly. An in-depth analysis of our results indicates that performance can\nbe greatly improved by modifying some of the underlying libraries. We describe\nand implement one such modification in the paper, showing up to $4.1\\times$\nspeedup. Twine is open-source, available at GitHub along with instructions to\nreproduce our experiments.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:10:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["M\u00e9n\u00e9trey", "J\u00e4mes", ""], ["Pasin", "Marcelo", ""], ["Felber", "Pascal", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2103.15947", "submitter": "Hadi Jamali-Rad", "authors": "Hadi Jamali-Rad, Mohammad Abdizadeh, Attila Szabo", "title": "Federated Learning with Taskonomy for Non-IID Data", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical federated learning approaches incur significant performance\ndegradation in the presence of non-IID client data. A possible direction to\naddress this issue is forming clusters of clients with roughly IID data. Most\nsolutions following this direction are iterative and relatively slow, also\nprone to convergence issues in discovering underlying cluster formations. We\nintroduce federated learning with taskonomy (FLT) that generalizes this\ndirection by learning the task-relatedness between clients for more efficient\nfederated aggregation of heterogeneous data. In a one-off process, the server\nprovides the clients with a pretrained (and fine-tunable) encoder to compress\ntheir data into a latent representation, and transmit the signature of their\ndata back to the server. The server then learns the task-relatedness among\nclients via manifold learning, and performs a generalization of federated\naveraging. FLT can flexibly handle a generic client relatedness graph, when\nthere are no explicit clusters of clients, as well as efficiently decompose it\ninto (disjoint) clusters for clustered federated learning. We demonstrate that\nFLT not only outperforms the existing state-of-the-art baselines in non-IID\nscenarios but also offers improved fairness across clients.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:47:45 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jamali-Rad", "Hadi", ""], ["Abdizadeh", "Mohammad", ""], ["Szabo", "Attila", ""]]}, {"id": "2103.16063", "submitter": "Masahiro Tanaka", "authors": "Masahiro Tanaka, Kenjiro Taura, Toshihiro Hanawa, Kentaro Torisawa", "title": "Automatic Graph Partitioning for Very Large-scale Deep Learning", "comments": "Accepted to the 35th IEEE International Parallel and Distributed\n  Processing Symposium (IPDPS 2021), May 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes RaNNC (Rapid Neural Network Connector) as middleware for\nautomatic hybrid parallelism. In recent deep learning research, as exemplified\nby T5 and GPT-3, the size of neural network models continues to grow. Since\nsuch models do not fit into the memory of accelerator devices, they need to be\npartitioned by model parallelism techniques. Moreover, to accelerate training\nfor huge training data, we need a combination of model and data parallelisms,\ni.e., hybrid parallelism. Given a model description for PyTorch without any\nspecification for model parallelism, RaNNC automatically partitions the model\ninto a set of subcomponents so that (1) each subcomponent fits a device memory\nand (2) a high training throughput for pipeline parallelism is achieved by\nbalancing the computation times of the subcomponents. In our experiments, we\ncompared RaNNC with two popular frameworks, Megatron-LM (hybrid parallelism)\nand GPipe (originally proposed for model parallelism, but a version allowing\nhybrid parallelism also exists), for training models with increasingly greater\nnumbers of parameters. In the pre-training of enlarged BERT models, RaNNC\nsuccessfully trained models five times larger than those Megatron-LM could, and\nRaNNC's training throughputs were comparable to Megatron-LM's when pre-training\nthe same models. RaNNC also achieved better training throughputs than GPipe on\nboth the enlarged BERT model pre-training (GPipe with hybrid parallelism) and\nthe enlarged ResNet models (GPipe with model parallelism) in all of the\nsettings we tried. These results are remarkable, since RaNNC automatically\npartitions models without any modification to their descriptions; Megatron-LM\nand GPipe require users to manually rewrite the models' descriptions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:26:04 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Tanaka", "Masahiro", ""], ["Taura", "Kenjiro", ""], ["Hanawa", "Toshihiro", ""], ["Torisawa", "Kentaro", ""]]}, {"id": "2103.16182", "submitter": "Nikolaos Kallimanis", "authors": "Nikolaos D. Kallimanis", "title": "Synch: A framework for concurrent data-structures and benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent advancements in multicore machines highlight the need to simplify\nconcurrent programming in order to leverage their computational power. One way\nto achieve this is by designing efficient concurrent data structures (e.g.\nstacks, queues, hash-tables, etc.) and synchronization techniques (e.g. locks,\ncombining techniques, etc.) that perform well in machines with large amounts of\ncores. In contrast to ordinary, sequential data-structures, the concurrent\ndata-structures allow multiple threads to simultaneously access and/or modify\nthem.\n  Synch is an open-source framework that not only provides some common\nhigh-performant concurrent data-structures, but it also provides researchers\nwith the tools for designing and benchmarking high performant concurrent\ndata-structures. The Synch framework contains a substantial set of concurrent\ndata-structures such as queues, stacks, combining-objects, hash-tables, locks,\netc. and it provides a user-friendly runtime for developing and benchmarking\nconcurrent data-structures. Among other features, the provided runtime provides\nfunctionality for creating threads easily (both POSIX and user-level threads),\ntools for measuring performance, etc. Moreover, the provided concurrent\ndata-structures and the runtime are highly optimized for contemporary NUMA\nmultiprocessors such as AMD Epyc and Intel Xeon.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:03:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kallimanis", "Nikolaos D.", ""]]}, {"id": "2103.16234", "submitter": "Antonio J. Pe\\v{n}a", "authors": "Marc Jord\\`a, Pedro Valero-Lara, Antonio J. Pe\\~na", "title": "cuConv: A CUDA Implementation of Convolution for CNN Inference", "comments": "This work has been submitted to the Springer for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutions are the core operation of deep learning applications based on\nConvolutional Neural Networks (CNNs). Current GPU architectures are highly\nefficient for training and deploying deep CNNs, and hence, these are largely\nused in production for this purpose. State-of-the-art implementations, however,\npresent a lack of efficiency for some commonly used network configurations.\n  In this paper we propose a GPU-based implementation of the convolution\noperation for CNN inference that favors coalesced accesses, without requiring\nprior data transformations. Our experiments demonstrate that our proposal\nyields notable performance improvements in a range of common CNN forward\npropagation convolution configurations, with speedups of up to 2.29x with\nrespect to the best implementation of convolution in cuDNN, hence covering a\nrelevant region in currently existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:33:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jord\u00e0", "Marc", ""], ["Valero-Lara", "Pedro", ""], ["Pe\u00f1a", "Antonio J.", ""]]}, {"id": "2103.16251", "submitter": "Sebastian Brandt", "authors": "Sebastian Brandt, Christoph Grunau, V\\'aclav Rozho\\v{n}", "title": "The randomized local computation complexity of the Lov\\'asz local lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Local Computation Algorithm (LCA) model is a popular model in the field\nof sublinear-time algorithms that measures the complexity of an algorithm by\nthe number of probes the algorithm makes in the neighborhood of one node to\ndetermine that node's output.\n  In this paper we show that the randomized LCA complexity of the Lov\\'asz\nLocal Lemma (LLL) on constant degree graphs is $\\Theta(\\log n)$. The lower\nbound follows by proving an $\\Omega(\\log n)$ lower bound for the Sinkless\nOrientation problem introduced in [Brandt et al. STOC 2016]. This answers a\nquestion of [Rosenbaum, Suomela PODC 2020].\n  Additionally, we show that every randomized LCA algorithm for a locally\ncheckable problem with a probe complexity of $o(\\sqrt{\\log{n}})$ can be turned\ninto a deterministic LCA algorithm with a probe complexity of $O(\\log^* n)$.\nThis improves exponentially upon the currently best known speed-up result from\n$o(\\log \\log n)$ to $O(\\log^* n)$ implied by the result of [Chang, Pettie FOCS\n2017] in the LOCAL model.\n  Finally, we show that for every fixed constant $c \\geq 2$, the deterministic\nVOLUME complexity of $c$-coloring a bounded degree tree is $\\Theta(n)$, where\nthe VOLUME model is a close relative of the LCA model that was recently\nintroduced by [Rosenbaum, Suomela PODC 2020].\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:06:05 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Brandt", "Sebastian", ""], ["Grunau", "Christoph", ""], ["Rozho\u0148", "V\u00e1clav", ""]]}, {"id": "2103.16995", "submitter": "Abdullah Alelyani", "authors": "Abdullah Alelyani, Ghulam Mubasher Hassan and Amitava Datta", "title": "Scheduling Applications on Containers Based on Dependency of The\n  Applications", "comments": "68 pages, 35 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing technology has been one of the most critical developments in\nprovisioning both hardware and software infrastructure in recent years.\nContainer technology is a new cloud technology that boosts the booting of\napplications, increases the ability to deploy applications on containers and\nimproves the host machine resource sharing. Thus, enhancing a cloud container\nsystem needs a robust algorithm that deploys the applications efficiently. Most\nof the schedulers associated with container technology are focused on load\nbalancing for increasing container performance. The traffic over networks plays\na significant role in the performance of containers. Container deployment\nconsidering only load balancing may not be the best scheduling strategy due to\nthe dependency between the applications that might be deployed in different\npods (zones) in the container's cloud. This project aims to develop an\nalgorithm that deploys applications into containers by considering the\ndependencies between applications as well as load balancing. The proposed\nalgorithm performs better in terms of improving the throughput and reducing the\nnetwork traffic as compared to state-of-the-art container scheduling\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:17:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Alelyani", "Abdullah", ""], ["Hassan", "Ghulam Mubasher", ""], ["Datta", "Amitava", ""]]}, {"id": "2103.16998", "submitter": "Georgios Mylonas", "authors": "Aikaterini Deligiannidou, Dimitrios Amaxilatis, Georgios Mylonas,\n  Evangelos Theodoridis", "title": "Knowledge co-creation in the OrganiCity: Data annotation with JAMAiCA", "comments": "Preprint submitted to 2016 IEEE 3rd World Forum on Internet of Things\n  (WF-IoT)", "journal-ref": null, "doi": "10.1109/WF-IoT.2016.7845492", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous smart city testbeds and system deployments have surfaced around the\nworld, aiming to provide services over unified large heterogeneous IoT\ninfrastructures. Although we have achieved new scales in smart city\ninstallations and systems, so far the focus has been to provide diverse sources\nof data to smart city services consumers, while neglecting to provide ways to\nsimplify making good use of them. We believe that knowledge creation in smart\ncities through data annotation, supported in both an automated and a\ncrowdsourced manner, is an aspect that will bring additional value to smart\ncities. We present here our approach, aiming to utilize an existing smart city\ndeployment and the OrganiCity software ecosystem. We discuss key challenges\nalong with characteristic use cases, and report on our design and\nimplementation, along with preliminary results.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:24:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Deligiannidou", "Aikaterini", ""], ["Amaxilatis", "Dimitrios", ""], ["Mylonas", "Georgios", ""], ["Theodoridis", "Evangelos", ""]]}, {"id": "2103.17023", "submitter": "Georgios Mylonas", "authors": "Dimitrios Amaxilatis, Evangelos Lagoudianakis, Georgios Mylonas,\n  Evangelos Theodoridis", "title": "Managing smartphone crowdsensing campaigns through the Organicity smart\n  city platform", "comments": "Preprint submitted to UbiComp/ISWC '16 Adjunct, September 12-16,\n  2016, Heidelberg, Germany", "journal-ref": null, "doi": "10.1145/2968219.2968588", "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We briefly present the design and architecture of a system that aims to\nsimplify the process of organizing, executing and administering crowdsensing\ncampaigns in a smart city context over smartphones volunteered by citizens. We\nbuilt our system on top of an Android app substrate on the end-user level,\nwhich enables us to utilize smartphone resources. Our system allows researchers\nand other developers to manage and distribute their \"mini\" smart city\napplications, gather data and publish their results through the Organicity\nsmart city platform. We believe this is the first time such a tool is paired\nwith a large scale IoT infrastructure, to enable truly city-scale IoT and smart\ncity experimentation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:13:59 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Amaxilatis", "Dimitrios", ""], ["Lagoudianakis", "Evangelos", ""], ["Mylonas", "Georgios", ""], ["Theodoridis", "Evangelos", ""]]}]