[{"id": "1301.0040", "submitter": "EPTCS", "authors": "Peter Hui (Pacific Northwest National Laboratory), Satish Chikkagoudar\n  (Pacific Northwest National Laboratory)", "title": "A Formal Model For Real-Time Parallel Computation", "comments": "In Proceedings FTSCS 2012, arXiv:1212.6574", "journal-ref": "EPTCS 105, 2012, pp. 39-55", "doi": "10.4204/EPTCS.105.4", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The imposition of real-time constraints on a parallel computing environment-\nspecifically high-performance, cluster-computing systems- introduces a variety\nof challenges with respect to the formal verification of the system's timing\nproperties. In this paper, we briefly motivate the need for such a system, and\nwe introduce an automaton-based method for performing such formal verification.\nWe define the concept of a consistent parallel timing system: a hybrid system\nconsisting of a set of timed automata (specifically, timed Buchi automata as\nwell as a timed variant of standard finite automata), intended to model the\ntiming properties of a well-behaved real-time parallel system. Finally, we give\na brief case study to demonstrate the concepts in the paper: a parallel matrix\nmultiplication kernel which operates within provable upper time bounds. We give\nthe algorithm used, a corresponding consistent parallel timing system, and\nempirical results showing that the system operates under the specified timing\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2013 01:54:28 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Hui", "Peter", "", "Pacific Northwest National Laboratory"], ["Chikkagoudar", "Satish", "", "Pacific Northwest National Laboratory"]]}, {"id": "1301.0047", "submitter": "Zaid Towfic", "authors": "Zaid J. Towfic, Jianshu Chen, Ali H. Sayed", "title": "On Distributed Online Classification in the Midst of Concept Drifts", "comments": "19 pages, 14 figures, to appear in Neurocomputing, 2013", "journal-ref": null, "doi": "10.1016/j.neucom.2012.12.043", "report-no": null, "categories": "math.OC cs.DC cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze the generalization ability of distributed online\nlearning algorithms under stationary and non-stationary environments. We derive\nbounds for the excess-risk attained by each node in a connected network of\nlearners and study the performance advantage that diffusion strategies have\nover individual non-cooperative processing. We conduct extensive simulations to\nillustrate the results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2013 02:02:51 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Towfic", "Zaid J.", ""], ["Chen", "Jianshu", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1301.0082", "submitter": "F. Ozgur Catak", "authors": "F. Ozgur Catak and M. Erdal Balaban", "title": "CloudSVM : Training an SVM Classifier in Cloud Computing Systems", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional method, distributed support vector machines (SVM) algorithms\nare trained over pre-configured intranet/internet environments to find out an\noptimal classifier. These methods are very complicated and costly for large\ndatasets. Hence, we propose a method that is referred as the Cloud SVM training\nmechanism (CloudSVM) in a cloud computing environment with MapReduce technique\nfor distributed machine learning applications. Accordingly, (i) SVM algorithm\nis trained in distributed cloud storage servers that work concurrently; (ii)\nmerge all support vectors in every trained cloud node; and (iii) iterate these\ntwo steps until the SVM converges to the optimal classifier function. Large\nscale data sets are not possible to train using SVM algorithm on a single\ncomputer. The results of this study are important for training of large scale\ndata sets for machine learning applications. We provided that iterative\ntraining of splitted data set in cloud computing environment using SVM will\nconverge to a global optimal classifier in finite iteration size.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2013 13:20:27 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Catak", "F. Ozgur", ""], ["Balaban", "M. Erdal", ""]]}, {"id": "1301.0101", "submitter": "Erekle Magradze Msc", "authors": "Julia Andreeva, Carlos Borrego Iglesias, Simone Campana, Alessandro Di\n  Girolamo, Ivan Dzhunov, Xavier Espinal Curull, Stavro Gayazov, Erekle\n  Magradze, Michal Maciej Nowotka, Lorenzo Rinaldi, Pablo Saiz, Jaroslava\n  Schovancova, Graeme Andrew Stewart, Michael Wright", "title": "Automating ATLAS Computing Operations using the Site Status Board", "comments": "The paper has been withdrawn by the author", "journal-ref": "Journal of Physics: Conference Series 396 (2012) 032072", "doi": "10.1088/1742-6596/396/3/032072", "report-no": null, "categories": "cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automation of operations is essential to reduce manpower costs and\nimprove the reliability of the system. The Site Status Board (SSB) is a\nframework which allows Virtual Organizations to monitor their computing\nactivities at distributed sites and to evaluate site performance. The ATLAS\nexperiment intensively uses the SSB for the distributed computing shifts, for\nestimating data processing and data transfer efficiencies at a particular site,\nand for implementing automatic exclusion of sites from computing activities, in\ncase of potential problems. The ATLAS SSB provides a real-time aggregated\nmonitoring view and keeps the history of the monitoring metrics. Based on this\nhistory, usability of a site from the perspective of ATLAS is calculated. The\npaper will describe how the SSB is integrated in the ATLAS operations and\ncomputing infrastructure and will cover implementation details of the ATLAS SSB\nsensors and alarm system, based on the information in the SSB. It will\ndemonstrate the positive impact of the use of the SSB on the overall\nperformance of ATLAS computing activities and will overview future plans.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2013 15:58:07 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2013 13:53:14 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Andreeva", "Julia", ""], ["Iglesias", "Carlos Borrego", ""], ["Campana", "Simone", ""], ["Di Girolamo", "Alessandro", ""], ["Dzhunov", "Ivan", ""], ["Curull", "Xavier Espinal", ""], ["Gayazov", "Stavro", ""], ["Magradze", "Erekle", ""], ["Nowotka", "Michal Maciej", ""], ["Rinaldi", "Lorenzo", ""], ["Saiz", "Pablo", ""], ["Schovancova", "Jaroslava", ""], ["Stewart", "Graeme Andrew", ""], ["Wright", "Michael", ""]]}, {"id": "1301.0523", "submitter": "Helene Cordier", "authors": "Cyril L'Orphelin (CC IN2P3), H\\'el\\`ene Cordier (CC IN2P3), Sylvain\n  Reynaud (CC IN2P3), Marcos Lins (CC IN2P3), Sinika Loikkanen (CC IN2P3),\n  Olivier Lequeux (CC IN2P3), Pierre Veyre (CC IN2P3)", "title": "EELA Operations: A standalone regional dashboard implementation", "comments": "Second EELA-2 Conference, CHORONI : Venezuela, Bolivarian Republic Of\n  (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid operators in EGEE use a dedicated dashboard as their central operational\ntool, stable and scalable for the last 5 years despite continuous upgrade from\nspecifications by users, monitoring tools or data providers. In EGEE-III,\nregionalisation of operations led the tool developers to conceive a standalone\ninstance of this tool. Hereby, we will present the concept and the EELA-II\nimplementation. Indeed, there-engineering of this tool led to an easily\ndeployable package that canconnect to EELA-II specific information sources such\nas EVENTUM, EELAGOCDB like or SAM EELA instance through the three components of\nthepackage: the generic and scalable data access mechanism, Lavoisier;\nthewidely spread php framework Symfony, for configuration flexibility and a\nMysql database.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 17:44:46 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["L'Orphelin", "Cyril", "", "CC IN2P3"], ["Cordier", "H\u00e9l\u00e8ne", "", "CC IN2P3"], ["Reynaud", "Sylvain", "", "CC IN2P3"], ["Lins", "Marcos", "", "CC IN2P3"], ["Loikkanen", "Sinika", "", "CC IN2P3"], ["Lequeux", "Olivier", "", "CC IN2P3"], ["Veyre", "Pierre", "", "CC IN2P3"]]}, {"id": "1301.0762", "submitter": "Helene Cordier", "authors": "H\\'el\\`ene Cordier (CC IN2P3), Cyril L'Orphelin (CC IN2P3), Sylvain\n  Reynaud (CC IN2P3), Olivier Lequeux (CC IN2P3), Sinika Loikkanen (CC IN2P3),\n  Pierre Veyre (CC IN2P3)", "title": "From EGEE OPerations Portal towards EGI OPerations Portal", "comments": "arXiv admin note: text overlap with arXiv:1301.0523", "journal-ref": "ISGC 2010, Taipei : Taiwan, Province Of China (2010)", "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EGEE to EGI structure based on NGIs evolution induces a large move from the\noperations that will rely on a sustainable and largely decentralized model. One\nof the key evolutions for the challenge in the regionalisation relies in the\nscalability and the flexibility required regarding information source types and\ninformation handling. For 5 years, we have developed and maintained a\nstandard-based component that allows us to address both theses issues. This\nopen-source tool, named Lavoisier, has been a critical success factor for the\noperations dashboard, one of the Operations Portal main features. Indeed, it\nenables coherent efficient and reliable data handling which is customizable and\nscalable, as Lavoisier is an extensible service designed to provide a unified\nview of data collected from multiple heterogeneous data sources. Data views are\nrepresented and accessed as XML documents through standard languages such as\nXSLT, XPath. Moreover, scalability and reliability are enforced by a caching\nmechanism adaptable to specific data sources and use-cases. We will namely\nexpose how the concept and the implementation enable clear roles separation\nbetween plug-in developer, service configuration administrator or end-user.\nAlso, maintainability of the portal code has increased dramatically as the\nlatter is now independent from the data sources technology or from the cache\nmanagement policies. Finally, integration of data has recently been simplified\nas the service administrator proceeds now through web interfaces\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2013 16:23:48 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Cordier", "H\u00e9l\u00e8ne", "", "CC IN2P3"], ["L'Orphelin", "Cyril", "", "CC IN2P3"], ["Reynaud", "Sylvain", "", "CC IN2P3"], ["Lequeux", "Olivier", "", "CC IN2P3"], ["Loikkanen", "Sinika", "", "CC IN2P3"], ["Veyre", "Pierre", "", "CC IN2P3"]]}, {"id": "1301.1071", "submitter": "Austin Benson", "authors": "Austin R. Benson, David F. Gleich, James Demmel", "title": "Direct QR factorizations for tall-and-skinny matrices in MapReduce\n  architectures", "comments": null, "journal-ref": "Proceedings of the IEEE International Conference on Big Data, 2013", "doi": "10.1109/BigData.2013.6691583", "report-no": null, "categories": "cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The QR factorization and the SVD are two fundamental matrix decompositions\nwith applications throughout scientific computing and data analysis. For\nmatrices with many more rows than columns, so-called \"tall-and-skinny\nmatrices,\" there is a numerically stable, efficient, communication-avoiding\nalgorithm for computing the QR factorization. It has been used in traditional\nhigh performance computing and grid computing environments. For MapReduce\nenvironments, existing methods to compute the QR decomposition use a\nnumerically unstable approach that relies on indirectly computing the Q factor.\nIn the best case, these methods require only two passes over the data. In this\npaper, we describe how to compute a stable tall-and-skinny QR factorization on\na MapReduce architecture in only slightly more than 2 passes over the data. We\ncan compute the SVD with only a small change and no difference in performance.\nWe present a performance comparison between our new direct TSQR method, a\nstandard unstable implementation for MapReduce (Cholesky QR), and the classic\nstable algorithm implemented for MapReduce (Householder QR). We find that our\nnew stable method has a large performance advantage over the Householder QR\nmethod. This holds both in a theoretical performance model as well as in an\nactual implementation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2013 22:56:36 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Benson", "Austin R.", ""], ["Gleich", "David F.", ""], ["Demmel", "James", ""]]}, {"id": "1301.1215", "submitter": "Sebastian Schaetz", "authors": "Sebastian Schaetz and Martin Uecker", "title": "A Multi-GPU Programming Library for Real-Time Applications", "comments": "15 pages, 10 figures", "journal-ref": "Algorithms and Architectures for Parallel Processing 7439 (2012)\n  114-128", "doi": "10.1007/978-3-642-33078-0_9", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MGPU, a C++ programming library targeted at single-node multi-GPU\nsystems. Such systems combine disproportionate floating point performance with\nhigh data locality and are thus well suited to implement real-time algorithms.\nWe describe the library design, programming interface and implementation\ndetails in light of this specific problem domain. The core concepts of this\nwork are a novel kind of container abstraction and MPI-like communication\nmethods for intra-system communication. We further demonstrate how MGPU is used\nas a framework for porting existing GPU libraries to multi-device\narchitectures. Putting our library to the test, we accelerate an iterative\nnon-linear image reconstruction algorithm for real-time magnetic resonance\nimaging using multiple GPUs. We achieve a speed-up of about 1.7 using 2 GPUs\nand reach a final speed-up of 2.1 with 4 GPUs. These promising results lead us\nto conclude that multi-GPU systems are a viable solution for real-time MRI\nreconstruction as well as signal-processing applications in general.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 14:50:24 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Schaetz", "Sebastian", ""], ["Uecker", "Martin", ""]]}, {"id": "1301.1549", "submitter": "Bernat Gaston", "authors": "Bernat Gast\\'on, Jaume Pujol, and Merc\\`e Villanueva", "title": "A realistic distributed storage system that minimizes data storage and\n  repair bandwidth", "comments": "10 pages, accepted as a poster in the Data Compression Conference\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a realistic distributed storage environment, storage nodes are usually\nplaced in racks, a metallic support designed to accommodate electronic\nequipment. It is known that the communication (bandwidth) cost between nodes\nwithin a rack is much lower than the communication (bandwidth) cost between\nnodes within different racks.\n  In this paper, a new model, where the storage nodes are placed in two racks,\nis proposed and analyzed. In this model, the storage nodes have different\nrepair costs to repair a node depending on the rack where they are placed. A\nthreshold function, which minimizes the amount of stored data per node and the\nbandwidth needed to regenerate a failed node, is shown. This threshold function\ngeneralizes the threshold function from previous distributed storage models.\nThe tradeoff curve obtained from this threshold function is compared with the\nones obtained from the previous models, and it is shown that this new model\noutperforms the previous ones in terms of repair cost.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 14:46:49 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Gast\u00f3n", "Bernat", ""], ["Pujol", "Jaume", ""], ["Villanueva", "Merc\u00e8", ""]]}, {"id": "1301.1704", "submitter": "Qi Hu", "authors": "Qi Hu, Nail A. Gumerov, Ramani Duraiswami", "title": "Parallel Algorithms for Constructing Data Structures for Fast Multipole\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient algorithms to build data structures and the lists needed\nfor fast multipole methods. The algorithms are capable of being efficiently\nimplemented on both serial, data parallel GPU and on distributed architectures.\nWith these algorithms it is possible to map the FMM efficiently on to the GPU\nor distributed heterogeneous CPU-GPU systems. Further, in dynamic problems, as\nthe distribution of the particles change, the reduced cost of building the data\nstructures improves performance. Using these algorithms, we demonstrate example\nhigh fidelity simulations with large problem sizes by using FMM on both single\nand multiple heterogeneous computing facilities equipped with multi-core CPU\nand many-core GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 21:57:20 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Hu", "Qi", ""], ["Gumerov", "Nail A.", ""], ["Duraiswami", "Ramani", ""]]}, {"id": "1301.1714", "submitter": "Teruyoshi Washizawa", "authors": "Teruyoshi Washizawa, Yasuhiro Nakahara", "title": "Parallel Computing of Discrete Element Method on GPU", "comments": "3 tables", "journal-ref": "Applied Mathematics, vol.4, no.1A, pp.242-247, (January 2013)", "doi": "10.4236/am.2013.41A037", "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate applicability of GPU to DEM. NVIDIA's code obtained superior\nperformance than CPU in computational time. A model of contact forces in\nNVIDIA's code is too simple for practical use. We modify this model by\nreplacing it with the practical model. The simulation shows that the practical\nmodel obtains the computing speed 6 times faster than the practical one on CPU\nwhile 7 times slower than the simple one on GPU. The result are analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 23:09:49 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Washizawa", "Teruyoshi", ""], ["Nakahara", "Yasuhiro", ""]]}, {"id": "1301.2130", "submitter": "Sophie Fosson", "authors": "Chiara Ravazzi, Sophie M. Fosson, and Enrico Magli", "title": "Distributed soft thresholding for sparse signal recovery", "comments": "Revised version. Main improvements: extension of the convergence\n  theorem to regular graphs; new numerical results and comparisons with other\n  algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT math.OC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we address the problem of distributed sparse recovery of\nsignals acquired via compressed measurements in a sensor network. We propose a\nnew class of distributed algorithms to solve Lasso regression problems, when\nthe communication to a fusion center is not possible, e.g., due to\ncommunication cost or privacy reasons. More precisely, we introduce a\ndistributed iterative soft thresholding algorithm (DISTA) that consists of\nthree steps: an averaging step, a gradient step, and a soft thresholding\noperation. We prove the convergence of DISTA in networks represented by regular\ngraphs, and we compare it with existing methods in terms of performance,\nmemory, and complexity.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 14:16:33 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 09:20:56 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Ravazzi", "Chiara", ""], ["Fosson", "Sophie M.", ""], ["Magli", "Enrico", ""]]}, {"id": "1301.2648", "submitter": "Y.F. Diao", "authors": "Yingfei Diao, Zhiyun Lin, Minyue Fu and Huanshui Zhang", "title": "A New Distributed Localization Method for Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of determining the sensor locations in a large\nsensor network using relative distance (range) measurements only. Our work\nfollows from a seminal paper by Khan et al. [1] where a distributed algorithm,\nknown as DILOC, for sensor localization is given using the barycentric\ncoordinate. A main limitation of the DILOC algorithm is that all sensor nodes\nmust be inside the convex hull of the anchor nodes. In this paper, we consider\na general sensor network without the convex hull assumption, which incurs\nchallenges in determining the sign pattern of the barycentric coordinate. A\ncriterion is developed to address this issue based on available distance\nmeasurements. Also, a new distributed algorithm is proposed to guarantee the\nasymptotic localization of all localizable sensor nodes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 05:06:35 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Diao", "Yingfei", ""], ["Lin", "Zhiyun", ""], ["Fu", "Minyue", ""], ["Zhang", "Huanshui", ""]]}, {"id": "1301.2649", "submitter": "Amirreza Zarrabi", "authors": "Amirreza Zarrabi", "title": "Dynamic Transparent General Purpose Process Migration For Linux", "comments": null, "journal-ref": "International Journal of Grid Computing & Applications (IJGCA)\n  Vol.3, No.4, December 2012", "doi": "10.5121/ijgca.2012.3402", "report-no": null, "categories": "cs.DC cs.OS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process migration refers to the act of transferring a process in the middle\nof its execution from one machine to another in a network. In this paper, we\nproposed a process migration framework for Linux OS. It is a multilayer\narchitecture to confine every functionality independent section of the system\nin separate layer. This architecture is capable of supporting diverse\napplications due to generic user space interface and dynamic structure that can\nbe modified according to demands.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 05:31:57 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Zarrabi", "Amirreza", ""]]}, {"id": "1301.2689", "submitter": "Thirumurugan Shomasundaram", "authors": "S. Thirumurugan, E. George Dharma Prakash Raj", "title": "An Extended Weighted Partitioning Around Cluster Head Mechanism for Ad\n  Hoc Network", "comments": "IJASUC 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wireless network places vital role in the present day communication\nscenario. The ad hoc nature of wireless communication adds flavour to suit\nvarious real world applications. This improves the performance of the network\ntremendously while the clustering mechanism gets added to the ad hoc network.\nIt has been found out that the existing WCA lacks in forming efficient\nclusters. Thus, this work proposes an Extended weighted partitioning around\ncluster head mechanism by considering W-PAC as a base to form clusters. The\ncluster members are configured with IPv6 address. This IPv6 clusters formed\nthrough W-PAC will be taken further for validation to determine the perfectness\nof clusters. The cluster formation and maintenance have been implemented in C++\nas a programming language. The cluster validation has been carried out using\nOMNET++ simulator.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 14:08:52 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Thirumurugan", "S.", ""], ["Raj", "E. George Dharma Prakash", ""]]}, {"id": "1301.2875", "submitter": "Alexandre Maurer", "authors": "Alexandre Maurer (LIP6, LINCS), S\\'ebastien Tixeuil (LIP6, LINCS, IUF)", "title": "On Byzantine Broadcast in Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reliably broadcasting information in a multihop\nasynchronous network in the presence of Byzantine failures: some nodes may\nexhibit unpredictable malicious behavior. We focus on completely decentralized\nsolutions. Few Byzantine-robust algorithms exist for loosely connected\nnetworks. A recent solution guarantees reliable broadcast on a torus when D >\n4, D being the minimal distance between two Byzantine nodes. In this paper, we\ngeneralize this result to 4-connected planar graphs. We show that reliable\nbroadcast can be guaranteed when D > Z, Z being the maximal number of edges per\npolygon. We also show that this bound on D is a lower bound for this class of\ngraphs. Our solution has the same time complexity as a simple broadcast. This\nis also the first solution where the memory required increases linearly\n(instead of exponentially) with the size of transmitted information. Important\ndisclaimer: these results have NOT yet been published in an international\nconference or journal. This is just a technical report presenting intermediary\nand incomplete results. A generalized version of these results may be under\nsubmission.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 07:44:22 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2013 07:35:36 GMT"}, {"version": "v3", "created": "Sat, 9 Feb 2013 11:14:16 GMT"}, {"version": "v4", "created": "Sat, 7 Dec 2013 19:12:38 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Maurer", "Alexandre", "", "LIP6, LINCS"], ["Tixeuil", "S\u00e9bastien", "", "LIP6, LINCS, IUF"]]}, {"id": "1301.2976", "submitter": "Ran Wolff", "authors": "Ran Wolff", "title": "Local Thresholding on Distributed Hash Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a binary routing tree protocol for distributed hash table\noverlays. Using this protocol each peer can independently route messages to its\nparent and two descendants on the fly without any maintenance, global context,\nand synchronization. The protocol is then extended to support tree change\nnotification with similar efficiency. The resulting tree is almost perfectly\ndense and balanced, and has O(1) stretch if the distributed hash table is\nsymmetric Chord. We use the tree routing protocol to overcome the main\nimpediment for implementation of local thresholding algorithms in peer-to-peer\nsystems -- their requirement for cycle free routing. Direct comparison of a\ngossip-based algorithm and a corresponding local thresholding algorithm on a\nmajority voting problem reveals that the latter obtains superior accuracy using\na fraction of the communication overhead.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 13:56:17 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Wolff", "Ran", ""]]}, {"id": "1301.3007", "submitter": "Dohy Hong", "authors": "Dohy Hong, Fabien Mathieu and G\\'erard Burnside", "title": "Convergence of the D-iteration algorithm: convergence rate and\n  asynchronous distributed scheme", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define the general framework to describe the diffusion\noperators associated to a positive matrix. We define the equations associated\nto diffusion operators and present some general properties of their state\nvectors. We show how this can be applied to prove and improve the convergence\nof a fixed point problem associated to the matrix iteration scheme, including\nfor distributed computation framework. The approach can be understood as a\ndecomposition of the matrix-vector product operation in elementary operations\nat the vector entry level.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 15:11:08 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Hong", "Dohy", ""], ["Mathieu", "Fabien", ""], ["Burnside", "G\u00e9rard", ""]]}, {"id": "1301.3118", "submitter": "Qasim  Nasar-Ullah", "authors": "Qasim Nasar-Ullah", "title": "A parallel implementation of a derivative pricing model incorporating\n  SABR calibration and probability lookup tables", "comments": "21 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a high performance parallel implementation of a derivative\npricing model, within which we introduce a new parallel method for the\ncalibration of the industry standard SABR (stochastic-\\alpha \\beta \\rho)\nstochastic volatility model using three strike inputs. SABR calibration\ninvolves a non-linear three dimensional minimisation and parallelisation is\nachieved by incorporating several assumptions unique to the SABR class of\nmodels. Our calibration method is based on principles of surface intersection,\nguarantees convergence to a unique solution and operates by iteratively\nrefining a two dimensional grid with local mesh refinement. As part of our\npricing model we additionally present a fast parallel iterative algorithm for\nthe creation of dynamically sized cumulative probability lookup tables that are\nable to cap maximum estimated linear interpolation error. We optimise\nperformance for probability distributions that exhibit clustering of linear\ninterpolation error. We also make an empirical assessment of error propagation\nthrough our pricing model as a result of changes in accuracy parameters within\nthe pricing model's multiple algorithmic steps. Algorithms are implemented on a\nGPU (graphics processing unit) using Nvidia's Fermi architecture. The pricing\nmodel targets the evaluation of spread options using copula methods, however\nthe presented algorithms can be applied to a wider class of financial\ninstruments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 20:33:00 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Nasar-Ullah", "Qasim", ""]]}, {"id": "1301.3223", "submitter": "Allison Lewko", "authors": "Allison Lewko and Mark Lewko", "title": "On the Complexity of Asynchronous Agreement Against Powerful Adversaries", "comments": "18 pages, updated discussion of related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new techniques for proving lower bounds on the running time of\nrandomized algorithms for asynchronous agreement against powerful adversaries.\nIn particular, we define a \\emph{strongly adaptive adversary} that is\ncomputationally unbounded and has a limited ability to corrupt a dynamic subset\nof processors by erasing their memories. We demonstrate that the randomized\nagreement algorithms designed by Ben-Or and Bracha to tolerate crash or\nByzantine failures in the asynchronous setting extend to defeat a strongly\nadaptive adversary. These algorithms have essentially perfect correctness and\ntermination, but at the expense of exponential running time. In the case of the\nstrongly adaptive adversary, we show that this dismally slow running time is\n\\emph{inherent}: we prove that any algorithm with essentially perfect\ncorrectness and termination against the strongly adaptive adversary must have\nexponential running time. We additionally interpret this result as yielding an\nenhanced understanding of the tools needed to simultaneously achieving perfect\ncorrectness and termination as well as fast running time for randomized\nalgorithms tolerating crash or Byzantine failures.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 04:39:01 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2013 14:01:03 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2013 01:35:22 GMT"}, {"version": "v4", "created": "Wed, 12 Jun 2013 04:31:55 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Lewko", "Allison", ""], ["Lewko", "Mark", ""]]}, {"id": "1301.3791", "submitter": "Maheswaran Sathiamoorthy", "authors": "Maheswaran Sathiamoorthy, Megasthenis Asteris, Dimitris\n  Papailiopoulos, Alexandros G. Dimakis, Ramkumar Vadali, Scott Chen, Dhruba\n  Borthakur", "title": "XORing Elephants: Novel Erasure Codes for Big Data", "comments": "Technical report, paper to appear in Proceedings of VLDB, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage systems for large clusters typically use replication to\nprovide reliability. Recently, erasure codes have been used to reduce the large\nstorage overhead of three-replicated systems. Reed-Solomon codes are the\nstandard design choice and their high repair cost is often considered an\nunavoidable price to pay for high storage efficiency and high reliability.\n  This paper shows how to overcome this limitation. We present a novel family\nof erasure codes that are efficiently repairable and offer higher reliability\ncompared to Reed-Solomon codes. We show analytically that our codes are optimal\non a recently identified tradeoff between locality and minimum distance.\n  We implement our new codes in Hadoop HDFS and compare to a currently deployed\nHDFS module that uses Reed-Solomon codes. Our modified HDFS implementation\nshows a reduction of approximately 2x on the repair disk I/O and repair network\ntraffic. The disadvantage of the new coding scheme is that it requires 14% more\nstorage compared to Reed-Solomon codes, an overhead shown to be information\ntheoretically optimal to obtain locality. Because the new codes repair failures\nfaster, this provides higher reliability, which is orders of magnitude higher\ncompared to replication.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 18:51:15 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Sathiamoorthy", "Maheswaran", ""], ["Asteris", "Megasthenis", ""], ["Papailiopoulos", "Dimitris", ""], ["Dimakis", "Alexandros G.", ""], ["Vadali", "Ramkumar", ""], ["Chen", "Scott", ""], ["Borthakur", "Dhruba", ""]]}, {"id": "1301.3996", "submitter": "Alexandre Maurer", "authors": "Alexandre Maurer (LIP6, LINCS), S\\'ebastien Tixeuil (LIP6, LINCS, IUF)", "title": "Parameterizable Byzantine Broadcast in Loosely Connected Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reliably broadcasting information in a multihop\nasynchronous network, despite the presence of Byzantine failures: some nodes\nare malicious and behave arbitrarly. We focus on non-cryptographic solutions.\nMost existing approaches give conditions for perfect reliable broadcast (all\ncorrect nodes deliver the good information), but require a highly connected\nnetwork. A probabilistic approach was recently proposed for loosely connected\nnetworks: the Byzantine failures are randomly distributed, and the correct\nnodes deliver the good information with high probability. A first solution\nrequire the nodes to initially know their position on the network, which may be\ndifficult or impossible in self-organizing or dynamic networks. A second\nsolution relaxed this hypothesis but has much weaker Byzantine tolerance\nguarantees. In this paper, we propose a parameterizable broadcast protocol that\ndoes not require nodes to have any knowledge about the network. We give a\ndeterministic technique to compute a set of nodes that always deliver authentic\ninformation, for a given set of Byzantine failures. Then, we use this technique\nto experimentally evaluate our protocol, and show that it significantely\noutperforms previous solutions with the same hypotheses. Important disclaimer:\nthese results have NOT yet been published in an international conference or\njournal. This is just a technical report presenting intermediary and incomplete\nresults. A generalized version of these results may be under submission.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 07:09:15 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2013 07:36:54 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Maurer", "Alexandre", "", "LIP6, LINCS"], ["Tixeuil", "S\u00e9bastien", "", "LIP6, LINCS, IUF"]]}, {"id": "1301.4019", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray, Anthony Lee and Pierre E. Jacob", "title": "Parallel resampling in the particle filter", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern parallel computing devices, such as the graphics processing unit\n(GPU), have gained significant traction in scientific and statistical\ncomputing. They are particularly well-suited to data-parallel algorithms such\nas the particle filter, or more generally Sequential Monte Carlo (SMC), which\nare increasingly used in statistical inference. SMC methods carry a set of\nweighted particles through repeated propagation, weighting and resampling\nsteps. The propagation and weighting steps are straightforward to parallelise,\nas they require only independent operations on each particle. The resampling\nstep is more difficult, as standard schemes require a collective operation,\nsuch as a sum, across particle weights. Focusing on this resampling step, we\nanalyse two alternative schemes that do not involve a collective operation\n(Metropolis and rejection resamplers), and compare them to standard schemes\n(multinomial, stratified and systematic resamplers). We find that, in certain\ncircumstances, the alternative resamplers can perform significantly faster on a\nGPU, and to a lesser extent on a CPU, than the standard approaches. Moreover,\nin single precision, the standard approaches are numerically biased for upwards\nof hundreds of thousands of particles, while the alternatives are not. This is\nparticularly important given greater single- than double-precision throughput\non modern devices, and the consequent temptation to use single precision with a\ngreater number of particles. Finally, we provide auxiliary functions useful for\nimplementation, such as for the permutation of ancestry vectors to enable\nin-place propagation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 09:14:43 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 06:58:30 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2015 11:32:37 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Murray", "Lawrence M.", ""], ["Lee", "Anthony", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1301.4200", "submitter": "Fabian Hueske", "authors": "Fabian Hueske, Aljoscha Krettek, Kostas Tzoumas", "title": "Enabling Operator Reordering in Data Flow Programs Through Static Code\n  Analysis", "comments": "4 pages, accepted and presented at the First International Workshop\n  on Cross-model Language Design and Implementation (XLDI), affiliated with\n  ICFP 2012, Copenhagen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many massively parallel data management platforms, programs are\nrepresented as small imperative pieces of code connected in a data flow. This\npopular abstraction makes it hard to apply algebraic reordering techniques\nemployed by relational DBMSs and other systems that use an algebraic\nprogramming abstraction. We present a code analysis technique based on reverse\ndata and control flow analysis that discovers a set of properties from user\ncode, which can be used to emulate algebraic optimizations in this setting.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 19:42:55 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Hueske", "Fabian", ""], ["Krettek", "Aljoscha", ""], ["Tzoumas", "Kostas", ""]]}, {"id": "1301.4204", "submitter": "Rajeev  Shakya PhD Scholar", "authors": "Rajeev K. Shakya, Satyam Agarwal, Y. N. Singh, Nishchal K. Verma, and\n  Amitabha Roy", "title": "DSAT-MAC : Dynamic Slot Allocation based TDMA MAC protocol for Cognitive\n  Radio Networks", "comments": "19 pages, 20 figures, Initial work in proc. of the Ninth IEEE\n  International Conference on Wireless and Optical Communications Networks\n  (IEEE WOCN-2012), Indore, INDIA, 20-22 September, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Radio Networks (CRN) have enabled us to efficiently reuse the\nunderutilized radio spectrum. The MAC protocol in CRN defines the spectrum\nusage by sharing the channels efficiently among users. In this paper we propose\na novel TDMA based MAC protocol with dynamically allocated slots. Most of the\nMAC protocols proposed in the literature employ Common Control Channel (CCC) to\nmanage the resources among Cognitive Radio (CR) users. Control channel\nsaturation in case of large number of CR users is one of the main drawbacks of\nthe CCC based MAC protocols. In contrast with CCC based MAC protocols, DSAT-MAC\nprotocol is based on the TDMA mechanism, without using any CCC for control\ninformation exchange. The channels are divided into time slots and CR users\nsend their control or data packets over their designated slot. The protocol\nensures that no slot is left vacant. This guarantees full use of the available\nspectrum. The protocol includes the provision for Quality of Service, where\nreal-time and safety critical data is transmitted with highest priority and\nleast delay. The protocol also ensures a fair sharing of available spectrum\namong the CR users, with the mechanism to regulate the transmission of\nmalicious nodes. Energy saving techniques are also presented for longer life of\nbattery operated CR nodes. Theoretical analysis and simulations over ns-2 of\nthe proposed protocol reveal that the protocol performs better in various CR\nadhoc network applications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 19:58:27 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Shakya", "Rajeev K.", ""], ["Agarwal", "Satyam", ""], ["Singh", "Y. N.", ""], ["Verma", "Nishchal K.", ""], ["Roy", "Amitabha", ""]]}, {"id": "1301.4490", "submitter": "Bharath Ramesh", "authors": "Bharath Ramesh and Calvin J. Ribbens and Srinidhi Varadarajan", "title": "Regional Consistency: Programmability and Performance for\n  Non-Cache-Coherent Systems", "comments": "8 pages, 7 figures, 1 table; as submitted to CCGRID 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel programmers face the often irreconcilable goals of programmability\nand performance. HPC systems use distributed memory for scalability, thereby\nsacrificing the programmability advantages of shared memory programming models.\nFurthermore, the rapid adoption of heterogeneous architectures, often with\nnon-cache-coherent memory systems, has further increased the challenge of\nsupporting shared memory programming models. Our primary objective is to define\na memory consistency model that presents the familiar thread-based shared\nmemory programming model, but allows good application performance on\nnon-cache-coherent systems, including distributed memory clusters and\naccelerator-based systems. We propose regional consistency (RegC), a new\nconsistency model that achieves this objective. Results on up to 256 processors\nfor representative benchmarks demonstrate the potential of RegC in the context\nof our prototype distributed shared memory system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 20:40:42 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Ramesh", "Bharath", ""], ["Ribbens", "Calvin J.", ""], ["Varadarajan", "Srinidhi", ""]]}, {"id": "1301.4539", "submitter": "Olivier Cessenat", "authors": "Olivier Cessenat (CEA CESTA)", "title": "Sophie, an FDTD code on the way to multicore, getting rid of the memory\n  bandwidth bottleneck better using cache", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FDTD codes, such as Sophie developed at CEA/DAM, no longer take advantage of\nthe processor's increased computing power, especially recently with the raising\nmulticore technology. This is rooted in the fact that low order numerical\nschemes need an important memory bandwidth to bring and store the computed\nfields. The aim of this article is to present a programming method at the\nsoftware's architecture level that improves the memory access pattern in order\nto reuse data in cache instead of constantly accessing RAM memory. We will\nexhibit a more than two computing time improvement in practical applications.\nThe target audience of this article is made of computing scientists and of\nelectrical engineers that develop simulation codes with no specific knowledge\nin computer science or electronics.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2013 08:13:27 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Cessenat", "Olivier", "", "CEA CESTA"]]}, {"id": "1301.4738", "submitter": "Yaqin Zhou", "authors": "Yaqin Zhou, Xiangyang Li, Min Liu, Xufei Mao, Shaojie Tang, Zhongcheng\n  Li", "title": "Throughput Optimizing Localized Link Scheduling for Multihop Wireless\n  Networks Under Physical Interference Model", "comments": "A earlier work \"Distributed Link Scheduling for Throughput\n  Maximization under Physical Interference Model\" is presented in Proc. IEEE\n  Infocom 2012", "journal-ref": null, "doi": "10.1109/TPDS.2013.210", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study throughput-optimum localized link scheduling in wireless networks.\nThe majority of results on link scheduling assume binary interference models\nthat simplify interference constraints in actual wireless communication. While\nthe physical interference model reflects the physical reality more precisely,\nthe problem becomes notoriously harder under the physical interference model.\nThere have been just a few existing results on link scheduling under the\nphysical interference model, and even fewer on more practical distributed or\nlocalized scheduling. In this paper, we tackle the challenges of localized link\nscheduling posed by the complex physical interference constraints. By\ncooperating the partition and shifting strategies into the pick-and-compare\nscheme, we present a class of localized scheduling algorithms with provable\nthroughput guarantee subject to physical interference constraints. The\nalgorithm in the linear power setting is the first localized algorithm that\nachieves at least a constant fraction of the optimal capacity region subject to\nphysical interference constraints. The algorithm in the uniform power setting\nis the first localized algorithm with a logarithmic approximation ratio to the\noptimal solution. Our extensive simulation results demonstrate correctness and\nperformance efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 02:57:50 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2013 14:40:20 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhou", "Yaqin", ""], ["Li", "Xiangyang", ""], ["Liu", "Min", ""], ["Mao", "Xufei", ""], ["Tang", "Shaojie", ""], ["Li", "Zhongcheng", ""]]}, {"id": "1301.4753", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Javid Taheri, Albert Y.Zomaya", "title": "Pattern Matching for Self- Tuning of MapReduce Jobs", "comments": "7 pages, previously published as \"On Using Pattern Matching\n  Algorithms in MapReduce Applications\" at ISPA 2011. arXiv admin note:\n  substantial text overlap with arXiv:1112.5505", "journal-ref": null, "doi": "10.1109/ISPA.2011.24", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study CPU utilization time patterns of several MapReduce\napplications. After extracting running patterns of several applications, they\nare saved in a reference database to be later used to tweak system parameters\nto efficiently execute unknown applications in future. To achieve this goal,\nCPU utilization patterns of new applications are compared with the already\nknown ones in the reference database to find/predict their most probable\nexecution patterns. Because of different patterns lengths, the Dynamic Time\nWarping (DTW) is utilized for such comparison; a correlation analysis is then\napplied to DTWs outcomes to produce feasible similarity patterns. Three real\napplications (WordCount, Exim Mainlog parsing and Terasort) are used to\nevaluate our hypothesis in tweaking system parameters in executing similar\napplications. Results were very promising and showed effectiveness of our\napproach on pseudo-distributed MapReduce platforms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 04:57:05 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Taheri", "Javid", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1301.4800", "submitter": "Omar Kermia", "authors": "Omar Kermia", "title": "Schedulability Analysis of Distributed Real-Time Applications under\n  Dependence and Several Latency Constraints", "comments": "8 pages, 6 figures, Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 62(14):1-7, January\n  2013. Published by Foundation of Computer Science, New York, USA", "doi": "10.5120/10145-4978", "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the analysis of real-time non preemptive multiprocessor\nscheduling with precedence and several latency constraints. It aims to specify\na schedulability condition which enables a designer to check a priori -without\nexecuting or simulating- if its scheduling of tasks will hold the precedences\nbetween tasks as well as several latency constraints imposed on determined\npairs of tasks. It is shown that the required analysis is closely linked to the\ntopological structure of the application graph. More precisely, it depends on\nthe configuration of tasks paths subject to latency constraints. As a result of\nthe study, a sufficient schedulability condition is introduced for precedences\nand latency constraints in the hardest configuration in term of complexity with\nan optimal number of processors in term of applications parallelism. In\naddition, the proposed conditions provides a practical lower bounds for general\ncases. Performances results and comparisons with an optimal approach\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 10:00:22 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Kermia", "Omar", ""]]}, {"id": "1301.4839", "submitter": "Adrian Klein", "authors": "Adrian Klein and Fuyuki Ishikawa and Shinichi Honiden", "title": "A Scalable Distributed Architecture for Network- and QoS-aware Service\n  Composition", "comments": "8 pages, 12 figures. This paper has been accepted and published at\n  the 3rd International Joint Agent Workshop and Symposium (IJAWS) 2012,\n  Kakegawa, Shizuoka, Japan, October 2012. This version has been copy-edited\n  for publication at arXiv.org, but left unchanged besides. Refer to\n  http://www.adrianobits.de for the original submission in the IJAWS format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service-Oriented Computing (SOC) enables the composition of loosely coupled\nservice agents provided with varying Quality of Service (QoS) levels,\neffectively forming a multiagent system (MAS). Selecting a (near-)optimal set\nof services for a composition in terms of QoS is crucial when many functionally\nequivalent services are available. As the number of distributed services,\nespecially in the cloud, is rising rapidly, the impact of the network on the\nQoS keeps increasing. Despite this and opposed to most MAS approaches, current\nservice approaches depend on a centralized architecture which cannot adapt to\nthe network. Thus, we propose a scalable distributed architecture composed of a\nflexible number of distributed control nodes. Our architecture requires no\nchanges to existing services and adapts from a centralized to a completely\ndistributed realization by adding control nodes as needed. Also, we propose an\nextended QoS aggregation algorithm that allows to accurately estimate network\nQoS. Finally, we evaluate the benefits and optimality of our architecture in a\ndistributed environment.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 12:25:16 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Klein", "Adrian", ""], ["Ishikawa", "Fuyuki", ""], ["Honiden", "Shinichi", ""]]}, {"id": "1301.5121", "submitter": "Alex Averbuch", "authors": "Alex Averbuch, Martin Neumann", "title": "Partitioning Graph Databases - A Quantitative Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic data is growing at increasing rates, in both size and\nconnectivity: the increasing presence of, and interest in, relationships\nbetween data. An example is the Twitter social network graph. Due to this\ngrowth demand is increasing for technologies that can process such data.\nCurrently relational databases are the predominant technology, but they are\npoorly suited to processing connected data as they are optimized for\nindex-intensive operations. Conversely, graph databases are optimized for graph\ncomputation. They link records by direct references, avoiding index lookups,\nand enabling retrieval of adjacent elements in constant time, regardless of\ngraph size. However, as data volume increases these databases outgrow the\nresources of one computer and data partitioning becomes necessary. We evaluate\nthe viability of using graph partitioning algorithms to partition graph\ndatabases. A prototype partitioned database was developed. Three partitioning\nalgorithms explored and one implemented. Three graph datasets were used: two\nreal and one synthetically generated. These were partitioned in various ways\nand the impact on database performance measured. We defined one synthetic\naccess pattern per dataset and executed each on the partitioned datasets.\nEvaluation took place in a simulation environment, ensuring repeatability and\nallowing measurement of metrics like network traffic and load balance. Results\nshow that compared to random partitioning the partitioning algorithm reduced\ntraffic by 40-90%. Executing the algorithm intermittently during usage\nmaintained partition quality, while requiring only 1% the computation of\ninitial partitioning. Strong correlations were found between theoretic quality\nmetrics and generated network traffic under non-uniform access patterns.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 09:48:34 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Averbuch", "Alex", ""], ["Neumann", "Martin", ""]]}, {"id": "1301.5887", "submitter": "Tamara Kolda", "authors": "Tamara G. Kolda and Ali Pinar and Todd Plantenga and C. Seshadhri and\n  Christine Task", "title": "Counting Triangles in Massive Graphs with MapReduce", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing, Vol. 36, No. 5, pp. S44-S77,\n  October 2014", "doi": "10.1137/13090729X", "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs and networks are used to model interactions in a variety of contexts.\nThere is a growing need to quickly assess the characteristics of a graph in\norder to understand its underlying structure. Some of the most useful metrics\nare triangle-based and give a measure of the connectedness of mutual friends.\nThis is often summarized in terms of clustering coefficients, which measure the\nlikelihood that two neighbors of a node are themselves connected. Computing\nthese measures exactly for large-scale networks is prohibitively expensive in\nboth memory and time. However, a recent wedge sampling algorithm has proved\nsuccessful in efficiently and accurately estimating clustering coefficients. In\nthis paper, we describe how to implement this approach in MapReduce to deal\nwith massive graphs. We show results on publicly-available networks, the\nlargest of which is 132M nodes and 4.7B edges, as well as artificially\ngenerated networks (using the Graph500 benchmark), the largest of which has\n240M nodes and 8.5B edges. We can estimate the clustering coefficient by degree\nbin (e.g., we use exponential binning) and the number of triangles per bin, as\nwell as the global clustering coefficient and total number of triangles, in an\naverage of 0.33 seconds per million edges plus overhead (approximately 225\nseconds total for our configuration). The technique can also be used to study\ntriangle statistics such as the ratio of the highest and lowest degree, and we\nhighlight differences between social and non-social networks. To the best of\nour knowledge, these are the largest triangle-based graph computations\npublished to date.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 20:32:25 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 21:45:16 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2013 20:37:01 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Kolda", "Tamara G.", ""], ["Pinar", "Ali", ""], ["Plantenga", "Todd", ""], ["Seshadhri", "C.", ""], ["Task", "Christine", ""]]}, {"id": "1301.5914", "submitter": "Weihua Geng", "authors": "Weihua Geng", "title": "Parallel Higher-order Boundary Integral Electrostatics Computation on\n  Molecular Surfaces with Curved Triangulation", "comments": "arXiv admin note: text overlap with arXiv:1301.5885", "journal-ref": null, "doi": "10.1016/j.jcp.2013.01.029", "report-no": null, "categories": "math.NA cs.DC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a parallel higher-order boundary integral method to\nsolve the linear Poisson-Boltzmann (PB) equation. In our method, a well-posed\nboundary integral formulation is used to ensure the fast convergence of Krylov\nsubspace linear solver such as GMRES. The molecular surfaces are first\ndiscretized with flat triangles and then converted to curved triangles with the\nassistance of normal information at vertices. To maintain the desired accuracy,\nfour-point Gauss-Radau quadratures are used on regular triangles and\nsixteen-point Gauss-Legendre quadratures together with regularization\ntransformations are applied on singular triangles. To speed up our method, we\ntake advantage of the embarrassingly parallel feature of boundary integral\nformulation, and parallelize the schemes with the message passing interface\n(MPI) implementation. Numerical tests show significantly improved accuracy and\nconvergence of the proposed higher-order boundary integral Poisson-Boltzmann\n(HOBI-PB) solver compared with boundary integral PB solver using often-seen\ncentroid collocation on flat triangles. The higher-order accuracy results\nachieved by present method are important to sensitive solvation analysis of\nbiomolecules, particularly when accurate electrostatic surface potentials are\ncritical in the molecular simulation. In addition, the higher-order boundary\nintegral schemes presented here and their associated parallelization\npotentially can be applied to solving boundary integral equations in a general\nsense.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 21:38:32 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Geng", "Weihua", ""]]}, {"id": "1301.5993", "submitter": "Majed ValadBeigi", "authors": "Farshad Safaei and Majed ValadBeigi", "title": "A Probabilistic Approach to Analysis of Reliability in n-D Meshes with\n  Interconnect Router Failures", "comments": "11 pages, 1 figure, 2 tables", "journal-ref": "International Journal of Computer Networks & Communications\n  (IJCNC) Vol.3, No.4, July 2011", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The routing algorithms for parallel computers, on-chip networks, multi-core\nprocessors, and multiprocessors system-on-chip (MP-SoCs) exhibit router\nfailures must be able to handle interconnect router failures that render a\nsymmetrical mesh non-symmetrically. When developing a routing methodology, the\ntime complexity of calculation should be minimal, and thus complicated routing\nstrategies to introduce profitable paths may not be appropriate. Several\nreports have been released in the literature on using the concept of fault\nrings to provide detour paths to messages blocked by faults and to route\nmessages around the fault regions. In order to analyze the performance of such\nalgorithms, it is required to investigate the characteristics of fault rings.\nIn this paper, we introduce a novel performance index of network reliability\npresenting the probability of message facing fault rings, and evaluating the\nperformance-related reliability of adaptive routing schemes in n-D mesh-based\ninterconnection networks with a variety of common cause fault patterns.\nSufficient simulation results of Monte-Carlo method are conducted to\ndemonstrate the correctness of the proposed analytical model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2013 09:18:58 GMT"}], "update_date": "2013-01-28", "authors_parsed": [["Safaei", "Farshad", ""], ["ValadBeigi", "Majed", ""]]}, {"id": "1301.6179", "submitter": "Konstantin Solnushkin S", "authors": "Konstantin S. Solnushkin", "title": "Automated Design of Two-Layer Fat-Tree Networks", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm to automatically design two-level fat-tree\nnetworks, such as ones widely used in large-scale data centres and cluster\nsupercomputers. The two levels may each use a different type of switches from\ndesign database to achieve an optimal network structure. Links between layers\ncan run in bundles to simplify cabling. Several sample network designs are\nexamined and their technical and economic characteristics are discussed.\n  The characteristic feature of our approach is that real life equipment prices\nand values of technical characteristics are used. This allows to select an\noptimal combination of hardware to build the network (including semi-populated\nconfigurations of modular switches) and accurately estimate the cost of this\nnetwork. We also show how technical characteristics of the network can be\nderived from its per-port metrics and suggest heuristics for equipment\nplacement.\n  The algorithm is useful as a part of a bigger design procedure that selects\noptimal hardware of cluster supercomputer as a whole. Therefore the article is\nfocused on the use of fat-trees for high-performance computing, although the\nresults are valid for any type of data centres.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2013 21:37:07 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Solnushkin", "Konstantin S.", ""]]}, {"id": "1301.6180", "submitter": "Konstantin Solnushkin S", "authors": "Konstantin S. Solnushkin", "title": "Automated Design of Torus Networks", "comments": "6 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm to automatically design networks with torus\ntopologies, such as ones widely used in large-scale supercomputers. The\ncharacteristic feature of our approach is that real life equipment prices and\nvalues of technical characteristics are used. As a result, we also have the\nopportunity to compare costs of torus and fat-tree networks.\n  The algorithm is useful as a part of a bigger design procedure that selects\noptimal hardware of cluster supercomputer as a whole.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2013 21:37:11 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Solnushkin", "Konstantin S.", ""]]}, {"id": "1301.6195", "submitter": "Samer Al-Kiswany", "authors": "Samer Al-Kiswany, Emalayan Vairavanathan, Lauro B. Costa, Hao Yang,\n  Matei Ripeanu", "title": "The Case for Cross-Layer Optimizations in Storage: A Workflow-Optimized\n  Storage System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes using file system custom metadata as a bidirectional\ncommunication channel between applications and the storage system. This channel\ncan be used to pass hints that enable cross-layer optimizations, an option\nhindered today by the ossified file-system interface. We study this approach in\ncontext of storage system support for large-scale workflow execution systems:\nOur workflow optimized storage system (WOSS), exploits application hints to\nprovide per-file optimized operations, and exposes data location to enable\nlocation-aware scheduling.\n  This paper argues that an incremental adoption path for adopting cross-layer\noptimizations in storage systems exists, presents the system architecture for a\nworkflow-optimized storage system and its integration with a workflow runtime\nengine, and evaluates the proposed approach using synthetic as well as real\napplications workloads.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 00:53:12 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Al-Kiswany", "Samer", ""], ["Vairavanathan", "Emalayan", ""], ["Costa", "Lauro B.", ""], ["Yang", "Hao", ""], ["Ripeanu", "Matei", ""]]}, {"id": "1301.6228", "submitter": "Andre Luckow", "authors": "Andre Luckow, Mark Santcroos, Ashley Zebrowski, Shantenu Jha", "title": "Pilot-Data: An Abstraction for Distributed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific problems that depend on processing large amounts of data require\novercoming challenges in multiple areas: managing large-scale data\ndistribution, controlling co-placement and scheduling of data with compute\nresources, and storing, transferring, and managing large volumes of data.\nAlthough there exist multiple approaches to addressing each of these\nchallenges, an integrative approach is missing; furthermore, extending existing\nfunctionality or enabling interoperable capabilities remains difficult at best.\nWe propose the concept of Pilot-Data to address the fundamental challenges of\nco-placement and scheduling of data and compute in heterogeneous and\ndistributed environments with interoperability and extensibility as first-order\nconcerns. Pilot-Data is an extension of the Pilot-Job abstraction for\nsupporting the management of data in conjunction with compute tasks. Pilot-Data\nseparates logical data units from physical storage, thereby providing the basis\nfor efficient compute/data placement and scheduling. In this paper, we discuss\nthe design and implementation of the Pilot-Data prototype, demonstrate its use\nby data-intensive applications on multiple production distributed\ncyberinfrastructure and illustrate the advantages arising from flexible\nexecution modes enabled by Pilot-Data. Our experiments utilize an\nimplementation of Pilot-Data in conjunction with a scalable Pilot-Job (BigJob)\nto establish the application performance that can be enabled by the use of\nPilot-Data. We demonstrate how the concept of Pilot-Data also provides the\nbasis upon which to build tools and support capabilities like affinity which in\nturn can be used for advanced data-compute co-placement and scheduling.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 10:06:13 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2013 12:04:38 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2013 02:04:44 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Luckow", "Andre", ""], ["Santcroos", "Mark", ""], ["Zebrowski", "Ashley", ""], ["Jha", "Shantenu", ""]]}, {"id": "1301.6297", "submitter": "Srivatsan Ravi Mr", "authors": "Hagit Attiya, Sandeep Hans, Petr Kuznetsov, Srivatsan Ravi", "title": "Safety of Deferred Update in Transactional Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional memory allows the user to declare sequences of instructions as\nspeculative \\emph{transactions} that can either \\emph{commit} or \\emph{abort}.\nIf a transaction commits, it appears to be executed sequentially, so that the\ncommitted transactions constitute a correct sequential execution. If a\ntransaction aborts, none of its instructions can affect other transactions.\n  The popular criterion of \\emph{opacity} requires that the views of aborted\ntransactions must also be consistent with the global sequential order\nconstituted by committed ones. This is believed to be important, since\ninconsistencies observed by an aborted transaction may cause a fatal\nirrecoverable error or waste of the system in an infinite loop. Intuitively, an\nopaque implementation must ensure that no intermediate view a transaction\nobtains before it commits or aborts can be affected by a transaction that has\nnot started committing yet, so called \\emph{deferred-update} semantics.\n  In this paper, we intend to grasp this intuition formally. We propose a\nvariant of opacity that explicitly requires the sequential order to respect the\ndeferred-update semantics. We show that our criterion is a safety property,\ni.e., it is prefix- and limit-closed. Unlike opacity, our property also ensures\nthat a serialization of a history implies serializations of its prefixes.\nFinally, we show that our property is equivalent to opacity if we assume that\nno two transactions commit identical values on the same variable, and present a\ncounter-example for scenarios when the \"unique-write\" assumption does not hold.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 23:35:25 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2013 16:29:29 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2013 12:39:36 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Attiya", "Hagit", ""], ["Hans", "Sandeep", ""], ["Kuznetsov", "Petr", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "1301.7265", "submitter": "Majid Gerami", "authors": "Majid Gerami, Ming Xiao, Carlo Fischione, Mikael Skoglund", "title": "Decentralized Minimum-Cost Repair for Distributed Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been emerging lots of applications for distributed storage systems\ne.g., those in wireless sensor networks or cloud storage. Since storage nodes\nin wireless sensor networks have limited battery, it is valuable to find a\nrepair scheme with optimal transmission costs (e.g., energy). The optimal-cost\nrepair has been recently investigated in a centralized way. However a\ncentralized control mechanism may not be available or is very expensive. For\nthe scenarios, it is interesting to study optimal-cost repair in a\ndecentralized setup. We formulate the optimal-cost repair as convex\noptimization problems for the network with convex transmission costs. Then we\nuse primal and dual decomposition approaches to decouple the problem into\nsubproblems to be solved locally. Thus, each surviving node, collaborating with\nother nodes, can minimize its transmission cost such that the global cost is\nminimized. We further study the optimality and convergence of the algorithms.\nFinally, we discuss the code construction and determine the field size for\nfinding feasible network codes in our approaches.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 15:44:36 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Gerami", "Majid", ""], ["Xiao", "Ming", ""], ["Fischione", "Carlo", ""], ["Skoglund", "Mikael", ""]]}, {"id": "1301.7533", "submitter": "Silvano Dal Zilio", "authors": "Rodrigo Tacla Saad (LAAS), Silvano Dal Zilio (LAAS), Bernard\n  Berthomieu (LAAS)", "title": "An Experiment on Parallel Model Checking of a CTL Fragment", "comments": "10th International Symposium, ATVA 2012, Automated Technology for\n  Verification and Analysis, Thiruvananthapuram : India (2012)", "journal-ref": null, "doi": "10.1007/978-3-642-33386-6_23", "report-no": "Rapport LAAS n&deg; 12400", "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parallel algorithm for local, on the fly, model checking of a\nfragment of CTL that is well-suited for modern, multi-core architectures. This\nmodel-checking algorithm takes bene t from a parallel state space construction\nalgorithm, which we described in a previous work, and shares the same basic set\nof principles: there are no assumptions on the models that can be analyzed; no\nrestrictions on the way states are distributed; and no restrictions on the way\nwork is shared among processors. We evaluate the performance of diff erent\nversions of our algorithm and compare our results with those obtained using\nother parallel model checking tools. One of the most novel contributions of\nthis work is to study a space-e fficient variant for CTL model-checking that\ndoes not require to store the whole transition graph but that operates on a\nreverse spanning tree.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 07:56:11 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Saad", "Rodrigo Tacla", "", "LAAS"], ["Zilio", "Silvano Dal", "", "LAAS"], ["Berthomieu", "Bernard", "", "LAAS"]]}, {"id": "1301.7699", "submitter": "Nicos Angelopoulos", "authors": "Rui Machado and Salvador Abreu and Daniel Diaz", "title": "Parallel Local Search: Experiments with a PGAS-based programming model", "comments": "Appeared in CICLOPS 2012. 17 Pages, 4 Figures. arXiv admin note: text\n  overlap with arXiv:1212.4287 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search is a successful approach for solving combinatorial optimization\nand constraint satisfaction problems. With the progressing move toward multi\nand many-core systems, GPUs and the quest for Exascale systems, parallelism has\nbecome mainstream as the number of cores continues to increase. New programming\nmodels are required and need to be better understood as well as data structures\nand algorithms. Such is the case for local search algorithms when run on\nhundreds or thousands of processing units. In this paper, we discuss some\nexperiments we have been doing with Adaptive Search and present a new parallel\nversion of it based on GPI, a recent API and programming model for the\ndevelopment of scalable parallel applications. Our experiments on different\nproblems show interesting speedups and, more importantly, a deeper\ninterpretation of the parallelization of Local Search methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 17:35:55 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 14:46:14 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Machado", "Rui", ""], ["Abreu", "Salvador", ""], ["Diaz", "Daniel", ""]]}]