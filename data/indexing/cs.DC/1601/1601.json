[{"id": "1601.00072", "submitter": "Mishal Almazrooie Mr", "authors": "Mishal Almazrooie, Mogana Vadiveloo, and Rosni Abdullah", "title": "GPU-Based Fuzzy C-Means Clustering Algorithm for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a fast and practical GPU-based implementation of Fuzzy\nC-Means(FCM) clustering algorithm for image segmentation is proposed. First, an\nextensive analysis is conducted to study the dependency among the image pixels\nin the algorithm for parallelization. The proposed GPU-based FCM has been\ntested on digital brain simulated dataset to segment white matter(WM), gray\nmatter(GM) and cerebrospinal fluid (CSF) soft tissue regions. The execution\ntime of the sequential FCM is 519 seconds for an image dataset with the size of\n1MB. While the proposed GPU-based FCM requires only 2.33 seconds for the\nsimilar size of image dataset. An estimated 245-fold speedup is measured for\nthe data size of 40 KB on a CUDA device that has 448 processors.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 11:18:31 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 02:27:45 GMT"}, {"version": "v3", "created": "Mon, 28 Mar 2016 09:47:29 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Almazrooie", "Mishal", ""], ["Vadiveloo", "Mogana", ""], ["Abdullah", "Rosni", ""]]}, {"id": "1601.00159", "submitter": "Zhongle Xie", "authors": "Zhongle Xie, Qingchao Cai, H.V. Jagadish, Beng Chin Ooi, Weng-Fai Wong", "title": "PI : a Parallel in-memory skip list based Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the coarse granularity of data accesses and the heavy use of latches,\nindices in the B-tree family are not efficient for in-memory databases,\nespecially in the context of today's multi-core architecture.\n  In this paper, we present PI, a Parallel in-memory skip list based Index that\nlends itself naturally to the parallel and concurrent environment, particularly\nwith non-uniform memory access. In PI, incoming queries are collected, and\ndisjointly distributed among multiple threads for processing to avoid the use\nof latches. For each query, PI traverses the index in a Breadth-First-Search\n(BFS) manner to find the list node with the matching key, exploiting SIMD\nprocessing to speed up the search process. In order for query processing to be\nlatch-free, PI employs a light-weight communication protocol that enables\nthreads to re-distribute the query workload among themselves such that each\nlist node that will be modified as a result of query processing will be\naccessed by exactly one thread. We conducted extensive experiments, and the\nresults show that PI can be up to three times as fast as the Masstree, a\nstate-of-the-art B-tree based index.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 10:11:57 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Xie", "Zhongle", ""], ["Cai", "Qingchao", ""], ["Jagadish", "H. V.", ""], ["Ooi", "Beng Chin", ""], ["Wong", "Weng-Fai", ""]]}, {"id": "1601.00221", "submitter": "Darren Chitty", "authors": "Darren M. Chitty", "title": "Faster GPU Based Genetic Programming Using A Two Dimensional Stack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Programming (GP) is a computationally intensive technique which also\nhas a high degree of natural parallelism. Parallel computing architectures have\nbecome commonplace especially with regards Graphics Processing Units (GPU).\nHence, versions of GP have been implemented that utilise these highly parallel\ncomputing platforms enabling significant gains in the computational speed of GP\nto be achieved. However, recently a two dimensional stack approach to GP using\na multi-core CPU also demonstrated considerable performance gains. Indeed,\nperformances equivalent to or exceeding that achieved by a GPU were\ndemonstrated. This paper will demonstrate that a similar two dimensional stack\napproach can also be applied to a GPU based approach to GP to better exploit\nthe underlying technology. Performance gains are achieved over a standard\nsingle dimensional stack approach when utilising a GPU. Overall, a peak\ncomputational speed of over 55 billion Genetic Programming Operations per\nSecond are observed, a two fold improvement over the best GPU based single\ndimensional stack approach from the literature.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 20:24:15 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Chitty", "Darren M.", ""]]}, {"id": "1601.00286", "submitter": "Christian Lorenz Staudt", "authors": "Michael Hamann, Gerd Lindner, Henning Meyerhenke, Christian L. Staudt,\n  Dorothea Wagner", "title": "Structure-Preserving Sparsification Methods for Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparsification reduces the size of networks while preserving structural and\nstatistical properties of interest. Various sparsifying algorithms have been\nproposed in different contexts. We contribute the first systematic conceptual\nand experimental comparison of \\textit{edge sparsification} methods on a\ndiverse set of network properties. It is shown that they can be understood as\nmethods for rating edges by importance and then filtering globally or locally\nby these scores. We show that applying a local filtering technique improves the\npreservation of all kinds of properties. In addition, we propose a new\nsparsification method (\\textit{Local Degree}) which preserves edges leading to\nlocal hub nodes. All methods are evaluated on a set of social networks from\nFacebook, Google+, Twitter and LiveJournal with respect to network properties\nincluding diameter, connected components, community structure, multiple node\ncentrality measures and the behavior of epidemic simulations. In order to\nassess the preservation of the community structure, we also include experiments\non synthetically generated networks with ground truth communities. Experiments\nwith our implementations of the sparsification methods (included in the\nopen-source network analysis tool suite NetworKit) show that many network\nproperties can be preserved down to about 20\\% of the original set of edges for\nsparse graphs with a reasonable density. The experimental results allow us to\ndifferentiate the behavior of different methods and show which method is\nsuitable with respect to which property. While our Local Degree method is best\nfor preserving connectivity and short distances, other newly introduced local\nvariants are best for preserving the community structure.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 12:28:37 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Hamann", "Michael", ""], ["Lindner", "Gerd", ""], ["Meyerhenke", "Henning", ""], ["Staudt", "Christian L.", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1601.00289", "submitter": "Christian Lorenz Staudt", "authors": "Jannis Koch, Christian L. Staudt, Maximilian Vogel, Henning Meyerhenke", "title": "An Empirical Comparison of Big Graph Frameworks in the Context of\n  Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex networks are relational data sets commonly represented as graphs. The\nanalysis of their intricate structure is relevant to many areas of science and\ncommerce, and data sets may reach sizes that require distributed storage and\nprocessing. We describe and compare programming models for distributed\ncomputing with a focus on graph algorithms for large-scale complex network\nanalysis. Four frameworks - GraphLab, Apache Giraph, Giraph++ and Apache Flink\n- are used to implement algorithms for the representative problems Connected\nComponents, Community Detection, PageRank and Clustering Coefficients. The\nimplementations are executed on a computer cluster to evaluate the frameworks'\nsuitability in practice and to compare their performance to that of the\nsingle-machine, shared-memory parallel network analysis package NetworKit. Out\nof the distributed frameworks, GraphLab and Apache Giraph generally show the\nbest performance. In our experiments a cluster of eight computers running\nApache Giraph enables the analysis of a network with about 2 billion edges,\nwhich is too large for a single machine of the same type. However, for networks\nthat fit into memory of one machine, the performance of the shared-memory\nparallel implementation is far better than the distributed ones. The study\nprovides experimental evidence for selecting the appropriate framework\ndepending on the task and data volume.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 12:53:21 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Koch", "Jannis", ""], ["Staudt", "Christian L.", ""], ["Vogel", "Maximilian", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1601.00738", "submitter": "Jiaan Zeng", "authors": "Jiaan Zeng", "title": "Resource Sharing for Multi-Tenant NoSQL Data Store in Cloud", "comments": "PhD dissertation, December 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-tenancy hosting of users in cloud NoSQL data stores is favored by cloud\nproviders because it enables resource sharing at low operating cost.\nMulti-tenancy takes several forms depending on whether the back-end file system\nis a local file system (LFS) or a parallel file system (PFS), and on whether\ntenants are independent or share data across tenants. In this thesis I focus on\nand propose solutions to two cases: independent data-local file system, and\nshared data-parallel file system.\n  In the independent data-local file system case, resource contention occurs\nunder certain conditions in Cassandra and HBase, two state-of-the-art NoSQL\nstores, causing performance degradation for one tenant by another. We\ninvestigate the interference and propose two approaches. The first provides a\nscheduling scheme that can approximate resource consumption, adapt to workload\ndynamics and work in a distributed fashion. The second introduces a\nworkload-aware resource reservation approach to prevent interference. The\napproach relies on a performance model obtained offline and plans the\nreservation according to different workload resource demands. Results show the\napproaches together can prevent interference and adapt to dynamic workloads\nunder multi-tenancy.\n  In the shared data-parallel file system case, it has been shown that running\na distributed NoSQL store over PFS for shared data across tenants is not cost\neffective. Overheads are introduced due to the unawareness of the NoSQL store\nof PFS. This dissertation targets the key-value store (KVS), a specific form of\nNoSQL stores, and proposes a lightweight KVS over a parallel file system to\nimprove efficiency. The solution is built on an embedded KVS for high\nperformance but uses novel data structures to support concurrent writes.\nResults show the proposed system outperforms Cassandra and Voldemort in several\ndifferent workloads.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 05:15:12 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Zeng", "Jiaan", ""]]}, {"id": "1601.00834", "submitter": "Jordane Lorandel M.", "authors": "Lorandel Jordane, Jean-Christophe Pr\\'evotet and Maryline H\\'elard", "title": "Fast Power and Energy Efficiency Analysis of FPGA-based Wireless\n  Base-band Processing", "comments": "Presented at HIP3ES, 2016", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2016/4", "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, demands for high performance keep on increasing in the wireless\ncommunication domain. This leads to a consistent rise of the complexity and\ndesigning such systems has become a challenging task. In this context, energy\nefficiency is considered as a key topic, especially for embedded systems in\nwhich design space is often very constrained. In this paper, a fast and\naccurate power estimation approach for FPGA-based hardware systems is applied\nto a typical wireless communication system. It aims at providing power\nestimates of complete systems prior to their implementations. This is made\npossible by using a dedicated library of high-level models that are\nrepresentative of hardware IPs. Based on high-level simulations, design space\nexploration is made a lot faster and easier. The definition of a scenario and\nthe monitoring of IP's time-activities facilitate the comparison of several\ndomain-specific systems. The proposed approach and its benefits are\ndemonstrated through a typical use case in the wireless communication domain.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 14:14:21 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Jordane", "Lorandel", ""], ["Pr\u00e9votet", "Jean-Christophe", ""], ["H\u00e9lard", "Maryline", ""]]}, {"id": "1601.00863", "submitter": "Ming Yan", "authors": "Zhimin Peng, Tianyu Wu, Yangyang Xu, Ming Yan, Wotao Yin", "title": "Coordinate Friendly Structures, Algorithms and Applications", "comments": null, "journal-ref": "Annals of Mathematical Sciences and Applications, 1 (2016), 57-119", "doi": "10.4310/AMSA.2016.v1.n1.a2", "report-no": "UCLA CAM Report 16-13", "categories": "math.OC cs.CE cs.DC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on coordinate update methods, which are useful for solving\nproblems involving large or high-dimensional datasets. They decompose a problem\ninto simple subproblems, where each updates one, or a small block of, variables\nwhile fixing others. These methods can deal with linear and nonlinear mappings,\nsmooth and nonsmooth functions, as well as convex and nonconvex problems. In\naddition, they are easy to parallelize.\n  The great performance of coordinate update methods depends on solving simple\nsub-problems. To derive simple subproblems for several new classes of\napplications, this paper systematically studies coordinate-friendly operators\nthat perform low-cost coordinate updates.\n  Based on the discovered coordinate friendly operators, as well as operator\nsplitting techniques, we obtain new coordinate update algorithms for a variety\nof problems in machine learning, image processing, as well as sub-areas of\noptimization. Several problems are treated with coordinate update for the first\ntime in history. The obtained algorithms are scalable to large instances\nthrough parallel and even asynchronous computing. We present numerical examples\nto illustrate how effective these algorithms are.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 15:33:05 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 23:05:07 GMT"}, {"version": "v3", "created": "Sun, 14 Aug 2016 14:29:53 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Peng", "Zhimin", ""], ["Wu", "Tianyu", ""], ["Xu", "Yangyang", ""], ["Yan", "Ming", ""], ["Yin", "Wotao", ""]]}, {"id": "1601.00894", "submitter": "Daniel Bates", "authors": "Daniel Bates, Alex Chadwick and Robert Mullins", "title": "Configurable memory systems for embedded many-core processors", "comments": "Presented at HIP3ES, 2016", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2016/2", "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memory system of a modern embedded processor consumes a large fraction of\ntotal system energy. We explore a range of different configuration options and\nshow that a reconfigurable design can make better use of the resources\navailable to it than any fixed implementation, and provide large improvements\nin both performance and energy consumption. Reconfigurability becomes\nincreasingly useful as resources become more constrained, so is particularly\nrelevant in the embedded space.\n  For an optimised architectural configuration, we show that a configurable\ncache system performs an average of 20% (maximum 70%) better than the best\nfixed implementation when two programs are competing for the same resources,\nand reduces cache miss rate by an average of 70% (maximum 90%). We then present\na case study of AES encryption and decryption, and find that a custom memory\nconfiguration can almost double performance, with further benefits being\nachieved by specialising the task of each core when parallelising the program.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 16:29:17 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 12:12:42 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Bates", "Daniel", ""], ["Chadwick", "Alex", ""], ["Mullins", "Robert", ""]]}, {"id": "1601.01008", "submitter": "Zahra Derakhshandeh", "authors": "Zahra Derakhshandeh, Robert Gmyr, Andrea W. Richa, Christian\n  Scheideler, Thim Strothmann", "title": "Universal Coating for Programmable Matter", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea behind universal coating is to have a thin layer of a specific\nsubstance covering an object of any shape so that one can measure a certain\ncondition (like temperature or cracks) at any spot on the surface of the object\nwithout requiring direct access to that spot. We study the universal coating\nproblem in the context of self-organizing programmable matter consisting of\nsimple computational elements, called particles, that can establish and release\nbonds and can actively move in a self-organized way. Based on that matter, we\npresent a worst-case work-optimal universal coating algorithm that uniformly\ncoats any object of arbitrary shape and size that allows a uniform coating. Our\nparticles are anonymous, do not have any global information, have constant-size\nmemory, and utilize only local interactions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 22:43:20 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Derakhshandeh", "Zahra", ""], ["Gmyr", "Robert", ""], ["Richa", "Andrea W.", ""], ["Scheideler", "Christian", ""], ["Strothmann", "Thim", ""]]}, {"id": "1601.01104", "submitter": "Amos Korman", "authors": "Pierre Fraigniaud (LIAFA, GANG), Amos Korman (LIAFA, GANG), Shay\n  Kutten, David Peleg, Emek Yuval", "title": "Notions of Connectivity in Overlay Networks", "comments": "Structural Information and Communication Complexity - 19th\n  International Colloquium, 2012, Jun 2012, Reykjavik, Iceland. 2015", "journal-ref": null, "doi": "10.1007/978-3-642-31104-8_3", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \" How well connected is the network? \" This is one of the most fundamental\nquestions one would ask when facing the challenge of designing a communication\nnetwork. Three major notions of connectivity have been considered in the\nliterature, but in the context of traditional (single-layer) networks, they\nturn out to be equivalent. This paper introduces a model for studying the three\nnotions of connectivity in multi-layer networks. Using this model, it is easy\nto demonstrate that in multi-layer networks the three notions may differ\ndramatically. Unfortunately, in contrast to the single-layer case, where the\nvalues of the three connectivity notions can be computed efficiently, it has\nbeen recently shown in the context of WDM networks (results that can be easily\ntranslated to our model) that the values of two of these notions of\nconnectivity are hard to compute or even approximate in multi-layer networks.\nThe current paper shed some positive light into the multi-layer connectivity\ntopic: we show that the value of the third connectivity notion can be computed\nin polynomial time and develop an approximation for the construction of well\nconnected overlay networks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 08:04:55 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Fraigniaud", "Pierre", "", "LIAFA, GANG"], ["Korman", "Amos", "", "LIAFA, GANG"], ["Kutten", "Shay", ""], ["Peleg", "David", ""], ["Yuval", "Emek", ""]]}, {"id": "1601.01498", "submitter": "Ali Gholami", "authors": "Ali Gholami, Erwin Laure", "title": "Security and Privacy of Sensitive Data in Cloud Computing: A Survey of\n  Recent Developments", "comments": null, "journal-ref": null, "doi": "10.5121/csit.2015.51611", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is revolutionizing many ecosystems by providing organizations\nwith computing resources featuring easy deployment, connectivity,\nconfiguration, automation and scalability. This paradigm shift raises a broad\nrange of security and privacy issues that must be taken into consideration.\nMulti-tenancy, loss of control, and trust are key challenges in cloud computing\nenvironments. This paper reviews the existing technologies and a wide array of\nboth earlier and state-of-the-art projects on cloud security and privacy. We\ncategorize the existing research according to the cloud reference architecture\norchestration, resource control, physical resource, and cloud service\nmanagement layers, in addition to reviewing the existing developments in\nprivacy-preserving sensitive data approaches in cloud computing such as privacy\nthreat modeling and privacy enhancing protocols and solutions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 11:53:20 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Gholami", "Ali", ""], ["Laure", "Erwin", ""]]}, {"id": "1601.01500", "submitter": "Ali Gholami", "authors": "Ali Gholami, Erwin Laure", "title": "Advanced Cloud Privacy Threat Modeling", "comments": null, "journal-ref": null, "doi": "10.5121/csit.2016.60120", "report-no": null, "categories": "cs.SE cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preservation for sensitive data has become a challenging issue in\ncloud computing. Threat modeling as a part of requirements engineering in\nsecure software development provides a structured approach for identifying\nattacks and proposing countermeasures against the exploitation of\nvulnerabilities in a system . This paper describes an extension of Cloud\nPrivacy Threat Modeling (CPTM) methodology for privacy threat modeling in\nrelation to processing sensitive data in cloud computing environments. It\ndescribes the modeling methodology that involved applying Method Engineering to\nspecify characteristics of a cloud privacy threat modeling methodology,\ndifferent steps in the proposed methodology and corresponding products. We\nbelieve that the extended methodology facilitates the application of a\nprivacy-preserving cloud software development approach from requirements\nengineering to design.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 12:00:16 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Gholami", "Ali", ""], ["Laure", "Erwin", ""]]}, {"id": "1601.01587", "submitter": "Jan Kr\\v{c}\\'al", "authors": "Holger Hermanns, Jan Kr\\v{c}\\'al, Steen Vester", "title": "Distributed Synthesis in Continuous Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a formalism modelling communication of distributed agents\nstrictly in continuous-time. Within this framework, we study the problem of\nsynthesising local strategies for individual agents such that a specified set\nof goal states is reached, or reached with at least a given probability. The\nflow of time is modelled explicitly based on continuous-time randomness, with\ntwo natural implications: First, the non-determinism stemming from interleaving\ndisappears. Second, when we restrict to a subclass of non-urgent models, the\nquantitative value problem for two players can be solved in EXPTIME. Indeed,\nthe explicit continuous time enables players to communicate their states by\ndelaying synchronisation (which is unrestricted for non-urgent models). In\ngeneral, the problems are undecidable already for two players in the\nquantitative case and three players in the qualitative case. The qualitative\nundecidability is shown by a reduction to decentralized POMDPs for which we\nprovide the strongest (and rather surprising) undecidability result so far.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 16:30:11 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Hermanns", "Holger", ""], ["Kr\u010d\u00e1l", "Jan", ""], ["Vester", "Steen", ""]]}, {"id": "1601.01607", "submitter": "Juan Juli\\'an Merelo-Guerv\\'os Pr.", "authors": "Juan-J. Merelo, Mario Garc\\'ia-Valdez, Pedro A. Castillo, Pablo\n  Garc\\'ia-S\\'anchez, P. de las Cuevas, Nuria Rico", "title": "NodIO, a JavaScript framework for volunteer-based evolutionary\n  algorithms : first results", "comments": "GeNeura 2006-01", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  JavaScript is an interpreted language mainly known for its inclusion in web\nbrowsers, making them a container for rich Internet based applications. This\nhas inspired its use, for a long time, as a tool for evolutionary algorithms,\nmainly so in browser-based volunteer computing environments. Several libraries\nhave also been published so far and are in use. However, the last years have\nseen a resurgence of interest in the language, becoming one of the most popular\nand thus spawning the improvement of its implementations, which are now the\nfoundation of many new client-server applications. We present such an\napplication for running distributed volunteer-based evolutionary algorithm\nexperiments, and we make a series of measurements to establish the speed of\nJavaScript in evolutionary algorithms that can serve as a baseline for\ncomparison with other distributed computing experiments. These experiments use\ndifferent integer and floating point problems, and prove that the speed of\nJavaScript is actually competitive with other languages commonly used by the\nevolutionary algorithm practitioner.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 17:21:38 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Merelo", "Juan-J.", ""], ["Garc\u00eda-Valdez", "Mario", ""], ["Castillo", "Pedro A.", ""], ["Garc\u00eda-S\u00e1nchez", "Pablo", ""], ["Cuevas", "P. de las", ""], ["Rico", "Nuria", ""]]}, {"id": "1601.01722", "submitter": "Alexandra Jimborean", "authors": "Jonatan Waern, Per Ekemark, Konstantinos Koukos, Stefanos Kaxiras,\n  Alexandra Jimborean", "title": "Profiling-Assisted Decoupled Access-Execute", "comments": "Presented at HIP3ES, 2016", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2016/1", "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As energy efficiency became a critical factor in the embedded systems domain,\ndynamic voltage and frequency scaling (DVFS) techniques have emerged as means\nto control the system's power and energy efficiency. Additionally, due to the\ncompact design, thermal issues become prominent. State of the art work promotes\nsoftware decoupled access-execution (DAE) that statically generates code\namenable to DVFS techniques. The compiler builds memory-bound access phases,\ndesigned to prefetch data in the cache at low frequency, and compute-bound\nphases, that consume the data and perform computations at high frequency. This\nwork investigates techniques to find the optimal balance between lightweight\nand efficient access phases. A profiling step guides the selection of loads to\nbe prefetched in the access phase. For applications whose behavior vary\nsignificantly with respect to the input data, the profiling can be performed\nonline, accompanied by just-in-time compilation. We evaluated the benefits in\nenergy efficiency and performance for both static and dynamic code generation\nand showed that precise prefetching of critical loads can result in 20% energy\nimprovements, on average. DAE is particularly beneficial for embedded systems\nas by alternating access phases (executed at low frequency) and execute phases\n(at high frequency) DAE proactively reduces the temperature and therefore\nprevents thermal emergencies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 22:43:43 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Waern", "Jonatan", ""], ["Ekemark", "Per", ""], ["Koukos", "Konstantinos", ""], ["Kaxiras", "Stefanos", ""], ["Jimborean", "Alexandra", ""]]}, {"id": "1601.01736", "submitter": "Elod Pal Csirmaz", "authors": "Elod Pal Csirmaz", "title": "Algebraic File Synchronization: Adequacy and Completeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With distributed computing and mobile applications, synchronizing diverging\nreplicas of data structures is a more and more common problem. We use algebraic\nmethods to reason about filesystem operations, and introduce a simplified\ndefinition of conflicting updates to filesystems. We also define algorithms for\nupdate detection and reconciliation and present rigorous proofs that they not\nonly work as intended, but also cannot be improved on.\n  To achieve this, we introduce a novel, symmetric set of filesystem commands\nwith higher information content, which removes edge cases and increases the\npredictive powers of our algebraic model. We also present a number of generally\nuseful classes and properties of sequences of commands.\n  While these results are often intuitive, providing exact proofs for them is\nfar from trivial. They contribute to our understanding of this special type of\nalgebraic model, and toward building more complete algebras of filesystem trees\nand extending algebraic approaches to other data storage protocols. They also\nform a theoretical basis for specifying and guaranteeing the error-free\noperation of applications that implement an algebraic approach to\nsynchronization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 01:01:55 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 23:44:04 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Csirmaz", "Elod Pal", ""]]}, {"id": "1601.01881", "submitter": "Fabio Lopez-Pires", "authors": "Jammily Ortigoza and Fabio Lopez-Pires and Benjam{\\i}n Baran", "title": "Dynamic Environments for Virtual Machine Placement considering\n  Elasticity and Overbooking", "comments": "arXiv admin note: text overlap with arXiv:1507.00090", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing datacenters provide millions of virtual machines in actual\ncloud markets. In this context, Virtual Machine Placement (VMP) is one of the\nmost challenging problems in cloud infrastructure management, considering the\nlarge number of possible optimization criteria and different formulations that\ncould be studied. Considering the on-demand model of cloud computing, the VMP\nproblem should be solved dynamically to efficiently attend typical workload of\nmodern applications. This work proposes a taxonomy in order to understand\npossible challenges for Cloud Service Providers (CSPs) in dynamic environments,\nbased on the most relevant dynamic parameters studied so far in the VMP\nliterature. Based on the proposed taxonomy, several unexplored environments\nhave been identified. To further study those research opportunities, sample\nworkload traces for each particular environment are required; therefore, basic\nexamples illustrate a preliminary work on dynamic workload trace generation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:14:54 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Ortigoza", "Jammily", ""], ["Lopez-Pires", "Fabio", ""], ["Baran", "Benjam\u0131n", ""]]}, {"id": "1601.01910", "submitter": "Harald Richter", "authors": "Harald Richter", "title": "About the Suitability of Clouds in High-Performance Computing", "comments": "10 pages", "journal-ref": "Journal Computer Science and Information Technology (CC&IT),\n  Volume 6, Number 1, January 2016, pp. 23-33", "doi": "10.5121/csit.2016.60103", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has become the ubiquitous computing and storage paradigm. It\nis also attractive for scientists, because they do not have to care any more\nfor their own IT infrastructure, but can outsource it to a Cloud Service\nProvider of their choice. However, for the case of High-Performance Computing\n(HPC) in a cloud, as it is needed in simulations or for Big Data analysis,\nthings are getting more intricate, because HPC codes must stay highly\nefficient, even when executed by many virtual cores (vCPUs). Older clouds or\nnew standard clouds can fulfil this only under special precautions, which are\ngiven in this article. The results can be extrapolated to other cloud OSes than\nOpenStack and to other codes than OpenFOAM, which were used as examples.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 15:35:41 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Richter", "Harald", ""]]}, {"id": "1601.01912", "submitter": "Mohamad Ahmadi", "authors": "Mohamad Ahmadi, Abdolhamid Ghodselahi, Fabian Kuhn, Anisur Rahaman\n  Molla", "title": "The Cost of Global Broadcast in Dynamic Radio Networks", "comments": "17 pages, conference version appeared in OPODIS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the single-message broadcast problem in dynamic radio networks. We\nshow that the time complexity of the problem depends on the amount of stability\nand connectivity of the dynamic network topology and on the adaptiveness of the\nadversary providing the dynamic topology. More formally, we model communication\nusing the standard graph-based radio network model. To model the dynamic\nnetwork, we use a generalization of the synchronous dynamic graph model\nintroduced in [Kuhn et al., STOC 2010]. For integer parameters $T\\geq 1$ and\n$k\\geq 1$, we call a dynamic graph $T$-interval $k$-connected if for every\ninterval of $T$ consecutive rounds, there exists a $k$-vertex-connected stable\nsubgraph. Further, for an integer parameter $\\tau\\geq 0$, we say that the\nadversary providing the dynamic network is $\\tau$-oblivious if for constructing\nthe graph of some round $t$, the adversary has access to all the randomness\n(and states) of the algorithm up to round $t-\\tau$.\n  As our main result, we show that for any $T\\geq 1$, any $k\\geq 1$, and any\n$\\tau\\geq 1$, for a $\\tau$-oblivious adversary, there is a distributed\nalgorithm to broadcast a single message in time\n$O\\big(\\big(1+\\frac{n}{k\\cdot\\min\\left\\{\\tau,T\\right\\}}\\big)\\cdot n\\log^3\nn\\big)$. We further show that even for large interval $k$-connectivity,\nefficient broadcast is not possible for the usual adaptive adversaries. For a\n$1$-oblivious adversary, we show that even for any $T\\leq\n(n/k)^{1-\\varepsilon}$ (for any constant $\\varepsilon>0$) and for any $k\\geq\n1$, global broadcast in $T$-interval $k$-connected networks requires at least\n$\\Omega(n^2/(k^2\\log n))$ time. Further, for a $0$ oblivious adversary,\nbroadcast cannot be solved in $T$-interval $k$-connected networks as long as\n$T<n-k$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 15:43:34 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Ahmadi", "Mohamad", ""], ["Ghodselahi", "Abdolhamid", ""], ["Kuhn", "Fabian", ""], ["Molla", "Anisur Rahaman", ""]]}, {"id": "1601.02130", "submitter": "Luis Veiga", "authors": "Pradeeban Kathiravelu and Lu\\'is Veiga", "title": "SENDIM for Incremental Development of Cloud Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "INESC-ID Tec. Rep. 23/2015, October 2015", "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the limited and varying availability of cheap infrastructure and\nresources, cloud network systems and applications are tested in simulation and\nemulation environments prior to physical deployments, at different stages of\ndevelopment. Configuration management tools manage deployments and migrations\nacross different cloud platforms, mitigating tedious system administration\nefforts. However, currently a cloud networking simulation cannot be migrated as\nan emulation, or vice versa, without rewriting and manually re-deploying the\nsimulated application. This paper presents SENDIM (Sendim is a northeastern\nPortuguese town close to the Spanish border, where the rare Mirandese language\nis spoken), a Simulation, Emulation, aNd Deployment Integration Middleware for\ncloud networks. As an orchestration platform for incrementally building\nSoftware-Defined Cloud Networks (SDCN), SENDIM manages the development and\ndeployment of algorithms and architectures the entire length from\nvisualization, simulation, emulation, to physical deployments. Hence, SENDIM\noptimizes the evaluation of cloud networks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 16:52:23 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kathiravelu", "Pradeeban", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1601.02131", "submitter": "Luis Veiga", "authors": "Pradeeban Kathiravelu and Tihana Galinac Grbac and Lu\\'is Veiga", "title": "A FIRM Approach to Software-Defined Service Composition", "comments": "INESC-ID Tec. Rep. 22/2015, October 2015", "journal-ref": null, "doi": null, "report-no": "INESC-ID Tec. Rep. 22/2015, October 2015", "categories": "cs.DC cs.NI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service composition is an aggregate of services often leveraged to automate\nthe enterprise business processes. While Service Oriented Architecture (SOA)\nhas been a forefront of service composition, services can be realized as\nefficient distributed and parallel constructs such as MapReduce, which are not\ntypically exploited in service composition. With the advent of\nSoftware\\-Defined Networking (SDN), global view and control of the entire\nnetwork is made available to the networking controller, which can further be\nleveraged in application level. This paper presents FIRM, an approach for\nSoftware-Defined Service Composition by leveraging SDN and MapReduce. FIRM\ncomprises Find, Invoke, Return, and Manage, as the core procedures in achieving\na QoS-Aware Service Composition.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 16:52:32 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kathiravelu", "Pradeeban", ""], ["Grbac", "Tihana Galinac", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1601.02245", "submitter": "C\\'esar A. Terrero-Escalante", "authors": "Alejandra Gait\\'an Montejo, Octavio A. Michel-Manzo and C\\'esar A.\n  Terrero-Escalante", "title": "On parallel solution of ordinary differential equations", "comments": "30 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the performance of a parallel iterated Runge-Kutta method is\ncompared versus those of the serial fouth order Runge-Kutta and Dormand-Prince\nmethods. It was found that, typically, the runtime for the parallel method is\ncomparable to that of the serial versions, thought it uses considerably more\ncomputational resources. A new algorithm is proposed where full parallelization\nis used to estimate the best stepsize for integration. It is shown that this\nnew method outperforms the others, notably, in the integration of very large\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 18:20:55 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Montejo", "Alejandra Gait\u00e1n", ""], ["Michel-Manzo", "Octavio A.", ""], ["Terrero-Escalante", "C\u00e9sar A.", ""]]}, {"id": "1601.02472", "submitter": "Vincenzo De Florio", "authors": "Vincenzo De Florio and Geert Deconinck and Rudy Lauwereins", "title": "An Application-Level Dependable Technique for Farmer-Worker Parallel\n  Programs", "comments": "In LNCS 1225 (1997), Proc. of the High-Performance Computing and\n  Networking Conference ISBN: 978-3-540-62898-9 (Print) 978-3-540-69041-2\n  (Online)", "journal-ref": null, "doi": "10.1007/BFb0031636", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An application-level technique is described for farmer-worker parallel\napplications which allows a worker to be added or removed from the computing\nfarm at any moment of the run time without affecting the overall outcome of the\ncomputation. The technique is based on uncoupling the farmer from the workers\nby means of a separate module which asynchronously feeds these latter with new\n\"units of work\" on an on-demand basis, and on a special feeding strategy based\non bookkeeping the status of each work-unit. An augmentation of the LINDA model\nis finally proposed to exploit the bookkeeping algorithm for tuple management.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 15:11:42 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["De Florio", "Vincenzo", ""], ["Deconinck", "Geert", ""], ["Lauwereins", "Rudy", ""]]}, {"id": "1601.02578", "submitter": "Luca Laurenti", "authors": "Luca Cardelli, Marta Kwiatkowska, Luca Laurenti", "title": "Programming Discrete Distributions with Chemical Reaction Networks", "comments": null, "journal-ref": "Cardelli, Luca, Marta Kwiatkowska, and Luca Laurenti. \"Programming\n  discrete distributions with chemical reaction networks.\" Natural computing\n  17.1 (2018): 131-145", "doi": null, "report-no": null, "categories": "cs.DC cs.DM q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the range of probabilistic behaviours that can be engineered with\nChemical Reaction Networks (CRNs). We show that at steady state CRNs are able\nto \"program\" any distribution with finite support in $\\mathbb{N}^m$, with $m\n\\geq 1$. Moreover, any distribution with countable infinite support can be\napproximated with arbitrarily small error under the $L^1$ norm. We also give\noptimized schemes for special distributions, including the uniform\ndistribution. Finally, we formulate a calculus to compute on distributions that\nis complete for finite support distributions, and can be compiled to a\nrestricted class of CRNs that at steady state realize those distributions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 20:14:13 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 10:41:58 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2018 21:44:27 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Cardelli", "Luca", ""], ["Kwiatkowska", "Marta", ""], ["Laurenti", "Luca", ""]]}, {"id": "1601.02687", "submitter": "Jan Huwald", "authors": "Jan Huwald, Stephan Richter, Peter Dittrich", "title": "Compressing molecular dynamics trajectories: breaking the\n  one-bit-per-sample barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT physics.comp-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular dynamics simulations yield large amounts of trajectory data. For\ntheir durable storage and accessibility an efficient compression algorithm is\nparamount. State of the art domain-specific algorithms combine quantization,\nHuffman encoding and occasionally domain knowledge. We propose the high\nresolution trajectory compression scheme (HRTC) that relies on piecewise linear\nfunctions to approximate quantized trajectories. By splitting the error budget\nbetween quantization and approximation, our approach beats the current state of\nthe art by several orders of magnitude given the same error tolerance. It\nallows storing samples at far less than one bit per sample. It is simple and\nfast enough to be integrated into the inner simulation loop, store every time\nstep, and become the primary representation of trajectory data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 15:35:51 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Huwald", "Jan", ""], ["Richter", "Stephan", ""], ["Dittrich", "Peter", ""]]}, {"id": "1601.02752", "submitter": "Rajkumar Buyya", "authors": "Amir Vahid Dastjerdi, Harshit Gupta, Rodrigo N. Calheiros, Soumya K.\n  Ghosh, and Rajkumar Buyya", "title": "Fog Computing: Principles, Architectures, and Applications", "comments": "26 pages, 6 figures, a Book Chapter in Internet of Things: Principles\n  and Paradigms, R. Buyya and A. Dastjerdi (eds), Morgan Kaufmann, Burlington,\n  Massachusetts, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Everything (IoE) solutions gradually bring every object\nonline, and processing data in centralized cloud does not scale to requirements\nof such environment. This is because, there are applications such as health\nmonitoring and emergency response that require low latency and delay caused by\ntransferring data to the cloud and then back to the application can seriously\nimpact the performance. To this end, Fog computing has emerged, where cloud\ncomputing is extended to the edge of the network to decrease the latency and\nnetwork congestion. Fog computing is a paradigm for managing a highly\ndistributed and possibly virtualized environment that provides compute and\nnetwork services between sensors and cloud data centers. This chapter provides\nbackground and motivations on emergence of Fog computing and defines its key\ncharacteristics. In addition, a reference architecture for Fog computing is\npresented and recent related development and applications are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 07:23:21 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 03:33:44 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Dastjerdi", "Amir Vahid", ""], ["Gupta", "Harshit", ""], ["Calheiros", "Rodrigo N.", ""], ["Ghosh", "Soumya K.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1601.02781", "submitter": "Mansaf Alam Dr", "authors": "Farhana Javed Zareen, Kashish Ara Shakil, Mansaf Alam and Suraiya\n  Jabin", "title": "BAMCloud: A Cloud Based Mobile Biometric Authentication Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an exponential increase in number of users switching to mobile banking,\nvarious countries are adopting biometric solutions as security measures. The\nmain reason for biometric technologies becoming more common in the everyday\nlives of consumers is because of the facility to easily capture biometric data\nin real time, using their mobile phones. Biometric technologies are providing\nthe potential security framework to make banking more convenient and secure\nthan it has ever been. At the same time, the exponential growth of enrollment\nin the biometric system produces massive amount of high dimensionality data\nthat leads to degradation in the performance of the mobile banking systems.\nTherefore, in order to overcome the performance issues arising due to this data\ndeluge, this paper aims to propose a distributed mobile biometric system based\non a high performance cluster Cloud. High availability, better time efficiency\nand scalability are some of the added advantages of using the proposed system.\nIn this paper a Cloud based mobile biometric authentication framework\n(BAMCloud) is proposed that uses dynamic signatures and performs\nauthentication. It includes the steps involving data capture using any handheld\nmobile device, then storage, preprocessing and training the system in a\ndistributed manner over Cloud. For this purpose we have implemented it using\nMapReduce on Hadoop platform and for training Levenberg-Marquardt\nbackpropagation neural network has been used. Moreover, the methodology adopted\nis very novel as it achieves a speedup of 8.5x and a performance of 96.23%.\nFurthermore, the cost benefit analysis of the implemented system shows that the\ncost of implementation and execution of the system is lesser than the existing\nones. The experiments demonstrate that the better performance is achieved by\nproposed framework as compared to the other methods used in the recent\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 09:38:44 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 11:23:33 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Zareen", "Farhana Javed", ""], ["Shakil", "Kashish Ara", ""], ["Alam", "Mansaf", ""], ["Jabin", "Suraiya", ""]]}, {"id": "1601.02800", "submitter": "Pedro Lopez-Garcia", "authors": "Umer Liqat, Zorana Bankovic, Pedro Lopez-Garcia and Manuel V.\n  Hermenegildo", "title": "Inferring Energy Bounds via Static Program Analysis and Evolutionary\n  Modeling of Basic Blocks", "comments": "Pre-proceedings paper presented at the 27th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2017), Namur,\n  Belgium, 10-12 October 2017 (arXiv:1708.07854). Improved version of the one\n  presented at the HIP3ES 2016 workshop (v1): more experimental results (added\n  benchmark to Table 1, added figure for new benchmark, added Table 3),\n  improved Fig. 1, added Fig. 4", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2017/31", "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever increasing number and complexity of energy-bound devices (such as\nthe ones used in Internet of Things applications, smart phones, and mission\ncritical systems) pose an important challenge on techniques to optimize their\nenergy consumption and to verify that they will perform their function within\nthe available energy budget. In this work we address this challenge from the\nsoftware point of view and propose a novel parametric approach to estimating\ntight bounds on the energy consumed by program executions that are practical\nfor their application to energy verification and optimization. Our approach\ndivides a program into basic (branchless) blocks and estimates the maximal and\nminimal energy consumption for each block using an evolutionary algorithm. Then\nit combines the obtained values according to the program control flow, using\nstatic analysis, to infer functions that give both upper and lower bounds on\nthe energy consumption of the whole program and its procedures as functions on\ninput data sizes. We have tested our approach on (C-like) embedded programs\nrunning on the XMOS hardware platform. However, our method is general enough to\nbe applied to other microprocessor architectures and programming languages. The\nbounds obtained by our prototype implementation can be tight while remaining on\nthe safe side of budgets in practice, as shown by our experimental evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 10:52:48 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 07:46:56 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Liqat", "Umer", ""], ["Bankovic", "Zorana", ""], ["Lopez-Garcia", "Pedro", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1601.03115", "submitter": "Rajkumar Buyya", "authors": "Caesar Wu, Rajkumar Buyya, and Kotagiri Ramamohanarao", "title": "Big Data Analytics = Machine Learning + Cloud Computing", "comments": "27 pages, 23 figures. a Book Chapter in \"Big Data: Principles and\n  Paradigms, R. Buyya, R. Calheiros, and A. Dastjerdi (eds), Morgan Kaufmann,\n  Burlington, Massachusetts, USA, 2016.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data can mean different things to different people. The scale and\nchallenges of Big Data are often described using three attributes, namely\nVolume, Velocity and Variety (3Vs), which only reflect some of the aspects of\ndata. In this chapter we review historical aspects of the term \"Big Data\" and\nthe associated analytics. We augment 3Vs with additional attributes of Big Data\nto make it more comprehensive and relevant. We show that Big Data is not just\n3Vs, but 32 Vs, that is, 9 Vs covering the fundamental motivation behind Big\nData, which is to incorporate Business Intelligence (BI) based on different\nhypothesis or statistical models so that Big Data Analytics (BDA) can enable\ndecision makers to make useful predictions for some crucial decisions or\nresearching results. History of Big Data has demonstrated that the most cost\neffective way of performing BDA is to employ Machine Learning (ML) on the Cloud\nComputing (CC) based infrastructure or simply, ML + CC -> BDA. This chapter is\ndevoted to help decision makers by defining BDA as a solution and opportunity\nto address their business needs.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 02:18:48 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Wu", "Caesar", ""], ["Buyya", "Rajkumar", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "1601.03144", "submitter": "Bruce Merry", "authors": "Bruce Merry", "title": "A Performance Comparison of Sort and Scan Libraries for GPUs", "comments": "Preprint of an article published in Parallel Processing Letters, 25,\n  4, 2015, 1550007 DOI: 10.1142/S0129626415500073 \\copyright\\ copyright World\n  Scientific Publishing Company\n  <http://www.worldscientific.com/worldscinet/ppl>", "journal-ref": "Parallel Processing Letters, 25(4), 2015, p. 1550007", "doi": "10.1142/S0129626415500073", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting and scanning are two fundamental primitives for constructing highly\nparallel algorithms. A number of libraries now provide implementations of these\nprimitives for GPUs, but there is relatively little information about the\nperformance of these implementations.\n  We benchmark seven libraries for 32-bit integer scan and sort, and sorting\n32-bit values by 32-bit integer keys. We show that there is a large variation\nin performance between the libraries, and that no one library has both optimal\nperformance and portability.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 06:57:50 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Merry", "Bruce", ""]]}, {"id": "1601.03225", "submitter": "Davide Frey", "authors": "Davide Frey, Hicham Lakhlef, Michel Raynal", "title": "Optimal Collision/Conflict-free Distance-2 Coloring in Synchronous\n  Broadcast/Receive Tree Networks", "comments": "19 pages including one appendix. One Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is on message-passing systems where communication is (a)\nsynchronous and (b) based on the \"broadcast/receive\" pair of communication\noperations. \"Synchronous\" means that time is discrete and appears as a sequence\nof time slots (or rounds) such that each message is received in the very same\nround in which it is sent. \"Broadcast/receive\" means that during a round a\nprocess can either broadcast a message to its neighbors or receive a message\nfrom one of them. In such a communication model, no two neighbors of the same\nprocess, nor a process and any of its neighbors, must be allowed to broadcast\nduring the same time slot (thereby preventing message collisions in the first\ncase, and message conflicts in the second case). From a graph theory point of\nview, the allocation of slots to processes is know as the distance-2 coloring\nproblem: a color must be associated with each process (defining the time slots\nin which it will be allowed to broadcast) in such a way that any two processes\nat distance at most 2 obtain different colors, while the total number of colors\nis \"as small as possible\". The paper presents a parallel message-passing\ndistance-2 coloring algorithm suited to trees, whose roots are dynamically\ndefined. This algorithm, which is itself collision-free and conflict-free, uses\n$\\Delta + 1$ colors where $\\Delta$ is the maximal degree of the graph (hence\nthe algorithm is color-optimal). It does not require all processes to have\ndifferent initial identities, and its time complexity is $O(d \\Delta)$, where d\nis the depth of the tree. As far as we know, this is the first distributed\ndistance-2 coloring algorithm designed for the broadcast/receive round-based\ncommunication model, which owns all the previous properties.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 13:13:49 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Frey", "Davide", ""], ["Lakhlef", "Hicham", ""], ["Raynal", "Michel", ""]]}, {"id": "1601.03341", "submitter": "Gorker Alp Malazgirt", "authors": "Gorker Alp Malazgirt, Deniz Candas and Arda Yurdakul", "title": "Taxim: A Toolchain for Automated and Configurable Simulation for\n  Embedded Multiprocessor Design", "comments": "Presented at HIP3ES, 2016", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2016/5", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicore embedded systems have been constantly researched to improve the\nefficiency by changing certain metrics, such as processor, memory, cache\nhierarchies and their cache configurations. Using Multi2Sim and McPAT\nsimulators in combination allows the user to design various multiprocessing\narchitectures and estimate performance, power, area and timing metrics.\nHowever, the design time required to simulate these systems is daunting and\nprone to human error. In this paper, we introduce Taxim, a toolchain that can\nautomatically create requested multicore on-chip topologies along with\nminimizing the simulation time due to repetitive tasks between architectural\npower, energy and timing simulations. Taxim's decision-tree-based topology\nsynthesis tool creates processor configuration files that can be highly\nerroneous when generated manually. The toolchain also automates the steps from\ndesign entry to output report extraction by running automation scripts, and\nlisting the results. Our experiments show that multiprocessing architectures\nwith 32 cores and irregular cache hierarchies are more than 1k lines of code in\nMulti2Sim's processor configuration format and Taxim can create such a file in\nless than 10 milliseconds. The source code is freely available at\nhttps://github.com/bouncaslab/TaXim/.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 18:25:41 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Malazgirt", "Gorker Alp", ""], ["Candas", "Deniz", ""], ["Yurdakul", "Arda", ""]]}, {"id": "1601.03555", "submitter": "Yue Cao Dr", "authors": "Yue Cao, Kaimin Wei, Geyong Min, Jian Weng, Xin Yang, Zhili Sun", "title": "A Geographic Multi-Copy Routing Scheme for DTNs With Heterogeneous\n  Mobility", "comments": "IEEE Systems Journal 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous geographic routing schemes in Delay/Disruption Tolerant Networks\n(DTNs) only consider the homogeneous scenario where nodal mobility is\nidentical. Motivated by this gap, we turn to design a DTN based geographic\nrouting scheme in heterogeneous scenario. Systematically, our target is\nachieved via two steps: 1) We first propose \"The-Best-Geographic-Relay (TBGR)\"\nrouting scheme to relay messages via a limited number of copies, under the\nhomogeneous scenario. We further overcome the local maximum problem of TBGR\ngiven a sparse network density, different from those efforts in dense networks\nlike clustered Wireless Sensor Networks (WSNs). 2) We next extend TBGR for\nheterogeneous scenario, and propose \"The-Best-Heterogeneity-Geographic-Relay\n(TBHGR)\" routing scheme considering individual nodal visiting preference\n(referred to non-identical nodal mobility). Extensive results under a realistic\nheterogeneous scenario show the advantage of TBHGR over literature works in\nterms of reliable message delivery, while with low routing overhead.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 11:10:44 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 13:33:26 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Cao", "Yue", ""], ["Wei", "Kaimin", ""], ["Min", "Geyong", ""], ["Weng", "Jian", ""], ["Yang", "Xin", ""], ["Sun", "Zhili", ""]]}, {"id": "1601.03623", "submitter": "Martina Prugger", "authors": "Martina Prugger, Lukas Einkemmer, Alexander Ostermann", "title": "Evaluation of the Partitioned Global Address Space (PGAS) model for an\n  inviscid Euler solver", "comments": "Parallel Computing 2016", "journal-ref": "Parallel Computing, Volume 60, December 2016, Pages 22-40", "doi": "10.1016/j.parco.2016.11.001", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate the performance of Unified Parallel C (which\nimplements the partitioned global address space programming model) using a\nnumerical method that is widely used in fluid dynamics. In order to evaluate\nthe incremental approach to parallelization (which is possible with UPC) and\nits performance characteristics, we implement different levels of optimization\nof the UPC code and compare it with an MPI parallelization on four different\nclusters of the Austrian HPC infrastructure (LEO3, LEO3E, VSC2, VSC3) and on an\nIntel Xeon Phi. We find that UPC is significantly easier to develop in compared\nto MPI and that the performance achieved is comparable to MPI in most\nsituations. The obtained results show worse performance (on VSC2), competitive\nperformance (on LEO3, LEO3E and VSC3), and superior performance (on the Intel\nXeon Phi).\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 15:30:47 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 20:50:57 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Prugger", "Martina", ""], ["Einkemmer", "Lukas", ""], ["Ostermann", "Alexander", ""]]}, {"id": "1601.03708", "submitter": "Piotr Dziurzanski", "authors": "Piotr Dziurzanski, Amit Kumar Singh, Leandro S. Indrusiak, Bj\\\"orn\n  Saballus", "title": "Benchmarking, System Design and Case-studies for Multi-core based\n  Embedded Automotive Systems", "comments": "2nd International Workshop on Dynamic Resource Allocation and\n  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016\n  (arXiv:cs/1449078)", "journal-ref": null, "doi": null, "report-no": "DREAMCloud/2016/04", "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, using of automotive use cases as benchmarks for real-time\nsystem design has been proposed. The use cases are described in a format\nsupported by AMALTHEA platform, which is a model based open source development\nenvironment for automotive multi-core systems. An example of a simple\nElectronic Control Unit has been analysed and presented with enough details to\nreconstruct this system in any format. For researchers willing to use AMALTHEA\nfile format directly, an appropriate parser has been developed and offered. An\nexample of applying this parser and benchmark for optimising makespan while not\nviolating the timing constraints by allocating functionality to different\nNetwork on Chip resource is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 19:39:44 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Dziurzanski", "Piotr", ""], ["Singh", "Amit Kumar", ""], ["Indrusiak", "Leandro S.", ""], ["Saballus", "Bj\u00f6rn", ""]]}, {"id": "1601.03767", "submitter": "Camille Coti", "authors": "Camille Coti and Charles Lakos and Laure Petrucci", "title": "Formally Proving and Enhancing a Self-Stabilising Distributed Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the benefits of formal modelling and verification\ntechniques for self-stabilising distributed algorithms. An algorithm is\nstudied, that takes a set of processes connected by a tree topology and\nconverts it to a ring configuration. The Coloured Petri net model not only\nfacilitates the proof that the algorithm is correct and self-stabilising but\nalso easily shows that it enjoys new properties of termination and silentness.\nFurther, the formal results show how the algorithm can be simplified without\nloss of generality.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 22:08:02 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Coti", "Camille", ""], ["Lakos", "Charles", ""], ["Petrucci", "Laure", ""]]}, {"id": "1601.03854", "submitter": "Junaid Shuja", "authors": "Misbah Liaqat, Shalini Ninoriya, Junaid Shuja, Raja Wasim Ahmad,\n  Abdullah Gani", "title": "Virtual Machine Migration Enabled Cloud Resource Management: A\n  Challenging Task", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization technology reduces cloud operational cost by increasing cloud\nresource utilization level. The incorporation of virtualization within cloud\ndata centers can severely degrade cloud performance if not properly managed.\nVirtual machine (VM) migration is a method that assists cloud service providers\nto efficiently manage cloud resources while eliminating the need of human\nsupervision. VM migration methodology migrates current-hosted workload from one\nserver to another by either employing live or non-live migration pattern. In\ncomparison to non-live migration, live migration does not suspend application\nservices prior to VM migration process. VM migration enables cloud operators to\nachieve various resource management goals, such as, green computing, load\nbalancing, fault management, and real time server maintenance. In this paper,\nwe have thoroughly surveyed VM migration methods and applications. We have\nbriefly discussed VM migration applications. Some open research issues have\nbeen highlighted to represent future challenges in this domain. A queue based\nmigration model has been proposed and discussed to efficiently migrate VM\nmemory pages.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 09:36:25 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Liaqat", "Misbah", ""], ["Ninoriya", "Shalini", ""], ["Shuja", "Junaid", ""], ["Ahmad", "Raja Wasim", ""], ["Gani", "Abdullah", ""]]}, {"id": "1601.03872", "submitter": "Blesson Varghese", "authors": "Blesson Varghese, Lawan Thamsuhang Subba, Long Thai, Adam Barker", "title": "Container-Based Cloud Virtual Machine Benchmarking", "comments": "Accepted to the IEEE International Conference on Cloud Engineering\n  (IEEE IC2E), Berlin, Germany, 2016 - 10 pages", "journal-ref": null, "doi": "10.1109/IC2E.2016.28", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of a wide range of cloud Virtual Machines (VMs) it is\ndifficult to determine which VMs can maximise the performance of an\napplication. Benchmarking is commonly used to this end for capturing the\nperformance of VMs. Most cloud benchmarking techniques are typically\nheavyweight - time consuming processes which have to benchmark the entire VM in\norder to obtain accurate benchmark data. Such benchmarks cannot be used in\nreal-time on the cloud and incur extra costs even before an application is\ndeployed.\n  In this paper, we present lightweight cloud benchmarking techniques that\nexecute quickly and can be used in near real-time on the cloud. The exploration\nof lightweight benchmarking techniques are facilitated by the development of\nDocLite - Docker Container-based Lightweight Benchmarking. DocLite is built on\nthe Docker container technology which allows a user-defined portion (such as\nmemory size and the number of CPU cores) of the VM to be benchmarked. DocLite\noperates in two modes, in the first mode, containers are used to benchmark a\nsmall portion of the VM to generate performance ranks. In the second mode,\nhistoric benchmark data is used along with the first mode as a hybrid to\ngenerate VM ranks. The generated ranks are evaluated against three scientific\nhigh-performance computing applications. The proposed techniques are up to 91\ntimes faster than a heavyweight technique which benchmarks the entire VM. It is\nobserved that the first mode can generate ranks with over 90% and 86% accuracy\nfor sequential and parallel execution of an application. The hybrid mode\nimproves the correlation slightly but the first mode is sufficient for\nbenchmarking cloud VMs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 10:57:02 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Varghese", "Blesson", ""], ["Subba", "Lawan Thamsuhang", ""], ["Thai", "Long", ""], ["Barker", "Adam", ""]]}, {"id": "1601.03980", "submitter": "Pradeeban Kathiravelu", "authors": "Pradeeban Kathiravelu", "title": "An Elastic Middleware Platform for Concurrent and Distributed Cloud and\n  MapReduce Simulations", "comments": "Thesis to obtain the Master of Science Degree in Information Systems\n  and Computer Engineering, Instituto Superior Tecnico, Universidade de Lisboa.\n  2014 September", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing researches involve a tremendous amount of entities such as\nusers, applications, and virtual machines. Due to the limited access and often\nvariable availability of such resources, researchers have their prototypes\ntested against the simulation environments, opposed to the real cloud\nenvironments. Existing cloud simulation environments such as CloudSim and\nEmuSim are executed sequentially, where a more advanced cloud simulation tool\ncould be created extending them, leveraging the latest technologies as well as\nthe availability of multi-core computers and the clusters in the research\nlaboratories. While computing has been evolving with multi-core programming,\nMapReduce paradigms, and middleware platforms, cloud and MapReduce simulations\nstill fail to exploit these developments themselves. This research develops\nCloud2Sim, which tries to fill the gap between the simulations and the actual\ntechnology that they are trying to simulate.\n  First, Cloud2Sim provides a concurrent and distributed cloud simulator, by\nextending CloudSim cloud simulator, using Hazelcast in-memory key-value store.\nThen, it also provides a quick assessment to MapReduce implementations of\nHazelcast and Infinispan, adaptively distributing the execution to a cluster,\nproviding means of simulating MapReduce executions. The dynamic scaler solution\nscales out the cloud and MapReduce simulations to multiple nodes running\nHazelcast and Infinispan, based on load. The distributed execution model and\nadaptive scaling solution could be leveraged as a general purpose auto scaler\nmiddleware for a multi-tenanted deployment.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 16:04:02 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Kathiravelu", "Pradeeban", ""]]}, {"id": "1601.04059", "submitter": "Gesualdo Scutari", "authors": "Gesualdo Scutari, Francisco Facchinei, Lorenzo Lampariello, Peiran\n  Song, and Stefania Sardellitti", "title": "Parallel and Distributed Methods for Nonconvex Optimization--Part II:\n  Applications", "comments": "Part I of this paper can be found at http://arxiv.org/abs/1410.4754", "journal-ref": null, "doi": "10.1109/TSP.2016.2637317", "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Part I of this paper, we proposed and analyzed a novel algorithmic\nframework for the minimization of a nonconvex (smooth) objective function,\nsubject to nonconvex constraints, based on inner convex approximations. This\nPart II is devoted to the application of the framework to some resource\nallocation problems in communication networks. In particular, we consider two\nnon-trivial case-study applications, namely: (generalizations of) i) the rate\nprofile maximization in MIMO interference broadcast networks; and the ii) the\nmax-min fair multicast multigroup beamforming problem in a multi-cell\nenvironment. We develop a new class of algorithms enjoying the following\ndistinctive features: i) they are \\emph{distributed} across the base stations\n(with limited signaling) and lead to subproblems whose solutions are computable\nin closed form; and ii) differently from current relaxation-based schemes\n(e.g., semidefinite relaxation), they are proved to always converge to\nd-stationary solutions of the aforementioned class of nonconvex problems.\nNumerical results show that the proposed (distributed) schemes achieve larger\nworst-case rates (resp. signal-to-noise interference ratios) than\nstate-of-the-art centralized ones while having comparable computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 20:45:34 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Scutari", "Gesualdo", ""], ["Facchinei", "Francisco", ""], ["Lampariello", "Lorenzo", ""], ["Song", "Peiran", ""], ["Sardellitti", "Stefania", ""]]}, {"id": "1601.04102", "submitter": "Rodrigo de Lamare", "authors": "R. C. de Lamare", "title": "Study of Distributed Conjugate Gradient Strategies for Distributed\n  Estimation Over Sensor Networks", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents distributed conjugate gradient algorithms for distributed\nparameter estimation and spectrum estimation over wireless sensor networks. In\nparticular, distributed conventional conjugate gradient (CCG) and modified\nconjugate gradient (MCG) are considered, together with incremental and\ndiffusion adaptive solutions. The distributed CCG and MCG algorithms have an\nimproved performance in terms of mean square error as compared with least--mean\nsquare (LMS)--based algorithms and a performance that is close to recursive\nleast--squares (RLS) algorithms. In comparison with existing centralized or\ndistributed estimation strategies, key features of the proposed algorithms are:\n1) more accurate estimates and faster convergence speed can be obtained; 2) the\ndesign of preconditioners for CG algorithms, which have the ability to improve\nthe performance of the proposed CG algorithms is presented and 3) the proposed\nalgorithms are implemented in the area of distributed parameter estimation and\nspectrum estimation. The performance of the proposed algorithms for distributed\nestimation is illustrated via simulations and the resulting algorithms are\ndistributed, cooperative and able to respond in real time to change in the\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 00:51:27 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["de Lamare", "R. C.", ""]]}, {"id": "1601.04231", "submitter": "Vincenzo De Florio", "authors": "Vincenzo De Florio and Geert Deconinck and Rudy Lauwereins", "title": "An Algorithm for Tolerating Crash Failures in Distributed Systems", "comments": "Appeared in the Proceedings of the 7th Annual IEEE Int.l Conference\n  and Workshop on the Engineering of Computer Based Systems (ECBS 2000),\n  Edinburgh, Scotland, April 3, 2000", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of the ESPRIT project 28620 \"TIRAN\" (tailorable fault\ntolerance frameworks for embedded applications), a toolset of error detection,\nisolation, and recovery components is being designed to serve as a basic means\nfor orchestrating application-level fault tolerance. These tools will be used\neither as stand-alone components or as the peripheral components of a\ndistributed application, that we call 'the backbone\". The backbone is to run in\nthe background of the user application. Its objectives include (1) gathering\nand maintaining error detection information produced by TIRAN components like\nwatchdog timers, trap handlers, or by external detection services working at\nkernel or driver level, and (2) using this information at error recovery time.\nIn particular, those TIRAN tools related to error detection and fault masking\nwill forward their deductions to the backbone that, in turn, will make use of\nthis information to orchestrate error recovery, requesting recovery and\nreconfiguration actions to those tools related to error isolation and recovery.\nClearly a key point in this approach is guaranteeing that the backbone itself\ntolerates internal and external faults. In this article we describe one of the\nmeans that are used within the TIRAN backbone to fulfill this goal: a\ndistributed algorithm for tolerating crash failures triggered by faults\naffecting at most all but one of the components of the backbone or at most all\nbut one of the nodes of the system. We call this the algorithm of mutual\nsuspicion.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 23:43:42 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["De Florio", "Vincenzo", ""], ["Deconinck", "Geert", ""], ["Lauwereins", "Rudy", ""]]}, {"id": "1601.04306", "submitter": "Peter Jeavons", "authors": "Peter Jeavons, Alex Scott, Lei Xu", "title": "Feedback from Nature: Simple Randomised Distributed Algorithms for\n  Maximal Independent Set Selection and Greedy Colouring", "comments": "arXiv admin note: text overlap with arXiv:1211.0235", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose distributed algorithms for two well-established problems that\noperate efficiently under extremely harsh conditions. Our algorithms achieve\nstate-of-the-art performance in a simple and novel way.\n  Our algorithm for maximal independent set selection operates on a network of\nidentical anonymous processors. The processor at each node has no prior\ninformation about the network. At each time step, each node can only broadcast\na single bit to all its neighbours, or remain silent. Each node can detect\nwhether one or more neighbours have broadcast, but cannot tell how many of its\nneighbours have broadcast, or which ones. We build on recent work of Afek et\nal. which was inspired by studying the development of a network of cells in the\nfruit fly~\\cite{Afek2011a}. However we incorporate for the first time another\nimportant feature of the biological system: varying the probability value used\nat each node based on local feedback from neighbouring nodes. Given any\n$n$-node network, our algorithm achieves the optimal expected time complexity\nof $O(\\log n)$ rounds and the optimal expected message complexity of $O(1)$\nsingle-bit messages broadcast by each node.We also show that the previous\napproach, without feedback, cannot achieve better than $\\Omega(\\log^2 n)$\nexpected time complexity, whatever global scheme is used to choose the\nprobabilities.\n  Our algorithm for distributed greedy colouring works under similar harsh\nconditions: each identical node has no prior information about the network, can\nonly broadcast a single message to all neighbours at each time step\nrepresenting a desired colour, and can only detect whether at least one\nneighbour has broadcast each colour value. We show that our algorithm has an\nexpected time complexity of $O(\\Delta+\\log n)$, where $\\Delta$ is the maximum\ndegree of the network, and expected message complexity of $O(1)$ messages\nbroadcast by each node.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 16:19:50 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Jeavons", "Peter", ""], ["Scott", "Alex", ""], ["Xu", "Lei", ""]]}, {"id": "1601.04595", "submitter": "Puxiao Han", "authors": "Puxiao Han, Junan Zhu, Ruixin Niu, Dror Baron", "title": "Multi-Processor Approximate Message Passing Using Lossy Compression", "comments": "to appear at icassp 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a communication-efficient multi-processor compressed sensing\nframework based on the approximate message passing algorithm is proposed. We\nperform lossy compression on the data being communicated between processors,\nresulting in a reduction in communication costs with a minor degradation in\nrecovery quality. In the proposed framework, a new state evolution formulation\ntakes the quantization error into account, and analytically determines the\ncoding rate required in each iteration. Two approaches for allocating the\ncoding rate, an online back-tracking heuristic and an optimal allocation scheme\nbased on dynamic programming, provide significant reductions in communication\ncosts.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 16:41:44 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Han", "Puxiao", ""], ["Zhu", "Junan", ""], ["Niu", "Ruixin", ""], ["Baron", "Dror", ""]]}, {"id": "1601.04602", "submitter": "Kevin Taylor-Sakyi", "authors": "Kevin Taylor-Sakyi", "title": "Big Data: Understanding Big Data", "comments": "8 pages, Big Data Analytics, Data Storage, MapReduce,\n  Knowledge-Space, Big Data Inconsistencies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steve Jobs, one of the greatest visionaries of our time was quoted in 1996\nsaying \"a lot of times, people do not know what they want until you show it to\nthem\" [38] indicating he advocated products to be developed based on human\nintuition rather than research. With the advancements of mobile devices, social\nnetworks and the Internet of Things, enormous amounts of complex data, both\nstructured and unstructured are being captured in hope to allow organizations\nto make better business decisions as data is now vital for an organizations\nsuccess. These enormous amounts of data are referred to as Big Data, which\nenables a competitive advantage over rivals when processed and analyzed\nappropriately. However Big Data Analytics has a few concerns including\nManagement of Data-lifecycle, Privacy & Security, and Data Representation. This\npaper reviews the fundamental concept of Big Data, the Data Storage domain, the\nMapReduce programming paradigm used in processing these large datasets, and\nfocuses on two case studies showing the effectiveness of Big Data Analytics and\npresents how it could be of greater good in the future if handled\nappropriately.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 19:10:43 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Taylor-Sakyi", "Kevin", ""]]}, {"id": "1601.04675", "submitter": "Piotr Dziurzanski", "authors": "Leandro Soares Indrusiak, Piotr Dziurzanski and Amit Kumar Singh", "title": "2nd International Workshop on Dynamic Resource Allocation and Management\n  in Embedded, High Performance and Cloud Computing (DREAMCloud 2016)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume represents the proceedings of the 2nd International Workshop on\nDynamic Resource Allocation and Management in Embedded, High Performance and\nCloud Computing (DREAMCloud 2016), co-located with HiPEAC 2016 on 19th January\n2016 in Prague, Czech Republic.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 08:41:55 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Indrusiak", "Leandro Soares", ""], ["Dziurzanski", "Piotr", ""], ["Singh", "Amit Kumar", ""]]}, {"id": "1601.04820", "submitter": "Michel Raynal", "authors": "Achour Mostefaoui (LINA), Michel Raynal (ASAP)", "title": "Time-Efficient Read/Write Register in Crash-prone Asynchronous\n  Message-Passing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The atomic register is certainly the most basic object of computing science.\nIts implementation on top of an n-process asynchronous message-passing system\nhas received a lot of attention. It has been shown that t \\textless{} n/2\n(where t is the maximal number of processes that may crash) is a necessary and\nsufficient requirement to build an atomic register on top of a crash-prone\nasynchronous message-passing system. Considering such a context, this paper\nvisits the notion of a fast implementation of an atomic register, and presents\na new time-efficient asynchronous algorithm. Its time-efficiency is measured\naccording to two different underlying synchrony assumptions. Whatever this\nassumption, a write operation always costs a round-trip delay, while a read\noperation costs always a round-trip delay in favorable circumstances\n(intuitively, when it is not concurrent with a write). When designing this\nalgorithm, the design spirit was to be as close as possible to the one of the\nfamous ABD algorithm (proposed by Attiya, Bar-Noy, and Dolev).\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 08:38:42 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Mostefaoui", "Achour", "", "LINA"], ["Raynal", "Michel", "", "ASAP"]]}, {"id": "1601.04862", "submitter": "Christoph Richter", "authors": "Christoph Richter, S\\\"oren Jentzsch, Rafael Hostettler, Jes\\'us A.\n  Garrido, Eduardo Ros, Alois C. Knoll, Florian R\\\"ohrbein, Patrick van der\n  Smagt, J\\\"org Conradt", "title": "Scalability in Neural Control of Musculoskeletal Robots", "comments": "Accepted at IEEE Robotics and Automation Magazine on 2015-12-31", "journal-ref": null, "doi": "10.1109/MRA.2016.2535081", "report-no": null, "categories": "cs.RO cs.DC cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anthropomimetic robots are robots that sense, behave, interact and feel like\nhumans. By this definition, anthropomimetic robots require human-like physical\nhardware and actuation, but also brain-like control and sensing. The most\nself-evident realization to meet those requirements would be a human-like\nmusculoskeletal robot with a brain-like neural controller. While both\nmusculoskeletal robotic hardware and neural control software have existed for\ndecades, a scalable approach that could be used to build and control an\nanthropomimetic human-scale robot has not been demonstrated yet. Combining\nMyorobotics, a framework for musculoskeletal robot development, with SpiNNaker,\na neuromorphic computing platform, we present the proof-of-principle of a\nsystem that can scale to dozens of neurally-controlled, physically compliant\njoints. At its core, it implements a closed-loop cerebellar model which\nprovides real-time low-level neural control at minimal power consumption and\nmaximal extensibility: higher-order (e.g., cortical) neural networks and\nneuromorphic sensors like silicon-retinae or -cochleae can naturally be\nincorporated.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 10:29:12 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Richter", "Christoph", ""], ["Jentzsch", "S\u00f6ren", ""], ["Hostettler", "Rafael", ""], ["Garrido", "Jes\u00fas A.", ""], ["Ros", "Eduardo", ""], ["Knoll", "Alois C.", ""], ["R\u00f6hrbein", "Florian", ""], ["van der Smagt", "Patrick", ""], ["Conradt", "J\u00f6rg", ""]]}, {"id": "1601.05052", "submitter": "Alessio Sclocco", "authors": "Alessio Sclocco, Henri E. Bal, Jason Hessels, Joeri van Leeuwen, Rob\n  V. van Nieuwpoort", "title": "Auto-Tuning Dedispersion for Many-Core Accelerators", "comments": "10 pages, published in the proceedings of IPDPS 2014", "journal-ref": null, "doi": "10.1109/IPDPS.2014.101", "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the parallelization of the dedispersion algorithm on\nmany-core accelerators, including GPUs from AMD and NVIDIA, and the Intel Xeon\nPhi. An important contribution is the computational analysis of the algorithm,\nfrom which we conclude that dedispersion is inherently memory-bound in any\nrealistic scenario, in contrast to earlier reports. We also provide empirical\nproof that, even in unrealistic scenarios, hardware limitations keep the\narithmetic intensity low, thus limiting performance. We exploit auto-tuning to\nadapt the algorithm, not only to different accelerators, but also to different\nobservations, and even telescopes. Our experiments show how the algorithm is\ntuned automatically for different scenarios and how it exploits and highlights\nthe underlying specificities of the hardware: in some observations, the tuner\nautomatically optimizes device occupancy, while in others it optimizes memory\nbandwidth. We quantitatively analyze the problem space, and by comparing the\nresults of optimal auto-tuned versions against the best performing fixed codes,\nwe show the impact that auto-tuning has on performance, and conclude that it is\nstatistically relevant.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 10:36:55 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Sclocco", "Alessio", ""], ["Bal", "Henri E.", ""], ["Hessels", "Jason", ""], ["van Leeuwen", "Joeri", ""], ["van Nieuwpoort", "Rob V.", ""]]}, {"id": "1601.05313", "submitter": "Rafael Rodriguez-Sanchez", "authors": "Rafael Rodr\\'iguez-S\\'anchez and Enrique S. Quintana-Ort\\'i", "title": "Architecture-Aware Optimization of an HEVC decoder on Asymmetric\n  Multicore Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-power asymmetric multicore processors (AMPs) attract considerable\nattention due to their appealing performance-power ratio for energy-constrained\nenvironments. However, these processors pose a significant programming\nchallenge due to the integration of cores with different performance\ncapabilities, asking for an asymmetry-aware scheduling solution that carefully\ndistributes the workload.\n  The recent HEVC standard, which offers several high-level parallelization\nstrategies, is an important application that can benefit from an implementation\ntailored for the low-power AMPs present in many current mobile or hand-held\ndevices. In this scenario, we present an architecture-aware implementation of\nan HEVC decoder that embeds a criticality-aware scheduling strategy tuned for a\nSamsung Exynos 5422 system-on-chip furnished with an ARM big.LITTLE AMP. The\nperformance and energy efficiency of our solution is further enhanced by\nexploiting the NEON vector engine available in the ARM big.LITTLE architecture.\nExperimental results expose a 1080p real-time HEVC decoding at 24 frames/sec,\nand a reduction of energy consumption over 20%.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 15:55:29 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Rodr\u00edguez-S\u00e1nchez", "Rafael", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""]]}, {"id": "1601.05384", "submitter": "Pedro Gonnet", "authors": "Pedro Gonnet, Aidan B. G. Chalk and Matthieu Schaller", "title": "QuickSched: Task-based parallelism with dependencies and conflicts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes QuickSched, a compact and efficient Open-Source\nC-language library for task-based shared-memory parallel programming.\nQuickSched extends the standard dependency-only scheme of task-based\nprogramming with the concept of task conflicts, i.e.~sets of tasks that can be\nexecuted in any order, yet not concurrently. These conflicts are modelled using\nexclusively lockable hierarchical resources. The scheduler itself prioritizes\ntasks along the critical path of execution and is shown to perform and scale\nwell on a 64-core parallel shared-memory machine for two example problems: A\ntiled QR decomposition and a task-based Barnes-Hut tree code.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 19:41:17 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Gonnet", "Pedro", ""], ["Chalk", "Aidan B. G.", ""], ["Schaller", "Matthieu", ""]]}, {"id": "1601.05400", "submitter": "Mads Kristensen", "authors": "Mads R. B. Kristensen, Simon A. F. Lund, Troels Blum, James Avery", "title": "Fusion of Array Operations at Runtime", "comments": "Preprint", "journal-ref": "Proceeding PACT '16 Proceedings of the 2016 International\n  Conference on Parallel Architectures and Compilation Pages 71-85", "doi": "10.1145/2967938.2967945", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of fusing array operations based on criteria such as\nshape compatibility, data reusability, and communication. We formulate the\nproblem as a graph partition problem that is general enough to handle loop\nfusion, combinator fusion, and other types of subroutines.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 20:21:01 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 10:31:17 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Kristensen", "Mads R. B.", ""], ["Lund", "Simon A. F.", ""], ["Blum", "Troels", ""], ["Avery", "James", ""]]}, {"id": "1601.05439", "submitter": "Antons Treikalis", "authors": "Antons Treikalis, Andre Merzky, Haoyuan Chen, Tai-Sung Lee, Darrin M.\n  York, Shantenu Jha", "title": "RepEx: A Flexible Framework for Scalable Replica Exchange Molecular\n  Dynamics Simulations", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replica Exchange (RE) simulations have emerged as an important algorithmic\ntool for the molecular sciences. RE simulations involve the concurrent\nexecution of independent simulations which infrequently interact and exchange\ninformation. The next set of simulation parameters are based upon the outcome\nof the exchanges.\n  Typically RE functionality is integrated into the molecular simulation\nsoftware package. A primary motivation of the tight integration of RE\nfunctionality with simulation codes has been performance. This is limiting at\nmultiple levels. First, advances in the RE methodology are tied to the\nmolecular simulation code. Consequently these advances remain confined to the\nmolecular simulation code for which they were developed. Second, it is\ndifficult to extend or experiment with novel RE algorithms, since expertise in\nthe molecular simulation code is typically required.\n  In this paper, we propose the RepEx framework which address these\naforementioned shortcomings of existing approaches, while striking the balance\nbetween flexibility (any RE scheme) and scalability (tens of thousands of\nreplicas) over a diverse range of platforms. RepEx is designed to use a\npilot-job based runtime system and support diverse RE Patterns and Execution\nModes. RE Patterns are concerned with synchronization mechanisms in RE\nsimulation, and Execution Modes with spatial and temporal mapping of workload\nto the CPU cores. We discuss how the design and implementation yield the\nfollowing primary contributions of the RepEx framework: (i) its ability to\nsupport different RE schemes independent of molecular simulation codes, (ii)\nprovide the ability to execute different exchange schemes and replica counts\nindependent of the specific availability of resources, (iii) provide a runtime\nsystem that has first-class support for task-level parallelism, and (iv)\nrequired scalability along multiple dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 21:23:10 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Treikalis", "Antons", ""], ["Merzky", "Andre", ""], ["Chen", "Haoyuan", ""], ["Lee", "Tai-Sung", ""], ["York", "Darrin M.", ""], ["Jha", "Shantenu", ""]]}, {"id": "1601.05458", "submitter": "Benoit Meister", "authors": "Benoit Meister, Muthu Baskaran, Benoit Pradelle, Thomas Henretty,\n  Richard Lethin", "title": "Efficient Compilation to Event-Driven Task Programs", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As illustrated by the emergence of a class of new languages and runtimes, it\nis expected that a large portion of the programs to run on extreme scale\ncomputers will need to be written as graphs of event-driven tasks (EDTs). EDT\nruntime systems, which schedule such collections of tasks, enable more\nconcurrency than traditional runtimes by reducing the amount of inter-task\nsynchronization, improving dynamic load balancing and making more operations\nasynchronous.\n  We present an efficient technique to generate such task graphs from a\npolyhedral representation of a program, both in terms of compilation time and\nasymptotic execution time. Task dependences become materialized in different\nforms, depending upon the synchronization model available with the targeted\nruntime.\n  We explore the different ways of programming EDTs using each synchronization\nmodel, and identify important sources of overhead associated with them. We\nevaluate these programming schemes according to the cost they entail in terms\nof sequential start-up, in-flight task management, space used for\nsynchronization objects, and garbage collection of these objects.\n  While our implementation and evaluation take place in a polyhedral compiler,\nthe presented overhead cost analysis is useful in the more general context of\nautomatic code generation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 22:26:03 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Meister", "Benoit", ""], ["Baskaran", "Muthu", ""], ["Pradelle", "Benoit", ""], ["Henretty", "Thomas", ""], ["Lethin", "Richard", ""]]}, {"id": "1601.05527", "submitter": "Emmanuel John Emmanuel John", "authors": "Emmanuel John and Ilya Safro", "title": "Single- and Multi-level Network Sparsification by Algebraic Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network sparsification methods play an important role in modern network\nanalysis when fast estimation of computationally expensive properties (such as\nthe diameter, centrality indices, and paths) is required. We propose a method\nof network sparsification that preserves a wide range of structural properties.\nDepending on the analysis goals, the method allows to distinguish between local\nand global range edges that can be filtered out during the sparsification.\nFirst we rank edges by their algebraic distances and then we sample them. We\nalso introduce a multilevel framework for sparsification that can be used to\ncontrol the sparsification process at various coarse-grained resolutions. Based\nprimarily on the matrix-vector multiplications, our method is easily\nparallelized for different architectures.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 07:09:21 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["John", "Emmanuel", ""], ["Safro", "Ilya", ""]]}, {"id": "1601.05590", "submitter": "Da Yan", "authors": "Da Yan, Yuzhen Huang, James Cheng, Huanhuan Wu", "title": "Efficient Processing of Very Large Graphs in a Small Cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the success of Google's Pregel, many systems have been developed\nrecently for iterative computation over big graphs. These systems provide a\nuser-friendly vertex-centric programming interface, where a programmer only\nneeds to specify the behavior of one generic vertex when developing a parallel\ngraph algorithm. However, most existing systems require the input graph to\nreside in memories of the machines in a cluster, and the few out-of-core\nsystems suffer from problems such as poor efficiency for sparse computation\nworkload, high demand on network bandwidth, and expensive cost incurred by\nexternal-memory join and group-by.\n  In this paper, we introduce the GraphD system for a user to process very\nlarge graphs with ordinary computing resources. GraphD fully overlaps\ncomputation with communication, by streaming edges and messages on local disks,\nwhile transmitting messages in parallel. For a broad class of Pregel algorithms\nwhere message combiner is applicable, GraphD eliminates the need of any\nexpensive external-memory join or group-by. These key techniques allow GraphD\nto achieve comparable performance to in-memory Pregel-like systems without\nkeeping edges and messages in memories. We prove that to process a graph G=(V,\nE) with n machines using GraphD, each machine only requires O(|V|/n) memory\nspace, allowing GraphD to scale to very large graphs with a small cluster.\nExtensive experiments show that GraphD beats existing out-of-core systems by\norders of magnitude, and achieves comparable performance to in-memory systems\nrunning with enough memories.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 11:18:34 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Yan", "Da", ""], ["Huang", "Yuzhen", ""], ["Cheng", "James", ""], ["Wu", "Huanhuan", ""]]}, {"id": "1601.05725", "submitter": "Joshua Booth", "authors": "Joshua Dennis Booth, Sivasankaran Rajamanickam and Heidi K. Thornquist", "title": "Basker: A Threaded Sparse LU Factorization Utilizing Hierarchical\n  Parallelism and Data Layouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable sparse LU factorization is critical for efficient numerical\nsimulation of circuits and electrical power grids. In this work, we present a\nnew scalable sparse direct solver called Basker. Basker introduces a new\nalgorithm to parallelize the Gilbert-Peierls algorithm for sparse LU\nfactorization. As architectures evolve, there exists a need for algorithms that\nare hierarchical in nature to match the hierarchy in thread teams, individual\nthreads, and vector level parallelism. Basker is designed to map well to this\nhierarchy in architectures. There is also a need for data layouts to match\nmultiple levels of hierarchy in memory. Basker uses a two-dimensional\nhierarchical structure of sparse matrices that maps to the hierarchy in the\nmemory architectures and to the hierarchy in parallelism. We present\nperformance evaluations of Basker on the Intel SandyBridge and Xeon Phi\nplatforms using circuit and power grid matrices taken from the University of\nFlorida sparse matrix collection and from Xyce circuit simulations. Basker\nachieves a geometric mean speedup of 5.91x on CPU (16 cores) and 7.4x on Xeon\nPhi (32 cores) relative to KLU. Basker outperforms Intel MKL Pardiso (PMKL) by\nas much as 53x on CPU (16 cores) and 13.3x on Xeon Phi (32 cores) for low\nfill-in circuit matrices. Furthermore, Basker provides 5.4x speedup on a\nchallenging matrix sequence taken from an actual Xyce simulation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 17:51:44 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Booth", "Joshua Dennis", ""], ["Rajamanickam", "Sivasankaran", ""], ["Thornquist", "Heidi K.", ""]]}, {"id": "1601.05904", "submitter": "Gang Mei", "authors": "Gang Mei, Nengxiong Xu, Liangliang Xu", "title": "Improving GPU-accelerated Adaptive IDW Interpolation Algorithm Using\n  Fast kNN Search", "comments": "Submitted manuscript. 9 Figures, 3 Tables", "journal-ref": "SpringerPlus 2016 5:1389", "doi": "10.1186/s40064-016-3035-2", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient parallel Adaptive Inverse Distance Weighting\n(AIDW) interpolation algorithm on modern Graphics Processing Unit (GPU). The\npresented algorithm is an improvement of our previous GPU-accelerated AIDW\nalgorithm by adopting fast k-Nearest Neighbors (kNN) search. In AIDW, it needs\nto find several nearest neighboring data points for each interpolated point to\nadaptively determine the power parameter; and then the desired prediction value\nof the interpolated point is obtained by weighted interpolating using the power\nparameter. In this work, we develop a fast kNN search approach based on the\nspace-partitioning data structure, even grid, to improve the previous\nGPU-accelerated AIDW algorithm. The improved algorithm is composed of the\nstages of kNN search and weighted interpolating. To evaluate the performance of\nthe improved algorithm, we perform five groups of experimental tests.\nExperimental results show that: (1) the improved algorithm can achieve a\nspeedup of up to 1017 over the corresponding serial algorithm; (2) the improved\nalgorithm is at least two times faster than our previous GPU-accelerated AIDW\nalgorithm; and (3) the utilization of fast kNN search can significantly improve\nthe computational efficiency of the entire GPU-accelerated AIDW algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 08:19:39 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Mei", "Gang", ""], ["Xu", "Nengxiong", ""], ["Xu", "Liangliang", ""]]}, {"id": "1601.05961", "submitter": "Alina S\\^irbu", "authors": "Alina S\\^irbu, Ozalp Babaoglu", "title": "Power Consumption Modeling and Prediction in a Hybrid CPU-GPU-MIC\n  Supercomputer (preliminary version)", "comments": "13 pages, 4 figures, 2 tables, Euro-Par 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power consumption is a major obstacle for High Performance Computing (HPC)\nsystems in their quest towards the holy grail of ExaFLOP performance.\nSignificant advances in power efficiency have to be made before this goal can\nbe attained and accurate modeling is an essential step towards power efficiency\nby optimizing system operating parameters to match dynamic energy needs. In\nthis paper we present a study of power consumption by jobs in Eurora, a hybrid\nCPU-GPU-MIC system installed at the largest Italian data center. Using data\nfrom a dedicated monitoring framework, we build a data-driven model of power\nconsumption for each user in the system and use it to predict the power\nrequirements of future jobs. We are able to achieve good prediction results for\nover 80% of the users in the system. For the remaining users, we identify\npossible reasons why prediction performance is not as good. Possible\napplications for our predictive modeling results include scheduling\noptimization, power-aware billing and system-scale power modeling. All the\nscripts used for the study have been made available on GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 11:16:03 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 08:58:06 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["S\u00eerbu", "Alina", ""], ["Babaoglu", "Ozalp", ""]]}, {"id": "1601.06040", "submitter": "Emanuele Guido Fusco", "authors": "Emanuele Guido Fusco, Andrzej Pelc, and Rossella Petreschi", "title": "Topology recognition with advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In topology recognition, each node of an anonymous network has to\ndeterministically produce an isomorphic copy of the underlying graph, with all\nports correctly marked. This task is usually unfeasible without any a priori\ninformation. Such information can be provided to nodes as advice. An oracle\nknowing the network can give a (possibly different) string of bits to each\nnode, and all nodes must reconstruct the network using this advice, after a\ngiven number of rounds of communication. During each round each node can\nexchange arbitrary messages with all its neighbors and perform arbitrary local\ncomputations. The time of completing topology recognition is the number of\nrounds it takes, and the size of advice is the maximum length of a string given\nto nodes.\n  We investigate tradeoffs between the time in which topology recognition is\naccomplished and the minimum size of advice that has to be given to nodes. We\nprovide upper and lower bounds on the minimum size of advice that is sufficient\nto perform topology recognition in a given time, in the class of all graphs of\nsize $n$ and diameter $D\\le \\alpha n$, for any constant $\\alpha< 1$. In most\ncases, our bounds are asymptotically tight.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 15:23:27 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Fusco", "Emanuele Guido", ""], ["Pelc", "Andrzej", ""], ["Petreschi", "Rossella", ""]]}, {"id": "1601.06060", "submitter": "Raphael Eidenbenz", "authors": "Raphael Eidenbenz and Thomas Locher", "title": "Task Allocation for Distributed Stream Processing", "comments": "Extended Version of the work published in the proceedings of IEEE\n  International Conference on Computer Communications (INFOCOM), 10-15 April\n  2016, San Francisco, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing demand for live, on-the-fly processing of increasingly\nlarge amounts of data. In order to ensure the timely and reliable processing of\nstreaming data, a variety of distributed stream processing architectures and\nplatforms have been developed, which handle the fundamental tasks of\n(dynamically) assigning processing tasks to the currently available physical\nresources and routing streaming data between these resources. However, while\nthere are plenty of platforms offering such functionality, the theory behind it\nis not well understood. In particular, it is unclear how to best allocate the\nprocessing tasks to the given resources. In this paper, we establish a\ntheoretical foundation by formally defining a task allocation problem for\ndistributed stream processing, which we prove to be NP-hard. Furthermore, we\npropose an approximation algorithm for the class of series-parallel\ndecomposable graphs, which captures a broad range of common stream processing\napplications. The algorithm achieves a constant-factor approximation under the\nassumptions that the number of resources scales at least logarithmically with\nthe number of computational tasks and the computational cost of the tasks\ndominates the cost of communication.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 16:22:23 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Eidenbenz", "Raphael", ""], ["Locher", "Thomas", ""]]}, {"id": "1601.06496", "submitter": "Da Yan", "authors": "Da Yan, James Cheng, Fan Yang", "title": "Lightweight Fault Tolerance in Large-Scale Distributed Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Google's Pregel framework in distributed graph processing has\ninspired a surging interest in developing Pregel-like platforms featuring a\nuser-friendly \"think like a vertex\" programming model. Existing Pregel-like\nsystems support a fault tolerance mechanism called checkpointing, which\nperiodically saves computation states as checkpoints to HDFS, so that when a\nfailure happens, computation rolls back to the latest checkpoint. However, a\ncheckpoint in existing systems stores a huge amount of data, including vertex\nstates, edges, and messages sent by vertices, which significantly degrades the\nfailure-free performance. Moreover, the high checkpointing cost prevents\nfrequent checkpointing, and thus recovery has to replay all the computations\nfrom a state checkpointed some time ago.\n  In this paper, we propose a novel checkpointing approach which only stores\nvertex states and incremental edge updates to HDFS as a lightweight checkpoint\n(LWCP), so that writing an LWCP is typically tens of times faster than writing\na conventional checkpoint. To recover from the latest LWCP, messages are\ngenerated from the vertex states, and graph topology is recovered by replaying\nincremental edge updates. We show how to realize lightweight checkpointing with\nminor modifications of the vertex-centric programming interface. We also apply\nthe same idea to a recently-proposed log-based approach for fast recovery, to\nmake it work efficiently in practice by significantly reducing the cost of\ngarbage collection of logs. Extensive experiments on large real graphs verified\nthe effectiveness of LWCP in improving both failure-free performance and the\nperformance of recovery.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 07:25:03 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Yan", "Da", ""], ["Cheng", "James", ""], ["Yang", "Fan", ""]]}, {"id": "1601.06497", "submitter": "Da Yan", "authors": "Da Yan, James Cheng, M. Tamer \\\"Ozsu, Fan Yang, Yi Lu, John C.S. Lui,\n  Qizhen Zhang, Wilfred Ng", "title": "Quegel: A General-Purpose Query-Centric Framework for Querying Big\n  Graphs", "comments": "This is a full version of our VLDB paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pioneered by Google's Pregel, many distributed systems have been developed\nfor large-scale graph analytics. These systems expose the user-friendly \"think\nlike a vertex\" programming interface to users, and exhibit good horizontal\nscalability. However, these systems are designed for tasks where the majority\nof graph vertices participate in computation, but are not suitable for\nprocessing light-workload graph queries where only a small fraction of vertices\nneed to be accessed. The programming paradigm adopted by these systems can\nseriously under-utilize the resources in a cluster for graph query processing.\nIn this work, we develop a new open-source system, called Quegel, for querying\nbig graphs, which treats queries as first-class citizens in the design of its\ncomputing model. Users only need to specify the Pregel-like algorithm for a\ngeneric query, and Quegel processes light-workload graph queries on demand\nusing a novel superstep-sharing execution model to effectively utilize the\ncluster resources. Quegel further provides a convenient interface for\nconstructing graph indexes, which significantly improve query performance but\nare not supported by existing graph-parallel systems. Our experiments verified\nthat Quegel is highly efficient in answering various types of graph queries and\nis up to orders of magnitude faster than existing systems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 07:27:53 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Yan", "Da", ""], ["Cheng", "James", ""], ["\u00d6zsu", "M. Tamer", ""], ["Yang", "Fan", ""], ["Lu", "Yi", ""], ["Lui", "John C. S.", ""], ["Zhang", "Qizhen", ""], ["Ng", "Wilfred", ""]]}, {"id": "1601.06915", "submitter": "Bader AlBdaiwi", "authors": "Bader AlBdaiwi, Zaid Hussain, Anton Cerny, and Robert Aldred", "title": "Edge-Disjoint Node-Independent Spanning Trees in Dense Gaussian Networks", "comments": null, "journal-ref": "The Journal of Supercomputing, December 2016, Vol. 72, No. 12, pp.\n  4718-4736", "doi": "10.1007/s11227-016-1768-x", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent trees are used in building secure and/or fault-tolerant network\ncommunication protocols. They have been investigated for different network\ntopologies including tori. Dense Gaussian networks are potential alternatives\nfor 2-dimensional tori. They have similar topological properties; however, they\nare superiors in carrying communications due to their node-distance\ndistributions and smaller diameters. In this paper, we present constructions of\nedge-disjoint node-independent spanning trees in dense Gaussian networks. Based\non the constructed trees, we design algorithms that could be used in\nfault-tolerant routing or secure message distribution.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 07:57:16 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 20:09:21 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["AlBdaiwi", "Bader", ""], ["Hussain", "Zaid", ""], ["Cerny", "Anton", ""], ["Aldred", "Robert", ""]]}, {"id": "1601.07028", "submitter": "Dong Yuan", "authors": "Dong Yuan, Lizhen Cui, Xiao Liu, Erjiang Fu, Yun Yang", "title": "A Cost-Effective Strategy for Storing Scientific Datasets with Multiple\n  Service Providers in the Cloud", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing provides scientists a platform that can deploy computation\nand data intensive applications without infrastructure investment. With\nexcessive cloud resources and a decision support system, large generated data\nsets can be flexibly 1 stored locally in the current cloud, 2 deleted and\nregenerated whenever reused or 3 transferred to cheaper cloud service for\nstorage. However, due to the pay for use model, the total application cost\nlargely depends on the usage of computation, storage and bandwidth resources,\nhence cutting the cost of cloud based data storage becomes a big concern for\ndeploying scientific applications in the cloud. In this paper, we propose a\nnovel strategy that can cost effectively store large generated data sets with\nmultiple cloud service providers. The strategy is based on a novel algorithm\nthat finds the trade off among computation, storage and bandwidth costs in the\ncloud, which are three key factors for the cost of data storage. Both general\n(random) simulations conducted with popular cloud service providers pricing\nmodels and three specific case studies on real world scientific applications\nshow that the proposed storage strategy is highly cost effective and practical\nfor run time utilization in the cloud.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 14:07:45 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Yuan", "Dong", ""], ["Cui", "Lizhen", ""], ["Liu", "Xiao", ""], ["Fu", "Erjiang", ""], ["Yang", "Yun", ""]]}, {"id": "1601.07047", "submitter": "Leandro Indrusiak", "authors": "Andrew Burkimsher and Leandro Soares Indrusiak", "title": "Bidding policies for market-based HPC workflow scheduling", "comments": "2nd International Workshop on Dynamic Resource Allocation and\n  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016\n  (arXiv:cs/1601.04675), DREAMCloud/2016/06", "journal-ref": null, "doi": null, "report-no": "DREAMCloud/2016/06", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the scheduling of jobs on distributed, heterogeneous\nHigh Performance Computing (HPC) clusters. Market-based approaches are known to\nbe efficient for allocating limited resources to those that are most prepared\nto pay. This context is applicable to an HPC or cloud computing scenario where\nthe platform is overloaded. In this paper, jobs are composed of dependent\ntasks. Each job has a non-increasing time-value curve associated with it. Jobs\nare submitted to and scheduled by a market-clearing centralised auctioneer.\nThis paper compares the performance of several policies for generating task\nbids. The aim investigated here is to maximise the value for the platform\nprovider while minimising the number of jobs that do not complete (or starve).\nIt is found that the Projected Value Remaining bidding policy gives the highest\nlevel of value under a typical overload situation, and gives the lowest number\nof starved tasks across the space of utilisation examined. It does this by\nattempting to capture the urgency of tasks in the queue. At high levels of\noverload, some alternative algorithms produce slightly higher value, but at the\ncost of a hugely higher number of starved workflows.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 14:35:29 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Burkimsher", "Andrew", ""], ["Indrusiak", "Leandro Soares", ""]]}, {"id": "1601.07089", "submitter": "Siavoosh Payandeh Azad", "authors": "Siavoosh Payandeh Azad, Behrad Niazmand, Jaan Raik, Gert Jervan,\n  Thomas Hollstein", "title": "Holistic Approach for Fault-Tolerant Network-on-Chip based Many-Core\n  Systems", "comments": "2nd International Workshop on Dynamic Resource Allocation and\n  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016\n  (arXiv:cs/1601.04675), DREAMCloud/2016/05", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a holistic approach for Fault-Tolerant\nNetwork-on-Chip (NoC) based many-core systems that incorporates a System Health\nMonitoring Unit (SHMU) which collects all the fault information from the\nsystem, classifies them and provides different solutions for different fault\nclasses. A Mapper/Scheduler Unit (MSU) is used for online generation of\ndifferent mapping and scheduling solutions based on the current fault\nconfiguration of the system. For detection of faults, we have leveraged\nconcurrent online checkers, able to capture faults with low detection latency\nand providing the fault information for SHMU, which can be later used for the\nrecovery process. The experimentation setup is performed in an open source\ntool, able to perform the mapping, scheduling and simulation of the system.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 16:36:01 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Azad", "Siavoosh Payandeh", ""], ["Niazmand", "Behrad", ""], ["Raik", "Jaan", ""], ["Jervan", "Gert", ""], ["Hollstein", "Thomas", ""]]}, {"id": "1601.07133", "submitter": "David Castells-Rufas", "authors": "David Castells-Rufas, Albert Saa-Garriga, Jordi Carrabina", "title": "Energy Efficiency of Many-Soft-Core Processors", "comments": "Presented at HIP3ES, 2016", "journal-ref": null, "doi": "10.13140/RG.2.1.1276.5042", "report-no": "HIP3ES/2016/7", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing capacity of integration allows to instantiate hundreds of\nsoft-core processors in a single FPGA to create a reconfigurable\nmultiprocessing system. Lately, FPGAs have been proven to give a higher energy\nefficiency than alternative platforms like CPUs and GPGPUs for certain\nworkloads and are increasingly used in data-centers. In this paper we\ninvestigate whether many-soft-core processors can achieve similar levels of\nenergy efficiency while providing a general purpose environment, more easily\nprogrammed, and allowing to run other applications without reconfiguring the\ndevice. With a simple application example we are able to create a\nreconfigurable multiprocessing system achieving an energy efficiency 58 times\nhigher than a recent ultra-low-power processor and 124 times higher than a\nrecent high performance GPGPU.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 19:09:09 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Castells-Rufas", "David", ""], ["Saa-Garriga", "Albert", ""], ["Carrabina", "Jordi", ""]]}, {"id": "1601.07195", "submitter": "Mikhail Smelyanskiy", "authors": "Mikhail Smelyanskiy, Nicolas P. D. Sawaya, Al\\'an Aspuru-Guzik", "title": "qHiPSTER: The Quantum High Performance Software Testing Environment", "comments": "9 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present qHiPSTER, the Quantum High Performance Software Testing\nEnvironment. qHiPSTER is a distributed high-performance implementation of a\nquantum simulator on a classical computer, that can simulate general\nsingle-qubit gates and two-qubit controlled gates. We perform a number of\nsingle- and multi-node optimizations, including vectorization, multi-threading,\ncache blocking, as well as overlapping computation with communication. Using\nthe TACC Stampede supercomputer, we simulate quantum circuits (\"quantum\nsoftware\") of up to 40 qubits. We carry out a detailed performance analysis to\nshow that our simulator achieves both high performance and high hardware\nefficiency, limited only by the sustainable memory and network bandwidth of the\nmachine.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 21:30:20 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 21:23:03 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Smelyanskiy", "Mikhail", ""], ["Sawaya", "Nicolas P. D.", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1601.07333", "submitter": "Tal Mizrahi", "authors": "Tal Mizrahi, Yoram Moses", "title": "OneClock to Rule Them All: Using Time in Networked Applications", "comments": "This technical report is an extended version of \"OneClock to Rule\n  Them All: Using Time in Networked Applications\", which was accepted to\n  IEEE/IFIP NOMS 2016", "journal-ref": "NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management\n  Symposium, Istanbul, Turkey, 2016, pp. 679-685", "doi": "10.1109/NOMS.2016.7502876", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces OneClock, a generic approach for using time in\nnetworked applications. OneClock provides two basic time-triggered primitives:\nthe ability to schedule an operation at a remote host or device, and the\nability to receive feedback about the time at which an event occurred or an\noperation was executed at a remote host or device. We introduce a novel\nprediction-based scheduling approach that uses timing information collected at\nruntime to accurately schedule future operations.\n  Our work includes an extension to the Network Configuration protocol\n(NETCONF), which enables OneClock in real-life systems. This extension has been\npublished as an Internet Engineering Task Force (IETF) RFC, and a prototype of\nour NETCONF time extension is publicly available as open source.\n  Experimental evaluation shows that our prediction-based approach allows\naccurate scheduling in diverse and heterogeneous environments, with various\nhardware capabilities and workloads. OneClock is a generic approach that can be\napplied to any managed device: sensors, actuators, Internet of Things (IoT)\ndevices, routers, or toasters.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 11:57:19 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Mizrahi", "Tal", ""], ["Moses", "Yoram", ""]]}, {"id": "1601.07352", "submitter": "Nicolas Nicolaou", "authors": "Nicolas Nicolaou and Antonio Fern\\'andez Anta and Chryssis Georgiou", "title": "CoVer-ability: Consistent Versioning for Concurrent Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An object type characterizes the domain space and the operations that can be\ninvoked on an object of that type. In this paper we introduce a new property\nfor concurrent objects, we call coverability, that aims to provide precise\nguarantees on the consistent evolution of an object. This new property is\nsuitable for a variety of distributed objects including concurrent file objects\nthat demand operations to manipulate the latest version of the object. We\npropose two levels of coverability: (i) strong coverability and (ii) weak\ncoverability. Strong coverability requires that only a single operation can\nmodify the latest version of the object, i.e. \"covers\" the latest version with\na new version, imposing a total order on object modifications. Weak\ncoverability relaxes the strong requirements of strong coverability and allows\nmultiple operations to modify the same version of an object, where each\nmodification leads to a different version. Weak coverability preserves\nconsistent evolution of the object, by demanding any subsequent operation to\nonly modify one of the newly introduced versions. Coverability combined with\natomic guarantees yield to coverable atomic read/write registers. We also show\nthat strongly coverable atomic registers are equivalent in power to consensus.\nThus, we focus on weakly coverable registers, and we demonstrate their\nimportance by showing that they cannot be implemented using similar types of\nregisters, like ranked-registers. Furthermore we show that weakly coverable\nregisters may be used to implement basic (weak) read-modify-write and file\nobjects. Finally, we implement weakly coverable registers by modifying an\nexisting MWMR atomic register implementation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 13:09:30 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 13:27:39 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 18:35:55 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Nicolaou", "Nicolas", ""], ["Anta", "Antonio Fern\u00e1ndez", ""], ["Georgiou", "Chryssis", ""]]}, {"id": "1601.07355", "submitter": "Pavel Skvortsov", "authors": "Pavel Skvortsov, Dennis Hoppe, Axel Tenschert, Michael Gienger", "title": "Monitoring in the Clouds: Comparison of ECO2Clouds and EXCESS Monitoring\n  Approaches", "comments": "2nd International Workshop on Dynamic Resource Allocation and\n  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016\n  (arXiv:cs/1601.04675)", "journal-ref": null, "doi": null, "report-no": "DREAMCloud/2016/01", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing adoption of private cloud infrastructures by providers\nand enterprises, the monitoring of these infrastructures is becoming crucial.\nThe rationale behind monitoring is manifold: reasons include saving energy,\nlowering costs, and better maintenance. In the e-Science sector, moreover, the\ncollection of infrastructure and application-specific data at high resolutions\nis immanent. In this paper, we present two monitoring approaches implemented\nthroughout two European projects: ECO2Clouds and EXCESS. The ECO2Clouds project\naims to minimize CO2 emissions caused by the execution of applications on the\ncloud infrastructure. In order to allow for eco-aware deployment and scheduling\nof applications, the ECO2Clouds monitoring framework provides the necessary set\nof metrics on different layers including physical, virtual and application\nlayer. In turn, the EXCESS project introduces new energy-aware execution models\nthat improve energy-efficiency on a software level. Having in-depth knowledge\nabout the energy consumption and overall behavior of applications on a given\ninfrastructure, subsequent executions can be optimized to save energy. To\nachieve this goal, the EXCESS monitoring framework provides APIs allowing\ndevelopers to collect application-specific data in addition to infrastructure\ndata at run-time. We perform a comparative analysis of both monitoring\napproaches, and highlighting use cases including a hybrid approach which\nbenefits from both monitoring solutions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 13:15:35 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Skvortsov", "Pavel", ""], ["Hoppe", "Dennis", ""], ["Tenschert", "Axel", ""], ["Gienger", "Michael", ""]]}, {"id": "1601.07400", "submitter": "Evangelos Angelou", "authors": "Evangelos Angelou, Konstantinos Kaffes, Athanasia Asiki, Georgios\n  Goumas, Nectarios Koziris", "title": "Improving virtual host efficiency through resource and interference\n  aware scheduling", "comments": "2nd International Workshop on Dynamic Resource Allocation and\n  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016\n  (arXiv:cs/1601.04675)", "journal-ref": null, "doi": null, "report-no": "DREAMCloud/2016/02", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Infrastructure-as-a-Service Clouds operate in a competitive\nenvironment that caters to any user's requirements for computing resources. The\nsharing of the various types of resources by diverse applications poses a\nseries of challenges in order to optimize resource utilization while avoiding\nperformance degradation caused by application interference. In this paper, we\npresent two scheduling methodologies enforcing consolidation techniques on\nmulticore physical machines. Our resource-aware and interference-aware\nscheduling schemes aim at improving physical host efficiency while preserving\nthe application performance by taking into account host oversubscription and\nthe resulting workload interference. We validate our fully operational\nframework through a set of real-life workloads representing a wide class of\nmodern cloud applications. The experimental results prove the efficiency of our\nsystem in optimizing resource utilization and thus energy consumption even in\nthe presence of oversubscription. Both methodologies achieve significant\nreductions of the CPU time consumed, reaching up to 50%, while at the same time\nmaintaining workload performance compared to widely used scheduling schemes\nunder a variety of representative cloud platform scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 15:02:02 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Angelou", "Evangelos", ""], ["Kaffes", "Konstantinos", ""], ["Asiki", "Athanasia", ""], ["Goumas", "Georgios", ""], ["Koziris", "Nectarios", ""]]}, {"id": "1601.07420", "submitter": "Manuel Selva", "authors": "Roman Ursu, Khalid Latif, David Novo, Manuel Selva, Abdoulaye Gamatie,\n  Gilles Sassatelli, Dmitry Khabi, Alexey Cheptsov", "title": "A Workflow for Fast Evaluation of Mapping Heuristics Targeting Cloud\n  Infrastructures", "comments": "2nd International Workshop on Dynamic Resource Allocation and\n  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016\n  (arXiv:cs/1601.04675)", "journal-ref": null, "doi": null, "report-no": "DREAMCloud/2016/03", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource allocation is today an integral part of cloud infrastructures\nmanagement to efficiently exploit resources. Cloud infrastructures centers\ngenerally use custom built heuristics to define the resource allocations. It is\nan immediate requirement for the management tools of these centers to have a\nfast yet reasonably accurate simulation and evaluation platform to define the\nresource allocation for cloud applications. This work proposes a framework\nallowing users to easily specify mappings for cloud applications described in\nthe AMALTHEA format used in the context of the DreamCloud European project and\nto assess the quality for these mappings. The two quality metrics provided by\nthe framework are execution time and energy consumption.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 15:48:40 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Ursu", "Roman", ""], ["Latif", "Khalid", ""], ["Novo", "David", ""], ["Selva", "Manuel", ""], ["Gamatie", "Abdoulaye", ""], ["Sassatelli", "Gilles", ""], ["Khabi", "Dmitry", ""], ["Cheptsov", "Alexey", ""]]}, {"id": "1601.07446", "submitter": "Christian Napoli", "authors": "Marcin Wozniak, Dawid Polap, Grzegorz Borowik, Christian Napoli", "title": "A First Attempt to Cloud-Based User Verification in Distributed System", "comments": "Final version published on: Asia-Pacific Conference on Computer Aided\n  System Engineering (APCASE), pp. 226-231 (2015)", "journal-ref": "Asia-Pacific Conference on Computer Aided System Engineering\n  (APCASE), pp. 226-231 (2015)", "doi": "10.1109/APCASE.2015.47", "report-no": null, "categories": "cs.NE cs.AI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the idea of client verification in distributed systems is\npresented. The proposed solution presents a sample system where client\nverification through cloud resources using input signature is discussed. For\ndifferent signatures the proposed method has been examined. Research results\nare presented and discussed to show potential advantages.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 16:54:51 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Wozniak", "Marcin", ""], ["Polap", "Dawid", ""], ["Borowik", "Grzegorz", ""], ["Napoli", "Christian", ""]]}, {"id": "1601.07613", "submitter": "Andrew Giuliani", "authors": "Andrew Giuliani and Lilia Krivodonova", "title": "Edge coloring in unstructured CFD codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a way of preventing race conditions in the evaluation of the\nsurface integral contribution in discontinuous Galerkin and finite volume flow\nsolvers by coloring the edges (or faces) of the computational mesh. In this\nwork we use a partitioning algorithm that separates the edges of triangular\nelements into three groups and the faces of quadrangular and tetrahedral\nelements into four groups; we then extend this partitioning to adaptively\nrefined, nonconforming meshes. We use the ascribed coloring to reduce code\nmemory requirements and optimize accessing the elemental data in memory. This\nprocess reduces memory access latencies and speeds up computations on graphics\nprocessing units.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 01:24:40 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 15:50:24 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Giuliani", "Andrew", ""], ["Krivodonova", "Lilia", ""]]}, {"id": "1601.07790", "submitter": "Dariusz Dereniowski", "authors": "Dariusz Dereniowski, Andrzej Pelc", "title": "Topology Recognition and Leader Election in Colored Networks", "comments": null, "journal-ref": "Theoretical Computer Science 621 (2016) 92-102", "doi": "10.1016/j.tcs.2016.01.037", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topology recognition and leader election are fundamental tasks in distributed\ncomputing in networks. The first of them requires each node to find a labeled\nisomorphic copy of the network, while the result of the second one consists in\na single node adopting the label 1 (leader), with all other nodes adopting the\nlabel 0 and learning a path to the leader. We consider both these problems in\nnetworks whose nodes are equipped with not necessarily distinct labels called\ncolors, and ports at each node of degree $d$ are arbitrarily numbered\n$0,1,\\dots, d-1$. Colored networks are generalizations both of labeled networks\nand anonymous networks.\n  In colored networks, topology recognition and leader election are not always\nfeasible. Hence we study two more general problems. The aim of the problem TOP\n(resp. LE), for a colored network and for input $I$ given to its nodes, is to\nsolve topology recognition (resp. leader election) in this network, if this is\npossible under input $I$, and to have all nodes answer \"unsolvable\" otherwise.\n  We show that nodes of a network can solve problems TOP and LE, if they are\ngiven, as input $I$, an upper bound $k$ on the number of nodes of a given\ncolor, called the size of this color. On the other hand we show that, if the\nnodes are given an input that does not bound the size of any color, then the\nanswer to TOP and LE must be \"unsolvable\", even for the class of rings.\n  Under the assumption that nodes are given an upper bound $k$ on the size of a\ngiven color, we study the time of solving problems TOP and LE in the $LOCAL$.\nWe give an algorithm to solve each of these problems in arbitrary $n$-node\nnetworks of diameter $D$ in time $O(kD+D\\log(n/D))$. We also show that this\ntime is optimal, by exhibiting classes of networks in which every algorithm\nsolving problems TOP or LE must use time $\\Omega(kD+D\\log(n/D))$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 15:00:26 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dereniowski", "Dariusz", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1601.07815", "submitter": "Leonid Yavits PhD", "authors": "L. Yavits, A. Morad, R. Ginosar, U. Weiser", "title": "Convex Optimization of Real Time SoC", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization methods are employed to optimize a real-time (RT)\nsystem-on-chip (SoC) under a variety of physical resource-driven constraints,\ndemonstrated on an industry MPEG2 encoder SoC. The power optimization is\ncompared to conventional performance-optimization framework, showing a factor\nof two and a half saving in power. Convex optimization is shown to be very\nefficient in a high-level early stage design exploration, guiding computer\narchitects as to the choice of area, voltage, and frequency of the individual\ncomponents of the Chip Multiprocessor (CMP).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 16:19:39 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 13:40:53 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Yavits", "L.", ""], ["Morad", "A.", ""], ["Ginosar", "R.", ""], ["Weiser", "U.", ""]]}, {"id": "1601.07944", "submitter": "Andrew Giuliani", "authors": "Martin Fuhry, Andrew Giuliani, Lilia Krivodonova", "title": "Discontinuous Galerkin methods on graphics processing units for\n  nonlinear hyperbolic conservation laws", "comments": "36 pages, 14 figures, 6 tables", "journal-ref": "International Journal for Numerical Methods in Fluids, 76(12),\n  982-1003 (2014)", "doi": "10.1002/fld.3963", "report-no": null, "categories": "cs.DC cs.MS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel implementation of the modal discontinuous Galerkin (DG)\nmethod for hyperbolic conservation laws in two dimensions on graphics\nprocessing units (GPUs) using NVIDIA's Compute Unified Device Architecture\n(CUDA). Both flexible and highly accurate, DG methods accommodate parallel\narchitectures well as their discontinuous nature produces element-local\napproximations. High performance scientific computing suits GPUs well, as these\npowerful, massively parallel, cost-effective devices have recently included\nsupport for double-precision floating point numbers. Computed examples for\nEuler equations over unstructured triangle meshes demonstrate the effectiveness\nof our implementation on an NVIDIA GTX 580 device. Profiling of our method\nreveals performance comparable to an existing nodal DG-GPU implementation for\nlinear problems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 22:49:50 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Fuhry", "Martin", ""], ["Giuliani", "Andrew", ""], ["Krivodonova", "Lilia", ""]]}, {"id": "1601.08039", "submitter": "Sharath Srivatsa", "authors": "Sharath Srivatsa", "title": "Analysis of Distributed Snapshot Algorithms", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snapshot recording durations at each process contribute to the overall\nefficiency of the algorithm. In this paper we are presenting the observed\nvariations in snapshot recording durations at processes in a distributed\nsystem. We conclude with key characteristics of a reliable and effective\nsnapshot algorithm. Simulations were achieved using SimGrid Java API.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 10:31:10 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Srivatsa", "Sharath", ""]]}, {"id": "1601.08067", "submitter": "Zhuolun Xiang", "authors": "Zhuolun Xiang, Nitin H.Vaidya", "title": "Relaxed Byzantine Vector Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Byzantine consensus problem requires that non-faulty processes reach\nagreement on a decision (or output) that is in the convex hull of the inputs at\nthe non-faulty processes. It is well-known that exact consensus is impossible\nin an asynchronous system in presence of faults, and in a synchronous system,\nn>=3f+1 is tight on the number of processes to achieve exact Byzantine\nconsensus with scalar inputs, in presence of up to f Byzantine faulty\nprocesses. Recent work has shown that when the inputs are d-dimensional vectors\nof reals, n>=max(3f+1,(d+1)f+1) is tight to achieve exact Byzantine consensus\nin synchronous systems, and n>= (d+2)f+1 for approximate Byzantine consensus in\nasynchronous systems.\n  Due to the dependence of the lower bound on vector dimension d, the number of\nprocesses necessary becomes large when the vector dimension is large. With the\nhope of reducing the lower bound on n, we consider two relaxed versions of\nByzantine vector consensus: k-Relaxed Byzantine vector consensus and\n(delta,p)-Relaxed Byzantine vector consensus. In k-relaxed consensus, the\nvalidity condition requires that the output must be in the convex hull of\nprojection of the inputs onto any subset of k-dimensions of the vectors. For\n(delta,p)-consensus the validity condition requires that the output must be\nwithin distance delta of the convex hull of the inputs of the non-faulty\nprocesses, where L_p norm is used as the distance metric. For\n(delta,p)-consensus, we consider two versions: in one version, delta is a\nconstant, and in the second version, delta is a function of the inputs\nthemselves.\n  We show that for k-relaxed consensus and (delta,p)-consensus with constant\ndelta>=0, the bound on n is identical to the bound stated above for the\noriginal vector consensus problem. On the other hand, when delta depends on the\ninputs, we show that the bound on n is smaller when d>=3.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:53:24 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Xiang", "Zhuolun", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1601.08116", "submitter": "Tiago Azevedo", "authors": "Tiago Azevedo, Rosaldo J. F. Rossetti, Jorge G. Barbosa", "title": "Densifying the sparse cloud SimSaaS: The need of a synergy among\n  agent-directed simulation, SimSaaS and HLA", "comments": null, "journal-ref": "Proceedings of the 5th International Conference on Simulation and\n  Modeling Methodologies, Technologies and Applications (2015) 172-177", "doi": "10.5220/0005542801720177", "report-no": null, "categories": "cs.MA cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modelling & Simulation (M&S) is broadly used in real scenarios where making\nphysical modifications could be highly expensive. With the so-called Simulation\nSoftware-as-a-Service (SimSaaS), researchers could take advantage of the huge\namount of resource that cloud computing provides. Even so, studying and\nanalysing a problem through simulation may need several simulation tools, hence\nraising interoperability issues. Having this in mind, IEEE developed a standard\nfor interoperability among simulators named High Level Architecture (HLA).\nMoreover, the multi-agent system approach has become recognised as a convenient\napproach for modelling and simulating complex systems. Despite all the recent\nworks and acceptance of these technologies, there is still a great lack of work\nregarding synergies among them. This paper shows by means of a literature\nreview this lack of work or, in other words, the sparse Cloud SimSaaS. The\nliterature review and the resulting taxonomy are the main contributions of this\npaper, as they provide a research agenda illustrating future research\nopportunities and trends.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 14:11:37 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Azevedo", "Tiago", ""], ["Rossetti", "Rosaldo J. F.", ""], ["Barbosa", "Jorge G.", ""]]}, {"id": "1601.08162", "submitter": "Tiago Azevedo", "authors": "Tiago Azevedo, Rosaldo J. F. Rossetti, Jorge G. Barbosa", "title": "A State-of-the-art Integrated Transportation Simulation Platform", "comments": null, "journal-ref": "Proceedings of the 4th International Conference on Models and\n  Technologies for Intelligent Transportation Systems (2015) 340-347", "doi": "10.1109/MTITS.2015.7223277", "report-no": null, "categories": "cs.MA cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, universities and companies have a huge need for simulation and\nmodelling methodologies. In the particular case of traffic and transportation,\nmaking physical modifications to the real traffic networks could be highly\nexpensive, dependent on political decisions and could be highly disruptive to\nthe environment. However, while studying a specific domain or problem,\nanalysing a problem through simulation may not be trivial and may need several\nsimulation tools, hence raising interoperability issues. To overcome these\nproblems, we propose an agent-directed transportation simulation platform,\nthrough the cloud, by means of services. We intend to use the IEEE standard HLA\n(High Level Architecture) for simulators interoperability and agents for\ncontrolling and coordination. Our motivations are to allow multiresolution\nanalysis of complex domains, to allow experts to collaborate on the analysis of\na common problem and to allow co-simulation and synergy of different\napplication domains. This paper will start by presenting some preliminary\nbackground concepts to help better understand the scope of this work. After\nthat, the results of a literature review is shown. Finally, the general\narchitecture of a transportation simulation platform is proposed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 15:44:21 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Azevedo", "Tiago", ""], ["Rossetti", "Rosaldo J. F.", ""], ["Barbosa", "Jorge G.", ""]]}]