[{"id": "1708.00033", "submitter": "Vladimir Mironov", "authors": "Vladimir Mironov, Yuri Alexeev, Kristopher Keipert, Michael D'mello,\n  Alexander Moskovsky, Mark S. Gordon", "title": "An efficient MPI/OpenMP parallelization of the Hartree-Fock method for\n  the second generation of Intel Xeon Phi processor", "comments": "SC17 conference paper, 12 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3126908.3126956", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern OpenMP threading techniques are used to convert the MPI-only\nHartree-Fock code in the GAMESS program to a hybrid MPI/OpenMP algorithm. Two\nseparate implementations that differ by the sharing or replication of key data\nstructures among threads are considered, density and Fock matrices. All\nimplementations are benchmarked on a super-computer of 3,000 Intel Xeon Phi\nprocessors. With 64 cores per processor, scaling numbers are reported on up to\n192,000 cores. The hybrid MPI/OpenMP implementation reduces the memory\nfootprint by approximately 200 times compared to the legacy code. The\nMPI/OpenMP code was shown to run up to six times faster than the original for a\nrange of molecular system sizes.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 18:38:52 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 11:29:54 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Mironov", "Vladimir", ""], ["Alexeev", "Yuri", ""], ["Keipert", "Kristopher", ""], ["D'mello", "Michael", ""], ["Moskovsky", "Alexander", ""], ["Gordon", "Mark S.", ""]]}, {"id": "1708.00117", "submitter": "Andre Xian Ming Chang", "authors": "Andre Xian Ming Chang, Aliasger Zaidy, Vinayak Gokhale, Eugenio\n  Culurciello", "title": "Compiling Deep Learning Models for Custom Hardware Accelerators", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) are the core of most state-of-the-art\ndeep learning algorithms specialized for object detection and classification.\nCNNs are both computationally complex and embarrassingly parallel. Two\nproperties that leave room for potential software and hardware optimizations\nfor embedded systems. Given a programmable hardware accelerator with a CNN\noriented custom instructions set, the compiler's task is to exploit the\nhardware's full potential, while abiding with the hardware constraints and\nmaintaining generality to run different CNN models with varying workload\nproperties. Snowflake is an efficient and scalable hardware accelerator\nimplemented on programmable logic devices. It implements a control pipeline for\na custom instruction set. The goal of this paper is to present Snowflake's\ncompiler that generates machine level instructions from Torch7 model\ndescription files. The main software design points explored in this work are:\nmodel structure parsing, CNN workload breakdown, loop rearrangement for memory\nbandwidth optimizations and memory access balancing. The performance achieved\nby compiler generated instructions matches against hand optimized code for\nconvolution layers. Generated instructions also efficiently execute AlexNet and\nResNet18 inference on Snowflake. Snowflake with $256$ processing units was\nsynthesized on Xilinx's Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in\n$93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$\nframes/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 01:01:18 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 18:12:33 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chang", "Andre Xian Ming", ""], ["Zaidy", "Aliasger", ""], ["Gokhale", "Vinayak", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1708.00276", "submitter": "Gregory Schwartzman", "authors": "Reuven Bar-Yehuda, Keren Censor-Hillel, Mohsen Ghaffari, Gregory\n  Schwartzman", "title": "Distributed Approximation of Maximum Independent Set and Maximum\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple distributed $\\Delta$-approximation algorithm for maximum\nweight independent set (MaxIS) in the $\\mathsf{CONGEST}$ model which completes\nin $O(\\texttt{MIS}(G)\\cdot \\log W)$ rounds, where $\\Delta$ is the maximum\ndegree, $\\texttt{MIS}(G)$ is the number of rounds needed to compute a maximal\nindependent set (MIS) on $G$, and $W$ is the maximum weight of a node. %Whether\nour algorithm is randomized or deterministic depends on the \\texttt{MIS}\nalgorithm used as a black-box.\n  Plugging in the best known algorithm for MIS gives a randomized solution in\n$O(\\log n \\log W)$ rounds, where $n$ is the number of nodes.\n  We also present a deterministic $O(\\Delta +\\log^* n)$-round algorithm based\non coloring.\n  We then show how to use our MaxIS approximation algorithms to compute a\n$2$-approximation for maximum weight matching without incurring any additional\nround penalty in the $\\mathsf{CONGEST}$ model. We use a known reduction for\nsimulating algorithms on the line graph while incurring congestion, but we show\nour algorithm is part of a broad family of \\emph{local aggregation algorithms}\nfor which we describe a mechanism that allows the simulation to run in the\n$\\mathsf{CONGEST}$ model without an additional overhead.\n  Next, we show that for maximum weight matching, relaxing the approximation\nfactor to ($2+\\varepsilon$) allows us to devise a distributed algorithm\nrequiring $O(\\frac{\\log \\Delta}{\\log\\log\\Delta})$ rounds for any constant\n$\\varepsilon>0$. For the unweighted case, we can even obtain a\n$(1+\\varepsilon)$-approximation in this number of rounds. These algorithms are\nthe first to achieve the provably optimal round complexity with respect to\ndependency on $\\Delta$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 12:19:50 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bar-Yehuda", "Reuven", ""], ["Censor-Hillel", "Keren", ""], ["Ghaffari", "Mohsen", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1708.00352", "submitter": "Hung Cao", "authors": "Lilian Hernandez, Hung Cao, Monica Wachowicz", "title": "Implementing an Edge-Fog-Cloud architecture for stream data management", "comments": "stream data life cycle, edge computing, cloud computing, fog\n  computing, Internet of Moving Things, will be published in OpenFog Congress\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Moving Things (IoMT) requires support for a data life cycle\nprocess ranging from sorting, cleaning and monitoring data streams to more\ncomplex tasks such as querying, aggregation, and analytics. Current solutions\nfor stream data management in IoMT have been focused on partial aspects of a\ndata life cycle process, with special emphasis on sensor networks. This paper\naims to address this problem by developing streaming data life cycle process\nthat incorporates an edge/fog/cloud architecture that is needed for handling\nheterogeneous, streaming and geographically-dispersed IoMT devices. We propose\na 3-tier architecture to support an instant intra-layer communication that\nestablishes a stream data flow in real-time to respond to immediate data life\ncycle tasks in the system. Communication and process are thus the defining\nfactors in the design of our stream data management solution for IoMT. We\ndescribe and evaluate our prototype implementation using real-time transit data\nfeeds. Preliminary results are showing the advantages of running data life\ncycle tasks for reducing the volume of data streams that are redundant and\nshould not be transported to the cloud.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:19:44 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 17:19:48 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Hernandez", "Lilian", ""], ["Cao", "Hung", ""], ["Wachowicz", "Monica", ""]]}, {"id": "1708.00544", "submitter": "Jeremy Kepner", "authors": "Michael Jones, Jeremy Kepner, William Arcand, David Bestor, Bill\n  Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Peter Michaleas,\n  Andrew Prout, Albert Reuther, Siddharth Samsi, Paul Monticiollo", "title": "Performance Measurements of Supercomputing and Cloud Storage Solutions", "comments": "5 pages, 4 figures, to appear in IEEE HPEC 2017", "journal-ref": null, "doi": "10.1109/HPEC.2017.8091073", "report-no": null, "categories": "cs.DC astro-ph.IM cs.NI cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing amounts of data from varied sources, particularly in the fields of\nmachine learning and graph analytics, are causing storage requirements to grow\nrapidly. A variety of technologies exist for storing and sharing these data,\nranging from parallel file systems used by supercomputers to distributed block\nstorage systems found in clouds. Relatively few comparative measurements exist\nto inform decisions about which storage systems are best suited for particular\ntasks. This work provides these measurements for two of the most popular\nstorage technologies: Lustre and Amazon S3. Lustre is an open-source, high\nperformance, parallel file system used by many of the largest supercomputers in\nthe world. Amazon's Simple Storage Service, or S3, is part of the Amazon Web\nServices offering, and offers a scalable, distributed option to store and\nretrieve data from anywhere on the Internet. Parallel processing is essential\nfor achieving high performance on modern storage systems. The performance tests\nused span the gamut of parallel I/O scenarios, ranging from single-client,\nsingle-node Amazon S3 and Lustre performance to a large-scale, multi-client\ntest designed to demonstrate the capabilities of a modern storage appliance\nunder heavy load. These results show that, when parallel I/O is used correctly\n(i.e., many simultaneous read or write processes), full network bandwidth\nperformance is achievable and ranged from 10 gigabits/s over a 10 GigE S3\nconnection to 0.35 terabits/s using Lustre on a 1200 port 10 GigE switch. These\nresults demonstrate that S3 is well-suited to sharing vast quantities of data\nover the Internet, while Lustre is well-suited to processing large quantities\nof data locally.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 22:48:06 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Jones", "Michael", ""], ["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Michaleas", "Peter", ""], ["Prout", "Andrew", ""], ["Reuther", "Albert", ""], ["Samsi", "Siddharth", ""], ["Monticiollo", "Paul", ""]]}, {"id": "1708.00720", "submitter": "Miles Cranmer", "authors": "Miles D. Cranmer, Benjamin R. Barsdell, Danny C. Price, Jayce Dowell,\n  Hugh Garsden, Veronica Dike, Tarraneh Eftekhari, Alexander M. Hegedus, Joseph\n  Malins, Kenneth S. Obenberger, Frank Schinzel, Kevin Stovall, Gregory B.\n  Taylor, Lincoln J. Greenhill", "title": "Bifrost: a Python/C++ Framework for High-Throughput Stream Processing in\n  Astronomy", "comments": "25 pages, 13 figures, submitted to JAI. For the code, see\n  https://github.com/ledatelescope/bifrost", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio astronomy observatories with high throughput back end instruments\nrequire real-time data processing. While computing hardware continues to\nadvance rapidly, development of real-time processing pipelines remains\ndifficult and time-consuming, which can limit scientific productivity.\nMotivated by this, we have developed Bifrost: an open-source software framework\nfor rapid pipeline development. Bifrost combines a high-level Python interface\nwith highly efficient reconfigurable data transport and a library of computing\nblocks for CPU and GPU processing. The framework is generalizable, but\ninitially it emphasizes the needs of high-throughput radio astronomy pipelines,\nsuch as the ability to process data buffers as if they were continuous streams,\nthe capacity to partition processing into distinct data sequences (e.g.,\nseparate observations), and the ability to extract specific intervals from\nbuffered data. Computing blocks in the library are designed for applications\nsuch as interferometry, pulsar dedispersion and timing, and transient search\npipelines. We describe the design and implementation of the Bifrost framework\nand demonstrate its use as the backbone in the correlation and beamforming back\nend of the Long Wavelength Array station in the Sevilleta National Wildlife\nRefuge, NM.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 12:19:22 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Cranmer", "Miles D.", ""], ["Barsdell", "Benjamin R.", ""], ["Price", "Danny C.", ""], ["Dowell", "Jayce", ""], ["Garsden", "Hugh", ""], ["Dike", "Veronica", ""], ["Eftekhari", "Tarraneh", ""], ["Hegedus", "Alexander M.", ""], ["Malins", "Joseph", ""], ["Obenberger", "Kenneth S.", ""], ["Schinzel", "Frank", ""], ["Stovall", "Kevin", ""], ["Taylor", "Gregory B.", ""], ["Greenhill", "Lincoln J.", ""]]}, {"id": "1708.00777", "submitter": "Zheng Li", "authors": "Zheng Li, Selome Tesfatsion, Saeed Bastani, Ahmed Ali-Eldin, Erik\n  Elmroth, Maria Kihl, Rajiv Ranjan", "title": "A Survey on Modeling Energy Consumption of Cloud Applications:\n  Deconstruction, State of the Art, and Trade-off Debates", "comments": "in press", "journal-ref": "IEEE Transactions on Sustainable Computing, 2017", "doi": "10.1109/TSUSC.2017.2722822", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the complexity and heterogeneity in Cloud computing scenarios, the\nmodeling approach has widely been employed to investigate and analyze the\nenergy consumption of Cloud applications, by abstracting real-world objects and\nprocesses that are difficult to observe or understand directly. It is clear\nthat the abstraction sacrifices, and usually does not need, the complete\nreflection of the reality to be modeled. Consequently, current energy\nconsumption models vary in terms of purposes, assumptions, application\ncharacteristics and environmental conditions, with possible overlaps between\ndifferent research works. Therefore, it would be necessary and valuable to\nreveal the state-of-the-art of the existing modeling efforts, so as to weave\ndifferent models together to facilitate comprehending and further investigating\napplication energy consumption in the Cloud domain. By systematically\nselecting, assessing and synthesizing 76 relevant studies, we rationalized and\norganized over 30 energy consumption models with unified notations. To help\ninvestigate the existing models and facilitate future modeling work, we\ndeconstructed the runtime execution and deployment environment of Cloud\napplications, and identified 18 environmental factors and 12 workload factors\nthat would be influential on the energy consumption. In particular, there are\ncomplicated trade-offs and even debates when dealing with the combinational\nimpacts of multiple factors.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 14:45:47 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Li", "Zheng", ""], ["Tesfatsion", "Selome", ""], ["Bastani", "Saeed", ""], ["Ali-Eldin", "Ahmed", ""], ["Elmroth", "Erik", ""], ["Kihl", "Maria", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1708.00898", "submitter": "Jo\\~ao Sedoc", "authors": "Jo\\~ao Sedoc, Aline Normoyle", "title": "Seating Assignment Using Constrained Signed Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method for constrained cluster size signed\nspectral clustering which allows us to subdivide large groups of people based\non their relationships. In general, signed clustering only requires K hard\nclusters and does not constrain the cluster sizes. We extend signed clustering\nto include cluster size constraints. Using an example of seating assignment, we\nefficiently find groups of people with high social affinity while mitigating\nawkward social interaction between people who dislike each other.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 19:08:14 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Sedoc", "Jo\u00e3o", ""], ["Normoyle", "Aline", ""]]}, {"id": "1708.01012", "submitter": "Fan Zhou", "authors": "Fan Zhou and Guojing Cong", "title": "On the convergence properties of a $K$-step averaging stochastic\n  gradient descent algorithm for nonconvex optimization", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2018/447", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their popularity, the practical performance of asynchronous\nstochastic gradient descent methods (ASGD) for solving large scale machine\nlearning problems are not as good as theoretical results indicate. We adopt and\nanalyze a synchronous K-step averaging stochastic gradient descent algorithm\nwhich we call K-AVG. We establish the convergence results of K-AVG for\nnonconvex objectives and explain why the K-step delay is necessary and leads to\nbetter performance than traditional parallel stochastic gradient descent which\nis a special case of K-AVG with $K=1$. We also show that K-AVG scales better\nthan ASGD. Another advantage of K-AVG over ASGD is that it allows larger\nstepsizes. On a cluster of $128$ GPUs, K-AVG is faster than ASGD\nimplementations and achieves better accuracies and faster convergence for\n\\cifar dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 06:18:36 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 20:28:04 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 22:38:30 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhou", "Fan", ""], ["Cong", "Guojing", ""]]}, {"id": "1708.01135", "submitter": "Eike Hermann M\\\"uller", "authors": "William R. Saunders, James Grant, Eike H. M\\\"uller", "title": "Long range forces in a performance portable Molecular Dynamics framework", "comments": "9 pages, 3 figures, submitted to ParCo 2017 Parallel Computing\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular Dynamics (MD) codes predict the fundamental properties of matter by\nfollowing the trajectories of a collection of interacting model particles. To\nexploit diverse modern manycore hardware, efficient codes must use all\navailable parallelism. At the same time they need to be portable and easily\nextendible by the domain specialist (physicist/chemist) without detailed\nknowledge of this hardware. To address this challenge, we recently described a\nnew Domain Specific Language (DSL) for the development of performance portable\nMD codes based on a \"Separation of Concerns\": a Python framework automatically\ngenerates efficient parallel code for a range of target architectures.\n  Electrostatic interactions between charged particles are important in many\nphysical systems and often dominate the runtime. Here we discuss the inclusion\nof long-range interaction algorithms in our code generation framework. These\nalgorithms require global communications and careful consideration has to be\ngiven to any impact on parallel scalability. We implemented an Ewald summation\nalgorithm for electrostatic forces, present scaling comparisons for different\nsystem sizes and compare to the performance of existing codes. We also report\non further performance optimisations delivered with OpenMP shared memory\nparallelism.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 13:46:07 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Saunders", "William R.", ""], ["Grant", "James", ""], ["M\u00fcller", "Eike H.", ""]]}, {"id": "1708.01159", "submitter": "Merijn Verstraaten", "authors": "Merijn Verstraaten, Ana Lucia Varbanescu, Cees de Laat", "title": "Using Graph Properties to Speed-up GPU-based Graph Traversal: A\n  Model-driven Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is well-known and acknowledged that the performance of graph\nalgorithms is heavily dependent on the input data, there has been surprisingly\nlittle research to quantify and predict the impact the graph structure has on\nperformance. Parallel graph algorithms, running on many-core systems such as\nGPUs, are no exception: most research has focused on how to efficiently\nimplement and tune different graph operations on a specific GPU. However, the\nperformance impact of the input graph has only been taken into account\nindirectly as a result of the graphs used to benchmark the system.\n  In this work, we present a case study investigating how to use the properties\nof the input graph to improve the performance of the breadth-first search (BFS)\ngraph traversal. To do so, we first study the performance variation of 15\ndifferent BFS implementations across 248 graphs. Using this performance data,\nwe show that significant speed-up can be achieved by combining the best\nimplementation for each level of the traversal. To make use of this\ndata-dependent optimization, we must correctly predict the relative performance\nof algorithms per graph level, and enable dynamic switching to the optimal\nalgorithm for each level at runtime.\n  We use the collected performance data to train a binary decision tree, to\nenable high-accuracy predictions and fast switching. We demonstrate empirically\nthat our decision tree is both fast enough to allow dynamic switching between\nimplementations, without noticeable overhead, and accurate enough in its\nprediction to enable significant BFS speedup. We conclude that our model-driven\napproach (1) enables BFS to outperform state of the art GPU algorithms, and (2)\ncan be adapted for other BFS variants, other algorithms, or more specific\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 14:33:22 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Verstraaten", "Merijn", ""], ["Varbanescu", "Ana Lucia", ""], ["de Laat", "Cees", ""]]}, {"id": "1708.01285", "submitter": "Maxwell Young", "authors": "Diksha Gupta, Jared Saia, Maxwell Young", "title": "Proof of Work Without All the Work: Computationally Efficient\n  Attack-Resistant Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof-of-work (PoW) is an algorithmic tool used to secure networks by\nimposing a computational cost on participating devices. Unfortunately,\ntraditional PoW schemes require that correct devices perform computational work\nperpetually, even when the system is not under attack.\n  We address this issue by designing a general PoW protocol that ensures two\nproperties. First, the network stays secure. In particular, the fraction of\nidentities in the system that are controlled by an attacker is always less than\n1/2. Second, our protocol's computational cost is commensurate with the cost of\nan attacker. In particular, the total computational cost of correct devices is\na linear function of the attacker's computational cost plus the number of\ncorrect devices that have joined the system. Consequently, if the network is\nattacked, we ensure security with cost that grows linearly with the attacker's\ncost; and, in the absence of attack, our computational cost remains small. We\nprove similar guarantees for bandwidth cost.\n  Our results hold in a dynamic, decentralized system where participants join\nand depart over time, and where the total computational power of the attacker\nis up to a constant fraction of the total computational power of correct\ndevices. We demonstrate how to leverage our results to address important\nsecurity problems in distributed computing including: Sybil attacks, Byzantine\nconsensus, and Committee election.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 19:22:23 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 18:13:23 GMT"}, {"version": "v3", "created": "Sat, 17 Feb 2018 18:42:58 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Gupta", "Diksha", ""], ["Saia", "Jared", ""], ["Young", "Maxwell", ""]]}, {"id": "1708.01304", "submitter": "Ivy Bo Peng", "authors": "Ivy Bo Peng, Roberto Gioiosa, Gokcen Kestor, Erwin Laure and Stefano\n  Markidis", "title": "Preparing HPC Applications for the Exascale Era: A Decoupling Strategy", "comments": "The 46th International Conference on Parallel Processing (ICPP-2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production-quality parallel applications are often a mixture of diverse\noperations, such as computation- and communication-intensive, regular and\nirregular, tightly coupled and loosely linked operations. In conventional\nconstruction of parallel applications, each process performs all the\noperations, which might result inefficient and seriously limit scalability,\nespecially at large scale. We propose a decoupling strategy to improve the\nscalability of applications running on large-scale systems.\n  Our strategy separates application operations onto groups of processes and\nenables a dataflow processing paradigm among the groups. This mechanism is\neffective in reducing the impact of load imbalance and increases the parallel\nefficiency by pipelining multiple operations. We provide a proof-of-concept\nimplementation using MPI, the de-facto programming system on current\nsupercomputers. We demonstrate the effectiveness of this strategy by decoupling\nthe reduce, particle communication, halo exchange and I/O operations in a set\nof scientific and data-analytics applications. A performance evaluation on\n8,192 processes of a Cray XC40 supercomputer shows that the proposed approach\ncan achieve up to 4x performance improvement.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 20:44:39 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Peng", "Ivy Bo", ""], ["Gioiosa", "Roberto", ""], ["Kestor", "Gokcen", ""], ["Laure", "Erwin", ""], ["Markidis", "Stefano", ""]]}, {"id": "1708.01306", "submitter": "Ivy Bo Peng", "authors": "Ivy Bo Peng, Stefano Markidis, Roberto Gioiosa, Gokcen Kestor and\n  Erwin Laure", "title": "MPI Streams for HPC Applications", "comments": "Advances in Parallel Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data streams are a sequence of data flowing between source and destination\nprocesses. Streaming is widely used for signal, image and video processing for\nits efficiency in pipelining and effectiveness in reducing demand for memory.\nThe goal of this work is to extend the use of data streams to support both\nconventional scientific applications and emerging data analytic applications\nrunning on HPC platforms. We introduce an extension called MPIStream to the\nde-facto programming standard on HPC, MPI. MPIStream supports data streams\neither within a single application or among multiple applications. We present\nthree use cases using MPI streams in HPC applications together with their\nparallel performance. We show the convenience of using MPI streams to support\nthe needs from both traditional HPC and emerging data analytics applications\nrunning on supercomputers.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 20:54:46 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Peng", "Ivy Bo", ""], ["Markidis", "Stefano", ""], ["Gioiosa", "Roberto", ""], ["Kestor", "Gokcen", ""], ["Laure", "Erwin", ""]]}, {"id": "1708.01341", "submitter": "Rui Han", "authors": "Rui Han, Fan Zhang, Zhentao Wang", "title": "AccurateML: Information-aggregation-based Approximate Processing for\n  Fast and Accurate Machine Learning on MapReduce", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": "838-846", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing demands of processing massive datasets have promoted irresistible\ntrends of running machine learning applications on MapReduce. When processing\nlarge input data, it is often of greater values to produce fast and accurate\nenough approximate results than slow exact results. Existing techniques produce\napproximate results by processing parts of the input data, thus incurring large\naccuracy losses when using short job execution times, because all the skipped\ninput data potentially contributes to result accuracy. We address this\nlimitation by proposing AccurateML that aggregates information of input data in\neach map task to create small aggregated data points. These aggregated points\nenable all map tasks producing initial outputs quickly to save computation\ntimes and decrease the outputs' size to reduce communication times. Our\napproach further identifies the parts of input data most related to result\naccuracy, thus first using these parts to improve the produced outputs to\nminimize accuracy losses. We evaluated AccurateML using real machine learning\napplications and datasets. The results show: (i) it reduces execution times by\n30 times with small accuracy losses compared to exact results; (ii) when using\nthe same execution times, it achieves 2.71 times reductions in accuracy losses\ncompared to existing approximate processing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 00:57:57 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Han", "Rui", ""], ["Zhang", "Fan", ""], ["Wang", "Zhentao", ""]]}, {"id": "1708.01349", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Jianxun Liu, Mengying Guo, Wenlong Ma and Yungang Bao", "title": "ACTS in Need: Automatic Configuration Tuning with Scalability Guarantees", "comments": null, "journal-ref": null, "doi": "10.1145/3124680.3124730", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To support the variety of Big Data use cases, many Big Data related systems\nexpose a large number of user-specifiable configuration parameters. Highlighted\nin our experiments, a MySQL deployment with well-tuned configuration parameters\nachieves a peak throughput as 12 times much as one with the default setting.\nHowever, finding the best setting for the tens or hundreds of configuration\nparameters is mission impossible for ordinary users. Worse still, many Big Data\napplications require the support of multiple systems co-deployed in the same\ncluster. As these co-deployed systems can interact to affect the overall\nperformance, they must be tuned together. Automatic configuration tuning with\nscalability guarantees (ACTS) is in need to help system users. Solutions to\nACTS must scale to various systems, workloads, deployments, parameters and\nresource limits. Proposing and implementing an ACTS solution, we demonstrate\nthat ACTS can benefit users not only in improving system performance and\nresource utilization, but also in saving costs and enabling fairer\nbenchmarking.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 01:33:46 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 18:19:28 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Zhu", "Yuqing", ""], ["Liu", "Jianxun", ""], ["Guo", "Mengying", ""], ["Ma", "Wenlong", ""], ["Bao", "Yungang", ""]]}, {"id": "1708.01365", "submitter": "Hui Liu Mr", "authors": "Hui Liu, Kun Wang, Bo Yang, Min Yang, Ruijian He, Lihua Shen, He\n  Zhong, Zhangxin Chen", "title": "Load Balancing using Hilbert Space-filling Curves for Parallel Reservoir\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of load balancing (grid partitioning) is to minimize overall\ncomputations and communications, and to make sure that all processors have a\nsimilar workload. Geometric methods divide a grid by using a location of a cell\nwhile topological methods work with connectivity of cells, which is generally\ndescribed as a graph. This paper introduces a Hilbert space-filling curve\nmethod. A space-filling curve is a continuous curve and defines a map between a\none-dimensional space and a multi-dimensional space. A Hilbert space-filling\ncurve is one special space-filling curve discovered by Hilbert and has many\nuseful characteristics, such as good locality, which means that two objects\nthat are close to each other in a multi-dimensional space are also close to\neach other in a one dimensional space. This property can model communications\nin grid-based parallel applications. The idea of the Hilbert space-filling\ncurve method is to map a computational domain into a one-dimensional space,\npartition the one-dimensional space to certain intervals, and assign all cells\nin a same interval to a MPI. To implement a load balancing method, a mapping\nkernel is required to convert high-dimensional coordinates to a scalar value\nand an efficient one-dimensional partitioning module that divides a\none-dimensional space and makes sure that all intervals have a similar\nworkload.\n  The Hilbert space-filling curve method is compared with ParMETIS, a famous\ngraph partitioning package. The results show that our Hilbert space-filling\ncurve method has good partition quality. It has been applied to grids with\nbillions of cells, and linear scalability has been obtained on IBM Blue Gene/Q.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 03:07:07 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Liu", "Hui", ""], ["Wang", "Kun", ""], ["Yang", "Bo", ""], ["Yang", "Min", ""], ["He", "Ruijian", ""], ["Shen", "Lihua", ""], ["Zhong", "He", ""], ["Chen", "Zhangxin", ""]]}, {"id": "1708.01388", "submitter": "Zheng Li", "authors": "Zheng Li and Maria Kihl and Qinghua Lu and Jens A. Andersson", "title": "Performance Overhead Comparison between Hypervisor and Container based\n  Virtualization", "comments": "Proceedings of the 31st IEEE International Conference on Advanced\n  Information Networking and Application (AINA 2017), pp. 955-962, March 27-29,\n  2017", "journal-ref": null, "doi": "10.1109/AINA.2017.79", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current virtualization solution in the Cloud widely relies on\nhypervisor-based technologies. Along with the recent popularity of Docker, the\ncontainer-based virtualization starts receiving more attention for being a\npromising alternative. Since both of the virtualization solutions are not\nresource-free, their performance overheads would lead to negative impacts on\nthe quality of Cloud services. To help fundamentally understand the performance\ndifference between these two types of virtualization solutions, we use a\nphysical machine with \"just-enough\" resource as a baseline to investigate the\nperformance overhead of a standalone Docker container against a standalone\nvirtual machine (VM). With findings contrary to the related work, our\nevaluation results show that the virtualization's performance overhead could\nvary not only on a feature-by-feature basis but also on a job-to-job basis.\nAlthough the container-based solution is undoubtedly lightweight, the\nhypervisor-based technology does not come with higher performance overhead in\nevery case. For example, Docker containers particularly exhibit lower QoS in\nterms of storage transaction speed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 06:06:23 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Li", "Zheng", ""], ["Kihl", "Maria", ""], ["Lu", "Qinghua", ""], ["Andersson", "Jens A.", ""]]}, {"id": "1708.01391", "submitter": "Zheng Li", "authors": "Zheng Li and Maria Kihl and Anders Robertsson", "title": "On a Feedback Control-based Mechanism of Bidding for Cloud Spot Service", "comments": "Proceedings of the 7th International Conference on Cloud Computing\n  Technology and Science (CloudCom 2015), pp. 290-297, Vancouver, Canada,\n  November 30 - December 03, 2015", "journal-ref": null, "doi": "10.1109/CloudCom.2015.76", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a cost-effective option for Cloud consumers, spot service has been\nconsidered to be a significant supplement for building a full-fledged market\neconomy for the Cloud ecosystem. However, unlike the static and straightforward\nway of trading on-demand and reserved Cloud services, the market-driven\nregulations of employing spot service could be too complicated for Cloud\nconsumers to comprehensively understand. In particular, it would be both\ndifficult and tedious for potential consumers to determine suitable bids from\ntime to time. To reduce the complexity in applying spot resources, we propose\nto use a feedback control to help make bidding decisions. Based on an\narccotangent-function-type system model, our novel bidding mechanism imitates\nfuzzy and intuitive human activities to refine and issue new bids according to\nprevious errors. The validation is conducted by using Amazon's historical spot\nprice trace to perform a set of simulations and comparisons. The result shows\nthat the feedback control-based mechanism obtains a better trade-off between\nbidding rationality and success rate than the other five comparable strategies.\nAlthough this mechanism is only for black-box bidding (price prediction) at\nthis current stage, it can be conveniently and gradually upgraded to take into\naccount external constraints in the future.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 06:26:02 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Li", "Zheng", ""], ["Kihl", "Maria", ""], ["Robertsson", "Anders", ""]]}, {"id": "1708.01397", "submitter": "Zheng Li", "authors": "Zheng Li and William Tarneberg and Maria Kihl and Anders Robertsson", "title": "Using a Predator-Prey Model to Explain Variations of Cloud Spot Price", "comments": "Proceedings of the 6th International Conference on Cloud Computing\n  and Services Science (CLOSER 2016), pp. 51-58, Rome, Italy, April 23-25, 2016", "journal-ref": null, "doi": "10.5220/0005808600510058", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spot pricing scheme has been considered to be resource-efficient for\nproviders and cost-effective for consumers in the Cloud market. Nevertheless,\nunlike the static and straightforward strategies of trading on-demand and\nreserved Cloud services, the market-driven mechanism for trading spot service\nwould be complicated for both implementation and understanding. The largely\ninvisible market activities and their complex interactions could especially\nmake Cloud consumers hesitate to enter the spot market. To reduce the\ncomplexity in understanding the Cloud spot market, we decided to reveal the\nbackend information behind spot price variations. Inspired by the methodology\nof reverse engineering, we developed a Predator-Prey model that can simulate\nthe interactions between demand and resource based on the visible spot price\ntraces. The simulation results have shown some basic regular patterns of market\nactivities with respect to Amazon's spot instance type m3.large. Although the\nfindings of this study need further validation by using practical data, our\nwork essentially suggests a promising approach (i.e.~using a Predator-Prey\nmodel) to investigate spot market activities.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 06:48:07 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Li", "Zheng", ""], ["Tarneberg", "William", ""], ["Kihl", "Maria", ""], ["Robertsson", "Anders", ""]]}, {"id": "1708.01401", "submitter": "Zheng Li", "authors": "Zheng Li and He Zhang and Liam O'Brien and Shu Jiang and You Zhou and\n  Maria Kihl and Rajiv Ranjan", "title": "Spot Pricing in the Cloud Ecosystem: A Comparative Investigation", "comments": null, "journal-ref": "Journal of Systems and Software, vol. 114, pp. 1-19 (2016)", "doi": "10.1016/j.jss.2015.10.042", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Spot pricing is considered as a significant supplement for\nbuilding a full-fledged market economy for the Cloud ecosystem. However, it\nseems that both providers and consumers are still hesitating to enter the Cloud\nspot market. The relevant academic community also has conflicting opinions\nabout Cloud spot pricing in terms of revenue generation. Aim: This work aims to\nsystematically identify, assess, synthesize and report the published evidence\nin favor of or against spot-price scheme compared with fixed-price scheme of\nCloud computing, so as to help relieve the aforementioned conflict. Method: We\nemployed the systematic literature review (SLR) method to collect and\ninvestigate the empirical studies of Cloud spot pricing indexed by major\nelectronic libraries. Results: This SLR identified 61 primary studies that\neither delivered discussions or conducted experiments to perform comparison\nbetween spot pricing and fixed pricing in the Cloud domain. The reported\nbenefits and limitations were summarized to facilitate cost-benefit analysis of\nbeing a Cloud spot pricing player, while four types of theories were\ndistinguished to help both researchers and practitioners better understand the\nCloud spot market. Conclusions: This SLR shows that the academic community\nstrongly advocates the emerging Cloud spot market. Although there is still a\nlack of practical and easily deployable market-driven mechanisms, the overall\nfindings of our work indicate that spot pricing plays a promising role in the\nsustainability of Cloud resource exploitation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 07:23:05 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Li", "Zheng", ""], ["Zhang", "He", ""], ["O'Brien", "Liam", ""], ["Jiang", "Shu", ""], ["Zhou", "You", ""], ["Kihl", "Maria", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1708.01412", "submitter": "Zheng Li", "authors": "Zheng Li and He Zhang and Liam O'Brien and Rainbow Cai and Shayne\n  Flint", "title": "On Evaluating Commercial Cloud Services: A Systematic Review", "comments": null, "journal-ref": "Journal of Systems and Software, vol. 86, no. 9, pp. 2371-2393\n  (2013)", "doi": "10.1016/j.jss.2013.04.021", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Cloud Computing is increasingly booming in industry with many\ncompeting providers and services. Accordingly, evaluation of commercial Cloud\nservices is necessary. However, the existing evaluation studies are relatively\nchaotic. There exists tremendous confusion and gap between practices and theory\nabout Cloud services evaluation. Aim: To facilitate relieving the\naforementioned chaos, this work aims to synthesize the existing evaluation\nimplementations to outline the state-of-the-practice and also identify research\nopportunities in Cloud services evaluation. Method: Based on a conceptual\nevaluation model comprising six steps, the Systematic Literature Review (SLR)\nmethod was employed to collect relevant evidence to investigate the Cloud\nservices evaluation step by step. Results: This SLR identified 82 relevant\nevaluation studies. The overall data collected from these studies essentially\nrepresent the current practical landscape of implementing Cloud services\nevaluation, and in turn can be reused to facilitate future evaluation work.\nConclusions: Evaluation of commercial Cloud services has become a world-wide\nresearch topic. Some of the findings of this SLR identify several research gaps\nin the area of Cloud services evaluation (e.g., the Elasticity and Security\nevaluation of commercial Cloud services could be a long-term challenge), while\nsome other findings suggest the trend of applying commercial Cloud services\n(e.g., compared with PaaS, IaaS seems more suitable for customers and is\nparticularly important in industry). This SLR study itself also confirms some\nprevious experiences and reveals new Evidence-Based Software Engineering (EBSE)\nlessons.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 08:12:59 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Li", "Zheng", ""], ["Zhang", "He", ""], ["O'Brien", "Liam", ""], ["Cai", "Rainbow", ""], ["Flint", "Shayne", ""]]}, {"id": "1708.01413", "submitter": "Navid Azizan Ruhi", "authors": "Navid Azizan-Ruhi, Farshad Lahouti, Salman Avestimehr, Babak Hassibi", "title": "Distributed Solution of Large-Scale Linear Systems via Accelerated\n  Projection-Based Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving a large-scale system of linear equations is a key step at the heart\nof many algorithms in machine learning, scientific computing, and beyond. When\nthe problem dimension is large, computational and/or memory constraints make it\ndesirable, or even necessary, to perform the task in a distributed fashion. In\nthis paper, we consider a common scenario in which a taskmaster intends to\nsolve a large-scale system of linear equations by distributing subsets of the\nequations among a number of computing machines/cores. We propose an accelerated\ndistributed consensus algorithm, in which at each iteration every machine\nupdates its solution by adding a scaled version of the projection of an error\nsignal onto the nullspace of its system of equations, and where the taskmaster\nconducts an averaging over the solutions with momentum. The convergence\nbehavior of the proposed algorithm is analyzed in detail and analytically shown\nto compare favorably with the convergence rate of alternative distributed\nmethods, namely distributed gradient descent, distributed versions of\nNesterov's accelerated gradient descent and heavy-ball method, the block\nCimmino method, and ADMM. On randomly chosen linear systems, as well as on\nreal-world data sets, the proposed method offers significant speed-up relative\nto all the aforementioned methods. Finally, our analysis suggests a novel\nvariation of the distributed heavy-ball method, which employs a particular\ndistributed preconditioning, and which achieves the same theoretical\nconvergence rate as the proposed consensus-based method.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 08:18:26 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 03:00:36 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Azizan-Ruhi", "Navid", ""], ["Lahouti", "Farshad", ""], ["Avestimehr", "Salman", ""], ["Hassibi", "Babak", ""]]}, {"id": "1708.01414", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and Rainbow Cai and He Zhang", "title": "Boosting Metrics for Cloud Services Evaluation -- The Last Mile of Using\n  Benchmark Suites", "comments": "Proceedings of the 27th IEEE International Conference on Advanced\n  Information Networking and Applications (AINA 2013), pp. 381-388, Barcelona,\n  Spain, March 25-28, 2013", "journal-ref": null, "doi": "10.1109/AINA.2013.99", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmark suites are significant for evaluating various aspects of Cloud\nservices from a holistic view. However, there is still a gap between using\nbenchmark suites and achieving holistic impression of the evaluated Cloud\nservices. Most Cloud service evaluation work intended to report individual\nbenchmarking results without delivering summary measures. As a result, it could\nbe still hard for customers with such evaluation reports to understand an\nevaluated Cloud service from a global perspective. Inspired by the boosting\napproaches to machine learning, we proposed the concept Boosting Metrics to\nrepresent all the potential approaches that are able to integrate a suite of\nbenchmarking results. This paper introduces two types of preliminary boosting\nmetrics, and demonstrates how the boosting metrics can be used to supplement\nprimary measures of individual Cloud service features. In particular, boosting\nmetrics can play a summary Response role in applying experimental design to\nCloud services evaluation. Although the concept Boosting Metrics was refined\nbased on our work in the Cloud Computing domain, we believe it can be easily\nadapted to the evaluation work of other computing paradigms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 08:25:15 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Cai", "Rainbow", ""], ["Zhang", "He", ""]]}, {"id": "1708.01419", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and Maria Kihl", "title": "DoKnowMe: Towards a Domain Knowledge-driven Methodology for Performance\n  Evaluation", "comments": null, "journal-ref": "ACM SIGMETRICS Performance Evaluation Review, vol. 43, no. 4, pp.\n  23-32 (2016)", "doi": "10.1145/2897356.2897360", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software engineering considers performance evaluation to be one of the key\nportions of software quality assurance. Unfortunately, there seems to be a lack\nof standard methodologies for performance evaluation even in the scope of\nexperimental computer science. Inspired by the concept of \"instantiation\" in\nobject-oriented programming, we distinguish the generic performance evaluation\nlogic from the distributed and ad-hoc relevant studies, and develop an abstract\nevaluation methodology (by analogy of \"class\") we name Domain Knowledge-driven\nMethodology (DoKnowMe). By replacing five predefined domain-specific knowledge\nartefacts, DoKnowMe could be instantiated into specific methodologies (by\nanalogy of \"object\") to guide evaluators in performance evaluation of different\nsoftware and even computing systems. We also propose a generic validation\nframework with four indicators (i.e.~usefulness, feasibility, effectiveness and\nrepeatability), and use it to validate DoKnowMe in the Cloud services\nevaluation domain. Given the positive and promising validation result, we plan\nto integrate more common evaluation strategies to improve DoKnowMe and further\nfocus on the performance evaluation of Cloud autoscaler systems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 08:33:10 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Kihl", "Maria", ""]]}, {"id": "1708.01462", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "How Amdahl's low restricts supercomputer applications and building ever\n  bigger supercomputers", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reinterprets Amdahl's law in terms of execution time and applies\nthis simple model to supercomputing. The systematic discussion results in\npractical formulas enabling to calculate expected running time using large\nnumber of processors from experimental runs using low number of processors,\ndelivers a quantitative measure of computational efficiency of supercomputing\napplications. Through separating non-parallelizable contribution to fractions\naccording to their origin, Amdahl's law enables to derive a timeline for\nsupercomputers (quite similar to Moore's law) and describes why Amdahl's law\nlimits the size of supercomputers. The paper validates that Amdahl's 50-years\nold model (with slight extension) correctly describes the performance\nlimitations of the present supercomputers. Using some simple and reasonable\nassumptions, the absolute performance bound of supercomputers is concluded,\nfurthermore that serious enhancements are still necessary to achieve the\nexaFLOPS dream value.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 11:56:45 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 06:26:45 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1708.01476", "submitter": "Georg Hager", "authors": "Thomas R\\\"ohl, Jan Eitzinger, Georg Hager, Gerhard Wellein", "title": "LIKWID Monitoring Stack: A flexible framework enabling job specific\n  performance monitoring for the masses", "comments": "4 pages, 4 figures. Accepted for HPCMASPA 2017, the Workshop on\n  Monitoring and Analysis for High Performance Computing Systems Plus\n  Applications, held in conjunction with IEEE Cluster 2017, Honolulu, HI,\n  September 5, 2017", "journal-ref": null, "doi": "10.1109/CLUSTER.2017.115", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System monitoring is an established tool to measure the utilization and\nhealth of HPC systems. Usually system monitoring infrastructures make no\nconnection to job information and do not utilize hardware performance\nmonitoring (HPM) data. To increase the efficient use of HPC systems automatic\nand continuous performance monitoring of jobs is an essential component. It can\nhelp to identify pathological cases, provides instant performance feedback to\nthe users, offers initial data to judge on the optimization potential of\napplications and helps to build a statistical foundation about application\nspecific system usage. The LIKWID monitoring stack is a modular framework build\non top of the LIKWID tools library. It aims on enabling job specific\nperformance monitoring using HPM data, system metrics and application-level\ndata for small to medium sized commodity clusters. Moreover, it is designed to\nintegrate in existing monitoring infrastructures to speed up the change from\npure system monitoring to job-aware monitoring.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 12:49:34 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["R\u00f6hl", "Thomas", ""], ["Eitzinger", "Jan", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1708.01797", "submitter": "Trevor Brown", "authors": "Maya Arbel-Raviv and Trevor Brown", "title": "Reuse, don't Recycle: Transforming Lock-free Algorithms that Throw Away\n  Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many lock-free algorithms, threads help one another, and each operation\ncreates a descriptor that describes how other threads should help it.\nAllocating and reclaiming descriptors introduces significant space and time\noverhead. We introduce the first descriptor abstract data type (ADT), which\ncaptures the usage of descriptors by lock-free algorithms. We then develop a\nweak descriptor ADT which has weaker semantics, but can be implemented\nsignificantly more efficiently. We show how a large class of lock-free\nalgorithms can be transformed to use weak descriptors, and demonstrate our\ntechnique by transforming several algorithms, including the leading\nk-compare-and-swap (k-CAS) algorithm. The original k-CAS algorithm allocates at\nleast k+1 new descriptors per k-CAS. In contrast, our implementation allocates\ntwo descriptors per process, and each process simply reuses its two\ndescriptors. Experiments on a variety of workloads show significant performance\nimprovements over implementations that reclaim descriptors, and reductions of\nup to three orders of magnitude in peak memory usage.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 18:04:26 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Arbel-Raviv", "Maya", ""], ["Brown", "Trevor", ""]]}, {"id": "1708.02030", "submitter": "Faisal Shahzad", "authors": "Faisal Shahzad, Jonas Thies, Moritz Kreutzer, Thomas Zeiser, Georg\n  Hager, Gerhard Wellein", "title": "CRAFT: A library for easier application-level Checkpoint/Restart and\n  Automatic Fault Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to efficiently use the future generations of supercomputers, fault\ntolerance and power consumption are two of the prime challenges anticipated by\nthe High Performance Computing (HPC) community. Checkpoint/Restart (CR) has\nbeen and still is the most widely used technique to deal with hard failures.\nApplication-level CR is the most effective CR technique in terms of overhead\nefficiency but it takes a lot of implementation effort. This work presents the\nimplementation of our C++ based library CRAFT (Checkpoint-Restart and Automatic\nFault Tolerance), which serves two purposes. First, it provides an extendable\nlibrary that significantly eases the implementation of application-level\ncheckpointing. The most basic and frequently used checkpoint data types are\nalready part of CRAFT and can be directly used out of the box. The library can\nbe easily extended to add more data types. As means of overhead reduction, the\nlibrary offers a build-in asynchronous checkpointing mechanism and also\nsupports the Scalable Checkpoint/Restart (SCR) library for node level\ncheckpointing. Second, CRAFT provides an easier interface for User-Level\nFailure Mitigation (ULFM) based dynamic process recovery, which significantly\nreduces the complexity and effort of failure detection and communication\nrecovery mechanism. By utilizing both functionalities together, applications\ncan write application-level checkpoints and recover dynamically from process\nfailures with very limited programming effort. This work presents the design\nand use of our library in detail. The associated overheads are thoroughly\nanalyzed using several benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 08:17:56 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Shahzad", "Faisal", ""], ["Thies", "Jonas", ""], ["Kreutzer", "Moritz", ""], ["Zeiser", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1708.02188", "submitter": "Ulrich Finkler", "authors": "Minsik Cho, Ulrich Finkler, Sameer Kumar, David Kung, Vaibhav Saxena,\n  Dheeraj Sreedhar", "title": "PowerAI DDL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks become more complex and input datasets grow larger,\nit can take days or even weeks to train a deep neural network to the desired\naccuracy. Therefore, distributed Deep Learning at a massive scale is a critical\ncapability, since it offers the potential to reduce the training time from\nweeks to hours. In this paper, we present a software-hardware co-optimized\ndistributed Deep Learning system that can achieve near-linear scaling up to\nhundreds of GPUs. The core algorithm is a multi-ring communication pattern that\nprovides a good tradeoff between latency and bandwidth and adapts to a variety\nof system configurations. The communication algorithm is implemented as a\nlibrary for easy use. This library has been integrated into Tensorflow, Caffe,\nand Torch. We train Resnet-101 on Imagenet 22K with 64 IBM Power8 S822LC\nservers (256 GPUs) in about 7 hours to an accuracy of 33.8 % validation\naccuracy. Microsoft's ADAM and Google's DistBelief results did not reach 30 %\nvalidation accuracy for Imagenet 22K. Compared to Facebook AI Research's recent\npaper on 256 GPU training, we use a different communication algorithm, and our\ncombined software and hardware system offers better communication overhead for\nResnet-50. A PowerAI DDL enabled version of Torch completed 90 epochs of\ntraining on Resnet 50 for 1K classes in 50 minutes using 64 IBM Power8 S822LC\nservers (256 GPUs).\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:24:00 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Cho", "Minsik", ""], ["Finkler", "Ulrich", ""], ["Kumar", "Sameer", ""], ["Kung", "David", ""], ["Saxena", "Vaibhav", ""], ["Sreedhar", "Dheeraj", ""]]}, {"id": "1708.02199", "submitter": "Kai Wu", "authors": "Kai Wu, Frank Ober, Shari Hamlin, Dong Li", "title": "Early Evaluation of Intel Optane Non-Volatile Memory with HPC I/O\n  Workloads", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High performance computing (HPC) applications have a high requirement on\nstorage speed and capacity. Non-volatile memory is a promising technology to\nreplace traditional storage devices to improve HPC performance. Earlier in\n2017, Intel and Micron released first NVM product -- Intel Optane SSDs. Optane\nis much faster and more durable than the traditional storage device. It creates\na bridge to narrow the performance gap between DRAM and storage. But is the\nexisting HPC I/O stack still suitable for new NVM devices like Intel Optane?\nHow does HPC I/O workload perform with Intel Optane?\n  In this paper, we analyze the performance of I/O intensive HPC applications\nwith Optane as a block device and try to answer the above questions. We study\nthe performance from three perspectives: (1) basic read and write bandwidth of\nOptane, (2) a performance comparison study between Optane and HDD, including\ncheckpoint workload, MPI individual I/O vs. POSIX I/O, and MPI individual I/O\nvs. MPI collective I/O, and (3) the impact of Optane on the performance of a\nparallel file system, PVFS2.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:54:19 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 05:23:32 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Wu", "Kai", ""], ["Ober", "Frank", ""], ["Hamlin", "Shari", ""], ["Li", "Dong", ""]]}, {"id": "1708.02419", "submitter": "Elias Rohrer", "authors": "Elias Rohrer, Jann-Frederik La{\\ss}, Florian Tschorsch", "title": "Towards a Concurrent and Distributed Route Selection for Payment Channel\n  Networks", "comments": "8 pages", "journal-ref": "ESORICS 2017, DPM 2017, CBT 2017. Lecture Notes in Computer\n  Science, vol 10436", "doi": "10.1007/978-3-319-67816-0_23", "report-no": null, "categories": "cs.NI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment channel networks use off-chain transactions to provide virtually\narbitrary transaction rates. In this paper, we provide a new perspective on\npayment channels and consider them as a flow network. We propose an extended\npush-relabel algorithm to find payment flows in a payment channel network. Our\nalgorithm enables a distributed and concurrent execution without violating\ncapacity constraints. To this end, we introduce the concept of capacity\nlocking. We prove that flows are valid and present first results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 09:27:46 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Rohrer", "Elias", ""], ["La\u00df", "Jann-Frederik", ""], ["Tschorsch", "Florian", ""]]}, {"id": "1708.02543", "submitter": "Amit Jacob Fanani", "authors": "Amit Jacob Fanani, Itay Harel", "title": "Impossibility of $n-1$-strong-equllibrium for Distributed Consensus with\n  Rational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for $n-1$-strong-equillibrium for distributed consensus in a\nring with rational agents was proposed by Afek et al. (2014). A proof of\nimpossibility of $n-1$-strong-equillibrium for distributed consensus in every\ntopology with rational agents, when $n$ is even, is presented. Furthermore, we\nshow that the algorithm proposed by Afek et al. is the only algorithm which can\nsolve the problem when $n$ is odd. Finally, we prove that the proposed\nalgorithm provides a $n-2$-strong-equillibrium in a synchronous ring when $n$\nis even.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:15:05 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Fanani", "Amit Jacob", ""], ["Harel", "Itay", ""]]}, {"id": "1708.02637", "submitter": "Illia Polosukhin", "authors": "Heng-Tze Cheng, Zakaria Haque, Lichan Hong, Mustafa Ispir, Clemens\n  Mewald, Illia Polosukhin, Georgios Roumpos, D Sculley, Jamie Smith, David\n  Soergel, Yuan Tang, Philipp Tucker, Martin Wicke, Cassandra Xia, Jianwei Xie", "title": "TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level\n  Machine Learning Frameworks", "comments": "8 pages, Appeared at KDD 2017, August 13--17, 2017, Halifax, NS,\n  Canada", "journal-ref": null, "doi": "10.1145/3097983.3098171", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for specifying, training, evaluating, and deploying\nmachine learning models. Our focus is on simplifying cutting edge machine\nlearning for practitioners in order to bring such technologies into production.\nRecognizing the fast evolution of the field of deep learning, we make no\nattempt to capture the design space of all possible model architectures in a\ndomain- specific language (DSL) or similar configuration language. We allow\nusers to write code to define their models, but provide abstractions that guide\ndevelop- ers to write models in ways conducive to productionization. We also\nprovide a unifying Estimator interface, making it possible to write downstream\ninfrastructure (e.g. distributed training, hyperparameter tuning) independent\nof the model implementation. We balance the competing demands for flexibility\nand simplicity by offering APIs at different levels of abstraction, making\ncommon model architectures available out of the box, while providing a library\nof utilities designed to speed up experimentation with model architectures. To\nmake out of the box models flexible and usable across a wide range of problems,\nthese canned Estimators are parameterized not only over traditional\nhyperparameters, but also using feature columns, a declarative specification\ndescribing how to interpret input data. We discuss our experience in using this\nframework in re- search and production environments, and show the impact on\ncode health, maintainability, and development speed.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 20:06:28 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Cheng", "Heng-Tze", ""], ["Haque", "Zakaria", ""], ["Hong", "Lichan", ""], ["Ispir", "Mustafa", ""], ["Mewald", "Clemens", ""], ["Polosukhin", "Illia", ""], ["Roumpos", "Georgios", ""], ["Sculley", "D", ""], ["Smith", "Jamie", ""], ["Soergel", "David", ""], ["Tang", "Yuan", ""], ["Tucker", "Philipp", ""], ["Wicke", "Martin", ""], ["Xia", "Cassandra", ""], ["Xie", "Jianwei", ""]]}, {"id": "1708.02638", "submitter": "Milad Makkie", "authors": "Milad Makkie, Xiang Li, Binbin Lin, Jieping Ye, Mojtaba Sedigh Fazli,\n  Tianming Liu, Shannon Quinn", "title": "Distributed rank-1 dictionary learning: Towards fast and scalable\n  solutions for fMRI big data analytics", "comments": "One of the authors name, Mojtaba Sedigh Fazli, has been mistakenly\n  missed from this paper presented at the IEEE Big Data confrence. In result we\n  are submitting this verison to correct the authors' names", "journal-ref": null, "doi": "10.1109/BigData.2016.7841000", "report-no": null, "categories": "cs.DS cs.DC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of functional brain imaging for research and diagnosis has benefitted\ngreatly from the recent advancements in neuroimaging technologies, as well as\nthe explosive growth in size and availability of fMRI data. While it has been\nshown in literature that using multiple and large scale fMRI datasets can\nimprove reproducibility and lead to new discoveries, the computational and\ninformatics systems supporting the analysis and visualization of such fMRI big\ndata are extremely limited and largely under-discussed. We propose to address\nthese shortcomings in this work, based on previous success in using dictionary\nlearning method for functional network decomposition studies on fMRI data. We\npresented a distributed dictionary learning framework based on rank-1 matrix\ndecomposition with sparseness constraint (D-r1DL framework). The framework was\nimplemented using the Spark distributed computing engine and deployed on three\ndifferent processing units: an in-house server, in-house high performance\nclusters, and the Amazon Elastic Compute Cloud (EC2) service. The whole\nanalysis pipeline was integrated with our neuroinformatics system for data\nmanagement, user input/output, and real-time visualization. Performance and\naccuracy of D-r1DL on both individual and group-wise fMRI Human Connectome\nProject (HCP) dataset shows that the proposed framework is highly scalable. The\nresulting group-wise functional network decompositions are highly accurate, and\nthe fast processing time confirm this claim. In addition, D-r1DL can provide\nreal-time user feedback and results visualization which are vital for\nlarge-scale data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 20:11:35 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Makkie", "Milad", ""], ["Li", "Xiang", ""], ["Lin", "Binbin", ""], ["Ye", "Jieping", ""], ["Fazli", "Mojtaba Sedigh", ""], ["Liu", "Tianming", ""], ["Quinn", "Shannon", ""]]}, {"id": "1708.02645", "submitter": "Amrita Mathuriya", "authors": "Amrita Mathuriya, Ye Luo, Raymond C. Clay III, Anouar Benali, Luke\n  Shulenburger and Jeongnim Kim", "title": "Embracing a new era of highly efficient and productive quantum Monte\n  Carlo simulations", "comments": "12 pages, 10 figures, 2 tables, to be published at SC17", "journal-ref": null, "doi": "10.1145/3126908.3126952", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QMCPACK has enabled cutting-edge materials research on supercomputers for\nover a decade. It scales nearly ideally but has low single-node efficiency due\nto the physics-based abstractions using array-of-structures objects, causing\ninefficient vectorization. We present a systematic approach to transform\nQMCPACK to better exploit the new hardware features of modern CPUs in portable\nand maintainable ways. We develop miniapps for fast prototyping and\noptimizations. We implement new containers in structure-of-arrays data layout\nto facilitate vectorizations by the compilers. Further speedup and smaller\nmemory-footprints are obtained by computing data on the fly with the vectorized\nroutines and expanding single-precision use. All these are seamlessly\nincorporated in production QMCPACK. We demonstrate upto 4.5x speedups on recent\nIntel processors and IBM Blue Gene/Q for representative workloads. Energy\nconsumption is reduced significantly commensurate to the speedup factor.\nMemory-footprints are reduced by up-to 3.8x, opening the possibility to solve\nmuch larger problems of future.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 20:30:57 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Mathuriya", "Amrita", ""], ["Luo", "Ye", ""], ["Clay", "Raymond C.", "III"], ["Benali", "Anouar", ""], ["Shulenburger", "Luke", ""], ["Kim", "Jeongnim", ""]]}, {"id": "1708.02825", "submitter": "Subhash Bhagat", "authors": "Subhash Bhagat and Krishnendu Mukhopadhyaya", "title": "Mutual Visibility by Robots with Persistent Memory", "comments": "10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the mutual visibility problem for a set of\nsemi-synchronous, opaque robots occupying distinct positions in the Euclidean\nplane. Since robots are opaque, if three robots lie on a line, the middle robot\nobstructs the visions of the two other robots. The mutual visibility problem\nasks the robots to coordinate their movements to form a configuration, within\nfinite time and without collision, in which no three robots are collinear.\nRobots are endowed with a constant bits of persistent memory. In this work, we\nconsider the FSTATE computational model in which the persistent memory is used\nby the robots only to remember their previous internal states. Except from this\npersistent memory, robots are oblivious i.e., they do not carry forward any\nother information from their previous computational cycles. The paper presents\na distributed algorithm to solve the mutual visibility problem for a set of\nsemi-synchronous robots using only 1 bit of persistent memory. The proposed\nalgorithm does not impose any other restriction on the capability of the robots\nand guarantees collision-free movements for the robots.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:27:41 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Bhagat", "Subhash", ""], ["Mukhopadhyaya", "Krishnendu", ""]]}, {"id": "1708.02835", "submitter": "Sameh Abdulah", "authors": "Sameh Abdulah, Hatem Ltaief, Ying Sun, Marc G. Genton, and David E.\n  Keyes", "title": "ExaGeoStat: A High Performance Unified Software for Geostatistics on\n  Manycore Systems", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TPDS.2018.2850749", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ExaGeoStat, a high performance framework for geospatial statistics\nin climate and environment modeling. In contrast to simulation based on partial\ndifferential equations derived from first-principles modeling, ExaGeoStat\nemploys a statistical model based on the evaluation of the Gaussian\nlog-likelihood function, which operates on a large dense covariance matrix.\nGenerated by the parametrizable Matern covariance function, the resulting\nmatrix is symmetric and positive definite. The computational tasks involved\nduring the evaluation of the Gaussian log-likelihood function become daunting\nas the number n of geographical locations grows, as O(n2) storage and O(n3)\noperations are required. While many approximation methods have been devised\nfrom the side of statistical modeling to ameliorate these polynomial\ncomplexities, we are interested here in the complementary approach of\nevaluating the exact algebraic result by exploiting advances in solution\nalgorithms and many-core computer architectures. Using state-of-the-art high\nperformance dense linear algebra libraries associated with various leading edge\nparallel architectures (Intel KNLs, NVIDIA GPUs, and distributed-memory\nsystems), ExaGeoStat raises the game for statistical applications from climate\nand environmental science. ExaGeoStat provides a reference evaluation of\nstatistical parameters, with which to assess the validity of the various\napproaches based on approximation. The framework takes a first step in the\nmerger of large-scale data analytics and extreme computing for geospatial\nstatistical applications, to be followed by additional complexity reducing\nimprovements from the solver side that can be implemented under the same\ninterface. Thus, a single uncompromised statistical model can ultimately be\nexecuted in a wide variety of emerging exascale environments.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:48:47 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 06:23:01 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 18:26:01 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Abdulah", "Sameh", ""], ["Ltaief", "Hatem", ""], ["Sun", "Ying", ""], ["Genton", "Marc G.", ""], ["Keyes", "David E.", ""]]}, {"id": "1708.02861", "submitter": "Won-Yong Shin", "authors": "Huifa Lin, Kwang Soon Kim, Won-Yong Shin", "title": "Interference-Aware Opportunistic Random Access in Dense IoT Networks", "comments": "20 pages, 10 figures, 3 tables. Published in the IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenging task to design a random access protocol that achieves the\noptimal throughput in multi-cell random access with decentralized transmission\ndue to the difficulty of coordination. In this paper, we present a\ndecentralized interference-aware opportunistic random access (IA-ORA) protocol\nthat enables us to obtain the optimal throughput scaling in an ultra-dense\nmulti-cell random access network with one access point (AP) and a number of\nusers. In sharp contrast to opportunistic scheduling for cellular multiple\naccess where users are selected by base stations, under the IA-ORA protocol,\neach user opportunistically transmits with a predefined physical layer (PHY)\ndata rate in a decentralized manner if not only the desired signal power to the\nserving AP is sufficiently large but also the generating interference leakage\npower to the other APs is sufficiently small (i.e., two threshold conditions\nare fulfilled). As a main result, it is shown that the optimal aggregate\nthroughput scaling (i.e., the MAC throughput of $\\frac{1}{e}$ in a cell and the\npower gain) is achieved in a high signal-to-noise ratio regime if the number of\nper-cell users exceeds some level. Additionally, it is numerically demonstrated\nvia computer simulations that under practical settings, the proposed IA-ORA\nprotocol outperforms conventional opportunistic random access protocols in\nterms of aggregate throughput.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 14:57:14 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 14:32:05 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 02:19:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Lin", "Huifa", ""], ["Kim", "Kwang Soon", ""], ["Shin", "Won-Yong", ""]]}, {"id": "1708.02906", "submitter": "Saptaparni Kumar", "authors": "Saptaparni Kumar, Jennifer Welch", "title": "Implementing $\\Diamond P$ with Bounded Messages on a Network of ADD\n  Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of the eventually perfect failure detector\n($\\Diamond P$) from the original hierarchy of the Chandra-Toueg oracles on an\narbitrary partitionable network composed of unreliable channels that can lose\nand reorder messages. Prior implementations of $\\Diamond P$ have assumed\ndifferent partially synchronous models ranging from bounded point-to-point\nmessage delay and reliable communication to unbounded message size and known\nnetwork topologies. We implement $\\Diamond P$ under very weak assumptions on an\narbitrary, partitionable network composed of Average Delayed/Dropped (ADD)\nchannels to model unreliable communication. Unlike older implementations, our\nfailure detection algorithm uses bounded-sized messages to eventually detect\nall nodes that are unreachable (crashed or disconnected) from it.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:58:43 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kumar", "Saptaparni", ""], ["Welch", "Jennifer", ""]]}, {"id": "1708.02937", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Manoj Kumar, Jos\\'e Moreira, Pratap Pattnaik, Mauricio\n  Serrano, Henry Tufo", "title": "Enabling Massive Deep Neural Networks with the GraphBLAS", "comments": "10 pages, 7 figures, to appear in the 2017 IEEE High Performance\n  Extreme Computing (HPEC) conference", "journal-ref": null, "doi": "10.1109/HPEC.2017.8091098", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have emerged as a core tool for machine learning.\nThe computations performed during DNN training and inference are dominated by\noperations on the weight matrices describing the DNN. As DNNs incorporate more\nstages and more nodes per stage, these weight matrices may be required to be\nsparse because of memory limitations. The GraphBLAS.org math library standard\nwas developed to provide high performance manipulation of sparse weight\nmatrices and input/output vectors. For sufficiently sparse matrices, a sparse\nmatrix library requires significantly less memory than the corresponding dense\nmatrix implementation. This paper provides a brief description of the\nmathematics underlying the GraphBLAS. In addition, the equations of a typical\nDNN are rewritten in a form designed to use the GraphBLAS. An implementation of\nthe DNN is given using a preliminary GraphBLAS C library. The performance of\nthe GraphBLAS implementation is measured relative to a standard dense linear\nalgebra library implementation. For various sizes of DNN weight matrices, it is\nshown that the GraphBLAS sparse implementation outperforms a BLAS dense\nimplementation as the weight matrix becomes sparser.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 03:24:40 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Kepner", "Jeremy", ""], ["Kumar", "Manoj", ""], ["Moreira", "Jos\u00e9", ""], ["Pattnaik", "Pratap", ""], ["Serrano", "Mauricio", ""], ["Tufo", "Henry", ""]]}, {"id": "1708.02983", "submitter": "Yang You", "authors": "Yang You, Aydin Buluc, James Demmel", "title": "Scaling Deep Learning on GPU and Knights Landing clusters", "comments": null, "journal-ref": null, "doi": "10.1145/3126908.3126912", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The speed of deep neural networks training has become a big bottleneck of\ndeep learning research and development. For example, training GoogleNet by\nImageNet dataset on one Nvidia K20 GPU needs 21 days. To speed up the training\nprocess, the current deep learning systems heavily rely on the hardware\naccelerators. However, these accelerators have limited on-chip memory compared\nwith CPUs. To handle large datasets, they need to fetch data from either CPU\nmemory or remote processors. We use both self-hosted Intel Knights Landing\n(KNL) clusters and multi-GPU clusters as our target platforms. From an\nalgorithm aspect, current distributed machine learning systems are mainly\ndesigned for cloud systems. These methods are asynchronous because of the slow\nnetwork and high fault-tolerance requirement on cloud systems. We focus on\nElastic Averaging SGD (EASGD) to design algorithms for HPC clusters. Original\nEASGD used round-robin method for communication and updating. The communication\nis ordered by the machine rank ID, which is inefficient on HPC clusters.\n  First, we redesign four efficient algorithms for HPC systems to improve\nEASGD's poor scaling on clusters. Async EASGD, Async MEASGD, and Hogwild EASGD\nare faster \\textcolor{black}{than} their existing counterparts (Async SGD,\nAsync MSGD, and Hogwild SGD, resp.) in all the comparisons. Finally, we design\nSync EASGD, which ties for the best performance among all the methods while\nbeing deterministic. In addition to the algorithmic improvements, we use some\nsystem-algorithm codesign techniques to scale up the algorithms. By reducing\nthe percentage of communication from 87% to 14%, our Sync EASGD achieves 5.3x\nspeedup over original EASGD on the same platform. We get 91.5% weak scaling\nefficiency on 4253 KNL cores, which is higher than the state-of-the-art\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:49:13 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["You", "Yang", ""], ["Buluc", "Aydin", ""], ["Demmel", "James", ""]]}, {"id": "1708.03053", "submitter": "Tevfik Kosar", "authors": "Engin Arslan and Tevfik Kosar", "title": "Application Level High Speed Transfer Optimization Based on Historical\n  Analysis and Real-time Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-intensive scientific and commercial applications increasingly require\nfrequent movement of large datasets from one site to the other(s). Despite\ngrowing network capacities, these data movements rarely achieve the promised\ndata transfer rates of the underlying physical network due to poorly tuned data\ntransfer protocols. Accurately and efficiently tuning the data transfer\nprotocol parameters in a dynamically changing network environment is a major\nchallenge and remains as an open research problem. In this paper, we present\npredictive end-to-end data transfer optimization algorithms based on historical\ndata analysis and real-time background traffic probing, dubbed HARP. Most of\nthe previous work in this area are solely based on real time network probing\nwhich results either in an excessive sampling overhead or fails to accurately\npredict the optimal transfer parameters. Combining historical data analysis\nwith real time sampling enables our algorithms to tune the application level\ndata transfer parameters accurately and efficiently to achieve close-to-optimal\nend-to-end data transfer throughput with very low overhead. Our experimental\nanalysis over a variety of network settings shows that HARP outperforms\nexisting solutions by up to 50% in terms of the achieved throughput.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 02:04:08 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Arslan", "Engin", ""], ["Kosar", "Tevfik", ""]]}, {"id": "1708.03157", "submitter": "Kai Staats", "authors": "Kai Staats, Edward Pantridge, Marco Cavaglia, Iurii Milovanov, Arun\n  Aniyan", "title": "TensorFlow Enabled Genetic Programming", "comments": "8 pages, 5 figures; presented at GECCO 2017, Berlin, Germany", "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO) Companion, ACM 2017, pp. 1872-1879", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Programming, a kind of evolutionary computation and machine learning\nalgorithm, is shown to benefit significantly from the application of vectorized\ndata and the TensorFlow numerical computation library on both CPU and GPU\narchitectures. The open source, Python Karoo GP is employed for a series of 190\ntests across 6 platforms, with real-world datasets ranging from 18 to 5.5M data\npoints. This body of tests demonstrates that datasets measured in tens and\nhundreds of data points see 2-15x improvement when moving from the scalar/SymPy\nconfiguration to the vector/TensorFlow configuration, with a single core\nperforming on par or better than multiple CPU cores and GPUs. A dataset\ncomposed of 90,000 data points demonstrates a single vector/TensorFlow CPU core\nperforming 875x better than 40 scalar/Sympy CPU cores. And a dataset containing\n5.5M data points sees GPU configurations out-performing CPU configurations on\naverage by 1.3x.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 10:33:42 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Staats", "Kai", ""], ["Pantridge", "Edward", ""], ["Cavaglia", "Marco", ""], ["Milovanov", "Iurii", ""], ["Aniyan", "Arun", ""]]}, {"id": "1708.03184", "submitter": "Peng Zhao", "authors": "Peng Zhao, Shusen Yang, Xinyu Yang, Wei Yu, and Jie Lin", "title": "Energy-efficient Analytics for Geographically Distributed Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics on geographically distributed datasets (across data\ncenters or clusters) has been attracting increasing interests from both\nacademia and industry, but also significantly complicates the system and\nalgorithm designs. In this article, we systematically investigate the\ngeo-distributed big-data analytics framework by analyzing the fine-grained\nparadigm and the key design principles. We present a dynamic global manager\nselection algorithm (GMSA) to minimize energy consumption cost by fully\nexploiting the system diversities in geography and variation over time. The\nalgorithm makes real-time decisions based on the measurable system parameters\nthrough stochastic optimization methods, while achieving the performance\nbalances between energy cost and latency. Extensive trace-driven simulations\nverify the effectiveness and efficiency of the proposed algorithm. We also\nhighlight several potential research directions that remain open and require\nfuture elaborations in analyzing geo-distributed big data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 12:44:31 GMT"}, {"version": "v2", "created": "Sat, 26 Aug 2017 16:50:21 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Zhao", "Peng", ""], ["Yang", "Shusen", ""], ["Yang", "Xinyu", ""], ["Yu", "Wei", ""], ["Lin", "Jie", ""]]}, {"id": "1708.03264", "submitter": "Jason Morton", "authors": "Jason Morton", "title": "Contextuality from missing and versioned data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally categorical data analysis (e.g. generalized linear models)\nworks with simple, flat datasets akin to a single table in a database with no\nnotion of missing data or conflicting versions. In contrast, modern data\nanalysis must deal with distributed databases with many partial local tables\nthat need not always agree. The computational agents tabulating these tables\nare spatially separated, with binding speed-of-light constraints and data\narriving too rapidly for these distributed views ever to be fully informed and\nglobally consistent. Contextuality is a mathematical property which describes a\nkind of inconsistency arising in quantum mechanics (e.g. in Bell's theorem). In\nthis paper we show how contextuality can arise in common data collection\nscenarios, including missing data and versioning (as in low-latency distributed\ndatabases employing snapshot isolation). In the companion paper, we develop\nstatistical models adapted to this regime.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:37:53 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Morton", "Jason", ""]]}, {"id": "1708.03274", "submitter": "Saptaparni Kumar", "authors": "Hagit Attiya, Hyun Chul Chung, Faith Ellen, Saptaparni Kumar and\n  Jennifer L. Welch", "title": "Simulating a Shared Register in a System that Never Stops Changing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating a shared register can mask the intricacies of designing algorithms\nfor asynchronous message-passing systems subject to crash failures, since it\nallows them to run algorithms designed for the simpler shared-memory model.\nTypically such simulations replicate the value of the register in multiple\nservers and require readers and writers to communicate with a majority of\nservers. The success of this approach for static systems, where the set of\nnodes (readers, writers, and servers) is fixed, has motivated several similar\nsimulations for dynamic systems, where nodes may enter and leave. However,\nexisting simulations need to assume that the system eventually stops changing\nfor a long enough period or that the system size is bounded. This paper\npresents the first simulation of an atomic read/write register in a crash-prone\nasynchronous system that can change size and withstand nodes continually\nentering and leaving. The simulation allows the system to keep changing,\nprovided that the number of nodes entering and leaving during a fixed time\ninterval is at most a constant fraction of the current system size. The\nsimulation also tolerates node crashes as long as the number of failed nodes in\nthe system is at most a constant fraction of the current system size.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:50:37 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Attiya", "Hagit", ""], ["Chung", "Hyun Chul", ""], ["Ellen", "Faith", ""], ["Kumar", "Saptaparni", ""], ["Welch", "Jennifer L.", ""]]}, {"id": "1708.03389", "submitter": "Qiang Cao", "authors": "Qiang Cao, Yuanjun Yao, Jeff Chase", "title": "A Logical Approach to Cloud Federation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated clouds raise a variety of challenges for managing identity,\nresource access, naming, connectivity, and object access control. This paper\nshows how to address these challenges in a comprehensive and uniform way using\na data-centric approach. The foundation of our approach is a trust logic in\nwhich participants issue authenticated statements about principals, objects,\nattributes, and relationships in a logic language, with reasoning based on\ndeclarative policy rules. We show how to use the logic to implement a trust\ninfrastructure for cloud federation that extends the model of NSF GENI, a\nfederated IaaS testbed. It captures shared identity management, GENI authority\nservices, cross-site interconnection using L2 circuits, and a naming and access\ncontrol system similar to AWS Identity and Access Management (IAM), but\nextended to a federated system without central control.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 21:17:18 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Cao", "Qiang", ""], ["Yao", "Yuanjun", ""], ["Chase", "Jeff", ""]]}, {"id": "1708.03549", "submitter": "Johan Thunberg", "authors": "Johan Thunberg, Johan Markdahl and Jorge Goncalves", "title": "Dynamic controllers for column synchronization of rotation matrices: a\n  QR-factorization approach", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multi-agent systems setting, this paper addresses continuous-time\ndistributed synchronization of columns of rotation matrices. More precisely, k\nspecific columns shall be synchronized and only the corresponding k columns of\nthe relative rotations between the agents are assumed to be available for the\ncontrol design. When one specific column is considered, the problem is\nequivalent to synchronization on the (d-1)-dimensional unit sphere and when all\nthe columns are considered, the problem is equivalent to synchronization on\nSO(d). We design dynamic control laws for these synchronization problems. The\ncontrol laws are based on the introduction of auxiliary variables in\ncombination with a QR-factorization approach. The benefit of this\nQR-factorization approach is that we can decouple the dynamics for the k\ncolumns from the remaining d-k ones. Under the control scheme, the closed loop\nsystem achieves almost global convergence to synchronization for quasi-strong\ninteraction graph topologies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 14:12:40 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 11:13:45 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Thunberg", "Johan", ""], ["Markdahl", "Johan", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1708.03604", "submitter": "Alfio Lazzaro", "authors": "Iain Bethune, Andeas Gloess, Juerg Hutter, Alfio Lazzaro, Hans Pabst,\n  Fiona Reid", "title": "Porting of the DBCSR library for Sparse Matrix-Matrix Multiplications to\n  Intel Xeon Phi systems", "comments": "Submitted to the ParCo2017 conference, Bologna, Italy 12-15 September\n  2017", "journal-ref": "Advances in Parallel Computing, Volume 32: Parallel Computing is\n  Everywhere, pp 47 - 56, 2018, IOS Press", "doi": "10.3233/978-1-61499-843-3-47", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication of two sparse matrices is a key operation in the simulation of\nthe electronic structure of systems containing thousands of atoms and\nelectrons. The highly optimized sparse linear algebra library DBCSR\n(Distributed Block Compressed Sparse Row) has been specifically designed to\nefficiently perform such sparse matrix-matrix multiplications. This library is\nthe basic building block for linear scaling electronic structure theory and low\nscaling correlated methods in CP2K. It is parallelized using MPI and OpenMP,\nand can exploit GPU accelerators by means of CUDA. We describe a performance\ncomparison of DBCSR on systems with Intel Xeon Phi Knights Landing (KNL)\nprocessors, with respect to systems with Intel Xeon CPUs (including systems\nwith GPUs). We find that the DBCSR on Cray XC40 KNL-based systems is 11%-14%\nslower than on a hybrid Cray XC50 with Nvidia P100 cards, at the same number of\nnodes. When compared to a Cray XC40 system equipped with dual-socket Intel Xeon\nCPUs, the KNL is up to 24% faster.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 16:35:59 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 11:55:20 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Bethune", "Iain", ""], ["Gloess", "Andeas", ""], ["Hutter", "Juerg", ""], ["Lazzaro", "Alfio", ""], ["Pabst", "Hans", ""], ["Reid", "Fiona", ""]]}, {"id": "1708.03792", "submitter": "Debasish Pattanayak", "authors": "Debasish Pattanayak, H. Ramesh, Partha Sarathi Mandal and Stefan\n  Schmid", "title": "Evacuating Two Robots from Two Unknown Exits on the Perimeter of a Disk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed evacuation of mobile robots is a recent development. We consider\nthe evacuation problem of two robots which are initially located at the center\nof a unit disk. Both the robots have to evacuate the disk through the exits\nsituated on the perimeter of the disk at an unknown location. The distance\nbetween two exits along the perimeter $d$ is given. We consider two different\ncommunication models. First, in the wireless model, the robots can send a\nmessage to each other over a long distance. Second, in face-to-face\ncommunication model, the robots can exchange information with each other only\nwhen they touch each other. The objective of the evacuation problem is to\ndesign an algorithm which minimizes the evacuation time of both the robots. For\nthe wireless communication model, we propose a generic algorithm for two robots\nmoving to two points on the perimeter with an initial separation of $\\zeta \\leq\nd$. We also investigate evacuation problem for both unlabeled and labeled exits\nin the wireless communication model. For the face-to-face communication model,\nwe propose two different algorithms for $\\zeta =0$ and $\\zeta =d$ for unlabeled\nexits. We also propose a generic algorithm for $\\zeta \\leq d$ for labeled\nexits. We provide lower bounds corresponding to different $d$ values in the\nface-to-face communication model. We evaluate the performance our algorithms\nwith simulation for both of the communication models.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 16:04:17 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Pattanayak", "Debasish", ""], ["Ramesh", "H.", ""], ["Mandal", "Partha Sarathi", ""], ["Schmid", "Stefan", ""]]}, {"id": "1708.03882", "submitter": "Raphael Jolly", "authors": "Raphael Jolly", "title": "Monadic Remote Invocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve Separation of Concerns in the domain of remote method\ninvocation, a small functional adapter is added atop Java RMI, eliminating the\nneed for every remote object to implement java.rmi.Remote and making it\npossible to remotely access existing code, unchanged. The Remote monad is\nintroduced, and its implementation and usage are detailed. Reusing the\nexisting, proven technology of RMI allows not to re-invent the underlying\nnetwork protocol. As a result, orthogonal remote invocation is achieved with\nlittle or no implementation effort.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 10:29:06 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Jolly", "Raphael", ""]]}, {"id": "1708.03903", "submitter": "Danupon Nanongkai", "authors": "Chien-Chung Huang, Danupon Nanongkai, Thatchaphol Saranurak", "title": "Distributed Exact Weighted All-Pairs Shortest Paths in $\\tilde\n  O(n^{5/4})$ Rounds", "comments": "Minor corrections in Section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study computing {\\em all-pairs shortest paths} (APSP) on distributed\nnetworks (the CONGEST model). The goal is for every node in the (weighted)\nnetwork to know the distance from every other node using communication. The\nproblem admits $(1+o(1))$-approximation $\\tilde O(n)$-time algorithms\n~\\cite{LenzenP-podc15,Nanongkai-STOC14}, which are matched with $\\tilde\n\\Omega(n)$-time lower\nbounds~\\cite{Nanongkai-STOC14,LenzenP_stoc13,FrischknechtHW12}\\footnote{$\\tilde\n\\Theta$, $\\tilde O$ and $\\tilde \\Omega$ hide polylogarithmic factors. Note that\nthe lower bounds also hold even in the unweighted case and in the weighted case\nwith polynomial approximation ratios.}. No $\\omega(n)$ lower bound or $o(m)$\nupper bound were known for exact computation.\n  In this paper, we present an $\\tilde O(n^{5/4})$-time randomized (Las Vegas)\nalgorithm for exact weighted APSP; this provides the first improvement over the\nnaive $O(m)$-time algorithm when the network is not so sparse. Our result also\nholds for the case where edge weights are {\\em asymmetric} (a.k.a. the directed\ncase where communication is bidirectional). Our techniques also yield an\n$\\tilde O(n^{3/4}k^{1/2}+n)$-time algorithm for the {\\em $k$-source shortest\npaths} problem where we want every node to know distances from $k$ sources;\nthis improves Elkin's recent bound~\\cite{Elkin-STOC17} when $k=\\tilde\n\\omega(n^{1/4})$.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 13:27:47 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 09:50:24 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Huang", "Chien-Chung", ""], ["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1708.04078", "submitter": "Fang-Zhou Jiang", "authors": "Fang-Zhou Jiang, Kanchana Thilakarathna, Sirine Mrabet, Mohamed Ali\n  Kaafar, and Aruna Seneviratne", "title": "uStash: a Novel Mobile Content Delivery System for Improving User QoE in\n  Public Transport", "comments": "14 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile data traffic is growing exponentially and it is even more challenging\nto distribute content efficiently while users are \"on the move\" such as in\npublic transport.The use of mobile devices for accessing content (e.g. videos)\nwhile commuting are both expensive and unreliable, although it is becoming\ncommon practice worldwide. Leveraging on the spatial and temporal correlation\nof content popularity and users' diverse network connectivity, we propose a\nnovel content distribution system, \\textit{uStash}, which guarantees better QoE\nwith regards to access delays and cost of usage. The proposed collaborative\ndownload and content stashing schemes provide the uStash provider the\nflexibility to control the cost of content access via cellular networks. We\nmodel the uStash system in a probabilistic framework and thereby analytically\nderive the optimal portions for collaborative downloading. Then, we validate\nthe proposed models using real-life trace driven simulations. In particular, we\nuse dataset from 22 inter-city buses running on 6 different routes and from a\nmobile VoD service provider to show that uStash reduces the cost of monthly\ncellular data by approximately 50\\% and the expected delay for content access\nby 60\\% compared to content downloaded via users' cellular network connections.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 11:28:13 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Jiang", "Fang-Zhou", ""], ["Thilakarathna", "Kanchana", ""], ["Mrabet", "Sirine", ""], ["Kaafar", "Mohamed Ali", ""], ["Seneviratne", "Aruna", ""]]}, {"id": "1708.04290", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Qizheng He, Wenzheng Li, Seth Pettie, Jara Uitto", "title": "The Complexity of Distributed Edge Coloring with Small Palettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of distributed edge coloring depends heavily on the palette\nsize as a function of the maximum degree $\\Delta$. In this paper we explore the\ncomplexity of edge coloring in the LOCAL model in different palette size\nregimes.\n  1. We simplify the \\emph{round elimination} technique of Brandt et al. and\nprove that $(2\\Delta-2)$-edge coloring requires $\\Omega(\\log_\\Delta \\log n)$\ntime w.h.p. and $\\Omega(\\log_\\Delta n)$ time deterministically, even on trees.\nThe simplified technique is based on two ideas: the notion of an irregular\nrunning time and some general observations that transform weak lower bounds\ninto stronger ones.\n  2. We give a randomized edge coloring algorithm that can use palette sizes as\nsmall as $\\Delta + \\tilde{O}(\\sqrt{\\Delta})$, which is a natural barrier for\nrandomized approaches. The running time of the algorithm is at most\n$O(\\log\\Delta \\cdot T_{LLL})$, where $T_{LLL}$ is the complexity of a\npermissive version of the constructive Lovasz local lemma.\n  3. We develop a new distributed Lovasz local lemma algorithm for\ntree-structured dependency graphs, which leads to a $(1+\\epsilon)\\Delta$-edge\ncoloring algorithm for trees running in $O(\\log\\log n)$ time. This algorithm\narises from two new results: a deterministic $O(\\log n)$-time LLL algorithm for\ntree-structured instances, and a randomized $O(\\log\\log n)$-time graph\nshattering method for breaking the dependency graph into independent $O(\\log\nn)$-size LLL instances.\n  4. A natural approach to computing $(\\Delta+1)$-edge colorings (Vizing's\ntheorem) is to extend partial colorings by iteratively re-coloring parts of the\ngraph. We prove that this approach may be viable, but in the worst case\nrequires recoloring subgraphs of diameter $\\Omega(\\Delta\\log n)$. This stands\nin contrast to distributed algorithms for Brooks' theorem, which exploit the\nexistence of $O(\\log_\\Delta n)$-length augmenting paths.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 19:47:53 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 03:34:02 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Chang", "Yi-Jun", ""], ["He", "Qizheng", ""], ["Li", "Wenzheng", ""], ["Pettie", "Seth", ""], ["Uitto", "Jara", ""]]}, {"id": "1708.04318", "submitter": "Chuan Li", "authors": "Chuan Li, Hongwei Zhang, Jayanthi Rao, Le Yi Wang, George Yin", "title": "Cyber-Physical Interference Modeling for Predictable Reliability of\n  Inter-Vehicle Communications", "comments": "10 page conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictable inter-vehicle communication reliability is a basis for the\nparadigm shift from the traditional singlevehicle-oriented safety and\nefficiency control to networked vehicle control. The lack of predictable\ninterference control in existing mechanisms of inter-vehicle communications,\nhowever, makes them incapable of ensuring predictable communication\nreliability. For predictable interference control, we propose the\nCyber-Physical Scheduling (CPS) framework that leverages the PRK interference\nmodel and addresses the challenges of vehicle mobility to PRK-based scheduling.\nIn particular, CPS leverage physical locations of vehicles to define the gPRK\ninterference model, a geometric approximation of the PRK model, for effective\ninterference relation estimation, and CPS leverages cyber-physical structures\nof vehicle traffic flows (particularly, spatiotemporal interference correlation\nas well as macro- and micro-scopic vehicle dynamics) for effective use of the\ngPRK model. Through experimental analysis with high-fidelity ns-3 and SUMO\nsimulation, we observe that CPS enables predictable reliability while achieving\nhigh throughput and low delay in communication. To the best of our knowledge,\nCPS is the first field deployable method that ensures predictable interference\ncontrol and thus reliability in inter-vehicle communications.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 20:49:31 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Li", "Chuan", ""], ["Zhang", "Hongwei", ""], ["Rao", "Jayanthi", ""], ["Wang", "Le Yi", ""], ["Yin", "George", ""]]}, {"id": "1708.04668", "submitter": "Abhinav Aggarwal", "authors": "Abhinav Aggarwal, Jos\\'e Abel Castellanos Joo, Diksha Gupta", "title": "Beating the Multiplicative Weights Update Algorithm", "comments": "Word done as part of UNM CS506 term paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplicative weights update algorithms have been used extensively in\ndesigning iterative algorithms for many computational tasks. The core idea is\nto maintain a distribution over a set of experts and update this distribution\nin an online fashion based on the parameters of the underlying optimization\nproblem. In this report, we study the behavior of a special MWU algorithm used\nfor generating a global coin flip in the presence of an adversary that tampers\nthe experts' advice. Specifically, we focus our attention on two adversarial\nstrategies: (1) non-adaptive, in which the adversary chooses a fixed set of\nexperts a priori and corrupts their advice in each round; and (2) adaptive, in\nwhich this set is chosen as the rounds of the algorithm progress. We formulate\nthese adversarial strategies as being greedy in terms of trying to maximize the\nshare of the corrupted experts in the final weighted advice the MWU computes\nand provide the underlying optimization problem that needs to be solved to\nachieve this goal. We provide empirical results to show that in the presence of\neither of the above adversaries, the MWU algorithm takes $\\mathcal{O}(n)$\nrounds in expectation to produce the desired output. This result compares well\nwith the current state of the art of $\\mathcal{O}(n^3)$ for the general\nByzantine consensus problem. Finally, we briefly discuss the extension of these\nadversarial strategies for a general MWU algorithm and provide an outline for\nthe framework in that setting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 20:13:48 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Aggarwal", "Abhinav", ""], ["Joo", "Jos\u00e9 Abel Castellanos", ""], ["Gupta", "Diksha", ""]]}, {"id": "1708.04701", "submitter": "Lei Jiang", "authors": "Lei Jiang, Langshi Chen, Judy Qiu", "title": "Performance Characterization of Multi-threaded Graph Processing\n  Applications on Intel Many-Integrated-Core Architecture", "comments": "published as L. Jiang, L. Chen and J. Qiu, \"Performance\n  Characterization of Multi-threaded Graph Processing Applications on\n  Many-Integrated-Core Architecture,\" 2018 IEEE International Symposium on\n  Performance Analysis of Systems and Software (ISPASS), Belfast, United\n  Kingdom, 2018, pp. 199-208", "journal-ref": null, "doi": "10.1109/ispass.2018.00033", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intel Xeon Phi many-integrated-core (MIC) architectures usher in a new era of\nterascale integration. Among emerging killer applications, parallel graph\nprocessing has been a critical technique to analyze connected data. In this\npaper, we empirically evaluate various computing platforms including an Intel\nXeon E5 CPU, a Nvidia Geforce GTX1070 GPU and an Xeon Phi 7210 processor\ncodenamed Knights Landing (KNL) in the domain of parallel graph processing. We\nshow that the KNL gains encouraging performance when processing graphs, so that\nit can become a promising solution to accelerating multi-threaded graph\napplications. We further characterize the impact of KNL architectural\nenhancements on the performance of a state-of-the art graph framework.We have\nfour key observations: 1 Different graph applications require distinctive\nnumbers of threads to reach the peak performance. For the same application,\nvarious datasets need even different numbers of threads to achieve the best\nperformance. 2 Only a few graph applications benefit from the high bandwidth\nMCDRAM, while others favor the low latency DDR4 DRAM. 3 Vector processing units\nexecuting AVX512 SIMD instructions on KNLs are underutilized when running the\nstate-of-the-art graph framework. 4 The sub-NUMA cache clustering mode offering\nthe lowest local memory access latency hurts the performance of graph\nbenchmarks that are lack of NUMA awareness. At last, We suggest future works\nincluding system auto-tuning tools and graph framework optimizations to fully\nexploit the potential of KNL for parallel graph processing.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 21:59:38 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 20:34:21 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Jiang", "Lei", ""], ["Chen", "Langshi", ""], ["Qiu", "Judy", ""]]}, {"id": "1708.04754", "submitter": "Hengfeng Wei", "authors": "Hengfeng Wei, Yu Huang, Jian Lu", "title": "Specification and Implementation of Replicated List: The Jupiter\n  Protocol Revisited", "comments": "41 pages. Extended version for the conference paper accepted by\n  OPODIS'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The replicated list object is frequently used to model the core functionality\nof replicated collaborative text editing systems. Since 1989, the convergence\nproperty has been a common specification of a replicated list object. Recently,\nAttiya et al. proposed the strong/weak list specification and conjectured that\nthe well-known Jupiter protocol satisfies the weak list specification. The\nmajor obstacle to proving this conjecture is the mismatch between the global\nproperty on all replica states prescribed by the specification and the local\nview each replica maintains in Jupiter using data structures like 1D buffer or\n2D state space. To address this issue, we propose CJupiter (Compact Jupiter)\nbased on a novel data structure called $n$-ary ordered state space for a\nreplicated client/server system with $n$ clients. At a high level, CJupiter\nmaintains only a single $n$-ary ordered state space which encompasses exactly\nall states of each replica. We prove that CJupiter and Jupiter are equivalent\nand that CJupiter satisfies the weak list specification, thus solving the\nconjecture above.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 03:11:07 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 06:11:44 GMT"}, {"version": "v3", "created": "Sat, 12 May 2018 05:35:41 GMT"}, {"version": "v4", "created": "Mon, 12 Nov 2018 12:47:27 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wei", "Hengfeng", ""], ["Huang", "Yu", ""], ["Lu", "Jian", ""]]}, {"id": "1708.04796", "submitter": "Alexandre Da Veith", "authors": "Alexandre Da Silva Veith (1), Julio C. S. dos Anjos (2), Edison\n  Pignaton de Freitas (2), Thomas Lampoltshammer, Claudio Geyer (3) ((1)\n  AVALON, (2) URFGS, (3) PPGC)", "title": "Strategies for Big Data Analytics through Lambda Architectures in\n  Volatile Environments", "comments": null, "journal-ref": "IFAC, Nov 2016, Porto Alegre, Brazil. pp.114-119", "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectations regarding the future growth of Internet of Things (IoT)-related\ntechnologies are high. These expectations require the realization of a\nsustainable general purpose application framework that is capable to handle\nthese kinds of environments with their complexity in terms of heterogeneity and\nvolatility. The paradigm of the Lambda architecture features key\ncharacteristics (such as robustness, fault tolerance, scalability,\ngeneralization, extensibility, ad-hoc queries, minimal maintenance, and\nlow-latency reads and updates) to cope with this complexity. The paper at hand\nsuggest a basic set of strategies to handle the arising challenges regarding\nthe volatility, heterogeneity, and desired low latency execution by reducing\nthe overall system timing (scheduling, execution, monitoring, and faults\nrecovery) as well as possible faults (churn, no answers to executions). The\nproposed strategies make use of services such as migration, replication,\nMapReduce simulation, and combined processing methods (batch- and\nstreaming-based). Via these services, a distribution of tasks for the best\nbalance of computational resources is achieved, while monitoring and management\ncan be performed asynchronously in the background. %An application of batch and\nstream-based methods are proposed to reduce the latency.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 07:46:53 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Veith", "Alexandre Da Silva", ""], ["Anjos", "Julio C. S. dos", ""], ["de Freitas", "Edison Pignaton", ""], ["Lampoltshammer", "Thomas", ""], ["Geyer", "Claudio", ""]]}, {"id": "1708.04838", "submitter": "Trevor Brown", "authors": "Trevor Brown", "title": "A Template for Implementing Fast Lock-free Trees Using HTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms that use hardware transactional memory (HTM) must provide a\nsoftware-only fallback path to guarantee progress. The design of the fallback\npath can have a profound impact on performance. If the fallback path is allowed\nto run concurrently with hardware transactions, then hardware transactions must\nbe instrumented, adding significant overhead. Otherwise, hardware transactions\nmust wait for any processes on the fallback path, causing concurrency\nbottlenecks, or move to the fallback path. We introduce an approach that\ncombines the best of both worlds. The key idea is to use three execution paths:\nan HTM fast path, an HTM middle path, and a software fallback path, such that\nthe middle path can run concurrently with each of the other two. The fast path\nand fallback path do not run concurrently, so the fast path incurs no\ninstrumentation overhead. Furthermore, fast path transactions can move to the\nmiddle path instead of waiting or moving to the software path. We demonstrate\nour approach by producing an accelerated version of the tree update template of\nBrown et al., which can be used to implement fast lock-free data structures\nbased on down-trees. We used the accelerated template to implement two\nlock-free trees: a binary search tree (BST), and an (a,b)-tree (a\ngeneralization of a B-tree). Experiments show that, with 72 concurrent\nprocesses, our accelerated (a,b)-tree performs between 4.0x and 4.2x as many\noperations per second as an implementation obtained using the original tree\nupdate template.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 10:35:25 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Brown", "Trevor", ""]]}, {"id": "1708.04863", "submitter": "Marius Poke", "authors": "Marius Poke and Colin W. Glass", "title": "Formal Specification and Safety Proof of a Leaderless Concurrent Atomic\n  Broadcast Algorithm", "comments": "11 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agreement plays a central role in distributed systems working on a common\ntask. The increasing size of modern distributed systems makes them more\nsusceptible to single component failures. Fault-tolerant distributed agreement\nprotocols rely for the most part on leader-based atomic broadcast algorithms,\nsuch as Paxos. Such protocols are mostly used for data replication, which\nrequires only a small number of servers to reach agreement. Yet, their\ncentralized nature makes them ill-suited for distributed agreement at large\nscales. The recently introduced atomic broadcast algorithm AllConcur enables\nhigh throughput for distributed agreement while being completely decentralized.\nIn this paper, we extend the work on AllConcur in two ways. First, we provide a\nformal specification of AllConcur that enables a better understanding of the\nalgorithm. Second, we formally prove AllConcur's safety property on the basis\nof this specification. Therefore, our work not only ensures operators safe\nusage of AllConcur, but also facilitates the further improvement of distributed\nagreement protocols based on AllConcur.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 12:47:32 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Poke", "Marius", ""], ["Glass", "Colin W.", ""]]}, {"id": "1708.05076", "submitter": "Cetin Savkli", "authors": "Cetin Savkli, Ryan Carr, Matthew Chapman, Brant Chee, David Minch", "title": "SOCRATES: A System For Scalable Graph Analytics", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/HPEC.2014.7040993", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed semantic graph processing system that provides locality\ncontrol, indexing, graph query, and parallel processing capabilities is\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 20:45:10 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Savkli", "Cetin", ""], ["Carr", "Ryan", ""], ["Chapman", "Matthew", ""], ["Chee", "Brant", ""], ["Minch", "David", ""]]}, {"id": "1708.05136", "submitter": "Robert Hannah", "authors": "Robert Hannah, Wotao Yin", "title": "More Iterations per Second, Same Quality -- Why Asynchronous Algorithms\n  may Drastically Outperform Traditional Ones", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the convergence of a very general\nasynchronous-parallel algorithm called ARock, that takes many well-known\nasynchronous algorithms as special cases (gradient descent, proximal gradient,\nDouglas Rachford, ADMM, etc.). In asynchronous-parallel algorithms, the\ncomputing nodes simply use the most recent information that they have access\nto, instead of waiting for a full update from all nodes in the system. This\nmeans that nodes do not have to waste time waiting for information, which can\nbe a major bottleneck, especially in distributed systems. When the system has\n$p$ nodes, asynchronous algorithms may complete $\\Theta(\\ln(p))$ more\niterations than synchronous algorithms in a given time period (\"more iterations\nper second\").\n  Although asynchronous algorithms may compute more iterations per second,\nthere is error associated with using outdated information. How many more\niterations in total are needed to compensate for this error is still an open\nquestion. The main results of this paper aim to answer this question. We prove,\nloosely, that as the size of the problem becomes large, the number of\nadditional iterations that asynchronous algorithms need becomes negligible\ncompared to the total number (\"same quality\" of the iterations). Taking these\nfacts together, our results provide solid evidence of the potential of\nasynchronous algorithms to vastly speed up certain distributed computations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 05:01:25 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Hannah", "Robert", ""], ["Yin", "Wotao", ""]]}, {"id": "1708.05264", "submitter": "Vincent Cicirello", "authors": "Vincent A. Cicirello", "title": "Design, Configuration, Implementation, and Performance of a Simple 32\n  Core Raspberry Pi Cluster", "comments": "Stockton University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, I describe the design and implementation of an inexpensive,\neight node, 32 core, cluster of raspberry pi single board computers, as well as\nthe performance of this cluster on two computational tasks, one that requires\nsignificant data transfer relative to computational time requirements, and one\nthat does not. We have two use-cases for the cluster: (a) as an educational\ntool for classroom usage, such as covering parallel algorithms in an algorithms\ncourse; and (b) as a test system for use during the development of parallel\nmetaheuristics, essentially serving as a personal desktop parallel computing\ncluster. Our preliminary results show that the slow 100 Mbps networking of the\nraspberry pi significantly limits such clusters to parallel computational tasks\nthat are either long running relative to data communications requirements, or\nthat which requires very little internode communications. Additionally,\nalthough the raspberry pi 3 has a quad-core processor, parallel speedup\ndegrades during attempts to utilize all four cores of all cluster nodes for a\nparallel computation, likely due to resource contention with operating system\nlevel processes. However, distributing a task across three cores of each\ncluster node does enable linear (or near linear) speedup.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 13:43:26 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Cicirello", "Vincent A.", ""]]}, {"id": "1708.05327", "submitter": "Johannes K\\\"ostler", "authors": "Johannes K\\\"ostler, Hans P. Reiser", "title": "Analysis of Static and Dynamic Configurability of Existing Group\n  Communication Systems", "comments": "Technical Report (38 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active replication following the state machine replication (SMR) approach is\na way to make existing systems and services more reliable and fault-tolerant.\nThe additional communication overhead has a negative impact on the system's\nthroughput and overall request latency. Today's systems should be highly\noptimized to their execution environment and usage scenario in order to remedy\nthe performance loss introduced by such group communication systems (GCS). In\naddition to that, systems should be able to adapt to changing environmental\nconditions. This report analyzes the available configuration options of three\nexisting GCSs. Therefore, it explains the available configuration parameters\nand describes the given reconfiguration mechanisms. The found parameters are\nthen classified in a parameter scheme.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 15:10:57 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["K\u00f6stler", "Johannes", ""], ["Reiser", "Hans P.", ""]]}, {"id": "1708.05357", "submitter": "Celestine D\\\"unner", "authors": "Celestine D\\\"unner, Thomas Parnell, Martin Jaggi", "title": "Efficient Use of Limited-Memory Accelerators for Linear Learning on\n  Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a generic algorithmic building block to accelerate training of\nmachine learning models on heterogeneous compute systems. Our scheme allows to\nefficiently employ compute accelerators such as GPUs and FPGAs for the training\nof large-scale machine learning models, when the training data exceeds their\nmemory capacity. Also, it provides adaptivity to any system's memory hierarchy\nin terms of size and processing speed. Our technique is built upon novel\ntheoretical insights regarding primal-dual coordinate methods, and uses duality\ngap information to dynamically decide which part of the data should be made\navailable for fast processing. To illustrate the power of our approach we\ndemonstrate its performance for training of generalized linear models on a\nlarge-scale dataset exceeding the memory size of a modern GPU, showing an\norder-of-magnitude speedup over existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 16:33:20 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 10:47:36 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["D\u00fcnner", "Celestine", ""], ["Parnell", "Thomas", ""], ["Jaggi", "Martin", ""]]}, {"id": "1708.05425", "submitter": "Tevfik Kosar", "authors": "Engin Arslan and Tevfik Kosar", "title": "A Heuristic Approach to Protocol Tuning for High Performance Data\n  Transfers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining optimal data transfer performance is of utmost importance to\ntoday's data-intensive distributed applications and wide-area data replication\nservices. Doing so necessitates effectively utilizing available network\nbandwidth and resources, yet in practice transfers seldom reach the levels of\nutilization they potentially could. Tuning protocol parameters such as\npipelining, parallelism, and concurrency can significantly increase utilization\nand performance, however determining the best settings for these parameters is\na difficult problem, as network conditions can vary greatly between sites and\nover time. Nevertheless, it is an important problem, since poor tuning can\ncause either under- or over-utilization of network resources and thus degrade\ntransfer performance. In this paper, we present three algorithms for\napplication-level tuning of different protocol parameters for maximizing\ntransfer throughput in wide-area networks. Our algorithms dynamically tune the\nnumber of parallel data streams per file (for large file optimization), the\nlevel of control channel pipelining (for small file optimization), and the\nnumber of concurrent file transfers to increase I/O throughput (a technique\nuseful for all types of files). The proposed heuristic algorithms improve the\ntransfer throughput up to 10x compared to the baseline and 7x compared to the\nstate of the art solutions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 20:13:42 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Arslan", "Engin", ""], ["Kosar", "Tevfik", ""]]}, {"id": "1708.05680", "submitter": "K\\'evin Atighehchi", "authors": "Kevin Atighehchi and Alexis Bonnecaze", "title": "Asymptotic Analysis of Plausible Tree Hash Modes for SHA-3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussions about the choice of a tree hash mode of operation for a\nstandardization have recently been undertaken. It appears that a single tree\nmode cannot address adequately all possible uses and specifications of a\nsystem. In this paper, we review the tree modes which have been proposed, we\ndiscuss their problems and propose remedies. We make the reasonable assumption\nthat communicating systems have different specifications and that software\napplications are of different types (securing stored content or live-streamed\ncontent). Finally, we propose new modes of operation that address the resource\nusage problem for the three most representative categories of devices and we\nanalyse their asymptotic behavior.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 16:26:11 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Atighehchi", "Kevin", ""], ["Bonnecaze", "Alexis", ""]]}, {"id": "1708.05746", "submitter": "Alexander Ulanov", "authors": "Mijung Kim, Jun Li, Haris Volos, Manish Marwah, Alexander Ulanov,\n  Kimberly Keeton, Joseph Tucek, Lucy Cherkasova, Le Xu, and Pradeep Fernando", "title": "Sparkle: Optimizing Spark for Large Memory Machines and Analytics", "comments": "14 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spark is an in-memory analytics platform that targets commodity server\nenvironments today. It relies on the Hadoop Distributed File System (HDFS) to\npersist intermediate checkpoint states and final processing results. In Spark,\nimmutable data are used for storing data updates in each iteration, making it\ninefficient for long running, iterative workloads. A non-deterministic garbage\ncollector further worsens this problem. Sparkle is a library that optimizes\nmemory usage in Spark. It exploits large shared memory to achieve better data\nshuffling and intermediate storage. Sparkle replaces the current TCP/IP-based\nshuffle with a shared memory approach and proposes an off-heap memory store for\nefficient updates. We performed a series of experiments on scale-out clusters\nand scale-up machines. The optimized shuffle engine leveraging shared memory\nprovides 1.3x to 6x faster performance relative to Vanilla Spark. The off-heap\nmemory store along with the shared-memory shuffle engine provides more than 20x\nperformance increase on a probabilistic graph processing workload that uses a\nlarge-scale real-world hyperlink graph. While Sparkle benefits at most from\nrunning on large memory machines, it also achieves 1.6x to 5x performance\nimprovements over scale out cluster with equivalent hardware setting.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 19:49:20 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Kim", "Mijung", ""], ["Li", "Jun", ""], ["Volos", "Haris", ""], ["Marwah", "Manish", ""], ["Ulanov", "Alexander", ""], ["Keeton", "Kimberly", ""], ["Tucek", "Joseph", ""], ["Cherkasova", "Lucy", ""], ["Xu", "Le", ""], ["Fernando", "Pradeep", ""]]}, {"id": "1708.05840", "submitter": "Disha Shrivastava", "authors": "Disha Shrivastava, Santanu Chaudhury and Dr. Jayadeva", "title": "A Data and Model-Parallel, Distributed and Scalable Framework for\n  Training of Deep Networks in Apache Spark", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks is expensive and time-consuming with the training\nperiod increasing with data size and growth in model parameters. In this paper,\nwe provide a framework for distributed training of deep networks over a cluster\nof CPUs in Apache Spark. The framework implements both Data Parallelism and\nModel Parallelism making it suitable to use for deep networks which require\nhuge training data and model parameters which are too big to fit into the\nmemory of a single machine. It can be scaled easily over a cluster of cheap\ncommodity hardware to attain significant speedup and obtain better results\nmaking it quite economical as compared to farm of GPUs and supercomputers. We\nhave proposed a new algorithm for training of deep networks for the case when\nthe network is partitioned across the machines (Model Parallelism) along with\ndetailed cost analysis and proof of convergence of the same. We have developed\nimplementations for Fully-Connected Feedforward Networks, Convolutional Neural\nNetworks, Recurrent Neural Networks and Long Short-Term Memory architectures.\nWe present the results of extensive simulations demonstrating the speedup and\naccuracy obtained by our framework for different sizes of the data and model\nparameters with variation in the number of worker cores/partitions; thereby\nshowing that our proposed framework can achieve significant speedup (upto 11X\nfor CNN) and is also quite scalable.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 13:17:58 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Shrivastava", "Disha", ""], ["Chaudhury", "Santanu", ""], ["Jayadeva", "Dr.", ""]]}, {"id": "1708.05965", "submitter": "Christophe Guyeux", "authors": "Ahmad Farhat and Abdallah Makhoul and Christophe Guyeux and Rami Tawil\n  and Ali Jaber and Abbas Hijazi", "title": "On the topology effects in wireless sensor networks based prognostics\n  and health management", "comments": "19th IEEE International Conference on Computational Science and\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the usage of wireless sensor networks (WSN) to\nmonitor an area of interest, in order to diagnose on real time its state. Each\nsensor node forwards information about relevant features towards the sink where\nthe data is processed. Nevertheless, energy conservation is a key issue in the\ndesign of such networks and once a sensor exhausts its resources, it will be\ndropped from the network. This will lead to broken links and data loss. It is\ntherefore important to keep the network running for as long as possible by\npreserving the energy held by the nodes. Indeed, saving the quality of service\n(QoS) of a wireless sensor network for a long period is very important in order\nto ensure accurate data. Then, the area diagnosing will be more accurate. From\nanother side, packet transmission is the phase that consumes the highest amount\nof energy comparing to other activities in the network. Therefore, we can see\nthat the network topology has an important impact on energy efficiency, and\nthus on data and diagnosis accuracies. In this paper, we study and compare four\nnetwork topologies: distributed, hierarchical, centralized, and decentralized\ntopology and show their impact on the resulting estimation of diagnostics. We\nhave used six diagnostic algorithms, to evaluate both prognostic and health\nmanagement with the variation of type of topology in WSN.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 13:48:25 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Farhat", "Ahmad", ""], ["Makhoul", "Abdallah", ""], ["Guyeux", "Christophe", ""], ["Tawil", "Rami", ""], ["Jaber", "Ali", ""], ["Hijazi", "Abbas", ""]]}, {"id": "1708.06127", "submitter": "Alexander Noe", "authors": "Monika Henzinger, Alexander Noe, Christian Schulz, Darren Strash", "title": "Practical Minimum Cut Algorithms", "comments": null, "journal-ref": "J. Exp. Algorithmics 23: 1.8:1-1.8:22 (2018)", "doi": "10.1145/3274662", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum cut problem for an undirected edge-weighted graph asks us to\ndivide its set of nodes into two blocks while minimizing the weight sum of the\ncut edges. Here, we introduce a linear-time algorithm to compute near-minimum\ncuts. Our algorithm is based on cluster contraction using label propagation and\nPadberg and Rinaldi's contraction heuristics [SIAM Review, 1991]. We give both\nsequential and shared-memory parallel implementations of our algorithm.\nExtensive experiments on both real-world and generated instances show that our\nalgorithm finds the optimal cut on nearly all instances significantly faster\nthan other state-of-the-art algorithms while our error rate is lower than that\nof other heuristic algorithms. In addition, our parallel algorithm shows good\nscalability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 09:34:13 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 08:25:54 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Henzinger", "Monika", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""]]}, {"id": "1708.06151", "submitter": "Christian Schulz", "authors": "Demian Hespe, Christian Schulz, Darren Strash", "title": "Scalable Kernelization for Maximum Independent Sets", "comments": "Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most efficient algorithms for finding maximum independent sets in both\ntheory and practice use reduction rules to obtain a much smaller problem\ninstance called a kernel. The kernel can then be solved quickly using exact or\nheuristic algorithms---or by repeatedly kernelizing recursively in the\nbranch-and-reduce paradigm. It is of critical importance for these algorithms\nthat kernelization is fast and returns a small kernel. Current algorithms are\neither slow but produce a small kernel, or fast and give a large kernel. We\nattempt to accomplish both of these goals simultaneously, by giving an\nefficient parallel kernelization algorithm based on graph partitioning and\nparallel bipartite maximum matching. We combine our parallelization techniques\nwith two techniques to accelerate kernelization further: dependency checking\nthat prunes reductions that cannot be applied, and reduction tracking that\nallows us to stop kernelization when reductions become less fruitful. Our\nalgorithm produces kernels that are orders of magnitude smaller than the\nfastest kernelization methods, while having a similar execution time.\nFurthermore, our algorithm is able to compute kernels with size comparable to\nthe smallest known kernels, but up to two orders of magnitude faster than\npreviously possible. Finally, we show that our kernelization algorithm can be\nused to accelerate existing state-of-the-art heuristic algorithms, allowing us\nto find larger independent sets faster on large real-world networks and\nsynthetic instances.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 11:14:42 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 08:16:16 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Hespe", "Demian", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""]]}, {"id": "1708.06183", "submitter": "Sebastien Tixeuil", "authors": "Adam Heriban (NPA), Xavier D\\'efago (TITECH), S\\'ebastien Tixeuil\n  (NPA, IUF, LINCS)", "title": "Optimally Gathering Two Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that ensures in finite time the gathering of two\nrobots in the non-rigid ASYNC model. To circumvent established impossibility\nresults, we assume robots are equipped with 2-colors lights and are able to\nmeasure distances between one another. Aside from its light, a robot has no\nmemory of its past actions, and its protocol is deterministic. Since, in the\nsame model, gathering is impossible when lights have a single color, our\nsolution is optimal with respect to the number of used colors.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 12:29:25 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Heriban", "Adam", "", "NPA"], ["D\u00e9fago", "Xavier", "", "TITECH"], ["Tixeuil", "S\u00e9bastien", "", "NPA, IUF, LINCS"]]}, {"id": "1708.06209", "submitter": "Quoc-Tuan Vien", "authors": "Quoc-Tuan Vien, Michael Opoku Agyeman, Tuan Anh Le, Terrence Mak", "title": "On the Nanocommunications at THz Band in Graphene-Enabled Wireless\n  Network-on-Chip", "comments": "13 pages, 8 figures", "journal-ref": "Mathematical Problems in Engineering (Hindawi), Vol. 2017, Article\n  ID 9768604, 13 pages", "doi": "10.1155/2017/9768604", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the main challenges towards the growing computation-intensive\napplications with scalable bandwidth requirement is the deployment of a dense\nnumber of on-chip cores within a chip package. To this end, this paper\ninvestigates the Wireless Network- on-Chip (WiNoC), which is enabled by\ngraphene-based nanoantennas (GNAs) in Terahertz frequency band. We first\ndevelop a channel model between the GNAs taking into account the practical\nissues of the propagation medium, such as transmission frequency, operating\ntemperature, ambient pressure, and distance between the GNAs. In the Terahertz\nband, not only dielectric propagation loss but also molecular absorption\nattenuation (MAA) caused by various molecules and their isotopologues within\nthe chip package constitutes the signal transmission loss. We further propose\nan optimal power allocation to achieve the channel capacity. The proposed\nchannel model shows that the MAA significantly degrades the performance at\ncertain frequency ranges compared to the conventional channel model, even when\nthe GNAs are very closely located. More specifically, at transmission frequency\nof 1 THz, the channel capacity of the proposed model is shown to be much lower\nthan that of the conventional model over the whole range of temperature and\nambient pressure of up to 26.8% and 25%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 20:33:45 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Vien", "Quoc-Tuan", ""], ["Agyeman", "Michael Opoku", ""], ["Le", "Tuan Anh", ""], ["Mak", "Terrence", ""]]}, {"id": "1708.06248", "submitter": "Linghao Song", "authors": "Linghao Song, Youwei Zhuo, Xuehai Qian, Hai Li, Yiran Chen", "title": "GraphR: Accelerating Graph Processing Using ReRAM", "comments": "Accepted to HPCA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GRAPHR, the first ReRAM-based graph processing\naccelerator. GRAPHR follows the principle of near-data processing and explores\nthe opportunity of performing massive parallel analog operations with low\nhardware and energy cost. The analog computation is suit- able for graph\nprocessing because: 1) The algorithms are iterative and could inherently\ntolerate the imprecision; 2) Both probability calculation (e.g., PageRank and\nCollaborative Filtering) and typical graph algorithms involving integers (e.g.,\nBFS/SSSP) are resilient to errors. The key insight of GRAPHR is that if a\nvertex program of a graph algorithm can be expressed in sparse matrix vector\nmultiplication (SpMV), it can be efficiently performed by ReRAM crossbar. We\nshow that this assumption is generally true for a large set of graph\nalgorithms. GRAPHR is a novel accelerator architecture consisting of two\ncomponents: memory ReRAM and graph engine (GE). The core graph computations are\nperformed in sparse matrix format in GEs (ReRAM crossbars). The\nvector/matrix-based graph computation is not new, but ReRAM offers the unique\nopportunity to realize the massive parallelism with unprecedented energy\nefficiency and low hardware cost. With small subgraphs processed by GEs, the\ngain of performing parallel operations overshadows the wastes due to sparsity.\nThe experiment results show that GRAPHR achieves a 16.01x (up to 132.67x)\nspeedup and a 33.82x energy saving on geometric mean compared to a CPU baseline\nsystem. Com- pared to GPU, GRAPHR achieves 1.69x to 2.19x speedup and consumes\n4.77x to 8.91x less energy. GRAPHR gains a speedup of 1.16x to 4.12x, and is\n3.67x to 10.96x more energy efficiency compared to PIM-based architecture.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:21:36 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 04:23:14 GMT"}, {"version": "v3", "created": "Sun, 22 Oct 2017 19:18:28 GMT"}, {"version": "v4", "created": "Fri, 8 Dec 2017 22:02:14 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Song", "Linghao", ""], ["Zhuo", "Youwei", ""], ["Qian", "Xuehai", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1708.06334", "submitter": "S\\'ergio Matos", "authors": "Carlos Viana-Ferreira, Ant\\'onio Guerra, Jo\\~ao F. Silva, S\\'ergio\n  Matos, Carlos Costa", "title": "An Intelligent Cloud Storage Gateway for Medical Imaging", "comments": "Preprint to be published in Journal of Medical Systems. 8 pages, 5\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Historically, medical imaging repositories have been supported by indoor\ninfrastructures. However, the amount of diagnostic imaging procedures has\ncontinuously increased over the last decades, imposing several challenges\nassociated with the storage volume, data redundancy and availability. Cloud\nplatforms are focused on delivering hardware and software services over the\nInternet, becoming an appealing solution for repository outsourcing. Although\nthis option may bring financial and technological benefits, it also presents\nnew challenges. In medical imaging scenarios, communication latency is a\ncritical issue that still hinders the adoption of this paradigm. This paper\nproposes an intelligent Cloud storage gateway that optimizes data access times.\nThis is achieved through a new cache architecture that combines static rules\nand pattern recognition for eviction and prefetching. The evaluation results,\nobtained through simulations over a real-world dataset, show that cache hit\nratios can reach around 80%, leading reductions of image retrieval times by\nover 60%. The combined use of eviction and prefetching policies pro- posed can\nsignificantly reduce communication latency, even when using a small cache in\ncomparison to the total size of the repository. Apart from the performance\ngains, the proposed system is capable of adjusting to specific workflows of\ndifferent institutions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 11:10:44 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Viana-Ferreira", "Carlos", ""], ["Guerra", "Ant\u00f3nio", ""], ["Silva", "Jo\u00e3o F.", ""], ["Matos", "S\u00e9rgio", ""], ["Costa", "Carlos", ""]]}, {"id": "1708.06423", "submitter": "Christopher Meiklejohn", "authors": "Christopher S. Meiklejohn, Vitor Enes, Junghun Yoo, Carlos Baquero,\n  Peter Van Roy, Annette Bieniusa", "title": "Practical Evaluation of the Lasp Programming Model at Large Scale - An\n  Experience Report", "comments": null, "journal-ref": null, "doi": "10.1145/3131851.3131862", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming models for building large-scale distributed applications assist\nthe developer in reasoning about consistency and distribution. However, many of\nthe programming models for weak consistency, which promise the largest\nscalability gains, have little in the way of evaluation to demonstrate the\npromised scalability. We present an experience report on the implementation and\nlarge-scale evaluation of one of these models, Lasp, originally presented at\nPPDP `15, which provides a declarative, functional programming style for\ndistributed applications. We demonstrate the scalability of Lasp's prototype\nruntime implementation up to 1024 nodes in the Amazon cloud computing\nenvironment. It achieves high scalability by uniquely combining hybrid gossip\nwith a programming model based on convergent computation. We report on the\nengineering challenges of this implementation and its evaluation, specifically\nrelated to operating research prototypes in a production cloud environment.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 21:22:24 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Meiklejohn", "Christopher S.", ""], ["Enes", "Vitor", ""], ["Yoo", "Junghun", ""], ["Baquero", "Carlos", ""], ["Van Roy", "Peter", ""], ["Bieniusa", "Annette", ""]]}, {"id": "1708.06542", "submitter": "Michael Feldmann M. Sc.", "authors": "Michael Feldmann and Christian Scheideler", "title": "A Self-Stabilizing General De Bruijn Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for other participants is one of the most important operations in a\ndistributed system. We are interested in topologies in which it is possible to\nroute a packet in a fixed number of hops until it arrives at its destination.\nGiven a constant $d$, this paper introduces a new self-stabilizing protocol for\nthe $q$-ary $d$-dimensional de Bruijn graph ($q = \\sqrt[d]{n}$) that is able to\nroute any search request in at most $d$ hops w.h.p., while significantly\nlowering the node degree compared to the clique: We require nodes to have a\ndegree of $\\mathcal O(\\sqrt[d]{n})$, which is asymptotically optimal for a\nfixed diameter $d$. The protocol keeps the expected amount of edge redirections\nper node in $\\mathcal O(\\sqrt[d]{n})$, when the number of nodes in the system\nincreases by factor $2^d$. The number of messages that are periodically sent\nout by nodes is constant.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 08:59:47 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 12:53:34 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Feldmann", "Michael", ""], ["Scheideler", "Christian", ""]]}, {"id": "1708.06866", "submitter": "Siddharth Samsi", "authors": "Siddharth Samsi, Vijay Gadepally, Michael Hurley, Michael Jones,\n  Edward Kao, Sanjeev Mohindra, Paul Monticciolo, Albert Reuther, Steven Smith,\n  William Song, Diane Staheli, Jeremy Kepner", "title": "Static Graph Challenge: Subgraph Isomorphism", "comments": null, "journal-ref": null, "doi": "10.1109/HPEC.2017.8091039", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of graph analytic systems has created a need for ways to measure and\ncompare the capabilities of these systems. Graph analytics present unique\nscalability difficulties. The machine learning, high performance computing, and\nvisual analytics communities have wrestled with these difficulties for decades\nand developed methodologies for creating challenges to move these communities\nforward. The proposed Subgraph Isomorphism Graph Challenge draws upon prior\nchallenges from machine learning, high performance computing, and visual\nanalytics to create a graph challenge that is reflective of many real-world\ngraph analytics processing systems. The Subgraph Isomorphism Graph Challenge is\na holistic specification with multiple integrated kernels that can be run\ntogether or independently. Each kernel is well defined mathematically and can\nbe implemented in any programming environment. Subgraph isomorphism is amenable\nto both vertex-centric implementations and array-based implementations (e.g.,\nusing the GraphBLAS.org standard). The computations are simple enough that\nperformance predictions can be made based on simple computing hardware models.\nThe surrounding kernels provide the context for each kernel that allows\nrigorous definition of both the input and the output for each kernel.\nFurthermore, since the proposed graph challenge is scalable in both problem\nsize and hardware, it can be used to measure and quantitatively compare a wide\nrange of present day and future systems. Serial implementations in C++, Python,\nPython with Pandas, Matlab, Octave, and Julia have been implemented and their\nsingle threaded performance have been measured. Specifications, data, and\nsoftware are publicly available at GraphChallenge.org.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 01:51:08 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Samsi", "Siddharth", ""], ["Gadepally", "Vijay", ""], ["Hurley", "Michael", ""], ["Jones", "Michael", ""], ["Kao", "Edward", ""], ["Mohindra", "Sanjeev", ""], ["Monticciolo", "Paul", ""], ["Reuther", "Albert", ""], ["Smith", "Steven", ""], ["Song", "William", ""], ["Staheli", "Diane", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1708.06881", "submitter": "Guoqiang Zhang", "authors": "Guoqiang Zhang, W. Bastiaan Kleijn and Richard Heusdens", "title": "On Relationship between Primal-Dual Method of Multipliers and Kalman\n  Filter", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the primal-dual method of multipliers (PDMM), a novel distributed\noptimization method, was proposed for solving a general class of decomposable\nconvex optimizations over graphic models. In this work, we first study the\nconvergence properties of PDMM for decomposable quadratic optimizations over\ntree-structured graphs. We show that with proper parameter selection, PDMM\nconverges to its optimal solution in finite number of iterations. We then apply\nPDMM for the causal estimation problem over a statistical linear state-space\nmodel. We show that PDMM and the Kalman filter have the same update\nexpressions, where PDMM can be interpreted as solving a sequence of quadratic\noptimizations over a growing chain graph.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 03:55:45 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Zhang", "Guoqiang", ""], ["Kleijn", "W. Bastiaan", ""], ["Heusdens", "Richard", ""]]}, {"id": "1708.06884", "submitter": "Saurabh Hukerikar", "authors": "Byung H. Park, Saurabh Hukerikar, Ryan Adamson, Christian Engelmann", "title": "Big Data Meets HPC Log Analytics: Scalable Approach to Understanding\n  Systems at Extreme Scale", "comments": "IEEE Cluster 2017 at Workshop on Monitoring and Analysis for High\n  Performance Computing Systems Plus Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's high-performance computing (HPC) systems are heavily instrumented,\ngenerating logs containing information about abnormal events, such as critical\nconditions, faults, errors and failures, system resource utilization, and about\nthe resource usage of user applications. These logs, once fully analyzed and\ncorrelated, can produce detailed information about the system health, root\ncauses of failures, and analyze an application's interactions with the system,\nproviding valuable insights to domain scientists and system administrators.\nHowever, processing HPC logs requires a deep understanding of hardware and\nsoftware components at multiple layers of the system stack. Moreover, most log\ndata is unstructured and voluminous, making it more difficult for system users\nand administrators to manually inspect the data. With rapid increases in the\nscale and complexity of HPC systems, log data processing is becoming a big data\nchallenge. This paper introduces a HPC log data analytics framework that is\nbased on a distributed NoSQL database technology, which provides scalability\nand high availability, and the Apache Spark framework for rapid in-memory\nprocessing of the log data. The analytics framework enables the extraction of a\nrange of information about the system so that system administrators and end\nusers alike can obtain necessary insights for their specific needs. We describe\nour experience with using this framework to glean insights from the log data\nabout system behavior from the Titan supercomputer at the Oak Ridge National\nLaboratory.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 04:41:47 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Park", "Byung H.", ""], ["Hukerikar", "Saurabh", ""], ["Adamson", "Ryan", ""], ["Engelmann", "Christian", ""]]}, {"id": "1708.06931", "submitter": "Christian M. Fuchs", "authors": "Christian M. Fuchs, Todor Stefanov, Nadia Murillo, Aske Plaat", "title": "Bringing Fault-Tolerant GigaHertz-Computing to Space: A Multi-Stage\n  Software-Side Fault-Tolerance Approach for Miniaturized Spacecraft", "comments": "26th IEEE Asian Test Symposium 2017, 27-30 Nov 2017, Taipei City,\n  Taiwan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern embedded technology is a driving factor in satellite miniaturization,\ncontributing to a massive boom in satellite launches and a rapidly evolving new\nspace industry. Miniaturized satellites, however, suffer from low reliability,\nas traditional hardware-based fault-tolerance (FT) concepts are ineffective for\non-board computers (OBCs) utilizing modern systems-on-a-chip (SoC). Therefore,\nlarger satellites continue to rely on proven processors with large feature\nsizes. Software-based concepts have largely been ignored by the space industry\nas they were researched only in theory, and have not yet reached the level of\nmaturity necessary for implementation. We present the first integral,\nreal-world solution to enable fault-tolerant general-purpose computing with\nmodern multiprocessor-SoCs (MPSoCs) for spaceflight, thereby enabling their use\nin future high-priority space missions. The presented multi-stage approach\nconsists of three FT stages, combining coarse-grained thread-level distributed\nself-validation, FPGA reconfiguration, and mixed criticality to assure\nlong-term FT and excellent scalability for both resource constrained and\ncritical high-priority space missions. Early benchmark results indicate a\ndrastic performance increase over state-of-the-art radiation-hard OBC designs\nand considerably lower software- and hardware development costs. This approach\nwas developed for a 4-year European Space Agency (ESA) project, and we are\nimplementing a tiled MPSoC prototype jointly with two industrial partners.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 09:31:28 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Fuchs", "Christian M.", ""], ["Stefanov", "Todor", ""], ["Murillo", "Nadia", ""], ["Plaat", "Aske", ""]]}, {"id": "1708.06947", "submitter": "Mor Perry", "authors": "Boaz Patt-Shamir and Mor Perry", "title": "Proof-Labeling Schemes: Broadcast, Unicast and In Between", "comments": "Full version of paper to be presented at the 19th International\n  Symposium on Stabilization, Safety, and Security of Distributed Systems (SSS\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of limiting the number of different messages a node can\ntransmit simultaneously on the verification complexity of proof-labeling\nschemes (PLS). In a PLS, each node is given a label, and the goal is to verify,\nby exchanging messages over each link in each direction, that a certain global\npredicate is satisfied by the system configuration. We consider a single\nparameter r that bounds the number of distinct messages that can be sent\nconcurrently by any node: in the case r=1, each node may only send the same\nmessage to all its neighbors (the broadcast model), in the case r is at least\nDelta, where Delta is the largest node degree in the system, each neighbor may\nbe sent a distinct message (the unicast model), and in general, for r between 1\nand Delta, each of the r messages is destined to a subset of the neighbors.\n  We show that message compression linear in r is possible for verifying\nfundamental problems such as the agreement between edge endpoints on the edge\nstate. Some problems, including verification of maximal matching, exhibit a\nlarge gap in complexity between r=1 and r>1. For some other important\npredicates, the verification complexity is insensitive to r, e.g., the question\nwhether a subset of edges constitutes a spanning-tree. We also consider the\ncongested clique model. We show that the crossing technique for proving lower\nbounds on the verification complexity can be applied in the case of congested\nclique only if r=1. Together with a new upper bound, this allows us to\ndetermine the verification complexity of MST in the broadcast clique. Finally,\nwe establish a general connection between the deterministic and randomized\nverification complexity for any given number r.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 10:31:29 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Patt-Shamir", "Boaz", ""], ["Perry", "Mor", ""]]}, {"id": "1708.07074", "submitter": "Sabeur Aridhi", "authors": "Sabeur Aridhi (1), Seyed Ziaeddin Alborzi (1), Malika Sma\\\"il-Tabbone\n  (2), Marie-Dominique Devignes (1), David Ritchie (1) ((1) CAPSID, (2)\n  ORPAILLEUR)", "title": "Neighborhood-Based Label Propagation in Large Protein Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding protein function is one of the keys to understanding life at\nthe molecular level. It is also important in several scenarios including human\ndisease and drug discovery. In this age of rapid and affordable biological\nsequencing, the number of sequences accumulating in databases is rising with an\nincreasing rate. This presents many challenges for biologists and computer\nscientists alike. In order to make sense of this huge quantity of data, these\nsequences should be annotated with functional properties. UniProtKB consists of\ntwo components: i) the UniProtKB/Swiss-Prot database containing protein\nsequences with reliable information manually reviewed by expert bio-curators\nand ii) the UniProtKB/TrEMBL database that is used for storing and processing\nthe unknown sequences. Hence, for all proteins we have available the sequence\nalong with few more information such as the taxon and some structural domains.\nPairwise similarity can be defined and computed on proteins based on such\nattributes. Other important attributes, while present for proteins in\nSwiss-Prot, are often missing for proteins in TrEMBL, such as their function\nand cellular localization. The enormous number of protein sequences now in\nTrEMBL calls for rapid procedures to annotate them automatically. In this work,\nwe present DistNBLP, a novel Distributed Neighborhood-Based Label Propagation\napproach for large-scale annotation of proteins. To do this, the functional\nannotations of reviewed proteins are used to predict those of non-reviewed\nproteins using label propagation on a graph representation of the protein\ndatabase. DistNBLP is built on top of the \"akka\" toolkit for building resilient\ndistributed message-driven applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 12:06:15 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Aridhi", "Sabeur", ""], ["Alborzi", "Seyed Ziaeddin", ""], ["Sma\u00efl-Tabbone", "Malika", ""], ["Devignes", "Marie-Dominique", ""], ["Ritchie", "David", ""]]}, {"id": "1708.07233", "submitter": "EPTCS", "authors": "Ian Cassar (Reykjavik University), Adrian Francalanza (University of\n  Malta), Claudio Antares Mezzina (IMT School for Advanced Studies Lucca,\n  Italy), Emilio Tuosto (University of Leicester, UK)", "title": "Reliability and Fault-Tolerance by Choreographic Design", "comments": "In Proceedings PrePost 2017, arXiv:1708.06889", "journal-ref": "EPTCS 254, 2017, pp. 69-80", "doi": "10.4204/EPTCS.254.6", "report-no": null, "categories": "cs.PL cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed programs are hard to get right because they are required to be\nopen, scalable, long-running, and tolerant to faults. In particular, the recent\napproaches to distributed software based on (micro-)services where different\nservices are developed independently by disparate teams exacerbate the problem.\nIn fact, services are meant to be composed together and run in open context\nwhere unpredictable behaviours can emerge. This makes it necessary to adopt\nsuitable strategies for monitoring the execution and incorporate recovery and\nadaptation mechanisms so to make distributed programs more flexible and robust.\nThe typical approach that is currently adopted is to embed such mechanisms in\nthe program logic, which makes it hard to extract, compare and debug. We\npropose an approach that employs formal abstractions for specifying failure\nrecovery and adaptation strategies. Although implementation agnostic, these\nabstractions would be amenable to algorithmic synthesis of code, monitoring and\ntests. We consider message-passing programs (a la Erlang, Go, or MPI) that are\ngaining momentum both in academia and industry. Our research agenda consists of\n(1) the definition of formal behavioural models encompassing failures, (2) the\nspecification of the relevant properties of adaptation and recovery strategy,\n(3) the automatic generation of monitoring, recovery, and adaptation logic in\ntarget languages of interest.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 00:39:55 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Cassar", "Ian", "", "Reykjavik University"], ["Francalanza", "Adrian", "", "University of\n  Malta"], ["Mezzina", "Claudio Antares", "", "IMT School for Advanced Studies Lucca,\n  Italy"], ["Tuosto", "Emilio", "", "University of Leicester, UK"]]}, {"id": "1708.07290", "submitter": "Hasanuzzaman Bhuiyan", "authors": "Hasanuzzaman Bhuiyan, Maleq Khan, and Madhav Marathe", "title": "A Parallel Algorithm for Generating a Random Graph with a Prescribed\n  Degree Sequence", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graphs (or networks) have gained a significant increase of interest\ndue to its popularity in modeling and simulating many complex real-world\nsystems. Degree sequence is one of the most important aspects of these systems.\nRandom graphs with a given degree sequence can capture many characteristics\nlike dependent edges and non-binomial degree distribution that are absent in\nmany classical random graph models such as the Erd\\H{o}s-R\\'{e}nyi graph model.\nIn addition, they have important applications in the uniform sampling of random\ngraphs, counting the number of graphs having the same degree sequence, as well\nas in string theory, random matrix theory, and matching theory. In this paper,\nwe present an OpenMP-based shared-memory parallel algorithm for generating a\nrandom graph with a prescribed degree sequence, which achieves a speedup of\n20.5 with 32 cores. One of the steps in our parallel algorithm requires\nchecking the Erd\\H{o}s-Gallai characterization, i.e., whether there exists a\ngraph obeying the given degree sequence, in parallel. This paper presents the\nfirst non-trivial parallel algorithm for checking the Erd\\H{o}s-Gallai\ncharacterization, which achieves a speedup of 23 using 32 cores.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 06:35:12 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 01:11:43 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Bhuiyan", "Hasanuzzaman", ""], ["Khan", "Maleq", ""], ["Marathe", "Madhav", ""]]}, {"id": "1708.07401", "submitter": "Grey Ballard", "authors": "Grey Ballard and Nicholas Knight and Kathryn Rouse", "title": "Communication Lower Bounds for Matricized Tensor Times Khatri-Rao\n  Product", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matricized-tensor times Khatri-Rao product computation is the typical\nbottleneck in algorithms for computing a CP decomposition of a tensor. In order\nto develop high performance sequential and parallel algorithms, we establish\ncommunication lower bounds that identify how much data movement is required for\nthis computation in the case of dense tensors. We also present sequential and\nparallel algorithms that attain the lower bounds and are therefore\ncommunication optimal. In particular, we show that the structure of the\ncomputation allows for less communication than the straightforward approach of\ncasting the computation as a matrix multiplication operation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 13:33:46 GMT"}, {"version": "v2", "created": "Sun, 22 Oct 2017 16:32:19 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Ballard", "Grey", ""], ["Knight", "Nicholas", ""], ["Rouse", "Kathryn", ""]]}, {"id": "1708.07422", "submitter": "Saurabh Hukerikar", "authors": "Saurabh Hukerikar and Christian Engelmann", "title": "Resilience Design Patterns: A Structured Approach to Resilience at\n  Extreme Scale", "comments": "Supercomputing Frontiers and Innovations. arXiv admin note: text\n  overlap with arXiv:1611.02717", "journal-ref": null, "doi": "10.14529/jsfi170301", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability is a serious concern for future extreme-scale high-performance\ncomputing (HPC) systems. While the HPC community has developed various\nresilience solutions, the solution space remains fragmented. There are no\nformal methods and metrics to integrate the various HPC resilience techniques\ninto composite solutions, nor are there methods to holistically evaluate the\nadequacy and efficacy of such solutions in terms of their protection coverage,\nand their performance & power efficiency characteristics. In this paper, we\ndevelop a structured approach to the design, evaluation and optimization of HPC\nresilience using the concept of design patterns. A design pattern is a general\nrepeatable solution to a commonly occurring problem. We identify the problems\ncaused by various types of faults, errors and failures in HPC systems and the\ntechniques used to deal with these events. Each well-known solution that\naddresses a specific HPC resilience challenge is described in the form of a\npattern. We develop a complete catalog of such resilience design patterns,\nwhich may be used as essential building blocks when designing and deploying\nresilience solutions. We also develop a design framework that enhances a\ndesigner's understanding the opportunities for integrating multiple patterns\nacross layers of the system stack and the important constraints during\nimplementation of the individual patterns. It is also useful for defining\nmechanisms and interfaces to coordinate flexible fault management across\nhardware and software components. The overall goal of this work is to establish\na systematic methodology for the design and evaluation of resilience\ntechnologies in extreme-scale HPC systems that keep scientific applications\nrunning to a correct solution in a timely and cost-efficient manner despite\nfrequent faults, errors, and failures of various types.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 04:21:22 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Hukerikar", "Saurabh", ""], ["Engelmann", "Christian", ""]]}, {"id": "1708.07481", "submitter": "Andrew Knyazev", "authors": "David Zhuzhunashvili and Andrew Knyazev", "title": "Preconditioned Spectral Clustering for Stochastic Block Partition\n  Streaming Graph Challenge", "comments": "6 pages. To appear in Proceedings of the 2017 IEEE High Performance\n  Extreme Computing Conference. Student Innovation Award Streaming Graph\n  Challenge: Stochastic Block Partition, see\n  http://graphchallenge.mit.edu/champions", "journal-ref": "2017 IEEE High Performance Extreme Computing Conference (HPEC),\n  Waltham, MA, USA, 2017, pp. 1-6", "doi": "10.1109/HPEC.2017.8091045", "report-no": null, "categories": "cs.MS cs.DC cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is\ndemonstrated to efficiently solve eigenvalue problems for graph Laplacians that\nappear in spectral clustering. For static graph partitioning, 10-20 iterations\nof LOBPCG without preconditioning result in ~10x error reduction, enough to\nachieve 100% correctness for all Challenge datasets with known truth\npartitions, e.g., for graphs with 5K/.1M (50K/1M) Vertices/Edges in 2 (7)\nseconds, compared to over 5,000 (30,000) seconds needed by the baseline Python\ncode. Our Python code 100% correctly determines 98 (160) clusters from the\nChallenge static graphs with 0.5M (2M) vertices in 270 (1,700) seconds using\n10GB (50GB) of memory. Our single-precision MATLAB code calculates the same\nclusters at half time and memory. For streaming graph partitioning, LOBPCG is\ninitiated with approximate eigenvectors of the graph Laplacian already computed\nfor the previous graph, in many cases reducing 2-3 times the number of required\nLOBPCG iterations, compared to the static case. Our spectral clustering is\ngeneric, i.e. assuming nothing specific of the block model or streaming, used\nto generate the graphs for the Challenge, in contrast to the base code.\nNevertheless, in 10-stage streaming comparison with the base code for the 5K\ngraph, the quality of our clusters is similar or better starting at stage 4 (7)\nfor emerging edging (snowballing) streaming, while the computations are over\n100-1000 faster.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:09:05 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Zhuzhunashvili", "David", ""], ["Knyazev", "Andrew", ""]]}, {"id": "1708.07575", "submitter": "Srivatsan Ravi Mr", "authors": "Miguel Pires, Srivatsan Ravi and Rodrigo Rodrigues", "title": "Generalized Paxos Made Byzantine (and Less Complex)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most recent members of the Paxos family of protocols is\nGeneralized Paxos. This variant of Paxos has the characteristic that it departs\nfrom the original specification of consensus, allowing for a weaker safety\ncondition where different processes can have a different views on a sequence\nbeing agreed upon. However, much like the original Paxos counterpart,\nGeneralized Paxos does not have a simple implementation. Furthermore, with the\nrecent practical adoption of Byzantine fault tolerant protocols, it is timely\nand important to understand how Generalized Paxos can be implemented in the\nByzantine model. In this paper, we make two main contributions. First, we\nprovide a description of Generalized Paxos that is easier to understand, based\non a simpler specification and the pseudocode for a solution that can be\nreadily implemented. Second, we extend the protocol to the Byzantine fault\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 23:22:05 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 23:17:51 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 19:03:01 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Pires", "Miguel", ""], ["Ravi", "Srivatsan", ""], ["Rodrigues", "Rodrigo", ""]]}, {"id": "1708.07883", "submitter": "Edward Kao", "authors": "Edward Kao, Vijay Gadepally, Michael Hurley, Michael Jones, Jeremy\n  Kepner, Sanjeev Mohindra, Paul Monticciolo, Albert Reuther, Siddharth Samsi,\n  William Song, Diane Staheli, Steven Smith", "title": "Streaming Graph Challenge: Stochastic Block Partition", "comments": "To be published in 2017 IEEE High Performance Extreme Computing\n  Conference (HPEC)", "journal-ref": null, "doi": "10.1109/HPEC.2017.8091040", "report-no": null, "categories": "cs.DC cs.DS cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important objective for analyzing real-world graphs is to achieve scalable\nperformance on large, streaming graphs. A challenging and relevant example is\nthe graph partition problem. As a combinatorial problem, graph partition is\nNP-hard, but existing relaxation methods provide reasonable approximate\nsolutions that can be scaled for large graphs. Competitive benchmarks and\nchallenges have proven to be an effective means to advance state-of-the-art\nperformance and foster community collaboration. This paper describes a graph\npartition challenge with a baseline partition algorithm of sub-quadratic\ncomplexity. The algorithm employs rigorous Bayesian inferential methods based\non a statistical model that captures characteristics of the real-world graphs.\nThis strong foundation enables the algorithm to address limitations of\nwell-known graph partition approaches such as modularity maximization. This\npaper describes various aspects of the challenge including: (1) the data sets\nand streaming graph generator, (2) the baseline partition algorithm with\npseudocode, (3) an argument for the correctness of parallelizing the Bayesian\ninference, (4) different parallel computation strategies such as node-based\nparallelism and matrix-based parallelism, (5) evaluation metrics for partition\ncorrectness and computational requirements, (6) preliminary timing of a\nPython-based demonstration code and the open source C++ code, and (7)\nconsiderations for partitioning the graph in streaming fashion. Data sets and\nsource code for the algorithm as well as metrics, with detailed documentation\nare available at GraphChallenge.org.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 21:10:06 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Kao", "Edward", ""], ["Gadepally", "Vijay", ""], ["Hurley", "Michael", ""], ["Jones", "Michael", ""], ["Kepner", "Jeremy", ""], ["Mohindra", "Sanjeev", ""], ["Monticciolo", "Paul", ""], ["Reuther", "Albert", ""], ["Samsi", "Siddharth", ""], ["Song", "William", ""], ["Staheli", "Diane", ""], ["Smith", "Steven", ""]]}, {"id": "1708.07941", "submitter": "Yinghao Yu", "authors": "Yinghao Yu, Wei Wang, Jun Zhang, Khaled B. Letaief", "title": "LERC: Coordinated Cache Management for Data-Parallel Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory caches are being aggressively used in today's data-parallel frameworks\nsuch as Spark, Tez and Storm. By caching input and intermediate data in memory,\ncompute tasks can witness speedup by orders of magnitude. To maximize the\nchance of in-memory data access, existing cache algorithms, be it recency- or\nfrequency-based, settle on cache hit ratio as the optimization objective.\nHowever, unlike the conventional belief, we show in this paper that simply\npursuing a higher cache hit ratio of individual data blocks does not\nnecessarily translate into faster task completion in data-parallel\nenvironments. A data-parallel task typically depends on multiple input data\nblocks. Unless all of these blocks are cached in memory, no speedup will\nresult. To capture this all-or-nothing property, we propose a more relevant\nmetric, called effective cache hit ratio. Specifically, a cache hit of a data\nblock is said to be effective if it can speed up a compute task. In order to\noptimize the effective cache hit ratio, we propose the Least Effective\nReference Count (LERC) policy that persists the dependent blocks of a compute\ntask as a whole in memory. We have implemented the LERC policy as a memory\nmanager in Spark and evaluated its performance through Amazon EC2 deployment.\nEvaluation results demonstrate that LERC helps speed up data-parallel jobs by\nup to 37% compared with the widely employed least-recently-used (LRU) policy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 07:03:01 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yu", "Yinghao", ""], ["Wang", "Wei", ""], ["Zhang", "Jun", ""], ["Letaief", "Khaled B.", ""]]}, {"id": "1708.07985", "submitter": "Alessio Arleo", "authors": "Alessio Arleo, Walter Didimo, Giuseppe Liotta, Fabrizio Montecchiani", "title": "GiViP: A Visual Profiler for Distributed Graph Processing Systems", "comments": "Appears in the Proceedings of the 25th International Symposium on\n  Graph Drawing and Network Visualization (GD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large-scale graphs provides valuable insights in different\napplication scenarios. While many graph processing systems working on top of\ndistributed infrastructures have been proposed to deal with big graphs, the\ntasks of profiling and debugging their massive computations remain time\nconsuming and error-prone. This paper presents GiViP, a visual profiler for\ndistributed graph processing systems based on a Pregel-like computation model.\nGiViP captures the huge amount of messages exchanged throughout a computation\nand provides an interactive user interface for the visual analysis of the\ncollected data. We show how to take advantage of GiViP to detect anomalies\nrelated to the computation and to the infrastructure, such as slow computing\nunits and anomalous message patterns.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 15:30:03 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 15:28:25 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Arleo", "Alessio", ""], ["Didimo", "Walter", ""], ["Liotta", "Giuseppe", ""], ["Montecchiani", "Fabrizio", ""]]}, {"id": "1708.08028", "submitter": "Vatche Ishakian", "authors": "Geoffrey C. Fox, Vatche Ishakian, Vinod Muthusamy, Aleksander\n  Slominski", "title": "Status of Serverless Computing and Function-as-a-Service(FaaS) in\n  Industry and Research", "comments": "Technical Report", "journal-ref": null, "doi": "10.13140/RG.2.2.15007.87206", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This whitepaper summarizes issues raised during the First International\nWorkshop on Serverless Computing (WoSC) 2017 held June 5th 2017 and especially\nin the panel and associated discussion that concluded the workshop. We also\ninclude comments from the keynote and submitted papers. A glossary at the end\n(section 8) defines many technical terms used in this report.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 00:51:59 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Fox", "Geoffrey C.", ""], ["Ishakian", "Vatche", ""], ["Muthusamy", "Vinod", ""], ["Slominski", "Aleksander", ""]]}, {"id": "1708.08155", "submitter": "Waheed Bajwa", "authors": "Zhixiong Yang and Waheed U. Bajwa", "title": "ByRDiE: Byzantine-resilient distributed coordinate descent for\n  decentralized learning", "comments": "Preprint of a paper accepted into IEEE Transactions on Signal and\n  Information Processing Over Networks; 16 pages, 5 figures, and 1 table", "journal-ref": "IEEE Trans. Signal Inform. Proc. over Netw., vol. 5, no. 4, pp.\n  611-627, Dec. 2019", "doi": "10.1109/TSIPN.2019.2928176", "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning algorithms enable learning of models from\ndatasets that are distributed over a network without gathering the data at a\ncentralized location. While efficient distributed algorithms have been\ndeveloped under the assumption of faultless networks, failures that can render\nthese algorithms nonfunctional occur frequently in the real world. This paper\nfocuses on the problem of Byzantine failures, which are the hardest to\nsafeguard against in distributed algorithms. While Byzantine fault tolerance\nhas a rich history, existing work does not translate into efficient and\npractical algorithms for high-dimensional learning in fully distributed (also\nknown as decentralized) settings. In this paper, an algorithm termed\nByzantine-resilient distributed coordinate descent (ByRDiE) is developed and\nanalyzed that enables distributed learning in the presence of Byzantine\nfailures. Theoretical analysis (convex settings) and numerical experiments\n(convex and nonconvex settings) highlight its usefulness for high-dimensional\ndistributed learning in the presence of Byzantine failures.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 00:22:18 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 20:29:11 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 15:08:58 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 17:00:42 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yang", "Zhixiong", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1708.08255", "submitter": "Fabrizio Luccio", "authors": "Fabrizio Luccio, Linda Pagli", "title": "Cops and robber on grids and tori", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a contribution to the classical cops and robber problem on a\ngraph, directed to two-dimensional grids and toroidal grids. These studies are\ngenerally aimed at determining the minimum number of cops needed to capture the\nrobber and proposing algorithms for the capture. We apply some new concepts to\npropose a new solution to the problem on grids that was already solved under a\ndifferent approach, and apply these concepts to give efficient algorithms for\nthe capture on toroidal grids. As for grids, we show that two cops suffice even\nin a semi-torus (i.e. a grid with toroidal closure in one dimension) and three\ncops are necessary and sufficient in a torus. Then we treat the problem in\nfunction of any number k of cops, giving efficient algorithms for grids and\ntori and computing lower and upper bounds on the capture time. Conversely we\ndetermine the minimum value of k needed for any given capture time and study a\npossible speed-up phenomenon.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 09:48:22 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 13:25:10 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Luccio", "Fabrizio", ""], ["Pagli", "Linda", ""]]}, {"id": "1708.08286", "submitter": "Nils Kohl", "authors": "Nils Kohl, Johannes H\\\"otzer, Florian Schornbaum, Martin Bauer,\n  Christian Godenschwager, Harald K\\\"ostler, Britta Nestler and Ulrich R\\\"ude", "title": "A Scalable and Extensible Checkpointing Scheme for Massively Parallel\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic simulations in engineering or in the materials sciences can consume\nenormous computing resources and thus require the use of massively parallel\nsupercomputers. The probability of a failure increases both with the runtime\nand with the number of system components. For future exascale systems it is\ntherefore considered critical that strategies are developed to make software\nresilient against failures. In this article, we present a scalable,\ndistributed, diskless, and resilient checkpointing scheme that can create and\nrecover snapshots of a partitioned simulation domain. We demonstrate the\nefficiency and scalability of the checkpoint strategy for simulations with up\nto $40$ billion computational cells executing on more than $400$ billion\nfloating point values. A checkpoint creation is shown to require only a few\nseconds and the new checkpointing scheme scales almost perfectly up to more\nthan $260\\,000$ ($2^{18}$) processes. To recover from a diskless checkpoint\nduring runtime, we realize the recovery algorithms using ULFM MPI. The\ncheckpointing mechanism is fully integrated in a state-of-the-art\nhigh-performance multi-physics simulation framework. We demonstrate the\nefficiency and robustness of the method with a realistic phase-field simulation\noriginating in the material sciences and with a lattice Boltzmann method\nimplementation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:26:37 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 11:58:57 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kohl", "Nils", ""], ["H\u00f6tzer", "Johannes", ""], ["Schornbaum", "Florian", ""], ["Bauer", "Martin", ""], ["Godenschwager", "Christian", ""], ["K\u00f6stler", "Harald", ""], ["Nestler", "Britta", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1708.08309", "submitter": "Marius Poke", "authors": "Marius Poke and Colin W. Glass", "title": "A Dual Digraph Approach for Leaderless Atomic Broadcast (Extended\n  Version)", "comments": "Overview: 25 pages, 6 sections, 3 appendices, 8 figures, 3 tables.\n  Modifications from previous version (restructuring): Table 1 moved to\n  Appendix B as Table 3; added Section IV; Section III-F moved to Section IV-A;\n  added Section IV-B (as a replacement to Section III-H); Section III-G moved\n  to Section IV-C; Section III-I moved to Section IV-D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many distributed systems work on a common shared state; in such systems,\ndistributed agreement is necessary for consistency. With an increasing number\nof servers, these systems become more susceptible to single-server failures,\nincreasing the relevance of fault-tolerance. Atomic broadcast enables\nfault-tolerant distributed agreement, yet it is costly to solve. Most practical\nalgorithms entail linear work per broadcast message. AllConcur -- a leaderless\napproach -- reduces the work, by connecting the servers via a sparse resilient\noverlay network; yet, this resiliency entails redundancy, limiting the\nreduction of work. In this paper, we propose AllConcur+, an atomic broadcast\nalgorithm that lifts this limitation: During intervals with no failures, it\nachieves minimal work by using a redundancy-free overlay network. When failures\ndo occur, it automatically recovers by switching to a resilient overlay\nnetwork. In our performance evaluation of non-failure scenarios, AllConcur+\nachieves comparable throughput to AllGather -- a non-fault-tolerant distributed\nagreement algorithm -- and outperforms AllConcur, LCR and Libpaxos both in\nterms of throughput and latency. Furthermore, our evaluation of failure\nscenarios shows that AllConcur+'s expected performance is robust with regard to\noccasional failures. Thus, for realistic use cases, leveraging redundancy-free\ndistributed agreement during intervals with no failures improves performance\nsignificantly.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 13:42:18 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 11:42:45 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 09:10:07 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2019 12:55:02 GMT"}, {"version": "v5", "created": "Thu, 12 Dec 2019 10:34:48 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Poke", "Marius", ""], ["Glass", "Colin W.", ""]]}, {"id": "1708.08399", "submitter": "Mudit Verma", "authors": "Mudit Verma (IBM Research - India), Mohan Dhawan (IBM Research -\n  India)", "title": "Towards a More Reliable and Available Docker-based Container Cloud", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operating System-level virtualization technology, or containers as they are\ncommonly known, represents the next generation of light-weight virtualization,\nand is primarily represented by Docker. However, Docker's current design does\nnot complement the SLAs from Docker-based container cloud offerings promising\nboth reliability and high availability. The tight coupling between the\ncontainers and the Docker daemon proves fatal for the containers' uptime during\ndaemon's unavailability due to either failure or upgrade. We present the design\nand implementation of HYDRA, which fundamentally isolates the containers from\nthe running daemon. Our evaluation shows that HYDRA imposes only moderate\noverheads even under load, while achieving much higher container availability.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 16:29:13 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Verma", "Mudit", "", "IBM Research - India"], ["Dhawan", "Mohan", "", "IBM Research -\n  India"]]}, {"id": "1708.08435", "submitter": "Prajakta Kalmegh", "authors": "Prajakta Kalmegh, Shivnath Babu, Sudeepa Roy", "title": "Analyzing Query Performance and Attributing Blame for Contentions in a\n  Cluster Computing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many approaches is use today to either prevent or minimize the\nimpact of inter-query interactions on a shared cluster. Despite these measures,\nperformance issues due to concurrent executions of mixed workloads still\nprevail causing undue waiting times for queries. Analyzing these resource\ninterferences is thus critical in order to answer time sensitive questions like\n'who is causing my query to slowdown' in a multi-tenant environment. More\nimportantly, dignosing whether the slowdown of a query is a result of resource\ncontentions caused by other queries or some other external factor can help an\nadmin narrow down the many possibilities of performance degradation. This\nprocess of investigating the symptoms of resource contentions and attributing\nblame to concurrent queries is non-trivial and tedious, and involves hours of\nmanually debugging through a cycle of query interactions.\n  In this paper, we present ProtoXplore - a Proto or first system to eXplore\ncontentions, that helps administrators determine whether the blame for resource\nbottlenecks can be attributed to concurrent queries, and uses a methodology\ncalled Resource Acquire Time Penalty (RATP) to quantify this blame towards\ncontentious sources accurately. Further, ProtoXplore builds on the theory of\nexplanations and enables a step-wise deep exploration of various levels of\nperformance bottlenecks faced by a query during its execution using a\nmulti-level directed acyclic graph called ProtoGraph. Our experimental\nevaluation uses ProtoXplore to analyze the interactions between TPC-DS queries\non Apache Spark to show how ProtoXplore provides explanations that help in\ndiagnosing contention related issues and better managing a changing mixed\nworkload in a shared cluster.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 17:44:44 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 22:56:59 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Kalmegh", "Prajakta", ""], ["Babu", "Shivnath", ""], ["Roy", "Sudeepa", ""]]}, {"id": "1708.08670", "submitter": "Yuri G. Gordienko", "authors": "Yuriy Kochura, Sergii Stirenko, Oleg Alienin, Michail Novotarskiy, and\n  Yuri Gordienko", "title": "Performance Analysis of Open Source Machine Learning Frameworks for\n  Various Parameters in Single-Threaded and Multi-Threaded Modes", "comments": "15 pages, 11 figures, 4 tables; this paper summarizes the activities\n  which were started recently and described shortly in the previous conference\n  presentations arXiv:1706.02248 and arXiv:1707.04940; it is accepted for\n  Springer book series \"Advances in Intelligent Systems and Computing\"", "journal-ref": "Advances in Intelligent Systems and Computing II. CSIT 2017.\n  Advances in Intelligent Systems and Computing, vol 689, pp 243-256. Springer,\n  Cham", "doi": "10.1007/978-3-319-70581-1_17", "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic features of some of the most versatile and popular open source\nframeworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are\nconsidered and compared. Their comparative analysis was performed and\nconclusions were made as to the advantages and disadvantages of these\nplatforms. The performance tests for the de facto standard MNIST data set were\ncarried out on H2O framework for deep learning algorithms designed for CPU and\nGPU platforms for single-threaded and multithreaded modes of operation Also, we\npresent the results of testing neural networks architectures on H2O platform\nfor various activation functions, stopping metrics, and other parameters of\nmachine learning algorithm. It was demonstrated for the use case of MNIST\ndatabase of handwritten digits in single-threaded mode that blind selection of\nthese parameters can hugely increase (by 2-3 orders) the runtime without the\nsignificant increase of precision. This result can have crucial influence for\noptimization of available and new machine learning methods, especially for\nimage recognition problems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 09:54:28 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Kochura", "Yuriy", ""], ["Stirenko", "Sergii", ""], ["Alienin", "Oleg", ""], ["Novotarskiy", "Michail", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1708.08810", "submitter": "Suzhi Bi", "authors": "Suzhi Bi and Ying-Jun Angela Zhang", "title": "Computation Rate Maximization for Wireless Powered Mobile-Edge Computing\n  with Binary Computation Offloading", "comments": "This paper has been accepted for publication in IEEE Transactions on\n  Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a multi-user mobile edge computing (MEC) network\npowered by wireless power transfer (WPT), where each energy-harvesting WD\nfollows a binary computation offloading policy, i.e., data set of a task has to\nbe executed as a whole either locally or remotely at the MEC server via task\noffloading. In particular, we are interested in maximizing the (weighted) sum\ncomputation rate of all the WDs in the network by jointly optimizing the\nindividual computing mode selection (i.e., local computing or offloading) and\nthe system transmission time allocation (on WPT and task offloading). The major\ndifficulty lies in the combinatorial nature of multi-user computing mode\nselection and its strong coupling with transmission time allocation. To tackle\nthis problem, we first consider a decoupled optimization, where we assume that\nthe mode selection is given and propose a simple bi-section search algorithm to\nobtain the conditional optimal time allocation. On top of that, a coordinate\ndescent method is devised to optimize the mode selection. The method is simple\nin implementation but may suffer from high computational complexity in a\nlarge-size network. To address this problem, we further propose a joint\noptimization method based on the ADMM (alternating direction method of\nmultipliers) decomposition technique, which enjoys much slower increase of\ncomputational complexity as the networks size increases. Extensive simulations\nshow that both the proposed methods can efficiently achieve near-optimal\nperformance under various network setups, and significantly outperform the\nother representative benchmark methods considered.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 15:04:19 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 02:39:46 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 04:45:33 GMT"}, {"version": "v4", "created": "Fri, 23 Mar 2018 00:50:44 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Bi", "Suzhi", ""], ["Zhang", "Ying-Jun Angela", ""]]}, {"id": "1708.08976", "submitter": "Koby Hayashi", "authors": "Koby Hayashi, Grey Ballard, Jeffrey Jiang, Michael Tobia", "title": "Shared Memory Parallelization of MTTKRP for Dense Tensors", "comments": "10 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matricized-tensor times Khatri-Rao product (MTTKRP) is the computational\nbottleneck for algorithms computing CP decompositions of tensors. In this\npaper, we develop shared-memory parallel algorithms for MTTKRP involving dense\ntensors. The algorithms cast nearly all of the computation as matrix operations\nin order to use optimized BLAS subroutines, and they avoid reordering tensor\nentries in memory. We benchmark sequential and parallel performance of our\nimplementations, demonstrating high sequential performance and efficient\nparallel scaling. We use our parallel implementation to compute a CP\ndecomposition of a neuroimaging data set and achieve a speedup of up to\n$7.4\\times$ over existing parallel software.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 18:59:46 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Hayashi", "Koby", ""], ["Ballard", "Grey", ""], ["Jiang", "Jeffrey", ""], ["Tobia", "Michael", ""]]}, {"id": "1708.09014", "submitter": "Naghmeh Ivaki", "authors": "Naghmeh Ivaki", "title": "13th European Dependable Computing Conference (EDCC 2017): Fast\n  Abstracts and Student Forum Proceedings", "comments": "13th European Dependable Computing Conference, Geneva, Switzerland,\n  4-8 September 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fast Abstracts are short presentations of work in progress or opinion pieces\nand aim to serve as a rapid and flexible mechanism to (i) Report on current\nwork that may or may not be complete; (ii) Introduce new ideas to the\ncommunity; (iii) State positions on controversial issues or open problems.\nStudent Forum is a vibrant and friendly environment where students can present\ntheir work, exchange ideas and experiences, get feedback on their work, get new\ninspirations and points of view. In addition, the forum stimulates interaction\nbetween young researchers, experienced researchers, and industry.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 19:30:00 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Ivaki", "Naghmeh", ""]]}, {"id": "1708.09135", "submitter": "Suzhen Wang", "authors": "Suzhen Wang, Jingjing Luo, Bruce Kwong-Bun Tong, Wing S. Wong", "title": "Randomized Load-balanced Routing for Fat-tree Networks", "comments": "13 pages, 1 table, 6 figure,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fat-tree networks have been widely adopted to High Performance Computing\n(HPC) clusters and to Data Center Networks (DCN). These parallel systems\nusually have a large number of servers and hosts, which generate large volumes\nof highly-volatile traffic. Thus, distributed load-balancing routing design\nbecomes critical to achieve high bandwidth utilization, and low-latency packet\ndelivery. Existing distributed designs rely on remote congestion feedbacks to\naddress congestion, which add overheads to collect and react to network-wide\ncongestion information. In contrast, we propose a simple but effective\nload-balancing scheme, called Dynamic Randomized load-Balancing (DRB), to\nachieve network-wide low levels of path collisions through local-link\nadjustment which is free of communications and cooperations between switches.\nFirst, we use D-mod-k path selection scheme to allocate default paths to all\nsource-destination (S-D) pairs in a fat-tree network, guaranteeing low levels\nof path collision over downlinks for any set of active S-D pairs. Then, we\npropose Threshold-based Two-Choice (TTC) randomized technique to balance uplink\ntraffic through local uplink adjustment at each switch. We theoretically show\nthat the proposed TTC for the uplink-load balancing in a fat-tree network have\na similar performance as the two-choice technique in the area of randomized\nload balancing. Simulation results show that DRB with TTC technique achieves a\nsignificant improvement over many randomized routing schemes for fat-tree\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 06:35:17 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Wang", "Suzhen", ""], ["Luo", "Jingjing", ""], ["Tong", "Bruce Kwong-Bun", ""], ["Wong", "Wing S.", ""]]}, {"id": "1708.09332", "submitter": "Doina Cosovan", "authors": "Sinica Alboaie, Doina Cosovan", "title": "Private Data System Enabling Self-Sovereign Storage Managed by\n  Executable Choreographies", "comments": "DAIS 2017", "journal-ref": "IFIP International Conference on Distributed Applications and\n  Interoperable Systems, DAIS 2017: Distributed Applications and Interoperable\n  Systems pp 83-98", "doi": "10.1007/978-3-319-59665-5_6", "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased use of Internet, governments and large companies store and\nshare massive amounts of personal data in such a way that leaves no space for\ntransparency. When a user needs to achieve a simple task like applying for\ncollege or a driving license, he needs to visit a lot of institutions and\norganizations, thus leaving a lot of private data in many places. The same\nhappens when using the Internet. These privacy issues raised by the centralized\narchitectures along with the recent developments in the area of serverless\napplications demand a decentralized private data layer under user control. We\nintroduce the Private Data System (PDS), a distributed approach which enables\nself-sovereign storage and sharing of private data. The system is composed of\nnodes spread across the entire Internet managing local key-value databases. The\ncommunication between nodes is achieved through executable choreographies,\nwhich are capable of preventing information leakage when executing across\ndifferent organizations with different regulations in place. The user has full\ncontrol over his private data and is able to share and revoke access to\norganizations at any time. Even more, the updates are propagated instantly to\nall the parties which have access to the data thanks to the system design.\nSpecifically, the processing organizations may retrieve and process the shared\ninformation, but are not allowed under any circumstances to store it on long\nterm. PDS offers an alternative to systems that aim to ensure self-sovereignty\nof specific types of data through blockchain inspired techniques but face\nvarious problems, such as low performance. Both approaches propose a\ndistributed database, but with different characteristics. While the\nblockchain-based systems are built to solve consensus problems, PDS's purpose\nis to solve the self-sovereignty aspects raised by the privacy laws, rules and\nprinciples.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 15:00:05 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Alboaie", "Sinica", ""], ["Cosovan", "Doina", ""]]}, {"id": "1708.09419", "submitter": "Carlos G. Oliver Mr", "authors": "Carlos G. Oliver, Alessandro Ricottone, Pericles Philippopoulos", "title": "Proposal for a fully decentralized blockchain and proof-of-work\n  algorithm for solving NP-complete problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a proof-of-work algorithm that rewards blockchain miners for using\ncomputational resources to solve NP-complete puzzles. The resulting blockchain\nwill publicly store and improve solutions to problems with real world\napplications while maintaining a secure and fully functional transaction\nledger.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 18:25:32 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 16:54:21 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Oliver", "Carlos G.", ""], ["Ricottone", "Alessandro", ""], ["Philippopoulos", "Pericles", ""]]}, {"id": "1708.09495", "submitter": "Alexandros Gerbessiotis", "authors": "Alexandros V. Gerbessiotis", "title": "Integer sorting on multicores: some (experiments and) observations", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many proposals for sorting integers on multicores/GPUs that\ninclude radix-sort and its variants or other approaches that exploit\nspecialized hardware features of a particular multicore architecture.\nComparison-based algorithms have also been used. Network-based algorithms have\nalso been used with primary example Batcher's bitonic sorting algorithm.\nAlthough such a latter approach is theoretically \"inefficient\", if there are\nfew keys to sort, it can lead to better running times as it has low overhead\nand is simple to implement.\n  In this work we perform an experimental study of integer sorting on multicore\nprocessors using not only multithreading but also multiprocessing parallel\nprogramming approaches. Our implementations work under Open MPI, MulticoreBSP,\nand BSPlib. We have implemented serial and parallel radix-sort for various\nradixes and also some previously little explored or unexplored variants of\nbitonic-sort and odd-even transposition sort.\n  We offer our observations on a performance evaluation using the MBSP model of\nsuch algorithm implementations on multiple platforms and architectures and\nmultiple programming libraries. If we can conclude anything is that modeling\ntheir performance by taking into consideration architecture dependent features\nsuch as the structure and characteristics of multiple memory hierarchies is\ndifficult and more often than not unsuccessful or unreliable. However we can\nstill draw some very simple conclusions using traditional architecture\nindependent parallel modeling.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 22:45:38 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Gerbessiotis", "Alexandros V.", ""]]}, {"id": "1708.09707", "submitter": "Peter Zaspel", "authors": "Peter Zaspel", "title": "Algorithmic patterns for $\\mathcal{H}$-matrices on many-core processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the reformulation of hierarchical ($\\mathcal{H}$)\nmatrix algorithms for many-core processors with a model implementation on\ngraphics processing units (GPUs). $\\mathcal{H}$ matrices approximate specific\ndense matrices, e.g., from discretized integral equations or kernel ridge\nregression, leading to log-linear time complexity in dense matrix-vector\nproducts. The parallelization of $\\mathcal{H}$ matrix operations on many-core\nprocessors is difficult due to the complex nature of the underlying algorithms.\nWhile previous algorithmic advances for many-core hardware focused on\naccelerating existing $\\mathcal{H}$ matrix CPU implementations by many-core\nprocessors, we here aim at totally relying on that processor type. As main\ncontribution, we introduce the necessary parallel algorithmic patterns allowing\nto map the full $\\mathcal{H}$ matrix construction and the fast matrix-vector\nproduct to many-core hardware. Here, crucial ingredients are space filling\ncurves, parallel tree traversal and batching of linear algebra operations. The\nresulting model GPU implementation hmglib is the, to the best of the authors\nknowledge, first entirely GPU-based Open Source $\\mathcal{H}$ matrix library of\nthis kind. We conclude this work by an in-depth performance analysis and a\ncomparative performance study against a standard $\\mathcal{H}$ matrix library,\nhighlighting profound speedups of our many-core parallel approach.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 13:50:42 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Zaspel", "Peter", ""]]}]