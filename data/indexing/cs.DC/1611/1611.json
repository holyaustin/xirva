[{"id": "1611.00323", "submitter": "Alexandru Iosup", "authors": "Alexandru Iosup, Xiaoyun Zhu, Arif Merchant, Eva Kalyvianaki, Martina\n  Maggio, Simon Spinner, Tarek Abdelzaher, Ole Mengshoel, Sara Bouchenak", "title": "Self-Awareness of Cloud Applications", "comments": "Overall: 40 pages, 1 figure, 135 references. Note: This is an\n  extended survey. A much shorter, revised version of this material will be\n  available in print, as part of a Springer book on \"Self-Aware Computing\". The\n  book is due to appear in 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.NI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud applications today deliver an increasingly larger portion of the\nInformation and Communication Technology (ICT) services. To address the scale,\ngrowth, and reliability of cloud applications, self-aware management and\nscheduling are becoming commonplace. How are they used in practice? In this\nchapter, we propose a conceptual framework for analyzing state-of-the-art\nself-awareness approaches used in the context of cloud applications. We map\nimportant applications corresponding to popular and emerging application\ndomains to this conceptual framework, and compare the practical\ncharacteristics, benefits, and drawbacks of self-awareness approaches. Last, we\npropose a roadmap for addressing open challenges in self-aware cloud and\ndatacenter applications.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 18:28:55 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Iosup", "Alexandru", ""], ["Zhu", "Xiaoyun", ""], ["Merchant", "Arif", ""], ["Kalyvianaki", "Eva", ""], ["Maggio", "Martina", ""], ["Spinner", "Simon", ""], ["Abdelzaher", "Tarek", ""], ["Mengshoel", "Ole", ""], ["Bouchenak", "Sara", ""]]}, {"id": "1611.00404", "submitter": "Jalal Khamse-Ashari", "authors": "Jalal Khamse-Ashari, Ioannis Lambadaris, George Kesidis, Bhuvan\n  Urgaonkar and Yiqiang Zhao", "title": "Per-Server Dominant-Share Fairness (PS-DSF): A Multi-Resource Fair\n  Allocation Mechanism for Heterogeneous Servers", "comments": "12 pages, 7 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of cloud computing platforms pose different types of demands for\nmultiple resources on servers (physical or virtual machines). Besides\ndifferences in their resource capacities, servers may be additionally\nheterogeneous in their ability to service users - certain users' tasks may only\nbe serviced by a subset of the servers. We identify important shortcomings in\nexisting multi-resource fair allocation mechanisms - Dominant Resource Fairness\n(DRF) and its follow up work - when used in such environments. We develop a new\nfair allocation mechanism called Per-Server Dominant-Share Fairness (PS-DSF)\nwhich we show offers all desirable sharing properties that DRF is able to offer\nin the case of a single \"resource pool\" (i.e., if the resources of all servers\nwere pooled together into one hypothetical server). We evaluate the performance\nof PS-DSF through simulations. Our evaluation shows the enhanced efficiency of\nPS-DSF compared to the existing allocation mechanisms. We argue how our\nproposed allocation mechanism is applicable in cloud computing networks and\nespecially large scale data-centers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 21:39:06 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 16:20:03 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Khamse-Ashari", "Jalal", ""], ["Lambadaris", "Ioannis", ""], ["Kesidis", "George", ""], ["Urgaonkar", "Bhuvan", ""], ["Zhao", "Yiqiang", ""]]}, {"id": "1611.00463", "submitter": "Zahra Khatami", "authors": "Zahra Khatami, Sungpack Hong, Jinsoo Lee, Siegfried Depner, Hassan\n  Chafi, J. Ramanujam, and Hartmut Kaiser", "title": "A Load-Balanced Parallel and Distributed Sorting Algorithm Implemented\n  with PGX.D", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting has been one of the most challenging studied problems in different\nscientific researches. Although many techniques and algorithms have been\nproposed on the theory of having efficient parallel sorting implementation,\nhowever achieving desired performance on different types of the architectures\nwith large number of processors is still a challenging issue. Maximizing\nparallelism level in applications can be achieved by minimizing overheads due\nto load imbalance and waiting time due to memory latencies. In this paper, we\npresent a distributed sorting algorithm implemented in PGX.D, a fast\ndistributed graph processing system, which outperforms the Spark's distributed\nsorting implementation by around 2x-3x by hiding communication latencies and\nminimizing unnecessary overheads. Furthermore, it shows that the proposed PGX.D\nsorting method handles dataset containing many duplicated data entries\nefficiently and always results in keeping balanced workloads for different\ninput data distribution types.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 03:56:31 GMT"}, {"version": "v2", "created": "Sat, 14 Jan 2017 20:17:32 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Khatami", "Zahra", ""], ["Hong", "Sungpack", ""], ["Lee", "Jinsoo", ""], ["Depner", "Siegfried", ""], ["Chafi", "Hassan", ""], ["Ramanujam", "J.", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "1611.00598", "submitter": "Junseok Park", "authors": "Junseok Park, Gwangmin Kim, Dongjin Jang, Sungji Choo, Sunghwa Bae,\n  Doheon Lee", "title": "A bioinformatics system for searching Co-Occurrence based on\n  Co-Operational Formation with Advanced Method (COCOFAM)", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature analysis is a key step in obtaining background information in\nbiomedical research. However, it is difficult for researchers to obtain\nknowledge of their interests in an efficient manner because of the massive\namount of the published biomedical literature. Therefore, efficient and\nsystematic search strategies are required, which allow ready access to the\nsubstantial amount of literature. In this paper, we propose a novel search\nsystem, named Co-Occurrence based on Co-Operational Formation with Advanced\nMethod(COCOFAM) which is suitable for the large-scale literature analysis.\nCOCOFAM is based on integrating both Spark for local clusters and a global job\nscheduler to gather crowdsourced co-occurrence data on global clusters. It will\nallow users to obtain information of their interests from the substantial\namount of literature.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 13:34:42 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Park", "Junseok", ""], ["Kim", "Gwangmin", ""], ["Jang", "Dongjin", ""], ["Choo", "Sungji", ""], ["Bae", "Sunghwa", ""], ["Lee", "Doheon", ""]]}, {"id": "1611.00606", "submitter": "Diego Fabregat-Traver", "authors": "Diego Fabregat-Traver (1), Davor Davidovi\\'c (2), Markus H\\\"ohnerbach\n  (1), Edoardo Di Napoli (3 and 4) ((1) AICES, RWTH Aachen University, (2) RBI,\n  Zagreb, Croatia, (3) J\\\"ulich Supercomputing Centre, (4) J\\\"ulich Aachen\n  Research Alliance -- High-performance Computing)", "title": "Hybrid CPU-GPU generation of the Hamiltonian and Overlap matrices in\n  FLAPW methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the integration of high-performance numerical\nlibraries in ab initio codes and the portability of performance and\nscalability. The target of our work is FLEUR, a software for electronic\nstructure calculations developed in the Forschungszentrum J\\\"ulich over the\ncourse of two decades. The presented work follows up on a previous effort to\nmodernize legacy code by re-engineering and rewriting it in terms of highly\noptimized libraries. We illustrate how this initial effort to get efficient and\nportable shared-memory code enables fast porting of the code to emerging\nheterogeneous architectures. More specifically, we port the code to nodes\nequipped with multiple GPUs. We divide our study in two parts. First, we show\nconsiderable speedups attained by minor and relatively straightforward code\nchanges to off-load parts of the computation to the GPUs. Then, we identify\nfurther possible improvements to achieve even higher performance and\nscalability. On a system consisting of 16-cores and 2 GPUs, we observe speedups\nof up to 5x with respect to our optimized shared-memory code, which in turn\nmeans between 7.5x and 12.5x speedup with respect to the original FLEUR code.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 13:48:29 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Fabregat-Traver", "Diego", "", "3 and 4"], ["Davidovi\u0107", "Davor", "", "3 and 4"], ["H\u00f6hnerbach", "Markus", "", "3 and 4"], ["Di Napoli", "Edoardo", "", "3 and 4"]]}, {"id": "1611.00714", "submitter": "Alexander Jung", "authors": "Alexander Jung and Alfred O. Hero III and Alexandru Mara and Sabeur\n  Aridhi", "title": "Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable method for semi-supervised (transductive) learning from\nmassive network-structured datasets. Our approach to semi-supervised learning\nis based on representing the underlying hypothesis as a graph signal with small\ntotal variation. Requiring a small total variation of the graph signal\nrepresenting the underlying hypothesis corresponds to the central smoothness\nassumption that forms the basis for semi-supervised learning, i.e., input\npoints forming clusters have similar output values or labels. We formulate the\nlearning problem as a nonsmooth convex optimization problem which we solve by\nappealing to Nesterovs optimal first-order method for nonsmooth optimization.\nWe also provide a message passing formulation of the learning method which\nallows for a highly scalable implementation in big data frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 18:27:53 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Jung", "Alexander", ""], ["Hero", "Alfred O.", "III"], ["Mara", "Alexandru", ""], ["Aridhi", "Sabeur", ""]]}, {"id": "1611.00946", "submitter": "Pablo Basanta-Val", "authors": "Pablo Basanta-Val, Neil Audsley, Andy Wellings (CS-YORK), Ian Gray\n  (CS-YORK), Norberto Fernandez", "title": "Architecting Time-Critical Big-Data Systems", "comments": "in IEEE Transactions on Big Data, 2016", "journal-ref": null, "doi": "10.1109/TBDATA.2016.2622719", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  - Current infrastructures for developing big-data applications are able to\nprocess --via big-data analytics-huge amounts of data, using clusters of\nmachines that collaborate to perform parallel computations. However, current\ninfrastructures were not designed to work with the requirements of\ntime-critical applications; they are more focused on general-purpose\napplications rather than time-critical ones. Addressing this issue from the\nperspective of the real-time systems community, this paper considers\ntime-critical big-data. It deals with the definition of a time-critical\nbig-data system from the point of view of requirements, analyzing the specific\ncharacteristics of some popular big-data applications. This analysis is\ncomplemented by the challenges stemmed from the infrastructures that support\nthe applications, proposing an architecture and offering initial performance\npatterns that connect application costs with infrastructure performance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:39:09 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Basanta-Val", "Pablo", "", "CS-YORK"], ["Audsley", "Neil", "", "CS-YORK"], ["Wellings", "Andy", "", "CS-YORK"], ["Gray", "Ian", "", "CS-YORK"], ["Fernandez", "Norberto", ""]]}, {"id": "1611.01137", "submitter": "Elias Stehle", "authors": "Elias Stehle, Hans-Arno Jacobsen", "title": "A Memory Bandwidth-Efficient Hybrid Radix Sort on GPUs", "comments": "16 pages, accepted at SIGMOD 2017", "journal-ref": "SIGMOD (2017) 417-432", "doi": "10.1145/3035918.3064043", "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is at the core of many database operations, such as index creation,\nsort-merge joins, and user-requested output sorting. As GPUs are emerging as a\npromising platform to accelerate various operations, sorting on GPUs becomes a\nviable endeavour. Over the past few years, several improvements have been\nproposed for sorting on GPUs, leading to the first radix sort implementations\nthat achieve a sorting rate of over one billion 32-bit keys per second. Yet,\nstate-of-the-art approaches are heavily memory bandwidth-bound, as they require\nsubstantially more memory transfers than their CPU-based counterparts.\n  Our work proposes a novel approach that almost halves the amount of memory\ntransfers and, therefore, considerably lifts the memory bandwidth limitation.\nBeing able to sort two gigabytes of eight-byte records in as little as 50\nmilliseconds, our approach achieves a 2.32-fold improvement over the\nstate-of-the-art GPU-based radix sort for uniform distributions, sustaining a\nminimum speed-up of no less than a factor of 1.66 for skewed distributions.\n  To address inputs that either do not reside on the GPU or exceed the\navailable device memory, we build on our efficient GPU sorting approach with a\npipelined heterogeneous sorting algorithm that mitigates the overhead\nassociated with PCIe data transfers. Comparing the end-to-end sorting\nperformance to the state-of-the-art CPU-based radix sort running 16 threads,\nour heterogeneous approach achieves a 2.06-fold and a 1.53-fold improvement for\nsorting 64 GB key-value pairs with a skewed and a uniform distribution,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:33:06 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 12:22:16 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Stehle", "Elias", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1611.01325", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Vittorio Ghini", "title": "Multi-level Simulation of Internet of Things on Smart Territories", "comments": "Simulation Modelling Practice and Theory, Elsevier, vol. 73 (April\n  2017)", "journal-ref": null, "doi": "10.1016/j.simpat.2016.10.008", "report-no": null, "categories": "cs.PF cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a methodology is presented and employed for simulating the\nInternet of Things (IoT). The requirement for scalability, due to the possibly\nhuge amount of involved sensors and devices, and the heterogeneous scenarios\nthat might occur, impose resorting to sophisticated modeling and simulation\ntechniques. In particular, multi-level simulation is regarded as a main\nframework that allows simulating large-scale IoT environments while keeping\nhigh levels of detail, when it is needed. We consider a use case based on the\ndeployment of smart services in decentralized territories. A two level\nsimulator is employed, which is based on a coarse agent-based, adaptive\nparallel and distributed simulation approach to model the general life of\nsimulated entities. However, when needed a finer grained simulator (based on\nOMNeT++) is triggered on a restricted portion of the simulated area, which\nallows considering all issues concerned with wireless communications. Based on\nthis use case, it is confirmed that the ad-hoc wireless networking technologies\ndo represent a principle tool to deploy smart services over decentralized\ncountrysides. Moreover, the performance evaluation confirms the viability of\nutilizing multi-level simulation for simulating large scale IoT environments.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 11:07:53 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 14:04:57 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Ghini", "Vittorio", ""]]}, {"id": "1611.01594", "submitter": "Peng Sun", "authors": "Peng Sun, Yonggang Wen, Ta Nguyen Binh Duong, Haiyong Xie", "title": "MetaFlow: a Scalable Metadata Lookup Service for Distributed File\n  Systems in Data Centers", "comments": "in IEEE Transactions on Big Data 2016", "journal-ref": null, "doi": "10.1109/TBDATA.2016.2612241", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale distributed file systems, efficient meta- data operations are\ncritical since most file operations have to interact with metadata servers\nfirst. In existing distributed hash table (DHT) based metadata management\nsystems, the lookup service could be a performance bottleneck due to its\nsignificant CPU overhead. Our investigations showed that the lookup service\ncould reduce system throughput by up to 70%, and increase system latency by a\nfactor of up to 8 compared to ideal scenarios. In this paper, we present\nMetaFlow, a scalable metadata lookup service utilizing software-defined\nnetworking (SDN) techniques to distribute lookup workload over network\ncomponents. MetaFlow tackles the lookup bottleneck problem by leveraging\nB-tree, which is constructed over the physical topology, to manage flow tables\nfor SDN-enabled switches. Therefore, metadata requests can be forwarded to\nappropriate servers using only switches. Extensive performance evaluations in\nboth simulations and testbed showed that MetaFlow increases system throughput\nby a factor of up to 3.2, and reduce system latency by a factor of up to 5\ncompared to DHT-based systems. We also deployed MetaFlow in a distributed file\nsystem, and demonstrated significant performance improvement.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 03:18:11 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 07:49:39 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Sun", "Peng", ""], ["Wen", "Yonggang", ""], ["Duong", "Ta Nguyen Binh", ""], ["Xie", "Haiyong", ""]]}, {"id": "1611.01598", "submitter": "Natalie  Perlin", "authors": "Natalie Perlin, Joel P. Zysman, Ben P. Kirtman", "title": "Practical scalability assesment for parallel scientific numerical\n  applications", "comments": "9 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of scalability analysis of numerical parallel applications has\nbeen revisited, with the specific goals defined for the performance estimation\nof research applications. A series of Community Climate Model System (CCSM)\nnumerical simulations were used to test the several MPI implementations,\ndetermine optimal use of the system resources, and their scalability. The\nscaling capacity and model throughput performance metrics for $N$ cores showed\na log-linear behavior approximated by a power fit in the form of $C(N)=bN^a$,\nwhere $a$ and $b$ are two empirical constants. Different metrics yielded\nidentical power coefficients ($a$), but different dimensionality coefficients\n($b$). This model was consistent except for the large numbers of N. The power\nfit approach appears to be very useful for scalability estimates, especially\nwhen no serial testing is possible. Scalability analysis of additional\nscientific application has been conducted in the similar way to validate the\nrobustness of the power fit approach.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 03:55:18 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Perlin", "Natalie", ""], ["Zysman", "Joel P.", ""], ["Kirtman", "Ben P.", ""]]}, {"id": "1611.01690", "submitter": "Vincenzo De Florio", "authors": "Vincenzo De Florio", "title": "A Fault-tolerance Linguistic Structure for Distributed Applications", "comments": "Doctoral thesis, successfully defended on October 13, 2000", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structures for the expression of fault-tolerance provisions into the\napplication software are the central topic of this dissertation. Structuring\ntechniques provide means to control complexity, the latter being a relevant\nfactor for the introduction of design faults. This fact and the ever increasing\ncomplexity of today's distributed software justify the need for simple,\ncoherent, and effective structures for the expression of fault-tolerance in the\napplication software. A first contribution of this dissertation is the\ndefinition of a base of structural attributes with which application-level\nfault-tolerance structures can be qualitatively assessed and compared with each\nother and with respect to the above mentioned need. This result is then used to\nprovide an elaborated survey of the state-of-the-art of software\nfault-tolerance structures. The key contribution of this work is a novel\nstructuring technique for the expression of the fault-tolerance design concerns\nin the application layer of those distributed software systems that are\ncharacterized by soft real-time requirements and with a number of processing\nnodes known at compile-time. The main thesis of this dissertation is that this\nnew structuring technique is capable of exhibiting satisfactory values of the\nstructural attributes in the domain of soft real-time, distributed and parallel\napplications. Following this novel approach, beside the conventional\nprogramming language addressing the functional design concerns, a\nspecial-purpose linguistic structure (the so-called \"recovery language\") is\navailable to address error recovery and reconfiguration. This recovery language\ncomes into play as soon as an error is detected by an underlying error\ndetection layer, or when some erroneous condition is signaled by the\napplication processes.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 19:29:00 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["De Florio", "Vincenzo", ""]]}, {"id": "1611.01985", "submitter": "Bernhard Etzlinger", "authors": "Bernhard Etzlinger, Florian Meyer, Franz Hlawatsch, Andreas Springer,\n  and Henk Wymeersch", "title": "Cooperative Simultaneous Localization and Synchronization in Mobile\n  Agent Networks", "comments": "13 pages, 6 figures, 3 tables; manuscript submitted to IEEE\n  Transaction on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2691665", "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative localization in agent networks based on interagent time-of-flight\nmeasurements is closely related to synchronization. To leverage this relation,\nwe propose a Bayesian factor graph framework for cooperative simultaneous\nlocalization and synchronization (CoSLAS). This framework is suited to mobile\nagents and time-varying local clock parameters. Building on the CoSLAS factor\ngraph, we develop a distributed (decentralized) belief propagation algorithm\nfor CoSLAS in the practically important case of an affine clock model and\nasymmetric time stamping. Our algorithm allows for real-time operation and is\nsuitable for a time-varying network connectivity. To achieve high accuracy at\nreduced complexity and communication cost, the algorithm combines particle\nimplementations with parametric message representations and takes advantage of\na conditional independence property. Simulation results demonstrate the good\nperformance of the proposed algorithm in a challenging scenario with\ntime-varying network connectivity.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 11:00:34 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Etzlinger", "Bernhard", ""], ["Meyer", "Florian", ""], ["Hlawatsch", "Franz", ""], ["Springer", "Andreas", ""], ["Wymeersch", "Henk", ""]]}, {"id": "1611.02101", "submitter": "Ilya Trofimov", "authors": "Ilya Trofimov, Alexander Genkin", "title": "Distributed Coordinate Descent for Generalized Linear Models with\n  Regularization", "comments": "fix typos. arXiv admin note: text overlap with arXiv:1411.6520", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear model with $L_1$ and $L_2$ regularization is a widely used\ntechnique for solving classification, class probability estimation and\nregression problems. With the numbers of both features and examples growing\nrapidly in the fields like text mining and clickstream data analysis\nparallelization and the use of cluster architectures becomes important. We\npresent a novel algorithm for fitting regularized generalized linear models in\nthe distributed environment. The algorithm splits data between nodes by\nfeatures, uses coordinate descent on each node and line search to merge results\nglobally. Convergence proof is provided. A modifications of the algorithm\naddresses slow node problem. For an important particular case of logistic\nregression we empirically compare our program with several state-of-the art\napproaches that rely on different algorithmic and data spitting methods.\nExperiments demonstrate that our approach is scalable and superior when\ntraining on large and sparse datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 15:19:54 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 13:35:23 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Trofimov", "Ilya", ""], ["Genkin", "Alexander", ""]]}, {"id": "1611.02274", "submitter": "Kyle Niemeyer", "authors": "Kyle E Niemeyer and Chih-Jen Sung", "title": "GPU-Based Parallel Integration of Large Numbers of Independent ODE\n  Systems", "comments": "21 pages, 2 figures", "journal-ref": "Numerical Computations with GPUs, Ch. 8 (2014) 159-182. V\n  Kindratenko (Ed.)", "doi": "10.1007/978-3-319-06548-9_8", "report-no": null, "categories": "cs.MS cs.DC physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of integrating a large number of independent ODE systems arises in\nvarious scientific and engineering areas. For nonstiff systems, common explicit\nintegration algorithms can be used on GPUs, where individual GPU threads\nconcurrently integrate independent ODEs with different initial conditions or\nparameters. One example is the fifth-order adaptive Runge-Kutta-Cash-Karp\n(RKCK) algorithm. In the case of stiff ODEs, standard explicit algorithms\nrequire impractically small time-step sizes for stability reasons, and implicit\nalgorithms are therefore commonly used instead to allow larger time steps and\nreduce the computational expense. However, typical high-order implicit\nalgorithms based on backwards differentiation formulae (e.g., VODE, LSODE)\ninvolve complex logical flow that causes severe thread divergence when\nimplemented on GPUs, limiting the performance. Therefore, alternate algorithms\nare needed. A GPU-based Runge-Kutta-Chebyshev (RKC) algorithm can handle\nmoderate levels of stiffness and performs significantly faster than not only an\nequivalent CPU version but also a CPU-based implicit algorithm (VODE) based on\nresults shown in the literature. In this chapter, we present the mathematical\nbackground, implementation details, and source code for the RKCK and RKC\nalgorithms for use integrating large numbers of independent systems of ODEs on\nGPUs. In addition, brief performance comparisons are shown for each algorithm,\ndemonstrating the potential benefit of moving to GPU-based ODE integrators.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 18:11:16 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Niemeyer", "Kyle E", ""], ["Sung", "Chih-Jen", ""]]}, {"id": "1611.02275", "submitter": "Frederic Le Mouel", "authors": "Roya Golchay (CITI), Fr\\'ed\\'eric Le Mou\\\"el (CITI), Julien Ponge\n  (CITI), Nicolas Stouls (CITI)", "title": "Automated Application Offloading through Ant-inspired Decision-Making", "comments": null, "journal-ref": "Proceedings of the 13th International Conference on New\n  Technologies in Distributed Systems (NOTERE'2016), Jul 2016, Paris, France", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  -The explosive trend of smartphone usage as the most effective and convenient\ncommunication tools of human life in recent years make developers build ever\nmore complex smartphone applications. Gaming, navigation, video editing,\naugmented reality, and speech recognition applications require considerable\ncomputational power and energy. Although smart- phones have a wide range of\ncapabilities - GPS, WiFi, cameras - their inherent limitations - frequent\ndisconnections, mobility - and significant constraints - size, lower weights,\nlonger battery life - make difficult to exploiting their full potential to run\ncomplex applications. Several research works have proposed solutions in\napplication offloading domain, but few ones concerning the highly changing\nproperties of the environment. To address these issues, we realize an automated\napplication offloading middleware, ACOMMA, with dynamic and re-adaptable\ndecision-making engine. The decision engine of ACOMMA is based on an ant-\ninspired algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 10:37:38 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Golchay", "Roya", "", "CITI"], ["Mou\u00ebl", "Fr\u00e9d\u00e9ric Le", "", "CITI"], ["Ponge", "Julien", "", "CITI"], ["Stouls", "Nicolas", "", "CITI"]]}, {"id": "1611.02445", "submitter": "Tadeusz Tomczak", "authors": "Tadeusz Tomczak and Roman G. Szafran", "title": "A new GPU implementation for lattice-Boltzmann simulations on sparse\n  geometries", "comments": "20 pages, 20 figures, sent to Computer Physics Comunications, after\n  including Reviewers comments", "journal-ref": null, "doi": null, "report-no": "W04/2016/P-036", "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a high-performance implementation of the lattice Boltzmann method\n(LBM) for sparse 3D geometries on graphic processors (GPU). The main\ncontribution of this work is a data layout that allows to minimise the number\nof redundant memory transactions during the propagation step of LBM. We show\nthat by using a uniform mesh of small three-dimensional tiles and a careful\ndata placement it is possible to utilise more than 70% of maximum theoretical\nGPU memory bandwidth for D3Q19 lattice and double precision numbers. The\nperformance of our implementation is thoroughly examined and compared with\nother GPU implementations of LBM. The proposed method performs the best for\nsparse geometries with good spatial locality.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 09:33:18 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 11:45:05 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Tomczak", "Tadeusz", ""], ["Szafran", "Roman G.", ""]]}, {"id": "1611.02496", "submitter": "Matthias F\\\"ugger", "authors": "Bernadette Charron-Bost and Matthias F\\\"ugger and Thomas Nowak", "title": "Multidimensional Asymptotic Consensus in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of asymptotic consensus as it occurs in a wide range of\napplications in both man-made and natural systems. In particular, we study\nsystems with directed communication graphs that may change over time.\n  We recently proposed a new family of convex combination algorithms in\ndimension one whose weights depend on the received values and not only on the\ncommunication topology. Here, we extend this approach to arbitrarily high\ndimensions by introducing two new algorithms: the ExtremePoint and the Centroid\nalgorithm. Contrary to classical convex combination algorithms, both have\ncomponent-wise contraction rates that are constant in the number of agents.\nPaired with a speed-up technique for convex combination algorithms, we get a\nconvergence time linear in the number of agents, which is optimal.\n  Besides their respective contraction rates, the two algorithms differ in the\nfact that the Centroid algorithm's update rule is independent of any coordinate\nsystem while the ExtremePoint algorithm implicitly assumes a common agreed-upon\ncoordinate system among agents. The latter assumption may be realistic in some\nman-made multi-agent systems but is highly questionable in systems designed for\nthe modelization of natural phenomena.\n  Finally we prove that our new algorithms also achieve asymptotic consensus\nunder very weak connectivity assumptions, provided that agent interactions are\nbidirectional.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 12:16:38 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Charron-Bost", "Bernadette", ""], ["F\u00fcgger", "Matthias", ""], ["Nowak", "Thomas", ""]]}, {"id": "1611.02589", "submitter": "Amos Korman", "authors": "Pierre Fraigniaud (IRIF), Amos Korman (IRIF)", "title": "An Optimal Ancestry Labeling Scheme with Applications to XML Trees and\n  Universal Posets", "comments": null, "journal-ref": "Journal of the ACM, ACM, 2016, 63, pp.1 - 31", "doi": "10.1145/2794076", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we solve the ancestry-labeling scheme problem which aims at\nassigning the shortest possible labels (bit strings) to nodes of rooted trees,\nso that ancestry queries between any two nodes can be answered by inspecting\ntheir assigned labels only. This problem was introduced more than twenty years\nago by Kannan et al. [STOC '88], and is among the most well-studied problems in\nthe field of informative labeling schemes. We construct an ancestry-labeling\nscheme for $n$-node trees with label size $\\log_2 n + O(\\log \\log n)$ bits,\nthus matching the $\\log_2 n + O(\\log \\log n)$ bits lower bound given by Alstrup\net al. [SODA '03]. Our scheme is based on a simplified ancestry scheme that\noperates extremely well on a restricted set of trees. In particular, for the\nset of n-node trees with depth at most d, the simplified ancestry scheme enjoys\nlabel size of $\\log_2 n + 2 \\log_2 d + O(1)$ bits. Since the depth of most XML\ntrees is at most some small constant, such an ancestry scheme may be of\npractical use. In addition, we also obtain an adjacency-labeling scheme that\nlabels n-node trees of depth d with labels of size $\\log_2 n + 3 \\log_2 d +\nO(1)$ bits. All our schemes assign the labels in linear time, and guarantee\nthat any query can be answered in constant time. Finally, our ancestry scheme\nfinds applications to the construction of small universal partially ordered\nsets (posets). Specifically, for any fixed integer k, it enables the\nconstruction of a universal poset of size $\\tilde O(n^k)$ for the family of\n$n$-element posets with tree-dimension at most $k$. Up to lower order terms,\nthis bound is tight thanks to a lower bound of $n^{k-o(1)}$ due to Alon and\nScheinerman [Order '88].\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 16:19:19 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Fraigniaud", "Pierre", "", "IRIF"], ["Korman", "Amos", "", "IRIF"]]}, {"id": "1611.02665", "submitter": "Amrita Mathuriya", "authors": "Amrita Mathuriya, Ye Luo, Anouar Benali, Luke Shulenburger, Jeongnim\n  Kim", "title": "Optimization and parallelization of B-spline based orbital evaluations\n  in QMC on multi/many-core shared memory processors", "comments": "11 pages, 10 figures, 4 tables", "journal-ref": null, "doi": "10.1109/IPDPS.2017.33", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B-spline based orbital representations are widely used in Quantum Monte Carlo\n(QMC) simulations of solids, historically taking as much as 50% of the total\nrun time. Random accesses to a large four-dimensional array make it challenging\nto efficiently utilize caches and wide vector units of modern CPUs. We present\nnode-level optimizations of B-spline evaluations on multi/many-core shared\nmemory processors. To increase SIMD efficiency and bandwidth utilization, we\nfirst apply data layout transformation from array-of-structures to\nstructure-of-arrays (SoA). Then by blocking SoA objects, we optimize cache\nreuse and get sustained throughput for a range of problem sizes. We implement\nefficient nested threading in B-spline orbital evaluation kernels, paving the\nway towards enabling strong scaling of QMC simulations. These optimizations are\nportable on four distinct cache-coherent architectures and result in up to 5.6x\nperformance enhancements on Intel Xeon Phi processor 7250P (KNL), 5.7x on Intel\nXeon Phi coprocessor 7120P, 10x on an Intel Xeon processor E5v4 CPU and 9.5x on\nBlueGene/Q processor. Our nested threading implementation shows nearly ideal\nparallel efficiency on KNL up to 16 threads. We employ roofline performance\nanalysis to model the impacts of our optimizations. This work combined with our\ncurrent efforts of optimizing other QMC kernels, result in greater than 4.5x\nspeedup of miniQMC on KNL.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 19:31:51 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Mathuriya", "Amrita", ""], ["Luo", "Ye", ""], ["Benali", "Anouar", ""], ["Shulenburger", "Luke", ""], ["Kim", "Jeongnim", ""]]}, {"id": "1611.02717", "submitter": "Saurabh Hukerikar", "authors": "Saurabh Hukerikar and Christian Engelmann", "title": "Resilience Design Patterns - A Structured Approach to Resilience at\n  Extreme Scale (version 1.0)", "comments": "Oak Ridge National Laboratory Technical Report version 1.0", "journal-ref": null, "doi": null, "report-no": "ORNL/TM-2016/687", "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we develop a structured approach to the management of HPC\nresilience based on the concept of resilience-based design patterns. A design\npattern is a general repeatable solution to a commonly occurring problem. We\nidentify the commonly occurring problems and solutions used to deal with\nfaults, errors and failures in HPC systems. The catalog of resilience design\npatterns provides designers with reusable design elements. We define a design\nframework that enhances our understanding of the important constraints and\nopportunities for solutions deployed at various layers of the system stack. The\nframework may be used to establish mechanisms and interfaces to coordinate\nflexible fault management across hardware and software components. The\nframework also enables optimization of the cost-benefit trade-offs among\nperformance, resilience, and power consumption. The overall goal of this work\nis to enable a systematic methodology for the design and evaluation of\nresilience technologies in extreme-scale HPC systems that keep scientific\napplications running to a correct solution in a timely and cost-efficient\nmanner in spite of frequent faults, errors, and failures of various types.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 21:02:29 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 18:51:50 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Hukerikar", "Saurabh", ""], ["Engelmann", "Christian", ""]]}, {"id": "1611.02758", "submitter": "Yuri Demchenko", "authors": "Yuri Demchenko, Paola Grosso, Cees de Laat, Sonja Filiposka, Migiel de\n  Vos", "title": "ZeroTouch Provisioning (ZTP) Model and Infrastructure Components for\n  Multi-provider Cloud Services Provisioning", "comments": "6 pages, 2 fugures", "journal-ref": "Fifth IEEE International Workshop on Cloud Computing Interclouds,\n  Multiclouds, Federations, and Interoperability (Intercloud 2016), In Proc.\n  IEEE International Conference on Cloud Engineering (IC2E), April 4 - 8, 2016,\n  Berlin, Germany", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents results of the ongoing development of the Cloud Services\nDelivery Infrastructure (CSDI) that provides a basis for infrastructure centric\ncloud services provisioning, operation and management in multi-cloud\nmulti-provider environment defined as a Zero Touch Provisioning, Operation and\nManagement (ZTP/ZTPOM) model. The presented work refers to use cases from data\nintensive research that require high performance computation resources and\nlarge storage volumes that are typically distributed between datacenters often\ninvolving multiple cloud providers. Automation for large scale scientific (and\nindustrial) applications should include provisioning of both inter-cloud\nnetwork infrastructure and intra-cloud application resources. It should provide\nsupport for the complete application operation workflow together with the\npossible application infrastructure and resources changes that can occur during\nthe application lifecycle. The authors investigate existing technologies for\nautomation of the service provisioning and management processes aiming to\ncross-pollinate best practices from currently disconnected domains such as\ncloud based applications provisioning and multi-domain high-performance network\nprovisioning. The paper refers to the previous and legacy research by authors,\nthe Open Cloud eXchange (OCX), that has been proposed to address the last mile\nproblem in cloud services delivery to campuses over trans-national backbone\nnetworks such as GEANT. OCX will serve as an integral component of the\nprospective ZTP infrastructure over the GEANT network. Another important\ncomponent, the Marketplace, is defined for providing cloud services and\napplications discovery (in generally intercloud environment) and may also\nsupport additional services such as services composition and trust brokering\nfor establishing customer-provider federations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 22:58:39 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Demchenko", "Yuri", ""], ["Grosso", "Paola", ""], ["de Laat", "Cees", ""], ["Filiposka", "Sonja", ""], ["de Vos", "Migiel", ""]]}, {"id": "1611.02823", "submitter": "Saurabh Hukerikar", "authors": "Saurabh Hukerikar and Christian Engelmann", "title": "Language Support for Reliable Memory Regions", "comments": "The 29th International Workshop on Languages and Compilers for\n  Parallel Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The path to exascale computational capabilities in high-performance computing\n(HPC) systems is challenged by the inadequacy of present software technologies\nto adapt to the rapid evolution of architectures of supercomputing systems. The\nconstraints of power have driven system designs to include increasingly\nheterogeneous architectures and diverse memory technologies and interfaces.\nFuture systems are also expected to experience an increased rate of errors,\nsuch that the applications will no longer be able to assume correct behavior of\nthe underlying machine. To enable the scientific community to succeed in\nscaling their applications, and to harness the capabilities of exascale\nsystems, we need software strategies that provide mechanisms for explicit\nmanagement of resilience to errors in the system, in addition to locality of\nreference in the complex memory hierarchies of future HPC systems.\n  In prior work, we introduced the concept of explicitly reliable memory\nregions, called havens. Memory management using havens supports reliability\nmanagement through a region-based approach to memory allocations. Havens enable\nthe creation of robust memory regions, whose resilient behavior is guaranteed\nby software-based protection schemes. In this paper, we propose language\nsupport for havens through type annotations that make the structure of a\nprogram's havens more explicit and convenient for HPC programmers to use. We\ndescribe how the extended haven-based memory management model is implemented,\nand demonstrate the use of the language-based annotations to affect the\nresiliency of a conjugate gradient solver application.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 05:49:52 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 05:49:25 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Hukerikar", "Saurabh", ""], ["Engelmann", "Christian", ""]]}, {"id": "1611.02905", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Eduardo R. Rodrigues, Renato L. F. Cunha, Marco A. S. Netto, Michael\n  Spriggs", "title": "Helping HPC Users Specify Job Memory Requirements via Machine Learning", "comments": "8 pages, 3 figures, presented at the Third Annual Workshop on HPC\n  User Support Tools", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource allocation in High Performance Computing (HPC) settings is still not\neasy for end-users due to the wide variety of application and environment\nconfiguration options. Users have difficulties to estimate the number of\nprocessors and amount of memory required by their jobs, select the queue and\npartition, and estimate when job output will be available to plan for next\nexperiments. Apart from wasting infrastructure resources by making wrong\nallocation decisions, overall user response time can also be negatively\nimpacted. Techniques that exploit batch scheduler systems to predict waiting\ntime and runtime of user jobs have already been proposed. However, we observed\nthat such techniques are not suitable for predicting job memory usage. In this\npaper we introduce a tool to help users predict their memory requirements using\nmachine learning. We describe the integration of the tool with a batch\nscheduler system, discuss how batch scheduler log data can be exploited to\ngenerate memory usage predictions through machine learning, and present results\nof two production systems containing thousands of jobs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 12:49:23 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Rodrigues", "Eduardo R.", ""], ["Cunha", "Renato L. F.", ""], ["Netto", "Marco A. S.", ""], ["Spriggs", "Michael", ""]]}, {"id": "1611.02917", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Bruno Silva, Marco A. S. Netto, Renato L. F. Cunha", "title": "SLA-aware Interactive Workflow Assistant for HPC Parameter Sweeping\n  Experiments", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common workflow in science and engineering is to (i) setup and deploy large\nexperiments with tasks comprising an application and multiple parameter values;\n(ii) generate intermediate results; (iii) analyze them; and (iv) reprioritize\nthe tasks. These steps are repeated until the desired goal is achieved, which\ncan be the evaluation/simulation of complex systems or model calibration. Due\nto time and cost constraints, sweeping all possible parameter values of the\nuser application is not always feasible. Experimental Design techniques can\nhelp users reorganize submission-execution-analysis workflows to bring a\nsolution in a more timely manner. This paper introduces a novel tool that\nleverages users' feedback on analyzing intermediate results of parameter\nsweeping experiments to advise them about their strategies on parameter\nselections tied to their SLA constraints. We evaluated our tool with three\napplications of distinct domains and search space shapes. Our main finding is\nthat users with submission-execution-analysis workflows can benefit from their\ninteraction with intermediate results and adapt themselves according to their\ndomain expertise and SLA constraints.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 13:20:47 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Silva", "Bruno", ""], ["Netto", "Marco A. S.", ""], ["Cunha", "Renato L. F.", ""]]}, {"id": "1611.02929", "submitter": "Johannes Holke", "authors": "Carsten Burstedde, Johannes Holke", "title": "Coarse mesh partitioning for tree based AMR", "comments": "28 pages, 11 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tree based adaptive mesh refinement, elements are partitioned between\nprocesses using a space filling curve. The curve establishes an ordering\nbetween all elements that derive from the same root element, the tree. When\nrepresenting more complex geometries by patching together several trees, the\nroots of these trees form an unstructured coarse mesh. We present an algorithm\nto partition the elements of the coarse mesh such that (a) the fine mesh can be\nload-balanced to equal element counts per process regardless of the\nelement-to-tree map and (b) each process that holds fine mesh elements has\naccess to the meta data of all relevant trees. As an additional feature, the\nalgorithm partitions the meta data of relevant ghost (halo) trees as well. We\ndevelop in detail how each process computes the communication pattern for the\npartition routine without handshaking and with minimal data movement. We\ndemonstrate the scalability of this approach on up to 917e3 MPI ranks and\n.37e12 coarse mesh elements, measuring run times of one second or less.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 13:41:02 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Burstedde", "Carsten", ""], ["Holke", "Johannes", ""]]}, {"id": "1611.02944", "submitter": "Jernej Vi\\v{c}i\\v{c}", "authors": "Jernej Vi\\v{c}i\\v{c}, Andrej Brodnik", "title": "Increasing the throughput of machine translation systems using clouds", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The manuscript presents an experiment at implementation of a Machine\nTranslation system in a MapReduce model. The empirical evaluation was done\nusing fully implemented translation systems embedded into the MapReduce\nprogramming model. Two machine translation paradigms were studied: shallow\ntransfer Rule Based Machine Translation and Statistical Machine Translation.\n  The results show that the MapReduce model can be successfully used to\nincrease the throughput of a machine translation system. Furthermore this\nmethod enhances the throughput of a machine translation system without\ndecreasing the quality of the translation output.\n  Thus, the present manuscript also represents a contribution to the seminal\nwork in natural language processing, specifically Machine Translation. It first\npoints toward the importance of the definition of the metric of throughput of\ntranslation system and, second, the applicability of the machine translation\ntask to the MapReduce paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 14:27:03 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Vi\u010di\u010d", "Jernej", ""], ["Brodnik", "Andrej", ""]]}, {"id": "1611.03215", "submitter": "Ken Bloom", "authors": "Kenneth Bloom", "title": "CMS software and computing for LHC Run 2", "comments": "Contribution to proceedings of the 38th International Conference on\n  High Energy Physics (ICHEP 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CMS offline software and computing system has successfully met the\nchallenge of LHC Run 2. In this presentation, we will discuss how the entire\nsystem was improved in anticipation of increased trigger output rate, increased\nrate of pileup interactions and the evolution of computing technology. The\nprimary goals behind these changes was to increase the flexibility of computing\nfacilities where ever possible, as to increase our operational efficiency, and\nto decrease the computing resources needed to accomplish the primary offline\ncomputing workflows. These changes have resulted in a new approach to\ndistributed computing in CMS for Run 2 and for the future as the LHC luminosity\nshould continue to increase. We will discuss changes and plans to our data\nfederation, which was one of the key changes towards a more flexible computing\nmodel for Run 2. Our software framework and algorithms also underwent\nsignificant changes. We will summarize the our experience with a new\nmulti-threaded framework as deployed on our prompt reconstruction farm for 2015\nand across the CMS WLCG Tier-1 facilities. We will discuss our experience with\na analysis data format which is ten times smaller than our primary Run 1\nformat. This \"miniAOD\" format has proven to be easier to analyze while be\nextremely flexible for analysts. Finally, we describe improvements to our\nworkflow management system that have resulted in increased automation and\nreliability for all facets of CMS production and user analysis operations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 08:21:06 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Bloom", "Kenneth", ""]]}, {"id": "1611.03226", "submitter": "Jani Boutellier", "authors": "Jani Boutellier and Ilkka Hautala", "title": "Executing Dynamic Data Rate Actor Networks on OpenCL Platforms", "comments": "2016 IEEE International Workshop on Signal Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous computing platforms consisting of general purpose processors\n(GPPs) and graphics processing units (GPUs) have become commonplace in personal\nmobile devices and embedded systems. For years, programming of these platforms\nwas very tedious and simultaneous use of all available GPP and GPU resources\nrequired low-level programming to ensure efficient synchronization and data\ntransfer between processors. However, in the last few years several high-level\nprogramming frameworks have emerged, which enable programmers to describe\napplications by means of abstractions such as dataflow or Kahn process networks\nand leave parallel execution, data transfer and synchronization to be handled\nby the framework.\n  Unfortunately, even the most advanced high-level programming frameworks have\nhad shortcomings that limit their applicability to certain classes of\napplications. This paper presents a new, dataflow-flavored programming\nframework targeting heterogeneous platforms, and differs from previous\napproaches by allowing GPU-mapped actors to have data dependent consumption of\ninputs / production of outputs. Such flexibility is essential for configurable\nand adaptive applications that are becoming increasingly common in signal\nprocessing. In our experiments it is shown that this feature allows up to 5x\nincrease in application throughput.\n  The proposed framework is validated by application examples from the video\nprocessing and wireless communications domains. In the experiments the\nframework is compared to a well-known reference framework and it is shown that\nthe proposed framework enables both a higher degree of flexibility and better\nthroughput.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 09:06:30 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Boutellier", "Jani", ""], ["Hautala", "Ilkka", ""]]}, {"id": "1611.03404", "submitter": "Jeffrey Regier", "authors": "Jeffrey Regier, Kiran Pamnany, Ryan Giordano, Rollin Thomas, David\n  Schlegel, Jon McAuliffe and Prabhat", "title": "Learning an Astronomical Catalog of the Visible Universe through\n  Scalable Bayesian Inference", "comments": "submitting to IPDPS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Celeste is a procedure for inferring astronomical catalogs that attains\nstate-of-the-art scientific results. To date, Celeste has been scaled to at\nmost hundreds of megabytes of astronomical images: Bayesian posterior inference\nis notoriously demanding computationally. In this paper, we report on a\nscalable, parallel version of Celeste, suitable for learning catalogs from\nmodern large-scale astronomical datasets. Our algorithmic innovations include a\nfast numerical optimization routine for Bayesian posterior inference and a\nstatistically efficient scheme for decomposing astronomical optimization\nproblems into subproblems.\n  Our scalable implementation is written entirely in Julia, a new high-level\ndynamic programming language designed for scientific and numerical computing.\nWe use Julia's high-level constructs for shared and distributed memory\nparallelism, and demonstrate effective load balancing and efficient scaling on\nup to 8192 Xeon cores on the NERSC Cori supercomputer.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:16:04 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Regier", "Jeffrey", ""], ["Pamnany", "Kiran", ""], ["Giordano", "Ryan", ""], ["Thomas", "Rollin", ""], ["Schlegel", "David", ""], ["McAuliffe", "Jon", ""], ["Prabhat", "", ""]]}, {"id": "1611.03562", "submitter": "Stavros Nikolaou", "authors": "Stavros Nikolaou and Robbert van Renesse", "title": "Moving Participants Turtle Consensus", "comments": "31 pages, 4 figures, OPODIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Moving Participants Turtle Consensus (MPTC), an asynchronous\nconsensus protocol for crash and Byzantine-tolerant distributed systems. MPTC\nuses various moving target defense strategies to tolerate certain\nDenial-of-Service (DoS) attacks issued by an adversary capable of compromising\na bounded portion of the system. MPTC supports on the fly reconfiguration of\nthe consensus strategy as well as of the processes executing this strategy when\nsolving the problem of agreement. It uses existing cryptographic techniques to\nensure that reconfiguration takes place in an unpredictable fashion thus\neliminating the adversary's advantage on predicting protocol and\nexecution-specific information that can be used against the protocol.\n  We implement MPTC as well as a State Machine Replication protocol and\nevaluate our design under different attack scenarios. Our evaluation shows that\nMPTC approximates best case scenario performance even under a well-coordinated\nDoS attack.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 01:42:19 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 05:17:03 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Nikolaou", "Stavros", ""], ["van Renesse", "Robbert", ""]]}, {"id": "1611.03947", "submitter": "Muktikanta Sa", "authors": "Sathya Peri, Muktikanta Sa, Nandini Singhal", "title": "A Pragmatic Non-Blocking Concurrent Directed Acyclic Graph", "comments": "18 pages, 8 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have developed two algorithms for maintaining acyclicity in\na concurrent directed graph. The first algorithm is based on a wait-free\nreachability query and the second one is based on partial snapshot-based\nobstruction-free reachability query. Interestingly, we are able to achieve the\nacyclic property in the dynamic setting without the need of helping using\ndescriptors by other threads or clean double collect mechanism. We present a\nproof to show that the graph remains acyclic at all times in the concurrent\nsetting. We also prove that the acyclic graph data-structure operations are\nlinearizable. We implement both the algorithms in C++ and test through a number\nof micro-benchmarks. Our experimental results show an average of 7x improvement\nover the sequential and global lock implementation.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 03:40:35 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 18:25:35 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 07:38:46 GMT"}, {"version": "v4", "created": "Mon, 24 Apr 2017 18:17:03 GMT"}, {"version": "v5", "created": "Mon, 17 Jul 2017 10:21:36 GMT"}, {"version": "v6", "created": "Sun, 12 May 2019 02:44:36 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Peri", "Sathya", ""], ["Sa", "Muktikanta", ""], ["Singhal", "Nandini", ""]]}, {"id": "1611.04022", "submitter": "Zhuolun Xiang", "authors": "Zhuolun Xiang, Nitin H. Vaidya", "title": "Timestamps for Partial Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining causal consistency in distributed shared memory systems using\nvector timestamps has received a lot of attention from both theoretical and\npractical prospective. However, most of the previous literature focuses on full\nreplication where each data is stored in all replicas, which may not be\nscalable due to the increasing amount of data. In this report, we investigate\nhow to achieve causal consistency in partial replicated systems, where each\nreplica may store different set of data. We propose an algorithm that tracks\ncausal dependencies via vector timestamp in client-server model for partial\nreplication. The cost of our algorithm in terms of timestamps size varies as a\nfunction of the manner in which the replicas share data, and the set of\nreplicas accessed by each client. We also establish a connection between our\nalgorithm with the previous work on full replication.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 17:32:48 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 20:37:53 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Xiang", "Zhuolun", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1611.04167", "submitter": "Richard Henwood Mr", "authors": "R. Henwood, N. W. Watkins, S. C. Chapman, R. McLay", "title": "A parallel workload has extreme variability", "comments": "7 pages, 2 figures, 1 code listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both high-performance computing (HPC) environments and the public cloud,\nthe duration of time to retrieve or save your results is simultaneously\nunpredictable and important to your over all resource budget. It is generally\naccepted (\"Google: Taming the Long Latency Tail - When More Machines Equals\nWorse Results\", Todd Hoff, highscalability.com 2012), but without a robust\nexplanation, that identical parallel tasks do take different durations to\ncomplete -- a phenomena known as variability. This paper advances understanding\nof this topic. We carefully choose a model from which system-level complexity\nemerges that can be studied directly. We find that a generalized extreme value\n(GEV) model for variability naturally emerges. Using the public cloud, we find\nreal-world observations have excellent agreement with our model. Since the GEV\ndistribution is a limit distribution this suggests a universal property of\nparallel systems gated by the slowest communication element of some sort.\nHence, this model is applicable to a variety of processing and IO tasks in\nparallel environments. These findings have important implications, ranging from\ncharacterizing ideal performance for parallel codes to detecting degraded\nbehaviour at extreme scales.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 18:18:36 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 15:37:14 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Henwood", "R.", ""], ["Watkins", "N. W.", ""], ["Chapman", "S. C.", ""], ["McLay", "R.", ""]]}, {"id": "1611.04211", "submitter": "Karol Gotfryd", "authors": "Karol Gotfryd, Marek Klonowski, Dominik Paj\\k{a}k", "title": "On Location Hiding in Distributed Systems", "comments": "Submitted to 10th International Conference on Algorithms and\n  Complexity CIAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem - a group of mobile agents perform some\ntask on a terrain modeled as a graph. In a given moment of time an adversary\ngets an access to the graph and positions of the agents. Shortly before\nadversary's observation the mobile agents have a chance to relocate themselves\nin order to hide their initial configuration. We assume that the initial\nconfiguration may possibly reveal to the adversary some information about the\ntask they performed. Clearly agents have to change their location in possibly\nshort time using minimal energy. In our paper we introduce a definition of a\n\\emph{well hiding} algorithm in which the starting and final configurations of\nthe agents have small mutual information. Then we discuss the influence of\nvarious features of the model on the running time of the optimal well-hiding\nalgorithm. We show that if the topology of the graph is known to the agents,\nthen the number of steps proportional to the diameter of the graph is\nsufficient and necessary. In the unknown topology scenario we only consider a\nsingle agent case. We first show that the task is impossible in the\ndeterministic case if the agent has no memory. Then we present a polynomial\nrandomized algorithm. Finally in the model with memory we show that the number\nof steps proportional to the number of edges of the graph is sufficient and\nnecessary. In some sense we investigate how complex is the problem of \"losing\"\ninformation about location (both physical and logical) for different settings.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 23:59:06 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Gotfryd", "Karol", ""], ["Klonowski", "Marek", ""], ["Paj\u0105k", "Dominik", ""]]}, {"id": "1611.04255", "submitter": "Linnan Wang", "authors": "Linnan Wang, Wei Wu, George Bosilca, Richard Vuduc, Zenglin Xu", "title": "Efficient Communications in Training Large Scale Neural Networks", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of how to reduce the cost of communication that is\nrequired for the parallel training of a neural network. The state-of-the-art\nmethod, Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD),\nrequires many collective communication operations, like broadcasts of\nparameters or reductions for sub-gradient aggregations, which for large\nmessages quickly dominates overall execution time and limits parallel\nscalability. To address this problem, we develop a new technique for collective\noperations, referred to as Linear Pipelining (LP). It is tuned to the message\nsizes that arise in BSP-SGD, and works effectively on multi-GPU systems.\nTheoretically, the cost of LP is invariant to $P$, where $P$ is the number of\nGPUs, while the cost of more conventional Minimum Spanning Tree (MST) scales\nlike $O(\\log P)$. LP also demonstrate up to 2x faster bandwidth than\nBidirectional Exchange (BE) techniques that are widely adopted by current MPI\nimplementations. We apply these collectives to BSP-SGD, showing that the\nproposed implementations reduce communication bottlenecks in practice while\npreserving the attractive convergence properties of BSP-SGD.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 05:59:58 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 20:11:17 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Wang", "Linnan", ""], ["Wu", "Wei", ""], ["Bosilca", "George", ""], ["Vuduc", "Richard", ""], ["Xu", "Zenglin", ""]]}, {"id": "1611.04276", "submitter": "Danny Dolev", "authors": "Danny Dolev and Eli Gafni", "title": "Byzantine Processors and Cuckoo Birds: Confining Maliciousness to the\n  Outset", "comments": "arXiv admin note: text overlap with arXiv:1607.01210", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are there Byzantine Animals? A Fooling Behavior is exhibited by the Cuckoo\nbird. It sneakily replaces some of the eggs of other species with its own. Lest\nthe Cuckoo extinct itself by destroying its host, it self-limits its power: It\ndoes not replace too large a fraction of the eggs. Here, we show that any\nByzantine Behavior that does not destroy the system it attacks, i.e. allows the\nsystem to solve an easy task like epsilon-agreement, then its maliciousness can\nbe confined to be the exact replica of the Cuckoo bird behavior: Undetectably\nreplace an input of a processor and let the processor behave correctly\nthereafter with respect to the new input. In doing so we reduce the study of\nByzantine behavior to fail-stop (benign) behavior with the Cuckoo caveat of a\nfraction of the inputs replaced. We establish a complete correspondence between\nthe Byzantine and the Benign, modulo different thresholds, and replaced inputs.\nThis work is yet another step in a line of work unifying seemingly distinct\ndistributed system models, dispelling the Myth that Distributed Computing is a\nplethora of distinct isolated models, each requiring its specialized tools and\nideas in order to determine solvability of tasks. Thus, hereafter, Byzantine\nComputability questions can be reduced to questions in the benign failure\nsetting. We also show that the known results about correlated faults in the\nasynchronous benign setting can be imported verbatim to the asynchronous\nByzantine setting. Finally, as in the benign case in which we have the property\nthat a processor can output once its faulty behavior stops for long enough, we\nshow this can be done in a similar manner in the Byzantine case. This\nnecessitated the generalization of Reliable Broadcast to what we term\nRecoverable Reliable Broadcast.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 07:52:21 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Dolev", "Danny", ""], ["Gafni", "Eli", ""]]}, {"id": "1611.04934", "submitter": "Ehsan Totoni", "authors": "Ehsan Totoni, Todd A. Anderson, Tatiana Shpeisman", "title": "HPAT: High Performance Analytics with Scripting Ease-of-Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics requires high programmer productivity and high performance\nsimultaneously on large-scale clusters. However, current big data analytics\nframeworks (e.g. Apache Spark) have prohibitive runtime overheads since they\nare library-based. We introduce a novel auto-parallelizing compiler approach\nthat exploits the characteristics of the data analytics domain such as the\nmap/reduce parallel pattern and is robust, unlike previous auto-parallelization\nmethods. Using this approach, we build High Performance Analytics Toolkit\n(HPAT), which parallelizes high-level scripting (Julia) programs automatically,\ngenerates efficient MPI/C++ code, and provides resiliency. Furthermore, it\nprovides automatic optimizations for scripting programs, such as fusion of\narray operations. Thus, HPAT is 369x to 2033x faster than Spark on the Cori\nsupercomputer and 20x to 256x times on Amazon AWS.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 16:59:35 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 23:49:33 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Totoni", "Ehsan", ""], ["Anderson", "Todd A.", ""], ["Shpeisman", "Tatiana", ""]]}, {"id": "1611.04999", "submitter": "Cyrus Rashtchian", "authors": "Paul Beame and Cyrus Rashtchian", "title": "Massively-Parallel Similarity Join, Edge-Isoperimetry, and Distance\n  Correlations on the Hypercube", "comments": "23 pages, plus references and appendix. To appear in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed protocols for finding all pairs of similar vectors in a\nlarge dataset. Our results pertain to a variety of discrete metrics, and we\ngive concrete instantiations for Hamming distance. In particular, we give\nimproved upper bounds on the overhead required for similarity defined by\nHamming distance $r>1$ and prove a lower bound showing qualitative optimality\nof the overhead required for similarity over any Hamming distance $r$. Our main\nconceptual contribution is a connection between similarity search algorithms\nand certain graph-theoretic quantities. For our upper bounds, we exhibit a\ngeneral method for designing one-round protocols using edge-isoperimetric\nshapes in similarity graphs. For our lower bounds, we define a new\ncombinatorial optimization problem, which can be stated in purely\ngraph-theoretic terms yet also captures the core of the analysis in previous\ntheoretical work on distributed similarity joins. As one of our main technical\nresults, we prove new bounds on distance correlations in subsets of the Hamming\ncube.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 19:36:28 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Beame", "Paul", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "1611.05161", "submitter": "Danny Dolev", "authors": "Danny Dolev and Meir Spielrien", "title": "Possibility and Impossibility of Reliable Broadcast in the Bounded Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Reliable Broadcast concept allows an honest party to send a message to\nall other parties and to make sure that all honest parties receive this\nmessage. In addition, it allows an honest party that received a message to know\nthat all other honest parties would also receive the same message. This\ntechnique is important to ensure distributed consistency when facing failures.\n  In the current paper, we study the ability to use \\RR to consistently\ntransmit a sequence of input values in an asynchronous environment with a\ndesignated sender. The task can be easily achieved using counters, but cannot\nbe achieved with a bounded memory facing failures. We weaken the problem and\nask whether the receivers can at least share a common suffix. We prove that in\na standard (lossless) asynchronous system no bounded memory protocol can\nguarantee a common suffix at all receivers for every input sequence if a single\nparty might crash.\n  We further study the problem facing transient faults and prove that when\nlimiting the problem to transmitting a stream of a single value being sent\nrepeatedly we show a bounded memory self-stabilizing protocol that can ensure a\ncommon suffix even in the presence of transient faults and an arbitrary number\nof crash faults. We further prove that this last problem is not solvable in the\npresence of a single Byzantine fault. Thus, this problem {\\bf separates}\nByzantine behavior from crash faults in an asynchronous environment.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 06:30:55 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Dolev", "Danny", ""], ["Spielrien", "Meir", ""]]}, {"id": "1611.05170", "submitter": "Charith Perera", "authors": "Luiz H. Nunes, Julio C. Estrella, Alexandre C. B. Delbem, Charith\n  Perera, Stephan Reiff-Marganiec", "title": "The Effects of Relative Importance of User Constraints in Cloud of\n  Things Resource Discovery: A Case Study", "comments": "Proceedings of the 9th IEEE/ACM International Conference on Utility\n  and Cloud Computing (UCC 2016) Shaghai, China, December, 2016", "journal-ref": "Proceedings of the 9th IEEE/ACM International Conference on\n  Utility and Cloud Computing (UCC 2016) Shaghai, China, December, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the number of smart objects connected to the\nInternet has grown exponentially in comparison to the number of services and\napplications. The integration between Cloud Computing and Internet of Things,\nnamed as Cloud of Things, plays a key role in managing the connected things,\ntheir data and services. One of the main challenges in Cloud of Things is the\nresource discovery of the smart objects and their reuse in different contexts.\nMost of the existent work uses some kind of multi-criteria decision analysis\nalgorithm to perform the resource discovery, but do not evaluate the impact\nthat the user constraints has in the final solution. In this paper, we analyse\nthe behaviour of the SAW, TOPSIS and VIKOR multi-objective decision analyses\nalgorithms and the impact of user constraints on them. We evaluated the quality\nof the proposed solutions using the Pareto-optimality concept.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 07:15:03 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nunes", "Luiz H.", ""], ["Estrella", "Julio C.", ""], ["Delbem", "Alexandre C. B.", ""], ["Perera", "Charith", ""], ["Reiff-Marganiec", "Stephan", ""]]}, {"id": "1611.05346", "submitter": "Zulqarnain Mehdi", "authors": "Zulqarnain Mehdi and Hani Ragab-Hassen", "title": "File Synchronization Systems Survey", "comments": "The Sixth International Conference on Computer Science, Engineering &\n  Applications (ICCSEA 2016)", "journal-ref": "ICCSEA 2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several solutions exist for file storage, sharing, and synchronization. Many\nof them involve a central server, or a collection of servers, that either store\nthe files, or act as a gateway for them to be shared. Some systems take a\ndecentralized approach, wherein interconnected users form a peer-to-peer (P2P)\nnetwork, and partake in the sharing process: they share the files they possess\nwith others, and can obtain the files owned by other peers. In this paper, we\nsurvey various technologies, both cloud-based and P2P-based, that users use to\nsynchronize their files across the network, and discuss their strengths and\nweaknesses.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 13:35:16 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Mehdi", "Zulqarnain", ""], ["Ragab-Hassen", "Hani", ""]]}, {"id": "1611.05539", "submitter": "Md. Redowan Mahmud", "authors": "Redowan Mahmud, Ramamohanarao Kotagiri and Rajkumar Buyya", "title": "Fog Computing: A Taxonomy, Survey and Future Directions", "comments": null, "journal-ref": "Internet of Everything. Internet of Things (Technology,\n  Communications and Computing), Springer 2017 103-130", "doi": "10.1007/978-981-10-5861-5_5", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the number of Internet of Things (IoT) devices/sensors has\nincreased to a great extent. To support the computational demand of real-time\nlatency-sensitive applications of largely geo-distributed IoT devices/sensors,\na new computing paradigm named \"Fog computing\" has been introduced. Generally,\nFog computing resides closer to the IoT devices/sensors and extends the\nCloud-based computing, storage and networking facilities. In this chapter, we\ncomprehensively analyse the challenges in Fogs acting as an intermediate layer\nbetween IoT devices/ sensors and Cloud datacentres and review the current\ndevelopments in this field. We present a taxonomy of Fog computing according to\nthe identified challenges and its key features.We also map the existing works\nto the taxonomy in order to identify current research gaps in the area of Fog\ncomputing. Moreover, based on the observations, we propose future directions\nfor research.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 02:43:03 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 03:52:55 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 04:22:53 GMT"}, {"version": "v4", "created": "Sat, 21 Oct 2017 00:59:18 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Mahmud", "Redowan", ""], ["Kotagiri", "Ramamohanarao", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1611.05549", "submitter": "Krzysztof Nowicki", "authors": "Krzysztof Nowicki", "title": "Parallel multiple selection by regular sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a deterministic parallel algorithm solving the\nmultiple selection problem in congested clique model. In this problem for given\nset of elements S and a set of ranks $K = \\{k_1 , k_2 , ..., k_r \\}$ we are\nasking for the $k_i$-th smallest element of $S$ for $1 \\leq i \\leq r$. The\npresented algorithm is deterministic, time optimal , and needs $O(\\log^*_{r+1}\n(n))$ communication rounds, where $n$ is the size of the input set, and $r$ is\nthe size of the rank set. This algorithm may be of theoretical interest, as for\n$r = 1$ (classic selection problem) it gives an improvement in the asymptotic\nsynchronization cost over previous $O(\\log \\log p)$ communication rounds\nsolution, where $p$ is size of clique.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 03:23:38 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 02:43:46 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Nowicki", "Krzysztof", ""]]}, {"id": "1611.05616", "submitter": "Johanne Cohen", "authors": "Johanne Cohen and Jonas Lef\\`evre and Khaled Ma\\^amra and Laurence\n  Pilard and Devan Sohier", "title": "Self-Stabilizing Maximal Matching and Anonymous Networks", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-stabilizing algorithm for computing a maximal matching in\nan anonymous network. The complexity is $O(n^3)$ moves with high probability,\nunder the adversarial distributed daemon. In this algorithm, each node can\ndetermine whether one of its neighbors points to it or to another node, leading\nto a contradiction with the anonymous assumption. To solve this problem, we\nprovide under the classical link-register model, a self-stabilizing algorithm\nthat gives a unique name to a link such that this name is shared by both\nextremities of the link.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 09:29:44 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Cohen", "Johanne", ""], ["Lef\u00e8vre", "Jonas", ""], ["Ma\u00e2mra", "Khaled", ""], ["Pilard", "Laurence", ""], ["Sohier", "Devan", ""]]}, {"id": "1611.05793", "submitter": "Aras Atalar", "authors": "Aras Atalar, Paul Renaud-Goud, Philippas Tsigas", "title": "How Lock-free Data Structures Perform in Dynamic Environments: Models\n  and Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": "2016:10", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two analytical frameworks for calculating the\nperformance of lock-free data structures. Lock-free data structures are based\non retry loops and are called by application-specific routines. In contrast to\nprevious work, we consider in this paper lock-free data structures in dynamic\nenvironments. The size of each of the retry loops, and the size of the\napplication routines invoked in between, are not constant but may change\ndynamically. The new frameworks follow two different approaches. The first\nframework, the simplest one, is based on queuing theory. It introduces an\naverage-based approach that facilitates a more coarse-grained analysis, with\nthe benefit of being ignorant of size distributions. Because of this\nindependence from the distribution nature it covers a set of complicated\ndesigns. The second approach, instantiated with an exponential distribution for\nthe size of the application routines, uses Markov chains, and is tighter\nbecause it constructs stochastically the execution, step by step.\n  Both frameworks provide a performance estimate which is close to what we\nobserve in practice. We have validated our analysis on (i) several fundamental\nlock-free data structures such as stacks, queues, deques and counters, some of\nthem employing helping mechanisms, and (ii) synthetic tests covering a wide\nrange of possible lock-free designs. We show the applicability of our results\nby introducing new back-off mechanisms, tested in application contexts, and by\ndesigning an efficient memory management scheme that typical lock-free\nalgorithms can utilize.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 17:25:42 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Atalar", "Aras", ""], ["Renaud-Goud", "Paul", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1611.06038", "submitter": "Laurence Pilard", "authors": "Johanne Cohen and Khaled Ma\\^amra and George Manoussakis and Laurence\n  Pilard", "title": "Polynomial self-stabilizing algorithm and proof for a 2/3-approximation\n  of a maximum matching", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first polynomial self-stabilizing algorithm for finding a\n$\\frac23$-approximation of a maximum matching in a general graph. The previous\nbest known algorithm has been presented by Manne \\emph{et al.}\n\\cite{ManneMPT11} and has a sub-exponential time complexity under the\ndistributed adversarial daemon \\cite{Coor}. Our new algorithm is an adaptation\nof the Manne \\emph{et al.} algorithm and works under the same daemon, but with\na time complexity in $O(n^3)$ moves. Moreover, our algorithm only needs one\nmore boolean variable than the previous one, thus as in the Manne \\emph{et al.}\nalgorithm, it only requires a constant amount of memory space (three\nidentifiers and $two$ booleans per node).\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 10:50:23 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Cohen", "Johanne", ""], ["Ma\u00e2mra", "Khaled", ""], ["Manoussakis", "George", ""], ["Pilard", "Laurence", ""]]}, {"id": "1611.06115", "submitter": "Ana Sovic Krzic", "authors": "Janja Paliska Soldo and Ana Sovic Krzic and and Damir Sersic", "title": "Fast low-level pattern matching algorithm", "comments": "14 pages, 7 tables This work has been fully supported by Croatian\n  Science Foundation under the project UIP-11-2013-7353 Algorithms for Genome\n  Sequence Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on pattern matching in the DNA sequence. It was inspired\nby a previously reported method that proposes encoding both pattern and\nsequence using prime numbers. Although fast, the method is limited to rather\nsmall pattern lengths, due to computing precision problem. Our approach\nsuccessfully deals with large patterns, due to our implementation that uses\nmodular arithmetic. In order to get the results very fast, the code was adapted\nfor multithreading and parallel implementations. The method is reduced to\nassembly language level instructions, thus the final result shows significant\ntime and memory savings compared to the reference algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:05:30 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Soldo", "Janja Paliska", ""], ["Krzic", "Ana Sovic", ""], ["Sersic", "and Damir", ""]]}, {"id": "1611.06172", "submitter": "Shihao Ji", "authors": "Shihao Ji, Nadathur Satish, Sheng Li, and Pradeep Dubey", "title": "Parallelizing Word2Vec in Multi-Core and Many-Core Architectures", "comments": "NIPS Workshop on Efficient Methods for Deep Neural Networks (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2vec is a widely used algorithm for extracting low-dimensional vector\nrepresentations of words. State-of-the-art algorithms including those by\nMikolov et al. have been parallelized for multi-core CPU architectures, but are\nbased on vector-vector operations with \"Hogwild\" updates that are\nmemory-bandwidth intensive and do not efficiently use computational resources.\nIn this paper, we propose \"HogBatch\" by improving reuse of various data\nstructures in the algorithm through the use of minibatching and negative sample\nsharing, hence allowing us to express the problem using matrix multiply\noperations. We also explore different techniques to distribute word2vec\ncomputation across nodes in a compute cluster, and demonstrate good strong\nscalability up to 32 nodes. The new algorithm is particularly suitable for\nmodern multi-core/many-core architectures, especially Intel's latest Knights\nLanding processors, and allows us to scale up the computation near linearly\nacross cores and nodes, and process hundreds of millions of words per second,\nwhich is the fastest word2vec implementation to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:47:44 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 19:20:25 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Ji", "Shihao", ""], ["Satish", "Nadathur", ""], ["Li", "Sheng", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1611.06213", "submitter": "Wei Zhang", "authors": "Wei Zhang, Minwei Feng, Yunhui Zheng, Yufei Ren, Yandong Wang, Ji Liu,\n  Peng Liu, Bing Xiang, Li Zhang, Bowen Zhou, Fei Wang", "title": "GaDei: On Scale-up Training As A Service For Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) training-as-a-service (TaaS) is an important emerging\nindustrial workload. The unique challenge of TaaS is that it must satisfy a\nwide range of customers who have no experience and resources to tune DL\nhyper-parameters, and meticulous tuning for each user's dataset is\nprohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with\nvalues that are applicable to all users. IBM Watson Natural Language Classifier\n(NLC) service, the most popular IBM cognitive service used by thousands of\nenterprise-level clients around the globe, is a typical TaaS service. By\nevaluating the NLC workloads, we show that only the conservative\nhyper-parameter setup (e.g., small mini-batch size and small learning rate) can\nguarantee acceptable model accuracy for a wide range of customers. We further\njustify theoretically why such a setup guarantees better model convergence in\ngeneral. Unfortunately, the small mini-batch size causes a high volume of\ncommunication traffic in a parameter-server based system. We characterize the\nhigh communication bandwidth requirement of TaaS using representative\nindustrial deep learning workloads and demonstrate that none of the\nstate-of-the-art scale-up or scale-out solutions can satisfy such a\nrequirement. We then present GaDei, an optimized shared-memory based scale-up\nparameter server design. We prove that the designed protocol is deadlock-free\nand it processes each gradient exactly once. Our implementation is evaluated on\nboth commercial benchmarks and public benchmarks to demonstrate that it\nsignificantly outperforms the state-of-the-art parameter-server based\nimplementation while maintaining the required accuracy and our implementation\nreaches near the best possible runtime performance, constrained only by the\nhardware limitation. Furthermore, to the best of our knowledge, GaDei is the\nonly scale-up DL system that provides fault-tolerance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:06:27 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 20:30:19 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Zhang", "Wei", ""], ["Feng", "Minwei", ""], ["Zheng", "Yunhui", ""], ["Ren", "Yufei", ""], ["Wang", "Yandong", ""], ["Liu", "Ji", ""], ["Liu", "Peng", ""], ["Xiang", "Bing", ""], ["Zhang", "Li", ""], ["Zhou", "Bowen", ""], ["Wang", "Fei", ""]]}, {"id": "1611.06309", "submitter": "Gilesh M P Gilesh M P", "authors": "M P Gilesh", "title": "Towards a Complete Framework for Virtual Data Center Embedding", "comments": "Technical Report, NIT Calicut", "journal-ref": null, "doi": null, "report-no": "CSED-02/2016", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is widely adopted by corporate as well as retail customers to\nreduce the upfront cost of establishing computing infrastructure. However,\nswitching to the cloud based services poses a multitude of questions, both for\ncustomers and for data center owners. In this work, we propose an algorithm for\noptimal placement of multiple virtual data centers on a physical data center.\nOur algorithm has two modes of operation - an online mode and a batch mode.\nCoordinated batch and online embedding algorithms are used to maximize resource\nusage while fulfilling the QoS demands. Experimental evaluation of our\nalgorithms show that acceptance rate is high - implying higher profit to\ninfrastructure provider. Additionaly, we try to keep a check on the number of\nVM migrations, which can increase operational cost and thus lead to service\nlevel agreement violations.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 05:35:13 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Gilesh", "M P", ""]]}, {"id": "1611.06334", "submitter": "Udayanga Wickramasinghe", "authors": "Udayanga Wickramasinghe, Andrew Lumsdaine", "title": "A Survey of Methods for Collective Communication Optimization and Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New developments in HPC technology in terms of increasing computing power on\nmulti/many core processors, high-bandwidth memory/IO subsystems and\ncommunication interconnects, pose a direct impact on software and runtime\nsystem development. These advancements have become useful in producing\nhigh-performance collective communication interfaces that integrate efficiently\non a wide variety of platforms and environments. However, number of\noptimization options that shows up with each new technology or software\nframework has resulted in a \\emph{combinatorial explosion} in feature space for\ntuning collective parameters such that finding the optimal set has become a\nnearly impossible task. Applicability of algorithmic choices available for\noptimizing collective communication depends largely on the scalability\nrequirement for a particular usecase. This problem can be further exasperated\nby any requirement to run collective problems at very large scales such as in\nthe case of exascale computing, at which impractical tuning by brute force may\nrequire many months of resources. Therefore application of statistical, data\nmining and artificial Intelligence or more general hybrid learning models seems\nessential in many collectives parameter optimization problems. We hope to\nexplore current and the cutting edge of collective communication optimization\nand tuning methods and culminate with possible future directions towards this\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 10:02:40 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wickramasinghe", "Udayanga", ""], ["Lumsdaine", "Andrew", ""]]}, {"id": "1611.06343", "submitter": "Suman Sourav", "authors": "Suman Sourav, Peter Robinson, Seth Gilbert", "title": "Slow links, fast links, and the cost of gossip", "comments": null, "journal-ref": "in IEEE Transactions on Parallel and Distributed Systems, vol. 30,\n  no. 9, pp. 2130-2147, 1 Sept. 2019", "doi": "10.1109/TPDS.2019.2905568", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the classical problem of information dissemination: one (or more)\nnodes in a network have some information that they want to distribute to the\nremainder of the network. In this paper, we study the cost of information\ndissemination in networks where edges have latencies, i.e., sending a message\nfrom one node to another takes some amount of time. We first generalize the\nidea of conductance to weighted graphs by defining $\\phi_*$ to be the \"critical\nconductance\" and $\\ell_*$ to be the \"critical latency\". One goal of this paper\nis to argue that $\\phi_*$ characterizes the connectivity of a weighted graph\nwith latencies in much the same way that conductance characterizes the\nconnectivity of unweighted graphs.\n  We give near tight lower and upper bounds on the problem of information\ndissemination, up to polylogarithmic factors. Specifically, we show that in a\ngraph with (weighted) diameter $D$ (with latencies as weights) and maximum\ndegree $\\Delta$, any information dissemination algorithm requires at least\n$\\Omega(\\min(D+\\Delta, \\ell_*/\\phi_*))$ time % in the worst case. We show\nseveral variants of the lower bound (e.g., for graphs with small diameter,\ngraphs with small max-degree, etc.) by reduction to a simple combinatorial\ngame.\n  We then give nearly matching algorithms, showing that information\ndissemination can be solved in $O(\\min((D+\\Delta)\\log^3{n}, (\\ell_*/\\phi_*)\\log\nn)$ time. This is achieved by combining two cases. We show that the classical\npush-pull algorithm is (near) optimal when the diameter or the maximum degree\nis large. For the case where the diameter and the maximum degree are small, we\ngive an alternative strategy in which we first discover the latencies and then\nuse an algorithm for known latencies based on a weighted spanner construction.\n(Our algorithms are within polylogarithmic factors of being tight both for\nknown and unknown latencies.)\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 11:35:33 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 20:15:02 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 16:41:23 GMT"}, {"version": "v4", "created": "Sat, 30 Nov 2019 13:36:05 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sourav", "Suman", ""], ["Robinson", "Peter", ""], ["Gilbert", "Seth", ""]]}, {"id": "1611.06365", "submitter": "Rafael Rodriguez-Sanchez", "authors": "Sandra Catal\\'an, Jos\\'e R. Herrero, Enrique S. Quintana-Ort\\'i,\n  Rafael Rodr\\'iguez-S\\'anchez, Robert van de Geijn", "title": "A Case for Malleable Thread-Level Linear Algebra Libraries: The LU\n  Factorization with Partial Pivoting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two novel techniques for overcoming load-imbalance encountered\nwhen implementing so-called look-ahead mechanisms in relevant dense matrix\nfactorizations for the solution of linear systems. Both techniques target the\nscenario where two thread teams are created/activated during the factorization,\nwith each team in charge of performing an independent task/branch of execution.\nThe first technique promotes worker sharing (WS) between the two tasks,\nallowing the threads of the task that completes first to be reallocated for use\nby the costlier task. The second technique allows a fast task to alert the\nslower task of completion, enforcing the early termination (ET) of the second\ntask, and a smooth transition of the factorization procedure into the next\niteration.\n  The two mechanisms are instantiated via a new malleable thread-level\nimplementation of the Basic Linear Algebra Subprograms (BLAS), and their\nbenefits are illustrated via an implementation of the LU factorization with\npartial pivoting enhanced with look-ahead. Concretely, our experimental results\non a six core Intel-Xeon processor show the benefits of combining WS+ET,\nreporting competitive performance in comparison with a task-parallel\nruntime-based solution.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 13:55:29 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Catal\u00e1n", "Sandra", ""], ["Herrero", "Jos\u00e9 R.", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""], ["Rodr\u00edguez-S\u00e1nchez", "Rafael", ""], ["van de Geijn", "Robert", ""]]}, {"id": "1611.06565", "submitter": "David Budden", "authors": "David Budden, Alexander Matveev, Shibani Santurkar, Shraman Ray\n  Chaudhuri and Nir Shavit", "title": "Deep Tensor Convolution on Multicores", "comments": "11 pages, 4 figures, 1 supplementary doc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow\njoint modeling of spatiotemporal features. These networks have improved\nperformance of video and volumetric image analysis, but have been limited in\nsize due to the low memory ceiling of GPU hardware. Existing CPU\nimplementations overcome this constraint but are impractically slow. Here we\nextend and optimize the faster Winograd-class of convolutional algorithms to\nthe $N$-dimensional case and specifically for CPU hardware. First, we remove\nthe need to manually hand-craft algorithms by exploiting the relaxed\nconstraints and cheap sparse access of CPU memory. Second, we maximize CPU\nutilization and multicore scalability by transforming data matrices to be\ncache-aware, integer multiples of AVX vector widths. Treating 2-dimensional\nConvNets as a special (and the least beneficial) case of our approach, we\ndemonstrate a 5 to 25-fold improvement in throughput compared to previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 18:41:48 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 15:01:13 GMT"}, {"version": "v3", "created": "Sun, 11 Jun 2017 15:29:16 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Budden", "David", ""], ["Matveev", "Alexander", ""], ["Santurkar", "Shibani", ""], ["Chaudhuri", "Shraman Ray", ""], ["Shavit", "Nir", ""]]}, {"id": "1611.06576", "submitter": "Ying Sun", "authors": "Ying Sun and Gesualdo Scutari", "title": "Distributed Nonconvex Optimization for Sparse Representation", "comments": "Submitted to ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-convex constrained Lagrangian formulation of a fundamental\nbi-criteria optimization problem for variable selection in statistical\nlearning; the two criteria are a smooth (possibly) nonconvex loss function,\nmeasuring the fitness of the model to data, and the latter function is a\ndifference-of-convex (DC) regularization, employed to promote some extra\nstructure on the solution, like sparsity. This general class of nonconvex\nproblems arises in many big-data applications, from statistical machine\nlearning to physical sciences and engineering. We develop the first unified\ndistributed algorithmic framework for these problems and establish its\nasymptotic convergence to d-stationary solutions. Two key features of the\nmethod are: i) it can be implemented on arbitrary networks (digraphs) with\n(possibly) time-varying connectivity; and ii) it does not require the\nrestrictive assumption that the (sub)gradient of the objective function is\nbounded, which enlarges significantly the class of statistical learning\nproblems that can be solved with convergence guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 19:31:42 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "1611.06796", "submitter": "Arief Wicaksana", "authors": "Arief Wicaksana (TIMA), Alban Bourge (TIMA), Olivier Muller (TIMA),\n  Fr\\'ed\\'eric Rousseau (TIMA)", "title": "Demonstration of a context-switch method for heterogeneous\n  reconfigurable systems", "comments": null, "journal-ref": "2016 26th International Conference on Field Programmable Logic and\n  Applications (FPL), Aug 2016, Lausanne, Switzerland. pp.1 - 1, 2016", "doi": "10.1109/FPL.2016.7577384", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, FPGAs are integrated in high-performance computing systems,\nservers, or even used as accelerators in System-on-Chip (SoC) platforms. Since\nthe execution is performed in hardware, FPGA gives much higher performance and\nlower energy consumption compared to most microprocessor-based systems.\nHowever, the room to improve FPGA performance still exists, e.g. when it is\nused by multiple users. In multi-user approaches, FPGA resources are shared\nbetween several users. Therefore, one must be able to interrupt a running\ncircuit at any given time and continue the task at will. An image of the state\nof the running circuit (context) is saved during interruption and restored when\nthe execution is continued. The ability to extract and restore the context is\nknown as context-switch.In the previous work [1], an automatic checkpoint\nselection method is proposed for circuit generation targeting reconfigurable\nsystems. The method relies on static analysis of the finite state machine of a\ncircuit to select the checkpoint states. States with minimum overhead will be\nselected as checkpoints, which allow optimal context save and restore. The\nmaximum time to reach a checkpoint will be defined by the user and consideredas\nthe context-switch latency. The method is implemented in C code and integrated\nas plugin in a free and open-source High-Level Synthesis tool AUGH [2].\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 14:15:51 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wicaksana", "Arief", "", "TIMA"], ["Bourge", "Alban", "", "TIMA"], ["Muller", "Olivier", "", "TIMA"], ["Rousseau", "Fr\u00e9d\u00e9ric", "", "TIMA"]]}, {"id": "1611.06816", "submitter": "Adem Efe Gencer", "authors": "Adem Efe Gencer, Robbert van Renesse, Emin G\\\"un Sirer", "title": "Service-Oriented Sharding with Aspen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of blockchain-based cryptocurrencies has led to an explosion of\nservices using distributed ledgers as their underlying infrastructure. However,\ndue to inherently single-service oriented blockchain protocols, such services\ncan bloat the existing ledgers, fail to provide sufficient security, or\ncompletely forego the property of trustless auditability. Security concerns,\ntrust restrictions, and scalability limits regarding the resource requirements\nof users hamper the sustainable development of loosely-coupled services on\nblockchains.\n  This paper introduces Aspen, a sharded blockchain protocol designed to\nsecurely scale with increasing number of services. Aspen shares the same trust\nmodel as Bitcoin in a peer-to-peer network that is prone to extreme churn\ncontaining Byzantine participants. It enables introduction of new services\nwithout compromising the security, leveraging the trust assumptions, or\nflooding users with irrelevant messages.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 14:53:56 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Gencer", "Adem Efe", ""], ["van Renesse", "Robbert", ""], ["Sirer", "Emin G\u00fcn", ""]]}, {"id": "1611.06864", "submitter": "Giuseppe Antonio Di Luna", "authors": "Giuseppe Antonio Di Luna, Paola Flocchini, Taisuke Izumi, Tomoko\n  Izumi, Nicola Santoro, Giovanni Viglietta", "title": "Population Protocols with Faulty Interactions: the Impact of a Leader", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simulating traditional population protocols under\nweaker models of communication, which include one-way interactions (as opposed\nto two-way interactions) and omission faults (i.e., failure by an agent to read\nits partner's state during an interaction), which in turn may be detectable or\nundetectable. We focus on the impact of a leader, and we give a complete\ncharacterization of the models in which the presence of a unique leader in the\nsystem allows the construction of simulators: when simulations are possible, we\ngive explicit protocols; when they are not, we give proofs of impossibility.\nSpecifically, if each agent has only a finite amount of memory, the simulation\nis possible only if there are no omission faults. If agents have an unbounded\namount of memory, the simulation is possible as long as omissions are\ndetectable. If an upper bound on the number of omissions involving the leader\nis known, the simulation is always possible, except in the one-way model in\nwhich one side is unable to detect the interaction.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:08:13 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Di Luna", "Giuseppe Antonio", ""], ["Flocchini", "Paola", ""], ["Izumi", "Taisuke", ""], ["Izumi", "Tomoko", ""], ["Santoro", "Nicola", ""], ["Viglietta", "Giovanni", ""]]}, {"id": "1611.06945", "submitter": "Matthew Moskewicz", "authors": "Matthew W. Moskewicz and Ali Jannesari and Kurt Keutzer", "title": "A Metaprogramming and Autotuning Framework for Deploying Deep Learning\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks (DNNs), have yielded strong results on\na wide range of applications. Graphics Processing Units (GPUs) have been one\nkey enabling factor leading to the current popularity of DNNs. However, despite\nincreasing hardware flexibility and software programming toolchain maturity,\nhigh efficiency GPU programming remains difficult: it suffers from high\ncomplexity, low productivity, and low portability. GPU vendors such as NVIDIA\nhave spent enormous effort to write special-purpose DNN libraries. However, on\nother hardware targets, especially mobile GPUs, such vendor libraries are not\ngenerally available. Thus, the development of portable, open, high-performance,\nenergy-efficient GPU code for DNN operations would enable broader deployment of\nDNN-based algorithms. Toward this end, this work presents a framework to enable\nproductive, high-efficiency GPU programming for DNN computations across\nhardware platforms and programming models. In particular, the framework\nprovides specific support for metaprogramming, autotuning, and DNN-tailored\ndata types. Using our framework, we explore implementing DNN operations on\nthree different hardware targets: NVIDIA, AMD, and Qualcomm GPUs. On NVIDIA\nGPUs, we show both portability between OpenCL and CUDA as well competitive\nperformance compared to the vendor library. On Qualcomm GPUs, we show that our\nframework enables productive development of target-specific optimizations, and\nachieves reasonable absolute performance. Finally, On AMD GPUs, we show initial\nresults that indicate our framework can yield reasonable performance on a new\nplatform with minimal effort.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:49:23 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Moskewicz", "Matthew W.", ""], ["Jannesari", "Ali", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1611.07083", "submitter": "Erik Schnetter", "authors": "Pekka J\\\"a\\\"askel\\\"ainen, Carlos S\\'anchez de La Lama, Erik Schnetter,\n  Kalle Raiskila, Jarmo Takala, Heikki Berg", "title": "pocl: A Performance-Portable OpenCL Implementation", "comments": "This article was published in 2015; it is now openly accessible via\n  arxiv", "journal-ref": "Int J Parallel Prog (2015) 43: 752", "doi": "10.1007/s10766-014-0320-y", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenCL is a standard for parallel programming of heterogeneous systems. The\nbenefits of a common programming standard are clear; multiple vendors can\nprovide support for application descriptions written according to the standard,\nthus reducing the program porting effort. While the standard brings the obvious\nbenefits of platform portability, the performance portability aspects are\nlargely left to the programmer. The situation is made worse due to multiple\nproprietary vendor implementations with different characteristics, and, thus,\nrequired optimization strategies.\n  In this paper, we propose an OpenCL implementation that is both portable and\nperformance portable. At its core is a kernel compiler that can be used to\nexploit the data parallelism of OpenCL programs on multiple platforms with\ndifferent parallel hardware styles. The kernel compiler is modularized to\nperform target-independent parallel region formation separately from the\ntarget-specific parallel mapping of the regions to enable support for various\nstyles of fine-grained parallel resources such as subword SIMD extensions, SIMD\ndatapaths and static multi-issue. Unlike previous similar techniques that work\non the source level, the parallel region formation retains the information of\nthe data parallelism using the LLVM IR and its metadata infrastructure. This\ndata can be exploited by the later generic compiler passes for efficient\nparallelization.\n  The proposed open source implementation of OpenCL is also platform portable,\nenabling OpenCL on a wide range of architectures, both already commercialized\nand on those that are still under research. The paper describes how the\nportability of the implementation is achieved. Our results show that most of\nthe benchmarked applications when compiled using pocl were faster or close to\nas fast as the best proprietary OpenCL implementation for the platform at hand.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 22:24:58 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["J\u00e4\u00e4skel\u00e4inen", "Pekka", ""], ["de La Lama", "Carlos S\u00e1nchez", ""], ["Schnetter", "Erik", ""], ["Raiskila", "Kalle", ""], ["Takala", "Jarmo", ""], ["Berg", "Heikki", ""]]}, {"id": "1611.07151", "submitter": "Mohammad Motamedi", "authors": "Mohammad Motamedi, Daniel Fong, Soheil Ghiasi", "title": "Fast and Energy-Efficient CNN Inference on IoT Devices", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) exhibit remarkable performance in\nvarious machine learning tasks. As sensor-equipped internet of things (IoT)\ndevices permeate into every aspect of modern life, it is increasingly important\nto run CNN inference, a computationally intensive application, on resource\nconstrained devices. We present a technique for fast and energy-efficient CNN\ninference on mobile SoC platforms, which are projected to be a major player in\nthe IoT space. We propose techniques for efficient parallelization of CNN\ninference targeting mobile GPUs, and explore the underlying tradeoffs.\nExperiments with running Squeezenet on three different mobile devices confirm\nthe effectiveness of our approach. For further study, please refer to the\nproject repository available on our GitHub page:\nhttps://github.com/mtmd/Mobile_ConvNet\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 05:53:22 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Motamedi", "Mohammad", ""], ["Fong", "Daniel", ""], ["Ghiasi", "Soheil", ""]]}, {"id": "1611.07238", "submitter": "Janna Burman", "authors": "James Aspnes (YALE), Joffroy Beauquier (LRI), Janna Burman (LRI),\n  Devan Sohier", "title": "Time and Space Optimal Counting in Population Protocols", "comments": null, "journal-ref": "20th International Conference on Principles of Distributed\n  Systems, OPODIS 2015, Dec 2016, Madrid, Spain. 2016, 20th International\n  Conference on Principles of Distributed Systems, OPODIS 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work concerns the general issue of combined optimality in terms of time\nand space complexity. In this context, we study the problem of (exact) counting\nresource-limited and passively mobile nodes in the model of population\nprotocols, in which the space complexity is crucial. The counted nodes are\nmemory-limited anonymous devices (called agents) communicating asynchronously\nin pairs (according to a fairness condition). Moreover, we assume that these\nagents are prone to failures so that they cannot be correctly initialized. This\nstudy considers two classical fairness conditions, and for each we investigate\nthe issue of time optimality of counting given the optimal space per agent. In\nthe case of randomly interacting agents (probabilistic fairness), as usual, the\nconvergence time is measured in terms of parallel time (or parallel\ninteractions), which is defined as the number of pairwise interactions until\nconvergence, divided by n (the number of agents). In case of weak fairness,\nwhere it is only required that every pair of agents interacts infinitely often,\nthe convergence time is defined in terms of non-null transitions, i.e, the\ntransitions that affect the states of the interacting agents.First, assuming\nprobabilistic fairness, we present a \"non-guessing\" time optimal protocol of\nO(n log n) expected time given an optimal space of only one bit, and we prove\nthe time optimality of this protocol. Then, for weak fairness, we show that a\nspace optimal (semi-uniform) solution cannot converge faster than in\n$\\Omega$(2^n) time (non-null transitions). This result, together with the time\ncomplexity analysis of an already known space optimal protocol, shows that it\nis also optimal in time (given the optimal space constrains).\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:35:28 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Aspnes", "James", "", "YALE"], ["Beauquier", "Joffroy", "", "LRI"], ["Burman", "Janna", "", "LRI"], ["Sohier", "Devan", ""]]}, {"id": "1611.07392", "submitter": "Santosh Aditham", "authors": "Santosh Aditham, Nagarajan Ranganathan and Srinivas Katkoori", "title": "Call Trace and Memory Access Pattern based Runtime Insider Threat\n  Detection for Big Data Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data platforms such as Hadoop and Spark are being widely adopted both by\nacademia and industry. In this paper, we propose a runtime intrusion detection\ntechnique that understands and works according to the properties of such\ndistributed compute platforms. The proposed method is based on runtime analysis\nof system and library calls and memory access patterns of tasks running on the\ndatanodes (slaves). First, the primary datanode of a big data system creates a\nbehavior profile for every task it executes. A behavior profile includes (a)\ntrace of the system & library calls made, and (b) sequence representing the\nsizes of private and shared memory accesses made during task execution. Then,\nthe process behavior profile is shared with other replica datanodes that are\nscheduled to execute the same task on their copy of the same data. Next, these\nreplica datanodes verify their local tasks with the help of the information\nembedded in the received behavior profiles. This is realized in two steps: (i)\ncomparing the system & library calls metadata, and (ii) statistical matching of\nthe memory access patterns. Finally, datanodes share their observations for\nconsensus and report an intrusion to the namenode (master) if they find any\ndiscrepancy. The proposed solution was tested on a small hadoop cluster using\nthe default MapReduce examples and the results show that our approach can\ndetect insider attacks that cannot be detected with the traditional analysis\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 16:23:39 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Aditham", "Santosh", ""], ["Ranganathan", "Nagarajan", ""], ["Katkoori", "Srinivas", ""]]}, {"id": "1611.07511", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Can Broken Multicore Hardware be Mended?", "comments": "3 figures; a Viewpoint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A suggestion is made for mending multicore hardware, which has been diagnosed\nas broken.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 19:14:05 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1611.07555", "submitter": "Peter Richtarik", "authors": "Jakub Kone\\v{c}n\\'y and Peter Richt\\'arik", "title": "Randomized Distributed Mean Estimation: Accuracy vs Communication", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the arithmetic average of a finite\ncollection of real vectors stored in a distributed fashion across several\ncompute nodes subject to a communication budget constraint. Our analysis does\nnot rely on any statistical assumptions about the source of the vectors. This\nproblem arises as a subproblem in many applications, including reduce-all\noperations within algorithms for distributed and federated optimization and\nlearning. We propose a flexible family of randomized algorithms exploring the\ntrade-off between expected communication cost and estimation error. Our family\ncontains the full-communication and zero-error method on one extreme, and an\n$\\epsilon$-bit communication and ${\\cal O}\\left(1/(\\epsilon n)\\right)$ error\nmethod on the opposite extreme. In the special case where we communicate, in\nexpectation, a single bit per coordinate of each vector, we improve upon\nexisting results by obtaining $\\mathcal{O}(r/n)$ error, where $r$ is the number\nof bits used to represent a floating point value.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:18:36 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1611.07556", "submitter": "Li Chen", "authors": "Li Chen, Colin Cunningham, Pooja Jain, Chenggang Qin, Kingsum Chow", "title": "Cultivating Software Performance in Cloud Computing", "comments": null, "journal-ref": "Pacific NW Software Quality Conference 2016", "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist multitudes of cloud performance metrics, including workload\nperformance, application placement, software/hardware optimization,\nscalability, capacity, reliability, agility and so on. In this paper, we\nconsider jointly optimizing the performance of the software applications in the\ncloud. The challenges lie in bringing a diversity of raw data into tidy data\nformat, unifying performance data from multiple systems based on timestamps,\nand assessing the quality of the processed performance data. Even after\nverifying the quality of cloud performance data, additional challenges block\noptimizing cloud computing. In this paper, we identify the challenges of cloud\ncomputing from the perspectives of computing environment, data collection,\nperformance analytics and production environment.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:19:54 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Chen", "Li", ""], ["Cunningham", "Colin", ""], ["Jain", "Pooja", ""], ["Qin", "Chenggang", ""], ["Chow", "Kingsum", ""]]}, {"id": "1611.07619", "submitter": "Xiaoxi Zhang", "authors": "Xiaoxi Zhang and Chuan Wu and Zongpeng Li and Francis C. M. Lau", "title": "A Truthful $(1-\\epsilon)$-Optimal Mechanism for On-demand Cloud Resource\n  Provisioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-demand resource provisioning in cloud computing provides tailor-made\nresource packages (typically in the form of VMs) to meet users' demands. Public\nclouds nowadays provide more and more elaborated types of VMs, but have yet to\noffer the most flexible dynamic VM assembly, which is partly due to the lack of\na mature mechanism for pricing tailor-made VMs on the spot. This work proposes\nan efficient randomized auction mechanism based on a novel application of\nsmoothed analysis and randomized reduction, for dynamic VM provisioning and\npricing in geo-distributed cloud data centers. This auction, to the best of our\nknowledge, is the first one in literature that achieves (i) truthfulness in\nexpectation, (ii) polynomial running time in expectation, and (iii)\n$(1-\\epsilon)$-optimal social welfare in expectation for resource allocation,\nwhere $\\epsilon$ can be arbitrarily close to 0. Our mechanism consists of three\nmodules: (1) an exact algorithm to solve the NP-hard social welfare\nmaximization problem, which runs in polynomial time in expectation, (2) a\nperturbation-based randomized resource allocation scheme which produces a VM\nprovisioning solution that is $(1-\\epsilon)$-optimal, and (3) an auction\nmechanism that applies the perturbation-based scheme for dynamic VM\nprovisioning and prices the customized VMs using a randomized VCG payment, with\na guarantee in truthfulness in expectation. We validate the efficacy of the\nmechanism through careful theoretical analysis and trace-driven simulations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 03:15:06 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Zhang", "Xiaoxi", ""], ["Wu", "Chuan", ""], ["Li", "Zongpeng", ""], ["Lau", "Francis C. M.", ""]]}, {"id": "1611.07623", "submitter": "EPTCS", "authors": "Maaz Bin Safeer Ahmad, Alvin Cheung", "title": "Leveraging Parallel Data Processing Frameworks with Verified Lifting", "comments": "In Proceedings SYNT 2016, arXiv:1611.07178", "journal-ref": "EPTCS 229, 2016, pp. 67-83", "doi": "10.4204/EPTCS.229.7", "report-no": null, "categories": "cs.PL cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many parallel data frameworks have been proposed in recent years that let\nsequential programs access parallel processing. To capitalize on the benefits\nof such frameworks, existing code must often be rewritten to the\ndomain-specific languages that each framework supports. This rewriting-tedious\nand error-prone-also requires developers to choose the framework that best\noptimizes performance given a specific workload.\n  This paper describes Casper, a novel compiler that automatically retargets\nsequential Java code for execution on Hadoop, a parallel data processing\nframework that implements the MapReduce paradigm. Given a sequential code\nfragment, Casper uses verified lifting to infer a high-level summary expressed\nin our program specification language that is then compiled for execution on\nHadoop. We demonstrate that Casper automatically translates Java benchmarks\ninto Hadoop. The translated results execute on average 3.3x faster than the\nsequential implementations and scale better, as well, to larger datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 03:16:38 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Ahmad", "Maaz Bin Safeer", ""], ["Cheung", "Alvin", ""]]}, {"id": "1611.07629", "submitter": "EPTCS", "authors": "Grigory Fedyukovich (UW), Rastislav Bod\\'ik (UW)", "title": "Approaching Symbolic Parallelization by Synthesis of Recurrence\n  Decompositions", "comments": "In Proceedings SYNT 2016, arXiv:1611.07178", "journal-ref": "EPTCS 229, 2016, pp. 55-66", "doi": "10.4204/EPTCS.229.6", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GraSSP, a novel approach to perform automated parallelization\nrelying on recent advances in formal verification and synthesis. GraSSP\naugments an existing sequential program with an additional functionality to\ndecompose data dependencies in loop iterations, to compute partial results, and\nto compose them together. We show that for some classes of the sequential\nprefix sum problems, such parallelization can be performed efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 03:28:09 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Fedyukovich", "Grigory", "", "UW"], ["Bod\u00edk", "Rastislav", "", "UW"]]}, {"id": "1611.07649", "submitter": "Santosh Aditham", "authors": "Santosh Aditham and Nagarajan Ranganathan", "title": "A Novel Control-flow based Intrusion Detection Technique for Big Data\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security and distributed infrastructure are two of the most common\nrequirements for big data software. But the security features of the big data\nplatforms are still premature. It is critical to identify, modify, test and\nexecute some of the existing security mechanisms before using them in the big\ndata world. In this paper, we propose a novel intrusion detection technique\nthat understands and works according to the needs of big data systems. Our\nproposed technique identifies program level anomalies using two methods - a\nprofiling method that models application behavior by creating process\nsignatures from control-flow graphs; and a matching method that checks for\ncoherence among the replica nodes of a big data system by matching the process\nsignatures. The profiling method creates a process signature by reducing the\ncontrol-flow graph of a process to a set of minimum spanning trees and then\ncreates a hash of that set. The matching method first checks for similarity in\nprocess behavior by matching the received process signature with the local\nsignature and then shares the result with all replica datanodes for consensus.\nExperimental results show only 0.8% overhead due to the proposed technique when\ntested on the hadoop map-reduce examples in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 05:24:38 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Aditham", "Santosh", ""], ["Ranganathan", "Nagarajan", ""]]}, {"id": "1611.07819", "submitter": "Steven Eliuk", "authors": "Steven Eliuk, Cameron Upright, Hars Vardhan, Stephen Walsh, Trevor\n  Gale", "title": "dMath: Distributed Linear Algebra for DL", "comments": "5 pages. arXiv admin note: text overlap with arXiv:1604.01416", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a parallel math library, dMath, that demonstrates leading\nscaling when using intranode, internode, and hybrid-parallelism for deep\nlearning (DL). dMath provides easy-to-use distributed primitives and a variety\nof domain-specific algorithms including matrix multiplication, convolutions,\nand others allowing for rapid development of scalable applications like deep\nneural networks (DNNs). Persistent data stored in GPU memory and advanced\nmemory management techniques avoid costly transfers between host and device.\ndMath delivers performance, portability, and productivity to its specific\ndomain of support.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 00:24:12 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Eliuk", "Steven", ""], ["Upright", "Cameron", ""], ["Vardhan", "Hars", ""], ["Walsh", "Stephen", ""], ["Gale", "Trevor", ""]]}, {"id": "1611.08209", "submitter": "Konstantinos Georgiou", "authors": "Jurek Czyzowicz, Konstantinos Georgiou, Evangelos Kranakis, Danny\n  Krizanc, Lata Narayanan, Jaroslav Opatrny, Sunil Shende", "title": "Search on a Line by Byzantine Robots", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fault-tolerant parallel search on an infinite line\nby $n$ robots. Starting from the origin, the robots are required to find a\ntarget at an unknown location. The robots can move with maximum speed $1$ and\ncan communicate in wireless mode among themselves. However, among the $n$\nrobots, there are $f$ robots that exhibit {\\em byzantine faults}. A faulty\nrobot can fail to report the target even after reaching it, or it can make\nmalicious claims about having found the target when in fact it has not. Given\nthe presence of such faulty robots, the search for the target can only be\nconcluded when the non-faulty robots have sufficient verification that the\ntarget has been found. We aim to design algorithms that minimize the value of\n$S_d(n,f)$, the time to find a target at a distance $d$ from the origin by $n$\nrobots among which $f$ are faulty. We give several different algorithms whose\nrunning time depends on the ratio $f/n$, the density of faulty robots, and also\nprove lower bounds. Our algorithms are optimal for some densities of faulty\nrobots.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 15:06:52 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Czyzowicz", "Jurek", ""], ["Georgiou", "Konstantinos", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""], ["Shende", "Sunil", ""]]}, {"id": "1611.08401", "submitter": "Christophe Guyeux", "authors": "Nadine Boudargham, Jacques Bou Abdo, Jacques Demerjian, Christophe\n  Guyeux, Abdallah Makhoul", "title": "Investigating Low Level Protocols for Wireless Body Sensor Networks", "comments": "Accepted to AICCSA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of medical sensors has increased the interest in\nWireless Body Area Network (WBAN) applications where physiological data from\nthe human body and its environment is gathered, monitored, and analyzed to take\nthe proper measures. In WBANs, it is essential to design MAC protocols that\nensure adequate Quality of Service (QoS) such as low delay and high\nscalability. This paper investigates Medium Access Control (MAC) protocols used\nin WBAN, and compares their performance in a high traffic environment. Such\nscenario can be induced in case of emergency for example, where physiological\ndata collected from all sensors on human body should be sent simultaneously to\ntake appropriate action. This study can also be extended to cover collaborative\nWBAN systems where information from different bodies is sent simultaneously\nleading to high traffic. OPNET simulations are performed to compare the delay\nand scalability performance of the different MAC protocols under the same\nexperimental conditions and to draw conclusions about the best protocol to be\nused in a high traffic environment.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 09:52:04 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Boudargham", "Nadine", ""], ["Abdo", "Jacques Bou", ""], ["Demerjian", "Jacques", ""], ["Guyeux", "Christophe", ""], ["Makhoul", "Abdallah", ""]]}, {"id": "1611.08554", "submitter": "Fabian Reiter", "authors": "Fabian Reiter", "title": "Asynchronous Distributed Automata: A Characterization of the Modal\n  Mu-Fragment", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2017.100", "report-no": null, "categories": "cs.FL cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We establish the equivalence between a class of asynchronous distributed\nautomata and a small fragment of least fixpoint logic, when restricted to\nfinite directed graphs. More specifically, the logic we consider is (a variant\nof) the fragment of the modal $\\mu$-calculus that allows least fixpoints but\nforbids greatest fixpoints. The corresponding automaton model uses a network of\nidentical finite-state machines that communicate in an asynchronous manner and\nwhose state diagram must be acyclic except for self-loops. Exploiting the\nconnection with logic, we also prove that the expressive power of those\nmachines is independent of whether or not messages can be lost.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 19:02:23 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 17:40:00 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Reiter", "Fabian", ""]]}, {"id": "1611.08573", "submitter": "Dhanya R. Krishnan", "authors": "Dhanya R Krishnan", "title": "The Marriage of Incremental and Approximate Computing", "comments": "http://dl.acm.org/citation.cfm?id=2883026", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most data analytics systems that require low-latency execution and efficient\nutilization of computing resources, increasingly adopt two computational\nparadigms, namely, incremental and approximate computing. Incremental\ncomputation updates the output incrementally instead of re-computing everything\nfrom scratch for successive runs of a job with input changes. Approximate\ncomputation returns an approximate output for a job instead of the exact\noutput.\n  Both paradigms rely on computing over a subset of data items instead of\ncomputing over the entire dataset, but they differ in their means for skipping\nparts of the computation. Incremental computing relies on the memoization of\nintermediate results of sub-computations, and reusing these memoized results\nacross jobs for sub-computations that are unaffected by the changed input.\nApproximate computing relies on representative sampling of the entire dataset\nto compute over a subset of data items.\n  In this thesis, we make the observation that these two computing paradigms\nare complementary, and can be married together! The high level idea is to:\ndesign a sampling algorithm that biases the sample selection to the memoized\ndata items from previous runs. To concretize this idea, we designed an online\nstratified sampling algorithm that uses self-adjusting computation to produce\nan incrementally updated approximate output with bounded error. We implemented\nour algorithm in a data analytics system called IncAppox based on Apache Spark\nStreaming. Our evaluation of the system shows that IncApprox achieves the\nbenefits of both incremental and approximate computing.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 20:05:08 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Krishnan", "Dhanya R", ""]]}, {"id": "1611.08743", "submitter": "Muhammad Shafique", "authors": "Muhammad Shafique", "title": "Eliminating Tight Coupling using Subscriptions Subgrouping in Structured\n  Overlays", "comments": "None. arXiv admin note: text overlap with arXiv:1604.06853", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertisements and subscriptions are tightly coupled to generate publication\nrouting paths in content--based publish/subscribe systems. Tight coupling\nrequires instantaneous updates in routing tables to generate alternative paths\nwhich prevents offering scalable and robust dynamic routing in cyclic overlays\nwhen link congestion is detected. We propose, OctopiA, first distributed\npublish/subscribe system for content--based inter--cluster dynamic routing\nusing purpose--built structured cyclic overlays. OctopiA uses a novel concept\nof subscription subgrouping, which divides subscriptions into disjoint sets\ncalled subscription subgroups. The purpose--built structured cyclic overlay is\ndivided into identical clusters where subscriptions in each subgroup are\nbroadcast to an exclusive cluster. Our advertisement and subscription\nforwarding algorithms use subscription subgrouping to eliminate tight coupling\nto offer inter--cluster dynamic routing without requiring updates in routing\ntables. Experiments on a cluster testbed with real world data show that OctopiA\nreduces the number of saved advertisements in routing tables by 93%,\nsubscription broadcast delay by 33%, static and dynamic publication delivery\ndelays by 25% and 54%, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 21:24:40 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 18:20:40 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 09:07:05 GMT"}, {"version": "v4", "created": "Sat, 8 Jul 2017 17:02:25 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Shafique", "Muhammad", ""]]}, {"id": "1611.08864", "submitter": "Konjaang James Kok", "authors": "J. Kok Konjaang, J.Y. Maipan-uku, Kumangkem Kennedy Kubuga", "title": "An Efficient Max-Min Resource Allocator and Task Scheduling Algorithm in\n  Cloud Computing Environment", "comments": "6 pages,6 figures, Article published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a new archetype that provides dynamic computing services\nto cloud users through the support of datacenters that employs the services of\ndatacenter brokers which discover resources and assign them Virtually. The\nfocus of this research is to efficiently optimize resource allocation in the\ncloud by exploiting the Max-Min scheduling algorithm and enhancing it to\nincrease efficiency in terms of completion time (makespan). This is key to\nenhancing the performance of cloud scheduling and narrowing the performance gap\nbetween cloud service providers and cloud resources consumers/users. The\ncurrent Max-Min algorithm selects tasks with maximum execution time on a faster\navailable machine or resource that is capable of giving minimum completion\ntime. The concern of this algorithm is to give priority to tasks with maximum\nexecution time first before assigning those with the minimum execution time for\nthe purpose of minimizing makespan. The drawback of this algorithm is that, the\nexecution of tasks with maximum execution time first may increase the makespan,\nand leads to a delay in executing tasks with minimum execution time if the\nnumber of tasks with maximum execution time exceeds that of tasks with minimum\nexecution time, hence the need to improve it to mitigate the delay in executing\ntasks with minimum execution time. CloudSim is used to compare the\neffectiveness of the improved Max-Min algorithm with the traditional one. The\nexperimented results show that the improved algorithm is efficient and can\nproduce better makespan than Max-Min and DataAware.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 15:29:24 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Konjaang", "J. Kok", ""], ["Maipan-uku", "J. Y.", ""], ["Kubuga", "Kumangkem Kennedy", ""]]}, {"id": "1611.08889", "submitter": "Seyed Hossein Ahmadpanah", "authors": "Seyed Hossein Ahmadpanah", "title": "Security Management Model in Cloud Computing Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the cloud computing environment, cloud virtual machine (VM) will be more\nand more the number of virtual machine security and management faced giant\nChallenge. In order to address security issues cloud computing virtualization\nenvironment, this paper presents a virtual machine based on efficient and\ndynamic deployment VM security management model state migration and scheduling,\nstudy of which virtual machine security architecture, based on AHP (Analytic\nHierarchy Process) virtual machine deployment and scheduling method, based on\nCUSUM (Cumulative Sum) DDoS attack detection algorithm, and the above-described\nmethod for functional testing and validation.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 19:09:27 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ahmadpanah", "Seyed Hossein", ""]]}, {"id": "1611.08938", "submitter": "Kokouvi Hounkanli", "authors": "Kokouvi Hounkanli and Andrzej Pelc", "title": "Asynchronous Broadcasting with Bivalent Beeps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In broadcasting, one node of a network has a message that must be learned by\nall other nodes. We study deterministic algorithms for this fundamental\ncommunication task in a very weak model of wireless communication. The only\nsignals sent by nodes are beeps. Moreover, they are delivered to neighbors of\nthe beeping node in an asynchronous way: the time between sending and reception\nis finite but unpredictable. We first observe that under this scenario, no\ncommunication is possible, if beeps are all of the same strength. Hence we\nstudy broadcasting in the bivalent beeping model, where every beep can be\neither soft or loud. At the receiving end, if exactly one soft beep is received\nby a node in a round, it is heard as soft. Any other combination of beeps\nreceived in a round is heard as a loud beep. The cost of a broadcasting\nalgorithm is the total number of beeps sent by all nodes.\n  We consider four levels of knowledge that nodes may have about the network:\nanonymity (no knowledge whatsoever), ad-hoc (all nodes have distinct labels and\nevery node knows only its own label), neighborhood awareness (every node knows\nits label and labels of all neighbors), and full knowledge (every node knows\nthe entire labeled map of the network and the identity of the source). We first\nshow that in the anonymous case, broadcasting is impossible even for very\nsimple networks. For each of the other three knowledge levels we provide upper\nand lower bounds on the minimum cost of a broadcasting algorithm. Our results\nshow separations between all these scenarios. Perhaps surprisingly, the jump in\nbroadcasting cost between the ad-hoc and neighborhood awareness levels is much\nlarger than between the neighborhood awareness and full knowledge levels,\nalthough in the two former levels knowledge of nodes is local, and in the\nlatter it is global.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 23:55:18 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Hounkanli", "Kokouvi", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1611.09048", "submitter": "Alexander Matthes", "authors": "Alexander Matthes (1 and 2), Axel Huebl (1 and 2), Ren\\'e Widera (2),\n  Sebastian Grottel (1), Stefan Gumhold (1), Michael Bussmann (2) ((1)\n  Helmholtz-Zentrum Dresden -- Rossendorf, (2) Technische Universit\\\"at\n  Dresden)", "title": "In situ, steerable, hardware-independent and data-structure agnostic\n  visualization with ISAAC", "comments": null, "journal-ref": "Supercomputing Frontiers and Innovations, [S.l.], v. 3, n. 4, p.\n  30-48, oct. 2016", "doi": "10.14529/jsfi160403", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation power of supercomputers grows faster than the bandwidth of\ntheir storage and network. Especially applications using hardware accelerators\nlike Nvidia GPUs cannot save enough data to be analyzed in a later step. There\nis a high risk of loosing important scientific information. We introduce the in\nsitu template library ISAAC which enables arbitrary applications like\nscientific simulations to live visualize their data without the need of deep\ncopy operations or data transformation using the very same compute node and\nhardware accelerator the data is already residing on. Arbitrary meta data can\nbe added to the renderings and user defined steering commands can be\nasynchronously sent back to the running application. Using an aggregating\nserver, ISAAC streams the interactive visualization video and enables user to\naccess their applications from everywhere.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 10:21:24 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Matthes", "Alexander", "", "1 and 2"], ["Huebl", "Axel", "", "1 and 2"], ["Widera", "Ren\u00e9", ""], ["Grottel", "Sebastian", ""], ["Gumhold", "Stefan", ""], ["Bussmann", "Michael", ""]]}, {"id": "1611.09168", "submitter": "Ivano Notarnicola", "authors": "Ivano Notarnicola, Mauro Franceschelli, Giuseppe Notarstefano", "title": "A duality-based approach for distributed min-max optimization", "comments": "Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a distributed optimization scenario in which a set\nof processors aims at cooperatively solving a class of min-max optimization\nproblems. This set-up is motivated by peak-demand minimization problems in\nsmart grids. Here, the goal is to minimize the peak value over a finite horizon\nwith: (i) the demand at each time instant being the sum of contributions from\ndifferent devices, and (ii) the device states at different time instants being\ncoupled through local constraints (e.g., the dynamics). The min-max structure\nand the double coupling (through the devices and over the time horizon) makes\nthis problem challenging in a distributed set-up (e.g., existing distributed\ndual decomposition approaches cannot be applied). We propose a distributed\nalgorithm based on the combination of duality methods and properties from\nmin-max optimization. Specifically, we repeatedly apply duality theory and\nproperly introduce ad-hoc slack variables in order to derive a series of\nequivalent problems. On the resulting problem we apply a dual subgradient\nmethod, which turns out to be a distributed algorithm consisting of a\nminimization on the original primal variables and a suitable dual update. We\nprove the convergence of the proposed algorithm in objective value. Moreover,\nwe show that every limit point of the primal sequence is an optimal (feasible)\nsolution. Finally, we provide numerical computations for a peak-demand\noptimization problem in a network of thermostatically controlled loads.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:15:32 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Notarnicola", "Ivano", ""], ["Franceschelli", "Mauro", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1611.09528", "submitter": "Francesco Pace", "authors": "Francesco Pace, Daniele Venzano, Damiano Carra, Pietro Michiardi", "title": "Flexible Scheduling of Distributed Analytic Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work addresses the problem of scheduling user-defined analytic\napplications, which we define as high-level compositions of frameworks, their\ncomponents, and the logic necessary to carry out work. The key idea in our\napplication definition, is to distinguish classes of components, including\nrigid and elastic types: the first being required for an application to make\nprogress, the latter contributing to reduced execution times. We show that the\nproblem of scheduling such applications poses new challenges, which existing\napproaches address inefficiently.\n  Thus, we present the design and evaluation of a novel, flexible heuristic to\nschedule analytic applications, that aims at high system responsiveness, by\nallocating resources efficiently. Our algorithm is evaluated using trace-driven\nsimulations, with large-scale real system traces: our flexible scheduler\noutperforms a baseline approach across a variety of metrics, including\napplication turnaround times, and resource allocation efficiency.\n  We also present the design and evaluation of a full-fledged system, which we\nhave called Zoe, that incorporates the ideas presented in this paper, and\nreport concrete improvements in terms of efficiency and performance, with\nrespect to prior generations of our system.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 08:53:17 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 09:35:49 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 13:10:29 GMT"}, {"version": "v4", "created": "Wed, 7 Dec 2016 15:15:35 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Pace", "Francesco", ""], ["Venzano", "Daniele", ""], ["Carra", "Damiano", ""], ["Michiardi", "Pietro", ""]]}, {"id": "1611.09569", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Server Structure Proposal and Automatic Verification Technology on IaaS\n  Cloud of Plural Type Servers", "comments": "13 pages, 9 figures, International Conference on Internet Studies\n  (NETs2015), July 2015", "journal-ref": "International Conference on Internet Studies (NETs2015), July 2015", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a server structure proposal and automatic\nperformance verification technology which proposes and verifies an appropriate\nserver structure on Infrastructure as a Service (IaaS) cloud with baremetal\nservers, container based virtual servers and virtual machines. Recently, cloud\nservices have been progressed and providers provide not only virtual machines\nbut also baremetal servers and container based virtual servers. However, users\nneed to design an appropriate server structure for their requirements based on\n3 types quantitative performances and users need much technical knowledge to\noptimize their system performances. Therefore, we study a technology which\nsatisfies users' performance requirements on these 3 types IaaS cloud. Firstly,\nwe measure performances of a baremetal server, Docker containers, KVM (Kernel\nbased Virtual Machine) virtual machines on OpenStack with virtual server number\nchanging. Secondly, we propose a server structure proposal technology based on\nthe measured quantitative data. A server structure proposal technology receives\nan abstract template of OpenStack Heat and function/performance requirements\nand then creates a concrete template with server specification information.\nThirdly, we propose an automatic performance verification technology which\nexecutes necessary performance tests automatically on provisioned user\nenvironments according to the template.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 11:18:12 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "1611.09570", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Proposal of Optimum Application Deployment Technology for Heterogeneous\n  IaaS Cloud", "comments": "4 pages, 1 figure, 2016 6th International Workshop on Computer\n  Science and Engineering (WCSE 2016), June 2016", "journal-ref": "2016 6th International Workshop on Computer Science and\n  Engineering (WCSE 2016), pp.34-37, June 2016. (c) 2016 WCSE2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, cloud systems composed of heterogeneous hardware have been\nincreased to utilize progressed hardware power. However, to program\napplications for heterogeneous hardware to achieve high performance needs much\ntechnical skill and is difficult for users. Therefore, to achieve high\nperformance easily, this paper proposes a PaaS which analyzes application\nlogics and offloads computations to GPU and FPGA automatically when users\ndeploy applications to clouds.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 11:18:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "1611.09774", "submitter": "Josiah McClurg", "authors": "Josiah McClurg and Raghuraman Mudumbai", "title": "Serving the Grid: an Experimental Study of Server Clusters as Real-Time\n  Demand Response Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand response is a crucial technology to allow large-scale penetration of\nintermittent renewable energy sources in the electric grid. This paper is based\non the thesis that datacenters represent especially attractive candidates for\nproviding flexible, real-time demand response services to the grid; they are\ncapable of finely-controllable power consumption, fast power ramp-rates, and\nlarge dynamic range. This paper makes two main contributions: (a) it provides\ndetailed experimental evidence justifying this thesis, and (b) it presents a\ncomparative investigation of three candidate software interfaces for power\ncontrol within the servers. All of these results are based on a series of\nexperiments involving real-time power measurements on a lab-scale server\ncluster. This cluster was specially instrumented for accurate and fast power\nmeasurements on a time-scale of 100 ms or less. Our results provide preliminary\nevidence for the feasibility of large scale demand response using datacenters,\nand motivates future work on exploiting this capability.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 18:39:04 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["McClurg", "Josiah", ""], ["Mudumbai", "Raghuraman", ""]]}, {"id": "1611.09944", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Yoshifumi Fukumoto and Hiroki Kumazaki", "title": "Proposal of Real Time Predictive Maintenance Platform with 3D Printer\n  for Business Vehicles", "comments": "5 pages, 3 figures, 5th International Conference on Software and\n  Information Engineering (ICSIE 2016), May 2016", "journal-ref": "5th International Conference on Software and Information\n  Engineering (ICSIE 2016), pp.6-10, May 2016. (c) 2016 ICSIE2016", "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a maintenance platform for business vehicles which\ndetects failure sign using IoT data on the move, orders to create repair parts\nby 3D printers and to deliver them to the destination. Recently, IoT and 3D\nprinter technologies have been progressed and application cases to\nmanufacturing and maintenance have been increased. Especially in air flight\nindustry, various sensing data are collected during flight by IoT technologies\nand parts are created by 3D printers. And IoT platforms which improve\ndevelopment/operation of IoT applications also have been appeared. However,\nexisting IoT platforms mainly targets to visualize \"things\" statuses by batch\nprocessing of collected sensing data, and 3 factors of real-time, automatic\norders of repair parts and parts stock cost are insufficient to accelerate\nbusinesses. This paper targets maintenance of business vehicles such as\nairplane or high-speed bus. We propose a maintenance platform with real-time\nanalysis, automatic orders of repair parts and minimum stock cost of parts. The\nproposed platform collects data via closed VPN, analyzes stream data and\npredicts failures in real-time by online machine learning framework Jubatus,\ncoordinates ERP or SCM via in memory DB to order repair parts and also\ndistributes repair parts data to 3D printers to create repair parts near the\ndestination.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 23:50:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yamato", "Yoji", ""], ["Fukumoto", "Yoshifumi", ""], ["Kumazaki", "Hiroki", ""]]}, {"id": "1611.10052", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Sindhu Padakandla, Chandrashekar L, Priyank Parihar, K\n  Gopinath, Shalabh Bhatnagar", "title": "Performance Tuning of Hadoop MapReduce: A Noisy Gradient Approach", "comments": null, "journal-ref": null, "doi": "10.1109/CLOUD.2017.55", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hadoop MapReduce is a framework for distributed storage and processing of\nlarge datasets that is quite popular in big data analytics. It has various\nconfiguration parameters (knobs) which play an important role in deciding the\nperformance i.e., the execution time of a given big data processing job.\nDefault values of these parameters do not always result in good performance and\nhence it is important to tune them. However, there is inherent difficulty in\ntuning the parameters due to two important reasons - firstly, the parameter\nsearch space is large and secondly, there are cross-parameter interactions.\nHence, there is a need for a dimensionality-free method which can automatically\ntune the configuration parameters by taking into account the cross-parameter\ndependencies. In this paper, we propose a novel Hadoop parameter tuning\nmethodology, based on a noisy gradient algorithm known as the simultaneous\nperturbation stochastic approximation (SPSA). The SPSA algorithm tunes the\nparameters by directly observing the performance of the Hadoop MapReduce\nsystem. The approach followed is independent of parameter dimensions and\nrequires only $2$ observations per iteration while tuning. We demonstrate the\neffectiveness of our methodology in achieving good performance on popular\nHadoop benchmarks namely \\emph{Grep}, \\emph{Bigram}, \\emph{Inverted Index},\n\\emph{Word Co-occurrence} and \\emph{Terasort}. Our method, when tested on a 25\nnode Hadoop cluster shows 66\\% decrease in execution time of Hadoop jobs on an\naverage, when compared to the default configuration. Further, we also observe a\nreduction of 45\\% in execution times, when compared to prior methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 08:52:11 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 09:45:04 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Kumar", "Sandeep", ""], ["Padakandla", "Sindhu", ""], ["L", "Chandrashekar", ""], ["Parihar", "Priyank", ""], ["Gopinath", "K", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1611.10068", "submitter": "Adva Zair", "authors": "Danny Dolev, Michael Erdmann, Neil Lutz, Michael Schapira and Adva\n  Zair", "title": "Stateless Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and explore a model of stateless and self-stabilizing distributed\ncomputation, inspired by real-world applications such as routing on today's\nInternet. Processors in our model do not have an internal state, but rather\ninteract by repeatedly mapping incoming messages (\"labels\") to outgoing\nmessages and output values. While seemingly too restrictive to be of interest,\nstateless computation encompasses both classical game-theoretic notions of\nstrategic interaction and a broad range of practical applications (e.g.,\nInternet protocols, circuits, diffusion of technologies in social networks). We\nembark on a holistic exploration of stateless computation. We tackle two\nimportant questions: (1) Under what conditions is self-stabilization, i.e.,\nguaranteed \"convergence\" to a \"legitimate\" global configuration, achievable for\nstateless computation? and (2) What is the computational power of stateless\ncomputation? Our results for self-stabilization include a general necessary\ncondition for self-stabilization and hardness results for verifying that a\nstateless protocol is self-stabilizing. Our main results for the power of\nstateless computation show that labels of logarithmic length in the number of\nprocessors yield substantial computational power even on ring topologies. We\npresent a separation between unidirectional and bidirectional rings (L/poly vs.\nP/poly), reflecting the sequential nature of computation on a unidirectional\nring, as opposed to the parallelism afforded by the bidirectional ring. We\nleave the reader with many exciting directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 09:56:15 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Dolev", "Danny", ""], ["Erdmann", "Michael", ""], ["Lutz", "Neil", ""], ["Schapira", "Michael", ""], ["Zair", "Adva", ""]]}, {"id": "1611.10204", "submitter": "Ruby Annette", "authors": "Annette J Ruby, Banu W Aisha and Chandran P Subash", "title": "Comparison of Multi Criteria Decision Making Algorithms for Ranking\n  Cloud Renderfarm Services", "comments": "5 pages", "journal-ref": "Indian Journal of Science and Technology, Vol. 9, No.31, 2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud services that provide a complete environment for the animators to\nrender their files using the resources in the cloud are called Cloud Renderfarm\nServices. The objective of this work is to rank and compare the performance of\nthese services using two popular Multi Criteria Decision Making (MCDM)\nAlgorithms namely the Analytical Hierarchical Processing (AHP) and SAW (Simple\nAdditive Weighting) methods. The performance of three real time cloud\nrenderfarm services are ranked and compared based on five Quality of Service\n(QoS) attributes that are important to these services namely the Render Node\nCost, File Upload Time, Availability, Elasticity and Service Response Time. The\nperformance of these cloud renderfarm services are ranked in four different\nsimulations by varying the weights assigned for each QoS attribute and the\nranking obtained are compared. The results show that AHP and SAW assigned\nsimilar ranks to all three cloud renderfarm services for all simulations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 05:27:47 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Ruby", "Annette J", ""], ["Aisha", "Banu W", ""], ["Subash", "Chandran P", ""]]}, {"id": "1611.10210", "submitter": "Ruby Annette", "authors": "Annette J Ruby, Banu W Aisha and Chandran P Subash", "title": "RenderSelect: a Cloud Broker Framework for Cloud Renderfarm Services", "comments": "13 pages, 10 figures", "journal-ref": "International Journal of Applied Engineering Research ,Vol.10,\n  No.20 ,2015", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 3D studios the animation scene files undergo a process called as\nrendering, where the 3D wire frame models are converted into 3D photorealistic\nimages. As the rendering process is both a computationally intensive and a time\nconsuming task, the cloud services based rendering in cloud render farms is\ngaining popularity among the animators. Though cloud render farms offer many\nbenefits, the animators hesitate to move from their traditional offline\nrendering to cloud services based render farms as they lack the knowledge,\nexpertise and the time to compare the render farm service providers based on\nthe Quality of Service (QoS) offered by them, negotiate the QoS and monitor\nwhether the agreed upon QoS is actually offered by the renderfarm service\nproviders. In this paper we propose a Cloud Service Broker (CSB) framework\ncalled the RenderSelect that helps in the dynamic ranking, selection,\nnegotiation and monitoring of the cloud based render farm services. The cloud\nservices based renderfarms are ranked and selected services based on multi\ncriteria QoS requirements. Analytical Hierarchical Process (AHP), the popular\nMulti Criteria Decision Making (MCDM) method is used for ranking and selecting\nthe cloud services based renderfarms. The AHP method of ranking is illustrated\nin detail with an example. It could be verified that AHP method ranks the cloud\nservices effectively with less time and complexity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 04:34:10 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Ruby", "Annette J", ""], ["Aisha", "Banu W", ""], ["Subash", "Chandran P", ""]]}, {"id": "1611.10338", "submitter": "Reyhane Askari Hemmat", "authors": "Reyhane Askari Hemmat, Abdelhakim Hafid", "title": "SLA Violation Prediction In Cloud Computing: A Machine Learning\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service level agreement (SLA) is an essential part of cloud systems to ensure\nmaximum availability of services for customers. With a violation of SLA, the\nprovider has to pay penalties. In this paper, we explore two machine learning\nmodels: Naive Bayes and Random Forest Classifiers to predict SLA violations.\nSince SLA violations are a rare event in the real world (~0.2 %), the\nclassification task becomes more challenging. In order to overcome these\nchallenges, we use several re-sampling methods. We find that random forests\nwith SMOTE-ENN re-sampling have the best performance among other methods with\nthe accuracy of 99.88 % and F_1 score of 0.9980.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:07:34 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Hemmat", "Reyhane Askari", ""], ["Hafid", "Abdelhakim", ""]]}]