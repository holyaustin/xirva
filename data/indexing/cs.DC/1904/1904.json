[{"id": "1904.00044", "submitter": "Chen Yuan", "authors": "Yiting Zhao, Chen Yuan, Sun Li, Guangyi Liu, Renchang Dai, Zhiwei Wang", "title": "Graph Computing based Fast Screening in Contingency Analysis", "comments": "6 pages, 9 figures, 6 tables, accepted by IEEE PES ISGT ASIA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During last decades, contingency analysis has been facing challenges from\nsignificant load demand increase and high penetrations of intermittent\nrenewable energy, fluctuant responsive loads and non-linear power electronic\ninterfaces. It requires an advanced approach for high-performance contingency\nanalysis as a safeguard of the power system operation. In this paper, a\ngraph-based method is employed for N-1 contingency analysis (CA) fast\nscreening. At first, bi-directional breadth-first search (BFS) is proposed and\nadopted on graph model to detect the potential shedding component in\ncontingency analysis. It implements hierarchical parallelism of the graph\ntraverse and speedup its process. Then, the idea of evolving graph is\nintroduced in this paper to improve computation performance. For each\ncontingency scenario, N-1 contingency graph quickly derives from system graph\nin basic status, and parallelly analyzes each contingency scenario using graph\ncomputing. The efficiency and effectiveness of the proposed approach have been\ntested and verified by IEEE 118-bus system and a practical case SC 2645-bus\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 18:47:02 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhao", "Yiting", ""], ["Yuan", "Chen", ""], ["Li", "Sun", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1904.00182", "submitter": "Joe Alexandersen", "authors": "Nicolo Pollini and Ole Sigmund and Casper Schousboe Andreasen and Joe\n  Alexandersen", "title": "A \"poor man's\" approach for high-resolution three-dimensional topology\n  optimization of natural convection problems", "comments": null, "journal-ref": "Advances in Engineering Software, Volume 140, February 2020,\n  102736", "doi": "10.1016/j.advengsoft.2019.102736", "report-no": null, "categories": "cs.CE cs.DC math.OC physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper treats topology optimization of natural convection problems. A\nsimplified model is suggested to describe the flow of an incompressible fluid\nin steady state conditions, similar to Darcy's law for fluid flow in porous\nmedia. The equations for the fluid flow are coupled to the thermal\nconvection-diffusion equation through the Boussinesq approximation. The coupled\nnon-linear system of equations is discretized with stabilized finite elements\nand solved in a parallel framework that allows for the optimization of high\nresolution three-dimensional problems. A density-based topology optimization\napproach is used, where a two-material interpolation scheme is applied to both\nthe permeability and conductivity of the distributed material. Due to the\nsimplified model, the proposed methodology allows for a significant reduction\nof the computational effort required in the optimization. At the same time, it\nis significantly more accurate than even simpler models that rely on convection\nboundary conditions based on Newton's law of cooling. The methodology discussed\nherein is applied to the optimization-based design of three-dimensional heat\nsinks. The final designs are formally compared with results of previous work\nobtained from solving the full set of Navier-Stokes equations. The results are\ncompared in terms of performance of the optimized designs and computational\ncost. The computational time is shown to be decreased to around 5-20% in terms\nof core-hours, allowing for the possibility of generating an optimized design\nduring the workday on a small computational cluster and overnight on a high-end\ndesktop.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 09:41:13 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Pollini", "Nicolo", ""], ["Sigmund", "Ole", ""], ["Andreasen", "Casper Schousboe", ""], ["Alexandersen", "Joe", ""]]}, {"id": "1904.00358", "submitter": "Parwat Anjana", "authors": "Parwat Singh Anjana, Hagit Attiya, Sweta Kumari, Sathya Peri, and\n  Archit Somani", "title": "Efficient Concurrent Execution of Smart Contracts in Blockchains using\n  Object-based Transactional Memory", "comments": "49 pages, 26 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient framework to execute Smart Contract\nTransactions (SCTs) concurrently based on object semantics, using optimistic\nSingle-Version Object-based Software Transactional Memory Systems (SVOSTMs) and\nMulti-Version OSTMs (MVOSTMs). In our framework, a multi-threaded miner\nconstructs a Block Graph (BG), capturing the object-conflicts relations between\nSCTs, and stores it in the block. Later, validators re-execute the same SCTs\nconcurrently and deterministically relying on this BG.\n  A malicious miner can modify the BG to harm the blockchain, e.g., to cause\ndouble-spending. To identify malicious miners, we propose Smart Multi-threaded\nValidator (SMV). Experimental analysis shows that the proposed multi-threaded\nminer and validator achieve significant performance gains over state-of-the-art\nSCT execution framework.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 08:57:52 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 17:22:12 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 09:01:47 GMT"}, {"version": "v4", "created": "Thu, 26 Mar 2020 12:19:21 GMT"}, {"version": "v5", "created": "Fri, 27 Mar 2020 19:07:54 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Anjana", "Parwat Singh", ""], ["Attiya", "Hagit", ""], ["Kumari", "Sweta", ""], ["Peri", "Sathya", ""], ["Somani", "Archit", ""]]}, {"id": "1904.00375", "submitter": "Yahya Hassanzadeh Nazarabadi", "authors": "Yahya Hassanzadeh-Nazarabadi, Alptekin K\\\"up\\c{c}\\\"u, \\\"Oznur\n  \\\"Ozkasap", "title": "LightChain: A DHT-based Blockchain for Resource Constrained Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an append-only distributed database, blockchain is utilized in a vast\nvariety of applications including the cryptocurrency and Internet-of-Things\n(IoT). The existing blockchain solutions have downsides in communication and\nstorage efficiency, convergence to centralization, and consistency problems. In\nthis paper, we propose LightChain, which is the first blockchain architecture\nthat operates over a Distributed Hash Table (DHT) of participating peers.\nLightChain is a permissionless blockchain that provides addressable blocks and\ntransactions within the network, which makes them efficiently accessible by all\nthe peers. Each block and transaction is replicated within the DHT of peers and\nis retrieved in an on-demand manner. Hence, peers in LightChain are not\nrequired to retrieve or keep the entire blockchain. LightChain is fair as all\nof the participating peers have a uniform chance of being involved in the\nconsensus regardless of their influence such as hashing power or stake.\nLightChain provides a deterministic fork-resolving strategy as well as a\nblacklisting mechanism, and it is secure against colluding adversarial peers\nattacking the availability and integrity of the system. We provide mathematical\nanalysis and experimental results on scenarios involving 10K nodes to\ndemonstrate the security and fairness of LightChain. As we experimentally show\nin this paper, compared to the mainstream blockchains like Bitcoin and\nEthereum, LightChain requires around 66 times less per node storage, and is\naround 380 times faster on bootstrapping a new node to the system, while each\nLightChain node is rewarded equally likely for participating in the protocol.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 10:23:35 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 15:22:43 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 22:11:50 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Hassanzadeh-Nazarabadi", "Yahya", ""], ["K\u00fcp\u00e7\u00fc", "Alptekin", ""], ["\u00d6zkasap", "\u00d6znur", ""]]}, {"id": "1904.00396", "submitter": "EPTCS", "authors": "Francisco Martins (University of the Azores), Dominic Orchard\n  (University of Kent)", "title": "Proceedings Programming Language Approaches to Concurrency- and\n  Communication-cEntric Software", "comments": null, "journal-ref": "EPTCS 291, 2019", "doi": "10.4204/EPTCS.291", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern hardware platforms, from the very small to the very large,\nincreasingly provide parallel and distributed computing resources for\napplications to maximise performance. Many applications therefore need to make\neffective use of tens, hundreds, and even thousands of compute nodes.\nComputation in such systems is thus inherently concurrent and communication\ncentric. Effectively programming such applications is challenging; performance,\ncorrectness, and scalability are difficult to achieve.\n  The development of effective programming methodologies for this increasingly\nparallel landscape therefore demands exploration and understanding of a wide\nvariety of foundational and practical ideas. The International Workshop on\nProgramming Language Approaches to Concurrency- and Communication-cEntric\nSoftware (PLACES) is dedicated to work in this area. The workshop offers a\nforum for researchers from different fields to exchange new ideas about these\nchallenges to modern and future programming, where concurrency and distribution\nare the norm rather than a marginal concern.\n  This proceedings covers the 11th edition of PLACES, which was co-located with\nETAPS 2019 in Prague, Czech Republic.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 12:35:33 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Martins", "Francisco", "", "University of the Azores"], ["Orchard", "Dominic", "", "University of Kent"]]}, {"id": "1904.00518", "submitter": "Abhishek Kulkarni", "authors": "Abhishek Kulkarni, Andrew Lumsdaine", "title": "A Comparative Study of Asynchronous Many-Tasking Runtimes: Cilk,\n  Charm++, ParalleX and AM++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We evaluate and compare four contemporary and emerging runtimes for\nhigh-performance computing(HPC) applications: Cilk, Charm++, ParalleX and AM++.\nWe compare along three bases: programming model, execution model and the\nimplementation on an underlying machine model. The comparison study includes a\nsurvey of each runtime system's programming models, their corresponding\nexecution models, their stated features, and performance and productivity\ngoals. We first qualitatively compare these runtimes with programmability in\nmind. The differences in expressivity and programmability arising from their\nsyntax and semantics are clearly enunciated through examples common to all\nruntimes. Then, the execution model of each runtime, which acts as a bridge\nbetween the programming model and the underlying machine model, is compared and\ncontrasted to that of the others. We also evaluate four mature implementations\nof these runtimes, namely: Intel Cilk++, Charm++ 6.5.1, AM++ and HPX, that\nembody the principles dictated by these models. With the emergence of the next\ngeneration of supercomputers, it is imperative for parallel programming models\nto evolve and address the integral challenges introduced by the increasing\nscale. Rather than picking a winner out of these four models under\nconsideration, we end with a discussion on lessons learned, and how such a\nstudy is instructive in the evolution of parallel programming frameworks to\naddress the said challenges.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 00:42:36 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kulkarni", "Abhishek", ""], ["Lumsdaine", "Andrew", ""]]}, {"id": "1904.00549", "submitter": "Rankyung Hong", "authors": "Benjamin Heintz, Rankyung Hong, Shivangi Singh, Gaurav Khandelwal,\n  Corey Tesdahl, Abhishek Chandra", "title": "MESH: A Flexible Distributed Hypergraph Processing System", "comments": "14 pages, 15 figures, 2019 IEEE International Conference on Cloud\n  Engineering (IC2E)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid growth of large online social networks, the ability to analyze\nlarge-scale social structure and behavior has become critically important, and\nthis has led to the development of several scalable graph processing systems.\nIn reality, however, social interaction takes place not only between pairs of\nindividuals as in the graph model, but rather in the context of multi-user\ngroups. Research has shown that such group dynamics can be better modeled\nthrough a more general hypergraph model, resulting in the need to build\nscalable hypergraph processing systems. In this paper, we present MESH, a\nflexible distributed framework for scalable hypergraph processing. MESH\nprovides an easy-to-use and expressive application programming interface that\nnaturally extends the think like a vertex model common to many popular graph\nprocessing systems. Our framework provides a flexible implementation based on\nan underlying graph processing system, and enables different design choices for\nthe key implementation issues of partitioning a hypergraph representation. We\nimplement MESH on top of the popular GraphX graph processing framework in\nApache Spark. Using a variety of real datasets and experiments conducted on a\nlocal 8-node cluster as well as a 65-node Amazon AWS testbed, we demonstrate\nthat MESH provides flexibility based on data and application characteristics,\nas well as scalability with cluster size. We further show that it is\ncompetitive in performance to HyperX, another hypergraph processing system\nbased on Spark, while providing a much simpler implementation (requiring about\n5X fewer lines of code), thus showing that simplicity and flexibility need not\ncome at the cost of performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 03:44:36 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 20:38:07 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Heintz", "Benjamin", ""], ["Hong", "Rankyung", ""], ["Singh", "Shivangi", ""], ["Khandelwal", "Gaurav", ""], ["Tesdahl", "Corey", ""], ["Chandra", "Abhishek", ""]]}, {"id": "1904.00629", "submitter": "Issam Damaj", "authors": "Safaa Kasbah, Issam Damaj, Ramzi Haraty", "title": "Multigrid Solvers in Reconfigurable Hardware", "comments": "24 Pages, 11 Figures, 10 Tables", "journal-ref": "Journal of Computational and Applied Mathematics, Elsevier. 213\n  (2008) 79-94", "doi": "10.1016/j.cam.2006.12.031", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the solution of Partial Differential Equations (PDEs)\nplays a central role in modeling real world problems. Over the past years,\nMultigrid solvers have showed their robustness over other techniques, due to\nits high convergence rate which is independent of the problem size. For this\nreason, many attempts for exploiting the inherent parallelism of Multigrid have\nbeen made to achieve the desired efficiency and scalability of the method. Yet,\nmost efforts fail in this respect due to many factors (time, resources)\ngoverned by software implementations. In this paper, we present a hardware\nimplementation of the V-cycle Multigrid method for finding the solution of a\n2D-Poisson equation. We use Handel-C to implement our hardware design, which we\nmap onto available Field Programmable Gate Arrays (FPGAs). We analyze the\nimplementation performance using the FPGA vendor's tools. We demonstrate the\nrobustness of Multigrid over other iterative solvers, such as Jacobi and\nSuccessive Over Relaxation (SOR), in both hardware and software. We compare our\nfindings with a C++ version of each algorithm. The obtained results show better\nperformance when compared to existing software versions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:17:47 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Kasbah", "Safaa", ""], ["Damaj", "Issam", ""], ["Haraty", "Ramzi", ""]]}, {"id": "1904.00690", "submitter": "Abdelrahim Ahmad", "authors": "Abdelrahim Kasem Ahmad, Assef Jafar and Kadan Aljoumaa", "title": "Customer churn prediction in telecom using machine learning and social\n  network analysis in big data platform", "comments": "24 pages, 14 figures. PDF https://rdcu.be/budKg", "journal-ref": "Journal of Big Data 2019 6:28", "doi": "10.1186/s40537-019-0191-6", "report-no": null, "categories": "cs.CY cs.DC cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer churn is a major problem and one of the most important concerns for\nlarge companies. Due to the direct effect on the revenues of the companies,\nespecially in the telecom field, companies are seeking to develop means to\npredict potential customer to churn. Therefore, finding factors that increase\ncustomer churn is important to take necessary actions to reduce this churn. The\nmain contribution of our work is to develop a churn prediction model which\nassists telecom operators to predict customers who are most likely subject to\nchurn. The model developed in this work uses machine learning techniques on big\ndata platform and builds a new way of features' engineering and selection. In\norder to measure the performance of the model, the Area Under Curve (AUC)\nstandard measure is adopted, and the AUC value obtained is 93.3%. Another main\ncontribution is to use customer social network in the prediction model by\nextracting Social Network Analysis (SNA) features. The use of SNA enhanced the\nperformance of the model from 84 to 93.3% against AUC standard. The model was\nprepared and tested through Spark environment by working on a large dataset\ncreated by transforming big raw data provided by SyriaTel telecom company. The\ndataset contained all customers' information over 9 months, and was used to\ntrain, test, and evaluate the system at SyriaTel. The model experimented four\nalgorithms: Decision Tree, Random Forest, Gradient Boosted Machine Tree \"GBM\"\nand Extreme Gradient Boosting \"XGBOOST\". However, the best results were\nobtained by applying XGBOOST algorithm. This algorithm was used for\nclassification in this churn predictive model.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 10:30:04 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ahmad", "Abdelrahim Kasem", ""], ["Jafar", "Assef", ""], ["Aljoumaa", "Kadan", ""]]}, {"id": "1904.00996", "submitter": "Pietro Ferraro", "authors": "Andrew Cullen, Pietro Ferraro, Christopher King, and Robert Shorten", "title": "Distributed Ledger Technology for IoT: Parasite Chain Attacks", "comments": null, "journal-ref": "in IEEE Internet of Things Journal, vol. 7, no. 8, pp. 7112-7122,\n  Aug. 2020", "doi": "10.1109/JIOT.2020.2983401", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed Acyclic Graph (DAG) based Distributed Ledgers can be useful in a\nnumber of applications in the IoT domain. A distributed ledger should serve as\nan immutable and irreversible record of transactions, however, a DAG structure\nis a more complicated mathematical object than its blockchain counterparts, and\nas a result, providing guarantees of immutability and irreversibility is more\ninvolved. In this paper, we analyse a commonly discussed attack scenario known\nas a parasite chain attack for the IOTA Foundation DAG based ledger. We analyse\nthe efficacy of IOTA core MCMC algorithm using a matrix model and present an\nextension which improves the ledger resistance to these attacks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 15:46:31 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 14:43:57 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Cullen", "Andrew", ""], ["Ferraro", "Pietro", ""], ["King", "Christopher", ""], ["Shorten", "Robert", ""]]}, {"id": "1904.01000", "submitter": "Issam Damaj", "authors": "Issam Damaj (1), Safaa Kasbah (2) ((1), American University of Kuwait,\n  (2) Lebanese American University)", "title": "An Analysis Framework for Hardware and Software Implementations with\n  Applications from Cryptography", "comments": "20 Pages, 6 Figures, 5 Tables", "journal-ref": "Electrical & Computer Engineering, Elsevier. 69 (2018) 572-584", "doi": "10.1016/j.compeleceng.2017.06.008", "report-no": null, "categories": "cs.DC cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the richness of present-day hardware architectures, tightening the\nsynergy between hardware and software has attracted a great attention. The\ninterest in unified approaches paved the way for newborn frameworks that target\nhardware and software co-design. This paper confirms that a unified statistical\nframework can successfully classify algorithms based on a combination of the\nheterogeneous characteristics of their hardware and software implementations.\nThe proposed framework produces customizable indicators for any hybridization\nof processing systems and can be contextualized for any area of application.\nThe framework is used to develop the Lightness Indicator System (LIS) as a\ncase-study that targets a set of cryptographic algorithms that are known in the\nliterature to be tiny and light. The LIS targets state-of-the-art multi-core\nprocessors and high-end Field Programmable Gate Arrays (FPGAs). The presented\nwork includes a generic benchmark model that aids the clear presentation of the\nframework and extensive performance analysis and evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 18:38:55 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Damaj", "Issam", ""], ["Kasbah", "Safaa", ""]]}, {"id": "1904.01004", "submitter": "Joerg Evermann", "authors": "Joerg Evermann and Henry Kim", "title": "Workflow Management on the Blockchain --- Implications and\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blockchain technology, originally popularized by cryptocurrencies, has been\nproposed as an infrastructure technology with applications in many areas of\nbusiness management. Blockchains provide an immutable record of transactions,\nwhich makes them useful in situations where business actors may not fully trust\neach other. The distributed nature of blockchains makes them particularly\nsuitable for inter-organizational e-Business applications. In this paper we\nexamine the use of blockchains for executing inter-organizational workflows. We\ndiscuss architectural options and describe prototype implementations of\nblockchain-based workflow management systems (WfMS), highlighting differences\nto traditional WfMS. Our main contribution is the identification of potential\nproblems raised by blockchain infrastructure and recommendations to address\nthem.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 16:25:10 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 15:18:30 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Evermann", "Joerg", ""], ["Kim", "Henry", ""]]}, {"id": "1904.01197", "submitter": "Afshin Abdi", "authors": "Afshin Abdi, and Faramarz Fekri", "title": "Nested Dithered Quantization for Communication Reduction in Distributed\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed training, the communication cost due to the transmission of\ngradients or the parameters of the deep model is a major bottleneck in scaling\nup the number of processing nodes. To address this issue, we propose\n\\emph{dithered quantization} for the transmission of the stochastic gradients\nand show that training with \\emph{Dithered Quantized Stochastic Gradients\n(DQSG)} is similar to the training with unquantized SGs perturbed by an\nindependent bounded uniform noise, in contrast to the other quantization\nmethods where the perturbation depends on the gradients and hence, complicating\nthe convergence analysis. We study the convergence of training algorithms using\nDQSG and the trade off between the number of quantization levels and the\ntraining time.\n  Next, we observe that there is a correlation among the SGs computed by\nworkers that can be utilized to further reduce the communication overhead\nwithout any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly \\emph{without requiring the workers communicating\nextra information to each other}. We prove that although NDQSG requires\nsignificantly less bits, it can achieve the same quantization variance bound as\nDQSG.\n  Our simulation results confirm the effectiveness of training using DQSG and\nNDQSG in reducing the communication bits or the convergence time compared to\nthe existing methods without sacrificing the accuracy of the trained model.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:47:28 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Abdi", "Afshin", ""], ["Fekri", "Faramarz", ""]]}, {"id": "1904.01576", "submitter": "Anirban Bhattacharjee", "authors": "Anirban Bhattacharjee, Ajay Dev Chhokra, Zhuangwei Kang, Hongyang Sun,\n  Aniruddha Gokhale, Gabor Karsai", "title": "BARISTA: Efficient and Scalable Serverless Serving System for Deep\n  Learning Prediction Services", "comments": null, "journal-ref": null, "doi": "10.1109/IC2E.2019.00-10", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained deep learning models are increasingly being used to offer a\nvariety of compute-intensive predictive analytics services such as fitness\ntracking, speech and image recognition. The stateless and highly parallelizable\nnature of deep learning models makes them well-suited for serverless computing\nparadigm. However, making effective resource management decisions for these\nservices is a hard problem due to the dynamic workloads and diverse set of\navailable resource configurations that have their deployment and management\ncosts. To address these challenges, we present a distributed and scalable\ndeep-learning prediction serving system called Barista and make the following\ncontributions. First, we present a fast and effective methodology for\nforecasting workloads by identifying various trends. Second, we formulate an\noptimization problem to minimize the total cost incurred while ensuring bounded\nprediction latency with reasonable accuracy. Third, we propose an efficient\nheuristic to identify suitable compute resource configurations. Fourth, we\npropose an intelligent agent to allocate and manage the compute resources by\nhorizontal and vertical scaling to maintain the required prediction latency.\nFinally, using representative real-world workloads for urban transportation\nservice, we demonstrate and validate the capabilities of Barista.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 01:46:38 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 16:00:14 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Bhattacharjee", "Anirban", ""], ["Chhokra", "Ajay Dev", ""], ["Kang", "Zhuangwei", ""], ["Sun", "Hongyang", ""], ["Gokhale", "Aniruddha", ""], ["Karsai", "Gabor", ""]]}, {"id": "1904.01630", "submitter": "Gowri Sankar Ramachandran Dr", "authors": "Gowri Sankar Ramachandran, Xiang Ji, Pavas Navaney, Licheng Zheng,\n  Martin Martinez, Bhaskar Krishnamachari", "title": "MOTIVE: Micropayments for trusted vehicular services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly, connected cars are becoming a decentralized data platform. With\ngreater autonomy, they have growing needs for computation and perceiving the\nworld around them through sensors. While todays generation of vehicles carry\nall the necessary sensor data and computation on board, we envision a future\nwhere vehicles can cooperate to increase their perception of the world beyond\ntheir immediate view, resulting in greater safety, coordination and more\ncomfortable experience for their human occupants. In order for vehicles to\nobtain data, compute and other services from other vehicles or road side\ninfrastructure, it is important to be able to make micro payments for those\nservices and for the services to run seamlessly despite the challenges posed by\nmobility and ephemeral interactions with a dynamic set of neighboring devices.\nWe present MOTIVE, a trusted and decentralized framework that allows vehicles\nto make peer to peer micropayments for data, compute and other services\nobtained from other vehicles or road side infrastructure within radio range.\nThe framework utilizes distributed ledger technologies including smart\ncontracts to enable autonomous operation and trusted interactions between\nvehicles and nearby entities.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 05:20:44 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ramachandran", "Gowri Sankar", ""], ["Ji", "Xiang", ""], ["Navaney", "Pavas", ""], ["Zheng", "Licheng", ""], ["Martinez", "Martin", ""], ["Krishnamachari", "Bhaskar", ""]]}, {"id": "1904.01631", "submitter": "Anthony Hsu", "authors": "Anthony Hsu, Keqiu Hu, Jonathan Hung, Arun Suresh, Zhe Zhang", "title": "TonY: An Orchestrator for Distributed Machine Learning Jobs", "comments": "2 pages, to be published in OpML '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning (ML) models on large datasets requires considerable\ncomputing power. To speed up training, it is typical to distribute training\nacross several machines, often with specialized hardware like GPUs or TPUs.\nManaging a distributed training job is complex and requires dealing with\nresource contention, distributed configurations, monitoring, and fault\ntolerance. In this paper, we describe TonY, an open-source orchestrator for\ndistributed ML jobs built at LinkedIn to address these challenges.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 00:18:21 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hsu", "Anthony", ""], ["Hu", "Keqiu", ""], ["Hung", "Jonathan", ""], ["Suresh", "Arun", ""], ["Zhang", "Zhe", ""]]}, {"id": "1904.01691", "submitter": "Sangkug Lym", "authors": "Sangkug Lym, Donghyuk Lee, Mike O'Connor, Niladrish Chatterjee, Mattan\n  Erez", "title": "DeLTA: GPU Performance Model for Deep Learning Applications with\n  In-depth Memory System Traffic Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/ISPASS.2019.00041", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Training convolutional neural networks (CNNs) requires intense compute\nthroughput and high memory bandwidth. Especially, convolution layers account\nfor the majority of the execution time of CNN training, and GPUs are commonly\nused to accelerate these layer workloads. GPU design optimization for efficient\nCNN training acceleration requires the accurate modeling of how their\nperformance improves when computing and memory resources are increased. We\npresent DeLTA, the first analytical model that accurately estimates the traffic\nat each GPU memory hierarchy level, while accounting for the complex reuse\npatterns of a parallel convolution algorithm. We demonstrate that our model is\nboth accurate and robust for different CNNs and GPU architectures. We then show\nhow this model can be used to carefully balance the scaling of different GPU\nresources for efficient CNN performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:30:06 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lym", "Sangkug", ""], ["Lee", "Donghyuk", ""], ["O'Connor", "Mike", ""], ["Chatterjee", "Niladrish", ""], ["Erez", "Mattan", ""]]}, {"id": "1904.01727", "submitter": "Anirban Bhattacharjee", "authors": "Anirban Bhattacharjee, Yogesh Barve, Shweta Khare, Shunxing Bao,\n  Aniruddha Gokhale and Thomas Damiano", "title": "Stratum: A Serverless Framework for Lifecycle Management of Machine\n  Learning based Data Analytics Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of machine learning (ML) libraries and frameworks, and\nthe programming languages that they use, along with operations of data loading,\ntransformation, preparation and mining, ML model development is becoming a\ndaunting task. Furthermore, with a plethora of cloud-based ML model development\nplatforms, heterogeneity in hardware, increased focus on exploiting edge\ncomputing resources for low-latency prediction serving and often a lack of a\ncomplete understanding of resources required to execute ML workflows\nefficiently, ML model deployment demands expertise for managing the lifecycle\nof ML workflows efficiently and with minimal cost. To address these challenges,\nwe propose an end-to-end data analytics, a serverless platform called Stratum.\nStratum can deploy, schedule and dynamically manage data ingestion tools, live\nstreaming apps, batch analytics tools, ML-as-a-service (for inference jobs),\nand visualization tools across the cloud-fog-edge spectrum. This paper\ndescribes the Stratum architecture highlighting the problems it resolves.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:07:04 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Bhattacharjee", "Anirban", ""], ["Barve", "Yogesh", ""], ["Khare", "Shweta", ""], ["Bao", "Shunxing", ""], ["Gokhale", "Aniruddha", ""], ["Damiano", "Thomas", ""]]}, {"id": "1904.01935", "submitter": "Diego Pennino", "authors": "Matteo Bernardini, Diego Pennino, Maurizio Pizzonia", "title": "Blockchains Meet Distributed Hash Tables: Decoupling Validation from\n  State Storage", "comments": "13 pages, 3 figures, conference (DLT19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first obstacle that regular users encounter when setting up a node for a\npublic blockchain is the time taken for downloading all the data needed for the\nnode to start operating correctly. In fact, this may last from hours to weeks\nfor the major networks. Our contribution is twofold. Firstly, we show a design\nthat enables mining and validation of new blocks keeping only a very small\nstate. Secondly, we show that it is possible to store the state of the\nblockchain in a distributed hash table obtaining a wide spectrum of trade-offs\nbetween storage committed by the nodes and replication factor. Our proposal is\nindependent from the consensus algorithm adopted, and copes well with\ntransactions that involve smart contracts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 16:53:24 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Bernardini", "Matteo", ""], ["Pennino", "Diego", ""], ["Pizzonia", "Maurizio", ""]]}, {"id": "1904.01936", "submitter": "Mohammad Maroufi Maein bolagh", "authors": "Mohammad Maroufi, Reza Abdolee, Behzad Mozaffari Tazekand", "title": "On the Convergence of Blockchain and Internet of Things (IoT)\n  Technologies", "comments": "Includes 11 Pages, 3 Figures, To publish in Journal of Strategic\n  Innovation and Sustainability for issue JSIS 14(1)", "journal-ref": null, "doi": "10.33423/jsis.v14i1.990", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) technology will soon become an integral part of\nour daily lives to facilitate the control and monitoring of processes and\nobjects and revolutionize the ways that human interacts with the physical\nworld. For all features of IoT to become fully functional in practice, there\nare several obstacles on the way to be surmounted and critical challenges to be\naddressed. These include, but are not limited to cybersecurity, data privacy,\nenergy consumption, and scalability. The Blockchain decentralized nature and\nits multi-faceted procedures offer a useful mechanism to tackle several of\nthese IoT challenges. However, applying the Blockchain protocols to IoT without\nconsidering their tremendous computational loads, delays, and bandwidth\noverhead can let to a new set of problems. This review evaluates some of the\nmain challenges we face in the integration of Blockchain and IoT technologies\nand provides insights and high-level solutions that can potentially handle the\nshortcomings and constraints of both IoT and Blockchain technologies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 14:18:36 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Maroufi", "Mohammad", ""], ["Abdolee", "Reza", ""], ["Tazekand", "Behzad Mozaffari", ""]]}, {"id": "1904.02124", "submitter": "Anup Joshi", "authors": "Prasad Jayanti and Siddhartha Jayanti and Anup Joshi", "title": "A Recoverable Mutex Algorithm with Sub-logarithmic RMR on Both CC and\n  DSM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of recent advances in non-volatile main memory technology, Golab and\nRamaraju reformulated the traditional mutex problem into the novel {\\em\nRecoverable Mutual Exclusion} (RME) problem. In the best known solution for\nRME, due to Golab and Hendler from PODC 2017, a process incurs at most\n$O(\\frac{\\log n}{\\log \\log n})$ remote memory references (RMRs) per passage,\nwhere a passage is an interval from when a process enters the Try section to\nwhen it subsequently returns to Remainder. Their algorithm, however, guarantees\nthis bound only for cache-coherent (CC) multiprocessors, leaving open the\nquestion of whether a similar bound is possible for distributed shared memory\n(DSM) multiprocessors.\n  We answer this question affirmatively by designing an algorithm that\nsatisfies the same complexity bound as Golab and Hendler's for both CC and DSM\nmultiprocessors. Our algorithm has some additional advantages over Golab and\nHendler's: (i) its Exit section is wait-free, (ii) it uses only the\nFetch-and-Store instruction, and (iii) on a CC machine our algorithm needs each\nprocess to have a cache of only $O(1)$ words, while their algorithm needs\n$O(n)$ words.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:37:52 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 04:23:55 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Jayanti", "Prasad", ""], ["Jayanti", "Siddhartha", ""], ["Joshi", "Anup", ""]]}, {"id": "1904.02241", "submitter": "Xuhao Chen", "authors": "Xuhao Chen", "title": "GraphCage: Cache Aware Graph Processing on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Graph processing is challenging because of the irregularity of\ngraph algorithms. Using GPUs to accelerate irregular graph algorithms is even\nmore difficult to be efficient, since GPU's highly structured SIMT architecture\nis not a natural fit for irregular applications. With lots of previous efforts\nspent on subtly mapping graph algorithms onto the GPU, the performance of graph\nprocessing on GPUs is still highly memory-latency bound, leading to low\nutilization of compute resources. Random memory accesses generated by the\nsparse graph data structure are the major causes of this significant memory\naccess latency. Simply applying the conventional cache blocking technique\nproposed for matrix computation have limited benefit due to the significant\noverhead on the GPU. We propose GraphCage, a cache centric optimization\nframework for highly efficient graph processing on GPUs. We first present a\nthroughput-oriented cache blocking scheme (TOCAB) in both push and pull\ndirections. Comparing with conventional cache blocking which suffers repeated\naccesses when processing large graphs on GPUs, TOCAB is specifically optimized\nfor the GPU architecture to reduce this overhead and improve memory access\nefficiency. To integrate our scheme into state-of-the-art implementations\nwithout significant overhead, we coordinate TOCAB with load balancing\nstrategies by considering the sparsity of subgraphs. To enable cache blocking\nfor traversal-based algorithms, we consider the benefit and overhead in\ndifferent iterations with different working set sizes, and apply TOCAB for\ntopology-driven kernels in pull direction. Evaluation shows that GraphCage can\nimprove performance by 2 ~ 4x compared to hand optimized implementations and\nstate-of-the-art frameworks (e.g. CuSha and Gunrock), with less memory\nconsumption than CuSha.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 21:19:13 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Chen", "Xuhao", ""]]}, {"id": "1904.02288", "submitter": "Jianliang Gao", "authors": "Jianliang Gao, Noureddin Sadawi, Ibrahim Karaman, Jake T M Pearce,\n  Pablo Moreno, Anders Larsson, Marco Capuccini, Paul Elliott, Jeremy K\n  Nicholson, Timothy M D Ebbels, Robert Glen", "title": "Metabolomics in the Cloud: Scaling Computational Tools to Big Data", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Metabolomics datasets are becoming increasingly large and\ncomplex, with multiple types of algorithms and workflows needed to process and\nanalyse the data. A cloud infrastructure with portable software tools can\nprovide much needed resources enabling faster processing of much larger\ndatasets than would be possible at any individual lab. The PhenoMeNal project\nhas developed such an infrastructure, allowing users to run analyses on local\nor commercial cloud platforms. We have examined the computational scaling\nbehaviour of the PhenoMeNal platform using four different implementations\nacross 1-1000 virtual CPUs using two common metabolomics tools.\n  Results: Our results show that data which takes up to 4 days to process on a\nstandard desktop computer can be processed in just 10 min on the largest\ncluster. Improved runtimes come at the cost of decreased efficiency, with all\nplatforms falling below 80% efficiency above approximately 1/3 of the maximum\nnumber of vCPUs. An economic analysis revealed that running on large scale\ncloud platforms is cost effective compared to traditional desktop systems.\n  Conclusions: Overall, cloud implementations of PhenoMeNal show excellent\nscalability for standard metabolomics computing tasks on a range of platforms,\nmaking them a compelling choice for research computing in metabolomics.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 00:58:43 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 04:57:28 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Gao", "Jianliang", ""], ["Sadawi", "Noureddin", ""], ["Karaman", "Ibrahim", ""], ["Pearce", "Jake T M", ""], ["Moreno", "Pablo", ""], ["Larsson", "Anders", ""], ["Capuccini", "Marco", ""], ["Elliott", "Paul", ""], ["Nicholson", "Jeremy K", ""], ["Ebbels", "Timothy M D", ""], ["Glen", "Robert", ""]]}, {"id": "1904.02838", "submitter": "Md Shahriar Iqbal", "authors": "Md Shahriar Iqbal, Lars Kotthoff, Pooyan Jamshidi", "title": "Transfer Learning for Performance Modeling of Deep Neural Network\n  Systems", "comments": "2 pages, 2 figures, USENIX Conference on Operational Machine\n  Learning, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural network (DNN) systems are highly configurable with large a\nnumber of options that significantly affect their non-functional behavior, for\nexample inference time and energy consumption. Performance models allow to\nunderstand and predict the effects of such configuration options on system\nbehavior, but are costly to build because of large configuration spaces.\nPerformance models from one environment cannot be transferred directly to\nanother; usually models are rebuilt from scratch for different environments,\nfor example different hardware. Recently, transfer learning methods have been\napplied to reuse knowledge from performance models trained in one environment\nin another. In this paper, we perform an empirical study to understand the\neffectiveness of different transfer learning strategies for building\nperformance models of DNN systems. Our results show that transferring\ninformation on the most influential configuration options and their\ninteractions is an effective way of reducing the cost to build performance\nmodels in new environments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 14:16:37 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Iqbal", "Md Shahriar", ""], ["Kotthoff", "Lars", ""], ["Jamshidi", "Pooyan", ""]]}, {"id": "1904.03257", "submitter": "Alexander Ratner", "authors": "Alexander Ratner, Dan Alistarh, Gustavo Alonso, David G. Andersen,\n  Peter Bailis, Sarah Bird, Nicholas Carlini, Bryan Catanzaro, Jennifer Chayes,\n  Eric Chung, Bill Dally, Jeff Dean, Inderjit S. Dhillon, Alexandros Dimakis,\n  Pradeep Dubey, Charles Elkan, Grigori Fursin, Gregory R. Ganger, Lise Getoor,\n  Phillip B. Gibbons, Garth A. Gibson, Joseph E. Gonzalez, Justin Gottschlich,\n  Song Han, Kim Hazelwood, Furong Huang, Martin Jaggi, Kevin Jamieson, Michael\n  I. Jordan, Gauri Joshi, Rania Khalaf, Jason Knight, Jakub Kone\\v{c}n\\'y, Tim\n  Kraska, Arun Kumar, Anastasios Kyrillidis, Aparna Lakshmiratan, Jing Li,\n  Samuel Madden, H. Brendan McMahan, Erik Meijer, Ioannis Mitliagkas, Rajat\n  Monga, Derek Murray, Kunle Olukotun, Dimitris Papailiopoulos, Gennady\n  Pekhimenko, Theodoros Rekatsinas, Afshin Rostamizadeh, Christopher R\\'e,\n  Christopher De Sa, Hanie Sedghi, Siddhartha Sen, Virginia Smith, Alex Smola,\n  Dawn Song, Evan Sparks, Ion Stoica, Vivienne Sze, Madeleine Udell, Joaquin\n  Vanschoren, Shivaram Venkataraman, Rashmi Vinayak, Markus Weimer, Andrew\n  Gordon Wilson, Eric Xing, Matei Zaharia, Ce Zhang, Ameet Talwalkar", "title": "MLSys: The New Frontier of Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) techniques are enjoying rapidly increasing adoption.\nHowever, designing and implementing the systems that support ML models in\nreal-world deployments remains a significant obstacle, in large part due to the\nradically different development and deployment profile of modern ML methods,\nand the range of practical concerns that come with broader adoption. We propose\nto foster a new systems machine learning research community at the intersection\nof the traditional systems and ML communities, focused on topics such as\nhardware systems for ML, software systems for ML, and ML optimized for metrics\nbeyond predictive accuracy. To do this, we describe a new conference, MLSys,\nthat explicitly targets research at the intersection of systems and machine\nlearning with a program committee split evenly between experts in systems and\nML, and an explicit focus on topics at the intersection of the two.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 12:43:36 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 04:55:56 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2019 20:27:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ratner", "Alexander", ""], ["Alistarh", "Dan", ""], ["Alonso", "Gustavo", ""], ["Andersen", "David G.", ""], ["Bailis", "Peter", ""], ["Bird", "Sarah", ""], ["Carlini", "Nicholas", ""], ["Catanzaro", "Bryan", ""], ["Chayes", "Jennifer", ""], ["Chung", "Eric", ""], ["Dally", "Bill", ""], ["Dean", "Jeff", ""], ["Dhillon", "Inderjit S.", ""], ["Dimakis", "Alexandros", ""], ["Dubey", "Pradeep", ""], ["Elkan", "Charles", ""], ["Fursin", "Grigori", ""], ["Ganger", "Gregory R.", ""], ["Getoor", "Lise", ""], ["Gibbons", "Phillip B.", ""], ["Gibson", "Garth A.", ""], ["Gonzalez", "Joseph E.", ""], ["Gottschlich", "Justin", ""], ["Han", "Song", ""], ["Hazelwood", "Kim", ""], ["Huang", "Furong", ""], ["Jaggi", "Martin", ""], ["Jamieson", "Kevin", ""], ["Jordan", "Michael I.", ""], ["Joshi", "Gauri", ""], ["Khalaf", "Rania", ""], ["Knight", "Jason", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Kraska", "Tim", ""], ["Kumar", "Arun", ""], ["Kyrillidis", "Anastasios", ""], ["Lakshmiratan", "Aparna", ""], ["Li", "Jing", ""], ["Madden", "Samuel", ""], ["McMahan", "H. Brendan", ""], ["Meijer", "Erik", ""], ["Mitliagkas", "Ioannis", ""], ["Monga", "Rajat", ""], ["Murray", "Derek", ""], ["Olukotun", "Kunle", ""], ["Papailiopoulos", "Dimitris", ""], ["Pekhimenko", "Gennady", ""], ["Rekatsinas", "Theodoros", ""], ["Rostamizadeh", "Afshin", ""], ["R\u00e9", "Christopher", ""], ["De Sa", "Christopher", ""], ["Sedghi", "Hanie", ""], ["Sen", "Siddhartha", ""], ["Smith", "Virginia", ""], ["Smola", "Alex", ""], ["Song", "Dawn", ""], ["Sparks", "Evan", ""], ["Stoica", "Ion", ""], ["Sze", "Vivienne", ""], ["Udell", "Madeleine", ""], ["Vanschoren", "Joaquin", ""], ["Venkataraman", "Shivaram", ""], ["Vinayak", "Rashmi", ""], ["Weimer", "Markus", ""], ["Wilson", "Andrew Gordon", ""], ["Xing", "Eric", ""], ["Zaharia", "Matei", ""], ["Zhang", "Ce", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1904.03271", "submitter": "Gowtham Raghunath Kurri", "authors": "Yanjun Han, Kedar Tatwawadi, Gowtham R. Kurri, Zhengqing Zhou, Vinod\n  M. Prabhakaran, and Tsachy Weissman", "title": "Optimal Communication Rates and Combinatorial Properties for Common\n  Randomness Generation", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study common randomness generation problems where $n$ players aim to\ngenerate same sequences of random coin flips where some subsets of the players\nshare an independent common coin which can be tossed multiple times, and there\nis a publicly seen blackboard through which the players communicate with each\nother. We provide a tight representation of the optimal communication rates via\nlinear programming, and more importantly, propose explicit algorithms for the\noptimal distributed simulation for a wide class of hypergraphs. In particular,\nthe optimal communication rate in complete hypergraphs is still achievable in\nsparser hypergraphs containing a path-connected cycle-free cluster of\ntopologically connected components. Some key steps in analyzing the upper\nbounds rely on two different definitions of connectivity in hypergraphs, which\nmay be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:47:07 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 22:29:18 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 06:12:47 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 16:40:18 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Han", "Yanjun", ""], ["Tatwawadi", "Kedar", ""], ["Kurri", "Gowtham R.", ""], ["Zhou", "Zhengqing", ""], ["Prabhakaran", "Vinod M.", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1904.03302", "submitter": "Urmish Thakker", "authors": "Urmish Thakker, Ganesh Dasika, Jesse Beu and Matthew Mattina", "title": "Measuring scheduling efficiency of RNNs for NLP applications", "comments": null, "journal-ref": "Fastpath 2019, 6th International Workshop on Performance Analysis\n  of Machine Learning Systems", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have shown state of the art results for\nspeech recognition, natural language processing, image captioning and video\nsummarizing applications. Many of these applications run on low-power\nplatforms, so their energy efficiency is extremely important. We observed that\ncache-oblivious RNN scheduling during inference typically results in 30-50x\nmore data transferred on and off the CPU than the application's working set\nsize. This can potentially impact its energy efficiency. This paper presents a\nnew metric called Data Reuse Efficiency to gauge the RNN scheduling efficiency\nof a platform and shows the factors that influence the DRE value. Additionally,\nthis paper discusses an optimization to improve reuse in RNNs and highlights\nthe positive impact of this optimization on the total amount of memory read\nfrom or written to the memory controller (and, hence, the DRE value) during the\nexecution of an RNN application for a mobile SoC.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 22:03:14 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Thakker", "Urmish", ""], ["Dasika", "Ganesh", ""], ["Beu", "Jesse", ""], ["Mattina", "Matthew", ""]]}, {"id": "1904.03329", "submitter": "Israt Nisa", "authors": "Israt Nisa, Jiajia Li, Aravind Sukumaran-Rajam, Richard Vuduc, P.\n  Sadayappan", "title": "Load-Balanced Sparse MTTKRP on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matricized tensor times Khatri-Rao product (MTTKRP) is one of the most\ncomputationally expensive kernels in sparse tensor computations. This work\nfocuses on optimizing the MTTKRP operation on GPUs, addressing both performance\nand storage requirements. We begin by identifying the performance bottlenecks\nin directly extending the state-of-the-art CSF (compressed sparse fiber) format\nfrom CPUs to GPUs. A significant challenge with GPUs compared to multicore CPUs\nis that of utilizing the much greater degree of parallelism in a load-balanced\nfashion for irregular computations like sparse MTTKRP. To address this issue,\nwe develop a new storage-efficient representation for tensors that enables\nhigh-performance, load-balanced execution of MTTKRP on GPUs. A GPU\nimplementation of sparse MTTKRP using the new sparse tensor representation is\nshown to outperform all currently known parallel sparse CPU and GPU MTTKRP\nimplementations.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 01:12:43 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Nisa", "Israt", ""], ["Li", "Jiajia", ""], ["Sukumaran-Rajam", "Aravind", ""], ["Vuduc", "Richard", ""], ["Sadayappan", "P.", ""]]}, {"id": "1904.03560", "submitter": "Paritosh Ramanan", "authors": "Paritosh Ramanan, Murat Yildirim, Edmond Chow and Nagi Gebraeel", "title": "An Asynchronous, Decentralized Solution Framework for the Large Scale\n  Unit Commitment Problem", "comments": null, "journal-ref": null, "doi": "10.1109/TPWRS.2019.2909664", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increased reliance on cyber infrastructure, large scale power networks\nface new challenges owing to computational scalability. In this paper we focus\non developing an asynchronous decentralized solution framework for the Unit\nCommitment(UC) problem for large scale power networks. We exploit the inherent\nasynchrony in a region based decomposition arising out of imbalance in regional\nsubproblems to boost computational efficiency. A two phase algorithm is\nproposed that relies on the convex relaxation and privacy preserving valid\ninequalities in order to deliver algorithmic improvements. Our algorithm\nemploys a novel interleaved binary mechanism that locally switches from the\nconvex subproblem to its binary counterpart based on consistent local\nconvergent behavior. We develop a high performance computing (HPC) oriented\nsoftware framework that uses Message Passing Interface (MPI) to drive our\nbenchmark studies. Our simulations performed on the IEEE 3012 bus case are\nbenchmarked against the centralized and a state of the art synchronous\ndecentralized method. The results demonstrate that the asynchronous method\nimproves computational efficiency by a significant amount and provides a\ncompetitive solution quality rivaling the benchmark methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:54:26 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 03:25:24 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Ramanan", "Paritosh", ""], ["Yildirim", "Murat", ""], ["Chow", "Edmond", ""], ["Gebraeel", "Nagi", ""]]}, {"id": "1904.03587", "submitter": "Yongli Zhu", "authors": "Yongli Zhu, Lingpeng Shi, Renchang Dai, Guangyi Liu", "title": "Fast Grid Splitting Detection for N-1 Contingency Analysis by Graph\n  Computing", "comments": "This paper has been accepted by the IEEE ISGT-ASIA 2019 conference,\n  Chengdu, China, May.21-24, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a graph-computing based grid splitting detection algorithm is\nproposed for contingency analysis in a graph-based EMS (Energy Management\nSystem). The graph model of a power system is established by storing its\nbus-branch information into the corresponding vertex objects and edge objects\nof the graph database. Numerical comparison to an up-to-date serial computing\nalgorithm is also investigated. Online tests on a real power system of China\nState Grid with 2752 buses and 3290 branches show that a 6 times speedup can be\nachieved, which lays a good foundation for advanced contingency analysis.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 05:37:03 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhu", "Yongli", ""], ["Shi", "Lingpeng", ""], ["Dai", "Renchang", ""], ["Liu", "Guangyi", ""]]}, {"id": "1904.03633", "submitter": "Sylvain Hall\\'e", "authors": "Quentin Betti and Rapha\\\"el Khoury and Sylvain Hall\\'e and Beno\\^it\n  Montreuil", "title": "Improving Hyperconnected Logistics with Blockchains and Smart Contracts", "comments": "8 pages, accepted for publication in IT Professional", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Physical Internet and hyperconnected logistics concepts promise an open,\nmore efficient and environmentally friendly supply chain for goods. Blockchain\nand Internet of Things technologies are increasingly regarded as main enablers\nof improvements in this domain. We describe how blockchain and smart contracts\npresent the potential of being applied to hyperconnected logistics by showing a\nconcrete example of its implementation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 12:19:38 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Betti", "Quentin", ""], ["Khoury", "Rapha\u00ebl", ""], ["Hall\u00e9", "Sylvain", ""], ["Montreuil", "Beno\u00eet", ""]]}, {"id": "1904.03684", "submitter": "Chaitanya Prasad Sishtla", "authors": "Chaitanya Prasad Sishtla, Steven W. D. Chien, Vyacheslav Olshevsky,\n  Erwin Laure, Stefano Markidis", "title": "Multi-GPU Acceleration of the iPIC3D Implicit Particle-in-Cell Code", "comments": "Accepted for publication in ICCS 2019", "journal-ref": null, "doi": "10.1007/978-3-030-22750-0_58", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  iPIC3D is a widely used massively parallel Particle-in-Cell code for the\nsimulation of space plasmas. However, its current implementation does not\nsupport execution on multiple GPUs. In this paper, we describe the porting of\niPIC3D particle mover to GPUs and the optimization steps to increase the\nperformance and parallel scaling on multiple GPUs. We analyze the strong\nscaling of the mover on two GPU clusters and evaluate its performance and\nacceleration. The optimized GPU version which uses pinned memory and\nasynchronous data prefetching outperform their corresponding CPU versions by\n5-10x on two different systems equipped with NVIDIA K80 and V100 GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 16:39:26 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Sishtla", "Chaitanya Prasad", ""], ["Chien", "Steven W. D.", ""], ["Olshevsky", "Vyacheslav", ""], ["Laure", "Erwin", ""], ["Markidis", "Stefano", ""]]}, {"id": "1904.03700", "submitter": "Archit Somani", "authors": "Chirag Juyal, Sandeep Kulkarni, Sweta Kumari, Sathya Peri, and Archit\n  Somani", "title": "Achieving Starvation-Freedom with Greater Concurrency in Multi-Version\n  Object-based Transactional Memory Systems", "comments": "68 pages, 24 figures. arXiv admin note: text overlap with\n  arXiv:1709.01033", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To utilize the multi-core processors properly concurrent programming is\nneeded. Concurrency control is the main challenge while designing a correct and\nefficient concurrent program. Software Transactional Memory Systems (STMs)\nprovides ease of multithreading to the programmer without worrying about\nconcurrency issues such as deadlock, livelock, priority inversion, etc. Most of\nthe STMs works on read-write operations known as RWSTMs. Some STMs work at\nhigh-level operations and ensure greater concurrency than RWSTMs. Such STMs are\nknown as Object-Based STMs (OSTMs). The transactions of OSTMs can return commit\nor abort. Aborted OSTMs transactions retry. But in the current setting of\nOSTMs, transactions may starve. So, we proposed a Starvation-Free OSTM\n(SF-OSTM) which ensures starvation-freedom in object based STM systems while\nsatisfying the correctness criteria as co-opacity. Databases, RWSTMs and OSTMs\nsay that maintaining multiple versions corresponding to each key of transaction\nreduces the number of aborts and improves the throughput. So, to achieve\ngreater concurrency, we proposed Starvation-Free Multi-Version OSTM (SF-MVOSTM)\nwhich ensures starvation-freedom while storing multiple versions corresponding\nto each key and satisfies the correctness criteria such as local opacity. To\nshow the performance benefits, We implemented three variants of SF-MVOSTM\n(SF-MVOSTM, SF-MVOSTM-GC and SF-KOSTM) and compared it with state-of-the-art\nSTMs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 18:00:30 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 14:45:17 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 20:37:52 GMT"}, {"version": "v4", "created": "Mon, 5 Aug 2019 08:45:15 GMT"}, {"version": "v5", "created": "Wed, 21 Aug 2019 11:06:03 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Juyal", "Chirag", ""], ["Kulkarni", "Sandeep", ""], ["Kumari", "Sweta", ""], ["Peri", "Sathya", ""], ["Somani", "Archit", ""]]}, {"id": "1904.03756", "submitter": "Issam Damaj", "authors": "Issam Damaj (Rafik Hariri University)", "title": "Higher-Level Hardware Synthesis of The KASUMI Algorithm", "comments": null, "journal-ref": "Jrnl. Comp. Sc. & Tech. Springer. 22(2007) 60-70", "doi": "10.1016/j.advengsoft.2006.01.009", "report-no": null, "categories": "cs.AR cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable Logic Devices (PLDs) continue to grow in size and currently\ncontain several millions of gates. At the same time, research effort is going\ninto higher-level hardware synthesis methodologies for reconfigurable computing\nthat can exploit PLD technology. In this paper, we explore the effectiveness\nand extend one such formal methodology in the design of massively parallel\nalgorithms. We take a step-wise refinement approach to the development of\ncorrect reconfigurable hardware circuits from formal specifications. A\nfunctional programming notation is used for specifying algorithms and for\nreasoning about them. The specifications are realised through the use of a\ncombination of function decomposition strategies, data refinement techniques,\nand off-the-shelf refinements based upon higher-order functions. The\noff-the-shelf refinements are inspired by the operators of Communicating\nSequential Processes (CSP) and map easily to programs in Handel-C (a hardware\ndescription language). The Handel-C descriptions are directly compiled into\nreconfigurable hardware. The practical realisation of this methodology is\nevidenced by a case studying the third generation mobile communication security\nalgorithms. The investigated algorithm is the KASUM} block cipher. In this\npaper, we obtain several hardware implementations with different performance\ncharacteristics by applying different refinements to the algorithm. The\ndeveloped designs are compiled and tested under Celoxica's RC-1000\nreconfigurable computer with its 2 million gates Virtex-E FPGA. Performance\nanalysis and evaluation of these implementations are included.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 22:15:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Damaj", "Issam", "", "Rafik Hariri University"]]}, {"id": "1904.03838", "submitter": "Canh Le Duc", "authors": "Duc-Canh Le, Chan-Hyun Youn", "title": "Criteria and Approaches for Virtualization on Modern FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern field programmable gate arrays (FPGAs) can produce high performance in\na wide range of applications, and their computational capacity is becoming\nabundant in personal computers. Regardless of this fact, FPGA virtualization is\nan emerging research field. Nowadays, challenges of the research area come from\nnot only technical difficulties but also from the ambiguous standards of\nvirtualization. In this paper, we introduce novel criteria of FPGA\nvirtualization and discuss several approaches to accomplish those criteria. In\naddition, we present and describe in detail the specific FPGA virtualization\narchitecture that we developed on Intel Arria 10 FPGA. We evaluate our solution\nwith a combination of applications and microbenchmarks. The result shows that\nour virtualization solution can provide a full abstraction of FPGA device in\nboth user and developer perspective while maintaining a reasonable performance\ncompared to native FPGA.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:56:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Le", "Duc-Canh", ""], ["Youn", "Chan-Hyun", ""]]}, {"id": "1904.03997", "submitter": "Viacheslav Dubeyko", "authors": "Viacheslav Dubeyko", "title": "Comparative Analysis of Distributed and Parallel File Systems' Internal\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A file system optimization is the most common task in the file system field.\nUsually, it is seen as the key file system problem. Moreover, it is possible to\nstate that optimization is dominant in commercial development. A problem of a\nnew file system architecture development arises more frequently in academia.\nEnd-user can treat file system performance as the key problem of file system\nevolving as technology. Such understanding arises from common treatment of\npersistent memory as slow subsystem. As a result, problem of improving\nperformance of data processing treats as a problem of file system performance\noptimization. However, evolution of physical technologies of persistent data\nstorage requires significant changing of concepts and approaches of file\nsystems' internal techniques. Generally speaking, only trying to improve the\nfile system efficiency cannot resolve all issue of file systems as\ntechnological direction. Moreover, it can impede evolution of file system\ntechnology at whole. It is impossible to satisfy end-user's expectations by\nmeans of file systems optimization only. New persistent storage technologies\ncan question about file systems necessity at whole without suggestion of\nrevolutionary new file system's approaches. However, file system contains\nparadigm of information structuring that is very important for end-user as a\nhuman being. It needs to distinguish the two classes of tasks: (1) optimization\ntask; (2) task of elaboration a new architecture vision or paradigm. But,\nfrequently, project goal degenerates into optimization task which is meant\nreally elaboration of a new paradigm. End-user expectations are complex and\ncontradictory set of requirements. Only optimization tasks cannot resolve the\nall current needs of end-user in the file system field. End-user's expectations\nrequire resolving tasks of a new architecture vision or paradigm elaboration.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:55:46 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Dubeyko", "Viacheslav", ""]]}, {"id": "1904.04003", "submitter": "Carla Mouradian", "authors": "Carla Mouradian, Somayeh Kianpisheh, Mohammad Abu-Lebdeh, Fereshteh\n  Ebrahimnezhad, Narjes Tahghigh Jahromi, Roch H. Glitho", "title": "Application Component Placement in NFV-based Hybrid Cloud/Fog Systems\n  with Mobile Fog Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing reduces the latency induced by distant clouds by enabling the\ndeployment of some application components at the edge of the network, on fog\nnodes, while keeping others in the cloud. Application components can be\nimplemented as Virtual Network Functions (VNFs) and their execution sequences\ncan be modeled by a combination of sub-structures like sequence, parallel,\nselection, and loops. Efficient placement algorithms are required to map the\napplication components onto the infrastructure nodes. Current solutions do not\nconsider the mobility of fog nodes, a phenomenon which may happen in real\nsystems. In this paper, we use the random waypoint mobility model for fog nodes\nto calculate the expected makespan and application execution cost. We then\nmodel the problem as an Integer Linear Programming (ILP) formulation which\nminimizes an aggregated weighted function of the makespan and cost. We propose\na Tabu Search-based Component Placement (TSCP) algorithm to find sub-optimal\nplacements. The results show that the proposed algorithm improves the makespan\nand the application execution cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 13:54:10 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mouradian", "Carla", ""], ["Kianpisheh", "Somayeh", ""], ["Abu-Lebdeh", "Mohammad", ""], ["Ebrahimnezhad", "Fereshteh", ""], ["Jahromi", "Narjes Tahghigh", ""], ["Glitho", "Roch H.", ""]]}, {"id": "1904.04024", "submitter": "Fan Yang", "authors": "Fan Yang, Zhan Wang, Xiaoxiao Ma, Guojun Yuan, Xuejun An", "title": "SwitchAgg:A Further Step Towards In-Network Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed applications adopt a partition/aggregation pattern to\nachieve high performance and scalability. The aggregation process, which\nusually takes a large portion of the overall execution time, incurs large\namount of network traffic and bottlenecks the system performance. To reduce\nnetwork traffic,some researches take advantage of network devices to commit\ninnetwork aggregation. However, these approaches use either special topology or\nmiddle-boxes, which cannot be easily deployed in current datacenters. The\nemerging programmable RMT switch brings us new opportunities to implement\nin-network computation task. However, we argue that the architecture of RMT\nswitch is not suitable for in-network aggregation since it is designed\nprimarily for implementing traditional network functions. In this paper, we\nfirst give a detailed analysis of in-network aggregation, and point out the key\nfactor that affects the data reduction ratio. We then propose SwitchAgg, which\nis an innetwork aggregation system that is compatible with current datacenter\ninfrastructures. We also evaluate the performance improvement we have gained\nfrom SwitchAgg. Our results show that, SwitchAgg can process data aggregation\ntasks at line rate and gives a high data reduction rate, which helps us to cut\ndown network traffic and alleviate pressure on server CPU. In the system\nperformance test, the job-completion-time can be reduced as much as 50%\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 03:09:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Yang", "Fan", ""], ["Wang", "Zhan", ""], ["Ma", "Xiaoxiao", ""], ["Yuan", "Guojun", ""], ["An", "Xuejun", ""]]}, {"id": "1904.04026", "submitter": "Moonmoon Chakraborty", "authors": "Moonmoon Chakraborty", "title": "Fog Computing Vs. Cloud Computing", "comments": "10 pages, 1 figure, 1 table", "journal-ref": null, "doi": "10.6084/m9.figshare.7886126", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives an overview of what Fog computing is, its uses and the\ncomparison between Fog computing and Cloud computing. Cloud is performing well\nin todays World and boosting the ability to use the internet more than ever.\nCloud computing gradually developed a method to use the benefits of it in most\nof the organizations. Fog computing can be apparent both in big data structures\nand large cloud systems, making reference to the growing complications in\nretrieving the data accurately. Fog computing is outspreading cloud computing\nby transporting computation on the advantage of network systems such as cell\nphone devices or fixed nodes with in-built data storage. Fog provides important\npoints of improved abilities, strong security controls, and processes,\nestablish data transmission capabilities carefully and in a flexible manner.\nThis paper gives an overview of the connections and attributes for both Fog\ncomputing and cloud varies by outline, preparation, directions, and strategies\nfor associations and clients. This also explains how Fog computing is flexible\nand provide better service for data processing by overwhelming low network\nbandwidth instead of moving whole data to the cloud platform.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 18:16:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chakraborty", "Moonmoon", ""]]}, {"id": "1904.04031", "submitter": "Didier El Baz", "authors": "Jia Luo (LAAS-CDA), Didier El Baz (LAAS)", "title": "A Survey on Parallel Genetic Algorithms for Shop Scheduling Problems", "comments": null, "journal-ref": "2018 IEEE International Parallel and Distributed Processing\n  Symposium Workshops (IPDPSW), May 2018, Vancouver, France. IEEE, pp.629-636", "doi": "10.1109/IPDPSW.2018.00103", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been extensive works dealing with genetic algorithms (GAs) for\nseeking optimal solutions of shop scheduling problems. Due to the NP hardness,\nthe time cost is always heavy. With the development of high performance\ncomputing (HPC) in last decades, the interest has been focused on parallel GAs\nfor shop scheduling problems. In this paper, we present the state of the art\nwith respect to the recent works on solving shop scheduling problems using\nparallel GAs. It showcases the most representative publications in this field\nby the categorization of parallel GAs and analyzes their designs based on the\nframeworks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:56:27 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Jia", "", "LAAS-CDA"], ["Baz", "Didier El", "", "LAAS"]]}, {"id": "1904.04034", "submitter": "Didier El Baz", "authors": "Didier El Baz (LAAS), Li Zhu", "title": "Smart systems, the fourth industrial revolution and new challenges in\n  distributed computing", "comments": null, "journal-ref": "2018", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart systems and the smart world concept are addressed in the framework of\nthe fourth industrial revolution. New challenges in distributed autonomous\nrobots and computing are considered. An illustration of a new kind of smart and\nreconfigurable distributed modular robot system is given. A prototype is also\npresented as well as the associated distributed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:58:51 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Baz", "Didier El", "", "LAAS"], ["Zhu", "Li", ""]]}, {"id": "1904.04098", "submitter": "Yang Xiao", "authors": "Yang Xiao, Ning Zhang, Wenjing Lou, Y. Thomas Hou", "title": "A Survey of Distributed Consensus Protocols for Blockchain Networks", "comments": "Accepted by the IEEE Communications Surveys and Tutorials for\n  publication", "journal-ref": null, "doi": "10.1109/COMST.2020.2969706", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the inception of Bitcoin, cryptocurrencies and the underlying\nblockchain technology have attracted an increasing interest from both academia\nand industry. Among various core components, consensus protocol is the defining\ntechnology behind the security and performance of blockchain. From incremental\nmodifications of Nakamoto consensus protocol to innovative alternative\nconsensus mechanisms, many consensus protocols have been proposed to improve\nthe performance of the blockchain network itself or to accommodate other\nspecific application needs.\n  In this survey, we present a comprehensive review and analysis on the\nstate-of-the-art blockchain consensus protocols. To facilitate the discussion\nof our analysis, we first introduce the key definitions and relevant results in\nthe classic theory of fault tolerance which help to lay the foundation for\nfurther discussion. We identify five core components of a blockchain consensus\nprotocol, namely, block proposal, block validation, information propagation,\nblock finalization, and incentive mechanism. A wide spectrum of blockchain\nconsensus protocols are then carefully reviewed accompanied by algorithmic\nabstractions and vulnerability analyses. The surveyed consensus protocols are\nanalyzed using the five-component framework and compared with respect to\ndifferent performance metrics. These analyses and comparisons provide us new\ninsights in the fundamental differences of various proposals in terms of their\nsuitable application scenarios, key assumptions, expected fault tolerance,\nscalability, drawbacks and trade-offs. We believe this survey will provide\nblockchain developers and researchers a comprehensive view on the\nstate-of-the-art consensus protocols and facilitate the process of designing\nfuture protocols.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:41:24 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 17:23:32 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 07:20:17 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 03:46:53 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Xiao", "Yang", ""], ["Zhang", "Ning", ""], ["Lou", "Wenjing", ""], ["Hou", "Y. Thomas", ""]]}, {"id": "1904.04174", "submitter": "John Lawson", "authors": "Rod Burns, John Lawson, Duncan McBain and Daniel Soutar", "title": "Accelerated Neural Networks on OpenCL Devices Using SYCL-DNN", "comments": "4 pages, 3 figures. In International Workshop on OpenCL (IWOCL '19),\n  May 13-15, 2019, Boston", "journal-ref": null, "doi": "10.1145/3318170.3318183", "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years machine learning has seen a renewed explosion of\ninterest, following a number of studies showing the effectiveness of neural\nnetworks in a range of tasks which had previously been considered incredibly\nhard. Neural networks' effectiveness in the fields of image recognition and\nnatural language processing stems primarily from the vast amounts of data\navailable to companies and researchers, coupled with the huge amounts of\ncompute power available in modern accelerators such as GPUs, FPGAs and ASICs.\nThere are a number of approaches available to developers for utilizing GPGPU\ntechnologies such as SYCL, OpenCL and CUDA, however many applications require\nthe same low level mathematical routines. Libraries dedicated to accelerating\nthese common routines allow developers to easily make full use of the available\nhardware without requiring low level knowledge of the hardware themselves,\nhowever such libraries are often provided by hardware manufacturers for\nspecific hardware such as cuDNN for Nvidia hardware or MIOpen for AMD hardware.\n  SYCL-DNN is a new open-source library dedicated to providing accelerated\nroutines for neural network operations which are hardware and vendor agnostic.\nBuilt on top of the SYCL open standard and written entirely in standard C++,\nSYCL-DNN allows a user to easily accelerate neural network code for a wide\nrange of hardware using a modern C++ interface. The library is tested on AMD's\nOpenCL for GPU, Intel's OpenCL for CPU and GPU, ARM's OpenCL for Mali GPUs as\nwell as ComputeAorta's OpenCL for R-Car CV engine and host CPU. In this talk we\nwill present performance figures for SYCL-DNN on this range of hardware, and\ndiscuss how high performance was achieved on such a varied set of accelerators\nwith such different hardware features.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:29:40 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Burns", "Rod", ""], ["Lawson", "John", ""], ["McBain", "Duncan", ""], ["Soutar", "Daniel", ""]]}, {"id": "1904.04250", "submitter": "Adrian Jackson", "authors": "Adrian Jackson, Andrew Turner, Michele Weiland, Nick Johnson, Olly\n  Perks, Mark Parsons", "title": "Evaluating the Arm Ecosystem for High Performance Computing", "comments": "18 pages, accepted at PASC19, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, Arm-based processors have arrived on the HPC scene, offering\nan alternative the existing status quo, which was largely dominated by x86\nprocessors. In this paper, we evaluate the Arm ecosystem, both the hardware\noffering and the software stack that is available to users, by benchmarking a\nproduction HPC platform that uses Marvell's ThunderX2 processors. We\ninvestigate the performance of complex scientific applications across multiple\nnodes, and we also assess the maturity of the software stack and the ease of\nuse from a users' perspective. This papers finds that the performance across\nour benchmarking applications is generally as good as, or better, than that of\nwell-established platforms, and we can conclude from our experience that there\nare no major hurdles that might hinder wider adoption of this ecosystem within\nthe HPC community.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:53:40 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Jackson", "Adrian", ""], ["Turner", "Andrew", ""], ["Weiland", "Michele", ""], ["Johnson", "Nick", ""], ["Perks", "Olly", ""], ["Parsons", "Mark", ""]]}, {"id": "1904.04279", "submitter": "Chen Yuan", "authors": "Guangyi Liu, Chen Yuan, Xi Chen, Jingjin Wu, Renchang Dai, Zhiwei Wang", "title": "A High-Performance Energy Management System based on Evolving Graph", "comments": "5 pages, 6 figures, 4 tables, accepted by IEEE Transactions on\n  Circuits and Systems II: Express Briefs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the fast growth and large integration of distributed generation, renewable\nenergy resource, energy storage system and load response, the modern power\nsystem operation becomes much more complicated with increasing uncertainties\nand frequent changes. Increased operation risks are introduced to the existing\ncommercial Energy Management System (EMS), due to its limited computational\ncapability. In this paper, a high-performance EMS analysis framework based on\nthe evolving graph is developed. A power grid is first modeled as an evolving\ngraph and then the power system dynamic analysis applications, like network\ntopology processing (NTP), state estimation (SE), power flow (PF), and\ncontingency analysis (CA), are efficiently implemented on the system evolving\ngraph to build a high-performance EMS analysis framework. Its computation\nperformance is field tested using a 2749-bus power system in Sichuan, China.\nThe results illustrate that the proposed EMS remarkably speeds up the\ncomputation performance and reaches the goal of real-time power system\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:16:07 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Liu", "Guangyi", ""], ["Yuan", "Chen", ""], ["Chen", "Xi", ""], ["Wu", "Jingjin", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1904.04291", "submitter": "Christina Peterson", "authors": "Kishore Debnath, Christina Peterson, Damian Dechev", "title": "Analysis of Commutativity with State-Chart Graph Representation of\n  Concurrent Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to check for commutativity in concurrent programs\nfrom their state-chart graphs. A set of operations are commutative if changing\nthe order of their execution on an object does not affect the abstract state of\nthe object and returns the same response. Concurrent operations that commute at\nobject-level can be executed concurrently at transaction-level, which boosts\nperformance while preserving the appearance of atomicity and isolation.\nUtilizing object-level commutativity in transactional execution enables the\nreuse of existing non-blocking programming techniques for thread-level\nsynchronization. In our approach, we generate state-chart graphs by tracking\ndata on the atomic instructions invoked on the concurrent object during model\nchecking and represent the atomic instructions as states in a state-transition\nrepresentation. Considering the non-deterministic nature of concurrent\nprograms, we determine commutativity by exhaustively searching for identical\nobject states captured at a thread-level granularity across all thread\ninterleavings. With this methodology, a user can not only verify commutativity\namong operations, but also can visually check ways in which methods commute at\nobject-level, which is an edge over current state-of-the-art tools. The\nobject-level commutative information helps in identifying faulty\nimplementations and performance improvement considerations. We use the graph\ndatabase, Neo4j, to represent object states as nodes that further assists the\nuser to check for concurrency properties using Cypher queries.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:30:47 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Debnath", "Kishore", ""], ["Peterson", "Christina", ""], ["Dechev", "Damian", ""]]}, {"id": "1904.04318", "submitter": "Rui Zhang", "authors": "Rui Zhang and Quanyan Zhu", "title": "Consensus-based Distributed Discrete Optimal Transport for Decentralized\n  Resource Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport has been used extensively in resource matching to promote\nthe efficiency of resources usages by matching sources to targets. However, it\nrequires a significant amount of computations and storage spaces for\nlarge-scale problems. In this paper, we take a consensus-based approach to\ndecentralize discrete optimal transport problems and develop fully distributed\nalgorithms with alternating direction method of multipliers. We show that our\nalgorithms guarantee certain levels of efficiency and privacy besides the\ndistributed nature. We further derive primal and dual algorithms by exploring\nthe primal and dual problems of discrete optimal transport with linear utility\nfunctions and prove the equivalence between them. We verify the convergence,\nonline adaptability, and the equivalence between the primal algorithm and the\ndual algorithm with numerical experiments. Our algorithms reflect the\nbargaining between sources and targets on the amounts and prices of transferred\nresources and reveal an averaging principle which can be used to regulate\nresource markets and improve resource efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 19:32:55 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zhang", "Rui", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1904.04341", "submitter": "Mohit Daga", "authors": "Mohit Daga, Monika Henzinger, Danupon Nanongkai, Thatchaphol Saranurak", "title": "Distributed Edge Connectivity in Sublinear Time", "comments": "Accepted at 51st ACM Symposium on Theory of Computing (STOC 2019)", "journal-ref": null, "doi": "10.1145/3313276.3316346", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first sublinear-time algorithm for a distributed\nmessage-passing network sto compute its edge connectivity $\\lambda$ exactly in\nthe CONGEST model, as long as there are no parallel edges. Our algorithm takes\n$\\tilde O(n^{1-1/353}D^{1/353}+n^{1-1/706})$ time to compute $\\lambda$ and a\ncut of cardinality $\\lambda$ with high probability, where $n$ and $D$ are the\nnumber of nodes and the diameter of the network, respectively, and $\\tilde O$\nhides polylogarithmic factors. This running time is sublinear in $n$ (i.e.\n$\\tilde O(n^{1-\\epsilon})$) whenever $D$ is. Previous sublinear-time\ndistributed algorithms can solve this problem either (i) exactly only when\n$\\lambda=O(n^{1/8-\\epsilon})$ [Thurimella PODC'95; Pritchard, Thurimella, ACM\nTrans. Algorithms'11; Nanongkai, Su, DISC'14] or (ii) approximately [Ghaffari,\nKuhn, DISC'13; Nanongkai, Su, DISC'14].\n  To achieve this we develop and combine several new techniques. First, we\ndesign the first distributed algorithm that can compute a $k$-edge connectivity\ncertificate for any $k=O(n^{1-\\epsilon})$ in time $\\tilde O(\\sqrt{nk}+D)$.\nSecond, we show that by combining the recent distributed expander decomposition\ntechnique of [Chang, Pettie, Zhang, SODA'19] with techniques from the\nsequential deterministic edge connectivity algorithm of [Kawarabayashi, Thorup,\nSTOC'15], we can decompose the network into a sublinear number of clusters with\nsmall average diameter and without any mincut separating a cluster (except the\n`trivial' ones). Finally, by extending the tree packing technique from [Karger\nSTOC'96], we can find the minimum cut in time proportional to the number of\ncomponents. As a byproduct of this technique, we obtain an $\\tilde O(n)$-time\nalgorithm for computing exact minimum cut for weighted graphs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:23:20 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Daga", "Mohit", ""], ["Henzinger", "Monika", ""], ["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1904.04396", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Kenjiro Cho, KC Claffy, Vijay Gadepally, Peter\n  Michaleas, Lauren Milechin", "title": "Hypersparse Neural Network Analysis of Large-Scale Internet Traffic", "comments": "11 pages, 10 figures, 3 tables, 60 citations; to appear in IEEE High\n  Performance Extreme Computing (HPEC) 2019", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916263", "report-no": null, "categories": "cs.NI cs.CY cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet is transforming our society, necessitating a quantitative\nunderstanding of Internet traffic. Our team collects and curates the largest\npublicly available Internet traffic data containing 50 billion packets.\nUtilizing a novel hypersparse neural network analysis of \"video\" streams of\nthis traffic using 10,000 processors in the MIT SuperCloud reveals a new\nphenomena: the importance of otherwise unseen leaf nodes and isolated links in\nInternet traffic. Our neural network approach further shows that a\ntwo-parameter modified Zipf-Mandelbrot distribution accurately describes a wide\nvariety of source/destination statistics on moving sample windows ranging from\n100,000 to 100,000,000 packets over collections that span years and continents.\nThe inferred model parameters distinguish different network streams and the\nmodel leaf parameter strongly correlates with the fraction of the traffic in\ndifferent underlying network topologies. The hypersparse neural network\npipeline is highly adaptable and different network statistics and training\nmodels can be incorporated with simple changes to the image filter functions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 23:31:38 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 23:00:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kepner", "Jeremy", ""], ["Cho", "Kenjiro", ""], ["Claffy", "KC", ""], ["Gadepally", "Vijay", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""]]}, {"id": "1904.04626", "submitter": "Apostolos Papadopoulos", "authors": "Panagiotis Kostoglou, Apostolos N. Papadopoulos, Yannis Manolopoulos", "title": "Distributed Computation of Top-$k$ Degrees in Hidden Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden graphs are flexible abstractions that are composed of a set of known\nvertices (nodes), whereas the set of edges are not known in advance. To uncover\nthe set of edges, multiple edge probing queries must be executed by evaluating\na function $f(u,v)$ that returns either true or false, if nodes $u$ and $v$ are\nconnected or not respectively. Evidently, the graph can be revealed completely\nif all possible $n(n-1)/2$ probes are executed for a graph containing $n$\nnodes. However, the function $f()$ is usually computationally intensive and\ntherefore executing all possible probing queries result in high execution\ncosts. The target is to provide answers to useful queries by executing as few\nprobing queries as possible. In this work, we study the problem of discovering\nthe top-$k$ nodes of a hidden bipartite graph with the highest degrees, by\nusing distributed algorithms. In particular, we use Apache Spark and provide\nexperimental results showing that significant performance improvements are\nachieved in comparison to existing centralized approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 12:43:33 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Kostoglou", "Panagiotis", ""], ["Papadopoulos", "Apostolos N.", ""], ["Manolopoulos", "Yannis", ""]]}, {"id": "1904.04702", "submitter": "Paul Ezhilchelvan", "authors": "Jim Webber, Paul Ezhilchelvan and Isi Mitrani", "title": "Modeling Corruption in Eventually-Consistent Graph Databases", "comments": "6 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a model and analysis of an eventually consistent graph database\nwhere loosely cooperating servers accept concurrent updates to a partitioned,\ndistributed graph. The model is high-fidelity and preserves design choices from\ncontemporary graph database management systems. To explore the problem space,\nwe use two common graph topologies as data models for realistic\nexperimentation. The analysis reveals, even assuming completely fault-free\nhardware and bug-free software, that if it is possible for updates to interfere\nwith one-another, corruption will occur and spread significantly through the\ngraph within the production database lifetime. Using our model, database\ndesigners and operators can compute the rate of corruption for their systems\nand determine whether they are sufficiently dependable for their intended use.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:35:42 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Webber", "Jim", ""], ["Ezhilchelvan", "Paul", ""], ["Mitrani", "Isi", ""]]}, {"id": "1904.04736", "submitter": "Marcus Paradies", "authors": "Bunjamin Memishi, Raja Appuswamy, Marcus Paradies", "title": "Cold Storage Data Archives: More Than Just a Bunch of Tapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of available sensor and derived data from large scientific\nexperiments, such as earth observation programs, radio astronomy sky surveys,\nand high-energy physics already exceeds the storage hardware globally\nfabricated per year. To that end, cold storage data archives are the---often\noverlooked---spearheads of modern big data analytics in scientific,\ndata-intensive application domains. While high-performance data analytics has\nreceived much attention from the research community, the growing number of\nproblems in designing and deploying cold storage archives has only received\nvery little attention.\n  In this paper, we take the first step towards bridging this gap in knowledge\nby presenting an analysis of four real-world cold storage archives from three\ndifferent application domains. In doing so, we highlight (i) workload\ncharacteristics that differentiate these archives from traditional,\nperformance-sensitive data analytics, (ii) design trade-offs involved in\nbuilding cold storage systems for these archives, and (iii) deployment\ntrade-offs with respect to migration to the public cloud. Based on our\nanalysis, we discuss several other important research challenges that need to\nbe addressed by the data management community.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:36:06 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Memishi", "Bunjamin", ""], ["Appuswamy", "Raja", ""], ["Paradies", "Marcus", ""]]}, {"id": "1904.05069", "submitter": "Moustafa Nakechbandi", "authors": "Jean-Yves Colin (LITIS), Herv\\'e Mathieu (GIN), Moustafa Nakechbandi\n  (LITIS)", "title": "A Proposal for an Open Logistics Interconnection Reference Model for a\n  Physical Internet", "comments": null, "journal-ref": "GOL 2016, The 3rd International IEEE Conference on Logistics\n  Operations Management, May 2016, FEZ, Morocco", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a New Open Logistics Interconnection (NOLI) reference\nmodel for a Physical Internet, inspired by the Open Systems Interconnection\n(OSI) reference model for data networks. This NOLI model is compared to the OSI\nmodel, and to the Transmission Control Protocol/Internet Protocol (TCP/IP)\nmodel of Internet. It is also compared to the OLI model for a Physical Internet\nproposed by Montreuil. The main differences between the presented NOLI model\nand all the other models named above are in the appearance of definitions of\nphysical objects in different layers and not just the lowest one. Also, the\nNOLI model we present locates the containerization and de-containerization\noperations in the topmost layer, and not in the layer below as does the OLI\nmodel. Finally, the NOLI model is closer to the TCP/IP and OSI models than the\nOLI model, keeping the integrity of the Link Layer that the OLI model divides\nin two layers, and keeping separate the Session and Transport OSI Layers that\nthe OLI model unites in just one layer.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:53:03 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Colin", "Jean-Yves", "", "LITIS"], ["Mathieu", "Herv\u00e9", "", "GIN"], ["Nakechbandi", "Moustafa", "", "LITIS"]]}, {"id": "1904.05208", "submitter": "Rima Kriauzien\\.e", "authors": "Rima Kriauzien\\.e, Andrej Bugajev, and Raimondas \\v{C}iegis", "title": "A Three-Level Parallelisation Scheme and Application to the Nelder-Mead\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a three-level parallelisation scheme. The second and third levels\ndefine a classical two-level parallelisation scheme and some load balancing\nalgorithm is used to distribute tasks among processes. It is well-known that\nfor many applications the efficiency of parallel algorithms of the second and\nthird level starts to drop down after some critical parallelisation degree is\nreached. This weakness of the two-level template is addressed by introduction\nof one additional parallelisation level. As an alternative to the basic solver\nsome new or modified algorithms are considered on this level. The idea of the\nproposed methodology is to increase the parallelisation degree by using less\nefficient algorithms in comparison with the basic solver. As an example we\ninvestigate two modified Nelder-Mead methods. For the selected application, a\nfew partial differential equations are solved numerically on the second level,\nand on the third level the parallel Wang's algorithm is used to solve systems\nof linear equations with tridiagonal matrices. A greedy workload balancing\nheuristic is proposed, which is oriented to the case of a large number of\navailable processors. The complexity estimates of the computational tasks are\nmodel-based, i.e. they use empirical computational data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 14:24:36 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 16:07:32 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Kriauzien\u0117", "Rima", ""], ["Bugajev", "Andrej", ""], ["\u010ciegis", "Raimondas", ""]]}, {"id": "1904.05211", "submitter": "Dylan Chapp", "authors": "Dylan Chapp, Danny Rorabaugh, Duncan Brown, Ewa Deelman, Karan Vahi,\n  Von Welch, and Michela Taufer", "title": "Applicability study of the PRIMAD model to LIGO gravitational wave\n  search workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PRIMAD model with its six components (i.e., Platform, Research Objective,\nImplementation, Methods, Actors, and Data), provides an abstract taxonomy to\nrepresent computational experiments and enforce reproducibility by design. In\nthis paper, we assess the model applicability to a set of Laser Interferometer\nGravitational-Wave Observatory (LIGO) workflows from literature sources (i.e.,\npublished papers). Our work outlines potentials and limits of the model in\nterms of its abstraction levels and application process.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 14:27:35 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chapp", "Dylan", ""], ["Rorabaugh", "Danny", ""], ["Brown", "Duncan", ""], ["Deelman", "Ewa", ""], ["Vahi", "Karan", ""], ["Welch", "Von", ""], ["Taufer", "Michela", ""]]}, {"id": "1904.05218", "submitter": "Tamara Radivilova A", "authors": "Kirichenko Lyudmyla and Radivilova Tamara", "title": "Analyzes of the Distributed System Load with Multifractal Input Data\n  Flows", "comments": "5 pages", "journal-ref": "2017 14th International Conference The Experience of Designing and\n  Application of CAD Systems in Microelectronics (CADSM), Lviv, 2017, pp.\n  260-264", "doi": "10.1109/CADSM.2017.7916130", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a solution an actual scientific problem related to load\nbalancing and efficient utilization of resources of the distributed system. The\nproposed method is based on calculation of load CPU, memory, and bandwidth by\nflows of different classes of service for each server and the entire\ndistributed system and taking into account multifractal properties of input\ndata flows. Weighting factors were introduced that allow to determine the\nsignificance of the characteristics of server relative to each other. Thus,\nthis method allows to calculate the imbalance of the all system servers and\nsystem utilization. The simulation of the proposed method for different\nmultifractal parameters of input flows was conducted. The simulation showed\nthat the characteristics of multifractal traffic have a appreciable effect on\nthe system imbalance. The usage of proposed method allows to distribute\nrequests across the servers thus that the deviation of the load servers from\nthe average value was minimal, that allows to get a higher metrics of system\nperformance and faster processing flows.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 14:40:28 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Lyudmyla", "Kirichenko", ""], ["Tamara", "Radivilova", ""]]}, {"id": "1904.05275", "submitter": "Estela Suarez", "authors": "Anke Kreuzer, Jorge Amaya, Norbert Eicker, Estela Suarez", "title": "Application performance on a Cluster-Booster system", "comments": "10 pages, 8 figures, IPDPS 2018 workshop HCW", "journal-ref": "2018 IEEE International Parallel and Distributed Processing\n  Symposium Workshops (IPDPSW), IPDPS, Vancouver, Canada, 21 May 2018 - 25 May\n  2018 IEEE 69 - 78 (2018)", "doi": "10.1109/IPDPSW.2018.00019", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DEEP projects have developed a variety of hardware and software\ntechnologies aiming at improving the efficiency and usability of next\ngeneration high-performance computers. They evolve around an innovative concept\nfor heterogeneous systems: the Cluster-Booster architecture. In it, a general\npurpose cluster is tightly coupled to a many-core system (the Booster). This\nmodular way of integrating heterogeneous components enables applications to\nfreely choose the kind of computing resources on which it runs most\nefficiently. Codes might even be partitioned to map specific requirements of\ncode-parts onto the best suited hardware. This paper presents for the first\ntime measurements done by a real world scientific application demonstrating the\nperformance gain achieved with this kind of code-partition approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:23:52 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Kreuzer", "Anke", ""], ["Amaya", "Jorge", ""], ["Eicker", "Norbert", ""], ["Suarez", "Estela", ""]]}, {"id": "1904.05316", "submitter": "Steve Jeffrey Tueno Fotso", "authors": "Steve Tueno, Romeo Tabue, Forentin Jiechieu, Yacynth Ndonna, Billy\n  Zafack, Audric Feuyan, Jonas Atibita, Alex Djouontse and Rodrigue Mbinkeu", "title": "Pear2Pear (On Wifi): A Data Sharing Protocol Over Wifi through a Peer to\n  Peer Network", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A peer-to-peer system is a distributed system in which equal nodes (in terms\nof role and usage) exchange information and services directly. This paper\ndescribes a distributed peer-to-peer protocol that allows wifi-enabled smart\ndevices (especially Android smartphones) to exchange data using only wifi. The\nprotocol is designed to allow the automatic establishment of a distributed\npeer-to-peer network of any size without geography constraint. It is applied to\nfile sharing between devices, but can easily be adapted to support any data\nsharing. The protocol defines two layers: (1) a kernel layer responsible for\ncreating, routing, establishing and maintaining links between nodes (peer),\naddressing a node and adding and removing nodes; and an (2) application layer\nto support data sharing (file sharing in this case). The structure of the\npeer-to-peer network is hybrid. Regarding the file sharing use case, the\napplication allows a node to (i) search for a file in the catalog of its subnet\nthat is held by the root node, and (ii) download a file: if the file is not in\nthe subnet, the root node delegates a node to make vouchers in neighboring\nsubnetworks to get the file and make it available. A proof of concept was made\non Android.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 21:38:59 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Tueno", "Steve", ""], ["Tabue", "Romeo", ""], ["Jiechieu", "Forentin", ""], ["Ndonna", "Yacynth", ""], ["Zafack", "Billy", ""], ["Feuyan", "Audric", ""], ["Atibita", "Jonas", ""], ["Djouontse", "Alex", ""], ["Mbinkeu", "Rodrigue", ""]]}, {"id": "1904.05347", "submitter": "Mehdi Goli", "authors": "John Lawson, Mehdi Goli, Duncan McBain, Daniel Soutar, Louis Sugy", "title": "Cross-Platform Performance Portability Using Highly Parametrized SYCL\n  Kernels", "comments": "11 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over recent years heterogeneous systems have become more prevalent across HPC\nsystems, with over 100 supercomputers in the TOP500 incorporating GPUs or other\naccelerators. These hardware platforms have different performance\ncharacteristics and optimization requirements. In order to make the most of\nmultiple accelerators a developer has to provide implementations of their\nalgorithms tuned for each device. Hardware vendors provide libraries targeting\ntheir devices specifically, which provide good performance but frequently have\ndifferent API designs, hampering portability.\n  The SYCL programming model allows users to write heterogeneous programs using\ncompletely standard C++, and so developers have access to the power of C++\ntemplates when developing compute kernels. In this paper we show that by\nwriting highly parameterized kernels for matrix multiplies and convolutions we\nachieve performance competitive with vendor implementations across different\narchitectures. Furthermore, tuning for new devices amounts to choosing the\ncombinations of kernel parameters that perform best on the hardware.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:58:23 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Lawson", "John", ""], ["Goli", "Mehdi", ""], ["McBain", "Duncan", ""], ["Soutar", "Daniel", ""], ["Sugy", "Louis", ""]]}, {"id": "1904.05437", "submitter": "Issam Damaj", "authors": "Issam Damaj (Dhofar University)", "title": "Parallel Algorithms Development for Programmable Devices with\n  Application from Cryptography", "comments": "47 Pages, 16 Figures, 4 Tables. arXiv admin note: text overlap with\n  arXiv:1904.03756", "journal-ref": "Intl. Jrnl. Par. Prog. Springer. 35(2007) 529-572", "doi": "10.1007/s10766-007-0046-1", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfigurable devices, such as Field Programmable Gate Arrays (FPGAs), have\nbeen witnessing a considerable increase in density. State-of-the-art FPGAs are\ncomplex hybrid devices that contain up to several millions of gates. Recently,\nresearch effort has been going into higher-level parallelization and hardware\nsynthesis methodologies that can exploit such a programmable technology. In\nthis paper, we explore the effectiveness of one such formal methodology in the\ndesign of parallel versions of the Serpent cryptographic algorithm. The\nsuggested methodology adopts a functional programming notation for specifying\nalgorithms and for reasoning about them. The specifications are realized\nthrough the use of a combination of function decomposition strategies, data\nrefinement techniques, and off-the-shelf refinements based upon higher-order\nfunctions. The refinements are inspired by the operators of Communicating\nSequential Processes (CSP) and map easily to programs in Handel-C (a hardware\ndescription language). In the presented research, we obtain several parallel\nSerpent implementations with different performance characteristics. The\ndeveloped designs are tested under Celoxica's RC-1000 reconfigurable computer\nwith its 2 million gates Virtex-E FPGA. Performance analysis and evaluation of\nthese implementations are included.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:20:36 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Damaj", "Issam", "", "Dhofar University"]]}, {"id": "1904.05456", "submitter": "Mohammad Hosseini", "authors": "Boyang Peng, Mohammad Hosseini, Zhihao Hong, Reza Farivar, Roy\n  Campbell", "title": "R-Storm: Resource-Aware Scheduling in Storm", "comments": "Proceedings of the 16th Annual ACM Middleware Conference, Pages\n  149-161, Vancouver, BC, Canada, December 07 - 11, 2015", "journal-ref": "In Proceedings of the 16th Annual Middleware Conference\n  (Middleware 2015). ACM, New York, NY, USA, 149-161", "doi": "10.1145/2814576.2814808", "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The era of big data has led to the emergence of new systems for real-time\ndistributed stream processing, e.g., Apache Storm is one of the most popular\nstream processing systems in industry today. However, Storm, like many other\nstream processing systems lacks an intelligent scheduling mechanism. The\ndefault round-robin scheduling currently deployed in Storm disregards resource\ndemands and availability, and can therefore be inefficient at times. We present\nR-Storm (Resource-Aware Storm), a system that implements resource-aware\nscheduling within Storm. R-Storm is designed to increase overall throughput by\nmaximizing resource utilization while minimizing network latency. When\nscheduling tasks, R-Storm can satisfy both soft and hard resource constraints\nas well as minimizing network distance between components that communicate with\neach other. We evaluate R-Storm on set of micro-benchmark Storm applications as\nwell as Storm applications used in production at Yahoo! Inc. From our\nexperimental results we conclude that R-Storm achieves 30-47% higher throughput\nand 69-350% better CPU utilization than default Storm for the micro-benchmarks.\nFor the Yahoo! Storm applications, R-Storm outperforms default Storm by around\n50% based on overall throughput. We also demonstrate that R-Storm performs much\nbetter when scheduling multiple Storm applications than default Storm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:45:39 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Peng", "Boyang", ""], ["Hosseini", "Mohammad", ""], ["Hong", "Zhihao", ""], ["Farivar", "Reza", ""], ["Campbell", "Roy", ""]]}, {"id": "1904.05474", "submitter": "Stefan Neumann", "authors": "Monika Henzinger, Stefan Neumann, Stefan Schmid", "title": "Efficient Distributed Workload (Re-)Embedding", "comments": "To appear at SIGMETRICS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern networked systems are increasingly reconfigurable, enabling\ndemand-aware infrastructures whose resources can be adjusted according to the\nworkload they currently serve. Such dynamic adjustments can be exploited to\nimprove network utilization and hence performance, by moving frequently\ninteracting communication partners closer, e.g., collocating them in the same\nserver or datacenter. However, dynamically changing the embedding of workloads\nis algorithmically challenging: communication patterns are often not known\nahead of time, but must be learned. During the learning process, overheads\nrelated to unnecessary moves (i.e., re-embeddings) should be minimized. This\npaper studies a fundamental model which captures the tradeoff between the\nbenefits and costs of dynamically collocating communication partners on $\\ell$\nservers, in an online manner. Our main contribution is a distributed online\nalgorithm which is asymptotically almost optimal, i.e., almost matches the\nlower bound (also derived in this paper) on the competitive ratio of any\n(distributed or centralized) online algorithm. As an application, we show that\nour algorithm can be used to solve a distributed union find problem in which\nthe sets are stored across multiple servers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 23:19:45 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Henzinger", "Monika", ""], ["Neumann", "Stefan", ""], ["Schmid", "Stefan", ""]]}, {"id": "1904.05522", "submitter": "Chien-Sheng Yang", "authors": "Chien-Sheng Yang, Ramtin Pedarsani, A. Salman Avestimehr", "title": "Timely-Throughput Optimal Coded Computing over Cloud Networks", "comments": "to appear in MobiHoc 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern distributed computing systems, unpredictable and unreliable\ninfrastructures result in high variability of computing resources. Meanwhile,\nthere is significantly increasing demand for timely and event-driven services\nwith deadline constraints. Motivated by measurements over Amazon EC2 clusters,\nwe consider a two-state Markov model for variability of computing speed in\ncloud networks. In this model, each worker can be either in a good state or a\nbad state in terms of the computation speed, and the transition between these\nstates is modeled as a Markov chain which is unknown to the scheduler. We then\nconsider a Coded Computing framework, in which the data is possibly encoded and\nstored at the worker nodes in order to provide robustness against nodes that\nmay be in a bad state. With timely computation requests submitted to the system\nwith computation deadlines, our goal is to design the optimal computation-load\nallocation scheme and the optimal data encoding scheme that maximize the timely\ncomputation throughput (i.e, the average number of computation tasks that are\naccomplished before their deadline). Our main result is the development of a\ndynamic computation strategy called Lagrange Estimate-and Allocate (LEA)\nstrategy, which achieves the optimal timely computation throughput. It is shown\nthat compared to the static allocation strategy, LEA increases the timely\ncomputation throughput by 1.4X - 17.5X in various scenarios via simulations and\nby 1.27X - 6.5X in experiments over Amazon EC2 clusters\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 04:16:00 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Yang", "Chien-Sheng", ""], ["Pedarsani", "Ramtin", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1904.05553", "submitter": "Phu Lai", "authors": "Phu Lai, Qiang He, Mohamed Abdelrazek, Feifei Chen, John Hosking, John\n  Grundy, Yun Yang", "title": "Optimal Edge User Allocation in Edge Computing with Variable Sized\n  Vector Bin Packing", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-03596-9_15", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In mobile edge computing, edge servers are geographically distributed around\nbase stations placed near end-users to provide highly accessible and efficient\ncomputing capacities and services. In the mobile edge computing environment, a\nservice provider can deploy its service on hired edge servers to reduce\nend-to-end service delays experienced by its end-users allocated to those edge\nservers. An optimal deployment must maximize the number of allocated end-users\nand minimize the number of hired edge servers while ensuring the required\nquality of service for end-users. In this paper, we model the edge user\nallocation (EUA) problem as a bin packing problem, and introduce a novel,\noptimal approach to solving the EUA problem based on the Lexicographic Goal\nProgramming technique. We have conducted three series of experiments to\nevaluate the proposed approach against two representative baseline approaches.\nExperimental results show that our approach significantly outperforms the other\ntwo approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 07:06:04 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Lai", "Phu", ""], ["He", "Qiang", ""], ["Abdelrazek", "Mohamed", ""], ["Chen", "Feifei", ""], ["Hosking", "John", ""], ["Grundy", "John", ""], ["Yang", "Yun", ""]]}, {"id": "1904.05627", "submitter": "Jukka Suomela", "authors": "Alkida Balliu, Juho Hirvonen, Christoph Lenzen, Dennis Olivetti, Jukka\n  Suomela", "title": "Locality of not-so-weak coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph problems are locally checkable: a solution is globally feasible if\nit looks valid in all constant-radius neighborhoods. This idea is formalized in\nthe concept of locally checkable labelings (LCLs), introduced by Naor and\nStockmeyer (1995). Recently, Chang et al. (2016) showed that in bounded-degree\ngraphs, every LCL problem belongs to one of the following classes:\n  - \"Easy\": solvable in $O(\\log^* n)$ rounds with both deterministic and\nrandomized distributed algorithms.\n  - \"Hard\": requires at least $\\Omega(\\log n)$ rounds with deterministic and\n$\\Omega(\\log \\log n)$ rounds with randomized distributed algorithms.\n  Hence for any parameterized LCL problem, when we move from local problems\ntowards global problems, there is some point at which complexity suddenly jumps\nfrom easy to hard. For example, for vertex coloring in $d$-regular graphs it is\nnow known that this jump is at precisely $d$ colors: coloring with $d+1$ colors\nis easy, while coloring with $d$ colors is hard.\n  However, it is currently poorly understood where this jump takes place when\none looks at defective colorings. To study this question, we define $k$-partial\n$c$-coloring as follows: nodes are labeled with numbers between $1$ and $c$,\nand every node is incident to at least $k$ properly colored edges.\n  It is known that $1$-partial $2$-coloring (a.k.a. weak $2$-coloring) is easy\nfor any $d \\ge 1$. As our main result, we show that $k$-partial $2$-coloring\nbecomes hard as soon as $k \\ge 2$, no matter how large a $d$ we have.\n  We also show that this is fundamentally different from $k$-partial\n$3$-coloring: no matter which $k \\ge 3$ we choose, the problem is always hard\nfor $d = k$ but it becomes easy when $d \\gg k$. The same was known previously\nfor partial $c$-coloring with $c \\ge 4$, but the case of $c < 4$ was open.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:11:02 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Balliu", "Alkida", ""], ["Hirvonen", "Juho", ""], ["Lenzen", "Christoph", ""], ["Olivetti", "Dennis", ""], ["Suomela", "Jukka", ""]]}, {"id": "1904.05736", "submitter": "Jingwei Li", "authors": "Jingwei Li and Patrick P. C. Lee and Chufeng Tan and Chuan Qin and\n  Xiaosong Zhang", "title": "Information Leakage in Encrypted Deduplication via Frequency Analysis:\n  Attacks and Defenses", "comments": "31 pages, Accepted by ACM Transactions on Storage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encrypted deduplication combines encryption and deduplication to\nsimultaneously achieve both data security and storage efficiency.\nState-of-the-art encrypted deduplication systems mainly build on deterministic\nencryption to preserve deduplication effectiveness. However, such deterministic\nencryption reveals the underlying frequency distribution of the original\nplaintext chunks. This allows an adversary to launch frequency analysis against\nthe ciphertext chunks and infer the content of the original plaintext chunks.\nIn this paper, we study how frequency analysis affects information leakage in\nencrypted deduplication storage, from both attack and defense perspectives.\nSpecifically, we target backup workloads, and propose a new inference attack\nthat exploits chunk locality to increase the coverage of inferred chunks. We\nfurther combine the new inference attack with the knowledge of chunk sizes and\nshow its attack effectiveness against variable-size chunks. We conduct\ntrace-driven evaluation on both real-world and synthetic datasets and show that\nour proposed attacks infer a significant fraction of plaintext chunks under\nbackup workloads. To defend against frequency analysis, we present two defense\napproaches, namely MinHash encryption and scrambling. Our trace-driven\nevaluation shows that our combined MinHash encryption and scrambling scheme\neffectively mitigates the severity of the inference attacks, while maintaining\nhigh storage efficiency and incurring limited metadata access overhead.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:49:34 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 14:07:27 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Li", "Jingwei", ""], ["Lee", "Patrick P. C.", ""], ["Tan", "Chufeng", ""], ["Qin", "Chuan", ""], ["Zhang", "Xiaosong", ""]]}, {"id": "1904.05787", "submitter": "Fr\\'ed\\'eric Gruau", "authors": "Frederic Gruau", "title": "Modular programming of computing media using spatial types, for\n  artificial physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our long term goal is to execute General Purpose computation on homogeneous\ncomputing media consisting of millions of small identical Processing Elements\n(PE) communicating locally. We proceed by simulating the Self-Development of a\nNetwork (SDN) of membranes, and this implies a medium able to implement\nartificial physics laws that simulates simplified membrane-agents, dividing and\nhomogenizing. This is a difficult challenge: our current version of SDN-media\nuses PEs with 77 bits of state and 13878 gates. This high level of complexity\nforced us to work out an efficient and expressive scheme for programming the\nmedium, the goal of this paper it to present it. The PE's communication graph\nhas to be a maximal planar graph. Fields of bits are spread in 2D, over three\nlocus: the vertices, edges and faces of this planar graph. They constitute\nthree data types, which abstract away the ensemble of PEs. The simplicial\nproximity between bits is used to define operations on fields, thus\nimplementing \"{\\it spatial type}\". Expression combining operations can be\ntranslated in logical circuits.\n  The efficiency is achieved because fields of different locus are combined\nusing reduction operation. This allows to factorize computation by exploiting\nthe symmetries always present when simulating physics.\n  The expressiveness is achieved by allowing a modular procedural programming:\nInstead of directly focusing on a specific target update function, we develop a\nlibrary or reusable functions mapping fields to other fields.\n  We consider two kinds of maximal planar graph: with isotropic distribution of\nPEs or with the hexagonal lattice structure. The first compares to amorphous\ncomputing medium and has a better potential for hardware scalability, the\nsecond compares with cellular automata computing medium, it is more efficient.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 15:41:45 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Gruau", "Frederic", ""]]}, {"id": "1904.05833", "submitter": "Anirban Bhattacharjee", "authors": "Yogesh D. Barve, Shashank Shekhar, Ajay Dev Chhokra, Shweta Khare,\n  Anirban Bhattacharjee, Zhuangwei Kang, Hongyang Sun, Aniruddha Gokhale", "title": "FECBench: A Holistic Interference-aware Approach for Application\n  Performance Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Services hosted in multi-tenant cloud platforms often encounter performance\ninterference due to contention for non-partitionable resources, which in turn\ncauses unpredictable behavior and degradation in application performance. To\ngrapple with these problems and to define effective resource management\nsolutions for their services, providers often must expend significant efforts\nand incur prohibitive costs in developing performance models of their services\nunder a variety of interference scenarios on different hardware. This is a hard\nproblem due to the wide range of possible co-located services and their\nworkloads, and the growing heterogeneity in the runtime platforms including the\nuse of fog and edge-based resources, not to mention the accidental complexity\nin performing application profiling under a variety of scenarios. To address\nthese challenges, we present FECBench, a framework to guide providers in\nbuilding performance interference prediction models for their services without\nincurring undue costs and efforts. The contributions of the paper are as\nfollows. First, we developed a technique to build resource stressors that can\nstress multiple system resources all at once in a controlled manner to gain\ninsights about the interference on an application's performance. Second, to\novercome the need for exhaustive application profiling, FECBench intelligently\nuses the design of experiments (DoE) approach to enable users to build\nsurrogate performance models of their services. Third, FECBench maintains an\nextensible knowledge base of application combinations that create resource\nstresses across the multi-dimensional resource design space. Empirical results\nusing real-world scenarios to validate the efficacy of FECBench show that the\npredicted application performance has a median error of only 7.6% across all\ntest cases, with 5.4% in the best case and 13.5% in the worst case.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:36:55 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 15:23:31 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Barve", "Yogesh D.", ""], ["Shekhar", "Shashank", ""], ["Chhokra", "Ajay Dev", ""], ["Khare", "Shweta", ""], ["Bhattacharjee", "Anirban", ""], ["Kang", "Zhuangwei", ""], ["Sun", "Hongyang", ""], ["Gokhale", "Aniruddha", ""]]}, {"id": "1904.05838", "submitter": "Amanda Bienz", "authors": "Amanda Bienz, Luke Olson, William Gropp", "title": "Reducing Communication in Algebraic Multigrid with Multi-step Node Aware\n  Communication", "comments": "11 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic multigrid (AMG) is often viewed as a scalable $\\mathcal{O}(n)$\nsolver for sparse linear systems. Yet, parallel AMG lacks scalability due to\nincreasingly large costs associated with communication, both in the initial\nconstruction of a multigrid hierarchy as well as the iterative solve phase.\nThis work introduces a parallel implementation of AMG to reduce the cost of\ncommunication, yielding an increase in scalability. Standard inter-process\ncommunication consists of sending data regardless of the send and receive\nprocess locations. Performance tests show notable differences in the cost of\nintra- and inter-node communication, motivating a restructuring of\ncommunication. In this case, the communication schedule takes advantage of the\nless costly intra-node communication, reducing both the number and size of\ninter-node messages. Node-centric communication extends to the range of\ncomponents in both the setup and solve phase of AMG, yielding an increase in\nthe weak and strong scalability of the entire method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:43:11 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 12:27:20 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Bienz", "Amanda", ""], ["Olson", "Luke", ""], ["Gropp", "William", ""]]}, {"id": "1904.05867", "submitter": "Tevfik Kosar", "authors": "Luigi Di Tacchio, Zulkar Nine, Tevfik Kosar, Fatih M. Bulut, and Jinho\n  Hwang", "title": "Energy-Efficient High-Throughput Data Transfers via Dynamic CPU\n  Frequency and Core Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy footprint of global data movement has surpassed 100 terawatt\nhours, costing more than 20 billion US dollars to the world economy. Depending\non the number of switches, routers, and hubs between the source and destination\nnodes, the networking infrastructure consumes 10% - 75% of the total energy\nduring active data transfers, and the rest is consumed by the end systems. Even\nthough there has been extensive research on reducing the power consumption at\nthe networking infrastructure, the work focusing on saving energy at the end\nsystems has been limited to the tuning of a few application level parameters\nsuch as parallelism, pipelining, and concurrency. In this paper, we introduce\nthree novel application-level parameter tuning algorithms which employ dynamic\nCPU frequency and core scaling, combining heuristics and runtime measurements\nto achieve energy efficient data transfers. Experimental results show that our\nproposed algorithms outperform the state-of-the-art solutions, achieving up to\n48% reduced energy consumption and 80% higher throughput.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:48:08 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Di Tacchio", "Luigi", ""], ["Nine", "Zulkar", ""], ["Kosar", "Tevfik", ""], ["Bulut", "Fatih M.", ""], ["Hwang", "Jinho", ""]]}, {"id": "1904.05923", "submitter": "Tamara Radivilova A", "authors": "Igor Ivanisenko and Tamara Radivilova", "title": "Survey of Major Load Balancing Algorithms in Distributed System", "comments": "4 pages", "journal-ref": "2015 Information Technologies in Innovation Business Conference\n  (ITIB), Kharkiv, 2015, pp. 89-92. doi: 10.1109/ITIB.2015.7355061", "doi": "10.1109/ITIB.2015.7355061", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of the most used load balancing algorithms in distributed\nsystems (including cloud technology, cluster systems, grid systems) is\ndescribed. Comparative analysis of types of the load balancing algorithms is\nconducted in accordance with the classification, the advantages and drawbags of\neach type of the algorithms are shown. Performance indicators characterizing\neach algorithm are indicated.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:54:11 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Ivanisenko", "Igor", ""], ["Radivilova", "Tamara", ""]]}, {"id": "1904.06023", "submitter": "Balaji Arun", "authors": "Balaji Arun, Sebastiano Peluso, Binoy Ravindran", "title": "ezBFT: Decentralizing Byzantine Fault-Tolerant State Machine Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ezBFT, a novel leaderless, distributed consensus protocol capable\nof tolerating byzantine faults. ezBFT's main goal is to minimize the\nclient-side latency in WAN deployments. It achieves this by (i) having no\ndesignated primary replica, and instead, enabling every replica to order the\nrequests that it receives from clients; (ii) using only three communication\nsteps to order requests in the common case; and (iii) involving clients\nactively in the consensus process. In addition, ezBFT minimizes the potentially\nnegative effect of a byzantine replica on the overall system performance. We\ndeveloped ezBFT's formal specification in TLA+, show that it provides the\nclassic properties of BFT protocols including consistency, stability, and\nliveness, and developed an implementation. Our experimental evaluation reveals\nthat ezBFT improves client-side latency by as much as 40% over state-of-the-art\nbyzantine fault-tolerant protocols including PBFT, FaB, and Zyzzyva.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 03:31:26 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Arun", "Balaji", ""], ["Peluso", "Sebastiano", ""], ["Ravindran", "Binoy", ""]]}, {"id": "1904.06079", "submitter": "David Monniaux", "authors": "Camille Coti (LIPN), David Monniaux (VERIMAG - IMAG), Hang Yu", "title": "Parallel parametric linear programming solving, and application to\n  polyhedral computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric linear programming is central in polyhedral computations and in\ncertain control applications.We propose a task-based scheme for parallelizing\nit, with quasi-linear speedup over large problems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 07:37:50 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Coti", "Camille", "", "LIPN"], ["Monniaux", "David", "", "VERIMAG - IMAG"], ["Yu", "Hang", ""]]}, {"id": "1904.06206", "submitter": "Wael Jaafar", "authors": "Gor Mack Diouf, Halima Elbiaze, Wael Jaafar", "title": "On Byzantine Fault Tolerance in Multi-Master Kubernertes Clusters", "comments": "28 pages, 11 figures, published in Elsevier Future Generation\n  Computer Systems Journal (Accepted 29-Mar-2020, Available online 7 April\n  2020)", "journal-ref": null, "doi": "10.1016/j.future.2020.03.060", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Docker container virtualization technology is being widely adopted in cloud\ncomputing environments because of its lightweight and effiency. However, it\nrequires adequate control and management via an orchestrator. As a result,\ncloud providers are adopting the open-access Kubernetes platform as the\nstandard orchestrator of containerized applications. To ensure applications'\navailability in Kubernetes, the latter uses Raft protocol's replication\nmechanism. Despite its simplicity, Raft assumes that machines fail only when\nshutdown. This failure event is rarely the only reason for a machine's\nmalfunction. Indeed, software errors or malicious attacks can cause machines to\nexhibit Byzantine (i.e. random) behavior and thereby corrupt the accuracy and\navailability of the replication protocol. In this paper, we propose a\nKubernetes multi-Master Robust (KmMR) platform to overcome this limitation.\nKmMR is based on the adaptation and integration of the BFT-SMaRt fault-tolerant\nreplication protocol into Kubernetes environment. Unlike Raft protocol,\nBFT-SMaRt is resistant to both Byzantine and non-Byzantine faults. Experimental\nresults show that KmMR is able to guarantee the continuity of services, even\nwhen the total number of tolerated faults is exceeded. In addition, KmMR\nprovides on average a consensus time 1000 times shorter than that achieved by\nthe conventional platform (with Raft), in such condition. Finally, we show that\nKmMR generates a small additional cost in terms of resource consumption\ncompared to the conventional platform.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 13:14:41 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 18:15:03 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 02:54:43 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Diouf", "Gor Mack", ""], ["Elbiaze", "Halima", ""], ["Jaafar", "Wael", ""]]}, {"id": "1904.06328", "submitter": "Chaodong Zheng", "authors": "Haimin Chen and Chaodong Zheng", "title": "Fast and Resource Competitive Broadcast in Multi-channel Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a single-hop, multi-channel, synchronous radio network in which a\nsource node needs to disseminate a message to all other $n-1$ nodes. An\nadversary called Eve, which captures environmental noise and potentially\nmalicious interference, aims to disrupt this process via jamming. Assume\nsending, listening, or jamming on one channel for one time slot costs unit\nenergy. The question is, if Eve spends $T$ units of energy on jamming, can we\ndevise broadcast algorithms in which each node's cost is $o(T)$? Previous\nresults show such resource competitive algorithms do exist in the\nsingle-channel setting: each node can receive the message within\n$\\tilde{O}(T+n)$ time slots while spending only $\\tilde{O}(\\sqrt{T/n}+1)$\nenergy.\n  In this paper, we show that when Eve is oblivious, the existence of multiple\nchannels allows even faster message dissemination, while preserving resource\ncompetitiveness. Specifically, we have identified an efficient \"epidemic\nbroadcast\" scheme in the multi-channel setting that is robust again jamming.\nExtending this scheme leads to a randomized algorithm called MultiCast which\nuses $n/2$ channels, and accomplishes broadcast in $\\tilde{O}(T/n+1)$ time\nslots while costing each node only $\\tilde{O}(\\sqrt{T/n}+1)$ energy. When the\nvalue of $n$ is unknown, we further propose MultiCastAdv, in which each node's\nrunning time is $\\tilde{O}(T/(n^{1-2\\alpha})+n^{2\\alpha})$, and each node's\ncost is $\\tilde{O}(\\sqrt{T/(n^{1-2\\alpha})}+n^{2\\alpha})$. Here, $0<\\alpha<1/4$\nis a tunable parameter affecting the constant hiding behind the big-$O$\nnotation.\n  To handle the issue of limited channel availability, we have also devised\nvariants for both MultiCast and MultiCastAdv that can work in networks in which\nonly $C$ channels are available, for any $C\\geq 1$. These variants remain to be\nresource competitive, and have (near) optimal time complexity in many cases.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:12:23 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Chen", "Haimin", ""], ["Zheng", "Chaodong", ""]]}, {"id": "1904.06451", "submitter": "Yuetsu Kodama", "authors": "Yuetsu Kodama, Tetsuya Odajima, Akira Asato and Mitsuhisa Sato", "title": "Evaluation of the RIKEN Post-K Processor Simulator", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the purpose of developing applications for Post-K at an early stage,\nRIKEN has developed a post-K processor simulator. This simulator is based on\nthe general-purpose processor simulator gem5. It does not simulate the actual\nhardware of a post-K processor. However, we believe that sufficient simulation\naccuracy can be obtained since it simulates the instruction pipeline of\nout-of-order execution with cycle-level accuracy along with performing detailed\nparameter tuning of out-of-order resources and function expansion of\ncache/memory hierarchy. In this simulator, we aim to estimate the execution\ncycles of one node application on a post-K processor with accuracy that enables\nrelative evaluation and application tuning. In this paper, we show the details\nof the implementation of this simulator and verify its accuracy compared with\nthat of a post-K test chip.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 00:11:43 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kodama", "Yuetsu", ""], ["Odajima", "Tetsuya", ""], ["Asato", "Akira", ""], ["Sato", "Mitsuhisa", ""]]}, {"id": "1904.06522", "submitter": "Asa Dan", "authors": "Asa Dan", "title": "Cryptocurrency with Fully Asynchronous Communication based on Banks and\n  Democracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptocurrencies came to the world in the recent decade and attempted to\noffer a new order where the financial system is not governed by a centralized\nentity, and where you have complete control over your account without the need\nto trust strangers (governments and banks above all). However, cryptocurrency\nsystems face many challenges that prevent them from being used as an everyday\ncoin. In this paper we attempt to take one step forward by introducing a\ncryptocurrency system that has many important properties. Perhaps the most\nrevolutionary property is its deterministic operation over a fully asynchronous\ncommunication network, which has sometimes been mistakenly considered to be\nimpossible. By avoiding any temporal assumptions, we get a system that is\nrobust against arbitrary delays in the network, and whose latency is only a\nfunction of the actual communication delay. The presented system is based on\nfamiliar concepts $-$ banking and democracy. Our banks, just like normal banks,\nkeep their clients' money and perform their clients' requests. However, because\nof the cryptographic scheme, your bank cannot do anything in your account\nwithout your permission and its entire operation is transparent so you don't\nhave to trust it blindly. The democracy means that every operation performed by\nthe banks (e.g., committing a client transaction) has to be accepted by a\nmajority of the coin holders, in a way that resembles representative democracy\nwhere the banks are the representatives and where each client implicitly\ndelegates his voting power (the sum of money in his account) to his bank. A\nclient can switch banks at any moment, by simply applying a corresponding\nrequest to the new bank of his choice.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 10:22:34 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 04:35:35 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 09:00:05 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Dan", "Asa", ""]]}, {"id": "1904.06584", "submitter": "Rohan Achar", "authors": "Rohan Achar and Cristina V. Lopes", "title": "Got: Git, but for Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We look at one important category of distributed applications characterized\nby the existence of multiple collaborating, and competing, components sharing\nmutable, long-lived, replicated objects. The problem addressed by our work is\nthat of object state synchronization among the components. As an organizing\nprinciple for replicated objects, we formally specify the Global Object Tracker\n(GoT) model, an object-oriented programming model based on causal consistency\nwith application-level conflict resolution strategies, whose elements and\ninterfaces mirror those found in decentralized version control systems: a\nversion graph, working data, diffs, commit, checkout, fetch, push, and merge.\nWe have implemented GoT in a framework called Spacetime, written in Python.\n  In its purest form, GoT is impractical for real systems, because of the\nunbounded growth of the version graph and because passing diff'ed histories\nover the network makes remote communication too slow. We present our solution\nto these problems that adds some constraints to GoT applications, but that\nmakes the model feasible in practice. We present a performance analysis of\nSpacetime for representative workloads, which shows that the additional\nconstraints added to GoT make it not just feasible, but viable for real\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 18:51:27 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Achar", "Rohan", ""], ["Lopes", "Cristina V.", ""]]}, {"id": "1904.06626", "submitter": "Kai Li", "authors": "Kai Li, Yuzhe Tang, Beom Heyn Kim, Jianliang Xu", "title": "Secure Consistency Verification for Untrusted Cloud Storage by Public\n  Blockchains", "comments": "SecureComm 19, ContractChecker, 21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents ContractChecker, a Blockchain-based security protocol for\nverifying the storage consistency between the mutually distrusting cloud\nprovider and clients. Unlike existing protocols, the ContractChecker uniquely\ndelegates log auditing to the Blockchain, and has the advantages in reducing\nclient cost and lowering requirements on client availability, lending itself to\nmodern scenarios with mobile and web clients.\n  The ContractChecker collects the logs from both clients and the cloud server,\nand verifies the consistency by cross-checking the logs. By this means, it does\nnot only detects the attacks from malicious clients and server forging their\nlogs, but also is able to mitigate those attacks and recover the system from\nthem. In addition, we design new attacks against ContractChecker exploiting\nvarious limits in real Blockchain systems (e.g., write unavailability,\nBlockchain forks, contract race conditions). We analyze and harden the security\nof ContractChecker protocols against the proposed new attacks.\n  For evaluating the cost, we build a functional prototype of the\nContractChecker on Ethereum/Solidity. By experiments on private and public\nEthereum testnets, we extensively evaluate the cost of the ContractChecker in\ncomparison with that of existing client-based log auditing works. The result\nshows the ContractChecker can scale to hundreds of clients and save client\ncosts by more than one order of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 04:42:54 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 17:03:13 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 19:28:43 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Li", "Kai", ""], ["Tang", "Yuzhe", ""], ["Kim", "Beom Heyn", ""], ["Xu", "Jianliang", ""]]}, {"id": "1904.06775", "submitter": "Yung-Hsiang Lu", "authors": "Yung-Hsiang Lu, George K. Thiruvathukal, Ahmed S. Kaseb, Kent Gauen,\n  Damini Rijhwani, Ryan Dailey, Deeptanshu Malik, Yutong Huang, Sarah\n  Aghajanzadeh, Minghao Guo", "title": "See the World through Network Cameras", "comments": "This paper is accepted by IEEE Computer for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of network cameras have been deployed worldwide. Real-time data from\nmany network cameras can offer instant views of multiple locations with\napplications in public safety, transportation management, urban planning,\nagriculture, forestry, social sciences, atmospheric information, and more. This\npaper describes the real-time data available from worldwide network cameras and\npotential applications. Second, this paper outlines the CAM2 System available\nto users at https://www.cam2project.net/. This information includes strategies\nto discover network cameras and create the camera database, user interface, and\ncomputing platforms. Third, this paper describes many opportunities provided by\ndata from network cameras and challenges to be addressed.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 22:37:51 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Lu", "Yung-Hsiang", ""], ["Thiruvathukal", "George K.", ""], ["Kaseb", "Ahmed S.", ""], ["Gauen", "Kent", ""], ["Rijhwani", "Damini", ""], ["Dailey", "Ryan", ""], ["Malik", "Deeptanshu", ""], ["Huang", "Yutong", ""], ["Aghajanzadeh", "Sarah", ""], ["Guo", "Minghao", ""]]}, {"id": "1904.06995", "submitter": "Mehdi Kargahi", "authors": "Ali Behnoudfar, Seyyed Hossein Hosseini Zahani, Mojtaba Hatami,\n  Mahmoud Naghibzadeh, Boshra Taheri, Fathieh Faghih, Mehdi Kargahi", "title": "The Proceedings of First Work-in-Progress Session of The CSI\n  International Symposium on Real-Time and Embedded Systems and Technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present volume contains the proceedings of RTEST WiP 2018, chaired by\nMarco Caccamo, University of Illinois at Urbana-Champaign. This event has been\norganized by the School of Electrical and Computer Engineering at the\nUniversity of Tehran, in conjunction with the Department of Computer\nEngineering at Sharif University of Technology, Tehran, Iran. The topics of\ninterest in RTEST WiP span over all theoretical and application-oriented\naspects, reporting design, analysis, implementation, evaluation, and empirical\nresults, of real-time and embedded systems, internet-of-things, and\ncyber-physical systems. The program committee of RTEST 2018 consists of 54 top\nresearchers in the mentioned fields from top universities, industries, and\nresearch centers around the world. RTEST 2018 has received a total of 41\nsubmissions, out of which we have accepted 14 regular papers and 4\nwork-in-progress papers. Each submission has been reviewed by 3 to 5\nindependent referees, for its quality, originality, contribution, clarity of\npresentation, and relevance to the symposium topics.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 19:47:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Behnoudfar", "Ali", ""], ["Zahani", "Seyyed Hossein Hosseini", ""], ["Hatami", "Mojtaba", ""], ["Naghibzadeh", "Mahmoud", ""], ["Taheri", "Boshra", ""], ["Faghih", "Fathieh", ""], ["Kargahi", "Mehdi", ""]]}, {"id": "1904.07024", "submitter": "Moustafa Nakechbandi", "authors": "Jean-Yves Colin (LITIS), Moustafa Nakechbandi (LITIS), Herv\\'e Mathieu", "title": "Management of mobile resources in Physical Internet logistic models", "comments": null, "journal-ref": "ICALT 2015, 4th IEEE Int. Conf. on Advanced Logistics & Transport,\n  Valenciennes, May 2015, Valenciennes, France", "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the concept of a 'Physical Internet', the idea of\nbuilding large logistics systems like the very successful Digital Internet\nnetwork. The idea is to handle mobile resources, such as containers, just like\nInternet data packets. Thus, it is possible to use the principles of\nencapsulation and routing to optimize the freight. The problem is that mobile\nresources, such as containers, are not quite similar to data packets, because\nthey are real and not dematerialized. Thus the handling and the storing of\nmobile resources, such as containers, will create imbalances in the logistics\nnetwork, leading to starvation or overstocking of logistic network nodes. We\npropose in this paper a study addressing this problem leading to some\nsolutions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 06:21:03 GMT"}], "update_date": "2019-04-21", "authors_parsed": [["Colin", "Jean-Yves", "", "LITIS"], ["Nakechbandi", "Moustafa", "", "LITIS"], ["Mathieu", "Herv\u00e9", ""]]}, {"id": "1904.07027", "submitter": "Felipe S. Abrah\\~ao", "authors": "Felipe S. Abrah\\~ao, \\'Itala M. Loffredo D'Ottaviano, Klaus Wehmuth,\n  Francisco Ant\\^onio D\\'oria, Artur Ziviani", "title": "Learning the undecidable from networked systems", "comments": "Revised preprint version for publication as a book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.SI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a theoretical investigation of computation beyond the\nTuring barrier from emergent behavior in distributed systems. In particular, we\npresent an algorithmic network that is a mathematical model of a networked\npopulation of randomly generated computable systems with a fixed communication\nprotocol. Then, in order to solve an undecidable problem, we study how nodes\n(i.e., Turing machines or computable systems) can harness the power of the\nmetabiological selection and the power of information sharing (i.e.,\ncommunication) through the network. Formally, we show that there is a pervasive\nnetwork topological condition, in particular, the small-diameter phenomenon,\nthat ensures that every node becomes capable of solving the halting problem for\nevery program with a length upper bounded by a logarithmic order of the\npopulation size. In addition, we show that this result implies the existence of\na central node capable of emergently solving the halting problem in the minimum\nnumber of communication rounds. Furthermore, we introduce an\nalgorithmic-informational measure of synergy for networked computable systems,\nwhich we call local algorithmic synergy. Then, we show that such algorithmic\nnetwork can produce an arbitrarily large value of expected local algorithmic\nsynergy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 23:32:45 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 20:04:48 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 21:07:08 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Abrah\u00e3o", "Felipe S.", ""], ["D'Ottaviano", "\u00cdtala M. Loffredo", ""], ["Wehmuth", "Klaus", ""], ["D\u00f3ria", "Francisco Ant\u00f4nio", ""], ["Ziviani", "Artur", ""]]}, {"id": "1904.07069", "submitter": "Pietro Danzi", "authors": "Pietro Danzi, Anders E. Kal{\\o}r, \\v{C}edomir Stefanovi\\'c, Petar\n  Popovski", "title": "Repeat-Authenticate Scheme for Multicasting of Blockchain Information in\n  IoT Systems", "comments": "Paper submitted to IEEE Global Communications Conference 2019 -\n  Workshop on Blockchain in Telecommunications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of efficiently disseminating authenticated blockchain\ninformation from blockchain nodes (servers) to Internet of Things (IoT)\ndevices, through a wireless base station (BS). In existing blockchain\nprotocols, upon generation of a new block, each IoT device receives a copy of\nthe block header, authenticated via digital signature by one or more trusted\nservers. Since it relies on unicast transmissions, the required communication\nresources grow linearly with the number of IoT devices. We propose a more\nefficient scheme, in which a single copy of each block header is multicasted,\ntogether with the signatures of servers. In addition, if IoT devices tolerate a\ndelay, we exploit the blockchain structure to amortize the authentication in\ntime, by transmitting only a subset of signature in each block period. Finally,\nthe BS sends redundant information, via a repetition code, to deal with the\nunreliable wireless channel, with the aim of decreasing the amount of feedback\nrequired from IoT devices. Our analysis shows the trade-off between timely\nauthentication of blocks and reliability of the communication, depending on the\npacket loss rate offered by the channel. The numerical results show that the\nperformance benefits of the proposed scheme makes it a viable starting point\nfor designing new lightweight protocols for blockchains.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:21:21 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 21:29:36 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 16:25:17 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Danzi", "Pietro", ""], ["Kal\u00f8r", "Anders E.", ""], ["Stefanovi\u0107", "\u010cedomir", ""], ["Popovski", "Petar", ""]]}, {"id": "1904.07098", "submitter": "Hema Venkata Krishna Giri Narra", "authors": "Krishna Giri Narra, Zhifeng Lin, Mehrdad Kiamari, Salman Avestimehr,\n  Murali Annavaram", "title": "Slack Squeeze Coded Computing for Adaptive Straggler Mitigation", "comments": "13 pages, SC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While performing distributed computations in today's cloud-based platforms,\nexecution speed variations among compute nodes can significantly reduce the\nperformance and create bottlenecks like stragglers. Coded computation\ntechniques leverage coding theory to inject computational redundancy and\nmitigate stragglers in distributed computations. In this paper, we propose a\ndynamic workload distribution strategy for coded computation called Slack\nSqueeze Coded Computation ($S^2C^2$). $S^2C^2$ squeezes the compute slack\n(i.e., overhead) that is built into the coded computing frameworks by\nefficiently assigning work for all fast and slow nodes according to their\nspeeds and without needing to re-distribute data. We implement an LSTM-based\nspeed prediction algorithm to predict speeds of compute nodes. We evaluate\n$S^2C^2$ on linear algebraic algorithms, gradient descent, graph ranking, and\ngraph filtering algorithms. We demonstrate 19% to 39% reduction in total\ncomputation latency using $S^2C^2$ compared to job replication and coded\ncomputation. We further show how $S^2C^2$ can be applied beyond matrix-vector\nmultiplication.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:56:59 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 19:16:20 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Narra", "Krishna Giri", ""], ["Lin", "Zhifeng", ""], ["Kiamari", "Mehrdad", ""], ["Avestimehr", "Salman", ""], ["Annavaram", "Murali", ""]]}, {"id": "1904.07104", "submitter": "Thomas Liebig", "authors": "Cedric Sanders and Thomas Liebig", "title": "Knowledge Discovery on Blockchains: Challenges and Opportunities", "comments": "16 pages, best viewed in color", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the applicability of blockchain technology for distributed event\ndetection under resource constraints. Therefore we provide a test-suite with\nseveral promising consensus methods (Proof-of-Work, Proof-of-Stake, Distributed\nProof-of-Work, and Practical Proof-of-Kernel-Work). This is the first work\nanalyzing the communication costs of blockchain consensus methods for knowledge\ndiscovery tasks in resource constraint devices. The experiments reveal that our\nproposed implementations of Distributed Proof-of-Work and Practical\nProof-of-Kernel-Work provide a benefit over Proof-of-Work in CPU usage and\ncommunication costs. The tests show further that in cases of low data rates,\nwhere latencies by mining do not cause harm proposed blockchain implementations\ncould be integrated. However, usage of blockchain requires data broadcasts,\nwhich leads to communication overhead as well as memory requirements based on\nthe address list.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 11:29:08 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 13:27:21 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Sanders", "Cedric", ""], ["Liebig", "Thomas", ""]]}, {"id": "1904.07124", "submitter": "Wei Bi", "authors": "Wei Bi, Xiangyu Liu, Maolin Zheng", "title": "$\\varepsilon$-differential agreement: A Parallel Data Sorting Mechanism\n  for Distributed Information Processing System", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The order of the input information plays a very important role in a\ndistributed information processing system (DIPS). This paper proposes a novel\ndata sorting mechanism named the {\\epsilon}-differential agreement (EDA) that\ncan support parallel data sorting. EDA adopts the collaborative consensus\nmechanism which is different from the traditional consensus mechanisms using\nthe competition mechanism, such as PoS, PoW, etc. In the system that employs\nthe EDA mechanism, all participants work together to compute the order of the\ninput information by using statistical and probability principles on a\nproportion of participants. Preliminary results show variable fault-tolerant\nrates and consensus delay for systems that have different configurations when\nreaching consensus, thus it suggests that it is possible to use EDA in a system\nand customize these parameters based on different requirements. With the unique\nmechanism, EDA can be used in DIPS of multi-center decision cluster, not just\nthe rotating center decision cluster.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 13:09:46 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Bi", "Wei", ""], ["Liu", "Xiangyu", ""], ["Zheng", "Maolin", ""]]}, {"id": "1904.07136", "submitter": "Germ\\'an Andr\\'es Delbianco", "authors": "Aleksandar Nanevski and Anindya Banerjee and Germ\\'an Andr\\'es\n  Delbianco and Ignacio F\\'abregas", "title": "Specifying Concurrent Programs in Separation Logic: Morphisms and\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In addition to pre- and postconditions, program specifications in recent\nseparation logics for concurrency have employed an algebraic structure of\nresources---a form of state transition system---to describe the state-based\nprogram invariants that must be preserved, and to record the permissible atomic\nchanges to program state. In this paper we introduce a novel notion of resource\nmorphism, i.e. structure-preserving function on resources, and show how to\neffectively integrate it into separation logic, using an associated notion of\nmorphism-specific simulation. We apply morphisms and simulations to programs\nverified under one resource, to compositionally adapt them to operate under\nanother resource, thus facilitating proof reuse.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:42:33 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 18:28:26 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 15:46:02 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Nanevski", "Aleksandar", ""], ["Banerjee", "Anindya", ""], ["Delbianco", "Germ\u00e1n Andr\u00e9s", ""], ["F\u00e1bregas", "Ignacio", ""]]}, {"id": "1904.07145", "submitter": "Maxwell Scale Uwadia Osagie", "authors": "Maxwell Scale Uwadia Osagie and Amenze Joy Osagie", "title": "The Architectural Dynamics of Encapsulated Botnet Detection (EDM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Botnet is one of the numerous attacks ravaging the networking environment.\nIts approach is said to be brutal and dangerous to network infrastructures as\nwell as client systems. Since the introduction of botnet, different design\nmethods have been employed to solve the divergent approach but the method of\ntaking over servers and client systems is unabated. To solve this, we first\nidentify Mpack, ICEpack and Fiesta as enhanced IRC tool. The analysis of its\nrole in data exchange using OSI model was carried out. This further gave the\nneeded proposal to the development of a High level architecture representing\nthe structural mechanism and the defensive mechanism within network server so\nas to control the botnet trend. Finally, the architecture was designed to\nrespond in a proactive state when scanning and synergizing the double data\nverification modules in an encapsulation manner within server system.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 23:04:28 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Osagie", "Maxwell Scale Uwadia", ""], ["Osagie", "Amenze Joy", ""]]}, {"id": "1904.07148", "submitter": "Issam Damaj", "authors": "Issam Damaj (1), Mahmoud Imdoukh (1), Rached Zantout (2) ((1) American\n  University of Kuwait, (2) Rafik Hariri University)", "title": "Parallel Hardware for Faster Morphological Analysis", "comments": "16 pages, 19 figures, 7 tables", "journal-ref": "Computer and Information Sciences, Elsevier. 30(2018) 531-546", "doi": "10.1016/j.jksuci.2017.07.003", "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological analysis in the Arabic language is computationally intensive,\nhas numerous forms and rules, and is intrinsically parallel. The investigation\npresented in this paper confirms that the effective development of parallel\nalgorithms and the derivation of corresponding processors in hardware enable\nimplementations with appealing performance characteristics. The presented\ndevelopments of parallel hardware comprise the application of a variety of\nalgorithm modelling techniques, strategies for concurrent processing, and the\ncreation of pioneering hardware implementations that target modern programmable\ndevices. The investigation includes the creation of a linguistic-based stemmer\nfor Arabic verb root extraction with extended infix processing to attain\nhigh-levels of accuracy. The implementations comprise three versions, namely,\nsoftware, non-pipelined processor, and pipelined processor with high\nthroughput. The targeted systems are high-performance multi-core processors for\nsoftware implementations and high-end Field Programmable Gate Array systems for\nhardware implementations. The investigation includes a thorough evaluation of\nthe methodology, and performance and accuracy analyses of the developed\nsoftware and hardware implementations. The pipelined processor achieved a\nsignificant speedup of 5571.4 over the software implementation. The developed\nstemmer for verb root extraction with infix processing attained accuracies of\n87% and 90.7% for analyzing the texts of the Holy Quran and its Chapter 29 -\nSurat Al-Ankabut.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 23:46:52 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Damaj", "Issam", ""], ["Imdoukh", "Mahmoud", ""], ["Zantout", "Rached", ""]]}, {"id": "1904.07162", "submitter": "Gurbinder Gill", "authors": "Gurbinder Gill (1), Roshan Dathathri (1), Loc Hoang (1), Ramesh Peri\n  (2), Keshav Pingali (1) ((1) The University of Texas at Austin, (2) Intel\n  Corporation)", "title": "Single Machine Graph Analytics on Massive Datasets Using Intel Optane DC\n  Persistent Memory", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intel Optane DC Persistent Memory (Optane PMM) is a new kind of\nbyte-addressable memory with higher density and lower cost than DRAM. This\nenables the design of affordable systems that support up to 6TB of randomly\naccessible memory. In this paper, we present key runtime and algorithmic\nprinciples to consider when performing graph analytics on extreme-scale graphs\non large-memory platforms of this sort.\n  To demonstrate the importance of these principles, we evaluate four existing\nshared-memory graph frameworks on large real-world web-crawls, using a machine\nwith 6TB of Optane PMM. Our results show that frameworks based on the runtime\nand algorithmic principles advocated in this paper (i) perform significantly\nbetter than the others, and (ii) are competitive with graph analytics\nframeworks running on large production clusters.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:20:58 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 17:31:32 GMT"}, {"version": "v3", "created": "Sun, 23 Feb 2020 22:11:55 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gill", "Gurbinder", ""], ["Dathathri", "Roshan", ""], ["Hoang", "Loc", ""], ["Peri", "Ramesh", ""], ["Pingali", "Keshav", ""]]}, {"id": "1904.07171", "submitter": "Alexey Gotsman", "authors": "Alexey Gotsman, Anatole Lefort, and Gregory Chockler", "title": "White-Box Atomic Multicast (Extended Version)", "comments": "Extended version of a paper in DSN'19: International Conference on\n  Dependable Systems and Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic multicast is a communication primitive that delivers messages to\nmultiple groups of processes according to some total order, with each group\nreceiving the projection of the total order onto messages addressed to it. To\nbe scalable, atomic multicast needs to be genuine, meaning that only the\ndestination processes of a message should participate in ordering it. In this\npaper we propose a novel genuine atomic multicast protocol that in the absence\nof failures takes as low as 3 message delays to deliver a message when no other\nmessages are multicast concurrently to its destination groups, and 5 message\ndelays in the presence of concurrency. This improves the latencies of both the\nfault-tolerant version of classical Skeen's multicast protocol (6 or 12 message\ndelays, depending on concurrency) and its recent improvement by Coelho et al.\n(4 or 8 message delays). To achieve such low latencies, we depart from the\ntypical way of guaranteeing fault-tolerance by replicating each group with\nPaxos. Instead, we weave Paxos and Skeen's protocol together into a single\ncoherent protocol, exploiting opportunities for white-box optimisations. We\nexperimentally demonstrate that the superior theoretical characteristics of our\nprotocol are reflected in practical performance pay-offs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:37:03 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gotsman", "Alexey", ""], ["Lefort", "Anatole", ""], ["Chockler", "Gregory", ""]]}, {"id": "1904.07349", "submitter": "Boyang Li", "authors": "Boyang Li, Changhao Chenli, Xiaowei Xu, Yiyu Shi, Taeho Jung", "title": "DLBC: A Deep Learning-Based Consensus in Blockchains for Deep Learning\n  Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing artificial intelligence application, deep neural network\n(DNN) has become an emerging task. However, to train a good deep learning model\nwill suffer from enormous computation cost and energy consumption. Recently,\nblockchain has been widely used, and during its operation, a huge amount of\ncomputation resources are wasted for the Proof of Work (PoW) consensus. In this\npaper, we propose DLBC to exploit the computation power of miners for deep\nlearning training as proof of useful work instead of calculating hash values.\nit distinguishes itself from recent proof of useful work mechanisms by\naddressing various limitations of them. Specifically, DLBC handles multiple\ntasks, larger model and training datasets, and introduces a comprehensive\nranking mechanism that considers tasks difficulty(e.g., model complexity,\nnetwork burden, data size, queue length). We also applied DNN-watermark [1] to\nimprove the robustness. In Section V, the average overhead of digital signature\nis 1.25, 0.001, 0.002 and 0.98 seconds, respectively, and the average overhead\nof network is 3.77, 3.01, 0.37 and 0.41 seconds, respectively. Embedding a\nwatermark takes 3 epochs and removing a watermark takes 30 epochs. This penalty\nof removing watermark will prevent attackers from stealing, improving, and\nresubmitting DL models from honest miners.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 22:28:45 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 03:44:56 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Li", "Boyang", ""], ["Chenli", "Changhao", ""], ["Xu", "Xiaowei", ""], ["Shi", "Yiyu", ""], ["Jung", "Taeho", ""]]}, {"id": "1904.07421", "submitter": "Sheng Zha", "authors": "Sheng Zha, Ziheng Jiang, Haibin Lin, Zhi Zhang", "title": "Just-in-Time Dynamic-Batching", "comments": "NeurIPS 2018 Systems for ML Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batching is an essential technique to improve computation efficiency in deep\nlearning frameworks. While batch processing for models with static feed-forward\ncomputation graphs is straightforward to implement, batching for dynamic\ncomputation graphs such as syntax trees or social network graphs is challenging\ndue to variable computation graph structure across samples. Through simulation\nand analysis of a Tree-LSTM model, we show the key trade-off between graph\nanalysis time and batching effectiveness in dynamic batching. Based on this\nfinding, we propose a dynamic batching method as an extension to MXNet Gluon's\njust-in-time compilation (JIT) framework. We show empirically that our method\nyields up to 6.25 times speed-up on a common dynamic workload, a tree-LSTM\nmodel for the semantic relatedness task.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:00:51 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zha", "Sheng", ""], ["Jiang", "Ziheng", ""], ["Lin", "Haibin", ""], ["Zhang", "Zhi", ""]]}, {"id": "1904.07494", "submitter": "Reza Fathi", "authors": "Reza Fathi, Anisur Rahaman Molla, Gopal Pandurangan", "title": "Efficient Distributed Community Detection in the Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective algorithms for community detection is an important and\nchallenging problem in {\\em large-scale} graphs, studied extensively in the\nliterature. Various solutions have been proposed, but many of them are\ncentralized with expensive procedures (requiring full knowledge of the input\ngraph) and have a large running time.\n  In this paper, we present a distributed algorithm for community detection in\nthe {\\em stochastic block model} (also called {\\em planted partition model}), a\nwidely-studied and canonical random graph model for community detection and\nclustering. Our algorithm called {\\em CDRW(Community Detection by Random\nWalks)} is based on random walks, and is localized and lightweight, and easy to\nimplement. A novel feature of the algorithm is that it uses the concept of {\\em\nlocal mixing time} to identify the community around a given node.\n  We present a rigorous theoretical analysis that shows that the algorithm can\naccurately identify the communities in the stochastic block model and\ncharacterize the model parameters where the algorithm works. We also present\nexperimental results that validate our theoretical analysis. We also analyze\nthe performance of our distributed algorithm under the CONGEST distributed\nmodel as well as the $k$-machine model, a model for large-scale distributed\ncomputations, and show that it can be efficiently implemented.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 07:03:06 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Fathi", "Reza", ""], ["Molla", "Anisur Rahaman", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1904.07532", "submitter": "Eylon Yogev", "authors": "Christoph Lenzen and Merav Parter and Eylon Yogev", "title": "Parallel Balanced Allocations: The Heavily Loaded Case", "comments": null, "journal-ref": null, "doi": "10.1145/3323165.3323203", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parallel algorithms for the classical balls-into-bins problem, in\nwhich $m$ balls acting in parallel as separate agents are placed into $n$ bins.\nAlgorithms operate in synchronous rounds, in each of which balls and bins\nexchange messages once. The goal is to minimize the maximal load over all bins\nusing a small number of rounds and few messages.\n  While the case of $m=n$ balls has been extensively studied, little is known\nabout the heavily loaded case. In this work, we consider parallel algorithms\nfor this somewhat neglected regime of $m\\gg n$. The naive solution of\nallocating each ball to a bin chosen uniformly and independently at random\nresults in maximal load $m/n+\\Theta(\\sqrt{m/n\\cdot \\log n})$ (for $m\\geq n \\log\nn$) w.h.p. In contrast, for the sequential setting Berenbrink et al (SIAM J.\nComput 2006) showed that letting each ball join the least loaded bin of two\nrandomly selected bins reduces the maximal load to $m/n+O(\\log\\log m)$ w.h.p.\nTo date, no parallel variant of such a result is known.\n  We present a simple parallel threshold algorithm that obtains a maximal load\nof $m/n+O(1)$ w.h.p. within $O(\\log\\log (m/n)+\\log^* n)$ rounds. The algorithm\nis symmetric (balls and bins all \"look the same\"), and balls send $O(1)$\nmessages in expectation per round. The additive term of $O(\\log^* n)$ in the\ncomplexity is known to be tight for such algorithms (Lenzen and Wattenhofer\nDistributed Computing 2016). We also prove that our analysis is tight, i.e.,\nalgorithms of the type we provide must run for $\\Omega(\\min\\{\\log\\log\n(m/n),n\\})$ rounds w.h.p.\n  Finally, we give a simple asymmetric algorithm (i.e., balls are aware of a\ncommon labeling of the bins) that achieves a maximal load of $m/n + O(1)$ in a\nconstant number of rounds w.h.p. Again, balls send only a single message per\nround, and bins receive $(1+o(1))m/n+O(\\log n)$ messages w.h.p.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:31:33 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Lenzen", "Christoph", ""], ["Parter", "Merav", ""], ["Yogev", "Eylon", ""]]}, {"id": "1904.07664", "submitter": "Mika\\\"el Rabie", "authors": "Carole Delporte-Gallet, Hugues Fauconnier, Pierre Fraigniaud, Mika\\\"el\n  Rabie", "title": "Distributed Computing in the Asynchronous LOCAL model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LOCAL model is among the main models for studying locality in the\nframework of distributed network computing. This model is however subject to\npertinent criticisms, including the facts that all nodes wake up\nsimultaneously, perform in lock steps, and are failure-free. We show that\nrelaxing these hypotheses to some extent does not hurt local computing. In\nparticular, we show that, for any construction task $T$ associated to a locally\ncheckable labeling (LCL), if $T$ is solvable in $t$ rounds in the LOCAL model,\nthen $T$ remains solvable in $O(t)$ rounds in the asynchronous LOCAL model.\nThis improves the result by Casta\\~neda et al. [SSS 2016], which was restricted\nto 3-coloring the rings. More generally, the main contribution of this paper is\nto show that, perhaps surprisingly, asynchrony and failures in the computations\ndo not restrict the power of the LOCAL model, as long as the communications\nremain synchronous and failure-free.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 13:40:25 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 14:05:49 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 17:27:16 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Delporte-Gallet", "Carole", ""], ["Fauconnier", "Hugues", ""], ["Fraigniaud", "Pierre", ""], ["Rabie", "Mika\u00ebl", ""]]}, {"id": "1904.07725", "submitter": "Estela Suarez", "authors": "Anke Kreuzer, Norbert Eicker, Jorge Amaya, Raphael Leger, Estela\n  Suarez", "title": "The DEEP-ER project: I/O and resiliency extensions for the\n  Cluster-Booster architecture", "comments": "8 pages, 10 figures, HPCC conference. arXiv admin note: text overlap\n  with arXiv:1904.05275", "journal-ref": "2018 IEEE 20th International Conference on High Performance\n  Computing and Communications (HPCC)", "doi": "10.1109/HPCC/SmartCity/DSS.2018.00046", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently completed research project DEEP-ER has developed a variety of\nhardware and software technologies to improve the I/O capabilities of next\ngeneration high-performance computers, and to enable applications recovering\nfrom the larger hardware failure rates expected on these machines.\n  The heterogeneous Cluster-Booster architecture --first introduced in the\npredecessor DEEP project-- has been extended by a multi-level memory hierarchy\nemploying non-volatile and network-attached memory devices. Based on this\nhardware infrastructure, an I/O and resiliency software stack has been\nimplemented combining and extending well established libraries and software\ntools, and sticking to standard user-interfaces. Real-world scientific codes\nhave tested the projects' developments and demonstrated the improvements\nachieved without compromising the portability of the applications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:54:51 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kreuzer", "Anke", ""], ["Eicker", "Norbert", ""], ["Amaya", "Jorge", ""], ["Leger", "Raphael", ""], ["Suarez", "Estela", ""]]}, {"id": "1904.07935", "submitter": "Gordon Moon", "authors": "Gordon E. Moon, Aravind Sukumaran-Rajam, Srinivasan Parthasarathy and\n  P. Sadayappan", "title": "PL-NMF: Parallel Locality-Optimized Non-negative Matrix Factorization", "comments": "11 pages, 5 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative Matrix Factorization (NMF) is a key kernel for unsupervised\ndimension reduction used in a wide range of applications, including topic\nmodeling, recommender systems and bioinformatics. Due to the compute-intensive\nnature of applications that must perform repeated NMF, several parallel\nimplementations have been developed in the past. However, existing parallel NMF\nalgorithms have not addressed data locality optimizations, which are critical\nfor high performance since data movement costs greatly exceed the cost of\narithmetic/logic operations on current computer systems. In this paper, we\ndevise a parallel NMF algorithm based on the HALS (Hierarchical Alternating\nLeast Squares) scheme that incorporates algorithmic transformations to enhance\ndata locality. Efficient realizations of the algorithm on multi-core CPUs and\nGPUs are developed, demonstrating significant performance improvement over\nexisting state-of-the-art parallel NMF algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 19:18:37 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Moon", "Gordon E.", ""], ["Sukumaran-Rajam", "Aravind", ""], ["Parthasarathy", "Srinivasan", ""], ["Sadayappan", "P.", ""]]}, {"id": "1904.08037", "submitter": "Thatchaphol Saranurak", "authors": "Yi-Jun Chang, Thatchaphol Saranurak", "title": "Improved Distributed Expander Decomposition and Nearly Optimal Triangle\n  Enumeration", "comments": "To appear at PODC'19. Added open problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(\\epsilon,\\phi)$-expander decomposition of a graph $G=(V,E)$ is a\nclustering of the vertices $V=V_{1}\\cup\\cdots\\cup V_{x}$ such that (1) each\ncluster $V_{i}$ induces subgraph with conductance at least $\\phi$, and (2) the\nnumber of inter-cluster edges is at most $\\epsilon|E|$. In this paper, we give\nan improved distributed expander decomposition.\n  Specifically, we construct an $(\\epsilon,\\phi)$-expander decomposition with\n$\\phi=(\\epsilon/\\log n)^{2^{O(k)}}$ in $O(n^{2/k}\\cdot\\text{poly}(1/\\phi,\\log\nn))$ rounds for any $\\epsilon\\in(0,1)$ and positive integer $k$. For example, a\n$(0.01,1/\\text{poly}\\log n)$-expander decomposition can be computed in\n$O(n^{\\gamma})$ rounds, for any arbitrarily small constant $\\gamma>0$.\nPreviously, the algorithm by Chang, Pettie, and Zhang can construct a\n$(1/6,1/\\text{poly}\\log n)$-expander decomposition using\n$\\tilde{O}(n^{1-\\delta})$ rounds for any $\\delta>0$, with a caveat that the\nalgorithm is allowed to throw away a set of edges into an extra part which\nforms a subgraph with arboricity at most $n^{\\delta}$. Our algorithm does not\nhave this caveat.\n  By slightly modifying the distributed algorithm for routing on expanders by\nGhaffari, Kuhn and Su [PODC'17], we obtain a triangle enumeration algorithm\nusing $\\tilde{O}(n^{1/3})$ rounds. This matches the lower bound by Izumi and Le\nGall [PODC'17] and Pandurangan, Robinson and Scquizzato [SPAA'18] of\n$\\tilde{\\Omega}(n^{1/3})$ which holds even in the CONGESTED CLIQUE model. This\nprovides the first non-trivial example for a distributed problem that has\nessentially the same complexity (up to a polylogarithmic factor) in both\nCONGEST and CONGESTED CLIQUE.\n  The key technique in our proof is the first distributed approximation\nalgorithm for finding a low conductance cut that is as balanced as possible.\nPrevious distributed sparse cut algorithms do not have this nearly most\nbalanced guarantee.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 00:50:26 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 02:53:30 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1904.08233", "submitter": "Issam Damaj", "authors": "Issam Damaj (1), Hassan Diab (2) ((1) London South Bank University,\n  (2) American University of Beirut)", "title": "Performance Analysis of Linear Algebraic Functions using Reconfigurable\n  Computing", "comments": "22 pages, 17 figures, 5 tables. arXiv admin note: substantial text\n  overlap with arXiv:1904.04953; text overlap with arXiv:1904.06198", "journal-ref": "Intl. Jrnl. of. Super. Comp. 24(2003) 91-107", "doi": "10.1023/A:1020993510939", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new mapping of geometrical transformation on the\nMorphoSys (M1) reconfigurable computing (RC) system. New mapping techniques for\nsome linear algebraic functions are recalled. A new mapping for geometrical\ntransformation operations is introduced and their performance on the M1 system\nis evaluated. The translation and scaling transformation addressed in this\nmapping employ some vector-vector and vector-scalar operations [6-7]. A\nperformance analysis study of the M1 RC system is also presented to evaluate\nthe efficiency of the algorithm execution. Numerical examples were simulated to\nvalidate our results, using the MorphoSys mULATE program, which emulates M1\noperations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 20:15:29 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Damaj", "Issam", ""], ["Diab", "Hassan", ""]]}, {"id": "1904.08335", "submitter": "Chao Zhang", "authors": "Chao Zhang", "title": "Truxen: A Trusted Computing Enhanced Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truxen is a Trusted Computing enhanced blockchain that uses Proof of\nIntegrity protocol as the consensus. Proof of Integrity protocol is derived\nfrom Trusted Computing and associated Remote Attestations, that can be used to\nvouch a node's identity and integrity to all of the other nodes in the\nblockchain network. In this paper we describe how Trusted Computing and Proof\nof Integrity can be used to enhance blockchain in the areas of mining block,\nexecuting transaction and smart contract, protecting sensitive data. Truxen\npresents a Single Execution Model, that can verify and execute transaction and\nsmart contract in a solo node, consequently enables remote calls to off-chain\napplications and performs in-deterministic tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:58:41 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Zhang", "Chao", ""]]}, {"id": "1904.08380", "submitter": "Laxman Dhulipala", "authors": "Laxman Dhulipala, Julian Shun, Guy Blelloch", "title": "Low-Latency Graph Streaming Using Compressed Purely-Functional Trees", "comments": "This is the full version of the paper appearing in the ACM SIGPLAN\n  conference on Programming Language Design and Implementation (PLDI), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the dynamic nature of real-world graphs, there has been a growing\ninterest in the graph-streaming setting where a continuous stream of graph\nupdates is mixed with arbitrary graph queries. In principle, purely-functional\ntrees are an ideal choice for this setting due as they enable safe parallelism,\nlightweight snapshots, and strict serializability for queries. However,\ndirectly using them for graph processing would lead to significant space\noverhead and poor cache locality.\n  This paper presents C-trees, a compressed purely-functional search tree data\nstructure that significantly improves on the space usage and locality of\npurely-functional trees. The key idea is to use a chunking technique over trees\nin order to store multiple entries per tree-node. We design\ntheoretically-efficient and practical algorithms for performing batch updates\nto C-trees, and also show that we can store massive dynamic real-world graphs\nusing only a few bytes per edge, thereby achieving space usage close to that of\nthe best static graph processing frameworks.\n  To study the efficiency and applicability of our data structure, we designed\nAspen, a graph-streaming framework that extends the interface of Ligra with\noperations for updating graphs. We show that Aspen is faster than two\nstate-of-the-art graph-streaming systems, Stinger and LLAMA, while requiring\nless memory, and is competitive in performance with the state-of-the-art static\ngraph frameworks, Galois, GAP, and Ligra+. With Aspen, we are able to\nefficiently process the largest publicly-available graph with over two hundred\nbillion edges in the graph-streaming setting using a single commodity multicore\nserver with 1TB of memory.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:28:18 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""], ["Blelloch", "Guy", ""]]}, {"id": "1904.08480", "submitter": "Jie You", "authors": "Jie You, Mosharaf Chowdhury", "title": "Terra: Scalable Cross-Layer GDA Optimizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-distributed analytics (GDA) frameworks transfer large datasets over the\nwide-area network (WAN). Yet existing frameworks often ignore the WAN topology.\nThis disconnect between WAN-bound applications and the WAN itself results in\nmissed opportunities for cross-layer optimizations. In this paper, we present\nTerra to bridge this gap. Instead of decoupled WAN routing and GDA transfer\nscheduling, Terra applies scalable cross-layer optimizations to minimize WAN\ntransfer times for GDA jobs. We present a two-pronged approach: (i) a scalable\nalgorithm for joint routing and scheduling to make fast decisions; and (ii) a\nscalable, overlay-based enforcement mechanism that avoids expensive switch rule\nupdates in the WAN. Together, they enable Terra to quickly react to WAN\nuncertainties such as large bandwidth fluctuations and failures in an\napplication-aware manner as well. Integration with the FloodLight SDN\ncontroller and Apache YARN, and evaluation on 4 workloads and 3 WAN topologies\nshow that Terra improves the average completion times of GDA jobs by\n1.55x-3.43x. GDA jobs running with Terra meets 2.82x-4.29x more deadlines and\ncan quickly react to WAN-level events in an application-aware manner.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:02:27 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["You", "Jie", ""], ["Chowdhury", "Mosharaf", ""]]}, {"id": "1904.08616", "submitter": "Grzegorz Korcyl", "authors": "G. Korcyl, P. Korcyl", "title": "Investigating the Dirac operator evaluation with FPGAs", "comments": "8 pages, 5 figures", "journal-ref": "ISSN: 2409-6008, vol. 6, no. 2, 2019", "doi": "10.14529/jsfi190204", "report-no": null, "categories": "cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the computational capacity of single Field Programmable Gate\nArrays (FPGA) devices as well as their versatility has increased significantly.\nAdding to that the High Level Synthesis frameworks allowing to program such\nprocessors in a high level language like C++, makes modern FPGA devices a\nserious candidate as building blocks of a general purpose High Performance\nComputing solution. In this contribution we describe benchmarks which we\nperformed using a Lattice QCD code, a highly compute-demanding HPC academic\ncode for elementary particle simulations. We benchmark the performance of a\nsingle FPGA device running in two modes: using the external or embedded memory.\nWe discuss both approaches in detail using the Xilinx U250 device and provide\nestimates for the necessary memory throughput and the minimal amount of\nresources needed to deliver optimal performance depending on the available\nhardware platform.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:30:54 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 20:15:32 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Korcyl", "G.", ""], ["Korcyl", "P.", ""]]}, {"id": "1904.08762", "submitter": "Ahsan Javed Awan Dr", "authors": "Stefano Corda, Gagandeep Singh, Ahsan Javed Awan, Roel Jordans and\n  Henk Corporaal", "title": "Memory and Parallelism Analysis Using a Platform-Independent Approach", "comments": "22nd ACM International Workshop on Software and Compilers for\n  Embedded Systems (SCOPES '19), May 2019", "journal-ref": null, "doi": "10.1145/3323439.3323988", "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging computing architectures such as near-memory computing (NMC) promise\nimproved performance for applications by reducing the data movement between CPU\nand memory. However, detecting such applications is not a trivial task. In this\nongoing work, we extend the state-of-the-art platform-independent software\nanalysis tool with NMC related metrics such as memory entropy, spatial\nlocality, data-level, and basic-block-level parallelism. These metrics help to\nidentify the applications more suitable for NMC architectures.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:25:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Corda", "Stefano", ""], ["Singh", "Gagandeep", ""], ["Awan", "Ahsan Javed", ""], ["Jordans", "Roel", ""], ["Corporaal", "Henk", ""]]}, {"id": "1904.08964", "submitter": "Xin Jin", "authors": "Hang Zhu, Zhihao Bai, Jialin Li, Ellis Michael, Dan Ports, Ion Stoica,\n  Xin Jin", "title": "Harmonia: Near-Linear Scalability for Replicated Storage with In-Network\n  Conflict Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage employs replication to mask failures and improve\navailability. However, these systems typically exhibit a hard tradeoff between\nconsistency and performance. Ensuring consistency introduces coordination\noverhead, and as a result the system throughput does not scale with the number\nof replicas. We present Harmonia, a replicated storage architecture that\nexploits the capability of new-generation programmable switches to obviate this\ntradeoff by providing near-linear scalability without sacrificing consistency.\nTo achieve this goal, Harmonia detects read-write conflicts in the network,\nwhich enables any replica to serve reads for objects with no pending writes.\nHarmonia implements this functionality at line rate, thus imposing no\nperformance overhead. We have implemented a prototype of Harmonia on a cluster\nof commodity servers connected by a Barefoot Tofino switch, and have integrated\nit with Redis. We demonstrate the generality of our approach by supporting a\nvariety of replication protocols, including primary-backup, chain replication,\nViewstamped Replication, and NOPaxos. Experimental results show that Harmonia\nimproves the throughput of these protocols by up to 10X for a replication\nfactor of 10, providing near-linear scalability up to the limit of our testbed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 18:17:59 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Zhu", "Hang", ""], ["Bai", "Zhihao", ""], ["Li", "Jialin", ""], ["Michael", "Ellis", ""], ["Ports", "Dan", ""], ["Stoica", "Ion", ""], ["Jin", "Xin", ""]]}, {"id": "1904.08988", "submitter": "Mhashilkar, Parag A.", "authors": "Parag Mhashilkar, Mine Altunay, Eileen Berman, David Dagenhart, Stuart\n  Fuess, Burt Holzman, James Kowalkowski, Dmitry Litvintsev, Qiming Lu,\n  Alexander Moibenko, Marc Paterno, Panagiotis Spentzouris, Steven Timm,\n  Anthony Tiradani, Eric Vaandering (Fermilab), John Hover, and Jose Caballero\n  Bejar (Brookhaven)", "title": "HEPCloud, an Elastic Hybrid HEP Facility using an Intelligent Decision\n  Support System", "comments": "8 pages", "journal-ref": null, "doi": "10.1051/epjconf/201921403060", "report-no": "FERMILAB-CONF-18-658-CD", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  HEPCloud is rapidly becoming the primary system for provisioning compute\nresources for all Fermilab-affiliated experiments. In order to reliably meet\nthe peak demands of the next generation of High Energy Physics experiments,\nFermilab must plan to elastically expand its computational capabilities to\ncover the forecasted need. Commercial cloud and allocation-based High\nPerformance Computing (HPC) resources both have explicit and implicit costs\nthat must be considered when deciding when to provision these resources, and at\nwhich scale. In order to support such provisioning in a manner consistent with\norganizational business rules and budget constraints, we have developed a\nmodular intelligent decision support system (IDSS) to aid in the automatic\nprovisioning of resources spanning multiple cloud providers, multiple HPC\ncenters, and grid computing federations. In this paper, we discuss the goals\nand architecture of the HEPCloud Facility, the architecture of the IDSS, and\nour early experience in using the IDSS for automated facility expansion both at\nFermi and Brookhaven National Laboratory.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 19:55:10 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Mhashilkar", "Parag", "", "Fermilab"], ["Altunay", "Mine", "", "Fermilab"], ["Berman", "Eileen", "", "Fermilab"], ["Dagenhart", "David", "", "Fermilab"], ["Fuess", "Stuart", "", "Fermilab"], ["Holzman", "Burt", "", "Fermilab"], ["Kowalkowski", "James", "", "Fermilab"], ["Litvintsev", "Dmitry", "", "Fermilab"], ["Lu", "Qiming", "", "Fermilab"], ["Moibenko", "Alexander", "", "Fermilab"], ["Paterno", "Marc", "", "Fermilab"], ["Spentzouris", "Panagiotis", "", "Fermilab"], ["Timm", "Steven", "", "Fermilab"], ["Tiradani", "Anthony", "", "Fermilab"], ["Vaandering", "Eric", "", "Fermilab"], ["Hover", "John", "", "Brookhaven"], ["Bejar", "Jose Caballero", "", "Brookhaven"]]}, {"id": "1904.09283", "submitter": "Rathish Das", "authors": "Rathish Das, Shih-Yu Tsai, Sharmila Duppala, Jayson Lynch, Esther M.\n  Arkin, Rezaul Chowdhury, Joseph S. B. Mitchell, and Steven Skiena", "title": "Data Races and the Discrete Resource-time Tradeoff Problem with Resource\n  Reuse over Paths", "comments": null, "journal-ref": null, "doi": "10.1145/3323165.3323209", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A determinacy race occurs if two or more logically parallel instructions\naccess the same memory location and at least one of them tries to modify its\ncontent. Races often lead to nondeterministic and incorrect program behavior. A\ndata race is a special case of a determinacy race which can be eliminated by\nassociating a mutual-exclusion lock or allowing atomic accesses to the memory\nlocation. However, such solutions can reduce parallelism by serializing all\naccesses to that location. For associative and commutative updates, reducers\nallow parallel race-free updates at the expense of using some extra space. We\nask the following question. Given a fixed budget of extra space to mitigate the\ncost of races in a parallel program, which memory locations should be assigned\nreducers and how should the space be distributed among the reducers in order to\nminimize the overall running time? We argue that the races can be captured by a\ndirected acyclic graph (DAG), with nodes representing memory cells and arcs\nrepresenting read-write dependencies between cells. We then formulate our\noptimization problem on DAGs. We concentrate on a variation of this problem\nwhere space reuse among reducers is allowed by routing extra space along a\nsource to sink path of the DAG and using it in the construction of reducers\nalong the path. We consider two reducers and the corresponding duration\nfunctions (i.e., reduction time as a function of space budget). We generalize\nour race-avoiding space-time tradeoff problem to a discrete resource-time\ntradeoff problem with general non-increasing duration functions and resource\nreuse over paths. For general DAGs, the offline problem is strongly NP-hard\nunder all three duration functions, and we give approximation algorithms. We\nalso prove hardness of approximation for the general resource-time tradeoff\nproblem and give a pseudo-polynomial time algorithm for series-parallel DAGs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 17:48:35 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Das", "Rathish", ""], ["Tsai", "Shih-Yu", ""], ["Duppala", "Sharmila", ""], ["Lynch", "Jayson", ""], ["Arkin", "Esther M.", ""], ["Chowdhury", "Rezaul", ""], ["Mitchell", "Joseph S. B.", ""], ["Skiena", "Steven", ""]]}, {"id": "1904.09363", "submitter": "Kyle Kuan", "authors": "Kyle Kuan and Tosiron Adegbija", "title": "Energy-Efficient Runtime Adaptable L1 STT-RAM Cache Design", "comments": null, "journal-ref": null, "doi": "10.1109/TCAD.2019.2912920", "report-no": null, "categories": "cs.AR cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research has shown that applications have variable runtime cache\nrequirements. In the context of the increasingly popular Spin-Transfer Torque\nRAM (STT-RAM) cache, the retention time, which defines how long the cache can\nretain a cache block in the absence of power, is one of the most important\ncache requirements that may vary for different applications. In this paper, we\npropose a Logically Adaptable Retention Time STT-RAM (LARS) cache that allows\nthe retention time to be dynamically adapted to applications' runtime\nrequirements. LARS cache comprises of multiple STT-RAM units with different\nretention times, with only one unit being used at a given time. LARS\ndynamically determines which STT-RAM unit to use during runtime, based on\nexecuting applications' needs. As an integral part of LARS, we also explore\ndifferent algorithms to dynamically determine the best retention time based on\ndifferent cache design tradeoffs. Our experiments show that by adapting the\nretention time to different applications' requirements, LARS cache can reduce\nthe average cache energy by 25.31%, compared to prior work, with minimal\noverheads.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 23:03:04 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Kuan", "Kyle", ""], ["Adegbija", "Tosiron", ""]]}, {"id": "1904.09496", "submitter": "DaeJin Kim", "authors": "DaeJin Kim, Hyegyeong Park, and Junkyun Choi", "title": "Optimal Load Allocation for Coded Distributed Computation in\n  Heterogeneous Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, coding has been a useful technique to mitigate the effect of\nstragglers in distributed computing. However, coding in this context has been\nmainly explored under the assumption of homogeneous workers, although the\nreal-world computing clusters can be often composed of heterogeneous workers\nthat have different computing capabilities. The uniform load allocation without\nthe awareness of heterogeneity possibly causes a significant loss in latency.\nIn this paper, we suggest the optimal load allocation for coded distributed\ncomputing with heterogeneous workers. Specifically, we focus on the scenario\nthat there exist workers having the same computing capability, which can be\nregarded as a group for analysis. We rely on the lower bound on the expected\nlatency and obtain the optimal load allocation by showing that our proposed\nload allocation achieves the minimum of the lower bound for a sufficiently\nlarge number of workers. From numerical simulations, when assuming the group\nheterogeneity, our load allocation reduces the expected latency by orders of\nmagnitude over the existing load allocation scheme.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 20:31:25 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 05:23:29 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kim", "DaeJin", ""], ["Park", "Hyegyeong", ""], ["Choi", "Junkyun", ""]]}, {"id": "1904.09538", "submitter": "James Stevens", "authors": "James D. Stevens, Andreas Kl\\\"ockner", "title": "A mechanism for balancing accuracy and scope in cross-machine black-box\n  GPU performance modeling", "comments": "25 pages, 9 figures", "journal-ref": "The International Journal of High Performance Computing\n  Applications, June 2020", "doi": "10.1177/1094342020921340", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to model, analyze, and predict execution time of computations is\nan important building block supporting numerous efforts, such as load\nbalancing, performance optimization, and automated performance tuning for high\nperformance, parallel applications. In today's increasingly heterogeneous\ncomputing environment, this task must be accomplished efficiently across\nmultiple architectures, including massively parallel coprocessors like GPUs. To\naddress this challenge, we present an approach for constructing customizable,\ncross-machine performance models for GPU kernels, including a mechanism to\nautomatically and symbolically gather performance-relevant kernel operation\ncounts, a tool for formulating mathematical models using these counts, and a\ncustomizable parameterized collection of benchmark kernels used to calibrate\nmodels to GPUs in a black-box fashion. Our approach empowers a user to manage\ntrade-offs between model accuracy, evaluation speed, and generalizability. A\nuser can define a model and customize the calibration process, making it as\nsimple or complex as desired, and as application-targeted or general as\ndesired. To evaluate our approach, we demonstrate both linear and nonlinear\nmodels; each example models execution times for multiple variants of a\nparticular computation: two matrix multiplication variants, four Discontinuous\nGalerkin (DG) differentiation operation variants, and two 2-D five-point finite\ndifference stencil variants. For each variant, we present accuracy results on\nGPUs from multiple vendors and hardware generations. We view this customizable\napproach as a response to a central question in GPU performance modeling: how\ncan we model GPU performance in a cost-explanatory fashion while maintaining\naccuracy, evaluation speed, portability, and ease of use, an attribute we\nbelieve precludes manual collection of kernel or hardware statistics.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 03:50:20 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 20:57:09 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 15:46:53 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Stevens", "James D.", ""], ["Kl\u00f6ckner", "Andreas", ""]]}, {"id": "1904.09595", "submitter": "Aleksandar To\\v{s}i\\'c", "authors": "Aleksandar To\\v{s}i\\'c, Jernej Vi\\v{c}i\\v{c}, Michael Mrissa", "title": "A Blockchain-based Decentralized Self-balancing Architecture for the Web\n  of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is a distributed computing paradigm that relies on\ncomputational resources of end devices in a network to bring benefits such as\nlow bandwidth utilization, responsiveness, scalability and privacy\npreservation. Applications range from large scale sensor networks to IoT, and\nconcern multiple domains (agriculture, supply chain, medicine. . . ). However,\nresource usage optimization, a challenge due to the limited capacity of edge\ndevices, is typically handled in a centralized way, which remains an important\nlimitation. In this paper, we propose a decentralized approach that relies on a\ncombination of blockchain and consensus algorithm to monitor network resources\nand if necessary, migrate applications at run-time. We integrate our solution\ninto an application container platform, thus providing an edge architecture\ncapable of general purpose computation. We validate and evaluate our solution\nwith a proof-of-concept implementation in a national cultural heritage\nbuilding.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 13:03:51 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["To\u0161i\u0107", "Aleksandar", ""], ["Vi\u010di\u010d", "Jernej", ""], ["Mrissa", "Michael", ""]]}, {"id": "1904.09610", "submitter": "Mengsu Ding", "authors": "Mengsu Ding, Muqiao Yang, Shimin Chen", "title": "Storing and Querying Large-Scale Spatio-Temporal Graphs with\n  High-Throughput Edge Insertions", "comments": "Section 5.1.1 is modified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world graphs often contain spatio-temporal information and evolve over\ntime. Compared with static graphs, spatio-temporal graphs have very different\ncharacteristics, presenting more significant challenges in data volume, data\nvelocity, and query processing. In this paper, we describe three representative\napplications to understand the features of spatio-temporal graphs. Based on the\ncommonalities of the applications, we define a formal spatio-temporal graph\nmodel, where a graph consists of location vertices, object vertices, and event\nedges. Then we discuss a set of design goals to meet the requirements of the\napplications: (i) supporting up to 10 billion object vertices, 10 million\nlocation vertices, and 100 trillion edges in the graph, (ii) supporting up to 1\ntrillion new edges that are streamed in daily, and (iii) minimizing\ncross-machine communication for query processing. We propose and evaluate PAST,\na framework for efficient PArtitioning and query processing of Spatio-Temporal\ngraphs. Experimental results show that PAST successfully achieves the above\ngoals. It improves query performance by orders of magnitude compared with\nstate-of-the-art solutions, including JanusGraph, Greenplum, Spark and\nST-Hadoop.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:52:00 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 10:35:11 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Ding", "Mengsu", ""], ["Yang", "Muqiao", ""], ["Chen", "Shimin", ""]]}, {"id": "1904.09681", "submitter": "Evangelos Pournaras", "authors": "Jovan Nikolic, Evangelos Pournaras", "title": "Structural Self-adaptation for Decentralized Pervasive Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication structure plays a key role in the learning capability of\ndecentralized systems. Structural self-adaptation, by means of\nself-organization, changes the order as well as the input information of the\nagents' collective decision-making. This paper studies the role of agents'\nrepositioning on the same communication structure, i.e. a tree, as the means to\nexpand the learning capacity in complex combinatorial optimization problems,\nfor instance, load-balancing power demand to prevent blackouts or efficient\nutilization of bike sharing stations. The optimality of structural\nself-adaptations is rigorously studied by constructing a novel large-scale\nbenchmark that consists of 4000 agents with synthetic and real-world data\nperforming 4 million structural self-adaptations during which almost 320\nbillion learning messages are exchanged. Based on this benchmark dataset, 124\ndeterministic structural criteria, applied as learning meta-features, are\nsystematically evaluated as well as two online structural self-adaptation\nstrategies designed to expand learning capacity. Experimental evaluation\nidentifies metrics that capture agents with influential information and their\noptimal positioning. Significant gain in learning performance is observed for\nthe two strategies especially under low-performing initialization. Strikingly,\nthe strategy that triggers structural self-adaptation in a more exploratory\nfashion is the most cost-effective.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 23:55:01 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 11:31:09 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Nikolic", "Jovan", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "1904.09814", "submitter": "Ganapati Bhat", "authors": "Ganapati Bhat, Suat Gumussoy, Umit Y. Ogras", "title": "Power and Thermal Analysis of Commercial Mobile Platforms: Experiments\n  and Case Studies", "comments": "To appear in proceedings of IEEE DATE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art mobile processors can deliver fast response time and high\nthroughput to maximize the user experience. However, high performance comes at\nthe expense of larger power density, which leads to higher skin temperatures.\nSince this can degrade the user experience, there is a strong need for power\nconsumption and thermal analysis in mobile processors. In this paper, we first\nperform experiments on the Nexus 6P phone to study the power, performance and\nthermal behavior of modern smartphones. Using the insight from these\nexperiments, we propose a control algorithm that throttles select applications\nwithout affecting other apps. We demonstrate our governor on the Exynos 5422\nprocessor employed in the Odroid-XU3 board.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 16:26:20 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Bhat", "Ganapati", ""], ["Gumussoy", "Suat", ""], ["Ogras", "Umit Y.", ""]]}, {"id": "1904.09818", "submitter": "Artur Andrzejak", "authors": "Artur Andrzejak, Oliver Wenz, Diego Costa", "title": "One DSL to Rule Them All: IDE-Assisted Code Generation for Agile Data\n  Analysis", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis is at the core of scientific studies, a prominent task that\nresearchers and practitioners typically undertake by programming their own set\nof automated scripts. While there is no shortage of tools and languages\navailable for designing data analysis pipelines, users spend substantial effort\nin learning the specifics of such languages/tools and often design solutions\ntoo project specific to be reused in future studies. Furthermore, users need to\nput further effort into making their code scalable, as parallel implementations\nare typically more complex.\n  We address these problems by proposing an advanced code recommendation tool\nwhich facilitates developing data science scripts. Users formulate their\nintentions in a human-readable Domain Specific Language (DSL) for dataframe\nmanipulation and analysis. The DSL statements can be converted into executable\nPython code during editing. To avoid the need to learn the DSL and increase\nuser-friendliness, our tool supports code completion in mainstream IDEs and\neditors. Moreover, DSL statements can generate executable code for different\ndata analysis frameworks (currently we support Pandas and PySpark). Overall,\nour approach attempts to accelerate programming of common data analysis tasks\nand to facilitate the conversion of the implementations between frameworks.\n  In a preliminary assessment based on a popular data processing tutorial, our\ntool was able to fully cover 9 out of 14 processing steps for Pandas and 10 out\nof 16 for PySpark, while partially covering 4 processing steps for each of the\nframeworks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:10:59 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Andrzejak", "Artur", ""], ["Wenz", "Oliver", ""], ["Costa", "Diego", ""]]}, {"id": "1904.09838", "submitter": "Issam Damaj", "authors": "Adnan I. Yaqzan (1), Issam W. Damaj (2), and Rached N. Zantout (3)\n  ((1) Millenium Group Services, (2) American University of Kuwait, (3) Prince\n  Sultan University)", "title": "GPS-Based Vehicle Tracking System-on-Chip", "comments": "6 Pages, 4 Tables, 6 Figures", "journal-ref": "Intl. Jrnl. of. Elec. & Comp. Sc. 10(2010) 7-12", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern powerful reconfigurable systems are suited in the implementation of\nvarious data-stream, data-parallel, and other applications. An application that\nneeds real-time, fast, and reliable data processing is the global positioning\nsystem (GPS)-based vehicle tracking system (VTS). In this paper, we build on a\nrecently produced VTS (The Aram Locator) offering a system-on-chip (SOC)\nreplacement of the current microcontroller-based implementation. The proposed\nSOC is built on a field programmable gate array (FPGA) promising a cheaper\ndesign, a more cohesive architecture, a faster processing time and an enhanced\nsystem interaction. Different designs, and their hardware implementations, are\nproposed with different levels of integration. Performance analysis and\nevaluation of the investigated designs are included.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 00:50:37 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yaqzan", "Adnan I.", ""], ["Damaj", "Issam W.", ""], ["Zantout", "Rached N.", ""]]}, {"id": "1904.09839", "submitter": "Uthsav Chitra", "authors": "Tarun Chitra and Uthsav Chitra", "title": "Committee Selection is More Similar Than You Think: Evidence from\n  Avalanche and Stellar", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased interest in scalable and high-throughput blockchains has led to an\nexplosion in the number of committee selection methods in the literature.\nCommittee selection mechanisms allow consensus protocols to safely select a\ncommittee, or a small subset of validators that is permitted to vote and verify\na block of transactions, in a distributed ledger. There are many such\nmechanisms, each with substantially different methodologies and guarantees on\ncommunication complexity, resource usage, and fairness. In this paper, we\nillustrate that, despite these implementation-level differences, there are\nstrong statistical similarities between committee selection mechanisms. We\nconcretely show this by proving that the committee selection of the Avalanche\nconsensus protocol can be used to choose committees in the Stellar Consensus\nProtocol that satisfy the necessary and sufficient conditions for Byzantine\nagreement. We also verify these claims using simulations and numerically\nobserve sharp phase transitions as a function of protocol parameters. Our\nresults suggest the existence of a \"statistical taxonomy\" of committee\nselection mechanisms in distributed consensus algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 20:34:46 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chitra", "Tarun", ""], ["Chitra", "Uthsav", ""]]}, {"id": "1904.09917", "submitter": "Alcardo Alex Barakabitze Aab", "authors": "Alcardo Alex Barakabitze, Lingfen Sun, Is-Haka Mkwawa, Emmanuel\n  Ifeachor", "title": "A Novel QoE-Aware SDN-enabled, NFV-based Management Architecture for\n  Future Multimedia Applications on 5G Systems", "comments": null, "journal-ref": "2016", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes a novel QoE-aware SDN enabled NFV architecture for\ncontrolling and managing Future Multimedia Applications on 5G systems. The aim\nis to improve the QoE of the delivered multimedia services through the\nfulfilment of personalized QoE application requirements. This novel approach\nprovides some new features, functionalities, concepts and opportunities for\novercoming the key QoE provisioning limitations in current 4G systems such as\nincreased network management complexity and inability to adapt dynamically to\nchanging application, network transmission or traffic or end-users demand.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 15:19:59 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Barakabitze", "Alcardo Alex", ""], ["Sun", "Lingfen", ""], ["Mkwawa", "Is-Haka", ""], ["Ifeachor", "Emmanuel", ""]]}, {"id": "1904.09940", "submitter": "Naftaly Minsky", "authors": "Naftaly Minsky and Chen Cong", "title": "Scalable, Secure and Broad-Spectrum Enforcement of Contracts, Without\n  Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a scalable and secure contract-enforcement mechanism,\ncalled Cop, which can be applied to a broad range of multi-agent systems\nincluding small and large systems, time-critical systems, and\nsystems-of-systems. Cop enforces contracts (or protocols) via the existing Law-\nGoverned Interaction (LGI) mechanism, coupled with a new protective layer that\nsignificantly enhances the dependability and security of such enforcement. Cop\nis arguably superior to the currently popular blockchain-based smart-contract\nmechanisms, due to its scalability, interoperability, and the breadth of the\nspectrum of its domain of applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 16:29:07 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Minsky", "Naftaly", ""], ["Cong", "Chen", ""]]}, {"id": "1904.09984", "submitter": "Moo-Ryong Ra", "authors": "Moo-Ryong Ra and Hee Won Lee", "title": "IOArbiter: Dynamic Provisioning of Backend Block Storage in the Cloud", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of virtualization technology, cloud computing realizes\non-demand computing. The capability of dynamic resource provisioning is a\nfundamental driving factor for users to adopt the cloud technology. The aspect\nis important for cloud service providers to optimize the expense for running\nthe infrastructure as well. Despite many technological advances in related\nareas, however, it is still the case that the infrastructure providers must\ndecide hardware configuration before deploying a cloud infrastructure,\nespecially from the storage's perspective. This static nature of the storage\nprovisioning practice can cause many problems in meeting tenant requirements,\nwhich often come later into the picture. In this paper, we propose a system\ncalled IOArbiter that enables the dynamic creation of underlying storage\nimplementation in the cloud. IOArbiter defers storage provisioning to the time\nat which a tenant actually requests a storage space. As a result, an underlying\nstorage implementation, e.g., RAID-5, 6 or Ceph storage pool with 6+3 erasure\ncoding, will be materialized at the volume creation time. Using our prototype\nimplementation with Openstack Cinder, we show that IOArbiter can simultaneously\nsatisfy a number of different tenant demands, which may not be possible with a\nstatic configuration strategy. Additionally QoS mechanisms such as admission\ncontrol and dynamic throttling help the system mitigate a noisy neighbor\nproblem significantly.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:14:49 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Ra", "Moo-Ryong", ""], ["Lee", "Hee Won", ""]]}, {"id": "1904.10067", "submitter": "Kartik Nayak", "authors": "Dahlia Malkhi and Kartik Nayak and Ling Ren", "title": "Flexible Byzantine Fault Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Flexible BFT, a new approach for BFT consensus solution\ndesign revolving around two pillars, stronger resilience and diversity. The\nfirst pillar, stronger resilience, involves a new fault model called\nalive-but-corrupt faults. Alive-but-corrupt replicas may arbitrarily deviate\nfrom the protocol in an attempt to break safety of the protocol. However, if\nthey cannot break safety, they will not try to prevent liveness of the\nprotocol. Combining alive-but-corrupt faults into the model, Flexible BFT is\nresilient to higher corruption levels than possible in a pure Byzantine fault\nmodel. The second pillar, diversity, designs consensus solutions whose protocol\ntranscript is used to draw different commit decisions under diverse beliefs.\nWith this separation, the same Flexible BFT solution supports synchronous and\nasynchronous beliefs, as well as varying resilience threshold combinations of\nByzantine and alive-but-corrupt faults.\n  At a technical level, Flexible BFT achieves the above results using two new\nideas. First, it introduces a synchronous BFT protocol in which only the commit\nstep requires to know the network delay bound and thus replicas execute the\nprotocol without any synchrony assumption. Second, it introduces a notion\ncalled Flexible Byzantine Quorums by dissecting the roles of different quorums\nin existing consensus protocols.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 21:12:01 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 20:27:17 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Malkhi", "Dahlia", ""], ["Nayak", "Kartik", ""], ["Ren", "Ling", ""]]}, {"id": "1904.10083", "submitter": "Lu Zhang", "authors": "Lu Zhang and Steven Swanson", "title": "Pangolin: A Fault-Tolerant Persistent Memory Programming Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile main memory (NVMM) allows programmers to build complex,\npersistent, pointer-based data structures that can offer substantial\nperformance gains over conventional approaches to managing persistent state.\nThis programming model removes the file system from the critical path which\nimproves performance, but it also places these data structures out of reach of\nfile system-based fault tolerance mechanisms (e.g., block-based checksums or\nerasure coding). Without fault-tolerance, using NVMM to hold critical data will\nbe much less attractive.\n  This paper presents Pangolin, a fault-tolerant persistent object library\ndesigned for NVMM. Pangolin uses a combination of checksums, parity, and\nmicro-buffering to protect an application's objects from both media errors and\ncorruption due to software bugs. It provides these protections for objects of\nany size and supports automatic, online detection of data corruption and\nrecovery. The required storage overhead is small (1% for gigabyte-sized pools\nof NVMM). Pangolin provides stronger protection, requires orders of magnitude\nless storage overhead, and achieves comparable performance relative to the\ncurrent state-of-the-art fault-tolerant persistent object library.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 22:35:33 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 16:48:36 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Zhang", "Lu", ""], ["Swanson", "Steven", ""]]}, {"id": "1904.10119", "submitter": "Doru Thom Popovici", "authors": "Doru Thom Popovici, Martin D. Schatz, Franz Franchetti, Tze Meng Low", "title": "A Flexible Framework for Parallel Multi-Dimensional DFTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional discrete Fourier transforms (DFT) are typically decomposed\ninto multiple 1D transforms. Hence, parallel implementations of any\nmulti-dimensional DFT focus on parallelizing within or across the 1D DFT.\nExisting DFT packages exploit the inherent parallelism across the 1D DFTs and\noffer rigid frameworks, that cannot be extended to incorporate both forms of\nparallelism and various data layouts to enable some of the parallelism.\nHowever, in the era of exascale, where systems have thousand of nodes and\nintricate network topologies, flexibility and parallel efficiency are key\naspects all multi-dimensional DFT frameworks need to have in order to map and\nscale the computation appropriately. In this work, we present a flexible\nframework, built on the Redistribution Operations and Tensor Expressions (ROTE)\nframework, that facilitates the development of a family of parallel\nmulti-dimensional DFT algorithms by 1) unifying the two parallelization schemes\nwithin a single framework, 2) exploiting the two different parallelization\nschemes to different degrees and 3) using different data layouts to distribute\nthe data across the compute nodes. We demonstrate the need of a versatile\nframework and thus a need for a family of parallel multi-dimensional DFT\nalgorithms on the K-Computer, where we show almost linear strong scaling\nresults for problem sizes of 1024^3 on 32k compute nodes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 02:16:49 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 21:04:22 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Popovici", "Doru Thom", ""], ["Schatz", "Martin D.", ""], ["Franchetti", "Franz", ""], ["Low", "Tze Meng", ""]]}, {"id": "1904.10125", "submitter": "Yogesh Simmhan", "authors": "Prateeksha Varshney and Yogesh Simmhan", "title": "Characterizing Application Scheduling on Edge, Fog and Cloud Computing\n  Resources", "comments": "Pre-print of journal article: Varshney P, Simmhan Y. Characterizing\n  application scheduling on edge, fog, and cloud computing resources. Softw:\n  Pract Exper. 2019; 1--37. https://doi.org/10.1002/spe.2699", "journal-ref": "Software: Practice and Experience, Volume 50, Issue 5, May 2020,\n  Pages 558-595", "doi": "10.1002/spe.2699", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has grown to become a popular distributed computing service\noffered by commercial providers. More recently, Edge and Fog computing\nresources have emerged on the wide-area network as part of Internet of Things\n(IoT) deployments. These three resource abstraction layers are complementary,\nand provide distinctive benefits. Scheduling applications on clouds has been an\nactive area of research, with workflow and dataflow models serving as a\nflexible abstraction to specify applications for execution. However, the\napplication programming and scheduling models for edge and fog are still\nmaturing, and can benefit from learnings on cloud resources. At the same time,\nthere is also value in using these resources cohesively for application\nexecution. In this article, we present a taxonomy of concepts essential for\nspecifying and solving the problem of scheduling applications on edge, for and\ncloud computing resources. We first characterize the resource capabilities and\nlimitations of these infrastructure, and design a taxonomy of application\nmodels, Quality of Service (QoS) constraints and goals, and scheduling\ntechniques, based on a literature review. We also tabulate key research\nprototypes and papers using this taxonomy. This survey benefits developers and\nresearchers on these distributed resources in designing and categorizing their\napplications, selecting the relevant computing abstraction(s), and developing\nor selecting the appropriate scheduling algorithm. It also highlights gaps in\nliterature where open problems remain.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 02:49:36 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Varshney", "Prateeksha", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1904.10221", "submitter": "Aur\\'elien Cavelan", "authors": "Aur\\'elien Cavelan, Rub\\'en M. Cabez\\'on, Florina M. Ciorba", "title": "Detection of Silent Data Corruptions in Smoothed Particle Hydrodynamics\n  Simulations", "comments": "20pages preprint, CCGRID'19 conference", "journal-ref": null, "doi": "10.1109/CCGRID.2019.00013", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Silent data corruptions (SDCs) hinder the correctness of long-running\nscientific applications on large scale computing systems. Selective particle\nreplication (SPR) is proposed herein as the first particle-based replication\nmethod for detecting SDCs in Smoothed particle hydrodynamics (SPH) simulations.\nSPH is a mesh-free Lagrangian method commonly used to perform hydrodynamical\nsimulations in astrophysics and computational fluid dynamics. SPH performs\ninterpolation of physical properties over neighboring discretization points\n(called SPH particles) that dynamically adapt their distribution to the mass\ndensity field of the fluid. When a fault (e.g., a bit-flip) strikes the\ncomputation or the data associated with a particle, the resulting error is\nsilently propagated to all nearest neighbors through such interpolation steps.\nSPR replicates the computation and data of a few carefully selected SPH\nparticles. SDCs are detected when the data of a particle differs, due to\ncorruption, from its replicated counterpart. SPR is able to detect many DRAM\nSDCs as they propagate by ensuring that all particles have at least one\nneighbor that is replicated. The detection capabilities of SPR were assessed\nthrough a set of error-injection and detection experiments and the overhead of\nSPR was evaluated via a set of strong-scaling experiments conducted on an HPC\nsystem. The results show that SPR achieves detection rates of 91-99.9%, no\nfalse-positives, at an overhead of 1-10%.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 09:39:27 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Cavelan", "Aur\u00e9lien", ""], ["Cabez\u00f3n", "Rub\u00e9n M.", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1904.10399", "submitter": "Lili Su", "authors": "Lili Su and Chia-Jung Chang and Nancy Lynch", "title": "Spike-Based Winner-Take-All Computation: Fundamental Limits and\n  Order-Optimal Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Winner-Take-All (WTA) refers to the neural operation that selects a\n(typically small) group of neurons from a large neuron pool. It is conjectured\nto underlie many of the brain's fundamental computational abilities. However,\nnot much is known about the robustness of a spike-based WTA network to the\ninherent randomness of the input spike trains. In this work, we consider a\nspike-based $k$--WTA model wherein $n$ randomly generated input spike trains\ncompete with each other based on their underlying statistics, and $k$ winners\nare supposed to be selected. We slot the time evenly with each time slot of\nlength $1\\, ms$, and model the $n$ input spike trains as $n$ independent\nBernoulli processes. The Bernoulli process is a good approximation of the\npopular Poisson process but is more biologically relevant as it takes the\nrefractory periods into account. Due to the randomness in the input spike\ntrains, no circuits can guarantee to successfully select the correct winners in\nfinite time. We focus on analytically characterizing the minimal amount of time\nneeded so that a target minimax decision accuracy (success probability) can be\nreached.\n  We first derive an information-theoretic lower bound on the decision time. We\nshow that to have a (minimax) decision error $\\le \\delta$ (where $\\delta \\in\n(0,1)$), the computation time of any WTA circuit is at least \\[ ((1-\\delta)\n\\log(k(n -k)+1) -1)T_{\\mathcal{R}}, \\] where $T_{\\mathcal{R}}$ is a difficulty\nparameter of a WTA task that is independent of $\\delta$, $n$, and $k$. We then\ndesign a simple WTA circuit whose decision time is \\[ O(\n\\log\\frac{1}{\\delta}+\\log k(n-k))T_{\\mathcal{R}}). \\] It turns out that for any\nfixed $\\delta \\in (0,1)$, this decision time is order-optimal in terms of its\nscaling in $n$, $k$, and $T_{\\mathcal{R}}$.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 02:45:09 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Su", "Lili", ""], ["Chang", "Chia-Jung", ""], ["Lynch", "Nancy", ""]]}, {"id": "1904.10489", "submitter": "Jingpeng Wu", "authors": "Jingpeng Wu, William M. Silversmith, Kisuk Lee, H. Sebastian Seung", "title": "Chunkflow: Distributed Hybrid Cloud Processing of Large 3D Images by\n  Convolutional Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now common to process volumetric biomedical images using 3D\nConvolutional Networks (ConvNets). This can be challenging for the teravoxel\nand even petavoxel images that are being acquired today by light or electron\nmicroscopy. Here we introduce chunkflow, a software framework for distributing\nConvNet processing over local and cloud GPUs and CPUs. The image volume is\ndivided into overlapping chunks, each chunk is processed by a ConvNet, and the\nresults are blended together to yield the output image. The frontend submits\nConvNet tasks to a cloud queue. The tasks are executed by local and cloud GPUs\nand CPUs. Thanks to the fault-tolerant architecture of Chunkflow, cost can be\ngreatly reduced by utilizing cheap unstable cloud instances. Chunkflow\ncurrently supports PyTorch for GPUs and PZnet for CPUs. To illustrate its\nusage, a large 3D brain image from serial section electron microscopy was\nprocessed by a 3D ConvNet with a U-Net style architecture. Chunkflow provides\nsome chunk operations for general use, and the operations can be composed\nflexibly in a command line interface.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 18:47:57 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 14:20:52 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 13:48:34 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Wu", "Jingpeng", ""], ["Silversmith", "William M.", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1904.10706", "submitter": "Martijn Struijs", "authors": "Kristian Hinnenthal and Christian Scheideler and Martijn Struijs", "title": "Fast Distributed Algorithms for LP-Type Problems of Bounded Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present various distributed algorithms for LP-type problems\nin the well-known gossip model. LP-type problems include many important classes\nof problems such as (integer) linear programming, geometric problems like\nsmallest enclosing ball and polytope distance, and set problems like hitting\nset and set cover. In the gossip model, a node can only push information to or\npull information from nodes chosen uniformly at random. Protocols for the\ngossip model are usually very practical due to their fast convergence, their\nsimplicity, and their stability under stress and disruptions. Our algorithms\nare very efficient (logarithmic rounds or better with just polylogarithmic\ncommunication work per node per round) whenever the combinatorial dimension of\nthe given LP-type problem is constant, even if the size of the given LP-type\nproblem is polynomially large in the number of nodes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 09:18:13 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Hinnenthal", "Kristian", ""], ["Scheideler", "Christian", ""], ["Struijs", "Martijn", ""]]}, {"id": "1904.10889", "submitter": "Chuan-Chi Lai", "authors": "Chuan-Chi Lai, Zulhaydar Fairozal Akbar, Chuan-Ming Liu, Van-Dai Ta,\n  Li-Chun Wang", "title": "Distributed Continuous Range-Skyline Query Monitoring over the Internet\n  of Mobile Things", "comments": "Accepted by IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2019.2909393", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Range-Skyline Query (RSQ) is the combination of range query and skyline\nquery. It is one of the practical query types in multi-criteria decision\nservices, which may include the spatial and non-spatial information as well as\nmake the resulting information more useful than skyline search when the\nlocation is concerned. Furthermore, Continuous Range-Skyline Query (CRSQ) is an\nextension of Range-Skyline Query (RSQ) that the system continuously reports the\nskyline results to a query within a given search range. This work focuses on\nthe RSQ and CRSQ within a specific range on Internet of Mobile Things (IoMT)\napplications. Many server-client approaches for CRSQ have been proposed but are\nsensitive to the number of moving objects. We propose an effective and\nnon-centralized approach, Distributed Continuous Range-Skyline Query process\n(DCRSQ process), for supporting RSQ and CRSQ in mobile environments. By\nconsidering the mobility, the proposed approach can predict the time when an\nobject falls in the query range and ignore more irrelevant information when\nderiving the results, thus saving the computation overhead. The proposed\napproach, DCRSQ process, is analyzed on cost and validated with extensive\nsimulated experiments. The results show that DCRSQ process outperforms the\nexisting approaches in different scenarios and aspects.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:55:34 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Lai", "Chuan-Chi", ""], ["Akbar", "Zulhaydar Fairozal", ""], ["Liu", "Chuan-Ming", ""], ["Ta", "Van-Dai", ""], ["Wang", "Li-Chun", ""]]}, {"id": "1904.10984", "submitter": "Frederik Mallmann-Trenn", "authors": "Frederik Mallmann-Trenn, Yannic Maus, Dominik Pajak", "title": "Noidy Conmunixatipn: On the Convergence of the Averaging Population\n  Protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a process of \\emph{averaging} in a distributed system with\n\\emph{noisy communication}. Each of the agents in the system starts with some\nvalue and the goal of each agent is to compute the average of all the initial\nvalues. In each round, one pair of agents is drawn uniformly at random from the\nwhole population, communicates with each other and each of these two agents\nupdates their local value based on their own value and the received message.\nThe communication is noisy and whenever an agent sends any value $v$, the\nreceiving agent receives $v+N$, where $N$ is a zero-mean Gaussian random\nvariable. The two quality measures of interest are (i) the total sum of squares\n$TSS(t)$, which measures the sum of square distances from the average load to\nthe \\emph{initial average} and (ii) $\\bar{\\phi}(t)$, measures the sum of square\ndistances from the average load to the \\emph{running average} (average at time\n$t$).\n  It is known that the simple averaging protocol---in which an agent sends its\ncurrent value and sets its new value to the average of the received value and\nits current value---converges eventually to a state where $\\bar{\\phi}(t)$ is\nsmall.\n  It has been observed that $TSS(t)$, due to the noise, eventually diverges and\nprevious research---mostly in control theory---has focused on showing eventual\nconvergence w.r.t. the running average.\n  We obtain the first probabilistic bounds on the convergence time of\n$\\bar{\\phi}(t)$ and precise bounds on the drift of $TSS(t)$ that show that\nalbeit $TSS(t)$ eventually diverges, for a wide and interesting range of\nparameters, $TSS(t)$ stays small for a number of rounds that is polynomial in\nthe number of agents.\n  Our results extend to the synchronous setting and settings where the agents\nare restricted to discrete values and perform rounding.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 18:01:15 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Mallmann-Trenn", "Frederik", ""], ["Maus", "Yannic", ""], ["Pajak", "Dominik", ""]]}, {"id": "1904.11323", "submitter": "Vitaly Aksenov", "authors": "Vitaly Aksenov Dan Alistarh Petr Kuznetsov", "title": "Performance Prediction for Coarse-Grained Locking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard design pattern found in many concurrent data structures, such as\nhash tables or ordered containers, is an alternation of parallelizable sections\nthat incur no data conflicts and critical sections that must run sequentially\nand are protected with locks. A lock can be viewed as a queue that arbitrates\nthe order in which the critical sections are executed, and a natural question\nis whether we can use stochastic analysis to predict the resulting throughput.\nAs a preliminary evidence to the affirmative, we describe a simple model that\ncan be used to predict the throughput of coarse-grained lock-based algorithms.\nWe show that our model works well for CLH lock, and we expect it to work for\nother popular lock designs such as TTAS, MCS, etc.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:25:57 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Kuznetsov", "Vitaly Aksenov Dan Alistarh Petr", ""]]}, {"id": "1904.11395", "submitter": "Alexander Setzer", "authors": "Christian Scheideler, Alexander Setzer", "title": "On the Complexity of Local Graph Transformations", "comments": "This publication is the full version of a paper that appeared at\n  ICALP'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of transforming a given graph $G_s$ into a desired\ngraph $G_t$ by applying a minimum number primitives from a particular set of\nlocal graph transformation primitives. These primitives are local in the sense\nthat each node can apply them based on local knowledge and by affecting only\nits $1$-neighborhood. Although the specific set of primitives we consider makes\nit possible to transform any (weakly) connected graph into any other (weakly)\nconnected graph consisting of the same nodes, they cannot disconnect the graph\nor introduce new nodes into the graph, making them ideal in the context of\nsupervised overlay network transformations. We prove that computing a minimum\nsequence of primitive applications (even centralized) for arbitrary $G_s$ and\n$G_t$ is NP-hard, which we conjecture to hold for any set of local graph\ntransformation primitives satisfying the aforementioned properties. On the\nother hand, we show that this problem admits a polynomial time algorithm with a\nconstant approximation ratio.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 14:59:13 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 14:19:38 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""]]}, {"id": "1904.11402", "submitter": "Erick Lavoie", "authors": "Erick Lavoie, Laurie Hendren, Fr\\'ederic Desprez, Miguel Correia", "title": "Genet: A Quickly Scalable Fat-Tree Overlay for Personal Volunteer\n  Computing using WebRTC", "comments": null, "journal-ref": null, "doi": "10.1109/SASO.2019.00023", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  WebRTC enables browsers to exchange data directly but the number of possible\nconcurrent connections to a single source is limited. We overcome the\nlimitation by organizing participants in a fat-tree overlay: when the maximum\nnumber of connections of a tree node is reached, the new participants connect\nto the node's children. Our design quickly scales when a large number of\nparticipants join in a short amount of time, by relying on a novel scheme that\nonly requires local information to route connection messages: the destination\nis derived from the hash value of the combined identifiers of the message's\nsource and of the node that is holding the message. The scheme provides\ndeterministic routing of a sequence of connection messages from a single source\nand probabilistic balancing of newer connections among the leaves. We show that\nthis design puts at least 83% of nodes at the same depth as a deterministic\nalgorithm, can connect a thousand browser windows in 21-55 seconds in a local\nnetwork, and can be deployed for volunteer computing to tap into 320 cores in\nless than 30 seconds on a local network to increase the total throughput on the\nCollatz application by two orders of magnitude compared to a single core.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:20:28 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Lavoie", "Erick", ""], ["Hendren", "Laurie", ""], ["Desprez", "Fr\u00e9deric", ""], ["Correia", "Miguel", ""]]}, {"id": "1904.11440", "submitter": "Keren Censor-Hillel", "authors": "Matthias Bonne and Keren Censor-Hillel", "title": "Distributed Detection of Cliques in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an in-depth study of the fundamental problems of finding\nsmall subgraphs in distributed dynamic networks. While some problems are\ntrivially easy to handle, such as detecting a triangle that emerges after an\nedge insertion, we show that, perhaps somewhat surprisingly, other problems\nexhibit a wide range of complexities in terms of the trade-offs between their\nround and bandwidth complexities. In the case of triangles, which are only\naffected by the topology of the immediate neighborhood, some end results are:\n  \\begin{itemize}\n  \\item The bandwidth complexity of $1$-round dynamic triangle detection or\nlisting is $\\Theta(1)$.\n  \\item The bandwidth complexity of $1$-round dynamic triangle membership\nlisting is $\\Theta(1)$ for node/edge deletions, $\\Theta(n^{1/2})$ for edge\ninsertions, and $\\Theta(n)$ for node insertions.\n  \\item The bandwidth complexity of $1$-round dynamic triangle membership\ndetection is $\\Theta(1)$ for node/edge deletions, $O(\\log n)$ for edge\ninsertions, and $\\Theta(n)$ for node insertions.\n  \\end{itemize}\n  Most of our upper and lower bounds are \\emph{tight}. Additionally, we provide\nalmost always tight upper and lower bounds for larger cliques.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:32:21 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Bonne", "Matthias", ""], ["Censor-Hillel", "Keren", ""]]}, {"id": "1904.11498", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan, Xiufeng Liu, Syed Arshad Ali, Mansaf Alam", "title": "Storage Solutions for Big Data Systems: A Qualitative Study and\n  Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data systems development is full of challenges in view of the variety of\napplication areas and domains that this technology promises to serve.\nTypically, fundamental design decisions involved in big data systems design\ninclude choosing appropriate storage and computing infrastructures. In this age\nof heterogeneous systems that integrate different technologies for optimized\nsolution to a specific real world problem, big data system are not an exception\nto any such rule. As far as the storage aspect of any big data system is\nconcerned, the primary facet in this regard is a storage infrastructure and\nNoSQL seems to be the right technology that fulfills its requirements. However,\nevery big data application has variable data characteristics and thus, the\ncorresponding data fits into a different data model. This paper presents\nfeature and use case analysis and comparison of the four main data models\nnamely document oriented, key value, graph and wide column. Moreover, a feature\nanalysis of 80 NoSQL solutions has been provided, elaborating on the criteria\nand points that a developer must consider while making a possible choice.\nTypically, big data storage needs to communicate with the execution engine and\nother processing and visualization technologies to create a comprehensive\nsolution. This brings forth second facet of big data storage, big data file\nformats, into picture. The second half of the research paper compares the\nadvantages, shortcomings and possible use cases of available big data file\nformats for Hadoop, which is the foundation for most big data computing\ntechnologies. Decentralized storage and blockchain are seen as the next\ngeneration of big data storage and its challenges and future prospects have\nalso been discussed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 08:31:50 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Khan", "Samiya", ""], ["Liu", "Xiufeng", ""], ["Ali", "Syed Arshad", ""], ["Alam", "Mansaf", ""]]}, {"id": "1904.11545", "submitter": "Christian G\\\"ottel", "authors": "Christian G\\\"ottel, Pascal Felber, Valerio Schiavoni", "title": "Developing Secure Services for IoT with OP-TEE: A First Look at\n  Performance and Usability", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": "In: Pereira J., Ricci L. (eds) DAIS 2019. Lecture Notes in\n  Computer Science, vol 11534. Springer, Cham", "doi": "10.1007/978-3-030-22496-7_11", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation, deployment and testing of secure services for Internet of\nThings devices is nowadays still at an early stage. Several frameworks have\nrecently emerged to help developers realize such services, abstracting the\ncomplexity of the many types of underlying hardware platforms and software\nlibraries. Assessing the performance and usability of a given framework remains\nchallenging, as they are largely influenced by the application and workload\nconsidered, as well as the target hardware. Since 15 years, ARM processors are\nproviding support for TrustZone, a set of security instructions that realize a\ntrusted execution environment inside the processor. OP-TEE is a free-software\nframework to implement trusted applications and services for TrustZone. In this\nshort paper we show how one can leverage OP-TEE for implementing a secure\nservice (i.e., a key-value store). We deploy and evaluate the performance of\nthis trusted service on common Raspberry Pi hardware platforms. We report our\nexperimental results with the data store and also compare it against OP-TEE's\nbuilt-in secure storage.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 19:17:39 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 15:36:17 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["G\u00f6ttel", "Christian", ""], ["Felber", "Pascal", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1904.11563", "submitter": "Suayb Arslan", "authors": "Suayb S. Arslan", "title": "Array BP-XOR Codes for Parallel Matrix Multiplication using Hierarchical\n  Computing", "comments": "9 pages, 5 figures, 4 tables. Accepted to International Conference on\n  Information Theory (ISIT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a novel coded computation technique for parallel\nmatrix-matrix product computation using hierarchical compute architectures that\noutperforms well known previous strategies in terms of total end-to-end\nexecution time.The proposed method uses array codes to achieve this performance\nby distributing the encoding operation over the cluster(slave) nodes at the\nexpense of increased master-slave communication. The matrix multiplication is\nperformed using MDS array Belief Propagation (BP)-decodable codes based on pure\nXOR operations. The proposed scheme is shown to be configurable and suited for\nmodern hierarchical compute architectures equipped with multiple nodes, each\nhaving multiple, independent and less capable processing units. In addition, to\naddress scaling number of stragglers, asymptotic versions of the code is used\nand latency analysis is conducted. We shall demonstrate that the proposed\nscheme achieves order-optimal computation in both the sublinear as well as the\nlinear regime in the size of the computed product from an end-to-end delay\nperspective while ensuring acceptable communication requirements that can be\naddressed by today's high speed network link infrastructures.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 19:59:47 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 16:28:32 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Arslan", "Suayb S.", ""]]}, {"id": "1904.11720", "submitter": "David Goz Dr.", "authors": "Giuliano Taffoni, Giuseppe Murante, Luca Tornatore, David Goz, Stefano\n  Borgani, Manolis Katevenis, Nikolaos Chrysos, and Manolis Marazakis", "title": "Shall numerical astrophysics step into the era of Exascale computing?", "comments": "3 figures, invited talk for proceedings of ADASS XXVI, accepted by\n  ASP Conference Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance computing numerical simulations are today one of the more\neffective instruments to implement and study new theoretical models, and they\nare mandatory during the preparatory phase and operational phase of any\nscientific experiment. New challenges in Cosmology and Astrophysics will\nrequire a large number of new extremely computationally intensive simulations\nto investigate physical processes at different scales. Moreover, the size and\ncomplexity of the new generation of observational facilities also implies a new\ngeneration of high performance data reduction and analysis tools pushing toward\nthe use of Exascale computing capabilities. Exascale supercomputers cannot be\nproduced today. We discuss the major technological challenges in the design,\ndevelopment and use of such computing capabilities and we will report on the\nprogresses that has been made in the last years in Europe, in particular in the\nframework of the ExaNeSt European funded project. We also discuss the impact of\nthis new computing resources on the numerical codes in Astronomy and\nAstrophysics.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 08:56:04 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Taffoni", "Giuliano", ""], ["Murante", "Giuseppe", ""], ["Tornatore", "Luca", ""], ["Goz", "David", ""], ["Borgani", "Stefano", ""], ["Katevenis", "Manolis", ""], ["Chrysos", "Nikolaos", ""], ["Marazakis", "Manolis", ""]]}, {"id": "1904.11812", "submitter": "George K. Thiruvathukal", "authors": "George K. Thiruvathukal and Cameron Christensen and Xiaoyong Jin and\n  Fran\\c{c}ois Tessier and Venkatram Vishwanath", "title": "A Benchmarking Study to Evaluate Apache Spark on Large-Scale\n  Supercomputers", "comments": "Submitted to IEEE Cloud 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As dataset sizes increase, data analysis tasks in high performance computing\n(HPC) are increasingly dependent on sophisticated dataflows and out-of-core\nmethods for efficient system utilization. In addition, as HPC systems grow,\nmemory access and data sharing are becoming performance bottlenecks. Cloud\ncomputing employs a data processing paradigm typically built on a loosely\nconnected group of low-cost computing nodes without relying upon shared storage\nand/or memory. Apache Spark is a popular engine for large-scale data analysis\nin the cloud, which we have successfully deployed via job submission scripts on\nproduction clusters.\n  In this paper, we describe common parallel analysis dataflows for both\nMessage Passing Interface (MPI) and cloud based applications. We developed an\neffective benchmark to measure the performance characteristics of these tasks\nusing both types of systems, specifically comparing MPI/C-based analyses with\nSpark. The benchmark is a data processing pipeline representative of a typical\nanalytics framework implemented using map-reduce. In the case of Spark, we also\nconsider whether language plays a role by writing tests using both Python and\nScala, a language built on the Java Virtual Machine (JVM). We include\nperformance results from two large systems at Argonne National Laboratory\nincluding Theta, a Cray XC40 supercomputer on which our experiments run with\n65,536 cores (1024 nodes with 64 cores each). The results of our experiments\nare discussed in the context of their applicability to future HPC\narchitectures. Beyond understanding performance, our work demonstrates that\ntechnologies such as Spark, while typically aimed at multi-tenant cloud-based\nenvironments, show promise for data analysis needs in a traditional\nclustering/supercomputing environment.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 12:52:02 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 22:26:39 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Thiruvathukal", "George K.", ""], ["Christensen", "Cameron", ""], ["Jin", "Xiaoyong", ""], ["Tessier", "Fran\u00e7ois", ""], ["Vishwanath", "Venkatram", ""]]}, {"id": "1904.12043", "submitter": "Haibin Lin", "authors": "Haibin Lin, Hang Zhang, Yifei Ma, Tong He, Zhi Zhang, Sheng Zha, Mu Li", "title": "Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the\n  Limbo of Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increasing demand for training powers for deep learning algorithms\nand the rapid growth of computation resources in data centers, it is desirable\nto dynamically schedule different distributed deep learning tasks to maximize\nresource utilization and reduce cost. In this process, different tasks may\nreceive varying numbers of machines at different time, a setting we call\nelastic distributed training. Despite the recent successes in large mini-batch\ndistributed training, these methods are rarely tested in elastic distributed\ntraining environments and suffer degraded performance in our experiments, when\nwe adjust the learning rate linearly immediately with respect to the batch\nsize. One difficulty we observe is that the noise in the stochastic momentum\nestimation is accumulated over time and will have delayed effects when the\nbatch size changes. We therefore propose to smoothly adjust the learning rate\nover time to alleviate the influence of the noisy momentum estimation. Our\nexperiments on image classification, object detection and semantic segmentation\nhave demonstrated that our proposed Dynamic SGD method achieves stabilized\nperformance when varying the number of GPUs from 8 to 128. We also provide\ntheoretical understanding on the optimality of linear learning rate scheduling\nand the effects of stochastic momentum.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 20:45:28 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 06:48:24 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Lin", "Haibin", ""], ["Zhang", "Hang", ""], ["Ma", "Yifei", ""], ["He", "Tong", ""], ["Zhang", "Zhi", ""], ["Zha", "Sheng", ""], ["Li", "Mu", ""]]}, {"id": "1904.12059", "submitter": "John Collomosse", "authors": "Tu Bui, Daniel Cooper, John Collomosse, Mark Bell, Alex Green, John\n  Sheridan, Jez Higgins, Arindra Das, Jared Keller, Olivier Thereaux, Alan\n  Brown", "title": "ARCHANGEL: Tamper-proofing Video Archives using Temporal Content Hashes\n  on the Blockchain", "comments": "Accepted to CVPR Blockchain Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ARCHANGEL; a novel distributed ledger based system for assuring\nthe long-term integrity of digital video archives. First, we describe a novel\ndeep network architecture for computing compact temporal content hashes (TCHs)\nfrom audio-visual streams with durations of minutes or hours. Our TCHs are\nsensitive to accidental or malicious content modification (tampering) but\ninvariant to the codec used to encode the video. This is necessary due to the\ncuratorial requirement for archives to format shift video over time to ensure\nfuture accessibility. Second, we describe how the TCHs (and the models used to\nderive them) are secured via a proof-of-authority blockchain distributed across\nmultiple independent archives. We report on the efficacy of ARCHANGEL within\nthe context of a trial deployment in which the national government archives of\nthe United Kingdom, Estonia and Norway participated.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:59:02 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bui", "Tu", ""], ["Cooper", "Daniel", ""], ["Collomosse", "John", ""], ["Bell", "Mark", ""], ["Green", "Alex", ""], ["Sheridan", "John", ""], ["Higgins", "Jez", ""], ["Das", "Arindra", ""], ["Keller", "Jared", ""], ["Thereaux", "Olivier", ""], ["Brown", "Alan", ""]]}, {"id": "1904.12068", "submitter": "Kai Li", "authors": "Yuzhe Tang, Ju Chen, Kai Li, Jianliang Xu, Qi Zhang", "title": "Authenticated Key-Value Stores with Hardware Enclaves", "comments": "eLSM, Enclave, key-value store, ADS, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authenticated data storage on an untrusted platform is an important computing\nparadigm for cloud applications ranging from big-data outsourcing, to\ncryptocurrency and certificate transparency log. These modern applications\nincreasingly feature update-intensive workloads, whereas existing authenticated\ndata structures (ADSs) designed with in-place updates are inefficient to handle\nsuch workloads. In this paper, we address this issue and propose a novel\nauthenticated log-structured merge tree (eLSM) based key-value store by\nleveraging Intel SGX enclaves.\n  We present a system design that runs the code of eLSM store inside enclave.\nTo circumvent the limited enclave memory (128 MB with the latest Intel CPUs),\nwe propose to place the memory buffer of the eLSM store outside the enclave and\nprotect the buffer using a new authenticated data structure by digesting\nindividual LSM-tree levels. We design protocols to support query authentication\nin data integrity, completeness (under range queries), and freshness. The proof\nin our protocol is made small by including only the Merkle proofs at selective\nlevels.\n  We implement eLSM on top of Google LevelDB and Facebook RocksDB with minimal\ncode change and performance interference. We evaluate the performance of eLSM\nunder the YCSB workload benchmark and show a performance advantage of up to\n4.5X speedup.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 22:45:04 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 19:28:39 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 23:38:02 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Tang", "Yuzhe", ""], ["Chen", "Ju", ""], ["Li", "Kai", ""], ["Xu", "Jianliang", ""], ["Zhang", "Qi", ""]]}, {"id": "1904.12210", "submitter": "Aditya Saligrama", "authors": "Aditya Saligrama, Andrew Shen, Jon Gjengset", "title": "A Practical Analysis of Rust's Concurrency Story", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correct concurrent programs are difficult to write; when multiple threads\nmutate shared data, they may lose writes, corrupt data, or produce erratic\nprogram behavior. While many of the data-race issues with concurrency can be\navoided by the placing of locks throughout the code, these often serialize\nprogram execution, and can significantly slow down performance-critical\napplications. Programmers also make mistakes, and often forget locks in\nless-executed code paths, which leads to programs that misbehave only in rare\nsituations.\n  Rust is a recent programming language from Mozilla that attempts to solve\nthese intertwined issues by detecting data-races at compile time. Rust's type\nsystem encodes a data-structure's ability to be shared between threads in the\ntype system, which in turn allows the compiler to reject programs where threads\ndirectly mutate shared state without locks or other protection mechanisms. In\nthis work, we examine how this aspect of Rust's type system impacts the\ndevelopment and refinement of a concurrent data structure, as well as its\nability to adapt to situations where correctness is guaranteed by lower-level\ninvariants (e.g., in lock-free algorithms) that are not directly expressible in\nthe type system itself. We detail the implementation of a concurrent lock-free\nhashmap in order to describe these traits of the Rust language. Our code is\npublicly available at https://github.com/saligrama/concache and is one of the\nfastest concurrent hashmaps for the Rust language, which leads to mitigating\nbottlenecks in concurrent programs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:37:20 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Saligrama", "Aditya", ""], ["Shen", "Andrew", ""], ["Gjengset", "Jon", ""]]}, {"id": "1904.12222", "submitter": "Hema Venkata Krishna Giri Narra", "authors": "Krishna Giri Narra, Zhifeng Lin, Ganesh Ananthanarayanan, Salman\n  Avestimehr, Murali Annavaram", "title": "Collage Inference: Using Coded Redundancy for Low Variance Distributed\n  Image Classification", "comments": "10 pages, Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLaaS (ML-as-a-Service) offerings by cloud computing platforms are becoming\nincreasingly popular. Hosting pre-trained machine learning models in the cloud\nenables elastic scalability as the demand grows. But providing low latency and\nreducing the latency variance is a key requirement. Variance is harder to\ncontrol in a cloud deployment due to uncertainties in resource allocations\nacross many virtual instances. We propose the collage inference technique which\nuses a novel convolutional neural network model, collage-cnn, to provide\nlow-cost redundancy. A collage-cnn model takes a collage image formed by\ncombining multiple images and performs multi-image classification in one shot,\nalbeit at slightly lower accuracy. We augment a collection of traditional\nsingle image classifier models with a single collage-cnn classifier which acts\nas their low-cost redundant backup. Collage-cnn provides backup classification\nresults if any single image classification requests experience slowdown.\nDeploying the collage-cnn models in the cloud, we demonstrate that the 99th\npercentile tail latency of inference can be reduced by 1.2x to 2x compared to\nreplication based approaches while providing high accuracy. Variation in\ninference latency can be reduced by 1.8x to 15x.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 22:56:10 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 17:25:42 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Narra", "Krishna Giri", ""], ["Lin", "Zhifeng", ""], ["Ananthanarayanan", "Ganesh", ""], ["Avestimehr", "Salman", ""], ["Annavaram", "Murali", ""]]}, {"id": "1904.12247", "submitter": "Danda Rawat", "authors": "Danda B. Rawat, Vijay Chaudhary and Ronald Doku", "title": "Blockchain: Emerging Applications and Use Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain also known as a distributed ledger technology stores different\ntransactions/operations in a chain of blocks in a distributed manner without\nneeding a trusted third-party. Blockchain is proven to be immutable which helps\nfor integrity and accountability, and, to some extent, confidentiality through\na pair of public and private keys. Blockchain has been in the spotlight after\nsuccessful boom of the Bitcoin. There have been efforts to leverage salient\nfeatures of Blockchain for different applications and use cases. This paper\npresent a comprehensive survey of applications and use cases of Blockchain\ntechnology. Specifically, readers of this paper can have thorough understanding\nof applications and user cases of Blockchain technology.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 03:02:43 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Rawat", "Danda B.", ""], ["Chaudhary", "Vijay", ""], ["Doku", "Ronald", ""]]}, {"id": "1904.12591", "submitter": "Cameron Musco", "authors": "Nancy Lynch and Cameron Musco and Merav Parter", "title": "Winner-Take-All Computation in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cond-mat.dis-nn cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study biological neural networks from an algorithmic\nperspective, focusing on understanding tradeoffs between computation time and\nnetwork complexity. Our goal is to abstract real neural networks in a way that,\nwhile not capturing all interesting features, preserves high-level behavior and\nallows us to make biologically relevant conclusions. Towards this goal, we\nconsider the implementation of algorithmic primitives in a simple yet\nbiologically plausible model of $stochastic\\ spiking\\ neural\\ networks$. In\nparticular, we show how the stochastic behavior of neurons in this model can be\nleveraged to solve a basic $symmetry-breaking\\ task$ in which we are given\nneurons with identical firing rates and want to select a distinguished one. In\ncomputational neuroscience, this is known as the winner-take-all (WTA) problem,\nand it is believed to serve as a basic building block in many tasks, e.g.,\nlearning, pattern recognition, and clustering. We provide efficient\nconstructions of WTA circuits in our stochastic spiking neural network model,\nas well as lower bounds in terms of the number of auxiliary neurons required to\ndrive convergence to WTA in a given number of steps. These lower bounds\ndemonstrate that our constructions are near-optimal in some cases.\n  This work covers and gives more in-depth proofs of a subset of results\noriginally published in [LMP17a]. It is adapted from the last chapter of C.\nMusco's Ph.D. thesis [Mus18].\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 19:49:40 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Lynch", "Nancy", ""], ["Musco", "Cameron", ""], ["Parter", "Merav", ""]]}, {"id": "1904.12595", "submitter": "Rohan Garg", "authors": "Rohan Garg, Gregory Price, Gene Cooperman", "title": "MANA for MPI: MPI-Agnostic Network-Agnostic Transparent Checkpointing", "comments": "24 pages; 9 figures; accepted at HPDC-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparently checkpointing MPI for fault tolerance and load balancing is a\nlong-standing problem in HPC. The problem has been complicated by the need to\nprovide checkpoint-restart services for all combinations of an MPI\nimplementation over all network interconnects. This work presents MANA\n(MPI-Agnostic Network-Agnostic transparent checkpointing), a single code base\nwhich supports all MPI implementation and interconnect combinations. The\nagnostic properties imply that one can checkpoint an MPI application under one\nMPI implementation and perhaps over TCP, and then restart under a second MPI\nimplementation over InfiniBand on a cluster with a different number of CPU\ncores per node. This technique is based on a novel \"split-process\" approach,\nwhich enables two separate programs to co-exist within a single process with a\nsingle address space. This work overcomes the limitations of the two most\nwidely adopted transparent checkpointing solutions, BLCR and DMTCP/InfiniBand,\nwhich require separate modifications to each MPI implementation and/or\nunderlying network API. The runtime overhead is found to be insignificant both\nfor checkpoint-restart within a single host, and when comparing a local MPI\ncomputation that was migrated to a remote cluster against an ordinary MPI\ncomputation running natively on that same remote cluster.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 16:11:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Garg", "Rohan", ""], ["Price", "Gregory", ""], ["Cooperman", "Gene", ""]]}, {"id": "1904.12609", "submitter": "Issam Damaj", "authors": "Issam Damaj, Suhaib Majzoub, Hassan Diab (American University of\n  Beirut)", "title": "2D and 3D Computer Graphics Algorithms under MorphoSys", "comments": "4 Pages, 2 Tables. arXiv admin note: text overlap with\n  arXiv:1904.08233", "journal-ref": "LLCS. 2438(2002) 493-512", "doi": "10.1007/3-540-46117-5_111", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new mappings of 2D and 3D geometrical transformation on\nthe MorphoSys (M1) reconfigurable computing (RC) prototype [2]. This improves\nthe system performance as a graphics accelerator [1-5]. Three algorithms are\nmapped including two for calculating 2D transformations, and one for 3D\ntransformations. The results presented indicate an improved performance. The\nspeedup achieved is explained as well as the advantages in the mapping of the\napplication. The transformations on an 8x8 RC array were run, and numerical\nexamples were simulated to validate our results, using the MorphoSys mULATE\nprogram, which simulates MorphoSys operations. Comparisons with other systems\nare presented, namely, with Intel processing systems and Celoxica RC-1000 FPGA.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 00:11:52 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Damaj", "Issam", "", "American University of\n  Beirut"], ["Majzoub", "Suhaib", "", "American University of\n  Beirut"], ["Diab", "Hassan", "", "American University of\n  Beirut"]]}, {"id": "1904.12636", "submitter": "Amrith Kumar", "authors": "Amrith Kumar, Kenneth Rugg", "title": "Brewers Conjecture and a characterization of the limits, and\n  relationships between Consistency, Availability and Partition Tolerance in a\n  distributed service", "comments": "This paper was originally published in October 2011 on the ParElastic\n  blog that can now be found archived on the Internet Wayback machine at\n  http://bit.ly/2Xv5kh4. The PDF document that was part of that six part blog\n  post is at http://bit.ly/2GBJ8fC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In designing a distributed service, three desirable attributes are\nConsistency, Availability and Partition Tolerance. In this note we explore a\nframework for characterizing these three in a manner that establishes definite\nlimits and relationships between them, and explore some implications of this\ncharacterization.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 10:01:16 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kumar", "Amrith", ""], ["Rugg", "Kenneth", ""]]}, {"id": "1904.12661", "submitter": "Zhen Lin", "authors": "Zhen Lin, Mohammad Alshboul, Yan Solihin, and Huiyang Zhou", "title": "Exploring Memory Persistency Models for GPUs", "comments": "18 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given its high integration density, high speed, byte addressability, and low\nstandby power, non-volatile or persistent memory is expected to\nsupplement/replace DRAM as main memory. Through persistency programming models\n(which define durability ordering of stores) and durable transaction\nconstructs, the programmer can provide recoverable data structure (RDS) which\nallows programs to recover to a consistent state after a failure. While\npersistency models have been well studied for CPUs, they have been neglected\nfor graphics processing units (GPUs). Considering the importance of GPUs as a\ndominant accelerator for high performance computing, we investigate persistency\nmodels for GPUs.\n  GPU applications exhibit substantial differences with CPUs applications,\nhence in this paper we adapt, re-architect, and optimize CPU persistency models\nfor GPUs. We design a pragma-based compiler scheme to express persistency\nmodels for GPUs. We identify that the thread hierarchy in GPUs offers intuitive\nscopes to form epochs and durable transactions. We find that undo logging\nproduces significant performance overheads. We propose to use idempotency\nanalysis to reduce both logging frequency and the size of logs. Through both\nreal-system and simulation evaluations, we show low overheads of our proposed\narchitecture support.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:20:07 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Lin", "Zhen", ""], ["Alshboul", "Mohammad", ""], ["Solihin", "Yan", ""], ["Zhou", "Huiyang", ""]]}, {"id": "1904.12666", "submitter": "Runyu Zhang", "authors": "Runyu Zhang and Chaoshu Yang", "title": "Reconstruct the Directories for In-Memory File Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing path lookup routines in file systems need to construct an auxiliary\nindex in memory or traverse the dentries of the directory file sequentially,\nwhich brings either heavy writes or large timing cost. This paper designs a\nnovel path lookup mechanism, Content-Indexed Browsing (CIB), for file systems\non persistent memory, in which the structure of directory files is an exclusive\nindex that can be searched in $O(log(n))$ time. We implement CIB in a real\npersistent memory file system, PMFS, denoted by CIB-PMFS. Comprehensive\nevaluations show that CIB can achieve times of performance improvement over the\nconventional lookup schemes in PMFS, and brings 20.4\\% improvement on the\noverall performance of PMFS. Furthermore, CIB reduces the writes on persistent\nmemory by orders of magnitude comparing with existing extra index schemes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:54:20 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 07:38:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Zhang", "Runyu", ""], ["Yang", "Chaoshu", ""]]}, {"id": "1904.12676", "submitter": "Mauricio Fadel Argerich", "authors": "M. Fadel Argerich, B. Cheng, J. F\\\"urst", "title": "Reinforcement Learning Based Orchestration for Elastic Services", "comments": "2019 IEEE 5th World Forum on Internet of Things (WF-IoT), 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the highly variable execution context in which edge services run,\nadapting their behavior to the execution context is crucial to comply with\ntheir requirements. However, adapting service behavior is a challenging task\nbecause it is hard to anticipate the execution contexts in which it will be\ndeployed, as well as assessing the impact that each behavior change will\nproduce. In order to provide this adaptation efficiently, we propose a\nReinforcement Learning (RL) based Orchestration for Elastic Services. We\nimplement and evaluate this approach by adapting an elastic service in\ndifferent simulated execution contexts and comparing its performance to a\nHeuristics based approach. We show that elastic services achieve high precision\nand requirement satisfaction rates while creating an overhead of less than 0.5%\nto the overall service. In particular, the RL approach proves to be more\nefficient than its rule-based counterpart; yielding a 10 to 25% higher\nprecision while being 25% less computationally expensive.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 10:20:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Argerich", "M. Fadel", ""], ["Cheng", "B.", ""], ["F\u00fcrst", "J.", ""]]}, {"id": "1904.12728", "submitter": "Alessio Mazzetto", "authors": "Alessio Mazzetto, Andrea Pietracaprina, Geppino Pucci", "title": "Accurate MapReduce Algorithms for $k$-median and $k$-means in General\n  Metric Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Center-based clustering is a fundamental primitive for data analysis and\nbecomes very challenging for large datasets. In this paper, we focus on the\npopular $k$-median and $k$-means variants which, given a set $P$ of points from\na metric space and a parameter $k<|P|$, require to identify a set $S$ of $k$\ncenters minimizing, respectively, the sum of the distances and of the squared\ndistances of all points in $P$ from their closest centers. Our specific focus\nis on general metric spaces, for which it is reasonable to require that the\ncenters belong to the input set (i.e., $S \\subseteq P$). We present\ncoreset-based 3-round distributed approximation algorithms for the above\nproblems using the MapReduce computational model. The algorithms are rather\nsimple and obliviously adapt to the intrinsic complexity of the dataset,\ncaptured by the doubling dimension $D$ of the metric space. Remarkably, the\nalgorithms attain approximation ratios that can be made arbitrarily close to\nthose achievable by the best known polynomial-time sequential approximations,\nand they are very space efficient for small $D$, requiring local memory sizes\nsubstantially sublinear in the input size. To the best of our knowledge, no\nprevious distributed approaches were able to attain similar quality-performance\nguarantees in general metric spaces.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:12:15 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 19:58:38 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Mazzetto", "Alessio", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""]]}, {"id": "1904.12924", "submitter": "Tarun Chitra", "authors": "Tarun Chitra, Monica Quaintance, Stuart Haber, Will Martino", "title": "Agent-Based Simulations of Blockchain protocols illustrated via Kadena's\n  Chainweb", "comments": "10 pages, 7 figures, accepted to the IEEE S&B 2019 conference", "journal-ref": null, "doi": "10.1109/EuroSPW.2019.00049", "report-no": null, "categories": "cs.CR cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many distributed consensus protocols provide robust liveness and\nconsistency guarantees under the presence of malicious actors, quantitative\nestimates of how economic incentives affect security are few and far between.\nIn this paper, we describe a system for simulating how adversarial agents, both\neconomically rational and Byzantine, interact with a blockchain protocol. This\nsystem provides statistical estimates for the economic difficulty of an attack\nand how the presence of certain actors influences protocol-level statistics,\nsuch as the expected time to regain liveness. This simulation system is\ninfluenced by the design of algorithmic trading and reinforcement learning\nsystems that use explicit modeling of an agent's reward mechanism to evaluate\nand optimize a fully autonomous agent. We implement and apply this simulation\nframework to Kadena's Chainweb, a parallelized Proof-of-Work system, that\ncontains complexity in how miner incentive compliance affects security and\ncensorship resistance. We provide the first formal description of Chainweb that\nis in the literature and use this formal description to motivate our simulation\ndesign. Our simulation results include a phase transition in block height\ngrowth rate as a function of shard connectivity and empirical evidence that\ncensorship in Chainweb is too costly for rational miners to engage in. We\nconclude with an outlook on how simulation can guide and optimize protocol\ndevelopment in a variety of contexts, including Proof-of-Stake parameter\noptimization and peer-to-peer networking design.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 19:48:20 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Chitra", "Tarun", ""], ["Quaintance", "Monica", ""], ["Haber", "Stuart", ""], ["Martino", "Will", ""]]}, {"id": "1904.12974", "submitter": "Fabrizio Romano Genovese PhD", "authors": "Fabrizio Genovese, Alex Gryzlov, Jelle Herold, Marco Perone, Erik\n  Post, Andr\\'e Videla", "title": "Computational Petri Nets: Adjunctions Considered Harmful", "comments": "29 Pages, 6 Figures, 1 Listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some of the endeavors in trying to connect Petri nets with free\nsymmetric monoidal categories. We give a list of requirement such connections\nshould respect if they are meant to be useful for practical/implementation\npurposes. We show how previous approaches do not satisfy them, and give\ncompelling evidence that this depends on trying to make the correspondence\nfunctorial in the direction from nets to free symmetric monoidal categories, in\norder to produce an adjunction. We show that dropping this immediately honors\nour desiderata, and conclude by introducing an Idris library which implements\nthem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 22:17:09 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 20:06:14 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Genovese", "Fabrizio", ""], ["Gryzlov", "Alex", ""], ["Herold", "Jelle", ""], ["Perone", "Marco", ""], ["Post", "Erik", ""], ["Videla", "Andr\u00e9", ""]]}, {"id": "1904.13093", "submitter": "Pedro Garcia Lopez", "authors": "Pedro Garcia Lopez, Alberto Montresor, Anwitaman Datta", "title": "Please, do not decentralize the Internet with (permissionless)\n  blockchains!", "comments": "11 pages, 1 figure, ICDCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The old mantra of decentralizing the Internet is coming again with fanfare,\nthis time around the blockchain technology hype. We have already seen a\ntechnology supposed to change the nature of the Internet: peer-to-peer. The\nreality is that peer-to-peer naming systems failed, peer-to-peer social\nnetworks failed, and yes, peer-to-peer storage failed as well. In this paper,\nwe will review the research on distributed systems in the last few years to\nidentify the limits of open peer-to-peer networks. We will address issues like\nsystem complexity, security and frailty, instability and performance. We will\nshow how many of the aforementioned problems also apply to the recent breed of\npermissionless blockchain networks. The applicability of such systems to mature\nindustrial applications is undermined by the same properties that make them so\ninteresting for a libertarian audience: namely, their openness, their\npseudo-anonymity and their unregulated cryptocurrencies. As such, we argue that\npermissionless blockchain networks are unsuitable to be the substrate for a\ndecentralized Internet. Yet, there is still hope for more decentralization,\nalbeit in a form somewhat limited with respect to the libertarian view of\ndecentralized Internet: in cooperation rather than in competition with the\nsuperpowerful datacenters that dominate the world today. This is derived from\nthe recent surge in interest in byzantine fault tolerance and permissioned\nblockchains, which opens the door to a world where use of trusted third parties\nis not the only way to arbitrate an ensemble of entities. The ability of\nestablish trust through permissioned blockchains enables to move the control\nfrom the datacenters to the edge, truly realizing the promises of edge-centric\ncomputing.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:59:05 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Lopez", "Pedro Garcia", ""], ["Montresor", "Alberto", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1904.13105", "submitter": "Mohamed Alrshah", "authors": "Mohamed A. Alrshah, Mohamed A. Al-Maqri, Mohamed Othman", "title": "Elastic-TCP: Flexible Congestion Control Algorithm to Adapt for High-BDP\n  Networks", "comments": "11 pages", "journal-ref": null, "doi": "10.1109/JSYST.2019.2896195", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, the demand for Internet applications has been increased,\nwhich increases the number of data centers across the world. These data centers\nare usually connected to each other using long-distance and high-speed\nnetworks. As known, the Transmission Control Protocol (TCP) is the predominant\nprotocol used to provide such connectivity among these data centers.\nUnfortunately, the huge bandwidth-delay product (BDP) of these networks hinders\nTCP from achieving full bandwidth utilization. In order to increase TCP\nflexibility to adapt for high-BDP networks, we propose a new delay-based and\nRTT-independent congestion control algorithm (CCA), namely Elastic-TCP. It\nmainly contributes the novel window-correlated weighting function (WWF) to\nincrease TCP bandwidth utilization over high-BDP networks. Extensive simulation\nand testbed experiments have been carried out to evaluate the proposed\nElastic-TCP by comparing its performance to the commonly used TCPs developed by\nMicrosoft, Linux, and Google. The results show that the proposed Elastic-TCP\nachieves higher average throughput than the other TCPs, while it maintains the\nsharing fairness and the loss ratio. Moreover, it is worth noting that the new\nElastic-TCP presents lower sensitivity to the variation of buffer size and\npacket error rate than the other TCPs, which grants high efficiency and\nstability.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 08:51:00 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Alrshah", "Mohamed A.", ""], ["Al-Maqri", "Mohamed A.", ""], ["Othman", "Mohamed", ""]]}, {"id": "1904.13206", "submitter": "Qian Yu", "authors": "Qian Yu and A. Salman Avestimehr", "title": "Harmonic Coding: An Optimal Linear Code for Privacy-Preserving\n  Gradient-Type Computation", "comments": "To appear in ISIT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributedly computing a general class of\nfunctions, referred to as gradient-type computation, while maintaining the\nprivacy of the input dataset. Gradient-type computation evaluates the sum of\nsome `partial gradients', defined as polynomials of subsets of the input. It\nunderlies many algorithms in machine learning and data analytics. We propose\nHarmonic Coding, which universally computes any gradient-type function, while\nrequiring the minimum possible number of workers. Harmonic Coding strictly\nimproves computing schemes developed based on prior works, such as Shamir's\nsecret sharing and Lagrange Coded Computing, by injecting coded redundancy\nusing harmonic progression. It enables the computing results of the workers to\nbe interpreted as the sum of partial gradients and some redundant results,\nwhich then allows the cancellation of non-gradient terms in the decoding\nprocess. By proving a matching converse, we demonstrate the optimality of\nHarmonic Coding, even compared to the schemes that are non-universal (i.e., can\nbe designed based on a specific gradient-type function).\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:15:12 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Yu", "Qian", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1904.13302", "submitter": "Minjeong Kim", "authors": "Minjeong Kim, Yujin Kwon, and Yongdae Kim", "title": "Is Stellar As Secure As You Think?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stellar is one of the top ten cryptocurrencies in terms of market\ncapitalization. It adopts a variant of Byzantine fault tolerance (BFT), named\nfederated Byzantine agreement (FBA), which generalizes the traditional BFT\nalgorithm to make it more suitable for open-membership blockchains. To this\nend, FBA introduces a quorum slice concept, which consists of a set of nodes.\nIn FBA, a node can complete one consensus round when it receives specific\nmessages from nodes in a quorum slice appointed by the node. In this study, we\nanalyze FBA, whose security is highly dependent on the structure of quorum\nslices, and demonstrate that it is not superior to the traditional BFT\nalgorithm in terms of safety and liveness. Then, to analyze the security of the\nStellar consensus protocol (SCP), which is a construction for FBA, we\ninvestigate the current quorum slices in Stellar. We analyze the structure of\nquorum slices and measure the influence of each node quantitatively using two\nmetrics, PageRank (PR) and the newly proposed NodeRank (NR). The results show\nthat the Stellar system is significantly centralized. Thereafter, to determine\nhow the centralized structure can have a negative impact on the Stellar system,\nwe study the cascading failure caused by deleting only a few nodes (i.e.,\nvalidators) in Stellar. We show that all of the nodes in Stellar cannot run SCP\nif only two nodes fail. To make matters worse, these two nodes are run and\ncontrolled by a single organization, the Stellar foundation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 06:29:34 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kim", "Minjeong", ""], ["Kwon", "Yujin", ""], ["Kim", "Yongdae", ""]]}, {"id": "1904.13373", "submitter": "Swanand Kadhe", "authors": "Swanand Kadhe, O. Ozan Koyluoglu, Kannan Ramchandran", "title": "Gradient Coding Based on Block Designs for Mitigating Adversarial\n  Stragglers", "comments": "Shorter version accepted in 2019 IEEE International Symposium on\n  Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed implementations of gradient-based methods, wherein a server\ndistributes gradient computations across worker machines, suffer from slow\nrunning machines, called 'stragglers'. Gradient coding is a coding-theoretic\nframework to mitigate stragglers by enabling the server to recover the gradient\nsum in the presence of stragglers. 'Approximate gradient codes' are variants of\ngradient codes that reduce computation and storage overhead per worker by\nallowing the server to approximately reconstruct the gradient sum.\n  In this work, our goal is to construct approximate gradient codes that are\nresilient to stragglers selected by a computationally unbounded adversary. Our\nmotivation for constructing codes to mitigate adversarial stragglers stems from\nthe challenge of tackling stragglers in massive-scale elastic and serverless\nsystems, wherein it is difficult to statistically model stragglers. Towards\nthis end, we propose a class of approximate gradient codes based on balanced\nincomplete block designs (BIBDs). We show that the approximation error for\nthese codes depends only on the number of stragglers, and thus, adversarial\nstraggler selection has no advantage over random selection. In addition, the\nproposed codes admit computationally efficient decoding at the server. Next, to\ncharacterize fundamental limits of adversarial straggling, we consider the\nnotion of 'adversarial threshold' -- the smallest number of workers that an\nadversary must straggle to inflict certain approximation error. We compute a\nlower bound on the adversarial threshold, and show that codes based on\nsymmetric BIBDs maximize this lower bound among a wide class of codes, making\nthem excellent candidates for mitigating adversarial stragglers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:13:32 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kadhe", "Swanand", ""], ["Koyluoglu", "O. Ozan", ""], ["Ramchandran", "Kannan", ""]]}]