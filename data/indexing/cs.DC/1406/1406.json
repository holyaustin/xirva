[{"id": "1406.0089", "submitter": "Tobin Isaac", "authors": "Tobin Isaac, Carsten Burstedde, Lucas C. Wilcox, Omar Ghattas", "title": "Recursive Algorithms for Distributed Forests of Octrees", "comments": "35 pages, 15 figures, 3 tables", "journal-ref": null, "doi": "10.1137/140970963", "report-no": null, "categories": "cs.DC cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forest-of-octrees approach to parallel adaptive mesh refinement and\ncoarsening (AMR) has recently been demonstrated in the context of a number of\nlarge-scale PDE-based applications. Although linear octrees, which store only\nleaf octants, have an underlying tree structure by definition, it is not often\nexploited in previously published mesh-related algorithms. This is because the\nbranches are not explicitly stored, and because the topological relationships\nin meshes, such as the adjacency between cells, introduce dependencies that do\nnot respect the octree hierarchy. In this work we combine hierarchical and\ntopological relationships between octree branches to design efficient recursive\nalgorithms.\n  We present three important algorithms with recursive implementations. The\nfirst is a parallel search for leaves matching any of a set of multiple search\ncriteria. The second is a ghost layer construction algorithm that handles\narbitrarily refined octrees that are not covered by previous algorithms, which\nrequire a 2:1 condition between neighboring leaves. The third is a universal\nmesh topology iterator. This iterator visits every cell in a domain partition,\nas well as every interface (face, edge and corner) between these cells. The\niterator calculates the local topological information for every interface that\nit visits, taking into account the nonconforming interfaces that increase the\ncomplexity of describing the local topology. To demonstrate the utility of the\ntopology iterator, we use it to compute the numbering and encoding of\nhigher-order $C^0$ nodal basis functions.\n  We analyze the complexity of the new recursive algorithms theoretically, and\nassess their performance, both in terms of single-processor efficiency and in\nterms of parallel scalability, demonstrating good weak and strong scaling up to\n458k cores of the JUQUEEN supercomputer.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 16:02:54 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 21:00:39 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2015 03:26:29 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Isaac", "Tobin", ""], ["Burstedde", "Carsten", ""], ["Wilcox", "Lucas C.", ""], ["Ghattas", "Omar", ""]]}, {"id": "1406.0609", "submitter": "Huanle Xu Mr", "authors": "Huanle Xu and Wing Cheong Lau", "title": "Optimization for Speculative Execution of Multiple Jobs in a\n  MapReduce-like Cluster", "comments": "This paper has been withdrawn due to the simulation part need to be\n  strengthened", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, a computing cluster in a typical data center can easily consist of\nhundreds of thousands of commodity servers, making component/ machine failures\nthe norm rather than exception. A parallel processing job can be delayed\nsubstantially as long as one of its many tasks is being assigned to a failing\nmachine. To tackle this so-called straggler problem, most parallel processing\nframeworks such as MapReduce have adopted various strategies under which the\nsystem may speculatively launch additional copies of the same task if its\nprogress is abnormally slow or simply because extra idling resource is\navailable. In this paper, we focus on the design of speculative execution\nschemes for a parallel processing cluster under different loading conditions.\nFor the lightly loaded case, we analyze and propose two optimization-based\nschemes, namely, the Smart Cloning Algorithm (SCA) which is based on maximizing\nthe job utility and the Straggler Detection Algorithm (SDA) which minimizes the\noverall resource consumption of a job. We also derive the workload threshold\nunder which SCA or SDA should be used for speculative execution. Our simulation\nresults show both SCA and SDA can reduce the job flowtime by nearly 60%\ncomparing to the speculative execution strategy of Microsoft Mantri. For the\nheavily loaded case, we propose the Enhanced Speculative Execution (ESE)\nalgorithm which is an extension of the Microsoft Mantri scheme. We show that\nthe ESE algorithm can beat the Mantri baseline scheme by 18% in terms of job\nflowtime while consuming the same amount of resource.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 07:44:33 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 08:25:47 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 04:13:26 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Xu", "Huanle", ""], ["Lau", "Wing Cheong", ""]]}, {"id": "1406.0641", "submitter": "Cristian Prisacariu", "authors": "Cristian Prisacariu", "title": "Extensions of Configuration Structures", "comments": null, "journal-ref": "Journal of Logical and Algebraic Methods in Programming, 85(6),\n  1201-1233, 2016", "doi": "10.1016/j.jlamp.2015.10.009", "report-no": null, "categories": "cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The present paper defines ST-structures (and an extension of these, called\nSTC-structures). The main purpose is to provide concrete relationships between\nhighly expressive concurrency models coming from two different schools of\nthought: the higher dimensional automata, a \\textit{state-based} approach of\nPratt and van Glabbeek; and the configuration structures and (in)pure event\nstructures, an \\textit{event-based} approach of van Glabbeek and Plotkin. In\nthis respect we make comparative studies of the expressive power of\nST-structures relative to the above models. Moreover, standard notions from\nother concurrency models are defined for ST(C)-structures, like steps and\npaths, bisimilarities, and action refinement, and related results are given.\nThese investigations of ST(C)-structures are intended to provide a better\nunderstanding of the \\textit{state-event duality} described by Pratt, and also\nof the (a)cyclic structures of higher dimensional automata.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 09:33:33 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Prisacariu", "Cristian", ""]]}, {"id": "1406.1066", "submitter": "Stefan Engblom", "authors": "Stefan Engblom and Dimitar Lukarski", "title": "Fast Matlab compatible sparse assembly on multicore computers", "comments": null, "journal-ref": "Parallel Comput. 56:1--17 (2016)", "doi": "10.1016/j.parco.2016.04.001", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and implement in this paper a fast sparse assembly algorithm, the\nfundamental operation which creates a compressed matrix from raw index data.\nSince it is often a quite demanding and sometimes critical operation, it is of\ninterest to design a highly efficient implementation. We show how to do this,\nand moreover, we show how our implementation can be parallelized to utilize the\npower of modern multicore computers. Our freely available code, fully Matlab\ncompatible, achieves about a factor of 5 times in speedup on a typical 6-core\nmachine and 10 times on a dual-socket 16 core machine compared to the built-in\nserial implementation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 15:01:23 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 09:54:33 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2015 10:58:34 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Engblom", "Stefan", ""], ["Lukarski", "Dimitar", ""]]}, {"id": "1406.1133", "submitter": "Jos\\'e Marinho", "authors": "Jos\\'e Marinho, Stefan M. Petters", "title": "Timing Analysis for DAG-based and GFP Scheduled Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern embedded systems have made the transition from single-core to\nmulti-core architectures, providing performance improvement via parallelism\nrather than higher clock frequencies. DAGs are considered among the most\ngeneric task models in the real-time domain and are well suited to exploit this\nparallelism. In this paper we provide a schedulability test using response-time\nanalysis exploiting exploring and bounding the self interference of a DAG task.\nAdditionally we bound the interference a high priority task has on lower\npriority ones.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 18:18:37 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Marinho", "Jos\u00e9", ""], ["Petters", "Stefan M.", ""]]}, {"id": "1406.1215", "submitter": "Maksudul Alam", "authors": "Maksudul Alam and Maleq Khan", "title": "Parallel Algorithms for Generating Random Networks with Given Degree\n  Sequences", "comments": "Accepted in NPC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random networks are widely used for modeling and analyzing complex processes.\nMany mathematical models have been proposed to capture diverse real-world\nnetworks. One of the most important aspects of these models is degree\ndistribution. Chung--Lu (CL) model is a random network model, which can produce\nnetworks with any given arbitrary degree distribution. The complex systems we\ndeal with nowadays are growing larger and more diverse than ever. Generating\nrandom networks with any given degree distribution consisting of billions of\nnodes and edges or more has become a necessity, which requires efficient and\nparallel algorithms. We present an MPI-based distributed memory parallel\nalgorithm for generating massive random networks using CL model, which takes\n$O(\\frac{m+n}{P}+P)$ time with high probability and $O(n)$ space per processor,\nwhere $n$, $m$, and $P$ are the number of nodes, edges and processors,\nrespectively. The time efficiency is achieved by using a novel load-balancing\nalgorithm. Our algorithms scale very well to a large number of processors and\ncan generate massive power--law networks with one billion nodes and $250$\nbillion edges in one minute using $1024$ processors.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 21:29:38 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 03:56:44 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 02:20:40 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Alam", "Maksudul", ""], ["Khan", "Maleq", ""]]}, {"id": "1406.1244", "submitter": "Stephan Holzer", "authors": "Alexandra Hochuli, Stephan Holzer, Roger Wattenhofer", "title": "Distributed Approximation of Minimum Routing Cost Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the NP-hard problem of approximating a Minimum Routing Cost Spanning\nTree in the message passing model with limited bandwidth (CONGEST model). In\nthis problem one tries to find a spanning tree of a graph $G$ over $n$ nodes\nthat minimizes the sum of distances between all pairs of nodes. In the\nconsidered model every node can transmit a different (but short) message to\neach of its neighbors in each synchronous round. We provide a randomized\n$(2+\\epsilon)$-approximation with runtime $O(D+\\frac{\\log n}{\\epsilon})$ for\nunweighted graphs. Here, $D$ is the diameter of $G$. This improves over both,\nthe (expected) approximation factor $O(\\log n)$ and the runtime $O(D\\log^2 n)$\nof the best previously known algorithm.\n  Due to stating our results in a very general way, we also derive an (optimal)\nruntime of $O(D)$ when considering $O(\\log n)$-approximations as done by the\nbest previously known algorithm. In addition we derive a deterministic\n$2$-approximation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 00:08:07 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Hochuli", "Alexandra", ""], ["Holzer", "Stephan", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1406.1923", "submitter": "Michel Paquette", "authors": "Evangelos Kranakis and Michel Paquette", "title": "Broadcasting in Networks of Unknown Topology in the Presence of Swamping", "comments": "A preliminary version of this paper appeared in Proc. 12th\n  International Symposium on Stabilization, Safety, and Security of Distributed\n  Systems (SSS 2010), New York City, USA, September 20-22, Vol 6366, pp.\n  267-281, LNCS Springer, 2010. Full proofs for this paper published 2010:\n  https://scs.carleton.ca/content/communication-networks-spatially-correlated-faults", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of broadcasting in a wireless network\nunder a novel communication model: the {\\em swamping} communication model. In\nthis model, nodes communicate only with those nodes at geometric distance\ngreater than $s$ and at most $r$ from them. Communication between nearby nodes\nunder this model can be very time consuming, as the length of the path between\ntwo nodes within distance $s$ is only bounded above by the diameter $D$, in\nmany cases. For the $n$-node lattice networks, we present algorithms of optimal\ntime complexity, respectively $O(n/r + r/(r-s))$ for the lattice line and\n$O(\\sqrt{n}/r + r/(r-s))$ for the two-dimensional lattice. We also consider\nnetworks of unknown topology of diameter $D$ and of a parameter $g$ ({\\em\ngranularity}). More specifically, we consider networks with $\\gamma$ the\nminimum distance between any two nodes and $g = 1/\\gamma$. We present broadcast\nalgorithms for networks of nodes placed on the line and on the plane with\nrespective time complexities $O(D/l + g^2)$ and $O(Dg/l + g^4)$, where $l \\in\n\\Theta(\\max\\{(1-s),\\gamma\\})$.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 19:19:37 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Kranakis", "Evangelos", ""], ["Paquette", "Michel", ""]]}, {"id": "1406.1974", "submitter": "Rio Yokota Dr.", "authors": "Rio Yokota, George Turkiyyah, David Keyes", "title": "Communication Complexity of the Fast Multipole Method and its Algebraic\n  Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A combination of hierarchical tree-like data structures and data access\npatterns from fast multipole methods and hierarchical low-rank approximation of\nlinear operators from H-matrix methods appears to form an algorithmic path\nforward for efficient implementation of many linear algebraic operations of\nscientific computing at the exascale. The combination provides asymptotically\noptimal computational and communication complexity and applicability to large\nclasses of operators that commonly arise in scientific computing applications.\nA convergence of the mathematical theories of the fast multipole and H-matrix\nmethods has been underway for over a decade. We recap this mathematical\nunification and describe implementation aspects of a hybrid of these two\ncompelling hierarchical algorithms on hierarchical distributed-shared memory\narchitectures, which are likely to be the first to reach the exascale. We\npresent a new communication complexity estimate for fast multipole methods on\nsuch architectures. We also show how the data structures and access patterns of\nH-matrices for low-rank operators map onto those of fast multipole, leading to\nan algebraically generalized form of fast multipole that compromises none of\nits architecturally ideal properties.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 11:48:44 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Yokota", "Rio", ""], ["Turkiyyah", "George", ""], ["Keyes", "David", ""]]}, {"id": "1406.2067", "submitter": "EPTCS", "authors": "Max Tschaikowski (University of Southampton), Mirco Tribastone\n  (University of Southampton)", "title": "Extended Differential Aggregations in Process Algebra for Performance\n  and Biology", "comments": "In Proceedings QAPL 2014, arXiv:1406.1567", "journal-ref": "EPTCS 154, 2014, pp. 34-47", "doi": "10.4204/EPTCS.154.3", "report-no": null, "categories": "cs.PF cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study aggregations for ordinary differential equations induced by fluid\nsemantics for Markovian process algebra which can capture the dynamics of\nperformance models and chemical reaction networks. Whilst previous work has\nrequired perfect symmetry for exact aggregation, we present approximate fluid\nlumpability, which makes nearby processes perfectly symmetric after a\nperturbation of their parameters. We prove that small perturbations yield\nnearby differential trajectories. Numerically, we show that many heterogeneous\nprocesses can be aggregated with negligible errors.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 03:47:53 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Tschaikowski", "Max", "", "University of Southampton"], ["Tribastone", "Mirco", "", "University of Southampton"]]}, {"id": "1406.2628", "submitter": "Oded Green", "authors": "Oded Green and Saher Odeh and Yitzhak Birk", "title": "Merge Path - A Visually Intuitive Approach to Parallel Merging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Merging two sorted arrays is a prominent building block for sorting and other\nfunctions. Its efficient parallelization requires balancing the load among\ncompute cores, minimizing the extra work brought about by parallelization, and\nminimizing inter-thread synchronization requirements. Efficient use of memory\nis also important.\n  We present a novel, visually intuitive approach to partitioning two input\nsorted arrays into pairs of contiguous sequences of elements, one from each\narray, such that 1) each pair comprises any desired total number of elements,\nand 2) the elements of each pair form a contiguous sequence in the output\nmerged sorted array. While the resulting partition and the computational\ncomplexity are similar to those of certain previous algorithms, our approach is\ndifferent, extremely intuitive, and offers interesting insights. Based on this,\nwe present a synchronization-free, cache-efficient merging (and sorting)\nalgorithm.\n  While we use a shared memory architecture as the basis, our algorithm is\neasily adaptable to additional architectures. In fact, our approach is even\nrelevant to cache-efficient sequential sorting. The algorithms are presented,\nalong with important cache-related insights.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 17:04:23 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 14:45:02 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Green", "Oded", ""], ["Odeh", "Saher", ""], ["Birk", "Yitzhak", ""]]}, {"id": "1406.2644", "submitter": "Ali Elouafiq", "authors": "Ali Elouafiq and Redouan Abid", "title": "Geographical Asynchronous Information Access for Distributed Systems", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Non-relational databases are the common means of data storage in the Cloud,\nand optimizing the data access is of paramount importance into determining the\noverall Cloud system performance. In this paper, we present GAIA, a novel model\nfor retrieving and managing correlated geo-localized data in the cloud\nenvironment. We survey and compare the existing models used mostly in\nGeographical Information Systems (GIS), mainly the Grid model and the\nCoordinates Projection model. Besides, we present a benchmark comparing the\nefficiency of the models. Using extensive experimentation, we show that GAIA\noutperforms the existing models by its high efficiency which is of O(log(n)),\nand this mainly thanks to its combination of projection with cell\ndecomposition. The other models have a linear efficiency of O(n). The presented\nmodel is designed from the ground up to support GIS and is designed to suit\nboth cloud and parallel computing.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 17:33:56 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Elouafiq", "Ali", ""], ["Abid", "Redouan", ""]]}, {"id": "1406.2844", "submitter": "Tarek Menouer", "authors": "Tarek Menouer (PRISM), Bertrand Le Cun (PRISM)", "title": "Partitionnement D\\'eterministe pour R\\'esoudre les Probl\\`emes de\n  Programmation Par Contraintes en utilisant le Framework Parall\\`ele Bobpp", "comments": "in French, ComPAS 2014 : conf\\'erence en parall\\'elisme, architecture\n  et syst\\`emes (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deterministic parallelization to explore a Constraint\nProgramming search space. This work is an answer to an industrial project named\nPAJERO, which is in need of a parallel constraint solver which always responds\nwith the same solution whether using sequential or parallel machines. It is\nwell known that parallel tree search changes the order in which the exploration\nof solution space is done. In the context where the first solution found is\nreturned, using a different number of cores may change the returned solution.\nIn the literature, several non deterministic strategies have been proposed to\nparallelize the exploration of Constraint Programming search space. Most of\nthem are based on the Work Stealing technique used to partition the Constraint\nProgramming search space on demand and during the execution of the search\nalgorithm. Our study focuses on the determinism of the parallel search versus\nthe sequential one. We consider that the sequential search algorithm is\ndeterministic, then propose an elegant solution introducing a total order on\nthe nodes in which the parallel algorithm always gives the same solution as the\nsequential one regardless of the number of cores used. To evaluate this\ndeterministic strategy, we ran tests using the Google OR-Tools Constraint\nProgramming solver on top of our parallel Bobpp framework. The performances are\nillustrated by solving Constraint Programming problems modeled in FlatZinc\nformat.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 09:50:52 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Menouer", "Tarek", "", "PRISM"], ["Cun", "Bertrand Le", "", "PRISM"]]}, {"id": "1406.2852", "submitter": "Micha{\\l} R\\'o\\.za\\'nski", "authors": "Tomasz Jurdzinski, Dariusz R. Kowalski, Michal Rozanski, Grzegorz\n  Stachowiak", "title": "On the Impact of Geometry on Ad Hoc Communication in Wireless Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the question how important is the knowledge of\ngeometric location and network density to the efficiency of (distributed)\nwireless communication in ad hoc networks. We study fundamental communication\ntask of broadcast and develop well-scalable, randomized algorithms that do not\nrely on GPS information, and which efficiency formulas do not depend on how\ndense the geometric network is. We consider two settings: with and without\nspontaneous wake-up of nodes. In the former setting, in which all nodes start\nthe protocol at the same time, our algorithm accomplishes broadcast in $O(D\\log\nn + \\log^2 n)$ rounds under the SINR model, with high probability (whp), where\n$D$ is the diameter of the communication graph and $n$ is the number of\nstations. In the latter setting, in which only the source node containing the\noriginal message is active in the beginning, we develop a slightly slower\nalgorithm working in $O(D\\log^2 n)$ rounds whp. Both algorithms are based on a\nnovel distributed coloring method, which is of independent interest and\npotential applicability to other communication tasks under the SINR wireless\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 10:27:14 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 16:04:49 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Jurdzinski", "Tomasz", ""], ["Kowalski", "Dariusz R.", ""], ["Rozanski", "Michal", ""], ["Stachowiak", "Grzegorz", ""]]}, {"id": "1406.3149", "submitter": "Christian Napoli", "authors": "Francesco Bonanno, Giacomo Capizzi, Grazia Lo Sciuto, Christian\n  Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "A Cascade Neural Network Architecture investigating Surface Plasmon\n  Polaritons propagation for thin metals in OpenMP", "comments": null, "journal-ref": "International conference on Artificial Intelligence and Soft\n  Computing (ICAISC 2014), Vol I, 22-33 (2014)", "doi": null, "report-no": null, "categories": "cs.NE cond-mat.mes-hall cond-mat.mtrl-sci cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Surface plasmon polaritons (SPPs) confined along metal-dielectric interface\nhave attracted a relevant interest in the area of ultracompact photonic\ncircuits, photovoltaic devices and other applications due to their strong field\nconfinement and enhancement. This paper investigates a novel cascade neural\nnetwork (NN) architecture to find the dependance of metal thickness on the SPP\npropagation. Additionally, a novel training procedure for the proposed cascade\nNN has been developed using an OpenMP-based framework, thus greatly reducing\ntraining time. The performed experiments confirm the effectiveness of the\nproposed NN architecture for the problem at hand.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 08:40:04 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Bonanno", "Francesco", ""], ["Capizzi", "Giacomo", ""], ["Sciuto", "Grazia Lo", ""], ["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1406.3156", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "A hybrid neuro--wavelet predictor for QoS control and stability", "comments": null, "journal-ref": "Proceedings of AI*IA 2013: Advances in Artificial Intelligence,\n  pages 527-538. Springer, 2013", "doi": "10.1007/978-3-319-03524-6_45", "report-no": null, "categories": "cs.NE cs.DC cs.NI cs.PF cs.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  For distributed systems to properly react to peaks of requests, their\nadaptation activities would benefit from the estimation of the amount of\nrequests. This paper proposes a solution to produce a short-term forecast based\non data characterising user behaviour of online services. We use \\emph{wavelet\nanalysis}, providing compression and denoising on the observed time series of\nthe amount of past user requests; and a \\emph{recurrent neural network} trained\nwith observed data and designed so as to provide well-timed estimations of\nfuture requests. The said ensemble has the ability to predict the amount of\nfuture user requests with a root mean squared error below 0.06\\%. Thanks to\nprediction, advance resource provision can be performed for the duration of a\nrequest peak and for just the right amount of resources, hence avoiding\nover-provisioning and associated costs. Moreover, reliable provision lets users\nenjoy a level of availability of services unaffected by load variations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 09:04:49 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1406.3313", "submitter": "EPTCS", "authors": "Alastair F. Donaldson (Imperial College London), Vasco T. Vasconcelos\n  (University of Lisbon)", "title": "Proceedings 7th Workshop on Programming Language Approaches to\n  Concurrency and Communication-cEntric Software", "comments": null, "journal-ref": "EPTCS 155, 2014", "doi": "10.4204/EPTCS.155", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the post-proceedings of PLACES 2014, the seventh\nWorkshop on Programming Language Approaches to Concurrency and\nCommunication-cEntric Software, which was held in Grenoble, France, on April\n12th 2014, and co-located with ETAPS, the European Joint Conferences on Theory\nand Practice of Software. The PLACES workshop series aims to offer a forum\nwhere researchers from different fields exchange new ideas on one of the\ncentral challenges for programming in the near future: the development of\nprogramming languages, methodologies and infrastructures where concurrency and\ndistribution are the norm rather than a marginal concern. Previous editions of\nPLACES were held in Rome (2013), Tallin (2012), Saarbrueken (2011), Paphos\n(2010) and York (2009), all co-located with ETAPS, and the first PLACES was\nheld in Oslo and co-located with DisCoTec (2008).\n  The Program Committee, after a careful and thorough reviewing process,\nselected nine papers out of 12 submissions for presentation at the workshop and\ninclusion in this post-proceedings. Each submission was evaluated by three\nreferees (with one paper receiving a fourth review), and the accepted papers\nwere selected during a week-long electronic discussion. One of the nine\naccepted papers was conditionally accepted subject to a process of shepherding\nby a PC member, which was successful and led to the paper's full acceptance.\n  In addition to the contributed papers, the workshop will feature an invited\ntalk by Akash Lal, Microsoft Research India, entitled \"Finding Concurrency Bugs\nUnder Imprecise Harnesses\".\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 18:39:04 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Donaldson", "Alastair F.", "", "Imperial College London"], ["Vasconcelos", "Vasco T.", "", "University of Lisbon"]]}, {"id": "1406.3479", "submitter": "EPTCS", "authors": "Sam Lindley (The University of Edinburgh), J. Garrett Morris (The\n  University of Edinburgh)", "title": "Sessions as Propositions", "comments": "In Proceedings PLACES 2014, arXiv:1406.3313", "journal-ref": "EPTCS 155, 2014, pp. 9-16", "doi": "10.4204/EPTCS.155.2", "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Wadler presented a continuation-passing translation from a\nsession-typed functional language, GV, to a process calculus based on classical\nlinear logic, CP. However, this translation is one-way: CP is more expressive\nthan GV. We propose an extension of GV, called HGV, and give translations\nshowing that it is as expressive as CP. The new translations shed light both on\nthe original translation from GV to CP, and on the limitations in\nexpressiveness of GV.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:16:41 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Lindley", "Sam", "", "The University of Edinburgh"], ["Morris", "J. Garrett", "", "The\n  University of Edinburgh"]]}, {"id": "1406.3482", "submitter": "EPTCS", "authors": "Rumyana Neykova (Imperial College London), Nobuko Yoshida (Imperial\n  College London)", "title": "Multiparty Session Actors", "comments": "In Proceedings PLACES 2014, arXiv:1406.3313", "journal-ref": "EPTCS 155, 2014, pp. 32-37", "doi": "10.4204/EPTCS.155.5", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actor coordination armoured with a suitable protocol description language has\nbeen a pressing problem in the actors community. We study the applicability of\nmultiparty session type (MPST) protocols for verification of actor programs. We\nincorporate sessions to actors by introducing minimum additions to the model\nsuch as the notion of actor roles and protocol mailbox. The framework uses\nScribble, which is a protocol description language based on multiparty session\ntypes. Our programming model supports actor-like syntax and runtime\nverification mechanism guaranteeing type-safety and progress of the\ncommunicating entities. An actor can implement multiple roles in a similar way\nas an object can implement multiple interfaces. Multiple roles allow for\ninter-concurrency in a single actor still preserving its progress property. We\ndemonstrate our framework by designing and implementing a session actor library\nin Python and its runtime verification mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:17:14 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Neykova", "Rumyana", "", "Imperial College London"], ["Yoshida", "Nobuko", "", "Imperial\n  College London"]]}, {"id": "1406.3483", "submitter": "EPTCS", "authors": "Tzu-chun Chen (Dipartimento di Informatica, Universita' di Torino,\n  Italy)", "title": "Lightening Global Types", "comments": "In Proceedings PLACES 2014, arXiv:1406.3313", "journal-ref": "EPTCS 155, 2014, pp. 38-46", "doi": "10.4204/EPTCS.155.6", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global session types prevent participants from waiting for never coming\nmessages. Some interactions take place just for the purpose of informing\nreceivers that some message will never arrive or the session is terminated. By\ndecomposing a big global type into several light global types, one can avoid\nsuch kind of redundant interactions. Lightening global types gives us cleaner\nglobal types, which keep all necessary communications. This work proposes a\nframework which allows to easily decompose global types into light global\ntypes, preserving the interaction sequences of the original ones but for\nredundant interactions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:17:23 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Chen", "Tzu-chun", "", "Dipartimento di Informatica, Universita' di Torino,\n  Italy"]]}, {"id": "1406.3485", "submitter": "EPTCS", "authors": "Janwillem Swalens (Vrije Universiteit Brussel), Stefan Marr (Vrije\n  Universiteit Brussel), Joeri De Koster (Vrije Universiteit Brussel), Tom Van\n  Cutsem (Vrije Universiteit Brussel)", "title": "Towards Composable Concurrency Abstractions", "comments": "In Proceedings PLACES 2014, arXiv:1406.3313", "journal-ref": "EPTCS 155, 2014, pp. 54-60", "doi": "10.4204/EPTCS.155.8", "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, many different programming models for managing\nconcurrency in applications have been proposed, such as the actor model,\nCommunicating Sequential Processes, and Software Transactional Memory. The\nubiquity of multi-core processors has made harnessing concurrency even more\nimportant. We observe that modern languages, such as Scala, Clojure, or F#,\nprovide not one, but multiple concurrency models that help developers manage\nconcurrency. Large end-user applications are rarely built using just a single\nconcurrency model. Programmers need to manage a responsive UI, deal with file\nor network I/O, asynchronous workflows, and shared resources. Different\nconcurrency models facilitate different requirements. This raises the issue of\nhow these concurrency models interact, and whether they are composable. After\nall, combining different concurrency models may lead to subtle bugs or\ninconsistencies.\n  In this paper, we perform an in-depth study of the concurrency abstractions\nprovided by the Clojure language. We study all pairwise combinations of the\nabstractions, noting which ones compose without issues, and which do not. We\nmake an attempt to abstract from the specifics of Clojure, identifying the\ngeneral properties of concurrency models that facilitate or hinder composition.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:17:43 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Swalens", "Janwillem", "", "Vrije Universiteit Brussel"], ["Marr", "Stefan", "", "Vrije\n  Universiteit Brussel"], ["De Koster", "Joeri", "", "Vrije Universiteit Brussel"], ["Van Cutsem", "Tom", "", "Vrije Universiteit Brussel"]]}, {"id": "1406.3699", "submitter": "Ivo Jimenez", "authors": "Ivo Jimenez, Carlos Maltzahn, Jay Lofstead", "title": "Distributed Versioned Object Storage -- Alternatives at the OSD layer\n  (Poster Extended Abstract)", "comments": "2 pages, 2 tables, poster extended abstract, HPDC '14, The ACM\n  International Symposium on High-Performance Parallel and Distributed\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to store multiple versions of a data item is a powerful primitive\nthat has had a wide variety of uses: relational databases, transactional\nmemory, version control systems, to name a few. However, each implementation\nuses a very particular form of versioning that is customized to the domain in\nquestion and hidden away from the user. In our going project, we are reviewing\nand analyzing multiple uses of versioning in distinct domains, with the goal of\nidentifying the basic components required to provide a generic distributed\nmultiversioning object storage service, and define how these can be customized\nin order to serve distinct needs. With this primitive, new services can\nleverage multiversioning to ease development and provide specific consistency\nguarantees that address particular use cases. This work presents early results\nthat quantify the trade-offs in implementing versioning at the local storage\nlayer.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 08:02:02 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Jimenez", "Ivo", ""], ["Maltzahn", "Carlos", ""], ["Lofstead", "Jay", ""]]}, {"id": "1406.3901", "submitter": "Liya Fan", "authors": "Liya Fan, Bo Gao, Fa Zhang, Zhiyong Liu", "title": "OS4M: Achieving Global Load Balance of MapReduce Workload by Scheduling\n  at the Operation Level", "comments": "arXiv admin note: substantial text overlap with arXiv:1401.0355", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of MapReduce is closely related to its load balance. Existing\nworks on MapReduce load balance focus on coarse-grained scheduling. This study\nconcerns fine-grained scheduling on MapReduce operations, with each operation\nrepresenting one invocation of the Map or Reduce function. By default,\nMapReduce adopts the hash-based method to schedule Reduce operations, which\noften leads to poor load balance. In addition, the copy phase of Reduce tasks\noverlaps with Map tasks, which significantly hinders the progress of Map tasks\ndue to I/O contention. Moreover, the three phases of Reduce tasks run in\nsequence, while consuming different resources, thereby under-utilizing\nresources. To overcome these problems, we introduce a set of mechanisms named\nOS4M (Operation Scheduling for MapReduce) to improve MapReduce's performance.\nOS4M achieves load balance by collecting statistics of all Map operations, and\ncalculates a globally optimal schedule to distribute Reduce operations. With\nOS4M, the copy phase of Reduce tasks no longer overlaps with Map tasks, and the\nthree phases of Reduce tasks are pipelined based on their operation loads. OS4M\nhas been transparently incorporated into MapReduce. Evaluations on standard\nbenchmarks show that OS4M's job duration can be shortened by up to 42%,\ncompared with a baseline of Hadoop.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 04:16:41 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Fan", "Liya", ""], ["Gao", "Bo", ""], ["Zhang", "Fa", ""], ["Liu", "Zhiyong", ""]]}, {"id": "1406.4291", "submitter": "Christopher Meiklejohn", "authors": "Christopher Meiklejohn", "title": "Vector Clocks in Coq: An Experience Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report documents the process of implementing vector clocks in the Coq\nproof assistant for extraction and use in the distributed Dynamo-inspired data\nstore, Riak. In this report, we focus on the technical challenges of using Core\nErlang code extracted from the proof assistant in a production-grade Erlang\napplication, as opposed to verification of the model itself.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 09:32:16 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Meiklejohn", "Christopher", ""]]}, {"id": "1406.4580", "submitter": "Seunghak Lee", "authors": "Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A. Gibson, Eric\n  P. Xing", "title": "Primitives for Dynamic Big Model Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training large machine learning models with many variables or\nparameters, a single machine is often inadequate since the model may be too\nlarge to fit in memory, while training can take a long time even with\nstochastic updates. A natural recourse is to turn to distributed cluster\ncomputing, in order to harness additional memory and processors. However,\nnaive, unstructured parallelization of ML algorithms can make inefficient use\nof distributed memory, while failing to obtain proportional convergence\nspeedups - or can even result in divergence. We develop a framework of\nprimitives for dynamic model-parallelism, STRADS, in order to explore\npartitioning and update scheduling of model variables in distributed ML\nalgorithms - thus improving their memory efficiency while presenting new\nopportunities to speed up convergence without compromising inference\ncorrectness. We demonstrate the efficacy of model-parallel algorithms\nimplemented in STRADS versus popular implementations for Topic Modeling, Matrix\nFactorization and Lasso.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 03:06:52 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Lee", "Seunghak", ""], ["Kim", "Jin Kyu", ""], ["Zheng", "Xun", ""], ["Ho", "Qirong", ""], ["Gibson", "Garth A.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1406.4840", "submitter": "David Castells-Rufas", "authors": "David Castells-Rufas, Jordi Carrabina, Pablo Gonz\\'alez de Aledo\n  Marug\\'an, Pablo S\\'anchez Espeso", "title": "Fast Trace Generation of Many-Core Embedded Systems with Native\n  Simulation", "comments": "Proceedings of HIP3ES Workshop, Vienna, January, 21st 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedded Software development and optimization are complex tasks. Late\navailably of hardware platforms, their usual low visibility and\ncontrollability, and their limiting resource constraints makes early\nperformance estimation an attractive option instead of using the final\nexecution platform. With early performance estimation, software development can\nprogress although the real hardware is not yet available or it is too complex\nto interact with. In this paper, we present how the native simulation framework\nSCoPE is extended to generate OTF trace files. Those trace files can be later\nvisualized with trace visualization tools, which recently were only used to\noptimize HPC workloads in order to iterate in the development process.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 18:59:45 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Castells-Rufas", "David", ""], ["Carrabina", "Jordi", ""], ["Marug\u00e1n", "Pablo Gonz\u00e1lez de Aledo", ""], ["Espeso", "Pablo S\u00e1nchez", ""]]}, {"id": "1406.4923", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, William Arcand, David Bestor, Bill Bergeron, Chansup\n  Byun, Vijay Gadepally, Matthew Hubbell, Peter Michaleas, Julie Mullen, Andrew\n  Prout, Albert Reuther, Antonio Rosa, Charles Yee (MIT)", "title": "Achieving 100,000,000 database inserts per second using Accumulo and D4M", "comments": "6 pages; to appear in IEEE High Performance Extreme Computing (HPEC)\n  2014", "journal-ref": null, "doi": "10.1109/HPEC.2014.7040945", "report-no": null, "categories": "cs.DB astro-ph.IM cs.CE cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apache Accumulo database is an open source relaxed consistency database\nthat is widely used for government applications. Accumulo is designed to\ndeliver high performance on unstructured data such as graphs of network data.\nThis paper tests the performance of Accumulo using data from the Graph500\nbenchmark. The Dynamic Distributed Dimensional Data Model (D4M) software is\nused to implement the benchmark on a 216-node cluster running the MIT\nSuperCloud software stack. A peak performance of over 100,000,000 database\ninserts per second was achieved which is 100x larger than the highest\npreviously published value for any other database. The performance scales\nlinearly with the number of ingest clients, number of database servers, and\ndata size. The performance was achieved by adapting several supercomputing\ntechniques to this application: distributed arrays, domain decomposition,\nadaptive load balancing, and single-program-multiple-data programming.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 00:44:12 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kepner", "Jeremy", "", "MIT"], ["Arcand", "William", "", "MIT"], ["Bestor", "David", "", "MIT"], ["Bergeron", "Bill", "", "MIT"], ["Byun", "Chansup", "", "MIT"], ["Gadepally", "Vijay", "", "MIT"], ["Hubbell", "Matthew", "", "MIT"], ["Michaleas", "Peter", "", "MIT"], ["Mullen", "Julie", "", "MIT"], ["Prout", "Andrew", "", "MIT"], ["Reuther", "Albert", "", "MIT"], ["Rosa", "Antonio", "", "MIT"], ["Yee", "Charles", "", "MIT"]]}, {"id": "1406.4929", "submitter": "Kohei Nakajima", "authors": "Kohei Nakajima, Tao Li, Helmut Hauser, Rolf Pfeifer", "title": "Exploiting short-term memory in soft body dynamics as a computational\n  resource", "comments": "22 pages, 11 figures; email address corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft materials are not only highly deformable but they also possess rich and\ndiverse body dynamics. Soft body dynamics exhibit a variety of properties,\nincluding nonlinearity, elasticity, and potentially infinitely many degrees of\nfreedom. Here we demonstrate that such soft body dynamics can be employed to\nconduct certain types of computation. Using body dynamics generated from a soft\nsilicone arm, we show that they can be exploited to emulate functions that\nrequire memory and to embed robust closed-loop control into the arm. Our\nresults suggest that soft body dynamics have a short-term memory and can serve\nas a computational resource. This finding paves the way toward exploiting\npassive body dynamics for control of a large class of underactuated systems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 01:45:36 GMT"}, {"version": "v2", "created": "Sat, 21 Jun 2014 02:04:31 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Nakajima", "Kohei", ""], ["Li", "Tao", ""], ["Hauser", "Helmut", ""], ["Pfeifer", "Rolf", ""]]}, {"id": "1406.4974", "submitter": "Adam Barker", "authors": "Adam Barker, Blesson Varghese, Jonathan Stuart Ward and Ian\n  Sommerville", "title": "Academic Cloud Computing Research: Five Pitfalls and Five Opportunities", "comments": "Accepted and presented at the 6th USENIX Workshop on Hot Topics in\n  Cloud Computing (HotCloud'14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This discussion paper argues that there are five fundamental pitfalls, which\ncan restrict academics from conducting cloud computing research at the\ninfrastructure level, which is currently where the vast majority of academic\nresearch lies. Instead academics should be conducting higher risk research, in\norder to gain understanding and open up entirely new areas.\n  We call for a renewed mindset and argue that academic research should focus\nless upon physical infrastructure and embrace the abstractions provided by\nclouds through five opportunities: user driven research, new programming\nmodels, PaaS environments, and improved tools to support elasticity and\nlarge-scale debugging. The objective of this paper is to foster discussion, and\nto define a roadmap forward, which will allow academia to make longer-term\nimpacts to the cloud computing community.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 08:08:14 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Barker", "Adam", ""], ["Varghese", "Blesson", ""], ["Ward", "Jonathan Stuart", ""], ["Sommerville", "Ian", ""]]}, {"id": "1406.5020", "submitter": "Rasha Alcattan", "authors": "Rasha Fouad AlCattan", "title": "Integration of Cloud Computing and Web2.0 Collaboration Technologies in\n  E-Learning", "comments": "10 pages, 5 figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V12(1):46-55, June 2014", "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing technology is an emerging new computing paradigm for\ndelivering computing services. Although it still in its early stage, it has\nchanged the way how many applications are developed and accessed. This\ncomputing approach relies on a number of existing technologies, such as Web2.0,\nvirtualization, Service oriented architecture SOA, Web services,etc.Cloud\ncomputing is growing rapidly and becoming an adoptable technology for the\norganizations especially education institutes, with its dynamic scalability and\nusage of virtualized resources as a service through the Internet.Today,\neLearning is also becoming a very popular and powerful trend.However,in\ntraditional web based eLearning systems,building and maintenance are located\nonsite in institutions or enterprises, which cause lot of problems to appear\nsuch as lacking the support of underlying infrastructure, which can dynamically\nallocate the needed calculation and storage resources for eLearning systems.As\nthe need for e learning is increasing continuously and its necessary for\neLearning systems to keep pace with the right technology needed for development\nand improvement.However, todays technologies such as Web 2.0, Cloud, etc.enable\nto build more successful and effective educational environment,that provide\ncollaboration and interaction in eLearning environments.The challenge is to use\nand integrate these technologies in order to construct tools that allow the\nbest possible learning results.Cloud computing and Web 2.0 are two areas that\nare starting to strongly effect how the development,deployment and usage of\neLearning application.This paper presents the benefits of using cloud computing\nwith the integration of Web 2.0 collaboration technologies in eLearning\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 12:24:30 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["AlCattan", "Rasha Fouad", ""]]}, {"id": "1406.5161", "submitter": "Jeyanthi Salem Narasimhan", "authors": "Jeyanthi Narasimhan, Abhinav Vishnu, Lawrence Holder, Adolfy Hoisie", "title": "Fast Support Vector Machines Using Parallel Adaptive Shrinking on\n  Distributed Systems", "comments": "10 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVM), a popular machine learning technique, has been\napplied to a wide range of domains such as science, finance, and social\nnetworks for supervised learning. Whether it is identifying high-risk patients\nby health-care professionals, or potential high-school students to enroll in\ncollege by school districts, SVMs can play a major role for social good. This\npaper undertakes the challenge of designing a scalable parallel SVM training\nalgorithm for large scale systems, which includes commodity multi-core\nmachines, tightly connected supercomputers and cloud computing systems.\nIntuitive techniques for improving the time-space complexity including adaptive\nelimination of samples for faster convergence and sparse format representation\nare proposed. Under sample elimination, several heuristics for {\\em earliest\npossible} to {\\em lazy} elimination of non-contributing samples are proposed.\nIn several cases, where an early sample elimination might result in a false\npositive, low overhead mechanisms for reconstruction of key data structures are\nproposed. The algorithm and heuristics are implemented and evaluated on various\npublicly available datasets. Empirical evaluation shows up to 26x speed\nimprovement on some datasets against the sequential baseline, when evaluated on\nmultiple compute nodes, and an improvement in execution time up to 30-60\\% is\nreadily observed on a number of other datasets against our parallel baseline.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 19:22:28 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Narasimhan", "Jeyanthi", ""], ["Vishnu", "Abhinav", ""], ["Holder", "Lawrence", ""], ["Hoisie", "Adolfy", ""]]}, {"id": "1406.5440", "submitter": "Johan Pouwelse", "authors": "Dinesh, Erlich, Gilfoyle, Jared, Richard, Johan Pouwelse", "title": "Operational Distributed Regulation for Bitcoin", "comments": "9 pages report by students who desire to remain anonymous", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On February 2014, $650.000.000 worth of Bitcoins disappeared. Currently it is\nunclear whether hackers or MtGox, the largest Bitcoin exchange, are to be\nblamed. In either case, the anonymous and unregulated nature of the Bitcoin\nsystem makes it practically impossible for innocent victims to get their money\nback. We have investigated the technical possibilities, solutions and\nimplications of introducing a regulatory framework based on redlisting Bitcoin\naccounts. Despite numerous proposals, the Bitcoin community has voiced a strong\nopinion against any form of regulation. However, most of the discussions were\nbased on speculations rather than facts. We strive to contribute a scientific\nfoundation to these discussions and illuminate the path to crypto-justice.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 16:06:46 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Dinesh", "", ""], ["Erlich", "", ""], ["Gilfoyle", "", ""], ["Jared", "", ""], ["Richard", "", ""], ["Pouwelse", "Johan", ""]]}, {"id": "1406.5687", "submitter": "Shaikh Arifuzzaman", "authors": "Shaikh Arifuzzaman, Maleq Khan, and Madhav Marathe", "title": "Parallel Algorithms for Counting Triangles in Networks with Large\n  Degrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the number of triangles in a network is an important problem in the\nanalysis of complex networks. The number of triangles also has important\napplications in data mining. Existing distributed memory parallel algorithms\nfor counting triangles are either Map-Reduce based or message passing interface\n(MPI) based and work with overlapping partitions of the given network. These\nalgorithms are designed for very sparse networks and do not work well when the\ndegrees of the nodes are relatively larger. For networks with larger degrees,\nMap-Reduce based algorithm generates prohibitively large intermediate data, and\nin MPI based algorithms with overlapping partitions, each partition can grow as\nlarge as the original network, wiping out the benefit of partitioning the\nnetwork.\n  In this paper, we present two efficient MPI-based parallel algorithms for\ncounting triangles in massive networks with large degrees. The first algorithm\nis a space-efficient algorithm for networks that do not fit in the main memory\nof a single compute node. This algorithm divides the network into\nnon-overlapping partitions. The second algorithm is for the case where the main\nmemory of each node is large enough to contain the entire network. We observe\nthat for such a case, computation load can be balanced dynamically and present\na dynamic load balancing scheme which improves the performance significantly.\nBoth of our algorithms scale well to large networks and to a large number of\nprocessors.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 09:11:22 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Arifuzzaman", "Shaikh", ""], ["Khan", "Maleq", ""], ["Marathe", "Madhav", ""]]}, {"id": "1406.5759", "submitter": "Abm Moniruzzaman", "authors": "A B M Moniruzzaman, Kawser Wazed Nafi and Syed Akther Hossain", "title": "An Experimental Study of Load Balancing of OpenNebula Open-Source Cloud\n  Computing Platform", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing is becoming a viable computing solution for services oriented\ncomputing. Several open-source cloud solutions are available to these supports.\nOpen-source software stacks offer a huge amount of customizability without huge\nlicensing fees. As a result, open source software are widely used for designing\ncloud, and private clouds are being built increasingly in the open source way.\nNumerous contributions have been made by the open-source community related to\nprivate-IaaS-cloud. OpenNebula - a cloud platform is one of the popular private\ncloud management software. However, little has been done to systematically\ninvestigate the performance evaluation of this open-source cloud solution in\nthe existing literature. The performance evaluation aids new and existing\nresearch, industry and international projects when selecting OpenNebula\nsoftware to their work. The objective of this paper is to evaluate the\nload-balancing performance of the OpenNebula cloud management software. For the\nperformance evaluation, the OpenNebula cloud management software is installed\nand configured as a prototype implementation and tested on the DIU Cloud Lab.\nIn this paper, two set of experiments are conducted to identify the load\nbalancing performance of the OpenNebula cloud management platform- (1) Delete\nand Add Virtual Machine (VM) from OpenNebula cloud platform; (2) Mapping\nPhysical Hosts to Virtual Machines (VMs) in the OpenNebula cloud platform.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 20:40:07 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Moniruzzaman", "A B M", ""], ["Nafi", "Kawser Wazed", ""], ["Hossain", "Syed Akther", ""]]}, {"id": "1406.5760", "submitter": "Abm Moniruzzaman", "authors": "A B M Moniruzzaman, Kawser Wazed Nafi, Syed Akther Hossain", "title": "Virtual Memory Streaming Technique for Virtual Machines (VMs) for Rapid\n  Scaling and High Performance in Cloud Environment", "comments": "Keywords Virtual memory, Memory Streaming, Virtual machine, Virtual\n  memory scaling, VM live migration, VM Cloning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the impact of Virtual Memory Streaming (VMS) technique\nin provisioning virtual machines (VMs) in cloud environment. VMS is a scaling\nvirtualization technology that allows different virtual machines rapid scale,\nhigh performance, and increase hardware utilization. Traditional hypervisors do\nnot support true no-downtime live migration, and its lack of memory\noversubscription can hurt the economics of a private cloud deployment by\nlimiting the number of VMs on each host. VMS brings together several advanced\nhypervisor memory management techniques including granular page sharing,\ndynamic memory footprint management, live migration, read caching, and a unique\nvirtual machine cloning capability. An architecture model is described,\ntogether with a proof-of-concept implementation, that VMS dynamically scaling\nof virtualized infrastructure with true live migration and cloning of VMs. This\npaper argues that VMS for Cloud allows requiring significantly reduced server\nmemory and reducing the time for virtualized resource scaling by instantly\nadding more virtual machines.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 20:44:10 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Moniruzzaman", "A B M", ""], ["Nafi", "Kawser Wazed", ""], ["Hossain", "Syed Akther", ""]]}, {"id": "1406.5761", "submitter": "Abm Moniruzzaman", "authors": "A B M Moniruzzaman and Syed Akther Hossain", "title": "A Low Cost Two-Tier Architecture Model For High Availability Clusters\n  Application Load Balancing", "comments": "Load balancing, high availability cluster, web server clusters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a design and implementation of a low cost two-tier\narchitecture model for high availability cluster combined with load-balancing\nand shared storage technology to achieve desired scale of three-tier\narchitecture for application load balancing e.g. web servers. The research work\nproposes a design that physically omits Network File System (NFS) server nodes\nand implements NFS server functionalities within the cluster nodes, through Red\nHat Cluster Suite (RHCS) with High Availability (HA) proxy load balancing\ntechnologies. In order to achieve a low-cost implementation in terms of\ninvestment in hardware and computing solutions, the proposed architecture will\nbe beneficial. This system intends to provide steady service despite any system\ncomponents fails due to uncertainly such as network system, storage and\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 20:46:57 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Moniruzzaman", "A B M", ""], ["Hossain", "Syed Akther", ""]]}, {"id": "1406.5975", "submitter": "Yogesh Simmhan", "authors": "Yogesh Simmhan, Charith Wickramaarachchi, Alok Kumbhare, Marc Frincu,\n  Soonil Nagarkar, Santosh Ravi, Cauligi Raghavendra, Viktor Prasanna", "title": "Scalable Analytics over Distributed Time-series Graphs using GoFFish", "comments": null, "journal-ref": "Proceedings of the IEEE International Parallel and Distributed\n  Processing Symposium (IPDPS) (2015) pp. 809-818", "doi": "10.1109/IPDPS.2015.66", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are a key form of Big Data, and performing scalable analytics over\nthem is invaluable to many domains. As our ability to collect data grows, there\nis an emerging class of inter-connected data which accumulates or varies over\ntime, and on which novel analytics - both over the network structure and across\nthe time-variant attribute values - is necessary. We introduce the notion of\ntime-series graph analytics and propose Gopher, a scalable programming\nabstraction to develop algorithms and analytics on such datasets. Our\nabstraction leverages a sub-graph centric programming model and extends it to\nthe temporal dimension using an iterative BSP (Bulk Synchronous Parallel)\napproach. Gopher is co-designed with GoFS, a distributed storage specialized\nfor time-series graphs, as part of the GoFFish distributed analytics platform.\nWe examine storage optimizations for GoFS, design patterns in Gopher to\nleverage the distributed data layout, and evaluate the GoFFish platform using\ntime-series graph data and applications on a commodity cluster.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 16:48:03 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Simmhan", "Yogesh", ""], ["Wickramaarachchi", "Charith", ""], ["Kumbhare", "Alok", ""], ["Frincu", "Marc", ""], ["Nagarkar", "Soonil", ""], ["Ravi", "Santosh", ""], ["Raghavendra", "Cauligi", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1406.5977", "submitter": "Yogesh Simmhan", "authors": "Yogesh Simmhan and Alok Kumbhare", "title": "Floe: A Continuous Dataflow Framework for Dynamic Cloud Applications", "comments": "Prepared in Dec, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in cyber-physical systems are increasingly coupled with online\ninstruments to perform long running, continuous data processing. Such \"always\non\" dataflow applications are dynamic, where they need to change the\napplications logic and performance at runtime, in response to external\noperational needs. Floe is a continuous dataflow framework that is designed to\nbe adaptive for dynamic applications on Cloud infrastructure. It offers\nadvanced dataflow patterns like BSP and MapReduce for flexible and holistic\ncomposition of streams and files, and supports dynamic recomposition at runtime\nwith minimal impact on the execution. Adaptive resource allocation strategies\nallow our framework to effectively use elastic Cloud resources to meet varying\ndata rates. We illustrate the design patterns of Floe by running an integration\npipeline and a tweet clustering application from the Smart Power Grids domain\non a private Eucalyptus Cloud. The responsiveness of our resource adaptation is\nvalidated through simulations for periodic, bursty and random workloads.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 16:56:48 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Simmhan", "Yogesh", ""], ["Kumbhare", "Alok", ""]]}, {"id": "1406.6158", "submitter": "Naohito Nakasato", "authors": "Tsuyoshi Watanabe and Naohito Nakasato", "title": "GPU accelerated Hybrid Tree Algorithm for Collision-less N-body\n  Simulations", "comments": "Paper presented at Fifth International Symposium on Highly-Efficient\n  Accelerators and Reconfigurable Technologies (HEART2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid tree algorithm for reducing calculation and communication\ncost of collision-less N-body simulations. The concept of our algorithm is that\nwe split interaction force into two parts: hard-force from neighbor particles\nand soft-force from distant particles, and applying different time integration\nfor the forces. For hard-force calculation, we can efficiently reduce the\ncalculation and communication cost of the parallel tree code because we only\nneed data of neighbor particles for this part. We implement the algorithm on\nGPU clusters to accelerate force calculation for both hard and soft force. As\nthe result of implementing the algorithm on GPU clusters, we were able to\nreduce the communication cost and the total execution time to 40% and 80% of\nthat of a normal tree algorithm, respectively. In addition, the reduction\nfactor relative the normal tree algorithm is smaller for large number of\nprocesses, and we expect that the execution time can be ultimately reduced down\nto about 70% of the normal tree algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 07:43:27 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Watanabe", "Tsuyoshi", ""], ["Nakasato", "Naohito", ""]]}, {"id": "1406.7285", "submitter": "Yaghoob Siahmargooei", "authors": "Yaghoob Siahmargooei, Mohammad Kazem Akbari, Seyyed Alireza Hashemi\n  Golpayegani and Saeed Sharifian", "title": "Near-Optimal Virtual Machine Packing Based on Resource Requirement of\n  Service Demands Using Pattern Clustering", "comments": null, "journal-ref": "IJASCSE journal, Volume 3, Issue 6, JUNE 2014", "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Upon the expansion of Cloud Computing and the positive outlook of\norganizations with regard to the movements towards using cloud computing and\ntheir expanding utilization of such valuable processing method, as well as the\nsolutions provided by the cloud infrastructure providers with regard to the\nreduction of the costs of processing resources, the problem of organizing\nresources in a cloud environment gained a high importance. One of the major\npreoccupations of the minds of cloud infrastructure clients is their lack of\nknowledge on the quantity of their required processing resources in different\nperiods of time. The managers and technicians are trying to make the most use\nof scalability and the flexibility of the resources in cloud computing. The\nmain challenge is with calculating the amount of the required processing\nresources per moment with regard to the quantity of incoming requests of the\nservice. Through deduction of the accurate amount of these items, one can have\nan accurate estimation of the requests per moment. This paper aims at\nintroducing a model for automatic scaling of the cloud resources that would\nreduce the cost of renting the resources for the clients of cloud\ninfrastructure. Thus, first we start with a thorough explanation of the\nproposal and the major components of the model. Then through calculating the\nincomings of the model through clustering and introducing the way that each of\nthese components work in different phases,...\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 19:56:15 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Siahmargooei", "Yaghoob", ""], ["Akbari", "Mohammad Kazem", ""], ["Golpayegani", "Seyyed Alireza Hashemi", ""], ["Sharifian", "Saeed", ""]]}, {"id": "1406.7423", "submitter": "Vinit Kumar", "authors": "Vinit Kumar and Ajay Agarwal", "title": "An Efficient Read Dominant Data Replication Protocol under Serial\n  Isolation using Quorum Consensus Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed systems, data replication provides better availability, higher\nread capacity, improved access efficiency and lower bandwidth requirements in\nthe system. In this paper, we propose a significantly efficient approach of the\ndata replication for serial isolation by using newly proposed Circular quorum\nsystems. This paper has three major contributions. First, we have proposed the\nCircular quorum systems that generalize the various existing quorum systems,\nsuch as Read-one-write-all (ROWA) quorum systems, Majority quorum systems, Grid\nquorum systems, Diamond quorum systems, D-Space quorum systems,\nMulti-dimensional-grid quorum systems and Generalized-grid quorum systems.\nSecond, Circular quorum systems not only generalizes but also improves the\nperformance over existing quorum systems of their category. Third, we proposed\na highly available Circular quorum consensus protocol for data replication\nunder serial isolation level that uses a suitable Circular quorum system for\nread dominant scenario.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 17:40:19 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Kumar", "Vinit", ""], ["Agarwal", "Ajay", ""]]}, {"id": "1406.7524", "submitter": "Nevin Vunka Jungum", "authors": "Nevin Vunka Jungum, Nawaz Mohamudally and Nimal Nissanke", "title": "Towards a Generic Application Partitioning and Retraction Framework for\n  Pervasive Environments", "comments": "3rd International Conference on Future Computer and Communication,\n  ICFCC 2011", "journal-ref": null, "doi": "10.1115/1.859711", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current mobile context-aware applications for pervasive environments have\nbeen designed to consume information from computational nodes or devices in\ntheir surroundings or environments. As the hardware industry continues making\nmuch smaller, compact and cheap hardware, the vision of having plenty of very\nsmall powerful digital networking nodes in, for e.g., the living room or\nbedroom, is not so far. Designing software that can make optimal use of all\nthese computational nodes when needed is still challenging; since software will\nnot only consume information from these nodes but parts of the software can be\nhosted on these different nodes. In this paper we propose the BubbleCodes\nFramework which is a generic application partitioning and retraction framework\nfor next generation context-aware applications that will have the capabilities\nto partition and retract themselves on multiple computational nodes in a\npervasive environment.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 17:08:17 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Jungum", "Nevin Vunka", ""], ["Mohamudally", "Nawaz", ""], ["Nissanke", "Nimal", ""]]}, {"id": "1406.7540", "submitter": "Samuel Benz", "authors": "Samuel Benz, Parisa Jalili Marandi, Fernando Pedone, Beno\\^it\n  Garbinato", "title": "Building global and scalable systems with Atomic Multicast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of worldwide Internet-scale services demands large distributed\nsystems. Indeed, when handling several millions of users, it is common to\noperate thousands of servers spread across the globe. Here, replication plays a\ncentral role, as it contributes to improve the user experience by hiding\nfailures and by providing acceptable latency. In this paper, we claim that\natomic multicast, with strong and well-defined properties, is the appropriate\nabstraction to efficiently design and implement globally scalable distributed\nsystems. We substantiate our claim with the design of two modern online\nservices atop atomic multicast, a strongly consistent key-value store and a\ndistributed log. In addition to presenting the design of these services, we\nexperimentally assess their performance in a geographically distributed\ndeployment.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 19:27:44 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Benz", "Samuel", ""], ["Marandi", "Parisa Jalili", ""], ["Pedone", "Fernando", ""], ["Garbinato", "Beno\u00eet", ""]]}, {"id": "1406.7570", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis", "title": "Streaming Graph Partitioning in the Planted Partition Model", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer increase in the size of graph data has created a lot of interest\ninto developing efficient distributed graph processing frameworks. Popular\nexisting frameworks such as Graphlab and Pregel rely on balanced graph\npartitioning in order to minimize communication and achieve work balance.\n  In this work we contribute to the recent research line of streaming graph\npartitioning \\cite{stantonstreaming,stanton,fennel} which computes an\napproximately balanced $k$-partitioning of the vertex set of a graph using a\nsingle pass over the graph stream using degree-based criteria. This graph\npartitioning framework is well tailored to processing large-scale and dynamic\ngraphs. In this work we introduce the use of higher length walks for streaming\ngraph partitioning and show that their use incurs a minor computational cost\nwhich can significantly improve the quality of the graph partition. We perform\nan average case analysis of our algorithm using the planted partition model\n\\cite{condon2001algorithms,mcsherry2001spectral}. We complement the recent\nresults of Stanton \\cite{stantonstreaming} by showing that our proposed method\nrecovers the true partition with high probability even when the gap of the\nmodel tends to zero as the size of the graph grows. Furthermore, among the wide\nnumber of choices for the length of the walks we show that the proposed length\nis optimal. Finally, we conduct experiments which verify the value of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 00:08:42 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 16:13:35 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""]]}]