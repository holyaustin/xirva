[{"id": "1712.00252", "submitter": "David Goz Dr.", "authors": "D. Goz, L. Tornatore, G. Taffoni, and G. Murante", "title": "Cosmological Simulations in Exascale Era", "comments": "submitted to ASP", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The architecture of Exascale computing facilities, which involves millions of\nheterogeneous processing units, will deeply impact on scientific applications.\nFuture astrophysical HPC applications must be designed to make such computing\nsystems exploitable. The ExaNeSt H2020 EU-funded project aims to design and\ndevelop an exascale ready prototype based on low-energy-consumption ARM64 cores\nand FPGA accelerators. We participate to the design of the platform and to the\nvalidation of the prototype with cosmological N-body and hydrodynamical codes\nsuited to perform large-scale, high-resolution numerical simulations of cosmic\nstructures formation and evolution. We discuss our activities on astrophysical\napplications to take advantage of the underlying architecture.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 09:47:08 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Goz", "D.", ""], ["Tornatore", "L.", ""], ["Taffoni", "G.", ""], ["Murante", "G.", ""]]}, {"id": "1712.00285", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim, Michael Elkin and Uri Goldenberg", "title": "Locally-Iterative Distributed (Delta + 1)-Coloring below\n  Szegedy-Vishwanathan Barrier, and Applications to Self-Stabilization and to\n  Restricted-Bandwidth Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider graph coloring and related problems in the distributed\nmessage-passing model. {Locally-iterative algorithms} are especially important\nin this setting. These are algorithms in which each vertex decides about its\nnext color only as a function of the current colors in its 1-hop neighborhood.\nIn STOC'93 Szegedy and Vishwanathan showed that any locally-iterative\n(Delta+1)-coloring algorithm requires Omega(Delta log Delta + log^* n) rounds,\nunless there is \"a very special type of coloring that can be very efficiently\nreduced\" \\cite{SV93}. In this paper we obtain this special type of coloring.\nSpecifically, we devise a locally-iterative (Delta+1)-coloring algorithm with\nrunning time O(Delta + log^* n), i.e., {below} Szegedy-Vishwanathan barrier.\nThis demonstrates that this barrier is not an inherent limitation for\nlocally-iterative algorithms. As a result, we also achieve significant\nimprovements for dynamic, self-stabilizing and bandwidth-restricted settings:\n  - We obtain self-stabilizing distributed algorithms for\n(Delta+1)-vertex-coloring, (2Delta-1)-edge-coloring, maximal independent set\nand maximal matching with O(Delta+log^* n) time. This significantly improves\npreviously-known results that have O(n) or larger running times \\cite{GK10}.\n  - We devise a (2Delta-1)-edge-coloring algorithm in the CONGEST model with\nO(Delta + log^* n) time and in the Bit-Round model with O(Delta + log n) time.\nPreviously-known algorithms had superlinear dependency on Delta for\n(2Delta-1)-edge-coloring in these models.\n  - We obtain an arbdefective coloring algorithm with running time O(\\sqrt\nDelta + log^* n). We employ it in order to compute proper colorings that\nimprove the recent state-of-the-art bounds of Barenboim from PODC'15 \\cite{B15}\nand Fraigniaud et al. from FOCS'16 \\cite{FHK16} by polylogarithmic factors.\n  - Our algorithms are applicable to the SET-LOCAL model of \\cite{HKMS15}.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 11:51:58 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Barenboim", "Leonid", ""], ["Elkin", "Michael", ""], ["Goldenberg", "Uri", ""]]}, {"id": "1712.00343", "submitter": "Ashkbiz Danehkar PhD", "authors": "Ashkbiz Danehkar (1), Michael A. Nowak (2), Julia C. Lee (3), Randall\n  K. Smith (1) ((1) CfA, (2) MIT, (3) Harvard)", "title": "MPI_XSTAR: MPI-based Parallelization of the XSTAR Photoionization\n  Program", "comments": "5 pages, 1 figure, accepted for publication in Publications of the\n  Astronomical Society of the Pacific (PASP)", "journal-ref": "Publ.Astron.Soc.Pac.130:024501,2018", "doi": "10.1088/1538-3873/aa9dff", "report-no": null, "categories": "astro-ph.HE astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a program for the parallel implementation of multiple runs of\nXSTAR, a photoionization code that is used to predict the physical properties\nof an ionized gas from its emission and/or absorption lines. The\nparallelization program, called MPI_XSTAR, has been developed and implemented\nin the C++ language by using the Message Passing Interface (MPI) protocol, a\nconventional standard of parallel computing. We have benchmarked parallel\nmultiprocessing executions of XSTAR, using MPI_XSTAR, against a serial\nexecution of XSTAR, in terms of the parallelization speedup and the computing\nresource efficiency. Our experience indicates that the parallel execution runs\nsignificantly faster than the serial execution, however, the efficiency in\nterms of the computing resource usage decreases with increasing the number of\nprocessors used in the parallel computing.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:02:06 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Danehkar", "Ashkbiz", "", "CfA"], ["Nowak", "Michael A.", "", "MIT"], ["Lee", "Julia C.", "", "Harvard"], ["Smith", "Randall K.", "", "CfA"]]}, {"id": "1712.00423", "submitter": "Jerome Soumagne", "authors": "M. Scot Breitenfeld, Neil Fortner, Jordan Henderson, Jerome Soumagne,\n  Mohamad Chaarawi, Johann Lombardi, Quincey Koziol", "title": "DAOS for Extreme-scale Systems in Scientific Applications", "comments": "Submitted to HiPC-2017 on Jun 30 2017, accepted for publication on\n  Sep 8 2017, withdrawn on Oct 20 2017 b/c no author was able to present", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exascale I/O initiatives will require new and fully integrated I/O models\nwhich are capable of providing straightforward functionality, fault tolerance\nand efficiency. One solution is the Distributed Asynchronous Object Storage\n(DAOS) technology, which is primarily designed to handle the next generation\nNVRAM and NVMe technologies envisioned for providing a high bandwidth/IOPS\nstorage tier close to the compute nodes in an HPC system. In conjunction with\nDAOS, the HDF5 library, an I/O library for scientific applications, will\nsupport end-to-end data integrity, fault tolerance, object mapping, index\nbuilding and querying. This paper details the implementation and performance of\nthe HDF5 library built over DAOS by using three representative scientific\napplication codes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 17:31:50 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Breitenfeld", "M. Scot", ""], ["Fortner", "Neil", ""], ["Henderson", "Jordan", ""], ["Soumagne", "Jerome", ""], ["Chaarawi", "Mohamad", ""], ["Lombardi", "Johann", ""], ["Koziol", "Quincey", ""]]}, {"id": "1712.00605", "submitter": "Anshu Shukla", "authors": "Anshu Shukla and Yogesh Simmhan", "title": "Toward Reliable and Rapid Elasticity for Streaming Dataflows on Clouds", "comments": "11 pages", "journal-ref": "Proceedings of the IEEE 38th International Conference on\n  Distributed Computing Systems (ICDCS), Vienna, Austria, 2018", "doi": "10.1109/ICDCS.2018.00109", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive availability of streaming data is driving interest in\ndistributed Fast Data platforms for streaming applications. Such\nlatency-sensitive applications need to respond to dynamism in the input rates\nand task behavior using scale-in and -out on elastic Cloud resources. Platforms\nlike Apache Storm do not provide robust capabilities for responding to such\ndynamism and for rapid task migration across VMs. We propose several dataflow\ncheckpoint and migration approaches that allow a running streaming dataflow to\nmigrate, without any loss of in-flight messages or their internal tasks states,\nwhile reducing the time to recover and stabilize. We implement and evaluate\nthese migration strategies on Apache Storm using micro and application\ndataflows for scaling in and out on up to 2-21 Azure VMs. Our results show that\nwe can migrate dataflows of large sizes within 50 sec, in comparison to Storm's\ndefault approach that takes over $100~sec$. We also find that our approaches\nstabilize the application much earlier and there is no failure and\nre-processing of messages.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 13:00:36 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Shukla", "Anshu", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1712.00994", "submitter": "Francesco Conti", "authors": "Paolo Meloni, Alessandro Capotondi, Gianfranco Deriu, Michele Brian,\n  Francesco Conti, Davide Rossi, Luigi Raffo, Luca Benini", "title": "NEURAghe: Exploiting CPU-FPGA Synergies for Efficient and Flexible CNN\n  Inference Acceleration on Zynq SoCs", "comments": "22 pages, 14 figures, submitted to ACM Transactions on Reconfigurable\n  Technology and Systems", "journal-ref": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 11\n  No. 3 (2018), Article 18", "doi": "10.1145/3284357", "report-no": null, "categories": "cs.NE cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) obtain outstanding results in tasks\nthat require human-level understanding of data, like image or speech\nrecognition. However, their computational load is significant, motivating the\ndevelopment of CNN-specialized accelerators. This work presents NEURAghe, a\nflexible and efficient hardware/software solution for the acceleration of CNNs\non Zynq SoCs. NEURAghe leverages the synergistic usage of Zynq ARM cores and of\na powerful and flexible Convolution-Specific Processor deployed on the\nreconfigurable logic. The Convolution-Specific Processor embeds both a\nconvolution engine and a programmable soft core, releasing the ARM processors\nfrom most of the supervision duties and allowing the accelerator to be\ncontrolled by software at an ultra-fine granularity. This methodology opens the\nway for cooperative heterogeneous computing: while the accelerator takes care\nof the bulk of the CNN workload, the ARM cores can seamlessly execute\nhard-to-accelerate parts of the computational graph, taking advantage of the\nNEON vector engines to further speed up computation. Through the companion\nNeuDNN SW stack, NEURAghe supports end-to-end CNN-based classification with a\npeak performance of 169 Gops/s, and an energy efficiency of 17 Gops/W. Thanks\nto our heterogeneous computing model, our platform improves upon the\nstate-of-the-art, achieving a frame rate of 5.5 fps on the end-to-end execution\nof VGG-16, and 6.6 fps on ResNet-18.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 10:41:53 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Meloni", "Paolo", ""], ["Capotondi", "Alessandro", ""], ["Deriu", "Gianfranco", ""], ["Brian", "Michele", ""], ["Conti", "Francesco", ""], ["Rossi", "Davide", ""], ["Raffo", "Luigi", ""], ["Benini", "Luca", ""]]}, {"id": "1712.01044", "submitter": "Trevor Brown", "authors": "Trevor Brown", "title": "Reclaiming memory for lock-free data structures: there has to be a\n  better way", "comments": "27 pages, full version of paper published at PODC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory reclamation for lock-based data structures is typically easy. However,\nit is a significant challenge for lock-free data structures. Automatic\ntechniques such as garbage collection are inefficient or use locks, and\nnon-automatic techniques either have high overhead, or do not work for many\ndata structures. For example, subtle problems can arise when hazard pointers,\none of the most common non-automatic techniques, are applied to many lock-free\ndata structures. Epoch based reclamation (EBR), which is by far the most\nefficient non-automatic technique, allows the number of unreclaimed objects to\ngrow without bound, because one crashed process can prevent all other processes\nfrom reclaiming memory.\n  We develop a more efficient, distributed variant of EBR that solves this\nproblem. It is based on signaling, which is provided by many operating systems,\nsuch as Linux and UNIX. Our new scheme takes $O(1)$ amortized steps per\nhigh-level operation on the data structure and $O(1)$ steps in the worst case\neach time an object is removed from the data structure. At any point, $O(mn^2)$\nobjects are waiting to be freed, where $n$ is the number of processes and $m$\nis a small constant for most data structures. Experiments show that our scheme\nhas very low overhead: on average 10\\%, and at worst 28\\%, for a balanced\nbinary search tree over many thread counts, operation mixes and contention\nlevels. Our scheme also outperforms a highly tuned implementation of hazard\npointers by an average of 75\\%.\n  Typically, memory reclamation is tightly woven into lock-free data structure\ncode. To improve modularity and facilitate the comparison of different memory\nreclamation schemes, we also introduce a highly flexible abstraction. It allows\na programmer to easily interchange schemes for reclamation, object pooling,\nallocation and deallocation with virtually no overhead, by changing a single\nline of code.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 12:44:28 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Brown", "Trevor", ""]]}, {"id": "1712.01139", "submitter": "Eylon Yogev", "authors": "Merav Parter and Eylon Yogev", "title": "Distributed Algorithms Made Secure: A Graph Theoretic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of distributed graph algorithms a number of network's entities\nwith local views solve some computational task by exchanging messages with\ntheir neighbors. Quite unfortunately, an inherent property of most existing\ndistributed algorithms is that throughout the course of their execution, the\nnodes get to learn not only their own output but rather learn quite a lot on\nthe inputs or outputs of many other entities. This leakage of information might\nbe a major obstacle in settings where the output (or input) of network's\nindividual is a private information. In this paper, we introduce a new\nframework for \\emph{secure distributed graph algorithms} and provide the first\n\\emph{general compiler} that takes any \"natural\" non-secure distributed\nalgorithm that runs in $r$ rounds, and turns it into a secure algorithm that\nruns in $\\widetilde{O}(r \\cdot D \\cdot poly(\\Delta))$ rounds where $\\Delta$ is\nthe maximum degree in the graph and $D$ is its diameter. The security of the\ncompiled algorithm is information-theoretic but holds only against a\nsemi-honest adversary that controls a single node in the network.\n  This compiler is made possible due to a new combinatorial structure called\n\\emph{private neighborhood trees}: a collection of $n$ trees\n$T(u_1),\\ldots,T(u_n)$, one for each vertex $u_i \\in V(G)$, such that each tree\n$T(u_i)$ spans the neighbors of $u_i$ {\\em without going through $u_i$}.\nIntuitively, each tree $T(u_i)$ allows all neighbors of $u_i$ to exchange a\n\\emph{secret} that is hidden from $u_i$, which is the basic graph\ninfrastructure of the compiler. In a $(d,c)$-private neighborhood trees each\ntree $T(u_i)$ has depth at most $d$ and each edge $e \\in G$ appears in at most\n$c$ different trees. We show a construction of private neighborhood trees with\n$d=\\widetilde{O}(\\Delta \\cdot D)$ and $c=\\widetilde{O}(D)$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 15:08:38 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 11:50:03 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 18:39:42 GMT"}, {"version": "v4", "created": "Sun, 9 Dec 2018 12:49:29 GMT"}, {"version": "v5", "created": "Sun, 13 Jan 2019 20:28:17 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Parter", "Merav", ""], ["Yogev", "Eylon", ""]]}, {"id": "1712.01197", "submitter": "Sandor P. Fekete", "authors": "Aaron T. Becker, Erik D. Demaine, S\\'andor P. Fekete, Jarrett\n  Lonsforda, Rose Morris-Wright", "title": "Particle Computation: Complexity, Algorithms, and Logic", "comments": "27 pages, 19 figures, full version that combines three previous\n  conference articles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CG cs.DC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate algorithmic control of a large swarm of mobile particles (such\nas robots, sensors, or building material) that move in a 2D workspace using a\nglobal input signal (such as gravity or a magnetic field). We show that a maze\nof obstacles to the environment can be used to create complex systems. We\nprovide a wide range of results for a wide range of questions. These can be\nsubdivided into external algorithmic problems, in which particle configurations\nserve as input for computations that are performed elsewhere, and internal\nlogic problems, in which the particle configurations themselves are used for\ncarrying out computations. For external algorithms, we give both negative and\npositive results. If we are given a set of stationary obstacles, we prove that\nit is NP-hard to decide whether a given initial configuration of unit-sized\nparticles can be transformed into a desired target configuration. Moreover, we\nshow that finding a control sequence of minimum length is PSPACE-complete. We\nalso work on the inverse problem, providing constructive algorithms to design\nworkspaces that efficiently implement arbitrary permutations between different\nconfigurations. For internal logic, we investigate how arbitrary computations\ncan be implemented. We demonstrate how to encode dual-rail logic to build a\nuniversal logic gate that concurrently evaluates and, nand, nor, and or\noperations. Using many of these gates and appropriate interconnects, we can\nevaluate any logical expression. However, we establish that simulating the full\nrange of complex interactions present in arbitrary digital circuits encounters\na fundamental difficulty: a fan-out gate cannot be generated. We resolve this\nmissing component with the help of 2x1 particles, which can create fan-out\ngates that produce multiple copies of the inputs. Using these gates we provide\nrules for replicating arbitrary digital circuits.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 17:03:10 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Becker", "Aaron T.", ""], ["Demaine", "Erik D.", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Lonsforda", "Jarrett", ""], ["Morris-Wright", "Rose", ""]]}, {"id": "1712.01209", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu and Benjamin Paul Chamberlain", "title": "Speeding Up BigClam Implementation on SNAP", "comments": "To appear in 2018 Imperial College Computing Student Workshop\n  (ICCSW'18); 12 pages, 4 figures, and 3 tables", "journal-ref": "2018 Imperial College Computing Student Workshop (ICCSW 2018).\n  OpenAccess Series in Informatics (OASIcs), vol. 66, pp. 1:1-1:13", "doi": "10.4230/OASIcs.ICCSW.2018.1", "report-no": null, "categories": "cs.SI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a detailed analysis of the C++ implementation of the Cluster\nAffiliation Model for Big Networks (BigClam) on the Stanford Network Analysis\nProject (SNAP). BigClam is a popular graph mining algorithm that is capable of\nfinding overlapping communities in networks containing millions of nodes. Our\nanalysis shows a key stage of the algorithm - determining if a node belongs to\na community - dominates the runtime of the implementation, yet the computation\nis not parallelized. We show that by parallelizing computations across multiple\nthreads using OpenMP we can speed up the algorithm by 5.3 times when solving\nlarge networks for communities, while preserving the integrity of the program\nand the result.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 17:23:11 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 17:43:35 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1712.01367", "submitter": "Dahlia Malkhi", "authors": "Ittai Abraham, Guy Gueta, Dahlia Malkhi, Lorenzo Alvisi, Rama Kotla,\n  Jean-Philippe Martin", "title": "Revisiting Fast Practical Byzantine Fault Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we observe a safety violation in Zyzzyva and a liveness\nviolation in FaB. To demonstrate these issues, we require relatively simple\nscenarios, involving only four replicas, and one or two view changes. In all of\nthem, the problem is manifested already in the first log slot.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 21:21:26 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Abraham", "Ittai", ""], ["Gueta", "Guy", ""], ["Malkhi", "Dahlia", ""], ["Alvisi", "Lorenzo", ""], ["Kotla", "Rama", ""], ["Martin", "Jean-Philippe", ""]]}, {"id": "1712.01487", "submitter": "EPTCS", "authors": "Silvio Ghilardi (Universit\\`a degli Studi di Milano), Elena Pagani\n  (Universit\\`a degli Studi di Milano)", "title": "Counter Simulations via Higher Order Quantifier Elimination: a\n  preliminary report", "comments": "In Proceedings PxTP 2017, arXiv:1712.00898", "journal-ref": "EPTCS 262, 2017, pp. 39-53", "doi": "10.4204/EPTCS.262.5", "report-no": null, "categories": "cs.LO cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quite often, verification tasks for distributed systems are accomplished via\ncounter abstractions. Such abstractions can sometimes be justified via\nsimulations and bisimulations. In this work, we supply logical foundations to\nthis practice, by a specifically designed technique for second order quantifier\nelimination. Our method, once applied to specifications of verification\nproblems for parameterized distributed systems, produces integer variables\nsystems that are ready to be model-checked by current SMT-based tools. We\ndemonstrate the feasibility of the approach with a prototype implementation and\nfirst experiments.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 05:48:30 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Ghilardi", "Silvio", "", "Universit\u00e0 degli Studi di Milano"], ["Pagani", "Elena", "", "Universit\u00e0 degli Studi di Milano"]]}, {"id": "1712.01704", "submitter": "Bo Li", "authors": "Bo Li, Shuang Li, Junfeng Wu, Hongsheng Qi", "title": "Quantum Clique Gossiping", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a framework for the acceleration of quantum gossip\nalgorithms by introducing local clique operations to networks of interconnected\nqubits. Cliques are local structures in complex networks being complete\nsubgraphs. Based on cyclic permutations, clique gossiping leads to collective\nmulti-party qubit interactions. This type of algorithm can be physically\nrealized by a series of local environments using coherent methods. First of\nall, we show that at reduced states, these cliques have the same acceleration\neffects as their roles in accelerating classical gossip algorithms, which can\neven make possible finite-time convergence for suitable network structures.\nNext, for randomized selection of cliques where node updates enjoy a more\nself-organized and scalable sequencing, we show that the rate of convergence is\nprecisely improved by $\\mathcal{O}(k/n)$ at the reduced states, where $k$ is\nthe size of the cliques and $n$ is the number of qubits in the network. The\nrate of convergence at the coherent states of the overall quantum network is\nproven to be decided by the spectrum of a mean-square error evolution matrix.\nExplicit calculation of such matrix is rather challenging, nonetheless, the\neffect of cliques on the coherent states' dynamics is illustrated via numerical\nexamples. Interestingly, the use of larger quantum cliques does not necessarily\nincrease the speed of the network density aggregation, suggesting quantum\nnetwork dynamics is not entirely decided by its classical topology.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 08:24:15 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Li", "Bo", ""], ["Li", "Shuang", ""], ["Wu", "Junfeng", ""], ["Qi", "Hongsheng", ""]]}, {"id": "1712.01887", "submitter": "Song Han", "authors": "Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for\n  Distributed Training", "comments": "we find 99.9% of the gradient exchange in distributed SGD is\n  redundant; we reduce the communication bandwidth by two orders of magnitude\n  without losing accuracy. Code is available at:\n  https://github.com/synxlin/deep-gradient-compression", "journal-ref": "ICLR 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed training requires significant communication bandwidth\nfor gradient exchange that limits the scalability of multi-node training, and\nrequires expensive high-bandwidth network infrastructure. The situation gets\neven worse with distributed training on mobile devices (federated learning),\nwhich suffers from higher latency, lower throughput, and intermittent poor\nconnections. In this paper, we find 99.9% of the gradient exchange in\ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to\ngreatly reduce the communication bandwidth. To preserve accuracy during\ncompression, DGC employs four methods: momentum correction, local gradient\nclipping, momentum factor masking, and warm-up training. We have applied Deep\nGradient Compression to image classification, speech recognition, and language\nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and\nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a\ngradient compression ratio from 270x to 600x without losing accuracy, cutting\nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from\n488MB to 0.74MB. Deep gradient compression enables large-scale distributed\ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed\ntraining on mobile. Code is available at:\nhttps://github.com/synxlin/deep-gradient-compression.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:48:11 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 19:38:39 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 03:28:30 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Lin", "Yujun", ""], ["Han", "Song", ""], ["Mao", "Huizi", ""], ["Wang", "Yu", ""], ["Dally", "William J.", ""]]}, {"id": "1712.02029", "submitter": "Aditya Devarakonda", "authors": "Aditya Devarakonda, Maxim Naumov, Michael Garland", "title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks with Stochastic Gradient Descent, or its\nvariants, requires careful choice of both learning rate and batch size. While\nsmaller batch sizes generally converge in fewer training epochs, larger batch\nsizes offer more parallelism and hence better computational efficiency. We have\ndeveloped a new training approach that, rather than statically choosing a\nsingle batch size for all epochs, adaptively increases the batch size during\nthe training process. Our method delivers the convergence rate of small batch\nsizes while achieving performance similar to large batch sizes. We analyse our\napproach using the standard AlexNet, ResNet, and VGG networks operating on the\npopular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate\nthat learning with adaptive batch sizes can improve performance by factors of\nup to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1%\nrelative to training with fixed batch sizes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 04:19:14 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 04:26:45 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Devarakonda", "Aditya", ""], ["Naumov", "Maxim", ""], ["Garland", "Michael", ""]]}, {"id": "1712.02293", "submitter": "Ryan Kim", "authors": "Wonje Choi, Karthi Duraisamy, Ryan Gary Kim, Janardhan Rao Doppa,\n  Partha Pratim Pande, Diana Marculescu, Radu Marculescu", "title": "On-Chip Communication Network for Efficient Training of Deep\n  Convolutional Networks on Heterogeneous Manycore Systems", "comments": "Accepted in a future publication of IEEE Transactions on Computers", "journal-ref": null, "doi": "10.1109/TC.2017.2777863", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have shown a great deal of success in\ndiverse application domains including computer vision, speech recognition, and\nnatural language processing. However, as the size of datasets and the depth of\nneural network architectures continue to grow, it is imperative to design\nhigh-performance and energy-efficient computing hardware for training CNNs. In\nthis paper, we consider the problem of designing specialized CPU-GPU based\nheterogeneous manycore systems for energy-efficient training of CNNs. It has\nalready been shown that the typical on-chip communication infrastructures\nemployed in conventional CPU-GPU based heterogeneous manycore platforms are\nunable to handle both CPU and GPU communication requirements efficiently. To\naddress this issue, we first analyze the on-chip traffic patterns that arise\nfrom the computational processes associated with training two deep CNN\narchitectures, namely, LeNet and CDBNet, to perform image classification. By\nleveraging this knowledge, we design a hybrid Network-on-Chip (NoC)\narchitecture, which consists of both wireline and wireless links, to improve\nthe performance of CPU-GPU based heterogeneous manycore platforms running the\nabove-mentioned CNN training workloads. The proposed NoC achieves 1.8x\nreduction in network latency and improves the network throughput by a factor of\n2.2 for training CNNs, when compared to a highly-optimized wireline mesh NoC.\nFor the considered CNN workloads, these network-level improvements translate\ninto 25% savings in full-system energy-delay-product (EDP). This demonstrates\nthat the proposed hybrid NoC for heterogeneous manycore architectures is\ncapable of significantly accelerating training of CNNs while remaining\nenergy-efficient.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:35:47 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Choi", "Wonje", ""], ["Duraisamy", "Karthi", ""], ["Kim", "Ryan Gary", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""], ["Marculescu", "Diana", ""], ["Marculescu", "Radu", ""]]}, {"id": "1712.02474", "submitter": "Ryan Killick", "authors": "Jurek Czyzowicz, Ryan Killick, Evangelos Kranakis, Danny Krizanc,\n  Oscar Morale-Ponce", "title": "Gathering in the Plane of Location-Aware Robots in the Presence of Spies", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of mobile robots (represented as points) is distributed in the\nCartesian plane. The collection contains an unknown subset of byzantine robots\nwhich are indistinguishable from the reliable ones. The reliable robots need to\ngather, i.e., arrive to a configuration in which at the same time, all of them\noccupy the same point on the plane. The robots are equipped with GPS devices\nand at the beginning of the gathering process they communicate the Cartesian\ncoordinates of their respective positions to the central authority. On the\nbasis of this information, without the knowledge of which robots are faulty,\nthe central authority designs a trajectory for every robot. The central\nauthority aims to provide the trajectories which result in the shortest\npossible gathering time of the healthy robots. The efficiency of a gathering\nstrategy is measured by its competitive ratio, i.e., the maximal ratio between\nthe time required for gathering achieved by the given trajectories and the\noptimal time required for gathering in the offline case, i.e., when the faulty\nrobots are known to the central authority in advance. The role of the byzantine\nrobots, controlled by the adversary, is to act so that the gathering is delayed\nand the resulting competitive ratio is maximized.\n  The objective of our paper is to propose efficient algorithms when the\ncentral authority is aware of an upper bound on the number of byzantine robots.\nWe give optimal algorithms for collections of robots known to contain at most\none faulty robot. When the proportion of byzantine robots is known to be less\nthan one half or one third, we provide algorithms with small constant\ncompetitive ratios. We also propose algorithms with bounded competitive ratio\nin the case where the proportion of faulty robots is arbitrary.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 02:31:12 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Czyzowicz", "Jurek", ""], ["Killick", "Ryan", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""], ["Morale-Ponce", "Oscar", ""]]}, {"id": "1712.02533", "submitter": "Marcin Copik", "authors": "Marcin Copik", "title": "Parallel Prefix Algorithms for the Registration of Arbitrarily Long\n  Electron Micrograph Series", "comments": "Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the technology of transmission electron microscopy have\nallowed for a more precise visualization of materials and physical processes,\nsuch as metal oxidation. Nevertheless, the quality of information is limited by\nthe damage caused by an electron beam, movement of the specimen or other\nenvironmental factors. A novel registration method has been proposed to remove\nthose limitations by acquiring a series of low dose microscopy frames and\nperforming a computational registration on them to understand and visualize the\nsample. This process can be represented as a prefix sum with a complex and\ncomputationally intensive binary operator and a parallelization is necessary to\nenable processing long series of microscopy images. With our parallelization\nscheme, the time of registration of results from ten seconds of microscopy\nacquisition has been decreased from almost thirteen hours to less than seven\nminutes on 512 Intel IvyBridge cores.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 08:20:43 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Copik", "Marcin", ""]]}, {"id": "1712.02546", "submitter": "Lu\\'is Alexandre", "authors": "Jose Marques, Gabriel Falcao, Lu\\'is A. Alexandre", "title": "Distributed learning of CNNs on heterogeneous CPU/GPU architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have shown to be powerful classification\ntools in tasks that range from check reading to medical diagnosis, reaching\nclose to human perception, and in some cases surpassing it. However, the\nproblems to solve are becoming larger and more complex, which translates to\nlarger CNNs, leading to longer training times that not even the adoption of\nGraphics Processing Units (GPUs) could keep up to. This problem is partially\nsolved by using more processing units and distributed training methods that are\noffered by several frameworks dedicated to neural network training. However,\nthese techniques do not take full advantage of the possible parallelization\noffered by CNNs and the cooperative use of heterogeneous devices with different\nprocessing capabilities, clock speeds, memory size, among others. This paper\npresents a new method for the parallel training of CNNs that can be considered\nas a particular instantiation of model parallelism, where only the\nconvolutional layer is distributed. In fact, the convolutions processed during\ntraining (forward and backward propagation included) represent from $60$-$90$\\%\nof global processing time. The paper analyzes the influence of network size,\nbandwidth, batch size, number of devices, including their processing\ncapabilities, and other parameters. Results show that this technique is capable\nof diminishing the training time without affecting the classification\nperformance for both CPUs and GPUs. For the CIFAR-10 dataset, using a CNN with\ntwo convolutional layers, and $500$ and $1500$ kernels, respectively, best\nspeedups achieve $3.28\\times$ using four CPUs and $2.45\\times$ with three GPUs.\nModern imaging datasets, larger and more complex than CIFAR-10 will certainly\nrequire more than $60$-$90$\\% of processing time calculating convolutions, and\nspeedups will tend to increase accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 09:22:50 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Marques", "Jose", ""], ["Falcao", "Gabriel", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "1712.02899", "submitter": "Rajkumar Buyya", "authors": "Sukhpal Singh Gill and Rajkumar Buyya", "title": "A Taxonomy and Future Directions for Sustainable Cloud Computing: 360\n  Degree View", "comments": "68 pages, 38 figures, ACM Computing Surveys, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cloud computing paradigm offers on-demand services over the Internet and\nsupports a wide variety of applications. With the recent growth of Internet of\nThings (IoT) based applications the usage of cloud services is increasing\nexponentially. The next generation of cloud computing must be energy-efficient\nand sustainable to fulfil the end-user requirements which are changing\ndynamically. Presently, cloud providers are facing challenges to ensure the\nenergy efficiency and sustainability of their services. The usage of large\nnumber of cloud datacenters increases cost as well as carbon footprints, which\nfurther effects the sustainability of cloud services. In this paper, we propose\na comprehensive taxonomy of sustainable cloud computing. The taxonomy is used\nto investigate the existing techniques for sustainability that need careful\nattention and investigation as proposed by several academic and industry\ngroups. Further, the current research on sustainable cloud computing is\norganized into several categories: application design, sustainability metrics,\ncapacity planning, energy management, virtualization, thermal-aware scheduling,\ncooling management, renewable energy and waste heat utilization. The existing\ntechniques have been compared and categorized based on the common\ncharacteristics and properties. A conceptual model for sustainable cloud\ncomputing has been proposed along with discussion on future research\ndirections.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 00:50:30 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 05:02:58 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Gill", "Sukhpal Singh", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1712.02944", "submitter": "Tevfik Kosar", "authors": "Asif Imran, Md S Q Zulkar Nine, Kemal Guner, Tevfik Kosar", "title": "OneDataShare: A Vision for Cloud-hosted Data Transfer Scheduling and\n  Optimization as a Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast, reliable, and efficient data transmission across wide-area networks is\na predominant bottleneck for data-intensive cloud applications. This paper\nintroduces OneDataShare, which is designed to eliminate the issues plaguing\neffective cloud-based data transfers of varying file sizes and across\nincompatible transfer end-points. The vision of OneDataShare is to achieve\nhigh-speed data communication, interoperability between multiple transfer\nprotocols, and accurate estimation of delivery time for advance planning,\nthereby maximizing user-profit through improved and faster data analysis for\nbusiness intelligence. The paper elaborates on the desirable features of\nOneDataShare as a cloud-hosted data transfer scheduling and optimization\nservice, and how it is aligned with the vision of harnessing the power of the\ncloud and distributed computing. Experimental evaluation and comparison with\nexisting real-life file transfer services show that the transfer throughout\nachieved by OneDataShare is 6.5 times greater.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 05:24:56 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Imran", "Asif", ""], ["Nine", "Md S Q Zulkar", ""], ["Guner", "Kemal", ""], ["Kosar", "Tevfik", ""]]}, {"id": "1712.03112", "submitter": "Tim Besard", "authors": "Tim Besard, Christophe Foket, Bjorn De Sutter", "title": "Effective Extensible Programming: Unleashing Julia on GPUs", "comments": null, "journal-ref": null, "doi": "10.1109/TPDS.2018.2872064", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs and other accelerators are popular devices for accelerating\ncompute-intensive, parallelizable applications. However, programming these\ndevices is a difficult task. Writing efficient device code is challenging, and\nis typically done in a low-level programming language. High-level languages are\nrarely supported, or do not integrate with the rest of the high-level language\necosystem. To overcome this, we propose compiler infrastructure to efficiently\nadd support for new hardware or environments to an existing programming\nlanguage.\n  We evaluate our approach by adding support for NVIDIA GPUs to the Julia\nprogramming language. By integrating with the existing compiler, we\nsignificantly lower the cost to implement and maintain the new compiler, and\nfacilitate reuse of existing application code. Moreover, use of the high-level\nJulia programming language enables new and dynamic approaches for GPU\nprogramming. This greatly improves programmer productivity, while maintaining\napplication performance similar to that of the official NVIDIA CUDA toolkit.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 15:02:29 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Besard", "Tim", ""], ["Foket", "Christophe", ""], ["De Sutter", "Bjorn", ""]]}, {"id": "1712.03254", "submitter": "David Carrera", "authors": "Nicola Cadenelli, Jorda Polo, David Carrera", "title": "Accelerating K-mer Frequency Counting with GPU and Non-Volatile Memory", "comments": "Submitted to the 19th IEEE International Conference on high\n  Performance Computing and Communication (HPC 2017). Partially funded by\n  European Research Council (ERC) under the European Union's Horizon 2020\n  research and innovation programme (grant agreement No 639595) - HiEST Project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Next Generation Sequencing (NGS) platforms has increased the\nthroughput of genomic sequencing and in turn the amount of data that needs to\nbe processed, requiring highly efficient computation for its analysis. In this\ncontext, modern architectures including accelerators and non-volatile memory\nare essential to enable the mass exploitation of these bioinformatics\nworkloads. This paper presents a redesign of the main component of a\nstate-of-the-art reference-free method for variant calling, SMUFIN, which has\nbeen adapted to make the most of GPUs and NVM devices. SMUFIN relies on\ncounting the frequency of \\textit{k-mers} (substrings of length $k$) in DNA\nsequences, which also constitutes a well-known problem for many bioinformatics\nworkloads, such as genome assembly. We propose techniques to improve the\nefficiency of k-mer counting and to scale-up workloads like \\sm that used to\nrequire 16 nodes of \\mn to a single machine with a GPU and NVM drives. Results\nshow that although the single machine is not able to improve the time to\nsolution of 16 nodes, its CPU time is 7.5x shorter than the aggregate CPU time\nof the 16 nodes, with a reduction in energy consumption of 5.5x.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:49:40 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Cadenelli", "Nicola", ""], ["Polo", "Jorda", ""], ["Carrera", "David", ""]]}, {"id": "1712.03366", "submitter": "Panagiotis Michailidis", "authors": "Panagiotis D. Michailidis", "title": "An Efficient Multi-core Implementation of the Jaya Optimisation\n  Algorithm", "comments": "This is an Accepted Manuscript of an article published by Taylor &\n  Francis Group in the International Journal of Parallel, Emergent &\n  Distributed Systems", "journal-ref": null, "doi": "10.1080/17445760.2017.1416387", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we propose a hybrid parallel Jaya optimisation algorithm for a\nmulti-core environment with the aim of solving large-scale global optimisation\nproblems. The proposed algorithm is called HHCPJaya, and combines the\nhyper-population approach with the hierarchical cooperation search mechanism.\nThe HHCPJaya algorithm divides the population into many small subpopulations,\neach of which focuses on a distinct block of the original population\ndimensions. In the hyper-population approach, we increase the small\nsubpopulations by assigning more than one subpopulation to each core, and each\nsubpopulation evolves independently to enhance the explorative and exploitative\nnature of the population. We combine this hyper-population approach with the\ntwo-level hierarchical cooperative search scheme to find global solutions from\nall subpopulations. Furthermore, we incorporate an additional updating phase on\nthe respective subpopulations based on global solutions, with the aim of\nfurther improving the convergence rate and the quality of solutions. Several\nexperiments applying the proposed parallel algorithm in different settings\nprove that it demonstrates sufficient promise in terms of the quality of\nsolutions and the convergence rate. Furthermore, a relatively small\ncomputational effort is required to solve complex and large-scale optimisation\nproblems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 09:50:59 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Michailidis", "Panagiotis D.", ""]]}, {"id": "1712.03530", "submitter": "Mohammad Noormohammadpour", "authors": "Mohammad Noormohammadpour and Cauligi S. Raghavendra", "title": "Datacenter Traffic Control: Understanding Techniques and Trade-offs", "comments": "Accepted for Publication in IEEE Communications Surveys and Tutorials", "journal-ref": "IEEE Communications Surveys Tutorials 20 (2018) 1492-1525", "doi": "10.1109/COMST.2017.2782753", "report-no": null, "categories": "cs.NI cs.DC cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datacenters provide cost-effective and flexible access to scalable compute\nand storage resources necessary for today's cloud computing needs. A typical\ndatacenter is made up of thousands of servers connected with a large network\nand usually managed by one operator. To provide quality access to the variety\nof applications and services hosted on datacenters and maximize performance, it\ndeems necessary to use datacenter networks effectively and efficiently.\nDatacenter traffic is often a mix of several classes with different priorities\nand requirements. This includes user-generated interactive traffic, traffic\nwith deadlines, and long-running traffic. To this end, custom transport\nprotocols and traffic management techniques have been developed to improve\ndatacenter network performance.\n  In this tutorial paper, we review the general architecture of datacenter\nnetworks, various topologies proposed for them, their traffic properties,\ngeneral traffic control challenges in datacenters and general traffic control\nobjectives. The purpose of this paper is to bring out the important\ncharacteristics of traffic control in datacenters and not to survey all\nexisting solutions (as it is virtually impossible due to massive body of\nexisting research). We hope to provide readers with a wide range of options and\nfactors while considering a variety of traffic control mechanisms. We discuss\nvarious characteristics of datacenter traffic control including management\nschemes, transmission control, traffic shaping, prioritization, load balancing,\nmultipathing, and traffic scheduling. Next, we point to several open challenges\nas well as new and interesting networking paradigms. At the end of this paper,\nwe briefly review inter-datacenter networks that connect geographically\ndispersed datacenters which have been receiving increasing attention recently\nand pose interesting and novel research problems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 14:03:16 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Noormohammadpour", "Mohammad", ""], ["Raghavendra", "Cauligi S.", ""]]}, {"id": "1712.03560", "submitter": "Filippo Maria Bianchi", "authors": "Alessandro Cinti, Filippo Maria Bianchi, Alessio Martino, Antonello\n  Rizzi", "title": "A novel algorithm for online inexact string matching and its FPGA\n  implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating inexact string matching procedures is of utmost importance when\ndealing with practical applications where huge amount of data must be processed\nin real time, as usual in bioinformatics or cybersecurity. Inexact matching\nprocedures can yield multiple shadow hits, which must be filtered, according to\nsome criterion, to obtain a concise and meaningful list of occurrences. The\nfiltering procedures are often computationally demanding and are performed\noffline in a post-processing phase. This paper introduces a novel algorithm for\nOnline Approximate String Matching (OASM) able to filter shadow hits on the\nfly, according to general purpose priority rules that greedily assign\npriorities to overlapping hits. An FPGA hardware implementation of OASM is\nproposed and compared with a serial software version. Even when implemented on\nentry level FPGAs, the proposed procedure can reach a high degree of\nparallelism and superior performance in time compared to the software\nimplementation, while keeping low the usage of logic elements. This makes the\ndeveloped architecture very competitive in terms of both performance and cost\nof the overall computing system.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 17:04:17 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 15:26:51 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Cinti", "Alessandro", ""], ["Bianchi", "Filippo Maria", ""], ["Martino", "Alessio", ""], ["Rizzi", "Antonello", ""]]}, {"id": "1712.03659", "submitter": "Hoang Dinh Thai DTH", "authors": "Kongrath Suankaewmanee, Dinh Thai Hoang, Dusit Niyato, Suttinee\n  Sawadsitang, Ping Wang, and Zhu Han", "title": "Performance Analysis and Application of Mobile Blockchain", "comments": "6 pages, 10 figures, IEEE ICNC Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile security has become more and more important due to the boom of mobile\ncommerce (m-commerce). However, the development of m-commerce is facing many\nchallenges regarding data security problems. Recently, blockchain has been\nintroduced as an effective security solution deployed successfully in many\napplications in practice, such as, Bitcoin, cloud computing, and\nInternet-of-Things. However, the blockchain technology has not been adopted and\nimplemented widely in m-commerce because its mining processes usually require\nto be performed on standard computing units, e.g., computers. Therefore, in\nthis paper, we introduce a new m-commerce application using blockchain\ntechnology, namely, MobiChain, to secure transactions in the m-commerce.\nEspecially, in the MobiChain application, the mining processes can be executed\nefficiently on mobile devices using our proposed Android core module. Through\nreal experiments, we evaluate the performance of the proposed model and show\nthat blockchain will be an efficient security solution for future m-commerce.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 06:49:40 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Suankaewmanee", "Kongrath", ""], ["Hoang", "Dinh Thai", ""], ["Niyato", "Dusit", ""], ["Sawadsitang", "Suttinee", ""], ["Wang", "Ping", ""], ["Han", "Zhu", ""]]}, {"id": "1712.03660", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Basem Assiri, Paul Rosen", "title": "Parallel Mapper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of Mapper has emerged in the last decade as a powerful and\neffective topological data analysis tool that approximates and generalizes\nother topological summaries, such as the Reeb graph, the contour tree, split,\nand joint trees. In this paper, we study the parallel analysis of the\nconstruction of Mapper. We give a provably correct parallel algorithm to\nexecute Mapper on multiple processors and discuss the performance results that\ncompare our approach to a reference sequential Mapper implementation. We report\nthe performance experiments that demonstrate the efficiency of our method.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 07:02:06 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 02:32:46 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 01:56:37 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hajij", "Mustafa", ""], ["Assiri", "Basem", ""], ["Rosen", "Paul", ""]]}, {"id": "1712.04048", "submitter": "Hao Zhang", "authors": "Hao Zhang, Shizhen Xu, Graham Neubig, Wei Dai, Qirong Ho, Guangwen\n  Yang, Eric P. Xing", "title": "Cavs: A Vertex-centric Programming Interface for Dynamic Neural Networks", "comments": "Short versions of this paper were presented at AISys workshop@SOSP\n  2017 and MLSys workshop@NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning (DL) models have moved beyond static network\narchitectures to dynamic ones, handling data where the network structure\nchanges every example, such as sequences of variable lengths, trees, and\ngraphs. Existing dataflow-based programming models for DL---both static and\ndynamic declaration---either cannot readily express these dynamic models, or\nare inefficient due to repeated dataflow graph construction and processing, and\ndifficulties in batched execution. We present Cavs, a vertex-centric\nprogramming interface and optimized system implementation for dynamic DL\nmodels. Cavs represents dynamic network structure as a static vertex function\n$\\mathcal{F}$ and a dynamic instance-specific graph $\\mathcal{G}$, and performs\nbackpropagation by scheduling the execution of $\\mathcal{F}$ following the\ndependencies in $\\mathcal{G}$. Cavs bypasses expensive graph construction and\npreprocessing overhead, allows for the use of static graph optimization\ntechniques on pre-defined operations in $\\mathcal{F}$, and naturally exposes\nbatched execution opportunities over different graphs. Experiments comparing\nCavs to two state-of-the-art frameworks for dynamic NNs (TensorFlow Fold and\nDyNet) demonstrate the efficacy of this approach: Cavs achieves a near one\norder of magnitude speedup on training of various dynamic NN architectures, and\nablations demonstrate the contribution of our proposed batching and memory\nmanagement strategies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 22:04:39 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Zhang", "Hao", ""], ["Xu", "Shizhen", ""], ["Neubig", "Graham", ""], ["Dai", "Wei", ""], ["Ho", "Qirong", ""], ["Yang", "Guangwen", ""], ["Xing", "Eric P.", ""]]}, {"id": "1712.04146", "submitter": "Salman Salloum", "authors": "Salman Salloum and Yulin He and Joshua Zhexue Huang and Xiaoliang\n  Zhang and Tamer Z. Emara and Chenghao Wei and Heping He", "title": "A Random Sample Partition Data Model for Big Data Analysis", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TII.2019.2912723", "report-no": null, "categories": "cs.DC cs.DS physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data sets must be carefully partitioned into statistically similar data\nsubsets that can be used as representative samples for big data analysis tasks.\nIn this paper, we propose the random sample partition (RSP) data model to\nrepresent a big data set as a set of non-overlapping data subsets, called RSP\ndata blocks, where each RSP data block has a probability distribution similar\nto the whole big data set. Under this data model, efficient block level\nsampling is used to randomly select RSP data blocks, replacing expensive record\nlevel sampling to select sample data from a big distributed data set on a\ncomputing cluster. We show how RSP data blocks can be employed to estimate\nstatistics of a big data set and build models which are equivalent to those\nbuilt from the whole big data set. In this approach, analysis of a big data set\nbecomes analysis of few RSP data blocks which have been generated in advance on\nthe computing cluster. Therefore, the new method for data analysis based on RSP\ndata blocks is scalable to big data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 06:49:28 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 10:59:15 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Salloum", "Salman", ""], ["He", "Yulin", ""], ["Huang", "Joshua Zhexue", ""], ["Zhang", "Xiaoliang", ""], ["Emara", "Tamer Z.", ""], ["Wei", "Chenghao", ""], ["He", "Heping", ""]]}, {"id": "1712.04161", "submitter": "Ziyao Zhang", "authors": "Ziyao Zhang, Liang Ma, Kin K. Leung, Franck Le, Sastry Kompella, and\n  Leandros Tassiulas", "title": "How Better is Distributed SDN? An Analytical Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed software-defined networks (SDN), consisting of multiple\ninter-connected network domains, each managed by one SDN controller, is an\nemerging networking architecture that offers balanced centralized control and\ndistributed operations. Under such networking paradigm, most existing works\nfocus on designing sophisticated controller-synchronization strategies to\nimprove joint controller-decision-making for inter-domain routing. However,\nthere is still a lack of fundamental understanding of how the performance of\ndistributed SDN is related to network attributes, thus impossible to justify\nthe necessity of complicated strategies. In this regard, we analyze and\nquantify the performance enhancement of distributed SDN architectures,\ninfluenced by intra-/inter-domain synchronization levels and network structural\nproperties. Based on a generic weighted network model, we establish analytical\nmethods for performance estimation under four synchronization scenarios with\nincreasing synchronization cost. Moreover, two of these synchronization\nscenarios correspond to extreme cases, i.e., minimum/maximum synchronization,\nwhich are, therefore, capable of bounding the performance of distributed SDN\nwith any given synchronization levels. Our theoretical results reveal how\nnetwork performance is related to synchronization levels and inter-domain\nconnections, the accuracy of which are confirmed by simulations based on both\nreal and synthetic networks. To the best of our knowledge, this is the first\nwork quantifying the performance of distributed SDN analytically, which\nprovides fundamental guidance for future SDN protocol designs and performance\nestimation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 08:11:03 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Zhang", "Ziyao", ""], ["Ma", "Liang", ""], ["Leung", "Kin K.", ""], ["Le", "Franck", ""], ["Kompella", "Sastry", ""], ["Tassiulas", "Leandros", ""]]}, {"id": "1712.04303", "submitter": "Jayvant Anantpur", "authors": "Jayvant Anantpur, Nagendra Gulur Dwarakanath, Shivaram\n  Kalyanakrishnan, Shalabh Bhatnagar, R. Govindarajan", "title": "RLWS: A Reinforcement Learning based GPU Warp Scheduler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Streaming Multiprocessors (SMs) of a Graphics Processing Unit (GPU)\nexecute instructions from a group of consecutive threads, called warps. At each\ncycle, an SM schedules a warp from a group of active warps and can context\nswitch among the active warps to hide various stalls. Hence the performance of\nwarp scheduler is critical to the performance of GPU. Several heuristic warp\nscheduling algorithms have been proposed which work well only for the\nsituations they are designed for. GPU workloads are becoming very diverse in\nnature and hence one heuristic may not work for all cases. To work well over a\ndiverse range of workloads, which might exhibit hitherto unseen\ncharacteristics, a warp scheduling algorithm must be able to adapt on-line.\n  We propose a Reinforcement Learning based Warp Scheduler (RLWS) which learns\nto schedule warps based on the current state of the core and the long-term\nbenefits of scheduling actions, adapting not only to different types of\nworkloads, but also to different execution phases in each workload. As the\ndesign space involving the state variables and the parameters (such as learning\nand exploration rates, reward and penalty values) used by RLWS is large, we use\nGenetic Algorithm to identify the useful subset of state variables and\nparameter values. We evaluated the proposed RLWS using the GPGPU-SIM simulator\non a large number of workloads from the Rodinia, Parboil, CUDA-SDK and\nGPGPU-SIM benchmark suites and compared with other state-of-the-art warp\nscheduling methods. Our RL based implementation achieved either the best or\nvery close to the best performance in 80\\% of kernels with an average speedup\nof 1.06x over the Loose Round Robin strategy and 1.07x over the Two-Level\nstrategy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 13:57:23 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Anantpur", "Jayvant", ""], ["Dwarakanath", "Nagendra Gulur", ""], ["Kalyanakrishnan", "Shivaram", ""], ["Bhatnagar", "Shalabh", ""], ["Govindarajan", "R.", ""]]}, {"id": "1712.04322", "submitter": "Francois Berry", "authors": "Kamel Abdelouahab (IP), Maxime Pelcat (IETR), Jocelyn S\\'erot (IP),\n  C\\'edric Bourrasset (IP), Fran\\c{c}ois Berry (IP), Jocelyn Serot (LASMEA)", "title": "Tactics to Directly Map CNN graphs on Embedded FPGAs", "comments": "IEEE Embedded Systems Letters, Institute of Electrical and\n  Electronics Engineers, A Para\\^itre, pp.1 - 1. arXiv admin note: text overlap\n  with arXiv:1705.04543", "journal-ref": null, "doi": "10.1109/LES.2017.2743247", "report-no": null, "categories": "cs.DC cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are the state-of-the-art in image\nclassification. Since CNN feed forward propagation involves highly regular\nparallel computation, it benefits from a significant speed-up when running on\nfine grain parallel programmable logic devices. As a consequence, several\nstudies have proposed FPGA-based accelerators for CNNs. However, because of the\nlarge computationalpower required by CNNs, none of the previous studies has\nproposed a direct mapping of the CNN onto the physical resources of an FPGA,\nallocating each processing actor to its own hardware instance.In this paper, we\ndemonstrate the feasibility of the so called direct hardware mapping (DHM) and\ndiscuss several tactics we explore to make DHM usable in practice. As a proof\nof concept, we introduce the HADDOC2 open source tool, that automatically\ntransforms a CNN description into a synthesizable hardware description with\nplatform-independent direct hardware mapping.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 08:13:39 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Abdelouahab", "Kamel", "", "IP"], ["Pelcat", "Maxime", "", "IETR"], ["S\u00e9rot", "Jocelyn", "", "IP"], ["Bourrasset", "C\u00e9dric", "", "IP"], ["Berry", "Fran\u00e7ois", "", "IP"], ["Serot", "Jocelyn", "", "LASMEA"]]}, {"id": "1712.04344", "submitter": "Waheed Iqbal", "authors": "Hassan Nazeer, Waheed Iqbal, Fawaz Bokhari, Faisal Bukhari, Shuja Ur\n  Rehman Baig", "title": "Real-time Text Analytics Pipeline Using Open-source Big Data Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time text processing systems are required in many domains to quickly\nidentify patterns, trends, sentiments, and insights. Nowadays, social networks,\ne-commerce stores, blogs, scientific experiments, and server logs are main\nsources generating huge text data. However, to process huge text data in real\ntime requires building a data processing pipeline. The main challenge in\nbuilding such pipeline is to minimize latency to process high-throughput data.\nIn this paper, we explain and evaluate our proposed real-time text processing\npipeline using open-source big data tools which minimize the latency to process\ndata streams. Our proposed data processing pipeline is based on Apache Kafka\nfor data ingestion, Apache Spark for in-memory data processing, Apache\nCassandra for storing processed results, and D3 JavaScript library for\nvisualization. We evaluate the effectiveness of the proposed pipeline under\nvarying deployment scenarios to perform sentiment analysis using Twitter\ndataset. Our experimental evaluations show less than a minute latency to\nprocess $466,700$ Tweets in $10.7$ minutes when three virtual machines\nallocated to the proposed pipeline.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 15:11:38 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Nazeer", "Hassan", ""], ["Iqbal", "Waheed", ""], ["Bokhari", "Fawaz", ""], ["Bukhari", "Faisal", ""], ["Baig", "Shuja Ur Rehman", ""]]}, {"id": "1712.04393", "submitter": "Nayuta Yanagisawa", "authors": "Carole Delporte-Gallet, Hugues Fauconnier, Sergio Rajsbaum, Nayuta\n  Yanagisawa", "title": "A characterization of colorless anonymous $t$-resilient task\n  computability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A task is a distributed problem for $n$ processes, in which each process\nstarts with a private input value, communicates with other processes, and\neventually decides an output value. A task is colorless if each process can\nadopt the input or output value of another process. Colorless tasks are well\nstudied in the non-anonymous shared-memory model where each process has a\ndistinct identifier that can be used to access a single-writer/multi-reader\nshared register. In the anonymous case, where processes have no identifiers and\ncommunicate through multi-writer/multi-reader registers, there is a recent\ntopological characterization of the colorless tasks that are solvable when any\nnumber of asynchronous processes may crash.\n  In this paper we study the case where at most $t$ processes may crash, where\n$1 \\le t < n$. We prove that a colorless task is $t$-resilient solvable\nnon-anonymously if and only if it is $t$-resilient solvable anonymously. This\nimplies a complete characterization of colorless anonymous t-resilient\nasynchronous task computability.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 17:02:08 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Delporte-Gallet", "Carole", ""], ["Fauconnier", "Hugues", ""], ["Rajsbaum", "Sergio", ""], ["Yanagisawa", "Nayuta", ""]]}, {"id": "1712.04495", "submitter": "Blesson Varghese", "authors": "Carlos Reano, Federico Silla, Dimitrios S. Nikolopoulos and Blesson\n  Varghese", "title": "Intra-node Memory Safe GPU Co-Scheduling", "comments": "Accepted on 12 Dec 2017, IEEE Transactions on Parallel and\n  Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs in High-Performance Computing systems remain under-utilised due to the\nunavailability of schedulers that can safely schedule multiple applications to\nshare the same GPU. The research reported in this paper is motivated to improve\nthe utilisation of GPUs by proposing a framework, we refer to as schedGPU, to\nfacilitate intra-node GPU co-scheduling such that a GPU can be safely shared\namong multiple applications by taking memory constraints into account. Two\napproaches, namely a client-server and a shared memory approach are explored.\nHowever, the shared memory approach is more suitable due to lower overheads\nwhen compared to the former approach.\n  Four policies are proposed in schedGPU to handle applications that are\nwaiting to access the GPU, two of which account for priorities. The feasibility\nof schedGPU is validated on three real-world applications. The key observation\nis that a performance gain is achieved.\n  For single applications, a gain of over 10 times, as measured by GPU\nutilisation and GPU memory utilisation, is obtained. For workloads comprising\nmultiple applications, a speed-up of up to 5x in the total execution time is\nnoted. Moreover, the average GPU utilisation and average GPU memory utilisation\nis increased by 5 and 12 times, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 20:15:16 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Reano", "Carlos", ""], ["Silla", "Federico", ""], ["Nikolopoulos", "Dimitrios S.", ""], ["Varghese", "Blesson", ""]]}, {"id": "1712.04786", "submitter": "Prabhu Ramachandran", "authors": "Prabhu Ramachandran", "title": "automan: a simple, Python-based, automation framework for numerical\n  computing", "comments": null, "journal-ref": "CiSE, vol. 20, no. 5, pp. 81-97, 2018", "doi": "10.1109/MCSE.2018.05329818", "report-no": null, "categories": "cs.OH cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an easy-to-use, Python-based framework that allows a researcher to\nautomate their computational simulations. In particular the framework\nfacilitates assembling several long-running computations and producing various\nplots from the data produced by these computations. The framework makes it\npossible to reproduce every figure made for a publication with a single\ncommand. It also allows one to distribute the computations across a network of\ncomputers. The framework has been used to write research papers in numerical\ncomputing. This paper discusses the design of the framework, and the benefits\nof using it. The ideas presented are general and should help researchers\norganize their computations for better reproducibility.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 21:58:40 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 11:52:35 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Ramachandran", "Prabhu", ""]]}, {"id": "1712.04893", "submitter": "Gabor Hannak", "authors": "Gabor Hannak, Alessandro Perelli, Norbert Goertz, Gerald Matz, Mike E.\n  Davies", "title": "Performance Analysis of Approximate Message Passing for Distributed\n  Compressed Sensing", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2018.2850754", "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian approximate message passing (BAMP) is an efficient method in\ncompressed sensing that is nearly optimal in the minimum mean squared error\n(MMSE) sense. Bayesian approximate message passing (BAMP) performs joint\nrecovery of multiple vectors with identical support and accounts for\ncorrelations in the signal of interest and in the noise. In this paper, we show\nhow to reduce the complexity of vector BAMP via a simple joint decorrelation\ndiagonalization) transform of the signal and noise vectors, which also\nfacilitates the subsequent performance analysis. We prove that BAMP and the\ncorresponding state evolution (SE) are equivariant with respect to the joint\ndecorrelation transform and preserve diagonality of the residual noise\ncovariance for the Bernoulli-Gauss (BG) prior. We use these results to analyze\nthe dynamics and the mean squared error (MSE) performance of BAMP via the\nreplica method, and thereby understand the impact of signal correlation and\nnumber of jointly sparse signals.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 18:01:05 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 19:03:46 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hannak", "Gabor", ""], ["Perelli", "Alessandro", ""], ["Goertz", "Norbert", ""], ["Matz", "Gerald", ""], ["Davies", "Mike E.", ""]]}, {"id": "1712.04989", "submitter": "Ajay Singh", "authors": "Ajay Singh, Marc Shapiro and Gael Thomas", "title": "Persistent Memory Programming Abstractions in Context of Concurrent\n  Applications", "comments": "Accepted in HiPC SRS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of non-volatile memory (NVM) technologies like PCM, STT,\nmemristors and Fe-RAM is believed to enhance the system performance by getting\nrid of the traditional memory hierarchy by reducing the gap between memory and\nstorage. This memory technology is considered to have the performance like that\nof DRAM and persistence like that of disks. Thus, it would also provide\nsignificant performance benefits for big data applications by allowing\nin-memory processing of large data with the lowest latency to persistence.\nLeveraging the performance benefits of this memory-centric computing technology\nthrough traditional memory programming is not trivial and the challenges\naggravate for parallel/concurrent applications. To this end, several\nprogramming abstractions have been proposed like NVthreads, Mnemosyne and\nintel's NVML. However, deciding upon a programming abstraction which is easier\nto program and at the same time ensures the consistency and balances various\nsoftware and architectural trade-offs is openly debatable and active area of\nresearch for NVM community.\n  We study the NVthreads, Mnemosyne and NVML libraries by building a concurrent\nand persistent set and open addressed hash-table data structure application. In\nthis process, we explore and report various tradeoffs and hidden costs involved\nin building concurrent applications for persistence in terms of achieving\nefficiency, consistency and ease of programming with these NVM programming\nabstractions. Eventually, we evaluate the performance of the set and hash-table\ndata structure applications. We observe that NVML is easiest to program with\nbut is least efficient and Mnemosyne is most performance friendly but involves\nsignificant programming efforts to build concurrent and persistent\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 20:03:27 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Singh", "Ajay", ""], ["Shapiro", "Marc", ""], ["Thomas", "Gael", ""]]}, {"id": "1712.05012", "submitter": "Morad Behandish", "authors": "Pouya Tavousi, Morad Behandish, Horea T. Ilies, and Kazem Kazerounian", "title": "Protofold II: Enhanced Model and Implementation for Kinetostatic Protein\n  Folding", "comments": "Shorter versions were presented in two conference papers in ASME\n  International Design Engineering Technical Conferences (IDETC'2013)", "journal-ref": "ASME Transactions, Journal of Nanotechnology in Engineering and\n  Medicine, 6(3), p.034601, 2016", "doi": "10.1115/1.4032759", "report-no": "CDL-TR-16-03", "categories": "cs.CE cs.DC cs.DS cs.RO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable prediction of 3D protein structures from sequence data remains a\nbig challenge due to both theoretical and computational difficulties. We have\npreviously shown that our kinetostatic compliance method (KCM) implemented into\nthe Protofold package can overcome some of the key difficulties faced by other\nde novo structure prediction methods, such as the very small time steps\nrequired by the molecular dynamics (MD) approaches or the very large number of\nsamples needed by the Monte Carlo (MC) sampling techniques. In this article, we\nimprove the free energy formulation used in Protofold by including the\ntypically underrated entropic effects, imparted due to differences in\nhydrophobicity of the chemical groups, which dominate the folding of most\nwater-soluble proteins. In addition to the model enhancement, we revisit the\nnumerical implementation by redesigning the algorithms and introducing\nefficient data structures that reduce the expected complexity from quadratic to\nlinear. Moreover, we develop and optimize parallel implementations of the\nalgorithms on both central and graphics processing units (CPU/GPU) achieving\nspeed-ups up to two orders of magnitude on the GPU. Our simulations are\nconsistent with the general behavior observed in the folding process in aqueous\nsolvent, confirming the effectiveness of model improvements. We report on the\nfolding process at multiple levels; namely, the formation of secondary\nstructural elements and tertiary interactions between secondary elements or\nacross larger domains. We also observe significant enhancements in running\ntimes that make the folding simulation tractable for large molecules.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 11:36:29 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Tavousi", "Pouya", ""], ["Behandish", "Morad", ""], ["Ilies", "Horea T.", ""], ["Kazerounian", "Kazem", ""]]}, {"id": "1712.05020", "submitter": "Trevor Brown", "authors": "Trevor Brown", "title": "B-slack trees: Highly Space Efficient B-trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B-slack trees, a subclass of B-trees that have substantially better\nworst-case space complexity, are introduced. They store $n$ keys in height\n$O(\\log_b n)$, where $b$ is the maximum node degree. Updates can be performed\nin $O(\\log_{\\frac b 2} n)$ amortized time. A relaxed balance version, which is\nwell suited for concurrent implementation, is also presented.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 21:51:02 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 16:07:02 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Brown", "Trevor", ""]]}, {"id": "1712.05040", "submitter": "Jian-Jia Chen", "authors": "Niklas Ueter and Georg von der Br\\\"uggen and Jian-Jia Chen and Jing\n  Li, and Kunal Agrawal", "title": "Reservation-Based Federated Scheduling for Parallel Real-Time Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the scheduling of parallel real-time tasks with\narbitrary-deadlines. Each job of a parallel task is described as a directed\nacyclic graph (DAG). In contrast to prior work in this area, where\ndecomposition-based scheduling algorithms are proposed based on the\nDAG-structure and inter-task interference is analyzed as self-suspending\nbehavior, this paper generalizes the federated scheduling approach. We propose\na reservation-based algorithm, called reservation-based federated scheduling,\nthat dominates federated scheduling. We provide general constraints for the\ndesign of such systems and prove that reservation-based federated scheduling\nhas a constant speedup factor with respect to any optimal DAG task scheduler.\nFurthermore, the presented algorithm can be used in conjunction with any\nscheduler and scheduling analysis suitable for ordinary arbitrary-deadline\nsporadic task sets, i.e., without parallelism.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 23:14:28 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Ueter", "Niklas", ""], ["von der Br\u00fcggen", "Georg", ""], ["Chen", "Jian-Jia", ""], ["Li", "Jing", ""], ["Agrawal", "Kunal", ""]]}, {"id": "1712.05101", "submitter": "Trevor Brown", "authors": "Trevor Brown and Hillel Avni", "title": "Range Queries in Non-blocking $k$-ary Search Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a linearizable, non-blocking $k$-ary search tree ($k$-ST) that\nsupports fast searches and range queries. Our algorithm uses single-word\ncompare-and-swap (CAS) operations, and tolerates any number of crash failures.\nPerformance experiments show that, for workloads containing small range\nqueries, our $k$-ST significantly outperforms other algorithms which support\nthese operations, and rivals the performance of a leading concurrent skip-list,\nwhich provides range queries that cannot always be linearized.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 06:09:04 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Brown", "Trevor", ""], ["Avni", "Hillel", ""]]}, {"id": "1712.05233", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan, Kashish Ara Shakil and Mansaf Alam", "title": "Big Data Computing Using Cloud-Based Technologies, Challenges and Future\n  Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The excessive amounts of data generated by devices and Internet-based sources\nat a regular basis constitute, big data. This data can be processed and\nanalyzed to develop useful applications for specific domains. Several\nmathematical and data analytics techniques have found use in this sphere. This\nhas given rise to the development of computing models and tools for big data\ncomputing. However, the storage and processing requirements are overwhelming\nfor traditional systems and technologies. Therefore, there is a need for\ninfrastructures that can adjust the storage and processing capability in\naccordance with the changing data dimensions. Cloud Computing serves as a\npotential solution to this problem. However, big data computing in the cloud\nhas its own set of challenges and research issues. This chapter surveys the big\ndata concept, discusses the mathematical and data analytics techniques that can\nbe used for big data and gives taxonomy of the existing tools, frameworks and\nplatforms available for different big data computing models. Besides this, it\nalso evaluates the viability of cloud-based big data computing, examines\nexisting challenges and opportunities, and provides future research directions\nin this field.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 07:26:51 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Khan", "Samiya", ""], ["Shakil", "Kashish Ara", ""], ["Alam", "Mansaf", ""]]}, {"id": "1712.05406", "submitter": "Trevor Brown", "authors": "Trevor Brown", "title": "Techniques for Constructing Efficient Lock-free Data Structures", "comments": "PhD thesis, Univ Toronto (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a library of concurrent data structures is an essential way to\nsimplify the difficult task of developing concurrent software. Lock-free data\nstructures, in which processes can help one another to complete operations,\noffer the following progress guarantee: If processes take infinitely many\nsteps, then infinitely many operations are performed. Handcrafted lock-free\ndata structures can be very efficient, but are notoriously difficult to\nimplement. We introduce numerous tools that support the development of\nefficient lock-free data structures, and especially trees.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 18:25:12 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Brown", "Trevor", ""]]}, {"id": "1712.05500", "submitter": "Siamak Taati", "authors": "Ir\\`ene Marcovici, Mathieu Sablik, Siamak Taati", "title": "Ergodicity of some classes of cellular automata subject to noise", "comments": "38 pages, 10 figures; Improved presentation in few places, added to\n  bibliography, corrected the statement of Lemma 4.5", "journal-ref": "Electronic Journal of Probability, Volume 24 (2019), paper no. 41,\n  44 pp", "doi": "10.1214/19-EJP297", "report-no": null, "categories": "math.PR cs.DC math.DS nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular automata (CA) are dynamical systems on symbolic configurations on\nthe lattice. They are also used as models of massively parallel computers. As\ndynamical systems, one would like to understand the effect of small random\nperturbations on the dynamics of CA. As models of computation, they can be used\nto study the reliability of computation against noise.\n  We consider various families of CA (nilpotent, permutive, gliders, CA with a\nspreading symbol, surjective, algebraic) and prove that they are highly\nunstable against noise, meaning that they forget their initial conditions under\nslightest positive noise. This is manifested as the ergodicity of the resulting\nprobabilistic CA. The proofs involve a collection of different techniques\n(couplings, entropy, Fourier analysis), depending on the dynamical properties\nof the underlying deterministic CA and the type of noise.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 01:29:49 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 16:21:52 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 09:36:47 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Marcovici", "Ir\u00e8ne", ""], ["Sablik", "Mathieu", ""], ["Taati", "Siamak", ""]]}, {"id": "1712.05554", "submitter": "Shilu Chang", "authors": "Yi Liang, Shilu Chang and Chao Su", "title": "A Workload-Specific Memory Capacity Configuration Approach for In-Memory\n  Data Analytic Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose WSMC, a workload-specific memory capacity configuration approach\nfor the Spark workloads, which guides users on the memory capacity\nconfiguration with the accurate prediction of the workload's memory requirement\nunder various input data size and parameter settings.First, WSMC classifies the\nin-memory computing workloads into four categories according to the workloads'\nData Expansion Ratio. Second, WSMC establishes a memory requirement prediction\nmodel with the consideration of the input data size, the shuffle data size, the\nparallelism of the workloads and the data block size. Finally, for each\nworkload category, WSMC calculates the shuffle data size in the prediction\nmodel in a workload-specific way. For the ad-hoc workload, WSMC can profile its\nData Expansion Ratio with small-sized input data and decide the category that\nthe workload falls into. Users can then determine the accurate configuration in\naccordance with the corresponding memory requirement prediction.Through the\ncomprehensive evaluations with SparkBench workloads, we found that, contrasting\nwith the default configuration, configuration with the guide of WSMC can save\nover 40% memory capacity with the workload performance slight degradation (only\n5%), and compared to the proper configuration found out manually, the\nconfiguration with the guide of WSMC leads to only 7% increase in the memory\nwaste with the workload's performance slight improvement (about 1%)\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 06:25:05 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Liang", "Yi", ""], ["Chang", "Shilu", ""], ["Su", "Chao", ""]]}, {"id": "1712.05878", "submitter": "M. Spiropulu", "authors": "Dustin Anderson, Jean-Roch Vlimant and Maria Spiropulu", "title": "An MPI-Based Python Framework for Distributed Training with Keras", "comments": "4 pages, 4 figures, 1 table, DS@HEP, SC17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a lightweight Python framework for distributed training of neural\nnetworks on multiple GPUs or CPUs. The framework is built on the popular Keras\nmachine learning library. The Message Passing Interface (MPI) protocol is used\nto coordinate the training process, and the system is well suited for job\nsubmission at supercomputing sites. We detail the software's features, describe\nits use, and demonstrate its performance on systems of varying sizes on a\nbenchmark problem drawn from high-energy physics research.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 00:01:27 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Anderson", "Dustin", ""], ["Vlimant", "Jean-Roch", ""], ["Spiropulu", "Maria", ""]]}, {"id": "1712.05889", "submitter": "Robert Nishihara", "authors": "Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov,\n  Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael\n  I. Jordan, Ion Stoica", "title": "Ray: A Distributed Framework for Emerging AI Applications", "comments": "17 pages, 14 figures, 13th USENIX Symposium on Operating Systems\n  Design and Implementation, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation of AI applications will continuously interact with the\nenvironment and learn from these interactions. These applications impose new\nand demanding systems requirements, both in terms of performance and\nflexibility. In this paper, we consider these requirements and present Ray---a\ndistributed system to address them. Ray implements a unified interface that can\nexpress both task-parallel and actor-based computations, supported by a single\ndynamic execution engine. To meet the performance requirements, Ray employs a\ndistributed scheduler and a distributed and fault-tolerant store to manage the\nsystem's control state. In our experiments, we demonstrate scaling beyond 1.8\nmillion tasks per second and better performance than existing specialized\nsystems for several challenging reinforcement learning applications.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 01:29:49 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 03:14:16 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Moritz", "Philipp", ""], ["Nishihara", "Robert", ""], ["Wang", "Stephanie", ""], ["Tumanov", "Alexey", ""], ["Liaw", "Richard", ""], ["Liang", "Eric", ""], ["Elibol", "Melih", ""], ["Yang", "Zongheng", ""], ["Paul", "William", ""], ["Jordan", "Michael I.", ""], ["Stoica", "Ion", ""]]}, {"id": "1712.05902", "submitter": "Jung Woo Ha", "authors": "Nako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang, Jingwoong Kim,\n  Leonard Lausen, Youngkwan Kim, Gayoung Lee, Donghyun Kwak, Jung-Woo Ha,\n  Sunghun Kim", "title": "NSML: A Machine Learning Platform That Enables You to Focus on Your\n  Models", "comments": "8 pages, 4figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning libraries such as TensorFlow and PyTorch simplify model\nimplementation. However, researchers are still required to perform a\nnon-trivial amount of manual tasks such as GPU allocation, training status\ntracking, and comparison of models with different hyperparameter settings. We\npropose a system to handle these tasks and help researchers focus on models. We\npresent the requirements of the system based on a collection of discussions\nfrom an online study group comprising 25k members. These include automatic GPU\nallocation, learning status visualization, handling model parameter snapshots\nas well as hyperparameter modification during learning, and comparison of\nperformance metrics between models via a leaderboard. We describe the system\narchitecture that fulfills these requirements and present a proof-of-concept\nimplementation, NAVER Smart Machine Learning (NSML). We test the system and\nconfirm substantial efficiency improvements for model development.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 04:28:14 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Sung", "Nako", ""], ["Kim", "Minkyu", ""], ["Jo", "Hyunwoo", ""], ["Yang", "Youngil", ""], ["Kim", "Jingwoong", ""], ["Lausen", "Leonard", ""], ["Kim", "Youngkwan", ""], ["Lee", "Gayoung", ""], ["Kwak", "Donghyun", ""], ["Ha", "Jung-Woo", ""], ["Kim", "Sunghun", ""]]}, {"id": "1712.05907", "submitter": "Erin Conlon", "authors": "Zheng Wei and Erin M. Conlon", "title": "Parallel Markov Chain Monte Carlo for Bayesian Hierarchical Models with\n  Big Data, in Two Stages", "comments": "30 pages, 2 figures. New simulation example for logistic regression.\n  MCMC efficiency measure added. Details of convergence diagnostics added. One\n  additional table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the escalating growth of big data sets in recent years, new Bayesian\nMarkov chain Monte Carlo (MCMC) parallel computing methods have been developed.\nThese methods partition large data sets by observations into subsets. However,\nfor Bayesian nested hierarchical models, typically only a few parameters are\ncommon for the full data set, with most parameters being group-specific. Thus,\nparallel Bayesian MCMC methods that take into account the structure of the\nmodel and split the full data set by groups rather than by observations are a\nmore natural approach for analysis. Here, we adapt and extend a recently\nintroduced two-stage Bayesian hierarchical modeling approach, and we partition\ncomplete data sets by groups. In stage 1, the group-specific parameters are\nestimated independently in parallel. The stage 1 posteriors are used as\nproposal distributions in stage 2, where the target distribution is the full\nmodel. Using three-level and four-level models, we show in both simulation and\nreal data studies that results of our method agree closely with the full data\nanalysis, with greatly increased MCMC efficiency and greatly reduced\ncomputation times. The advantages of our method versus existing parallel MCMC\ncomputing methods are also described.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 06:14:18 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 22:07:54 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Wei", "Zheng", ""], ["Conlon", "Erin M.", ""]]}, {"id": "1712.05913", "submitter": "Thai Hoang Dinh DTH", "authors": "Dinh Thai Hoang, Dusit Niyato, Ping Wang, Shaun Shuxun Wang, Diep\n  Nguyen, and Eryk Dutkiewicz", "title": "A Stochastic Programming Approach for Risk Management in Mobile Cloud\n  Computing", "comments": "6 pages, 4 figures, 4 tables, WCNC 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of mobile cloud computing has brought many benefits to mobile\nusers as well as cloud service providers. However, mobile cloud computing is\nfacing some challenges, especially security-related problems due to the growing\nnumber of cyberattacks which can cause serious losses. In this paper, we\npropose a dynamic framework together with advanced risk management strategies\nto minimize losses caused by cyberattacks to a cloud service provider. In\nparticular, this framework allows the cloud service provider to select\nappropriate security solutions, e.g., security software/hardware implementation\nand insurance policies, to deal with different types of attacks. Furthermore,\nthe stochastic programming approach is adopted to minimize the expected total\nloss for the cloud service provider under its financial capability and\nuncertainty of attacks and their potential losses. Through numerical\nevaluation, we show that our approach is an effective tool in not only dealing\nwith cyberattacks under uncertainty, but also minimizing the total loss for the\ncloud service provider given its available budget.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 07:18:46 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Hoang", "Dinh Thai", ""], ["Niyato", "Dusit", ""], ["Wang", "Ping", ""], ["Wang", "Shaun Shuxun", ""], ["Nguyen", "Diep", ""], ["Dutkiewicz", "Eryk", ""]]}, {"id": "1712.05914", "submitter": "Thai Hoang Dinh DTH", "authors": "Khoi Khac Nguyen, Dinh Thai Hoang, Dusit Niyato, Ping Wang, Diep\n  Nguyen, and Eryk Dutkiewicz", "title": "Cyberattack Detection in Mobile Cloud Computing: A Deep Learning\n  Approach", "comments": "6 pages, 3 figures, 1 table, WCNC 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of mobile applications and cloud computing, mobile\ncloud computing has attracted great interest from both academia and industry.\nHowever, mobile cloud applications are facing security issues such as data\nintegrity, users' confidentiality, and service availability. A preventive\napproach to such problems is to detect and isolate cyber threats before they\ncan cause serious impacts to the mobile cloud computing system. In this paper,\nwe propose a novel framework that leverages a deep learning approach to detect\ncyberattacks in mobile cloud environment. Through experimental results, we show\nthat our proposed framework not only recognizes diverse cyberattacks, but also\nachieves a high accuracy (up to 97.11%) in detecting the attacks. Furthermore,\nwe present the comparisons with current machine learning-based approaches to\ndemonstrate the effectiveness of our proposed solution.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 07:24:55 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Nguyen", "Khoi Khac", ""], ["Hoang", "Dinh Thai", ""], ["Niyato", "Dusit", ""], ["Wang", "Ping", ""], ["Nguyen", "Diep", ""], ["Dutkiewicz", "Eryk", ""]]}, {"id": "1712.06047", "submitter": "Aditya Devarakonda", "authors": "Aditya Devarakonda, Kimon Fountoulakis, James Demmel, Michael W.\n  Mahoney", "title": "Avoiding Synchronization in First-Order Methods for Sparse Convex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel computing has played an important role in speeding up convex\noptimization methods for big data analytics and large-scale machine learning\n(ML). However, the scalability of these optimization methods is inhibited by\nthe cost of communicating and synchronizing processors in a parallel setting.\nIterative ML methods are particularly sensitive to communication cost since\nthey often require communication every iteration. In this work, we extend\nwell-known techniques from Communication-Avoiding Krylov subspace methods to\nfirst-order, block coordinate descent methods for Support Vector Machines and\nProximal Least-Squares problems. Our Synchronization-Avoiding (SA) variants\nreduce the latency cost by a tunable factor of $s$ at the expense of a factor\nof $s$ increase in flops and bandwidth costs. We show that the SA-variants are\nnumerically stable and can attain large speedups of up to $5.1\\times$ on a Cray\nXC30 supercomputer.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 02:15:15 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Devarakonda", "Aditya", ""], ["Fountoulakis", "Kimon", ""], ["Demmel", "James", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1712.06071", "submitter": "Abdulhamit Subasi", "authors": "Samed Jukic, Abdulhamit Subasi", "title": "A MapReduce-based rotation forest classifier for epileptic seizure\n  prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era, big data applications including biomedical are becoming\nattractive as the data generation and storage is increased in the last years.\nThe big data processing to extract knowledge becomes challenging since the data\nmining techniques are not adapted to the new requirements. In this study, we\nanalyse the EEG signals for epileptic seizure detection in the big data\nscenario using Rotation Forest classifier. Specifically, MSPCA is used for\ndenoising, WPD is used for feature extraction and Rotation Forest is used for\nclassification in a MapReduce framework to correctly predict the epileptic\nseizure. This paper presents a MapReduce-based distributed ensemble algorithm\nfor epileptic seizure prediction and trains a Rotation Forest on each dataset\nin parallel using a cluster of computers. The results of MapReduce based\nRotation Forest show that the proposed framework reduces the training time\nsignificantly while accomplishing a high level of performance in\nclassifications.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 08:37:49 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Jukic", "Samed", ""], ["Subasi", "Abdulhamit", ""]]}, {"id": "1712.06128", "submitter": "Tiancheng Li", "authors": "Tiancheng Li and Franz Hlawatsch", "title": "A Distributed Particle-PHD Filter with Arithmetic-Average PHD Fusion", "comments": "13 pages, codes available upon e-mail request", "journal-ref": "Information Fusion, Volume 73, September 2021, Pages 111-124", "doi": "10.1016/j.inffus.2021.02.020", "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a particle-based distributed PHD filter for tracking an unknown,\ntime-varying number of targets. To reduce communication, the local PHD filters\nat neighboring sensors communicate Gaussian mixture (GM) parameters. In\ncontrast to most existing distributed PHD filters, our filter employs an\n`arithmetic average' fusion. For particles--GM conversion, we use a method that\navoids particle clustering and enables a significance-based pruning of the GM\ncomponents. For GM--particles conversion, we develop an importance sampling\nbased method that enables a parallelization of filtering and\ndissemination/fusion operations. The proposed distributed particle-PHD filter\nis able to integrate GM-based local PHD filters. Simulations demonstrate the\nexcellent performance and small communication and computation requirements of\nour filter.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 15:08:03 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 15:08:21 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Tiancheng", ""], ["Hlawatsch", "Franz", ""]]}, {"id": "1712.06134", "submitter": "Manuel P\\\"oter", "authors": "Manuel P\\\"oter, Jesper Larsson Tr\\\"aff", "title": "A new and five older Concurrent Memory Reclamation Schemes in Comparison\n  (Stamp-it)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory management is a critical component in almost all shared-memory,\nconcurrent data structures and algorithms, consisting in the efficient\nallocation and the subsequent reclamation of shared memory resources. This\npaper contributes a new, lock-free, amortized constant-time memory reclamation\nscheme called \\emph{Stamp-it}, and compares it to five well-known, selectively\nefficient schemes from the literature, namely Lock-free Reference Counting,\nHazard Pointers, Quiescent State-based Reclamation, Epoch-based Reclamation,\nand New Epoch-based Reclamation. An extensive, experimental evaluation with\nboth new and commonly used benchmarks is provided, on four different\nshared-memory systems with hardware supported thread counts ranging from 48 to\n512, showing Stamp-it to be competitive with and in many cases and aspects\noutperforming other schemes.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 16:21:49 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["P\u00f6ter", "Manuel", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1712.06139", "submitter": "Christopher Olston", "authors": "Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li\n  Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, Jordan Soyke", "title": "TensorFlow-Serving: Flexible, High-Performance ML Serving", "comments": "Presented at NIPS 2017 Workshop on ML Systems\n  (http://learningsys.org/nips17/acceptedpapers.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe TensorFlow-Serving, a system to serve machine learning models\ninside Google which is also available in the cloud and via open-source. It is\nextremely flexible in terms of the types of ML platforms it supports, and ways\nto integrate with systems that convey new models and updated versions from\ntraining to serving. At the same time, the core code paths around model lookup\nand inference have been carefully optimized to avoid performance pitfalls\nobserved in naive implementations. Google uses it in many production\ndeployments, including a multi-tenant model hosting service called TFS^2.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 16:36:26 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 20:43:02 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Olston", "Christopher", ""], ["Fiedel", "Noah", ""], ["Gorovoy", "Kiril", ""], ["Harmsen", "Jeremiah", ""], ["Lao", "Li", ""], ["Li", "Fangwei", ""], ["Rajashekhar", "Vinu", ""], ["Ramesh", "Sukriti", ""], ["Soyke", "Jordan", ""]]}, {"id": "1712.06497", "submitter": "Andreas Kurth", "authors": "Andreas Kurth, Pirmin Vogel, Alessandro Capotondi, Andrea Marongiu,\n  Luca Benini", "title": "HERO: Heterogeneous Embedded Research Platform for Exploring RISC-V\n  Manycore Accelerators on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous embedded systems on chip (HESoCs) co-integrate a standard host\nprocessor with programmable manycore accelerators (PMCAs) to combine\ngeneral-purpose computing with domain-specific, efficient processing\ncapabilities. While leading companies successfully advance their HESoC\nproducts, research lags behind due to the challenges of building a prototyping\nplatform that unites an industry-standard host processor with an open research\nPMCA architecture. In this work we introduce HERO, an FPGA-based research\nplatform that combines a PMCA composed of clusters of RISC-V cores, implemented\nas soft cores on an FPGA fabric, with a hard ARM Cortex-A multicore host\nprocessor. The PMCA architecture mapped on the FPGA is silicon-proven,\nscalable, configurable, and fully modifiable. HERO includes a complete software\nstack that consists of a heterogeneous cross-compilation toolchain with support\nfor OpenMP accelerator programming, a Linux driver, and runtime libraries for\nboth host and PMCA. HERO is designed to facilitate rapid exploration on all\nsoftware and hardware layers: run-time behavior can be accurately analyzed by\ntracing events, and modifications can be validated through fully automated hard\nware and software builds and executed tests. We demonstrate the usefulness of\nHERO by means of case studies from our research.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 16:14:49 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Kurth", "Andreas", ""], ["Vogel", "Pirmin", ""], ["Capotondi", "Alessandro", ""], ["Marongiu", "Andrea", ""], ["Benini", "Luca", ""]]}, {"id": "1712.06687", "submitter": "Trevor Brown", "authors": "Trevor Brown, Faith Ellen, Eric Ruppert", "title": "A General Technique for Non-blocking Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general technique for obtaining provably correct, non-blocking\nimplementations of a large class of tree data structures where pointers are\ndirected from parents to children. Updates are permitted to modify any\ncontiguous portion of the tree atomically. Our non-blocking algorithms make use\nof the LLX, SCX and VLX primitives, which are multi-word generalizations of the\nstandard LL, SC and VL primitives and have been implemented from single-word\nCAS.\n  To illustrate our technique, we describe how it can be used in a fairly\nstraightforward way to obtain a non-blocking implementation of a chromatic\ntree, which is a relaxed variant of a red-black tree. The height of the tree at\nany time is $O(c+ \\log n)$, where $n$ is the number of keys and $c$ is the\nnumber of updates in progress. We provide an experimental performance analysis\nwhich demonstrates that our Java implementation of a chromatic tree rivals, and\noften significantly outperforms, other leading concurrent dictionaries.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 21:42:46 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Brown", "Trevor", ""], ["Ellen", "Faith", ""], ["Ruppert", "Eric", ""]]}, {"id": "1712.06688", "submitter": "Trevor Brown", "authors": "Trevor Brown, Faith Ellen, Eric Ruppert", "title": "Pragmatic Primitives for Non-blocking Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new set of primitive operations that greatly simplify the\nimplementation of non-blocking data structures in asynchronous shared-memory\nsystems. The new operations operate on a set of Data-records, each of which\ncontains multiple fields. The operations are generalizations of the well-known\nload-link (LL) and store-conditional (SC) operations called LLX and SCX. The\nLLX operation takes a snapshot of one Data-record. An SCX operation by a\nprocess $p$ succeeds only if no Data-record in a specified set has been changed\nsince $p$ last performed an LLX on it. If successful, the SCX atomically\nupdates one specific field of a Data-record in the set and prevents any future\nchanges to some specified subset of those Data-records. We provide a provably\ncorrect implementation of these new primitives from single-word\ncompare-and-swap. As a simple example, we show how to implement a non-blocking\nmultiset data structure in a straightforward way using LLX and SCX.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 21:46:20 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Brown", "Trevor", ""], ["Ellen", "Faith", ""], ["Ruppert", "Eric", ""]]}, {"id": "1712.06790", "submitter": "Jieyang Chen", "authors": "Jieyang Chen, Qiang Guan, Xin Liang, Louis James Vernon, Allen\n  McPherson, Li-Ta Lo, Zizhong Chen, James Paul Ahrens", "title": "Build and Execution Environment (BEE): an Encapsulated Environment\n  Enabling HPC Applications Running Everywhere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variations in High Performance Computing (HPC) system software configurations\nmean that applications are typically configured and built for specific HPC\nenvironments. Building applications can require a significant investment of\ntime and effort for application users and requires application users to have\nadditional technical knowledge. Linux container technologies such as Docker and\nCharliecloud bring great benefits to the application development, build and\ndeployment processes. While cloud platforms already widely support containers,\nHPC systems still have non-uniform support of container technologies. In this\nwork, we propose a unified runtime framework -- Build and Execution Environment\n(BEE) across both HPC and cloud platforms that allows users to run their\ncontainerized HPC applications across all supported platforms without\nmodification. We design four BEE backends for four different classes of HPC or\ncloud platform so that together they cover the majority of mainstream computing\nplatforms for HPC users. Evaluations show that BEE provides an easy-to-use\nunified user interface, execution environment, and comparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 05:29:37 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 20:15:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chen", "Jieyang", ""], ["Guan", "Qiang", ""], ["Liang", "Xin", ""], ["Vernon", "Louis James", ""], ["McPherson", "Allen", ""], ["Lo", "Li-Ta", ""], ["Chen", "Zizhong", ""], ["Ahrens", "James Paul", ""]]}, {"id": "1712.07021", "submitter": "Yi-Peng Wei", "authors": "Yi-Peng Wei and Karim Banawan and Sennur Ulukus", "title": "Cache-Aided Private Information Retrieval with Partially Known Uncoded\n  Prefetching: Fundamental Limits", "comments": "Submitted for publication, December 2017. arXiv admin note:\n  substantial text overlap with arXiv:1709.01056", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of private information retrieval (PIR) from $N$\nnon-colluding and replicated databases, when the user is equipped with a cache\nthat holds an uncoded fraction $r$ of the symbols from each of the $K$ stored\nmessages in the databases. This model operates in a two-phase scheme, namely,\nthe prefetching phase where the user acquires side information and the\nretrieval phase where the user privately downloads the desired message. In the\nprefetching phase, the user receives $\\frac{r}{N}$ uncoded fraction of each\nmessage from the $n$th database. This side information is known only to the\n$n$th database and unknown to the remaining databases, i.e., the user possesses\n\\emph{partially known} side information. We investigate the optimal normalized\ndownload cost $D^*(r)$ in the retrieval phase as a function of $K$, $N$, $r$.\nWe develop lower and upper bounds for the optimal download cost. The bounds\nmatch in general for the cases of very low caching ratio ($r \\leq\n\\frac{1}{N^{K-1}}$) and very high caching ratio ($r \\geq\n\\frac{K-2}{N^2-3N+KN}$). We fully characterize the optimal download cost\ncaching ratio tradeoff for $K=3$. For general $K$, $N$, and $r$, we show that\nthe largest gap between the achievability and the converse bounds is\n$\\frac{5}{32}$.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 18:50:42 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Wei", "Yi-Peng", ""], ["Banawan", "Karim", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1712.07206", "submitter": "Edoardo Di Napoli", "authors": "Davor Davidovi\\'c, Diego Fabregat-Traver, Markus H\\\"ohnerbach, and\n  Edoardo di Napoli", "title": "Accelerating the computation of FLAPW methods on heterogeneous\n  architectures", "comments": "22 pages, submitted to special issue of CCPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legacy codes in computational science and engineering have been very\nsuccessful in providing essential functionality to researchers. However, they\nare not capable of exploiting the massive parallelism provided by emerging\nheterogeneous architectures. The lack of portable performance and scalability\nputs them at high risk: either they evolve or they are doomed to disappear. One\nexample of legacy code which would heavily benefit from a modern design is\nFLEUR, a software for electronic structure calculations. In previous work, the\ncomputational bottleneck of FLEUR was partially re-engineered to have a modular\ndesign that relies on standard building blocks, namely BLAS and LAPACK. In this\npaper, we demonstrate how the initial redesign enables the portability to\nheterogeneous architectures. More specifically, we study different approaches\nto port the code to architectures consisting of multi-core CPUs equipped with\none or more coprocessors such as Nvidia GPUs and Intel Xeon Phis. Our final\ncode attains over 70\\% of the architectures' peak performance, and outperforms\nNvidia's and Intel's libraries. Finally, on JURECA, the supercomputer where\nFLEUR is often executed, the code takes advantage of the full power of the\ncomputing nodes, attaining $5\\times$ speedup over the sole use of the CPUs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 20:58:08 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Davidovi\u0107", "Davor", ""], ["Fabregat-Traver", "Diego", ""], ["H\u00f6hnerbach", "Markus", ""], ["di Napoli", "Edoardo", ""]]}, {"id": "1712.07495", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng, Aur\\'elien Bellet, Patrick Gallinari", "title": "A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with\n  the Trace Norm", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-018-5713-5", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a high-dimensional but low-rank matrix\nfrom a large-scale dataset distributed over several machines, where\nlow-rankness is enforced by a convex trace norm constraint. We propose\nDFW-Trace, a distributed Frank-Wolfe algorithm which leverages the low-rank\nstructure of its updates to achieve efficiency in time, memory and\ncommunication usage. The step at the heart of DFW-Trace is solved approximately\nusing a distributed version of the power method. We provide a theoretical\nanalysis of the convergence of DFW-Trace, showing that we can ensure sublinear\nconvergence in expectation to an optimal solution with few power iterations per\nepoch. We implement DFW-Trace in the Apache Spark distributed programming\nframework and validate the usefulness of our approach on synthetic and real\ndata, including the ImageNet dataset with high-dimensional features extracted\nfrom a deep neural network.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 14:28:21 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 12:09:11 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zheng", "Wenjie", ""], ["Bellet", "Aur\u00e9lien", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1712.07697", "submitter": "Iosif Salem", "authors": "Marco Canini (1), Iosif Salem (2), Liron Schiff (3), Elad Michael\n  Schiller (2), Stefan Schmid (4 and 5) ((1) Universit\\'e catholique de\n  Louvain, (2) Chalmers University of Technology, (3) GuardiCore Labs, (4)\n  University of Vienna, (5) Aalborg University)", "title": "Renaissance: Self-Stabilizing Distributed SDN Control Plane", "comments": "v2 includes: refined presentation, simpler notation in Algorithm 2,\n  additional explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By introducing programmability, automated verification, and innovative\ndebugging tools, Software-Defined Networks (SDNs) are poised to meet the\nincreasingly stringent dependability requirements of today's communication\nnetworks. However, the design of fault-tolerant SDNs remains an open challenge.\nThis paper considers the design of dependable SDNs through the lenses of\nself-stabilization - a very strong notion of fault-tolerance. In particular, we\ndevelop algorithms for an in-band and distributed control plane for SDNs,\ncalled Renaissance, which tolerate a wide range of (concurrent) controller,\nlink, and communication failures. Our self-stabilizing algorithms ensure that\nafter the occurrence of an arbitrary combination of failures, (i) every\nnon-faulty SDN controller can reach any switch (or another controller) in the\nnetwork within a bounded communication delay (in the presence of a bounded\nnumber of concurrent failures) and (ii) every switch is managed by at least one\ncontroller (as long as at least one controller is not faulty). We evaluate\nRenaissance through a rigorous worst-case analysis as well as a prototype\nimplementation (based on OVS and Floodlight), and we report on our experiments\nusing Mininet.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 20:23:43 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 15:54:23 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Canini", "Marco", "", "4 and 5"], ["Salem", "Iosif", "", "4 and 5"], ["Schiff", "Liron", "", "4 and 5"], ["Schiller", "Elad Michael", "", "4 and 5"], ["Schmid", "Stefan", "", "4 and 5"]]}, {"id": "1712.08205", "submitter": "Iosif Salem", "authors": "Iosif Salem (1) and Elad Michael Schiller (1) ((1) Chalmers University\n  of Technology)", "title": "Practically-Self-Stabilizing Vector Clocks in the Absence of Execution\n  Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector clock algorithms are basic wait-free building blocks that facilitate\ncausal ordering of events. As wait-free algorithms, they are guaranteed to\ncomplete their operations within a finite number of steps. Stabilizing\nalgorithms allow the system to recover after the occurrence of transient\nfaults, such as soft errors and arbitrary violations of the assumptions\naccording to which the system was designed to behave. We present the first, to\nthe best of our knowledge, stabilizing vector clock algorithm for asynchronous\ncrash-prone message-passing systems that can recover in a wait-free manner\nafter the occurrence of transient faults. In these settings, it is challenging\nto demonstrate a finite and wait-free recovery from (communication and crash\nfailures as well as) transient faults, bound the message and storage sizes,\ndeal with the removal of all stale information without blocking, and deal with\ncounter overflow events (which occur at different network nodes concurrently).\n  We present an algorithm that never violates safety in the absence of\ntransient faults and provides bounded time recovery during fair executions that\nfollow the last transient fault. The novelty is that in the absence of\nexecution fairness, the algorithm guarantees a bound on the number of times in\nwhich the system might violate safety (while existing algorithms might block\nforever due to the presence of both transient faults and crash failures).\n  Since vector clocks facilitate a number of elementary synchronization\nbuilding blocks (without requiring remote replica synchronization) in\nasynchronous systems, we believe that our analytical insights are useful for\nthe design of other systems that cannot guarantee execution fairness.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 20:45:47 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Salem", "Iosif", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "1712.08230", "submitter": "Albin Severinson", "authors": "Albin Severinson, Alexandre Graell i Amat, Eirik Rosnes", "title": "Block-Diagonal and LT Codes for Distributed Computing With Straggling\n  Servers", "comments": "To appear in IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two coded schemes for the distributed computing problem of\nmultiplying a matrix by a set of vectors. The first scheme is based on\npartitioning the matrix into submatrices and applying maximum distance\nseparable (MDS) codes to each submatrix. For this scheme, we prove that up to a\ngiven number of partitions the communication load and the computational delay\n(not including the encoding and decoding delay) are identical to those of the\nscheme recently proposed by Li et al., based on a single, long MDS code.\nHowever, due to the use of shorter MDS codes, our scheme yields a significantly\nlower overall computational delay when the delay incurred by encoding and\ndecoding is also considered. We further propose a second coded scheme based on\nLuby Transform (LT) codes under inactivation decoding. Interestingly, LT codes\nmay reduce the delay over the partitioned scheme at the expense of an increased\ncommunication load. We also consider distributed computing under a deadline and\nshow numerically that the proposed schemes outperform other schemes in the\nliterature, with the LT code-based scheme yielding the best performance for the\nscenarios considered.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:02:25 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 14:17:50 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 10:00:57 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Severinson", "Albin", ""], ["Amat", "Alexandre Graell i", ""], ["Rosnes", "Eirik", ""]]}, {"id": "1712.08342", "submitter": "Michael Borkowski", "authors": "Michael Borkowski, Walid Fdhila, Matteo Nardelli, Stefanie\n  Rinderle-Ma, Stefan Schulte", "title": "Event-based Failure Prediction in Distributed Business Processes", "comments": null, "journal-ref": null, "doi": "10.1016/j.is.2017.12.005", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traditionally, research in Business Process Management has put a strong focus\non centralized and intra-organizational processes. However, today's business\nprocesses are increasingly distributed, deviating from a centralized layout,\nand therefore calling for novel methodologies of detecting and responding to\nunforeseen events, such as errors occurring during process runtime. In this\narticle, we demonstrate how to employ event-based failure prediction in\nbusiness processes. This approach allows to make use of the best of both\ntraditional Business Process Management Systems and event-based systems. Our\napproach employs machine learning techniques and considers various types of\nevents. We evaluate our solution using two business process data sets,\nincluding one from a real-world event log, and show that we are able to detect\nerrors and predict failures with high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 08:45:55 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 11:56:16 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Borkowski", "Michael", ""], ["Fdhila", "Walid", ""], ["Nardelli", "Matteo", ""], ["Rinderle-Ma", "Stefanie", ""], ["Schulte", "Stefan", ""]]}, {"id": "1712.08345", "submitter": "EPTCS", "authors": "Timo Kehrer (Humboldt-University of Berlin), Alice Miller (University\n  of Glasgow)", "title": "Proceedings Third Workshop on Graphs as Models", "comments": null, "journal-ref": "EPTCS 263, 2017", "doi": "10.4204/EPTCS.263", "report-no": null, "categories": "cs.LO cs.DC cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are used as models in many areas of computer science and computer\nengineering. For example graphs are used to represent syntax, control and data\nflow, dependency, state spaces, models such as UML and other types of\ndomain-specific models, and social network graphs. In all of these examples,\nthe graph serves as an intuitive yet mathematically precise foundation for many\npurposes, both in theory building as well as in practical applications.\nGraph-based models serve as an abstract communication medium and are used to\ndescribe various concepts and phenomena. Moreover, once such graph-based models\nare constructed, they can be analyzed and transformed to verify the correctness\nof static and dynamic properties, to discover new properties, to deeply study a\nparticular domain of interest or to produce new equivalent and/or optimized\nversions of graph-based models.\n  The Graphs as Models (GaM) workshop series combines the strengths of two\npre-existing workshop series: GT-VMT (Graph Transformation and Visual Modelling\nTechniques) and GRAPHITE (Graph Inspection and Traversal Engineering), but also\nsolicits research from other related areas, such as social network analysis.\nGaM offers a platform for exchanging new ideas and results for active\nresearchers in these areas, with a particular aim of boosting inter- and\ntransdisciplinary research exploiting new applications of graphs as models in\nany area of computational science. This year (2017), the third edition of the\nGaM workshop was co-located with the European Joint Conferences on Theory and\nPractice of Software 2017 (ETAPS'17), held in Uppsala, Sweden.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 08:48:17 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Kehrer", "Timo", "", "Humboldt-University of Berlin"], ["Miller", "Alice", "", "University\n  of Glasgow"]]}, {"id": "1712.08367", "submitter": "Christian Mayer", "authors": "Christian Mayer, Ruben Mayer, Muhammad Adnan Tariq, Heiko Geppert,\n  Larissa Laich, Lukas Rieger, and Kurt Rothermel", "title": "ADWISE: Adaptive Window-based Streaming Edge Partitioning for High-Speed\n  Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the graph partitioning problem gained importance as a\nmandatory preprocessing step for distributed graph processing on very large\ngraphs. Existing graph partitioning algorithms minimize partitioning latency by\nassigning individual graph edges to partitions in a streaming manner --- at the\ncost of reduced partitioning quality. However, we argue that the mere\nminimization of partitioning latency is not the optimal design choice in terms\nof minimizing total graph analysis latency, i.e., the sum of partitioning and\nprocessing latency. Instead, for complex and long-running graph processing\nalgorithms that run on very large graphs, it is beneficial to invest more time\ninto graph partitioning to reach a higher partitioning quality --- which\ndrastically reduces graph processing latency. In this paper, we propose ADWISE,\na novel window-based streaming partitioning algorithm that increases the\npartitioning quality by always choosing the best edge from a set of edges for\nassignment to a partition. In doing so, ADWISE controls the partitioning\nlatency by adapting the window size dynamically at run-time. Our evaluations\nshow that ADWISE can reach the sweet spot between graph partitioning latency\nand graph processing latency, reducing the total latency of partitioning plus\nprocessing by up to 23-47 percent compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:37:15 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 11:15:38 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Mayer", "Christian", ""], ["Mayer", "Ruben", ""], ["Tariq", "Muhammad Adnan", ""], ["Geppert", "Heiko", ""], ["Laich", "Larissa", ""], ["Rieger", "Lukas", ""], ["Rothermel", "Kurt", ""]]}, {"id": "1712.08437", "submitter": "Alexander Ponomarenko", "authors": "Alexander Ponomarenko, Irina Utkina and Mikhail Batsyn", "title": "A Model of Optimal Network Structure for Decentralized Nearest Neighbor\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the approaches for the nearest neighbor search problem is to build a\nnetwork which nodes correspond to the given set of indexed objects. In this\ncase the search of the closest object can be thought as a search of a node in a\nnetwork. A procedure in a network is called decentralized if it uses only local\ninformation about visited nodes and its neighbors. Networks, which structure\nallows efficient performing the nearest neighbour search by a decentralised\nsearch procedure started from any node, are of particular interest especially\nfor pure distributed systems. Several algorithms that construct such networks\nhave been proposed in literature. However, the following questions arise: \"Are\nthere network models in which decentralised search can be performed faster?\";\n\"What are the optimal networks for the decentralised search?\"; \"What are their\nproperties?\". In this paper we partially give answers to these questions. We\npropose a mathematical programming model for the problem of determining an\noptimal network structure for decentralized nearest neighbor search. We have\nfound an exact solution for a regular lattice of size 4x4 and heuristic\nsolutions for sizes from 5x5 to 7x7. As a distance function we use L1 , L2 and\nL_inf metrics. We hope that our results and the proposed model will initiate\nstudy of optimal network structures for decentralised nearest neighbour search.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 13:34:15 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Ponomarenko", "Alexander", ""], ["Utkina", "Irina", ""], ["Batsyn", "Mikhail", ""]]}, {"id": "1712.08634", "submitter": "Shunxing Bao", "authors": "Shunxing Bao, Yuankai Huo, Prasanna Parvathaneni, Andrew J. Plassard,\n  Camilo Bermudez, Yuang Yao, Ilwoo Llyu, Aniruddha Gokhale, Bennett A. Landman", "title": "A Data Colocation Grid Framework for Big Data Medical Image Processing -\n  Backend Design", "comments": "Accepted and awaiting publication at SPIE Medical Imaging,\n  International Society for Optics and Photonics, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When processing large medical imaging studies, adopting high performance grid\ncomputing resources rapidly becomes important. We recently presented a \"medical\nimage processing-as-a-service\" grid framework that offers promise in utilizing\nthe Apache Hadoop ecosystem and HBase for data colocation by moving computation\nclose to medical image storage. However, the framework has not yet proven to be\neasy to use in a heterogeneous hardware environment. Furthermore, the system\nhas not yet validated when considering variety of multi-level analysis in\nmedical imaging. Our target criteria are (1) improving the framework's\nperformance in a heterogeneous cluster, (2) performing population based summary\nstatistics on large datasets, and (3) introducing a table design scheme for\nrapid NoSQL query. In this paper, we present a backend interface application\nprogram interface design for Hadoop & HBase for Medical Image Processing. The\nAPI includes: Upload, Retrieve, Remove, Load balancer and MapReduce templates.\nA dataset summary statistic model is discussed and implemented by MapReduce\nparadigm. We introduce a HBase table scheme for fast data query to better\nutilize the MapReduce model. Briefly, 5153 T1 images were retrieved from a\nuniversity secure database and used to empirically access an in-house grid with\n224 heterogeneous CPU cores. Three empirical experiments results are presented\nand discussed: (1) load balancer wall-time improvement of 1.5-fold compared\nwith a framework with built-in data allocation strategy, (2) a summary\nstatistic model is empirically verified on grid framework and is compared with\nthe cluster when deployed with a standard Sun Grid Engine, which reduces 8-fold\nof wall clock time and 14-fold of resource time, and (3) the proposed HBase\ntable scheme improves MapReduce computation with 7 fold reduction of wall time\ncompare with a na\\\"ive scheme when datasets are relative small.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 19:50:15 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Bao", "Shunxing", ""], ["Huo", "Yuankai", ""], ["Parvathaneni", "Prasanna", ""], ["Plassard", "Andrew J.", ""], ["Bermudez", "Camilo", ""], ["Yao", "Yuang", ""], ["Llyu", "Ilwoo", ""], ["Gokhale", "Aniruddha", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1712.08644", "submitter": "Michael Bechtel", "authors": "Michael G. Bechtel, Elise McEllhiney, Minje Kim, Heechul Yun", "title": "DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car", "comments": "To be published as a conference paper at RTCSA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepPicar, a low-cost deep neural network based autonomous car\nplatform. DeepPicar is a small scale replication of a real self-driving car\ncalled DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN),\nwhich takes images from a front-facing camera as input and produces car\nsteering angles as output. DeepPicar uses the same network architecture---9\nlayers, 27 million connections and 250K parameters---and can drive itself in\nreal-time using a web camera and a Raspberry Pi 3 quad-core platform. Using\nDeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end\ndeep learning based real-time control of autonomous vehicles. We also\nsystematically compare other contemporary embedded computing platforms using\nthe DeepPicar's CNN-based real-time control workload. We find that all tested\nplatforms, including the Pi 3, are capable of supporting the CNN-based\nreal-time control, from 20 Hz up to 100 Hz, depending on hardware platform.\nHowever, we find that shared resource contention remains an important issue\nthat must be considered in applying CNN models on shared memory based embedded\ncomputing platforms; we observe up to 11.6X execution time increase in the CNN\nbased control loop due to shared resource contention. To protect the CNN\nworkload, we also evaluate state-of-the-art cache partitioning and memory\nbandwidth throttling techniques on the Pi 3. We find that cache partitioning is\nineffective, while memory bandwidth throttling is an effective solution.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 22:24:08 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 04:33:17 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 01:08:25 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 02:29:01 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Bechtel", "Michael G.", ""], ["McEllhiney", "Elise", ""], ["Kim", "Minje", ""], ["Yun", "Heechul", ""]]}, {"id": "1712.09098", "submitter": "Harishchandra Dubey", "authors": "Rabindra K. Barik and Harishchandra Dubey and Kunal Mankodiya", "title": "SoA-Fog: Secure Service-Oriented Edge Computing Architecture for Smart\n  Health Big Data Analytics", "comments": "6 pages, 2 Figures, 1 Table. 5th IEEE Global Conference on Signal and\n  Information Processing GlobalSIP 2017, November 14-16, 2017, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smart health paradigms employ Internet-connected wearables for\ntelemonitoring, diagnosis for providing inexpensive healthcare solutions. Fog\ncomputing reduces latency and increases throughput by processing data near the\nbody sensor network. In this paper, we proposed a secure serviceorientated edge\ncomputing architecture that is validated on recently released public dataset.\nResults and discussions support the applicability of proposed architecture for\nsmart health applications. We proposed SoA-Fog i.e. a three-tier secure\nframework for efficient management of health data using fog devices. It discuss\nthe security aspects in client layer, fog layer and the cloud layer. We design\nthe prototype by using win-win spiral model with use case and sequence diagram.\nOverlay analysis was performed using proposed framework on malaria vector borne\ndisease positive maps of Maharastra state in India from 2011 to 2014. The\nmobile clients were taken as test case. We performed comparative analysis\nbetween proposed secure fog framework and state-of-the art cloud-based\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 16:29:45 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Barik", "Rabindra K.", ""], ["Dubey", "Harishchandra", ""], ["Mankodiya", "Kunal", ""]]}, {"id": "1712.09121", "submitter": "Jason Li", "authors": "Mohsen Ghaffari, Jason Li", "title": "Improved Distributed Algorithms for Exact Shortest Paths", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing shortest paths is one of the central problems in the theory of\ndistributed computing. For the last few years, substantial progress has been\nmade on the approximate single source shortest paths problem, culminating in an\nalgorithm of Becker et al. [DISC'17] which deterministically computes\n$(1+o(1))$-approximate shortest paths in $\\tilde O(D+\\sqrt n)$ time, where $D$\nis the hop-diameter of the graph. Up to logarithmic factors, this time\ncomplexity is optimal, matching the lower bound of Elkin [STOC'04].\n  The question of exact shortest paths however saw no algorithmic progress for\ndecades, until the recent breakthrough of Elkin [STOC'17], which established a\nsublinear-time algorithm for exact single source shortest paths on undirected\ngraphs. Shortly after, Huang et al. [FOCS'17] provided improved algorithms for\nexact all pairs shortest paths problem on directed graphs.\n  In this paper, we present a new single-source shortest path algorithm with\ncomplexity $\\tilde O(n^{3/4}D^{1/4})$. For polylogarithmic $D$, this improves\non Elkin's $\\tilde{O}(n^{5/6})$ bound and gets closer to the\n$\\tilde{\\Omega}(n^{1/2})$ lower bound of Elkin [STOC'04]. For larger values of\n$D$, we present an improved variant of our algorithm which achieves complexity\n$\\tilde{O}\\left( n^{3/4+o(1)}+ \\min\\{ n^{3/4}D^{1/6},n^{6/7}\\}+D\\right)$, and\nthus compares favorably with Elkin's bound of $\\tilde{O}(n^{5/6} +\nn^{2/3}D^{1/3} + D ) $ in essentially the entire range of parameters. This\nalgorithm provides also a qualitative improvement, because it works for the\nmore challenging case of directed graphs (i.e., graphs where the two directions\nof an edge can have different weights), constituting the first sublinear-time\nalgorithm for directed graphs. Our algorithm also extends to the case of exact\n$\\kappa$-source shortest paths...\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 19:57:16 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 20:20:06 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 00:56:01 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 22:51:31 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Li", "Jason", ""]]}, {"id": "1712.09152", "submitter": "Bashir Mohammed", "authors": "Bashir Mohammed, Sibusiso Moyo, K. M Maiyama, Sulayman Kinteh, Al\n  Noaman M.K. Al-Shaidy, M. A. Kamala, M. Kiran", "title": "Technical Report on Deploying a highly secured OpenStack Cloud\n  Infrastructure using BradStack as a Case Study", "comments": "38 pages, 19 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has emerged as a popular paradigm and an attractive model for\nproviding a reliable distributed computing model.it is increasing attracting\nhuge attention both in academic research and industrial initiatives. Cloud\ndeployments are paramount for institution and organizations of all scales. The\navailability of a flexible, free open source cloud platform designed with no\npropriety software and the ability of its integration with legacy systems and\nthird-party applications are fundamental. Open stack is a free and opensource\nsoftware released under the terms of Apache license with a fragmented and\ndistributed architecture making it highly flexible. This project was initiated\nand aimed at designing a secured cloud infrastructure called BradStack, which\nis built on OpenStack in the Computing Laboratory at the University of\nBradford. In this report, we present and discuss the steps required in\ndeploying a secured BradStack Multi-node cloud infrastructure and conducting\nPenetration testing on OpenStack Services to validate the effectiveness of the\nsecurity controls on the BradStack platform. This report serves as a practical\nguideline, focusing on security and practical infrastructure related issues. It\nalso serves as a reference for institutions looking at the possibilities of\nimplementing a secured cloud solution.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 01:03:30 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Mohammed", "Bashir", ""], ["Moyo", "Sibusiso", ""], ["Maiyama", "K. M", ""], ["Kinteh", "Sulayman", ""], ["Al-Shaidy", "Al Noaman M. K.", ""], ["Kamala", "M. A.", ""], ["Kiran", "M.", ""]]}, {"id": "1712.09168", "submitter": "Jumana Dakka", "authors": "Jumana Dakka, Matteo Turilli, David W Wright, Stefan J Zasada, Vivek\n  Balasubramanian, Shunzhou Wan, Peter V Coveney, Shantenu Jha", "title": "High-throughput Binding Affinity Calculations at Extreme Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resistance to chemotherapy and molecularly targeted therapies is a major\nfactor in limiting the effectiveness of cancer treatment. In many cases,\nresistance can be linked to genetic changes in target proteins, either\npre-existing or evolutionarily selected during treatment. Key to overcoming\nthis challenge is an understanding of the molecular determinants of drug\nbinding. Using multi-stage pipelines of molecular simulations we can gain\ninsights into the binding free energy and the residence time of a ligand, which\ncan inform both stratified and personal treatment regimes and drug development.\nTo support the scalable, adaptive and automated calculation of the binding free\nenergy on high-performance computing resources, we introduce the High-\nthroughput Binding Affinity Calculator (HTBAC). HTBAC uses a building block\napproach in order to attain both workflow flexibility and performance. We\ndemonstrate close to perfect weak scaling to hundreds of concurrent multi-stage\nbinding affinity calculation pipelines. This permits a rapid time-to-solution\nthat is essentially invariant of the calculation protocol, size of candidate\nligands and number of ensemble simulations. As such, HTBAC advances the state\nof the art of binding affinity calculations and protocols.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 03:23:16 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 23:21:26 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 15:22:32 GMT"}, {"version": "v4", "created": "Tue, 13 Feb 2018 19:30:33 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Dakka", "Jumana", ""], ["Turilli", "Matteo", ""], ["Wright", "David W", ""], ["Zasada", "Stefan J", ""], ["Balasubramanian", "Vivek", ""], ["Wan", "Shunzhou", ""], ["Coveney", "Peter V", ""], ["Jha", "Shantenu", ""]]}, {"id": "1712.09282", "submitter": "Harishchandra Dubey", "authors": "Rabindra K. Barik and Rakesh K. Lenka and N.V.R. Simha and\n  Harishchandra Dubey and Kunal Mankodiya", "title": "Fog Computing based SDI Framework for Mineral Resources Information\n  Infrastructure Management in India", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial Data Infrastructure (SDI) is an important concept for sharing spatial\ndata across the web. With cumulative techniques with spatial cloud computing\nand fog computing, SDI has the greater potential and has been emerged as a tool\nfor processing, analysis and transmission of spatial data. The Fog computing is\na paradigm where Fog devices help to increase throughput and reduce latency at\nthe edge of the client with respect to cloud computing environment. This paper\nproposed and developed a fog computing based SDI framework for mining analytics\nfrom spatial big data for mineral resources management in India. We built a\nprototype using Raspberry Pi, an embedded microprocessor. We validated by\ntaking suitable case study of mineral resources management in India by doing\npreliminary analysis including overlay analysis. Results showed that fog\ncomputing hold a great promise for analysis of spatial data. We used open\nsource GIS i.e. QGIS and QIS plugin for reducing the transmission to cloud from\nthe fog node.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 15:16:19 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 18:16:06 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Barik", "Rabindra K.", ""], ["Lenka", "Rakesh K.", ""], ["Simha", "N. V. R.", ""], ["Dubey", "Harishchandra", ""], ["Mankodiya", "Kunal", ""]]}, {"id": "1712.09381", "submitter": "Richard Liaw", "authors": "Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox,\n  Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, Ion Stoica", "title": "RLlib: Abstractions for Distributed Reinforcement Learning", "comments": "Published in the International Conference on Machine Learning (ICML\n  2018), 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms involve the deep nesting of highly\nirregular computation patterns, each of which typically exhibits opportunities\nfor distributed computation. We argue for distributing RL components in a\ncomposable way by adapting algorithms for top-down hierarchical control,\nthereby encapsulating parallelism and resource requirements within\nshort-running compute tasks. We demonstrate the benefits of this principle\nthrough RLlib: a library that provides scalable software primitives for RL.\nThese primitives enable a broad range of algorithms to be implemented with high\nperformance, scalability, and substantial code reuse. RLlib is available at\nhttps://rllib.io/.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 19:43:59 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 02:40:19 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 00:01:53 GMT"}, {"version": "v4", "created": "Fri, 29 Jun 2018 00:19:24 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Liang", "Eric", ""], ["Liaw", "Richard", ""], ["Moritz", "Philipp", ""], ["Nishihara", "Robert", ""], ["Fox", "Roy", ""], ["Goldberg", "Ken", ""], ["Gonzalez", "Joseph E.", ""], ["Jordan", "Michael I.", ""], ["Stoica", "Ion", ""]]}, {"id": "1712.09388", "submitter": "Amrita Mathuriya", "authors": "Amrita Mathuriya, Thorsten Kurth, Vivek Rane, Mustafa Mustafa, Lei\n  Shao, Debbie Bard, Prabhat, Victor W Lee", "title": "Scaling GRPC Tensorflow on 512 nodes of Cori Supercomputer", "comments": "Published as a poster in NIPS 2017 Workshop: Deep Learning At\n  Supercomputer Scale", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore scaling of the standard distributed Tensorflow with GRPC\nprimitives on up to 512 Intel Xeon Phi (KNL) nodes of Cori supercomputer with\nsynchronous stochastic gradient descent (SGD), and identify causes of scaling\ninefficiency at higher node counts. To our knowledge, this is the first\nexploration of distributed GRPC Tensorflow scalability on a HPC supercomputer\nat such large scale with synchronous SGD. We studied scaling of two convolution\nneural networks - ResNet-50, a state-of-the-art deep network for classification\nwith roughly 25.5 million parameters, and HEP-CNN, a shallow topology with less\nthan 1 million parameters for common scientific usages. For ResNet-50, we\nachieve >80% scaling efficiency on up to 128 workers, using 32 parameter\nservers (PS tasks) with a steep decline down to 23% for 512 workers using 64 PS\ntasks. Our analysis of the efficiency drop points to low network bandwidth\nutilization due to combined effect of three factors. (a) Heterogeneous\ndistributed parallelization algorithm which uses PS tasks as centralized\nservers for gradient averaging is suboptimal for utilizing interconnect\nbandwidth. (b) Load imbalance among PS tasks hinders their efficient scaling.\n(c) Underlying communication primitive GRPC is currently inefficient on Cori\nhigh-speed interconnect. The HEP-CNN demands less interconnect bandwidth, and\nshows >80% weak scaling efficiency for up to 256 nodes with only 1 PS task. Our\nfindings are applicable to other deep learning networks. Big networks with\nmillions of parameters stumble upon the issues discussed here. Shallower\nnetworks like HEP-CNN with relatively lower number of parameters can\nefficiently enjoy weak scaling even with a single parameter server.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 20:00:08 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Mathuriya", "Amrita", ""], ["Kurth", "Thorsten", ""], ["Rane", "Vivek", ""], ["Mustafa", "Mustafa", ""], ["Shao", "Lei", ""], ["Bard", "Debbie", ""], ["Prabhat", "", ""], ["Lee", "Victor W", ""]]}, {"id": "1712.09494", "submitter": "EPTCS", "authors": "Nathan Cassee (Eindhoven University of Technology, Eindhoven, The\n  Netherlands), Anton Wijs (Eindhoven University of Technology, Eindhoven, The\n  Netherlands)", "title": "Analysing the Performance of GPU Hash Tables for State Space Exploration", "comments": "In Proceedings GaM 2017, arXiv:1712.08345", "journal-ref": "EPTCS 263, 2017, pp. 1-15", "doi": "10.4204/EPTCS.263.1", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, General Purpose Graphics Processors (GPUs) have been\nused to significantly speed up numerous applications. One of the areas in which\nGPUs have recently led to a significant speed-up is model checking. In model\nchecking, state spaces, i.e., large directed graphs, are explored to verify\nwhether models satisfy desirable properties. GPUexplore is a GPU-based model\nchecker that uses a hash table to efficiently keep track of already explored\nstates. As a large number of states is discovered and stored during such an\nexploration, the hash table should be able to quickly handle many inserts and\nqueries concurrently. In this paper, we experimentally compare two different\nhash tables optimised for the GPU, one being the GPUexplore hash table, and the\nother using Cuckoo hashing. We compare the performance of both hash tables\nusing random and non-random data obtained from model checking experiments, to\nanalyse the applicability of the two hash tables for state space exploration.\nWe conclude that Cuckoo hashing is three times faster than GPUexplore hashing\nfor random data, and that Cuckoo hashing is five to nine times faster for\nnon-random data. This suggests great potential to further speed up GPUexplore\nin the near future.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 05:14:23 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Cassee", "Nathan", "", "Eindhoven University of Technology, Eindhoven, The\n  Netherlands"], ["Wijs", "Anton", "", "Eindhoven University of Technology, Eindhoven, The\n  Netherlands"]]}, {"id": "1712.09552", "submitter": "Yogesh Simmhan", "authors": "Yogesh Simmhan", "title": "Big Data and Fog Computing", "comments": "To Appear as a contribution in Encyclopedia of Big Data Technologies,\n  Sherif Sakr and Albert Zomaya eds., Springer Nature, 2018", "journal-ref": "Book chapter \"Big Data and Fog Computing\", 2018. In: Encyclopedia\n  of Big Data Technologies, Sakr S., Zomaya A. (eds), Springer, Cham", "doi": "10.1007/978-3-319-63962-8_41-1", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing serves as a computing layer that sits between the edge devices\nand the cloud in the network topology. They have more compute capacity than the\nedge but much less so than cloud data centers. They typically have high uptime\nand always-on Internet connectivity. Applications that make use of the fog can\navoid the network performance limitation of cloud computing while being less\nresource constrained than edge computing. As a result, they offer a useful\nbalance of the current paradigms. This article explores various aspects of fog\ncomputing in the context of big data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 11:27:30 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Simmhan", "Yogesh", ""]]}, {"id": "1712.09645", "submitter": "Harishchandra Dubey", "authors": "Rabindra K. Barik, Satish Kumar Gudey, Gujji Giridhar Reddy, Meenakshi\n  Pant, Harishchandra Dubey, Kunal Mankodiya, Vinay Kumar", "title": "FogGrid: Leveraging Fog Computing for Enhanced Smart Grid Network", "comments": "6 pages, 10 figures, INDICON-2017 14TH IEEE India Council\n  International Conference 2017, Dec 15-17, IIT Roorkee, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present manuscript concentrates on the application of Fog computing to a\nSmart Grid Network that comprises of a Distribution Generation System known as\na Microgrid. It addresses features and advantages of a smart grid. Two\ncomputational methods for on-demand processing based on shared information\nresources is discussed. Fog Computing acts as an additional layer of\ncomputational and/or communication nodes that offload the Cloud backend from\nmulti-tasking while dealing with large amounts of data. Both Fog computing and\nCloud computing hierarchical architecture is compared with respect to efficient\nutilization of resources. To alleviate the advantages of Fog computing, a Fog\ncomputing framework based on Intel Edison is proposed. The proposed\narchitecture has been hardware implemented for a microgrid system. The results\nobtained show the efficacy of Fog Computing for smart grid network in terms of\nlow power consumption, reduced storage requirement and overlay analysis\ncapabilities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 18:19:25 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Barik", "Rabindra K.", ""], ["Gudey", "Satish Kumar", ""], ["Reddy", "Gujji Giridhar", ""], ["Pant", "Meenakshi", ""], ["Dubey", "Harishchandra", ""], ["Mankodiya", "Kunal", ""], ["Kumar", "Vinay", ""]]}, {"id": "1712.09686", "submitter": "Dietmar Berwanger", "authors": "Dietmar Berwanger and R. Ramanujam", "title": "Deviator Detection under Imperfect Monitoring", "comments": "Extended abstract, presented at the 5th Workshop on Strategic\n  Reasoning 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grim-trigger strategies are a fundamental mechanism for sustaining equilibria\nin iterated games: the players cooperate along an agreed path, and as soon as\none player deviates, the others form a coalition to play him down to his minmax\nlevel. A precondition to triggering such a strategy is that the identity of the\ndeviating player becomes common knowledge among the other players. This can be\ndifficult or impossible to attain in games where the information structure\nallows only imperfect monitoring of the played actions or of the global state.\n  We study the problem of synthesising finite-state strategies for detecting\nthe deviator from an agreed strategy profile in games played on finite graphs\nwith different information structures. We show that the problem is undecidable\nin the general case where the global state cannot be monitored. On the other\nhand, we prove that under perfect monitoring of the global state and imperfect\nmonitoring of actions, the problem becomes decidable, and we present an\neffective synthesis procedure that covers infinitely repeated games with\nprivate monitoring.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 21:09:22 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Berwanger", "Dietmar", ""], ["Ramanujam", "R.", ""]]}, {"id": "1712.09731", "submitter": "Bryan Perozzi", "authors": "Eduardo Fleury, Silvio Lattanzi, Vahab Mirrokni, Bryan Perozzi", "title": "ASYMP: Fault-tolerant Mining of Massive Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ASYMP, a distributed graph processing system developed for the\ntimely analysis of graphs with trillions of edges. ASYMP has several\ndistinguishing features including a robust fault tolerance mechanism, a\nlockless architecture which scales seamlessly to thousands of machines, and\nefficient data access patterns to reduce per-machine overhead. ASYMP is used to\nanalyze the largest graphs at Google, and the graphs we consider in our\nempirical evaluation here are, to the best of our knowledge, the largest\nconsidered in the literature.\n  Our experimental results show that compared to previous graph processing\nframeworks at Google, ASYMP can scale to larger graphs, operate on more crowded\nclusters, and complete real-world graph mining analytic tasks faster. First, we\nevaluate the speed of ASYMP, where we show that across a diverse selection of\ngraphs, it runs Connected Component 3-50x faster than state of the art\nimplementations in MapReduce and Pregel. Then we demonstrate the scalability\nand parallelism of this framework: first by showing that the running time\nincreases linearly by increasing the size of the graphs (without changing the\nnumber of machines), and then by showing the gains in running time while\nincreasing the number of machines. Finally, we demonstrate the fault-tolerance\nproperties for the framework, showing that inducing 50% of our machines to fail\nincreases the running time by only 41%.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 01:21:37 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Fleury", "Eduardo", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab", ""], ["Perozzi", "Bryan", ""]]}, {"id": "1712.09803", "submitter": "Archit Somani", "authors": "Chirag Juyal, Sandeep Kulkarni, Sweta Kumari, Sathya Peri and Archit\n  Somani", "title": "An Innovative Approach to Achieve Compositionality Efficiently using\n  Multi-Version Object Based Transactional Systems", "comments": "35 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modern era of multicore processors, utilizing cores is a tedious job.\nSynchronization and communication among processors involve high cost. Software\ntransaction memory systems (STMs) addresses this issues and provide better\nconcurrency in which programmer need not have to worry about consistency\nissues. Another advantage of STMs is that they facilitate compositionality of\nconcurrent programs with great ease. Different concurrent operations that need\nto be composed to form a single atomic unit is achieved by encapsulating them\nin a single transaction. In this paper, we introduce a new STM system as\nmulti-version object based STM (MVOSTM) which is the combination of both of\nthese ideas for harnessing greater concurrency in STMs. As the name suggests\nMVOSTM, works on a higher level and maintains multiple versions corresponding\nto each key. We have developed MVOSTM with the unlimited number of versions\ncorresponding to each key. In addition to that, we have developed garbage\ncollection for MVOSTM (MVOSTM-GC) to delete unwanted versions corresponding to\nthe keys to reduce traversal overhead. MVOSTM provides greater concurrency\nwhile reducing the number of aborts and it ensures compositionality by making\nthe transactions atomic. Here, we have used MVOSTM for the list and hash-table\ndata structure as list-MVOSTM and HT- MVOSTM. Experimental results of\nlist-MVOSTM outperform almost two to twenty fold speedup than existing\nstate-of-the-art list based STMs (Trans-list, Boosting-list, NOrec-list,\nlist-MVTO, and list-OSTM). HT-MVOSTM shows a significant performance gain of\nalmost two to nineteen times better than existing state-of-the-art hash-table\nbased STMs (ESTM, RWSTMs, HT-MVTO, and HT-OSTM). MVOSTM with list and\nhash-table shows the least number of aborts among all the existing STM\nalgorithms. MVOSTM satisfies correctness-criteria as opacity.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 10:15:54 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 10:44:20 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 05:02:02 GMT"}, {"version": "v4", "created": "Mon, 11 Jun 2018 17:18:50 GMT"}, {"version": "v5", "created": "Thu, 14 Jun 2018 04:13:22 GMT"}, {"version": "v6", "created": "Mon, 30 Jul 2018 09:43:56 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Juyal", "Chirag", ""], ["Kulkarni", "Sandeep", ""], ["Kumari", "Sweta", ""], ["Peri", "Sathya", ""], ["Somani", "Archit", ""]]}, {"id": "1712.09876", "submitter": "Emanuel Onica", "authors": "Mihai Rotaru, Florentin Olariu, Emanuel Onica, Etienne Rivi\\`ere", "title": "Reliable Messaging to Millions of Users with MigratoryData", "comments": null, "journal-ref": "Middleware 2017 - Proceedings of the 18th ACM/IFIP/USENIX\n  Middleware Conference: Industrial Track, Pages 1-7", "doi": "10.1145/3154448.3154449", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-based notification services are used by a large range of businesses to\nselectively distribute live updates to customers, following the\npublish/subscribe (pub/sub) model. Typical deployments can involve millions of\nsubscribers expecting ordering and delivery guarantees together with low\nlatencies. Notification services must be vertically and horizontally scalable,\nand adopt replication to provide a reliable service. We report our experience\nbuilding and operating MigratoryData, a highly-scalable notification service.\nWe discuss the typical requirements of MigratoryData customers, and describe\nthe architecture and design of the service, focusing on scalability and fault\ntolerance. Our evaluation demonstrates the ability of MigratoryData to handle\nmillions of concurrent connections and support a reliable notification service\ndespite server failures and network disconnections.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 14:48:28 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Rotaru", "Mihai", ""], ["Olariu", "Florentin", ""], ["Onica", "Emanuel", ""], ["Rivi\u00e8re", "Etienne", ""]]}, {"id": "1712.10056", "submitter": "Karl Palmskog", "authors": "Edgar Pek, Pranav Garg, Muntasir Raihan Rahman, Karl Palmskog,\n  Indranil Gupta, P. Madhusudan", "title": "Inferring Formal Properties of Production Key-Value Stores", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production distributed systems are challenging to formally verify, in\nparticular when they are based on distributed protocols that are not rigorously\ndescribed or fully understood. In this paper, we derive models and properties\nfor two core distributed protocols used in eventually consistent production\nkey-value stores such as Riak and Cassandra. We propose a novel modeling called\ncertified program models, where complete distributed systems are captured as\nprograms written in traditional systems languages such as concurrent C.\nSpecifically, we model the read-repair and hinted-handoff recovery protocols as\nconcurrent C programs, test them for conformance with real systems, and then\nverify that they guarantee eventual consistency, modeling precisely the\nspecification as well as the failure assumptions under which the results hold.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 20:58:46 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Pek", "Edgar", ""], ["Garg", "Pranav", ""], ["Rahman", "Muntasir Raihan", ""], ["Palmskog", "Karl", ""], ["Gupta", "Indranil", ""], ["Madhusudan", "P.", ""]]}, {"id": "1712.10081", "submitter": "Chin-Jung Hsu", "authors": "Chin-Jung Hsu, Vivek Nair, Vincent W. Freeh, Tim Menzies", "title": "Low-Level Augmented Bayesian Optimization for Finding the Best Cloud VM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of big data applications, which tends to have longer\nexecution time, choosing the right cloud VM to run these applications has\nsignificant performance as well as economic implications. For example, in our\nlarge-scale empirical study of 107 different workloads on three popular big\ndata systems, we found that a wrong choice can lead to a 20 times slowdown or\nan increase in cost by 10 times.\n  Bayesian optimization is a technique for optimizing expensive (black-box)\nfunctions. Previous attempts have only used instance-level information (such as\n# of cores, memory size) which is not sufficient to represent the search space.\nIn this work, we discover that this may lead to the fragility problem---either\nincurs high search cost or finds only the sub-optimal solution. The central\ninsight of this paper is to use low-level performance information to augment\nthe process of Bayesian Optimization. Our novel low-level augmented Bayesian\nOptimization is rarely worse than current practices and often performs much\nbetter (in 46 of 107 cases). Further, it significantly reduces the search cost\nin nearly half of our case studies.\n  Based on this work, we conclude that it is often insufficient to use\ngeneral-purpose off-the-shelf methods for configuring cloud instances without\naugmenting those methods with essential systems knowledge such as CPU\nutilization, working memory size and I/O wait time.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 23:10:32 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Hsu", "Chin-Jung", ""], ["Nair", "Vivek", ""], ["Freeh", "Vincent W.", ""], ["Menzies", "Tim", ""]]}, {"id": "1712.10114", "submitter": "Jalal Khamse-Ashari", "authors": "Jalal Khamse-Ashari, Ioannis Lambadaris, George Kesidis, Bhuvan\n  Urgaonkar, Yiqiang Zhao", "title": "An Efficient and Fair Multi-Resource Allocation Mechanism for\n  Heterogeneous Servers", "comments": "Technical Report, 20 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and fair allocation of multiple types of resources is a crucial\nobjective in a cloud/distributed computing cluster. Users may have diverse\nresource needs. Furthermore, diversity in server properties/ capabilities may\nmean that only a subset of servers may be usable by a given user. In platforms\nwith such heterogeneity, we identify important limitations in existing\nmulti-resource fair allocation mechanisms, notably Dominant Resource Fairness\n(DRF) and its follow-up work. To overcome such limitations, we propose a new\nserver-based approach; each server allocates resources by maximizing a\nper-server utility function. We propose a specific class of utility functions\nwhich, when appropriately parameterized, adjusts the trade-off between\nefficiency and fairness, and captures a variety of fairness measures (such as\nour recently proposed Per-Server Dominant Share Fairness). We establish\nconditions for the proposed mechanism to satisfy certain properties that are\ngenerally deemed desirable, e.g., envy-freeness, sharing incentive, bottleneck\nfairness, and Pareto optimality. To implement our resource allocation\nmechanism, we develop an iterative algorithm which is shown to be globally\nconvergent. Finally, we show how the proposed mechanism could be implemented in\na distributed fashion. We carry out extensive trace-driven simulations to show\nthe enhanced performance of our proposed mechanism over the existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 04:40:17 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Khamse-Ashari", "Jalal", ""], ["Lambadaris", "Ioannis", ""], ["Kesidis", "George", ""], ["Urgaonkar", "Bhuvan", ""], ["Zhao", "Yiqiang", ""]]}, {"id": "1712.10128", "submitter": "Mihailo Jovanovic", "authors": "Neil K. Dhingra, Marcello Colombino, Mihailo R. Jovanovi\\'c", "title": "Structured decentralized control of positive systems with applications\n  to combination drug therapy and leader selection in directed networks", "comments": "11 pages, 7 figures", "journal-ref": "IEEE Trans. Control Netw. Syst., vol. 6, no. 1, pp. 352-362, March\n  2019", "doi": "10.1109/TCNS.2018.2820499", "report-no": null, "categories": "math.OC cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of structured optimal control problems in which the main\ndiagonal of the dynamic matrix is a linear function of the design variable.\nWhile such problems are in general challenging and nonconvex, for positive\nsystems we prove convexity of the $H_2$ and $H_\\infty$ optimal control\nformulations which allow for arbitrary convex constraints and regularization of\nthe control input. Moreover, we establish differentiability of the $H_\\infty$\nnorm when the graph associated with the dynamical generator is weakly connected\nand develop a customized algorithm for computing the optimal solution even in\nthe absence of differentiability. We apply our results to the problems of\nleader selection in directed consensus networks and combination drug therapy\nfor HIV treatment. In the context of leader selection, we address the\ncombinatorial challenge by deriving upper and lower bounds on optimal\nperformance. For combination drug therapy, we develop a customized subgradient\nmethod for efficient treatment of diseases whose mutation patterns are not\nconnected.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 06:38:32 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 08:41:38 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Dhingra", "Neil K.", ""], ["Colombino", "Marcello", ""], ["Jovanovi\u0107", "Mihailo R.", ""]]}, {"id": "1712.10201", "submitter": "Sathish Vadhiyar", "authors": "Prakash Murali and Sathish Vadhiyar", "title": "Metascheduling of HPC Jobs in Day-Ahead Electricity Markets", "comments": "Appears in IEEE Transactions on Parallel and Distributed Systems", "journal-ref": null, "doi": "10.1109/TPDS.2017.2769082", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance grid computing is a key enabler of large scale collaborative\ncomputational science. With the promise of exascale computing, high performance\ngrid systems are expected to incur electricity bills that grow super-linearly\nover time. In order to achieve cost effectiveness in these systems, it is\nessential for the scheduling algorithms to exploit electricity price\nvariations, both in space and time, that are prevalent in the dynamic\nelectricity price markets. In this paper, we present a metascheduling algorithm\nto optimize the placement of jobs in a compute grid which consumes electricity\nfrom the day-ahead wholesale market. We formulate the scheduling problem as a\nMinimum Cost Maximum Flow problem and leverage queue waiting time and\nelectricity price predictions to accurately estimate the cost of job execution\nat a system. Using trace based simulation with real and synthetic workload\ntraces, and real electricity price data sets, we demonstrate our approach on\ntwo currently operational grids, XSEDE and NorduGrid. Our experimental setup\ncollectively constitute more than 433K processors spread across 58 compute\nsystems in 17 geographically distributed locations. Experiments show that our\napproach simultaneously optimizes the total electricity cost and the average\nresponse time of the grid, without being unfair to users of the local batch\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 12:25:28 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Murali", "Prakash", ""], ["Vadhiyar", "Sathish", ""]]}, {"id": "1712.10222", "submitter": "Simina Br\\^anzei", "authors": "Simina Br\\^anzei and Erel Segal-Halevi and Aviv Zohar", "title": "How to Charge Lightning", "comments": "Presented at Scaling Bitcoin 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-chain transaction channels represent one of the leading techniques to\nscale the transaction throughput in cryptocurrencies. However, the economic\neffect of transaction channels on the system has not been explored much until\nnow. We study the economics of Bitcoin transaction channels, and present a\nframework for an economic analysis of the lightning network and its effect on\ntransaction fees on the blockchain. Our framework allows us to reason about\ndifferent patterns of demand for transactions and different topologies of the\nlightning network, and to derive the resulting fees for transacting both on and\noff the blockchain.\n  Our initial results indicate that while the lightning network does allow for\na substantially higher number of transactions to pass through the system, it\ndoes not necessarily provide higher fees to miners, and as a result may in fact\nlead to lower participation in mining within the system.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 13:33:46 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Br\u00e2nzei", "Simina", ""], ["Segal-Halevi", "Erel", ""], ["Zohar", "Aviv", ""]]}]