[{"id": "1203.0321", "submitter": "Niels Drost", "authors": "Niels Drost, Jason Maassen, Maarten A.J. van Meersbergen, Henri E.\n  Bal, F. Inti Pelupessy, Simon Portegies Zwart, Michael Kliphuis, Henk A.\n  Dijkstra and Frank J. Seinstra", "title": "High-Performance Distributed Multi-Model / Multi-Kernel Simulations: A\n  Case-Study in Jungle Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance scientific applications require more and more compute power.\nThe concurrent use of multiple distributed compute resources is vital for\nmaking scientific progress. The resulting distributed system, a so-called\nJungle Computing System, is both highly heterogeneous and hierarchical,\npotentially consisting of grids, clouds, stand-alone machines, clusters,\ndesktop grids, mobile devices, and supercomputers, possibly with accelerators\nsuch as GPUs.\n  One striking example of applications that can benefit greatly of Jungle\nComputing Systems are Multi-Model / Multi-Kernel simulations. In these\nsimulations, multiple models, possibly implemented using different techniques\nand programming models, are coupled into a single simulation of a physical\nsystem. Examples include the domain of computational astrophysics and climate\nmodeling.\n  In this paper we investigate the use of Jungle Computing Systems for such\nMulti-Model / Multi-Kernel simulations. We make use of the software developed\nin the Ibis project, which addresses many of the problems faced when running\napplications on Jungle Computing Systems. We create a prototype Jungle-aware\nversion of AMUSE, an astrophysical simulation framework. We show preliminary\nexperiments with the resulting system, using clusters, grids, stand-alone\nmachines, and GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 21:38:04 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Drost", "Niels", ""], ["Maassen", "Jason", ""], ["van Meersbergen", "Maarten A. J.", ""], ["Bal", "Henri E.", ""], ["Pelupessy", "F. Inti", ""], ["Zwart", "Simon Portegies", ""], ["Kliphuis", "Michael", ""], ["Dijkstra", "Henk A.", ""], ["Seinstra", "Frank J.", ""]]}, {"id": "1203.0429", "submitter": "Pierre de Leusse", "authors": "Theo Dimitrakos, David Brossard and Pierre de Leusse", "title": "Securing business operations in an SOA", "comments": null, "journal-ref": "BT Technology Journal, vol.27, no.2, 2009", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service-oriented infrastructures pose new challenges in a number of areas,\nnotably with regard to security and dependability. BT has developed a\ncombination of innovative security solutions and governance frameworks that can\naddress these challenges. They include advances in identity federation;\ndistributed usage and access management; context-aware secure messaging,\nrouting and transformation; and (security) policy governance for\nservice-oriented architectures. This paper discusses these developments and the\nsteps being taken to validate their functionality and performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 11:51:20 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Dimitrakos", "Theo", ""], ["Brossard", "David", ""], ["de Leusse", "Pierre", ""]]}, {"id": "1203.0432", "submitter": "Pierre de Leusse", "authors": "Pierre de Leusse and Krzysztof Zielinski", "title": "Toward Governance of Cross-Cloud Application Deployment", "comments": null, "journal-ref": "ServiceWave 2011, Second Optimising Cloud Services Workshop", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the authors introduce the main ideas around the governance\nof cross-Cloud application deployment and their related concepts. It is argued\nthat, due to the increasing complexity and nature of the Cloud market, an\nintermediary specialized in brokering the deployment of different components of\na same application onto different Cloud products could both facilitate said\ndeployment and in some cases improve its quality in terms of cost, security &\nreliability and QoS. In order to fulfill these objectives, the authors propose\na high level architecture that relies on their previous work on governance of\npolicy & rule driven distributed systems. This architecture aims at supplying\nfive main functions of 1) translation of Service Level Agreements (SLAs) and\npricing into a common shared DSL, 2) correlation of analytical data (e.g.\nmonitoring, metering), 3) combination of Cloud products, 4) information from\nthird parties regarding different aspects of Quality of Service (QoS) and 5)\ncross-Cloud application deployment specification and governance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 12:03:02 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["de Leusse", "Pierre", ""], ["Zielinski", "Krzysztof", ""]]}, {"id": "1203.0435", "submitter": "Pierre de Leusse", "authors": "Pierre de Leusse, Bartosz Kwolek and Krzysztof Zielinski", "title": "A common interface for multi-rule-engine distributed systems", "comments": null, "journal-ref": "RuleML-2010 Challenge: 4th international rule challenge : Web rule\n  symposium : Washington, DC, USA, October, 21-23, 2010, eds. Monica Palmirani,\n  [et al.]", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rule technological landscape is becoming ever more complex, with an\nextended number of specifications and products. It is therefore becoming\nincreasingly difficult to integrate rule-driven components and manage\ninteroperability in multi-rule engine environments. The described work presents\nthe possibility to provide a common interface for rule-driven components in a\ndistributed system. The authors' approach leverages on a set of discovery\nprotocol, rule interchange and user interface to alleviate the environment's\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 12:07:57 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["de Leusse", "Pierre", ""], ["Kwolek", "Bartosz", ""], ["Zielinski", "Krzysztof", ""]]}, {"id": "1203.0443", "submitter": "Pierre de Leusse", "authors": "Pierre de Leusse, Panos Periorellis, Paul Watson, Andreas Maierhofer", "title": "Secure & Rapid Composition of Infrastructure Services in the Cloud", "comments": null, "journal-ref": "SENSORCOMM '08. Second International Conference on , vol., no.,\n  pp.770-775, 25-31 Aug. 2008", "doi": "10.1109/SENSORCOMM.2008.130", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental ambition of grid and distributed systems is to be capable of\nsustaining evolution and allowing for adaptability ((F. Losavio et al., 2002),\n(S. Radhakrishnan, 2005)). Furthermore, as the complexity and sophistication of\ntheses structures increases, so does the need for adaptability of each\ncomponent. One of the primary benefits of service oriented architecture (SOA)\nis the ability to compose applications, processes or more complex services from\nother services which increases the capacity for adaptation. This document\nproposes a novel infrastructure composition model that aims at increasing the\nadaptability of the capabilities exposed through it by dynamically managing\ntheir non functional requirements.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 12:25:09 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["de Leusse", "Pierre", ""], ["Periorellis", "Panos", ""], ["Watson", "Paul", ""], ["Maierhofer", "Andreas", ""]]}, {"id": "1203.0651", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Albert Y. Zomaya, Ali Javadzadeh Boloori,\n  Javid Taheri", "title": "On Modeling Dependency between MapReduce Configuration Parameters and\n  Total Execution Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an analytical method to model the dependency\nbetween configuration parameters and total execution time of Map-Reduce\napplications. Our approach has three key phases: profiling, modeling, and\nprediction. In profiling, an application is run several times with different\nsets of MapReduce configuration parameters to profile the execution time of the\napplication on a given platform. Then in modeling, the relation between these\nparameters and total execution time is modeled by multivariate linear\nregression. Among the possible configuration parameters, two main parameters\nhave been used in this study: the number of Mappers, and the number of\nReducers. For evaluation, two standard applications (WordCount, and Exim\nMainlog parsing) are utilized to evaluate our technique on a 4-node MapReduce\nplatform.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 13:18:51 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Zomaya", "Albert Y.", ""], ["Boloori", "Ali Javadzadeh", ""], ["Taheri", "Javid", ""]]}, {"id": "1203.0740", "submitter": "Bo  Li", "authors": "Bo Li, Yijian Pei, Bin Shen, Hao Wu, Min He, Jundong Yang", "title": "Resource Availability-Aware Advance Reservation for Parallel Jobs with\n  Deadlines", "comments": "25 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Advance reservation is important to guarantee the quality of services of jobs\nby allowing exclusive access to resources over a defined time interval on\nresources. It is a challenge for the scheduler to organize available resources\nefficiently and to allocate them for parallel AR jobs with deadline constraint\nappropriately. This paper provides a slot-based data structure to organize\navailable resources of multiprocessor systems in a way that enables efficient\nsearch and update operations, and formulates a suite of scheduling policies to\nallocate resources for dynamically arriving AR requests. The performance of the\nscheduling algorithms were investigated by simulations with different job sizes\nand durations, system loads and scheduling flexibilities. Simulation results\nshow that job sizes and durations, system load and the flexibility of\nscheduling will impact the performance metrics of all the scheduling\nalgorithms, and the PE-Worst-Fit algorithm becomes the best algorithm for the\nscheduler with the highest acceptance rate of AR requests, and the jobs with\nthe First-Fit algorithm experience the lowest average slowdown. The data\nstructure and scheduling policies can be used to organize and allocate\nresources for parallel AR jobs with deadline constraint in large-scale\ncomputing systems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2012 14:11:56 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Li", "Bo", ""], ["Pei", "Yijian", ""], ["Shen", "Bin", ""], ["Wu", "Hao", ""], ["He", "Min", ""], ["Yang", "Jundong", ""]]}, {"id": "1203.0964", "submitter": "Fernando Brito e Abreu", "authors": "Fernando Brito e Abreu", "title": "The cloud paradigm: Are you tuned for the lyrics?", "comments": "Position paper to introduce a keynote, proceedings of WAMPS'2011 - VI\n  Annual MPS.BR Workshop, pp. 20-25, Campinas, Brazil, October 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major players, business angels and opinion-makers are broadcasting beguiled\nlyrics on the most recent IT hype: your software should ascend to the clouds.\nThere are many clouds and the stake is high. Distractedly, many of us became\nassiduous users of the cloud, but perhaps due to the legacy systems and legacy\nknowledge, IT professionals, mainly those many that work in business\ninformation systems for the long tail, are not as much plunged into producing\ncloud-based systems for their clients.\n  This keynote will delve into several aspects of this cloud paradigm, from\nmore generic concerns regarding security and value for money, to more specific\nworries that reach software engineers in general. Do we need a different\nsoftware development process? Are development techniques and tools mature\nenough? What about the role of open-source in the cloud? How do we assess the\nquality in cloud-based development? Please stay tuned for more!\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2012 09:50:28 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Abreu", "Fernando Brito e", ""]]}, {"id": "1203.1395", "submitter": "Anirban Kundu (PhD)", "authors": "Anirban Kundu, Chunlin Ji", "title": "Swarm Behavior of Intelligent Cloud", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the main aim is to exhibit swarm intelligence power in cloud\nbased scenario. Heterogeneous environment has been configured at server-side\nnetwork of the whole cloud network. In the proposed system, different types of\nservers are being used to manage useful assorted atmosphere. Swarm intelligence\nhas been adopted for enhancing the performance of overall system network.\nSpecific location at server-side of the network is going to be selected by the\nswarm intelligence concept for accessing desired elements. Flexibility,\nrobustness and self-organization, which are to be considered at the time of\ndesigning the system environment, are the main features of swarm intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 08:02:35 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["Kundu", "Anirban", ""], ["Ji", "Chunlin", ""]]}, {"id": "1203.1463", "submitter": "Anuj Gupta", "authors": "Anuj Gupta and Prasant Gopal and Piyush Bansal and Kannan Srinathan", "title": "A New Look at Composition of Authenticated Byzantine Generals", "comments": "27 pages. Keywords: Protocol composition, Authenticated Byzantine\n  Generals, Universal composability, Unique session identifiers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Authenticated Byzantine Generals (ABG) aims to simulate a\nvirtual reliable broadcast channel from the General to all the players via a\nprotocol over a real (point-to-point) network in the presence of faults. We\npropose a new model to study the self-composition of ABG protocols. The central\ndogma of our approach can be phrased as follows: Consider a player who\ndiligently executes (only) the delegated protocol but the adversary steals some\nprivate information from him. Should such a player be considered faulty? With\nrespect to ABG protocols, we argue that the answer has to be no.\n  In the new model we show that in spite of using unique session identifiers,\nif $n < 2t$, there cannot exist any ABG protocol that composes in parallel even\ntwice. Further, for $n \\geq 2t$, we design ABG protocols that compose for any\nnumber of parallel executions. Besides investigating the composition of ABG\nunder a new light, our work also brings out several new insights into Canetti's\nUniversal Composability framework. Specifically, we show that there are several\nundesirable effects if one deviates from our dogma. This provides further\nevidence as to why our dogma is the right framework to study the composition of\nABG protocols.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 13:29:31 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2012 04:08:55 GMT"}], "update_date": "2012-07-11", "authors_parsed": [["Gupta", "Anuj", ""], ["Gopal", "Prasant", ""], ["Bansal", "Piyush", ""], ["Srinathan", "Kannan", ""]]}, {"id": "1203.1466", "submitter": "Riccardo Murri", "authors": "Riccardo Murri and Sergio Maffioletti", "title": "Batch-oriented software appliances", "comments": "11 pages, no figures. Submitted to VTDC'12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents AppPot, a system for creating Linux software appliances.\nAppPot can be run as a regular batch or grid job and executed in user space,\nand requires no special virtualization support in the infrastructure.\n  The main design goal of AppPot is to bring the benefits of a\nvirtualization-based IaaS cloud to existing batch-oriented computing\ninfrastructures.\n  In particular, AppPot addresses the application deployment and configuration\non large heterogeneous computing infrastructures: users are enabled to prepare\ntheir own customized virtual appliance for providing a safe execution\nenvironment for their applications. These appliances can then be executed on\nvirtually any computing infrastructure being in a private or public cloud as\nwell as any batch-controlled computing clusters the user may have access to.\n  We give an overview of AppPot and its features, the technology that makes it\npossible, and report on experiences running it in production use within the\nSwiss National Grid infrastructure SMSCG.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 13:45:24 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["Murri", "Riccardo", ""], ["Maffioletti", "Sergio", ""]]}, {"id": "1203.1505", "submitter": "Pascal Bianchi", "authors": "Pascal Bianchi and Gersende Fort and Walid Hachem", "title": "Performance of a Distributed Stochastic Approximation Algorithm", "comments": "IEEE Transactions on Information Theory 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a distributed stochastic approximation algorithm is studied.\nApplications of such algorithms include decentralized estimation, optimization,\ncontrol or computing. The algorithm consists in two steps: a local step, where\neach node in a network updates a local estimate using a stochastic\napproximation algorithm with decreasing step size, and a gossip step, where a\nnode computes a local weighted average between its estimates and those of its\nneighbors. Convergence of the estimates toward a consensus is established under\nweak assumptions. The approach relies on two main ingredients: the existence of\na Lyapunov function for the mean field in the agreement subspace, and a\ncontraction property of the random matrices of weights in the subspace\northogonal to the agreement subspace. A second order analysis of the algorithm\nis also performed under the form of a Central Limit Theorem. The\nPolyak-averaged version of the algorithm is also considered.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 15:44:31 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 20:53:04 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Bianchi", "Pascal", ""], ["Fort", "Gersende", ""], ["Hachem", "Walid", ""]]}, {"id": "1203.1681", "submitter": "Robert Lychev", "authors": "Robert Lychev and Sharon Goldberg and Michael Schapira", "title": "Network-Destabilizing Attacks", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Border Gateway Protocol (BGP) sets up routes between the smaller networks\nthat make up the Internet. Despite its crucial role, BGP is notoriously\nvulnerable to serious problems, including (1) propagation of bogus routing\ninformation due to attacks or misconfigurations, and (2) network instabilities\nin the form of persistent routing oscillations. The conditions required to\navoid BGP instabilities are quite delicate. How, then, can we explain the\nobserved stability of today's Internet in the face of common configuration\nerrors and attacks? This work explains this phenomenon by first noticing that\nalmost every observed attack and misconfiguration to date shares a common\ncharacteristic: even when a router announces egregiously bogus information, it\nwill continue to announce the same bogus information for the duration of its\nattack/misconfiguration. We call these the \"fixed-route attacks\", and show\nthat, while even simple fixed-route attacks can destabilize a network, the\ncommercial routing policies used in today's Internet prevent such attacks from\ncreating instabilities.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 02:33:19 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2012 22:33:18 GMT"}], "update_date": "2012-09-03", "authors_parsed": [["Lychev", "Robert", ""], ["Goldberg", "Sharon", ""], ["Schapira", "Michael", ""]]}, {"id": "1203.1715", "submitter": "Dohy Hong", "authors": "Dohy Hong", "title": "D-iteration: Evaluation of a Dynamic Partition Strategy", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present a first evaluation of a dynamic partition\nstrategy associated to the recently proposed asynchronous distributed\ncomputation scheme based on the D-iteration approach. The D-iteration is a\nfluid diffusion point of view based iteration method to solve numerically\nlinear equations. Using a simple static partition strategy, it has been shown\nthat, when the computation is distributed over K virtual machines (PIDs), the\nmemory size to be handled by each virtual machine decreases linearly with K and\nthe computation speed increases almost linearly with K with a slope becoming\ncloser to one when the number N of linear equations to be solved increases.\nHere, we want to evaluate how further those results can be improved when a\nsimple dynamic partition strategy is deployed and to show that the dynamic\npartition strategy allows one to control and equalize the computation load\nbetween PIDs without any deep analysis of the matrix or of the underlying graph\nstructure.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 09:18:46 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2012 10:17:25 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Hong", "Dohy", ""]]}, {"id": "1203.1728", "submitter": "Ajith Nongmaithem Mr.", "authors": "N. Ajith Singh and M. Hemalatha", "title": "High performance computing network for cloud environment using\n  simulators", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is the next generation computing. Adopting the cloud\ncomputing is like signing up new form of a website. The GUI which controls the\ncloud computing make is directly control the hardware resource and your\napplication. The difficulty part in cloud computing is to deploy in real\nenvironment. Its' difficult to know the exact cost and it's requirement until\nand unless we buy the service not only that whether it will support the\nexisting application which is available on traditional data center or had to\ndesign a new application for the cloud computing environment. The security\nissue, latency, fault tolerance are some parameter which we need to keen care\nbefore deploying, all this we only know after deploying but by using simulation\nwe can do the experiment before deploying it to real environment. By simulation\nwe can understand the real environment of cloud computing and then after it\nsuccessful result we can start deploying your application in cloud computing\nenvironment. By using the simulator it will save us lots of time and money.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 10:06:49 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Singh", "N. Ajith", ""], ["Hemalatha", "M.", ""]]}, {"id": "1203.1730", "submitter": "Anh Le", "authors": "Anh Le, Athina Markopoulou, Alexandros G. Dimakis", "title": "Auditing for Distributed Storage Systems", "comments": "ToN 2014 Submission with Data Dynamics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage codes have recently received a lot of attention in the\ncommunity. Independently, another body of work has proposed integrity checking\nschemes for cloud storage, none of which, however, is customized for\ncoding-based storage or can efficiently support repair. In this work, we bridge\nthe gap between these two currently disconnected bodies of work. We propose\nNC-Audit, a novel cryptography-based remote data integrity checking scheme,\ndesigned specifically for network coding-based distributed storage systems.\nNC-Audit combines, for the first time, the following desired properties: (i)\nefficient checking of data integrity, (ii) efficient support for repairing\nfailed nodes, and (iii) protection against information leakage when checking is\nperformed by a third party. The key ingredient of the design of NC-Audit is a\nnovel combination of SpaceMac, a homomorphic message authentication code (MAC)\nscheme for network coding, and NCrypt, a novel chosen-plaintext attack (CPA)\nsecure encryption scheme that is compatible with SpaceMac. Our evaluation of a\nJava implementation of NC-Audit shows that an audit costs the storage node and\nthe auditor a modest amount computation time and lower bandwidth than prior\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 10:10:34 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2012 05:58:33 GMT"}, {"version": "v3", "created": "Sat, 2 Jun 2012 02:05:18 GMT"}, {"version": "v4", "created": "Wed, 15 May 2013 11:49:43 GMT"}, {"version": "v5", "created": "Mon, 12 May 2014 21:10:23 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Le", "Anh", ""], ["Markopoulou", "Athina", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1203.1888", "submitter": "Lewis Tseng", "authors": "Nitin Vaidya", "title": "Matrix Representation of Iterative Approximate Byzantine Consensus in\n  Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a proof of correctness of an iterative approximate\nByzantine consensus (IABC) algorithm for directed graphs. The iterative\nalgorithm allows fault- free nodes to reach approximate conensus despite the\npresence of up to f Byzantine faults. Necessary conditions on the underlying\nnetwork graph for the existence of a correct IABC algorithm were shown in our\nrecent work [15, 16]. [15] also analyzed a specific IABC algorithm and showed\nthat it performs correctly in any network graph that satisfies the necessary\ncondition, proving that the necessary condition is also sufficient. In this\npaper, we present an alternate proof of correctness of the IABC algorithm,\nusing a familiar technique based on transition matrices [9, 3, 17, 19].\n  The key contribution of this paper is to exploit the following observation:\nfor a given evolution of the state vector corresponding to the state of the\nfault-free nodes, many alternate state transition matrices may be chosen to\nmodel that evolution cor- rectly. For a given state evolution, we identify one\napproach to suitably \"design\" the transition matrices so that the standard\ntools for proving convergence can be applied to the Byzantine fault-tolerant\nalgorithm as well. In particular, the transition matrix for each iteration is\ndesigned such that each row of the matrix contains a large enough number of\nelements that are bounded away from 0.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 19:00:19 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Vaidya", "Nitin", ""]]}, {"id": "1203.1900", "submitter": "Hannah Arendt", "authors": "Hannah Arendt, Jorgensen Jost", "title": "Consensus on Moving Neighborhood Model of Peterson Graph", "comments": "Submitted to European Physical Journal B", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the consensus problem of multiple agents on a kind of\nfamous graph, Peterson graph. It is an undirected graph with 10 vertices and 15\nedges. Each agent randomly walks on this graph and communicates with each other\nif and only if they coincide on a node at the same time. We conduct numerical\nstudy on the consensus problem in this framework and show that global consensus\ncan be achieved.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 19:54:31 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Arendt", "Hannah", ""], ["Jost", "Jorgensen", ""]]}, {"id": "1203.2081", "submitter": "Matthew Felice Pace", "authors": "Matthew Felice Pace (University of Warwick)", "title": "BSP vs MapReduce", "comments": "13 pages, appeared at ICCS 2012", "journal-ref": null, "doi": "10.1016/j.procs.2012.04.026", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MapReduce framework has been generating a lot of interest in a wide range\nof areas. It has been widely adopted in industry and has been used to solve a\nnumber of non-trivial problems in academia. Putting MapReduce on strong\ntheoretical foundations is crucial in understanding its capabilities. This work\nlinks MapReduce to the BSP model of computation, underlining the relevance of\nBSP to modern parallel algorithm design and defining a subclass of BSP\nalgorithms that can be efficiently implemented in MapReduce.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 13:42:03 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2012 23:06:58 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Pace", "Matthew Felice", "", "University of Warwick"]]}, {"id": "1203.2366", "submitter": "Johan Montagnat", "authors": "Franck Michel, Johan Montagnat, Tristan Glatard (CREATIS)", "title": "Technical support for Life Sciences communities on a production grid\n  infrastructure", "comments": "HealthGrid'12, Amsterdam : Netherlands (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production operation of large distributed computing infrastructures (DCI)\nstill requires a lot of human intervention to reach acceptable quality of\nservice. This may be achievable for scientific communities with solid IT\nsupport, but it remains a show-stopper for others. Some application execution\nenvironments are used to hide runtime technical issues from end users. But they\nmostly aim at fault-tolerance rather than incident resolution, and their\noperation still requires substantial manpower. A longer-term support activity\nis thus needed to ensure sustained quality of service for Virtual Organisations\n(VO). This paper describes how the biomed VO has addressed this challenge by\nsetting up a technical support team. Its organisation, tooling, daily tasks,\nand procedures are described. Results are shown in terms of resource usage by\nend users, amount of reported incidents, and developed software tools. Based on\nour experience, we suggest ways to measure the impact of the technical support,\nperspectives to decrease its human cost and make it more community-specific.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 19:22:51 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Michel", "Franck", "", "CREATIS"], ["Montagnat", "Johan", "", "CREATIS"], ["Glatard", "Tristan", "", "CREATIS"]]}, {"id": "1203.2946", "submitter": "Zbigniew Koza", "authors": "Zbigniew Koza, Maciej Matyka, Sebastian Szkoda, and {\\L}ukasz\n  Miros{\\l}aw", "title": "Compressed Multi-Row Storage Format for Sparse Matrices on Graphics\n  Processing Units", "comments": null, "journal-ref": "SIAM J. Sci. Comput. 36-2 (2014), pp. C219-C239", "doi": "10.1137/120900216", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new format for storing sparse matrices is proposed for efficient sparse\nmatrix-vector (SpMV) product calculation on modern graphics processing units\n(GPUs). This format extends the standard compressed row storage (CRS) format\nand can be quickly converted to and from it. Computational performance of two\nSpMV kernels for the new format is determined for over 130 sparse matrices on\nFermi-class and Kepler-class GPUs and compared with that of five existing\ngeneric algorithms and industrial implementations, including Nvidia cuSparse\nCSR and HYB kernels. We found the speedup of up to $\\approx 60%$ over the best\nof the five alternative kernels.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 20:39:24 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 21:19:43 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Koza", "Zbigniew", ""], ["Matyka", "Maciej", ""], ["Szkoda", "Sebastian", ""], ["Miros\u0142aw", "\u0141ukasz", ""]]}, {"id": "1203.3013", "submitter": "Marko Obrovac", "authors": "Marin Bertier (INRIA - IRISA), Marko Obrovac (INRIA - IRISA), C\\'edric\n  Tedeschi (INRIA - IRISA)", "title": "A Protocol for the Atomic Capture of Multiple Molecules at Large Scale", "comments": "13th International Conference on Distributed Computing and Networking\n  (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of service-oriented computing, applications are more and more\nbased on coordination of autonomous services. Envisioned over largely\ndistributed and highly dynamic platforms, expressing this coordination calls\nfor alternative programming models. The chemical programming paradigm, which\nmodels applications as chemical solutions where molecules representing digital\nentities involved in the computation, react together to produce a result, has\nbeen recently shown to provide the needed abstractions for autonomic\ncoordination of services. However, the execution of such programs over large\nscale platforms raises several problems hindering this paradigm to be actually\nleveraged. Among them, the atomic capture of molecules participating in concur-\nrent reactions is one of the most significant. In this paper, we propose a\nprotocol for the atomic capture of these molecules distributed and evolving\nover a large scale platform. As the density of possible reactions is crucial\nfor the liveness and efficiency of such a capture, the protocol proposed is\nmade up of two sub-protocols, each of them aimed at addressing different levels\nof densities of potential reactions in the solution. While the decision to\nchoose one or the other is local to each node participating in a program's\nexecution, a global coherent behaviour is obtained. Proof of liveness, as well\nas intensive simulation results showing the efficiency and limited overhead of\nthe protocol are given.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 07:30:11 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Bertier", "Marin", "", "INRIA - IRISA"], ["Obrovac", "Marko", "", "INRIA - IRISA"], ["Tedeschi", "C\u00e9dric", "", "INRIA - IRISA"]]}, {"id": "1203.3092", "submitter": "Riccardo Murri", "authors": "S\\'ebastien Moretti, Riccardo Murri, Sergio Maffioletti, Arnold\n  Kuzniar, Bris\\'e\\\"is Castella, Nicolas Salamin, Marc Robinson-Rechavi, and\n  Heinz Stockinger", "title": "gcodeml: A Grid-enabled Tool for Detecting Positive Selection in\n  Biological Evolution", "comments": "10 pages, 4 figures. To appear in the HealthGrid 2012 conf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important questions in biological evolution is to know if certain\nchanges along protein coding genes have contributed to the adaptation of\nspecies. This problem is known to be biologically complex and computationally\nvery expensive. It, therefore, requires efficient Grid or cluster solutions to\novercome the computational challenge. We have developed a Grid-enabled tool\n(gcodeml) that relies on the PAML (codeml) package to help analyse large\nphylogenetic datasets on both Grids and computational clusters. Although we\nreport on results for gcodeml, our approach is applicable and customisable to\nrelated problems in biology or other scientific domains.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 14:08:12 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Moretti", "S\u00e9bastien", ""], ["Murri", "Riccardo", ""], ["Maffioletti", "Sergio", ""], ["Kuzniar", "Arnold", ""], ["Castella", "Bris\u00e9\u00efs", ""], ["Salamin", "Nicolas", ""], ["Robinson-Rechavi", "Marc", ""], ["Stockinger", "Heinz", ""]]}, {"id": "1203.3098", "submitter": "Ahmet Husainov A.", "authors": "Ahmet A. Husainov, Ekaterina S. Kudryashova", "title": "Generalized Asynchronous Systems", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is devoted to a mathematical model of concurrency the special case\nof which is asynchronous system. Distributed asynchronous automata are\nintroduced here. It is proved that the Petri nets and transition systems with\nindependence can be considered like the distributed asynchronous automata. Time\ndistributed asynchronous automata are defined in standard way by the map which\nassigns time intervals to events. It is proved that the time distributed\nasynchronous automata are generalized the time Petri nets and asynchronous\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 14:37:25 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Husainov", "Ahmet A.", ""], ["Kudryashova", "Ekaterina S.", ""]]}, {"id": "1203.3351", "submitter": "Amad Mourad", "authors": "Mourad Amad, Ahmed Meddahi, Djamil A\\\"issani", "title": "Peer to Peer Networks Management Survey", "comments": "10 pages, 1 figure", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 3, January 2012", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer-to-Peer systems are based on the concept of resources localization and\nmutualisation in dynamic context. In specific environment such as mobile\nnetworks, characterized by high variability and dynamicity of network\nconditions and performances, where nodes can join and leave the network\ndynamically, resources reliability and availability constitute a critical\nissue. The resource discovery problem arises in the context of peer to peer\n(P2P) networks, where at any point of time a peer may be placed at or removed\nfrom any location over a general purpose network. Locating a resource or\nservice efficiently is one of the most important issues related to peer-to-peer\nnetworks. The objective of a search mechanism is to successfully locate\nresources while incurring low overhead and low delay. This paper presents a\nsurvey on P2P networks management: classification, applications, platforms,\nsimulators and security.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 12:57:56 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Amad", "Mourad", ""], ["Meddahi", "Ahmed", ""], ["A\u00efssani", "Djamil", ""]]}, {"id": "1203.3575", "submitter": "Swan Dubois", "authors": "Swan Dubois (LIP6), S\\'ebastien Tixeuil (LIP6, IUF), Nini Zhu (LIP6)", "title": "The Byzantine Brides Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the hardness of establishing as many stable marriages (that\nis, marriages that last forever) in a population whose memory is placed in some\narbitrary state with respect to the considered problem, and where traitors try\nto jeopardize the whole process by behaving in a harmful manner. On the\nnegative side, we demonstrate that no solution that is completely insensitive\nto traitors can exist, and we propose a protocol for the problem that is\noptimal with respect to the traitor containment radius.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 21:26:34 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Dubois", "Swan", "", "LIP6"], ["Tixeuil", "S\u00e9bastien", "", "LIP6, IUF"], ["Zhu", "Nini", "", "LIP6"]]}, {"id": "1203.3885", "submitter": "Bogdan Alexandru Caprarescu", "authors": "Bogdan Alexandru Caprarescu, Dana Petcu", "title": "Decentralized Probabilistic Auto-Scaling for Heterogeneous Systems", "comments": "Submitted to ADAPTIVE2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DEPAS (Decentralized Probabilistic Auto-Scaling) algorithm assumes an\noverlay network of computing nodes where each node probabilistically decides to\nshut down, allocate one or more other nodes or do nothing. DEPAS was\nformulated, tested, and theoretically analyzed for the simplified case of\nhomogenous systems. In this paper, we extend DEPAS to heterogeneous systems.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 18:48:23 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Caprarescu", "Bogdan Alexandru", ""], ["Petcu", "Dana", ""]]}, {"id": "1203.3997", "submitter": "Michael Menzel", "authors": "Michael Menzel and Rajiv Ranjan", "title": "CloudGenius: Decision Support for Web Server Cloud Migration", "comments": "10 pages, Proceedings of the 21st International Conference on World\n  Wide Web", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is the latest computing paradigm that delivers hardware and\nsoftware resources as virtualized services in which users are free from the\nburden of worrying about the low-level system administration details. Migrating\nWeb applications to Cloud services and integrating Cloud services into existing\ncomputing infrastructures is non-trivial. It leads to new challenges that often\nrequire innovation of paradigms and practices at all levels: technical,\ncultural, legal, regulatory, and social. The key problem in mapping Web\napplications to virtualized Cloud services is selecting the best and compatible\nmix of software images (e.g., Web server image) and infrastructure services to\nensure that Quality of Service (QoS) targets of an application are achieved.\nThe fact that, when selecting Cloud services, engineers must consider\nheterogeneous sets of criteria and complex dependencies between infrastructure\nservices and software images, which are impossible to resolve manually, is a\ncritical issue. To overcome these challenges, we present a framework (called\nCloudGenius) which automates the decision-making process based on a model and\nfactors specifically for Web server migration to the Cloud. CloudGenius\nleverages a well known multi-criteria decision making technique, called\nAnalytic Hierarchy Process, to automate the selection process based on a model,\nfactors, and QoS parameters related to an application. An example application\ndemonstrates the applicability of the theoretical CloudGenius approach.\nMoreover, we present an implementation of CloudGenius that has been validated\nthrough experiments.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2012 21:19:40 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Menzel", "Michael", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1203.4054", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Javid Taheri, Reza Moraveji, Albert Y. Zomaya", "title": "On Modelling and Prediction of Total CPU Usage for Applications in\n  MapReduce Environments", "comments": "This paper has been accepted to 12th International Conference on\n  Algorithms and Architectures for Parallel Processing (ICA3PP 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, businesses have started using MapReduce as a popular computation\nframework for processing large amount of data, such as spam detection, and\ndifferent data mining tasks, in both public and private clouds. Two of the\nchallenging questions in such environments are (1) choosing suitable values for\nMapReduce configuration parameters -e.g., number of mappers, number of\nreducers, and DFS block size-, and (2) predicting the amount of resources that\na user should lease from the service provider. Currently, the tasks of both\nchoosing configuration parameters and estimating required resources are solely\nthe users' responsibilities. In this paper, we present an approach to provision\nthe total CPU usage in clock cycles of jobs in MapReduce environment. For a\nMapReduce job, a profile of total CPU usage in clock cycles is built from the\njob past executions with different values of two configuration parameters e.g.,\nnumber of mappers, and number of reducers. Then, a polynomial regression is\nused to model the relation between these configuration parameters and total CPU\nusage in clock cycles of the job. We also briefly study the influence of input\ndata scaling on measured total CPU usage in clock cycles. This derived model\nalong with the scaling result can then be used to provision the total CPU usage\nin clock cycles of the same jobs with different input data size. We validate\nthe accuracy of our models using three realistic applications (WordCount, Exim\nMainLog parsing, and TeraSort). Results show that the predicted total CPU usage\nin clock cycles of generated resource provisioning options are less than 8% of\nthe measured total CPU usage in clock cycles in our 20-node virtual Hadoop\ncluster.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 09:03:37 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2012 01:58:50 GMT"}], "update_date": "2012-07-30", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Taheri", "Javid", ""], ["Moraveji", "Reza", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1203.4257", "submitter": "Lotfi Bouzguenda", "authors": "Mahdi Abdelkafi, Lotfi Bouzguenda, Faiez Gargouri", "title": "DiscopFlow: A new Tool for Discovering Organizational Structures and\n  Interaction Protocols in WorkFlow", "comments": "Journal of E-Technology, Volume: 2, Issue: 2 May 2011;\n  http://www.dline.info/jet/v2n2.php, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with Workflow Mining (WM) a very active and promising\nresearch area. First, in this paper we give a critical and comparative study of\nthree representative WM systems of this area: the ProM, InWolve and\nWorkflowMiner systems. The comparison is made according to quality criteria\nthat we have defined such as the capacity to filter and convert a Workflow log,\nthe capacity to discover workflow perspectives and the capacity to support\nMulti-Analysis of processes. The major drawback of these systems is the non\npossibility to deal with organizational perspective discovering issue. We mean\nby organizational perspective, the organizational structures (federation,\ncoalition, market or hierarchy) and interaction protocols (contract net,\nauction or vote). This paper defends the idea that organizational dimension in\nMulti-Agent System is an appropriate approach to support the discovering of\nthis organizational perspective. Second, the paper proposes a Workflow log\nmeta-model which extends the classical one by considering the interactions\namong actors thanks to the FIPA-ACL Performatives. Third, it describes in\ndetails our DiscopFlow tool which validates our contribution.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 20:53:03 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Abdelkafi", "Mahdi", ""], ["Bouzguenda", "Lotfi", ""], ["Gargouri", "Faiez", ""]]}, {"id": "1203.4324", "submitter": "Xiaohui Bei", "authors": "Xiaohui Bei, Wei Chen, Jialin Zhang", "title": "Distributed Consensus Resilient to Both Crash Failures and Strategic\n  Manipulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study distributed consensus in synchronous systems subject\nto both unexpected crash failures and strategic manipulations by rational\nagents in the system. We adapt the concept of collusion-resistant Nash\nequilibrium to model protocols that are resilient to both crash failures and\nstrategic manipulations of a group of colluding agents. For a system with $n$\ndistributed agents, we design a deterministic protocol that tolerates 2\ncolluding agents and a randomized protocol that tolerates $n - 1$ colluding\nagents, and both tolerate any number of failures. We also show that if\ncolluders are allowed an extra communication round after each synchronous\nround, there is no protocol that can tolerate even 2 colluding agents and 1\ncrash failure.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 06:38:03 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2012 06:59:56 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2012 11:26:54 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Bei", "Xiaohui", ""], ["Chen", "Wei", ""], ["Zhang", "Jialin", ""]]}, {"id": "1203.4367", "submitter": "Nasrin Jaberi", "authors": "Hamidreza Barati, Nasrin Jaberi", "title": "Thesis Report: Resource Utilization Provisioning in MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis report, we have a survey on state-of-the-art methods for\nmodelling resource utilization of MapReduce applications regard to its\nconfiguration parameters. After implementation of one of the algorithms in\nliterature, we tried to find that if CPU usage modelling of a MapReduce\napplication can be used to predict CPU usage of another MapReduce application.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 10:06:24 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Barati", "Hamidreza", ""], ["Jaberi", "Nasrin", ""]]}, {"id": "1203.4751", "submitter": "Srivatsan Ravi Mr", "authors": "Vincent Gramoli, Petr Kuznetsov, Srivatsan Ravi", "title": "Optimism for Boosting Concurrency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern concurrent programming benefits from a large variety of\nsynchronization techniques. These include conventional pessimistic locking, as\nwell as optimistic techniques based on conditional synchronization primitives\nor transactional memory. Yet, it is unclear which of these approaches better\nleverage the concurrency inherent to multi-cores.\n  In this paper, we compare the level of concurrency one can obtain by\nconverting a sequential program into a concurrent one using optimistic or\npessimistic techniques. To establish fair comparison of such implementations,\nwe introduce a new correctness criterion for concurrent programs, defined\nindependently of the synchronization techniques they use.\n  We treat a program's concurrency as its ability to accept a concurrent\nschedule, a metric inspired by the theories of both databases and transactional\nmemory. We show that pessimistic locking can provide strictly higher\nconcurrency than transactions for some applications whereas transactions can\nprovide strictly higher concurrency than pessimistic locks for others. Finally,\nwe show that combining the benefits of the two synchronization techniques can\nprovide strictly more concurrency than any of them individually. We propose a\nlist-based set algorithm that is optimal in the sense that it accepts all\ncorrect concurrent schedules. As we show via experimentation, the optimality in\nterms of concurrency is reflected by scalability gains.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 14:44:40 GMT"}, {"version": "v2", "created": "Thu, 24 May 2012 12:47:54 GMT"}, {"version": "v3", "created": "Mon, 27 May 2013 15:01:56 GMT"}, {"version": "v4", "created": "Wed, 3 Jul 2013 15:18:00 GMT"}, {"version": "v5", "created": "Fri, 4 Oct 2013 18:16:19 GMT"}, {"version": "v6", "created": "Fri, 14 Feb 2014 16:49:05 GMT"}, {"version": "v7", "created": "Tue, 27 May 2014 09:41:32 GMT"}, {"version": "v8", "created": "Tue, 13 Oct 2015 21:02:53 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Gramoli", "Vincent", ""], ["Kuznetsov", "Petr", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "1203.4754", "submitter": "Pierre Lescanne", "authors": "Silvia Ghilezan, Pierre Lescanne (LIP), Dragisa Zunic", "title": "Computational interpretation of classical logic with explicit structural\n  rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a calculus providing a Curry-Howard correspondence to classical\nlogic represented in the sequent calculus with explicit structural rules,\nnamely weakening and contraction. These structural rules introduce explicit\nerasure and duplication of terms, respectively. We present a type system for\nwhich we prove the type-preservation under reduction. A mutual relation with\nclassical calculus featuring implicit structural rules has been studied in\ndetail. From this analysis we derive strong normalisation property.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 14:53:02 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Ghilezan", "Silvia", "", "LIP"], ["Lescanne", "Pierre", "", "LIP"], ["Zunic", "Dragisa", ""]]}, {"id": "1203.4938", "submitter": "Luis Cabellos", "authors": "Luis Cabellos", "title": "Advanced Programming Platform for efficient use of Data Parallel\n  Hardware", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Graphics processing units (GPU) had evolved from a specialized hardware\ncapable to render high quality graphics in games to a commodity hardware for\neffective processing blocks of data in a parallel schema. This evolution is\nparticularly interesting for scientific groups, which traditionally use mainly\nCPU as a work horse, and now can profit of the arrival of GPU hardware to HPC\nclusters. This new GPU hardware promises a boost in peak performance, but it is\nnot trivial to use. In this article a programming platform designed to promote\na direct use of this specialized hardware is presented. This platform includes\na visual editor of parallel data flows and it is oriented to the execution in\ndistributed clusters with GPUs. Examples of application in two characteristic\nproblems, Fast Fourier Transform and Image Compression, are also shown.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 09:54:58 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2012 12:56:24 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Cabellos", "Luis", ""]]}, {"id": "1203.5004", "submitter": "Colm \\'O D\\'unlaing", "authors": "Colm O. Dunlaing", "title": "CUDA implementation of Wagener's 2D convex hull PRAM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a CUDA implementation of Wagener's PRAM convex hull\nalgorithm in two dimensions. It is presented in Knuth's literate programming\nstyle.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 14:30:25 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2012 10:02:35 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Dunlaing", "Colm O.", ""]]}, {"id": "1203.5026", "submitter": "Kuang Xu", "authors": "Kuang Xu", "title": "On the Power of Centralization in Distributed Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we propose and analyze a multi-server model that captures a\nperformance trade-off between centralized and distributed processing. In our\nmodel, a fraction $p$ of an available resource is deployed in a centralized\nmanner (e.g., to serve a most-loaded station) while the remaining fraction\n$1-p$ is allocated to local servers that can only serve requests addressed\nspecifically to their respective stations.\n  Using a fluid model approach, we demonstrate a surprising phase transition in\nthe steady-state delay, as $p$ changes: in the limit of a large number of\nstations, and when any amount of centralization is available ($p>0$), the\naverage queue length in steady state scales as $\\log_{1/(1-p)} 1/(1-\\lambda)$\nwhen the traffic intensity $\\lambda$ goes to 1. This is exponentially smaller\nthan the usual M/M/1-queue delay scaling of $1/(1-\\lambda)$, obtained when all\nresources are fully allocated to local stations ($p=0$). This indicates a\nstrong qualitative impact of even a small degree of centralization.\n  We prove convergence to a fluid limit, and characterize both the transient\nand steady-state behavior of the finite system, in the limit as the number of\nstations $N$ goes to infinity. We show that the sequence of queue-length\nprocesses converges to a unique fluid trajectory (over any finite time\ninterval, as $N$ approaches infinity, and that this fluid trajectory converges\nto a unique invariant state $v^I$, for which a simple closed-form expression is\nobtained. We also show that the steady-state distribution of the $N$-server\nsystem concentrates on $v^I$ as $N$ goes to infinity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 16:05:33 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Xu", "Kuang", ""]]}, {"id": "1203.5128", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury", "title": "Acceleration of the shiftable O(1) algorithm for bilateral filtering and\n  non-local means", "comments": "10 figures, 6 tables", "journal-ref": "IEEE Transactions on Image Processing, vol. 22(4), pp. 1291- 1300,\n  2013", "doi": "10.1109/TIP.2012.2222903", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A direct implementation of the bilateral filter [1] requires O(\\sigma_s^2)\noperations per pixel, where \\sigma_s is the (effective) width of the spatial\nkernel. A fast implementation of the bilateral filter was recently proposed in\n[2] that required O(1) operations per pixel with respect to \\sigma_s. This was\ndone by using trigonometric functions for the range kernel of the bilateral\nfilter, and by exploiting their so-called shiftability property. In particular,\na fast implementation of the Gaussian bilateral filter was realized by\napproximating the Gaussian range kernel using raised cosines. Later, it was\ndemonstrated in [3] that this idea could be extended to a larger class of\nfilters, including the popular non-local means filter [4]. As already observed\nin [2], a flip side of this approach was that the run time depended on the\nwidth \\sigma_r of the range kernel. For an image with (local) intensity\nvariations in the range [0,T], the run time scaled as O(T^2/\\sigma^2_r) with\n\\sigma_r. This made it difficult to implement narrow range kernels,\nparticularly for images with large dynamic range. We discuss this problem in\nthis note, and propose some simple steps to accelerate the implementation in\ngeneral, and for small \\sigma_r in particular.\n  [1] C. Tomasi and R. Manduchi, \"Bilateral filtering for gray and color\nimages\", Proc. IEEE International Conference on Computer Vision, 1998.\n  [2] K.N. Chaudhury, Daniel Sage, and M. Unser, \"Fast O(1) bilateral filtering\nusing trigonometric range kernels\", IEEE Transactions on Image Processing,\n2011.\n  [3] K.N. Chaudhury, \"Constant-time filtering using shiftable kernels\", IEEE\nSignal Processing Letters, 2011.\n  [4] A. Buades, B. Coll, and J.M. Morel, \"A review of image denoising\nalgorithms, with a new one\", Multiscale Modeling and Simulation, 2005.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 21:09:37 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2012 18:57:45 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Chaudhury", "Kunal N.", ""]]}, {"id": "1203.5160", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Albert Y. Zomaya, Young Choon Lee, Ali\n  Javadzadeh Boloori, Javid Taheri", "title": "Multiple Frequency Selection in DVFS-Enabled Processors to Minimize\n  Energy Consumption", "comments": "Chapter 17- Book title: \"Energy Efficient Distributed Computing\",\n  Edited by Albert Y.Zomaya, Young Choon Lee Wiley", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we focus on slack reclamation and propose a new slack\nreclamation technique, Multiple Frequency Selection DVFS (MFS-DVFS). The key\nidea is to execute each task with a linear combination of more than one\nfrequency such that this combination results in using the lowest energy by\ncovering the whole slack time of the task. We have tested our algorithm with\nboth random and real-world application task graphs and compared with the\nresults in previous researches in [9] and [12-13]. The experimental results\nshow that our approach can achieve energy almost identical to the optimum\nenergy saving.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 02:42:38 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 00:30:17 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Zomaya", "Albert Y.", ""], ["Lee", "Young Choon", ""], ["Boloori", "Ali Javadzadeh", ""], ["Taheri", "Javid", ""]]}, {"id": "1203.5196", "submitter": "Rajkumar Buyya", "authors": "Rajkumar Buyya, Suraj Pandey, and Christian Vecchiola", "title": "Market-Oriented Cloud Computing and the Cloudbus Toolkit", "comments": "43 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has penetrated the Information Technology industry deep\nenough to influence major companies to adopt it into their mainstream business.\nA strong thrust on the use of virtualization technology to realize\nInfrastructure-as-a-Service (IaaS) has led enterprises to leverage\nsubscription-oriented computing capabilities of public Clouds for hosting their\napplication services. In parallel, research in academia has been investigating\ntransversal aspects such as security, software frameworks, quality of service,\nand standardization. We believe that the complete realization of the Cloud\ncomputing vision will lead to the introduction of a virtual market where Cloud\nbrokers, on behalf of end users, are in charge of selecting and composing the\nservices advertised by different Cloud vendors. In order to make this happen,\nexisting solutions and technologies have to be redesigned and extended from a\nmarket-oriented perspective and integrated together, giving rise to what we\nterm Market-Oriented Cloud Computing.\n  In this paper, we will assess the current status of Cloud computing by\nproviding a reference model, discuss the challenges that researchers and IT\npractitioners are facing and will encounter in the near future, and present the\napproach for solving them from the perspective of the Cloudbus toolkit, which\ncomprises of a set of technologies geared towards the realization of Market\nOriented Cloud Computing vision. We provide experimental results demonstrating\nmarket-oriented resource provisioning and brokering within a Cloud and across\nmultiple distributed resources. We also include an application illustrating the\nhosting of ECG analysis as SaaS on Amazon IaaS (EC2 and S3) services.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 08:50:57 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Pandey", "Suraj", ""], ["Vecchiola", "Christian", ""]]}, {"id": "1203.5399", "submitter": "Ido Ben-Zvi", "authors": "Ido Ben-Zvi and Yoram Moses", "title": "Agent-time Epistemics and Coordination", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A minor change to the standard epistemic logical language, replacing $K_{i}$\nwith $K_{\\node{i,t}}$ where $t$ is a time instance, gives rise to a generalized\nand more expressive form of knowledge and common knowledge operators. We\ninvestigate the communication structures that are necessary for such\ngeneralized epistemic states to arise, and the inter-agent coordination tasks\nthat require such knowledge. Previous work has established a relation between\nlinear event ordering and nested knowledge, and between simultaneous event\noccurrences and common knowledge. In the new, extended, formalism, epistemic\nnecessity is decoupled from temporal necessity. Nested knowledge and event\nordering are shown to be related even when the nesting order does not match the\ntemporal order of occurrence. The generalized form of common knowledge does\n{\\em not} correspond to simultaneity. Rather, it corresponds to a notion of\ntight coordination, of which simultaneity is an instance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 10:12:05 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Ben-Zvi", "Ido", ""], ["Moses", "Yoram", ""]]}, {"id": "1203.5485", "submitter": "Sameer Agarwal", "authors": "Sameer Agarwal, Aurojit Panda, Barzan Mozafari, Samuel Madden, Ion\n  Stoica", "title": "BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very\n  Large Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present BlinkDB, a massively parallel, sampling-based\napproximate query engine for running ad-hoc, interactive SQL queries on large\nvolumes of data. The key insight that BlinkDB builds on is that one can often\nmake reasonable decisions in the absence of perfect answers. For example,\nreliably detecting a malfunctioning server using a distributed collection of\nsystem logs does not require analyzing every request processed by the system.\nBased on this insight, BlinkDB allows one to trade-off query accuracy for\nresponse time, enabling interactive queries over massive data by running\nqueries on data samples and presenting results annotated with meaningful error\nbars. To achieve this, BlinkDB uses two key ideas that differentiate it from\nprevious work in this area: (1) an adaptive optimization framework that builds\nand maintains a set of multi-dimensional, multi-resolution samples from\noriginal data over time, and (2) a dynamic sample selection strategy that\nselects an appropriately sized sample based on a query's accuracy and/or\nresponse time requirements. We have built an open-source version of BlinkDB and\nvalidated its effectiveness using the well-known TPC-H benchmark as well as a\nreal-world analytic workload derived from Conviva Inc. Our experiments on a 100\nnode cluster show that BlinkDB can answer a wide range of queries from a\nreal-world query trace on up to 17 TBs of data in less than 2 seconds (over\n100\\times faster than Hive), within an error of 2 - 10%.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2012 11:11:21 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2012 19:01:31 GMT"}], "update_date": "2012-06-20", "authors_parsed": [["Agarwal", "Sameer", ""], ["Panda", "Aurojit", ""], ["Mozafari", "Barzan", ""], ["Madden", "Samuel", ""], ["Stoica", "Ion", ""]]}, {"id": "1203.5737", "submitter": "Tom\\'a\\v{s} Oberhuber", "authors": "Martin Heller and Tom\\'a\\v{s} Oberhuber", "title": "Adaptive Row-grouped CSR Format for Storing of Sparse Matrices on GPU", "comments": "9 pages, 5 figures, 1 code listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new adaptive format for storing sparse matrices on GPU. We compare\nit with several other formats including CUSPARSE which is today probably the\nbest choice for processing of sparse matrices on GPU in CUDA. Contrary to\nCUSPARSE which works with common CSR format, our new format requires\nconversion. However, multiplication of sparse-matrix and vector is\nsignificantly faster for many atrices. We demonstrate it on set of 1 600\nmatrices and we show for what types of matrices our format is profitable.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 17:29:58 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Heller", "Martin", ""], ["Oberhuber", "Tom\u00e1\u0161", ""]]}, {"id": "1203.6049", "submitter": "Gene Pang", "authors": "Tim Kraska, Gene Pang, Michael J. Franklin, Samuel Madden", "title": "MDCC: Multi-Data Center Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicating data across multiple data centers not only allows moving the data\ncloser to the user and, thus, reduces latency for applications, but also\nincreases the availability in the event of a data center failure. Therefore, it\nis not surprising that companies like Google, Yahoo, and Netflix already\nreplicate user data across geographically different regions.\n  However, replication across data centers is expensive. Inter-data center\nnetwork delays are in the hundreds of milliseconds and vary significantly.\nSynchronous wide-area replication is therefore considered to be unfeasible with\nstrong consistency and current solutions either settle for asynchronous\nreplication which implies the risk of losing data in the event of failures,\nrestrict consistency to small partitions, or give up consistency entirely. With\nMDCC (Multi-Data Center Consistency), we describe the first optimistic commit\nprotocol, that does not require a master or partitioning, and is strongly\nconsistent at a cost similar to eventually consistent protocols. MDCC can\ncommit transactions in a single round-trip across data centers in the normal\noperational case. We further propose a new programming model which empowers the\napplication developer to handle longer and unpredictable latencies caused by\ninter-data center communication. Our evaluation using the TPC-W benchmark with\nMDCC deployed across 5 geographically diverse data centers shows that MDCC is\nable to achieve throughput and latency similar to eventually consistent quorum\nprotocols and that MDCC is able to sustain a data center outage without a\nsignificant impact on response times while guaranteeing strong consistency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 19:03:53 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Kraska", "Tim", ""], ["Pang", "Gene", ""], ["Franklin", "Michael J.", ""], ["Madden", "Samuel", ""]]}, {"id": "1203.6096", "submitter": "Yehuda Afek", "authors": "Yehuda Afek and Eli Gafni", "title": "Asynchrony from Synchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider synchronous dynamic networks which like radio networks may have\nasymmetric communication links, and are affected by communication rather than\nprocessor failures. In this paper we investigate the minimal message\nsurvivability in a per round basis that allows for the minimal global\ncooperation, i.e., allows to solve any task that is wait-free read-write\nsolvable. The paper completely characterizes this survivability requirement.\nMessage survivability is formalized by considering adversaries that have a\nlimited power to remove messages in a round. Removal of a message on a link in\none direction does not necessarily imply the removal of the message on that\nlink in the other direction. Surprisingly there exist a single strongest\nadversary which solves any wait-free read/write task. Any different adversary\nthat solves any wait-free read/write task is weaker, and any stronger adversary\nwill not solve any wait-free read/write task. ABD \\cite{ABD} who considered\nprocessor failure, arrived at an adversary that is $n/2$ resilient,\nconsequently can solve tasks, such as $n/2$-set-consensus, which are not\nread/write wait-free solvable. With message adversaries, we arrive at an\nadversary which has exactly the read-write wait-free power. Furthermore, this\nadversary allows for a considerably simpler (simplest that we know of) proof\nthat the protocol complex of any read/write wait-free task is a subdivided\nsimplex, finally making this proof accessible for students with no\nalgebraic-topology prerequisites, and alternatively dispensing with the\nassumption that the Immediate Snapshot complex is a subdivided simplex.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 22:26:02 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Afek", "Yehuda", ""], ["Gafni", "Eli", ""]]}, {"id": "1203.6806", "submitter": "Matteo Camilli M.Sc.", "authors": "Carlo Bellettini, Matteo Camilli, Lorenzo Capra, Mattia Monga", "title": "State Space Exploration of RT Systems in the Cloud", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing availability of distributed and cloud computing frameworks make\nit possible to face complex computational problems in a more effective and\nconvenient way. A notable example is state-space exploration of discrete-event\nsystems specified in a formal way. The exponential complexity of this task is a\nmajor limitation to the usage of consolidated analysis techniques and tools. We\npresent and compare two different approaches to state-space explosion, relying\non distributed and cloud frameworks, respectively. These approaches were\ndesigned and implemented following the same computational schema, a sort of map\n& fold. They are applied on symbolic state-space exploration of real-time\nsystems specified by (a timed extension of) Petri Nets, by readapting a\nsequential algorithm implemented as a command-line Java tool. The outcome of\nseveral tests performed on a benchmarking specification are presented, thus\nshowing the convenience of cloud approaches.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 13:13:49 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Bellettini", "Carlo", ""], ["Camilli", "Matteo", ""], ["Capra", "Lorenzo", ""], ["Monga", "Mattia", ""]]}]