[{"id": "1811.00143", "submitter": "Minghuang Ma", "authors": "Minghuang Ma, Hadi Pouransari, Daniel Chao, Saurabh Adya, Santiago\n  Akle Serrano, Yi Qin, Dan Gimnicher, Dominic Walsh", "title": "Democratizing Production-Scale Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest and demand for training deep neural networks have been\nexperiencing rapid growth, spanning a wide range of applications in both\nacademia and industry. However, training them distributed and at scale remains\ndifficult due to the complex ecosystem of tools and hardware involved. One\nconsequence is that the responsibility of orchestrating these complex\ncomponents is often left to one-off scripts and glue code customized for\nspecific problems. To address these restrictions, we introduce \\emph{Alchemist}\n- an internal service built at Apple from the ground up for \\emph{easy},\n\\emph{fast}, and \\emph{scalable} distributed training. We discuss its design,\nimplementation, and examples of running different flavors of distributed\ntraining. We also present case studies of its internal adoption in the\ndevelopment of autonomous systems, where training times have been reduced by\n10x to keep up with the ever-growing data collection.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:39:59 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 05:47:41 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ma", "Minghuang", ""], ["Pouransari", "Hadi", ""], ["Chao", "Daniel", ""], ["Adya", "Saurabh", ""], ["Serrano", "Santiago Akle", ""], ["Qin", "Yi", ""], ["Gimnicher", "Dan", ""], ["Walsh", "Dominic", ""]]}, {"id": "1811.00156", "submitter": "Beau Johnston", "authors": "Beau Johnston, Greg Falzon and Josh Milthorpe", "title": "OpenCL Performance Prediction using Architecture-Independent Features", "comments": "9 pages, 6 figures, International Workshop on High Performance and\n  Dynamic Reconfigurable Systems and Networks (DRSN-2018) published in\n  conjunction with The 2018 International Conference on High Performance\n  Computing & Simulation (HPCS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenCL is an attractive model for heterogeneous high-performance computing\nsystems, with wide support from hardware vendors and significant performance\nportability. To support efficient scheduling on HPC systems it is necessary to\nperform accurate performance predictions for OpenCL workloads on varied compute\ndevices, which is challenging due to diverse computation, communication and\nmemory access characteristics which result in varying performance between\ndevices. The Architecture Independent Workload Characterization (AIWC) tool can\nbe used to characterize OpenCL kernels according to a set of\narchitecture-independent features. This work presents a methodology where AIWC\nfeatures are used to form a model capable of predicting accelerator execution\ntimes. We used this methodology to predict execution times for a set of 37\ncomputational kernels running on 15 different devices representing a broad\nrange of CPU, GPU and MIC architectures. The predictions are highly accurate,\ndiffering from the measured experimental run-times by an average of only 1.2%,\nand correspond to actual execution time mispredictions of 9 {\\mu}s to 1 sec\naccording to problem size. A previously unencountered code can be instrumented\nonce and the AIWC metrics embedded in the kernel, to allow performance\nprediction across the full range of modelled devices. The results suggest that\nthis methodology supports correct selection of the most appropriate device for\na previously unencountered code, which is highly relevant to the HPC scheduling\nsetting.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:33:15 GMT"}], "update_date": "2018-11-04", "authors_parsed": [["Johnston", "Beau", ""], ["Falzon", "Greg", ""], ["Milthorpe", "Josh", ""]]}, {"id": "1811.00424", "submitter": "Daniel Rodriguez", "authors": "Raul-Jose Palma-Mendoza, Daniel Rodriguez, Luis de-Marcos", "title": "Distributed ReliefF based Feature Selection in Spark", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-017-1145-y", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection (FS) is a key research area in the machine learning and\ndata mining fields, removing irrelevant and redundant features usually helps to\nreduce the effort required to process a dataset while maintaining or even\nimproving the processing algorithm's accuracy. However, traditional algorithms\ndesigned for executing on a single machine lack scalability to deal with the\nincreasing amount of data that has become available in the current Big Data\nera. ReliefF is one of the most important algorithms successfully implemented\nin many FS applications. In this paper, we present a completely redesigned\ndistributed version of the popular ReliefF algorithm based on the novel Spark\ncluster computing model that we have called DiReliefF. Spark is increasing its\npopularity due to its much faster processing times compared with Hadoop's\nMapReduce model implementation. The effectiveness of our proposal is tested on\nfour publicly available datasets, all of them with a large number of instances\nand two of them with also a large number of features. Subsets of these datasets\nwere also used to compare the results to a non-distributed implementation of\nthe algorithm. The results show that the non-distributed implementation is\nunable to handle such large volumes of data without specialized hardware, while\nour design can process them in a scalable way with much better processing times\nand memory usage.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:11:32 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Palma-Mendoza", "Raul-Jose", ""], ["Rodriguez", "Daniel", ""], ["de-Marcos", "Luis", ""]]}, {"id": "1811.00607", "submitter": "Rui Rodrigues De Mello Junior", "authors": "Rui R. Mello Junior, Leandro S. Araujo, Tiago A. O. Alves, Leandro A.\n  J. Marzulo, Gabriel A. L. Paillard and Felipe M. G. Fran\\c{c}a", "title": "Exploring the Equivalence between Dynamic Dataflow Model and Gamma -\n  General Abstract Model for Multiset mAnipulation", "comments": "Study submitted to the IPDPS 2019 - IEEE International Parallel and\n  Distributed Processing Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase of the search for computational models where the expression\nof parallelism occurs naturally, some paradigms arise as options for the next\ngeneration of computers. In this context, dynamic Dataflow and Gamma - General\nAbstract Model for Multiset mAnipulation) - emerge as interesting computational\nmodels choices. In the dynamic Dataflow model, operations are performed as soon\nas their associated operators are available, without rely on a Program Counter\nto dictate the execution order of instructions. The Gamma paradigm is based on\na parallel multiset rewriting scheme. It provides a non-deterministic execution\nmodel inspired by an abstract chemical machine metaphor, where operations are\nformulated as reactions that occur freely among matching elements belonging to\nthe multiset. In this work, equivalence relations between the dynamic Dataflow\nand Gamma paradigms are exposed and explored, while methods to convert from\nDataflow to Gamma paradigm and vice versa are provided. It is shown that\nvertices and edges of a dynamic Dataflow graph can correspond, respectively, to\nreactions and multiset elements in the Gamma paradigm. Implementation aspects\nof execution environments that could be mutually beneficial to both models are\nalso discussed. This work provides the scientific community with the\npossibility of taking profit of both parallel programming models, contributing\nwith a versatility component to researchers and developers. Finally, it is\nimportant to state that, to the best of our knowledge, the similarity relations\nbetween both dynamic Dataflow and Gamma models presented here have not been\nreported in any previous work.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 19:44:31 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Junior", "Rui R. Mello", ""], ["Araujo", "Leandro S.", ""], ["Alves", "Tiago A. O.", ""], ["Marzulo", "Leandro A. J.", ""], ["Paillard", "Gabriel A. L.", ""], ["Fran\u00e7a", "Felipe M. G.", ""]]}, {"id": "1811.00652", "submitter": "Heiko Ludwig", "authors": "Nadja Brouns, Samir Tata, Heiko Ludwig, E. Serral Asensio, Paul Grefen", "title": "Modeling IoT-aware Business Processes - A State of the Art Report", "comments": "42 pages", "journal-ref": "IBM Research Report 2018", "doi": null, "report-no": "RJ 10540", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research report presents an analysis of the state of the art of modeling\nInternet of Things (IoT)-aware business processes. IOT links the physical world\nto the digital world. Traditionally, we would find information about events and\nprocesses in the physical world in the digital world entered by humans and\nhumans using this information to control the physical world. In the IoT\nparadigm, the physical world is equipped with sensors and actuators to create a\ndirect link with the digital world. Business processes are used to coordinate a\ncomplex environment including multiple actors for a common goal, typically in\nthe context of administrative work. In the past few years, we have seen\nresearch efforts on the possibilities to model IoT- aware business processes,\nextending process coordination to real world entities directly. This set of\nresearch efforts is relatively small when compared to the overall research\neffort into the IoT and much of the work is still in the early research stage.\nTo create a basis for a bridge between IoT and BPM, the goal of this report is\nto collect and analyze the state of the art of existing frameworks for modeling\nIoT-aware business processes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:06:47 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Brouns", "Nadja", ""], ["Tata", "Samir", ""], ["Ludwig", "Heiko", ""], ["Asensio", "E. Serral", ""], ["Grefen", "Paul", ""]]}, {"id": "1811.00742", "submitter": "Eunjin (EJ) Jung", "authors": "Jean-Philippe Martin and Eunjin (EJ) Jung", "title": "Rationality-proof consensus: extended abstract", "comments": "extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain systems benefit from lessons in prior art such as fault tolerance,\ndistributed systems, peer-to-peer systems, and game theory. In this paper we\nargue that blockchain algorithms should tolerate both rational\n(self-interested) users and Byzantine (malicious) ones, rather than assuming\nall non-Byzantine users are altruistic and follow the protocols blindly. Such\nalgorithms are called BAR-tolerant [1]. To design a BAR-tolerant system, one\ncan follow these three steps: clearly define the utility function for the\nrational users, prove the algorithm is such that there is no benefit from\nunilaterally deviating (that is, it's a Byzantine Nash Equilibrium), then prove\nthe algorithm correct assuming the rational actors follow the protocol. We\npresent an example attack by rational users: the gatekeeping attack, where\nmembers of a system selfishly decide to prevent newcomers from joining. This\nattack may affect any stake-based system where the existing members prevent\nnewcomers from making a stake, and essentially form a cartel. We then sketch a\nBAR-tolerant consensus protocol for blockchain that can defend against this\nattack. It relies on a strict order to decide who gets to propose a new block\n(so there's no need to race to solve a crypto puzzle) and it relies on hardware\nID tokens to make sure every computer is only represented at most once as a\nblock proposer to mitigate Sybil attacks. It also defends against the\ngatekeeper attack. The BAR-tolerant approach is naturally also applicable to\nother blockchain algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 05:21:34 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Martin", "Jean-Philippe", "", "EJ"], ["Eunjin", "", "", "EJ"], ["Jung", "", ""]]}, {"id": "1811.00834", "submitter": "Kaustav Bose", "authors": "Kaustav Bose, Ranendu Adhikary, Manash Kumar Kundu and Buddhadeb Sau", "title": "Arbitrary Pattern Formation on Infinite Grid by Asynchronous Oblivious\n  Robots", "comments": "This is the full version of the paper, with the same title and\n  authors, that was accepted in the 13th International Conference and Workshops\n  on Algorithms and Computation (WALCOM 2019), February 27 - March 02, 2019,\n  Guwahati, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Arbitrary Pattern Formation problem asks to design a distributed\nalgorithm that allows a set of autonomous mobile robots to form any specific\nbut arbitrary geometric pattern given as input. The problem has been\nextensively studied in literature in continuous domains. This paper\ninvestigates a discrete version of the problem where the robots are operating\non a two dimensional infinite grid. The robots are assumed to be autonomous,\nidentical, anonymous and oblivious. They operate in Look-Compute-Move cycles\nunder a fully asynchronous scheduler. The robots do not agree on any common\nglobal coordinate system or chirality. We have shown that a set of robots can\nform any arbitrary pattern, if their starting configuration is asymmetric.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 12:22:12 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Bose", "Kaustav", ""], ["Adhikary", "Ranendu", ""], ["Kundu", "Manash Kumar", ""], ["Sau", "Buddhadeb", ""]]}, {"id": "1811.00901", "submitter": "Ahmed Eleliemy", "authors": "Ahmed Eleliemy, Ali Mohammed, Florina M. Ciorba", "title": "Efficient Generation of Parallel Spin-images Using Dynamic Loop\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance computing (HPC) systems underwent a significant increase in\ntheir processing capabilities. Modern HPC systems combine large numbers of\nhomogeneous and heterogeneous computing resources. Scalability is, therefore,\nan essential aspect of scientific applications to efficiently exploit the\nmassive parallelism of modern HPC systems. This work introduces an efficient\nversion of the parallel spin-image algorithm (PSIA), called EPSIA. The PSIA is\na parallel version of the spin-image algorithm (SIA). The (P)SIA is used in\nvarious domains, such as 3D object recognition, categorization, and 3D face\nrecognition. EPSIA refers to the extended version of the PSIA that integrates\nvarious well-known dynamic loop scheduling (DLS) techniques. The present work:\n(1) Proposes EPSIA, a novel flexible version of PSIA; (2) Showcases the\nbenefits of applying DLS techniques for optimizing the performance of the PSIA;\n(3) Assesses the performance of the proposed EPSIA by conducting several\nscalability experiments. The performance results are promising and show that\nusing well-known DLS techniques, the performance of the EPSIA outperforms the\nperformance of the PSIA by a factor of 1.2 and 2 for homogeneous and\nheterogeneous computing resources, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:46:35 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Eleliemy", "Ahmed", ""], ["Mohammed", "Ali", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1811.00989", "submitter": "David A. Monge", "authors": "David A. Monge (1), Elina Pacini (1, 2), Cristian Mateos (3), Enrique\n  Alba (4), Carlos Garc\\'ia Garino (1) ((1) ITIC, Universidad Nacional de Cuyo.\n  Mendoza, Argentina, (2) Consejo Nacional de Investigaciones Cient\\'ificas y\n  T\\'ecnicas (CONICET). Argentina., (3) ISISTAN-UNICEN-CONICET. Tandil, Buenos\n  Aires, Argentina., (4) Departamento de Lenguajes y Ciencias de la\n  Computaci\\'on, Universidad de M\\'alaga. Spain.)", "title": "CMI: An Online Multi-objective Genetic Autoscaler for Scientific and\n  Engineering Workflows in Cloud Infrastructures with Unreliable Virtual\n  Machines", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing is becoming the leading paradigm for executing scientific and\nengineering workflows. The large-scale nature of the experiments they model and\ntheir variable workloads make clouds the ideal execution environment due to\nprompt and elastic access to huge amounts of computing resources. Autoscalers\nare middleware-level software components that allow scaling up and down the\ncomputing platform by acquiring or terminating virtual machines (VM) at the\ntime that workflow's tasks are being scheduled. In this work we propose a novel\nonline multi-objective autoscaler for workflows denominated Cloud\nMulti-objective Intelligence (CMI), that aims at the minimization of makespan,\nmonetary cost and the potential impact of errors derived from unreliable VMs.\nIn addition, this problem is subject to monetary budget constraints. CMI is\nresponsible for periodically solving the autoscaling problems encountered along\nthe execution of a workflow. Simulation experiments on four well-known\nworkflows exhibit that CMI significantly outperforms a state-of-the-art\nautoscaler of similar characteristics called Spot Instances Aware Autoscaling\n(SIAA). These results convey a solid base for deepening in the study of other\nmeta-heuristic methods for autoscaling workflow applications using cheap but\nunreliable infrastructures.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:11:57 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Monge", "David A.", ""], ["Pacini", "Elina", ""], ["Mateos", "Cristian", ""], ["Alba", "Enrique", ""], ["Garino", "Carlos Garc\u00eda", ""]]}, {"id": "1811.01161", "submitter": "Engin Arslan", "authors": "Engin Arslan, Ahmed Alhussen", "title": "Fast Integrity Verification for High-Speed File Transfers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data generated by scientific and commercial applications is\ngrowing at an ever-increasing pace. This data is often moved between\ngeographically distributed sites for various purposes such as collaboration and\nbackup which has led to significant increase in data transfer rates. Surge in\ndata transfer rates when combined with proliferation of scientific applications\nthat cannot tolerate data corruption triggered enhanced integrity verification\ntechniques to be developed. End-to-end integrity verification minimizes the\nlikelihood of silent data corruption by comparing checksum of files at source\nand destination servers using secure hash algorithms such as MD5 and SHA1.\nHowever, it imposes significant performance penalty due to overhead of checksum\ncomputation. In this paper, we propose Fast Integrity VERification (FIVER)\nalgorithm which overlaps checksum computation and data transfer operations of\nfiles to minimize the cost of integrity verification. Extensive experiments\nshow that FIVER is able to bring down the cost from 60% by the state-of-the-art\nsolutions to below 10% by concurrently executing transfer and checksum\noperations and enabling file I/O share between them. We also implemented\nFIVER-Hybrid to mimic disk access patterns of sequential integrity verification\napproach to capture possible data corruption that may occur during file write\noperations which FIVER may miss. Results show that FIVER-Hybrid is able to\nreduce execution time by 20% compared to sequential approach without\ncompromising the reliability of integrity verification.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 04:56:15 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Arslan", "Engin", ""], ["Alhussen", "Ahmed", ""]]}, {"id": "1811.01201", "submitter": "Enzo Rucci PhD", "authors": "Enzo Rucci, Armando De Giusti and Marcelo Naiouf", "title": "Blocked All-Pairs Shortest Paths Algorithm on Intel Xeon Phi KNL\n  Processor: A Case Study", "comments": "Computer Science - CACIC 2017. Springer Communications in Computer\n  and Information Science, vol 790", "journal-ref": null, "doi": "10.1007/978-3-319-75214-3_5", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manycores are consolidating in HPC community as a way of improving\nperformance while keeping power efficiency. Knights Landing is the recently\nreleased second generation of Intel Xeon Phi architecture. While optimizing\napplications on CPUs, GPUs and first Xeon Phi's has been largely studied in the\nlast years, the new features in Knights Landing processors require the revision\nof programming and optimization techniques for these devices. In this work, we\nselected the Floyd-Warshall algorithm as a representative case study of graph\nand memory-bound applications. Starting from the default serial version, we\nshow how data, thread and compiler level optimizations help the parallel\nimplementation to reach 338 GFLOPS.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 12:57:42 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Rucci", "Enzo", ""], ["De Giusti", "Armando", ""], ["Naiouf", "Marcelo", ""]]}, {"id": "1811.01235", "submitter": "David Doty", "authors": "Amanda Belleville, David Doty, and David Soloveichik", "title": "Hardness of computing and approximating predicates and functions with\n  leaderless population protocols", "comments": "published in Proceedings of ICALP 2017", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2017.141", "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a distributed computing model appropriate for\ndescribing massive numbers of agents with limited computational power. A\npopulation protocol \"has an initial leader\" if every valid initial\nconfiguration contains a single agent in a special \"leader\" state that helps to\ncoordinate the computation. Although the class of predicates and functions\ncomputable with probability 1 is the same whether or not there is an initial\nleader (semilinear functions and predicates), it is not known whether a leader\nis necessary for fast computation. Efficient population protocols are generally\ndefined as those computing in polylogarithmic in $n$ (parallel) time. We\nconsider leaderless population protocols, regarding the computation finished\nwhen a configuration is reached from which a different output is no longer\nreachable.\n  In this setting we show that a wide class of functions and predicates\ncomputable by population protocols are not efficiently computable (they require\nat least linear time to stabilize on a correct answer), nor are some linear\nfunctions even efficiently approximable. For example, the widely studied\nparity, majority, and equality predicates cannot be computed in sublinear time.\nMoreover, it requires at least linear time for a population protocol even to\napproximate any linear function with a coefficient outside of $\\mathbb{N}$: for\nsufficiently small $\\gamma > 0$, the output of a sublinear time protocol can\nstabilize outside the interval $f(m) (1 \\pm \\gamma)$ on infinitely many inputs\n$m$. We also show that it requires linear time to exactly compute a wide range\nof semilinear functions (e.g., $f(m)=m$ if $m$ is even and $2m$ if $m$ is odd).\n  Finally, we show that with a sufficiently large value of $\\gamma$, a\npopulation protocol can approximate any linear $f$ with nonnegative rational\ncoefficients, within approximation factor $\\gamma$, in $O(\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 15:36:30 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Belleville", "Amanda", ""], ["Doty", "David", ""], ["Soloveichik", "David", ""]]}, {"id": "1811.01268", "submitter": "Samvit Jain", "authors": "Samvit Jain and Xun Zhang and Yuhao Zhou and Ganesh Ananthanarayanan\n  and Junchen Jiang and Yuanchao Shu and Joseph Gonzalez", "title": "ReXCam: Resource-Efficient, Cross-Camera Video Analytics at Scale", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enterprises are increasingly deploying large camera networks for video\nanalytics. Many target applications entail a common problem template: searching\nfor and tracking an object or activity of interest (e.g. a speeding vehicle, a\nbreak-in) through a large camera network in live video. Such cross-camera\nanalytics is compute and data intensive, with cost growing with the number of\ncameras and time. To address this cost challenge, we present ReXCam, a new\nsystem for efficient cross-camera video analytics. ReXCam exploits spatial and\ntemporal locality in the dynamics of real camera networks to guide its\ninference-time search for a query identity. In an offline profiling phase,\nReXCam builds a cross-camera correlation model that encodes the locality\nobserved in historical traffic patterns. At inference time, ReXCam applies this\nmodel to filter frames that are not spatially and temporally correlated with\nthe query identity's current position. In the cases of occasional missed\ndetections, ReXCam performs a fast-replay search on recently filtered video\nframes, enabling gracefully recovery. Together, these techniques allow ReXCam\nto reduce compute workload by 8.3x on an 8-camera dataset, and by 23x - 38x on\na simulated 130-camera dataset. ReXCam has been implemented and deployed on a\ntestbed of 5 AWS DeepLens cameras.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 19:15:15 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 22:50:29 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 05:22:54 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 03:17:48 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Jain", "Samvit", ""], ["Zhang", "Xun", ""], ["Zhou", "Yuhao", ""], ["Ananthanarayanan", "Ganesh", ""], ["Jiang", "Junchen", ""], ["Shu", "Yuanchao", ""], ["Gonzalez", "Joseph", ""]]}, {"id": "1811.01270", "submitter": "Amos Korman", "authors": "Amos Korman and Yoav Rodeh", "title": "Multi-Round Cooperative Search Games with Multiple Players", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that a treasure is placed in one of $M$ boxes according to a known\ndistribution and that $k$ searchers are searching for it in parallel during $T$\nrounds. We study the question of how to incentivize selfish players so that the\nsuccess probability, namely, the probability that at least one player finds the\ntreasure, would be maximized. We focus on congestion policies $C(s)$ that\nspecify the reward that a player receives if it is one of $s$ players that\n(simultaneously) find the treasure for the first time. Our main technical\ncontribution is proving that the exclusive policy, in which $C(1)=1$ and\n$C(s)=0$ for $s>1$, yields a price of anarchy of $(1-(1-{1}/{k})^{k})^{-1}$,\nand that this is the best possible price among all symmetric reward mechanisms.\nFor this policy we also have an explicit description of a symmetric\nequilibrium, which is in some sense unique, and moreover enjoys the best\nsuccess probability among all symmetric profiles. For general congestion\npolicies, we show how to polynomially find, for any $e>0$, a symmetric\nmultiplicative $(1+e)(1+C(k))$-equilibrium. Together with an appropriate reward\npolicy, a central entity can suggest players to play a particular profile at\nequilibrium. As our main conceptual contribution, we advocate the use of\nsymmetric equilibria for such purposes. Besides being fair, we argue that in\nmany cases, despite the fact that some small fraction of players crash,\nsymmetric equilibria remain efficient in terms of their group performances and,\nat the same time, serve as approximate equilibria. We show that this principle\nholds for a class of games, which we call monotonously scalable games. This\napplies in particular to our search game, assuming the sharing policy, in which\n$C(s)=1/s$. For the exclusive policy, this general result does not hold, but we\nshow that the symmetric equilibrium is nevertheless robust under mild\nassumptions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 19:53:00 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 07:36:34 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 16:08:39 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Korman", "Amos", ""], ["Rodeh", "Yoav", ""]]}, {"id": "1811.01319", "submitter": "Remesh Babu K R", "authors": "Sreelekshmi S and K R Remesh Babu", "title": "Synchronized Multi-Load Balancer with Fault Tolerance in Cloud", "comments": "8 Pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this method, service of one load balancer can be borrowed or shared among\nother load balancers when any correction is needed in the estimation of the\nload.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 03:55:53 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["S", "Sreelekshmi", ""], ["Babu", "K R Remesh", ""]]}, {"id": "1811.01332", "submitter": "Alexander Spiegelman", "authors": "Ittai Abraham, Dahlia Malkhi, and Alexander Spiegelman", "title": "Validated Asynchronous Byzantine Agreement with Optimal Resilience and\n  Asymptotically Optimal Time and Word Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new protocol for Validated Asynchronous Byzantine Agreement.\nValidated (multi-valued) Asynchronous Byzantine Agreement is a key building\nblock in constructing Atomic Broadcast and fault-tolerant state machine\nreplication in the asynchronous setting. Our protocol can withstand the optimal\nnumber $f<n/3$ of Byzantine failures and reaches agreement in the\nasymptotically optimal expected $O(1)$ running time. Honest parties in our\nprotocol send only an expected $O(n^2)$ messages where each message contains a\nvalue and a constant number of signatures. Hence our total expected\ncommunication is $O(n^2)$ words. The best previous result of Cachin et al. from\n2001 solves Validated Byzantine Agreement with optimal resilience and $O(1)$\nexpected time but with $O(n^3)$ expected word communication. Our work addresses\nan open question of Cachin et al. from 2001 and improves the expected word\ncommunication from $O(n^3)$ to the asymptotically optimal $O(n^2)$.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 07:44:50 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Abraham", "Ittai", ""], ["Malkhi", "Dahlia", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "1811.01344", "submitter": "Ahmed Eleliemy", "authors": "Ahmed Eleliemy, Ali Mohammed, Florina M. Ciorba", "title": "Exploring the Relation Between Two Levels of Scheduling Using a Novel\n  Simulation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern high performance computing (HPC) systems exhibit a rapid growth in\nsize, both \"horizontally\" in the number of nodes, as well as \"vertically\" in\nthe number of cores per node. As such, they offer additional levels of hardware\nparallelism. Each such level requires and employs algorithms for appropriately\nscheduling the computational work at the respective level. The present work\nexplores the relation between two scheduling levels: batch and application.\nUnderstanding this relation is important for improving the performance of\nscientific applications, that are scheduled and executed in batches on HPC\nsystems. The relation between batch and application level scheduling is\nunderstudied in the literature. Understanding the relation and interaction\nbetween these two scheduling levels requires their simultaneous analysis during\noperation. In this work, such an analysis is performed via simultaneous\nsimulation of batch and application level scheduling for a number of scenarios.\nA generic simulation approach is presented that bridges two existing simulators\nfrom the two scheduling levels. A novel two-level simulator that implements the\nproposed approach is introduced. The two-level simulator is used to simulate\nall combinations of three batch scheduling and four application scheduling\nalgorithms from the literature. These combinations are considered for\nallocating resources and executing the parallel jobs of two batches from two\nproduction HPC systems. The results of the scheduling experiments reveal the\nstrong relation between the two scheduling levels and their mutual influence.\nComplementing the simulations, the two-level simulator produces standard\nparallel execution traces, which can visually be examined and which illustrate\nthe execution of different jobs and, for each job, the execution of its tasks\nat node and core levels, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 09:21:04 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Eleliemy", "Ahmed", ""], ["Mohammed", "Ali", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1811.01421", "submitter": "Faith Ellen", "authors": "Dan Alistarh, James Aspnes, Faith Ellen, Rati Gelashvili, Leqi Zhu", "title": "Why Extension-Based Proofs Fail", "comments": "This version of the paper is for the NIS model. Previous versions of\n  the paper are for the NIIS model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce extension-based proofs, a class of impossibility proofs that\nincludes valency arguments. They are modelled as an interaction between a\nprover and a protocol. Using proofs based on combinatorial topology, it has\nbeen shown that it is impossible to deterministically solve k-set agreement\namong n > k > 1 processes in a wait-free manner in certain asynchronous models.\nHowever, it was unknown whether proofs based on simpler techniques were\npossible. We show that this impossibility result cannot be obtained for one of\nthese models by an extension-based proof and, hence, extension-based proofs are\nlimited in power.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 19:14:48 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 04:09:06 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 23:50:12 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Alistarh", "Dan", ""], ["Aspnes", "James", ""], ["Ellen", "Faith", ""], ["Gelashvili", "Rati", ""], ["Zhu", "Leqi", ""]]}, {"id": "1811.01532", "submitter": "Sungho Shin", "authors": "Sungho Shin, Youngmin Jo, Jungwook Choi, Swagath Venkataramani,\n  Vijayalakshmi Srinivasan, and Wonyong Sung", "title": "Workload-aware Automatic Parallelization for Multi-GPU DNN Training", "comments": "This paper is accepted in ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have emerged as successful solutions for variety\nof artificial intelligence applications, but their very large and deep models\nimpose high computational requirements during training. Multi-GPU\nparallelization is a popular option to accelerate demanding computations in DNN\ntraining, but most state-of-the-art multi-GPU deep learning frameworks not only\nrequire users to have an in-depth understanding of the implementation of the\nframeworks themselves, but also apply parallelization in a straight-forward way\nwithout optimizing GPU utilization. In this work, we propose a workload-aware\nauto-parallelization framework (WAP) for DNN training, where the work is\nautomatically distributed to multiple GPUs based on the workload\ncharacteristics. We evaluate WAP using TensorFlow with popular DNN benchmarks\n(AlexNet and VGG-16), and show competitive training throughput compared with\nthe state-of-the-art frameworks, and also demonstrate that WAP automatically\noptimizes GPU assignment based on the workload's compute requirements, thereby\nimproving energy efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 07:02:40 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 01:47:07 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Shin", "Sungho", ""], ["Jo", "Youngmin", ""], ["Choi", "Jungwook", ""], ["Venkataramani", "Swagath", ""], ["Srinivasan", "Vijayalakshmi", ""], ["Sung", "Wonyong", ""]]}, {"id": "1811.01643", "submitter": "Dennis Olivetti", "authors": "Alkida Balliu, Juho Hirvonen, Dennis Olivetti, Jukka Suomela", "title": "Hardness of minimal symmetry breaking in distributed computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is weakly $2$-colored if the nodes are labeled with colors black and\nwhite such that each black node is adjacent to at least one white node and vice\nversa. In this work we study the distributed computational complexity of weak\n$2$-coloring in the standard LOCAL model of distributed computing, and how it\nis related to the distributed computational complexity of other graph problems.\nFirst, we show that weak $2$-coloring is a minimal distributed\nsymmetry-breaking problem for regular even-degree trees and high-girth graphs:\nif there is any non-trivial locally checkable labeling problem that is solvable\nin $o(\\log^* n)$ rounds with a distributed graph algorithm in the middle of a\nregular even-degree tree, then weak $2$-coloring is also solvable in $o(\\log^*\nn)$ rounds there. Second, we prove a tight lower bound of $\\Omega(\\log^* n)$\nfor the distributed computational complexity of weak $2$-coloring in regular\ntrees; previously only a lower bound of $\\Omega(\\log \\log^* n)$ was known. By\nminimality, the same lower bound holds for any non-trivial locally checkable\nproblem inside regular even-degree trees.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 12:39:00 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 11:59:54 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 14:52:28 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Balliu", "Alkida", ""], ["Hirvonen", "Juho", ""], ["Olivetti", "Dennis", ""], ["Suomela", "Jukka", ""]]}, {"id": "1811.01669", "submitter": "Yongzhe Zhang", "authors": "Yongzhe Zhang and Zhenjiang Hu", "title": "Composing Optimization Techniques for Vertex-Centric Graph Processing\n  via Communication Channels", "comments": "10 pages, 4 figures, submitted to IPDPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pregel's vertex-centric model allows us to implement many interesting graph\nalgorithms, where optimization plays an important role in making it practically\nuseful. Although many optimizations have been developed for dealing with\ndifferent performance issues, it is hard to compose them together to optimize\ncomplex algorithms, where we have to deal with multiple performance issues at\nthe same time. In this paper, we propose a new approach to composing\noptimizations, by making use of the \\emph{channel} interface, as a replacement\nof Pregel's message passing and aggregator mechanism, which can better\nstructure the communication in Pregel algorithms. We demonstrate that it is\nconvenient to optimize a Pregel program by simply using a proper channel from\nthe channel library or composing them to deal with multiple performance issues.\nWe intensively evaluate the approach through many nontrivial examples. By\nadopting the channel interface, our system achieves an all-around performance\ngain for various graph algorithms. In particular, the composition of different\noptimizations makes the S-V algorithm 2.20x faster than the current best\nimplementation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:28:45 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "Yongzhe", ""], ["Hu", "Zhenjiang", ""]]}, {"id": "1811.01672", "submitter": "Alkida Balliu", "authors": "Alkida Balliu, Sebastian Brandt, Yi-Jun Chang, Dennis Olivetti,\n  Mika\\\"el Rabie, Jukka Suomela", "title": "The distributed complexity of locally checkable problems on paths is\n  decidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a computer network that consists of a path with $n$ nodes. The nodes\nare labeled with inputs from a constant-sized set, and the task is to find\noutput labels from a constant-sized set subject to some local\nconstraints---more formally, we have an LCL (locally checkable labeling)\nproblem. How many communication rounds are needed (in the standard LOCAL model\nof computing) to solve this problem?\n  It is well known that the answer is always either $O(1)$ rounds, or\n$\\Theta(\\log^* n)$ rounds, or $\\Theta(n)$ rounds. In this work we show that\nthis question is decidable (albeit PSPACE-hard): we present an algorithm that,\ngiven any LCL problem defined on a path, outputs the distributed computational\ncomplexity of this problem and the corresponding asymptotically optimal\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:37:30 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 13:55:30 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Chang", "Yi-Jun", ""], ["Olivetti", "Dennis", ""], ["Rabie", "Mika\u00ebl", ""], ["Suomela", "Jukka", ""]]}, {"id": "1811.01770", "submitter": "Giovanni Farina", "authors": "Silvia Bonomi, Giovanni Farina (NPA), S\\'ebastien Tixeuil (NPA)", "title": "Reliable Broadcast in Dynamic Networks with Locally Bounded Byzantine\n  Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring reliable communication despite possibly malicious participants is a\nprimary objective in any distributed system or network. In this paper, we\ninvestigate the possibility of reliable broadcast in a dynamic network whose\ntopology may evolve while the broadcast is in progress. In particular, we adapt\nthe Certified Propagation Algorithm (CPA) to make it work on dynamic networks\nand we present conditions (on the underlying dynamic graph) to enable safety\nand liveness properties of the reliable broadcast. We furthermore explore the\ncomplexity of assessing these conditions for various classes of dynamic\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:04:30 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Bonomi", "Silvia", "", "NPA"], ["Farina", "Giovanni", "", "NPA"], ["Tixeuil", "S\u00e9bastien", "", "NPA"]]}, {"id": "1811.01929", "submitter": "Sayed Chhattan Shah", "authors": "Sayed Chhattan Shah", "title": "Mobile Edge Cloud: Opportunities and Challenges", "comments": "4th Annual Conference on Computational Science and Computational\n  Intelligence, December 14-16, 2017, Las Vegas, Nevada, USA. arXiv admin note:\n  text overlap with arXiv:1810.07042", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge cloud is emerging as a promising technology to the internet of\nthings and cyber-physical system applications such as smart home and\nintelligent video surveillance. In a smart home, various sensors are deployed\nto monitor the home environment and physiological health of individuals. The\ndata collected by sensors are sent to an application, where numerous algorithms\nfor emotion and sentiment detection, activity recognition and situation\nmanagement are applied to provide healthcare- and emergency-related services\nand to manage resources at the home. The executions of these algorithms require\na vast amount of computing and storage resources. To address the issue, the\nconventional approach is to send the collected data to an application on an\ninternet cloud. This approach has several problems such as high communication\nlatency, communication energy consumption and unnecessary data traffic to the\ncore network. To overcome the drawbacks of the conventional cloud-based\napproach, a new system called mobile edge cloud is proposed. In mobile edge\ncloud, multiple mobiles and stationary devices interconnected through wireless\nlocal area networks are combined to create a small cloud infrastructure at a\nlocal physical area such as a home. Compared to traditional mobile distributed\ncomputing systems, mobile edge cloud introduces several complex challenges due\nto the heterogeneous computing environment, heterogeneous and dynamic network\nenvironment, node mobility, and limited battery power. The real-time\nrequirements associated with the internet of things and cyber-physical system\napplications make the problem even more challenging. In this paper, we describe\nthe applications and challenges associated with the design and development of\nmobile edge cloud system and propose an architecture based on a cross layer\ndesign approach for effective decision making.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 04:41:01 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Shah", "Sayed Chhattan", ""]]}, {"id": "1811.01997", "submitter": "Ami Paz", "authors": "Keren Censor-Hillel, Ami Paz, Noam Ravid", "title": "The Sparsest Additive Spanner via Multiple Weighted BFS Trees", "comments": "Preliminary versions appeared in OPODIS 2018 conference and in TCS\n  journal", "journal-ref": null, "doi": "10.1016/j.tcs.2020.05.035", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanners are fundamental graph structures that sparsify graphs at the cost of\nsmall stretch. In particular, in recent years, many sequential algorithms\nconstructing additive all-pairs spanners were designed, providing very sparse\nsmall-stretch subgraphs. Remarkably, it was then shown that the known\n(+6)-spanner constructions are essentially the sparsest possible, that is, a\nlarger additive stretch cannot guarantee a sparser spanner, which brought the\nstretch-sparsity trade-off to its limit. Distributed constructions of spanners\nare also abundant. However, for additive spanners, while there were algorithms\nconstructing (+2) and (+4)-all-pairs spanners, the sparsest case of\n(+6)-spanners remained elusive.\n  We remedy this by designing a new sequential algorithm for constructing a\n(+6)-spanner with the essentially-optimal sparsity of roughly O(n^{4/3}) edges.\nWe then show a distributed implementation of our algorithm, answering an open\nproblem in [Censor-Hillel et al., DISC 2016].\n  A main ingredient in our distributed algorithm is an efficient construction\nof multiple weighted BFS trees. A weighted BFS tree is a BFS tree in a weighted\ngraph, that consists of the lightest among all shortest paths from the root to\neach node. We present a distributed algorithm in the CONGEST model, that\nconstructs multiple weighted BFS trees in |S|+D-1 rounds, where S is the set of\nsources and D is the diameter of the network graph.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 19:44:50 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 14:20:52 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Paz", "Ami", ""], ["Ravid", "Noam", ""]]}, {"id": "1811.02084", "submitter": "Noam Shazeer", "authors": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani,\n  Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\n  Young, Ryan Sepassi, Blake Hechtman", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch-splitting (data-parallelism) is the dominant distributed Deep Neural\nNetwork (DNN) training strategy, due to its universal applicability and its\namenability to Single-Program-Multiple-Data (SPMD) programming. However,\nbatch-splitting suffers from problems including the inability to train very\nlarge models (due to memory constraints), high latency, and inefficiency at\nsmall batch sizes. All of these can be solved by more general distribution\nstrategies (model-parallelism). Unfortunately, efficient model-parallel\nalgorithms tend to be complicated to discover, describe, and to implement,\nparticularly on large clusters. We introduce Mesh-TensorFlow, a language for\nspecifying a general class of distributed tensor computations. Where\ndata-parallelism can be viewed as splitting tensors and operations along the\n\"batch\" dimension, in Mesh-TensorFlow, the user can specify any\ntensor-dimensions to be split across any dimensions of a multi-dimensional mesh\nof processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting\nof parallel operations coupled with collective communication primitives such as\nAllreduce. We use Mesh-TensorFlow to implement an efficient data-parallel,\nmodel-parallel version of the Transformer sequence-to-sequence model. Using TPU\nmeshes of up to 512 cores, we train Transformer models with up to 5 billion\nparameters, surpassing state of the art results on WMT'14 English-to-French\ntranslation task and the one-billion-word language modeling benchmark.\nMesh-Tensorflow is available at https://github.com/tensorflow/mesh .\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:25:02 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Shazeer", "Noam", ""], ["Cheng", "Youlong", ""], ["Parmar", "Niki", ""], ["Tran", "Dustin", ""], ["Vaswani", "Ashish", ""], ["Koanantakool", "Penporn", ""], ["Hawkins", "Peter", ""], ["Lee", "HyoukJoong", ""], ["Hong", "Mingsheng", ""], ["Young", "Cliff", ""], ["Sepassi", "Ryan", ""], ["Hechtman", "Blake", ""]]}, {"id": "1811.02144", "submitter": "Li Tang", "authors": "Li Tang, Konstantinos Konstantinidis and Aditya Ramamoorthy", "title": "Erasure coding for distributed matrix multiplication for matrices with\n  bounded entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed matrix multiplication is widely used in several scientific\ndomains. It is well recognized that computation times on distributed clusters\nare often dominated by the slowest workers (called stragglers). Recent work has\ndemonstrated that straggler mitigation can be viewed as a problem of designing\nerasure codes. For matrices $\\mathbf A$ and $\\mathbf B$, the technique\nessentially maps the computation of $\\mathbf A^T \\mathbf B$ into the\nmultiplication of smaller (coded) submatrices. The stragglers are treated as\nerasures in this process. The computation can be completed as long as a certain\nnumber of workers (called the recovery threshold) complete their assigned\ntasks.\n  We present a novel coding strategy for this problem when the absolute values\nof the matrix entries are sufficiently small. We demonstrate a tradeoff between\nthe assumed absolute value bounds on the matrix entries and the recovery\nthreshold. At one extreme, we are optimal with respect to the recovery\nthreshold and on the other extreme, we match the threshold of prior work.\nExperimental results on cloud-based clusters validate the benefits of our\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 03:24:06 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 17:34:47 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Tang", "Li", ""], ["Konstantinidis", "Konstantinos", ""], ["Ramamoorthy", "Aditya", ""]]}, {"id": "1811.02403", "submitter": "Andrey Demichev", "authors": "Alexander Kryukov and Andrey Demichev", "title": "Architecture of Distributed Data Storage for Astroparticle Physics", "comments": "11 pages, 2 figures", "journal-ref": "Lobachevskii Journal of Mathematics, 2018, Vol. 39, No. 9, pp.\n  1199-1206", "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the successful development of the astrophysics and, accordingly, for\nobtaining more complete knowledge of the Universe, it is extremely important to\ncombine and comprehensively analyze information of various types (e.g., about\ncharged cosmic particles, gamma rays, neutrinos, etc.) obtained by using divers\nlarge-scale experimental setups located throughout the world. It is obvious\nthat all kinds of activities must be performed continually across all stages of\nthe data life cycle to help support effective data management, in particular,\nthe collection and storage of data, its processing and analysis, refining the\nphysical model, making preparations for publication, and data reprocessing\ntaking refinement into account. In this paper we present a general approach to\nconstruction and the architecture of a system to be able to collect, store, and\nprovide users' access to astrophysical data. We also suggest a new approach to\nthe construction of a metadata registry based on the blockchain technology.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 15:22:50 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Kryukov", "Alexander", ""], ["Demichev", "Andrey", ""]]}, {"id": "1811.02512", "submitter": "Chen Yuan", "authors": "Junjie Shi, Guangyi Liu, Renchang Dai, Jingjin Wu, Chen Yuan, Zhiwei\n  Wang", "title": "Graph Based Power Flow Calculation for Energy Management System", "comments": "5 pages, 4 figures, 3 tables, Proc. of 2018 IEEE Power and Energy\n  Society General Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power flow calculation in EMS is required to accommodate a large and complex\npower system. To achieve a faster than real-time calculation, a graph based\npower flow calculation is proposed in this paper. Graph database and graph\ncomputing advantages in power system calculations are presented. A linear\nsolver for power flow application is formulated and decomposed in nodal\nparallelism and hierarchical parallelism to fully utilize graph parallel\ncomputing capability. Comparison of the algorithm with traditional sequential\nprograms shows significant benefits on computation efficiency. Case studies on\npractical large-scale systems provide supporting evidence that the new\nalgorithm is promising for online computing for EMS.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 16:27:16 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Shi", "Junjie", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wu", "Jingjin", ""], ["Yuan", "Chen", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1811.02605", "submitter": "Ronald Caplan", "authors": "R. M. Caplan, J. A. Linker, Z. Miki\\'c, C. Downs, T. T\\\"or\\\"ok, and V.\n  S. Titov", "title": "GPU Acceleration of an Established Solar MHD Code using OpenACC", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1088/1742-6596/1225/1/012012", "report-no": null, "categories": "physics.comp-ph astro-ph.SR cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU accelerators have had a notable impact on high-performance computing\nacross many disciplines. They provide high performance with low cost/power, and\ntherefore have become a primary compute resource on many of the largest\nsupercomputers. Here, we implement multi-GPU acceleration into our Solar MHD\ncode (MAS) using OpenACC in a fully portable, single-source manner. Our\npreliminary implementation is focused on MAS running in a reduced physics\n\"zero-beta\" mode. While valuable on its own, our main goal is to pave the way\nfor a full physics, thermodynamic MHD implementation. We describe the OpenACC\nimplementation methodology and challenges. \"Time-to-solution\" performance\nresults of a production-level flux rope eruption simulation on multi-CPU and\nmulti-GPU systems are shown. We find that the GPU-accelerated MAS code has the\nability to run \"zero-beta\" simulations on a single multi-GPU server at speeds\npreviously requiring multiple CPU server-nodes of a supercomputer.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:30:50 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Caplan", "R. M.", ""], ["Linker", "J. A.", ""], ["Miki\u0107", "Z.", ""], ["Downs", "C.", ""], ["T\u00f6r\u00f6k", "T.", ""], ["Titov", "V. S.", ""]]}, {"id": "1811.02638", "submitter": "Maria Gorlatova", "authors": "Maria Gorlatova, Hazer Inaltekin, Mung Chiang", "title": "Characterizing Task Completion Latencies in Fog Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing, which distributes computing resources to multiple locations\nbetween the Internet of Things (IoT) devices and the cloud, is attracting\nconsiderable attention from academia and industry. Yet, despite the excitement\nabout the potential of fog computing, few comprehensive quantitative\ncharacteristics of the properties of fog computing architectures have been\nconducted. In this paper we examine the properties of task completion latencies\nin fog computing. First, we present the results of our empirical\nbenchmarking-based study of task completion latencies. The study covered a\nrange of settings, and uniquely considered both traditional and serverless fog\ncomputing execution points. It demonstrated the range of execution point\ncharacteristics in different locations and the relative stability of latency\ncharacteristics for a given location. It also highlighted properties of\nserverless execution that are not incorporated in existing fog computing\nalgorithms. Second, we present a framework we developed for co-optimizing task\ncompletion quality and latency, which was inspired by the insights of our\nempirical study. We describe fog computing task assignment problems we\nformulated under this framework, and present the algorithms we developed for\nsolving them.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:57:47 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Gorlatova", "Maria", ""], ["Inaltekin", "Hazer", ""], ["Chiang", "Mung", ""]]}, {"id": "1811.02653", "submitter": "Vipul Gupta", "authors": "Vipul Gupta and Shusen Wang and Thomas Courtade and Kannan Ramchandran", "title": "OverSketch: Approximate Matrix Multiplication for the Cloud", "comments": "Published in Proc. IEEE Big Data 2018. Updated version provides\n  details of distributed sketching and highlights other advantages of\n  OverSketch", "journal-ref": "2018 IEEE International Conference on Big Data (Big Data),\n  Seattle, WA, USA, 2018, pp. 298-304", "doi": "10.1109/BigData.2018.8622139", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose OverSketch, an approximate algorithm for distributed matrix\nmultiplication in serverless computing. OverSketch leverages ideas from matrix\nsketching and high-performance computing to enable cost-efficient\nmultiplication that is resilient to faults and straggling nodes pervasive in\nlow-cost serverless architectures. We establish statistical guarantees on the\naccuracy of OverSketch and empirically validate our results by solving a\nlarge-scale linear program using interior-point methods and demonstrate a 34%\nreduction in compute time on AWS Lambda.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 21:09:43 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 01:29:05 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Gupta", "Vipul", ""], ["Wang", "Shusen", ""], ["Courtade", "Thomas", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1811.02760", "submitter": "Slobodan Mitrovi\\'c", "authors": "Buddhima Gamlath, Sagar Kale, Slobodan Mitrovi\\'c, Ola Svensson", "title": "Weighted Matchings via Unweighted Augmentations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a generic method for reducing the task of finding weighted\nmatchings to that of finding short augmenting paths in unweighted graphs. This\nmethod enables us to provide efficient implementations for approximating\nweighted matchings in the streaming model and in the massively parallel\ncomputation (MPC) model.\n  In the context of streaming with random edge arrivals, our techniques yield a\n$(1/2+c)$-approximation algorithm thus breaking the natural barrier of $1/2$.\nFor multi-pass streaming and the MPC model, we show that any algorithm\ncomputing a $(1-\\delta)$-approximate unweighted matching in bipartite graphs\ncan be translated into an algorithm that computes a\n$(1-\\varepsilon(\\delta))$-approximate maximum weighted matching. Furthermore,\nthis translation incurs only a constant factor (that depends on $\\varepsilon>\n0$) overhead in the complexity. Instantiating this with the current best\nmulti-pass streaming and MPC algorithms for unweighted matchings yields the\nfollowing results for maximum weighted matchings:\n  * A $(1-\\varepsilon)$-approximation streaming algorithm that uses\n$O_\\varepsilon(1)$ passes and $O_\\varepsilon(n\\, \\text{poly} (\\log n))$ memory.\nThis is the first $(1-\\varepsilon)$-approximation streaming algorithm for\nweighted matchings that uses a constant number of passes (only depending on\n$\\varepsilon$).\n  * A $(1 - \\varepsilon)$-approximation algorithm in the MPC model that uses\n$O_\\varepsilon(\\log \\log n)$ rounds, $O(m/n)$ machines per round, and\n$O_\\varepsilon(n\\, \\text{poly}(\\log n))$ memory per machine. This improves upon\nthe previous best approximation guarantee of $(1/2-\\varepsilon)$ for weighted\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 04:48:57 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Gamlath", "Buddhima", ""], ["Kale", "Sagar", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Svensson", "Ola", ""]]}, {"id": "1811.02881", "submitter": "Seong Hah Cho", "authors": "Seong Hah Cho, Cody A Cushing, Kunal Patel, Alok Kothari, Rongjian\n  Lan, Matthias Michel, Mouslim Cherkaoui, and Hakwan Lau", "title": "Blockchain and human episodic memory", "comments": "30 pages, 2 figures; Minor edits, added figures, revised and updated\n  sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We relate the concepts used in decentralized ledger technology to studies of\nepisodic memory in the mammalian brain. Specifically, we introduce the standard\nconcepts of linked list, hash functions, and sharding, from computer science.\nWe argue that these concepts may be more relevant to studies of the neural\nmechanisms of memory than has been previously appreciated. In turn, we also\nhighlight that certain phenomena studied in the brain, namely metacognition,\nreality monitoring, and how perceptual conscious experiences come about, may\ninspire development in blockchain technology too, specifically regarding\nprobabilistic consensus protocols.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 04:35:22 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 05:49:27 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 04:41:24 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Cho", "Seong Hah", ""], ["Cushing", "Cody A", ""], ["Patel", "Kunal", ""], ["Kothari", "Alok", ""], ["Lan", "Rongjian", ""], ["Michel", "Matthias", ""], ["Cherkaoui", "Mouslim", ""], ["Lau", "Hakwan", ""]]}, {"id": "1811.02882", "submitter": "Thierry Garaix", "authors": "F Croce, Thierry Garaix (LIMOS, CIS-ENSMSE), A. Grosso", "title": "Iterated local search and very large neighborhoods for the\n  parallel-machines total tardiness problem", "comments": null, "journal-ref": "Computers \\& Operations Research, 2012, 39 (6), pp.1213 - 1217", "doi": "10.1016/j.cor.2010.10.017", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present computational results with a heuristic algorithm for the parallel\nmachines total weighted tardiness problem. The algorithm combines generalized\npairwise interchange neighborhoods, dynasearch optimization and a new\nmachine-based neighborhood whose size is non-polynomial in the number of\nmachines. The computational results significantly improve over the current\nstate of the art for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 14:47:01 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Croce", "F", "", "LIMOS, CIS-ENSMSE"], ["Garaix", "Thierry", "", "LIMOS, CIS-ENSMSE"], ["Grosso", "A.", ""]]}, {"id": "1811.02883", "submitter": "Ananda Samajdar", "authors": "Ananda Samajdar, Yuhao Zhu, Paul Whatmough, Matthew Mattina and Tushar\n  Krishna", "title": "SCALE-Sim: Systolic CNN Accelerator Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systolic Arrays are one of the most popular compute substrates within Deep\nLearning accelerators today, as they provide extremely high efficiency for\nrunning dense matrix multiplications. However, the research community lacks\ntools to insights on both the design trade-offs and efficient mapping\nstrategies for systolic-array based accelerators. We introduce Systolic CNN\nAccelerator Simulator (SCALE-Sim), which is a configurable systolic array based\ncycle accurate DNN accelerator simulator. SCALE-Sim exposes various\nmicro-architectural features as well as system integration parameters to the\ndesigner to enable comprehensive design space exploration. This is the first\nsystolic-array simulator tuned for running DNNs to the best of our knowledge.\nUsing SCALE-Sim, we conduct a suite of case studies and demonstrate the effect\nof bandwidth, data flow and aspect ratio on the overall runtime and energy of\nDeep Learning kernels across vision, speech, text, and games. We believe that\nthese insights will be highly beneficial to architects and ML practitioners.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 22:33:46 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 04:01:45 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Samajdar", "Ananda", ""], ["Zhu", "Yuhao", ""], ["Whatmough", "Paul", ""], ["Mattina", "Matthew", ""], ["Krishna", "Tushar", ""]]}, {"id": "1811.02884", "submitter": "Yifan Sun", "authors": "Yifan Sun, Trinayan Baruah, Saiful A. Mojumder, Shi Dong, Rafael Ubal,\n  Xiang Gong, Shane Treadway, Yuhui Bao, Vincent Zhao, Jos\\'e L. Abell\\'an,\n  John Kim, Ajay Joshi, David Kaeli", "title": "MGSim + MGMark: A Framework for Multi-GPU System Research", "comments": "Updated typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly growing popularity and scale of data-parallel workloads demand a\ncorresponding increase in raw computational power of GPUs (Graphics Processing\nUnits). As single-GPU systems struggle to satisfy the performance demands,\nmulti-GPU systems have begun to dominate the high-performance computing world.\nThe advent of such systems raises a number of design challenges, including the\nGPU microarchitecture, multi-GPU interconnect fabrics, runtime libraries and\nassociated programming models. The research community currently lacks a\npublically available and comprehensive multi-GPU simulation framework and\nbenchmark suite to evaluate multi-GPU system design solutions.\n  In this work, we present MGSim, a cycle-accurate, extensively validated,\nmulti-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set\narchitecture. We complement MGSim with MGMark, a suite of multi-GPU workloads\nthat explores multi-GPU collaborative execution patterns. Our simulator is\nscalable and comes with in-built support for multi-threaded execution to enable\nfast and efficient simulations. In terms of performance accuracy, MGSim differs\n$5.5\\%$ on average when compared against actual GPU hardware. We also achieve a\n$3.5\\times$ and a $2.5\\times$ average speedup in function emulation and\narchitectural simulation with 4 CPU cores, while delivering the same accuracy\nas the serial simulation.\n  We illustrate the novel simulation capabilities provided by our simulator\nthrough a case study exploring programming models based on a unified multi-GPU\nsystem (U-MGPU) and a discrete multi-GPU system (D-MGPU) that both utilize\nunified memory space and cross-GPU memory access. We evaluate the design\nimplications from our case study, suggesting that D-MGPU is an attractive\nprogramming model for future multi-GPU systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 18:38:57 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 02:17:30 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 20:20:00 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Sun", "Yifan", ""], ["Baruah", "Trinayan", ""], ["Mojumder", "Saiful A.", ""], ["Dong", "Shi", ""], ["Ubal", "Rafael", ""], ["Gong", "Xiang", ""], ["Treadway", "Shane", ""], ["Bao", "Yuhui", ""], ["Zhao", "Vincent", ""], ["Abell\u00e1n", "Jos\u00e9 L.", ""], ["Kim", "John", ""], ["Joshi", "Ajay", ""], ["Kaeli", "David", ""]]}, {"id": "1811.03254", "submitter": "Yun Kuen Cheung", "authors": "Yun Kuen Cheung, Richard Cole and Yixin Tao", "title": "Fully Asynchronous Stochastic Coordinate Descent: A Tight Lower Bound on\n  the Parallelism Achieving Linear Speedup", "comments": "Accepted for publication in Mathematical Programming (Series A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek tight bounds on the viable parallelism in asynchronous\nimplementations of coordinate descent that achieves linear speedup. We focus on\nasynchronous coordinate descent (ACD) algorithms on convex functions which\nconsist of the sum of a smooth convex part and a possibly non-smooth separable\nconvex part.\n  We quantify the shortfall in progress compared to the standard sequential\nstochastic gradient descent. This leads to a simple yet tight analysis of the\nstandard stochastic ACD in a partially asynchronous environment, generalizing\nand improving the bounds in prior work. We also give a considerably more\ninvolved analysis for general asynchronous environments in which the only\nconstraint is that each update can overlap with at most q others. The new lower\nbound on the maximum degree of parallelism attaining linear speedup is tight\nand improves the best prior bound almost quadratically.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 03:52:51 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 19:12:31 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 21:04:01 GMT"}, {"version": "v4", "created": "Sun, 2 Aug 2020 18:08:34 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Cheung", "Yun Kuen", ""], ["Cole", "Richard", ""], ["Tao", "Yixin", ""]]}, {"id": "1811.03278", "submitter": "Chaodong Zheng", "authors": "Calvin Newport and Chaodong Zheng", "title": "Approximate Neighbor Counting in Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many distributed algorithms, neighborhood size is an important parameter.\nIn radio networks, however, obtaining this information can be difficult due to\nad hoc deployments and communication that occurs on a collision-prone shared\nchannel. This paper conducts a comprehensive survey of the approximate neighbor\ncounting problem, which requires nodes to obtain a constant factor\napproximation of the size of their network neighborhood. We produce new lower\nand upper bounds for three main variations of this problem in the radio network\nmodel: (a) the network is single-hop and every node must obtain an estimate of\nits neighborhood size; (b) the network is multi-hop and only a designated node\nmust obtain an estimate of its neighborhood size; and (c) the network is\nmulti-hop and every node must obtain an estimate of its neighborhood size. In\nstudying these problem variations, we consider solutions with and without\ncollision detection, and with both constant and high success probability. Some\nof our results are extensions of existing strategies, while others require\ntechnical innovations. We argue this collection of results provides insight\ninto the nature of this well-motivated problem (including how it differs from\nrelated symmetry breaking tasks in radio networks), and provides a useful\ntoolbox for algorithm designers tackling higher level problems that might\nbenefit from neighborhood size estimates.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:21:10 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Newport", "Calvin", ""], ["Zheng", "Chaodong", ""]]}, {"id": "1811.03337", "submitter": "Danupon Nanongkai", "authors": "Aaron Bernstein, Danupon Nanongkai", "title": "Distributed Exact Weighted All-Pairs Shortest Paths in Near-Linear Time", "comments": "Full version of STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the {\\em distributed all-pairs shortest paths} problem (APSP), every node\nin the weighted undirected distributed network (the CONGEST model) needs to\nknow the distance from every other node using least number of communication\nrounds (typically called {\\em time complexity}). The problem admits\n$(1+o(1))$-approximation $\\tilde\\Theta(n)$-time algorithm and a nearly-tight\n$\\tilde \\Omega(n)$ lower bound [Nanongkai, STOC'14; Lenzen and Patt-Shamir\nPODC'15]\\footnote{$\\tilde \\Theta$, $\\tilde O$ and $\\tilde \\Omega$ hide\npolylogarithmic factors. Note that the lower bounds also hold even in the\nunweighted case and in the weighted case with polynomial approximation\nratios~\\cite{LenzenP_podc13,HolzerW12,PelegRT12,Nanongkai-STOC14}.}. For the\nexact case, Elkin [STOC'17] presented an $O(n^{5/3} \\log^{2/3} n)$ time bound,\nwhich was later improved to $\\tilde O(n^{5/4})$ [Huang, Nanongkai, Saranurak\nFOCS'17]. It was shown that any super-linear lower bound (in $n$) requires a\nnew technique [Censor-Hillel, Khoury, Paz, DISC'17], but otherwise it remained\nwidely open whether there exists a $\\tilde O(n)$-time algorithm for the exact\ncase, which would match the best possible approximation algorithm.\n  This paper resolves this question positively: we present a randomized (Las\nVegas) $\\tilde O(n)$-time algorithm, matching the lower bound up to\npolylogarithmic factors. Like the previous $\\tilde O(n^{5/4})$ bound, our\nresult works for directed graphs with zero (and even negative) edge weights. In\naddition to the improved running time, our algorithm works in a more general\nsetting than that required by the previous $\\tilde O(n^{5/4})$ bound; in our\nsetting (i) the communication is only along edge directions (as opposed to\nbidirectional), and (ii) edge weights are arbitrary (as opposed to integers in\n{1, 2, ... poly(n)}). ...\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 10:00:29 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 23:00:01 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Bernstein", "Aaron", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1811.03419", "submitter": "Mark Christopher Ballandies", "authors": "Mark C. Ballandies, Marcus M. Dapp and Evangelos Pournaras", "title": "Decrypting Distributed Ledger Design -- Taxonomy, Classification and\n  Blockchain Community Evaluation", "comments": "Cluster Comput (2021)", "journal-ref": null, "doi": "10.1007/s10586-021-03256-w", "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than 1000 distributed ledger technology (DLT) systems raising $600\nbillion in investment in 2016 feature the unprecedented and disruptive\npotential of blockchain technology. A systematic and data-driven analysis,\ncomparison and rigorous evaluation of the different design choices of\ndistributed ledgers and their implications is a challenge. The rapidly evolving\nnature of the blockchain landscape hinders reaching a common understanding of\nthe techno-socio-economic design space of distributed ledgers and the\ncryptoeconomies they support. To fill this gap, this paper makes the following\ncontributions: (i) A conceptual architecture of DLT systems with which (ii) a\ntaxonomy is designed and (iii) a rigorous classification of DLT systems is made\nusing real-world data and wisdom of the crowd. (iv) A DLT design guideline is\nthe end result of applying machine learning methodologies on the classification\ndata. Compared to related work and as defined in earlier taxonomy theory, the\nproposed taxonomy is highly comprehensive, robust, explanatory and extensible.\nThe findings of this paper can provide new insights and better understanding of\nthe key design choices evolving the modeling complexity of DLT systems, while\nidentifying opportunities for new research contributions and business\ninnovation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 22:53:42 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 09:40:30 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 17:07:53 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ballandies", "Mark C.", ""], ["Dapp", "Marcus M.", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "1811.03617", "submitter": "Youjie Li", "authors": "Mingchao Yu, Zhifeng Lin, Krishna Narra, Songze Li, Youjie Li, Nam\n  Sung Kim, Alexander Schwing, Murali Annavaram, Salman Avestimehr", "title": "GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient\n  Aggregation in Distributed CNN Training", "comments": "Accepted at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism can boost the training speed of convolutional neural\nnetworks (CNN), but could suffer from significant communication costs caused by\ngradient aggregation. To alleviate this problem, several scalar quantization\ntechniques have been developed to compress the gradients. But these techniques\ncould perform poorly when used together with decentralized aggregation\nprotocols like ring all-reduce (RAR), mainly due to their inability to directly\naggregate compressed gradients. In this paper, we empirically demonstrate the\nstrong linear correlations between CNN gradients, and propose a gradient vector\nquantization technique, named GradiVeQ, to exploit these correlations through\nprincipal component analysis (PCA) for substantial gradient dimension\nreduction. GradiVeQ enables direct aggregation of compressed gradients, hence\nallows us to build a distributed learning system that parallelizes GradiVeQ\ngradient compression and RAR communications. Extensive experiments on popular\nCNNs demonstrate that applying GradiVeQ slashes the wall-clock gradient\naggregation time of the original RAR by more than 5X without noticeable\naccuracy loss, and reduces the end-to-end training time by almost 50%. The\nresults also show that GradiVeQ is compatible with scalar quantization\ntechniques such as QSGD (Quantized SGD), and achieves a much higher speed-up\ngain under the same compression ratio.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:59:50 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 06:01:28 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Yu", "Mingchao", ""], ["Lin", "Zhifeng", ""], ["Narra", "Krishna", ""], ["Li", "Songze", ""], ["Li", "Youjie", ""], ["Kim", "Nam Sung", ""], ["Schwing", "Alexander", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "1811.03619", "submitter": "Youjie Li", "authors": "Youjie Li, Mingchao Yu, Songze Li, Salman Avestimehr, Nam Sung Kim,\n  Alexander Schwing", "title": "Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep\n  Net Training", "comments": "Accepted at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of deep nets is an important technique to address some\nof the present day computing challenges like memory consumption and\ncomputational demands. Classical distributed approaches, synchronous or\nasynchronous, are based on the parameter server architecture, i.e., worker\nnodes compute gradients which are communicated to the parameter server while\nupdated parameters are returned. Recently, distributed training with AllReduce\noperations gained popularity as well. While many of those operations seem\nappealing, little is reported about wall-clock training time improvements. In\nthis paper, we carefully analyze the AllReduce based setup, propose timing\nmodels which include network latency, bandwidth, cluster size and compute time,\nand demonstrate that a pipelined training with a width of two combines the best\nof both synchronous and asynchronous training. Specifically, for a setup\nconsisting of a four-node GPU cluster we show wall-clock time training\nimprovements of up to 5.4x compared to conventional approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:59:55 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 04:54:32 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 08:38:49 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Li", "Youjie", ""], ["Yu", "Mingchao", ""], ["Li", "Songze", ""], ["Avestimehr", "Salman", ""], ["Kim", "Nam Sung", ""], ["Schwing", "Alexander", ""]]}, {"id": "1811.03642", "submitter": "Alexey Gotsman", "authors": "\\'Alvaro Garc\\'ia-P\\'erez, Alexey Gotsman", "title": "Federated Byzantine Quorum Systems (Extended Version)", "comments": "Extended version of a paper from OPODIS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the recent blockchain proposals, such as Stellar and Ripple, use\nquorum-like structures typical for Byzantine consensus while allowing for open\nmembership. This is achieved by constructing quorums in a decentralised way:\neach participant independently chooses whom to trust, and quorums arise from\nthese individual decisions. Unfortunately, the theoretical foundations\nunderlying such blockchains have not been thoroughly investigated. To close\nthis gap, in this paper we study decentralised quorum construction by means of\nfederated Byzantine quorum systems, used by Stellar. We rigorously prove the\ncorrectness of basic broadcast abstractions over federated quorum systems and\nestablish their relationship to the classical Byzantine quorum systems. In\nparticular, we prove correctness in the realistic setting where Byzantine nodes\nmay lie about their trust choices. We show that this setting leads to a novel\nvariant of Byzantine quorum systems where different nodes may have different\nunderstanding of what constitutes a quorum.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 19:03:56 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Garc\u00eda-P\u00e9rez", "\u00c1lvaro", ""], ["Gotsman", "Alexey", ""]]}, {"id": "1811.03657", "submitter": "Andrea Camisa", "authors": "Andrea Camisa, Ivano Notarnicola, Giuseppe Notarstefano", "title": "A Primal Decomposition Method with Suboptimality Bounds for Distributed\n  Mixed-Integer Linear Programming", "comments": "57th IEEE Conference on Decision and Control", "journal-ref": null, "doi": "10.1109/CDC.2018.8619597", "report-no": null, "categories": "cs.SY cs.DC cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with a network of agents seeking to solve in a\ndistributed way Mixed-Integer Linear Programs (MILPs) with a coupling\nconstraint (modeling a limited shared resource) and local constraints. MILPs\nare NP-hard problems and several challenges arise in a distributed framework,\nso that looking for suboptimal solutions is of interest. To achieve this goal,\nthe presence of a linear coupling calls for tailored decomposition approaches.\nWe propose a fully distributed algorithm based on a primal decomposition\napproach and a suitable tightening of the coupling constraints. Agents\nrepeatedly update local allocation vectors, which converge to an optimal\nresource allocation of an approximate version of the original problem. Based on\nsuch allocation vectors, agents are able to (locally) compute a mixed-integer\nsolution, which is guaranteed to be feasible after a sufficiently large time.\nAsymptotic and finite-time suboptimality bounds are established for the\ncomputed solution. Numerical simulations highlight the efficacy of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 19:32:41 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Camisa", "Andrea", ""], ["Notarnicola", "Ivano", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1811.03748", "submitter": "Umair Mohammad", "authors": "Umair Mohammad and Sameh Sorour", "title": "Adaptive Task Allocation for Mobile Edge Learning", "comments": "8 pages, 2 figures, submitted to IEEE WCNC Workshop 2019, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to establish a new optimization paradigm for implementing\nrealistic distributed learning algorithms, with performance guarantees, on\nwireless edge nodes with heterogeneous computing and communication capacities.\nWe will refer to this new paradigm as `Mobile Edge Learning (MEL)'. The problem\nof dynamic task allocation for MEL is considered in this paper with the aim to\nmaximize the learning accuracy, while guaranteeing that the total times of data\ndistribution/aggregation over heterogeneous channels, and local computing\niterations at the heterogeneous nodes, are bounded by a preset duration. The\nproblem is first formulated as a quadratically-constrained integer linear\nproblem. Being an NP-hard problem, the paper relaxes it into a non-convex\nproblem over real variables. We thus proposed two solutions based on deriving\nanalytical upper bounds of the optimal solution of this relaxed problem using\nLagrangian analysis and KKT conditions, and the use of suggest-and-improve\nstarting from equal batch allocation, respectively. The merits of these\nproposed solutions are exhibited by comparing their performances to both\nnumerical approaches and the equal task allocation approach.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:49:53 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 21:53:59 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Mohammad", "Umair", ""], ["Sorour", "Sameh", ""]]}, {"id": "1811.03767", "submitter": "Xin Long", "authors": "Xin Long and Jigang Wu and Long Chen", "title": "Energy-Efficient Offloading in Mobile Edge Computing with Edge-Cloud\n  Collaboration", "comments": "Accepted by the 18th International Conference on Algorithms and\n  Architectures for Parallel Processing (ICA3PP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple access mobile edge computing is an emerging technique to bring\ncomputation resources close to end mobile users. By deploying edge servers at\nWiFi access points or cellular base stations, the computation capabilities of\nmobile users can be extended. Existing works mostly assume the remote cloud\nserver can be viewed as a special edge server or the edge servers are willing\nto cooperate, which is not practical. In this work, we propose an edge-cloud\ncooperative architecture where edge servers can rent for the remote cloud\nservers to expedite the computation of tasks from mobile users. With this\narchitecture, the computation offloading problem is modeled as a mixed integer\nprogramming with delay constraints, which is NP-hard. The objective is to\nminimize the total energy consumption of mobile devices. We propose a greedy\nalgorithm as well as a simulated annealing algorithm to effectively solve the\nproblem. Extensive simulation results demonstrate that, the proposed greedy\nalgorithm and simulated annealing algorithm can achieve the near optimal\nperformance. On average, the proposed greedy algorithm can achieve the same\napplication completing time budget performance of the Brute Force optional\nalgorithm with only 31\\% extra energy cost. The simulated annealing algorithm\ncan achieve similar performance with the greedy algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 04:14:22 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Long", "Xin", ""], ["Wu", "Jigang", ""], ["Chen", "Long", ""]]}, {"id": "1811.03852", "submitter": "Chris Maynard", "authors": "Christopher M Maynard and David N Walters", "title": "Precision of the ENDGame: Mixed-precision arithmetic in the iterative\n  solver of the Unified Model", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Met Office's weather and climate simulation code the Unified Model is\nused for both operational Numerical Weather Prediction and Climate modelling.\nThe computational performance of the model running on parallel supercomputers\nis a key consideration. A Krylov sub-space solver is employed to solve the\nequations of the dynamical core of the model, known as ENDGame. These describe\nthe evolution of the Earth's atmosphere. Typically, 64-bit precision is used\nthroughout weather and climate applications. This work presents a\nmixed-precision implementation of the solver, the beneficial effect on run-time\nand the impact on solver convergence. The complex interplay of errors arising\nfrom accumulated round-off in floating-point arithmetic and other numerical\neffects is discussed. A careful analysis is required, however, the\nmixed-precision solver is now employed in the operational forecast to satisfy\nrun-time constraints without compromising the accuracy of the solution.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 10:30:26 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Maynard", "Christopher M", ""], ["Walters", "David N", ""]]}, {"id": "1811.03882", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Hirofumi Noguchi, Misao Kataoka, Takuma Isoda and Tatsuya\n  Demizu", "title": "Parallel processing area extraction and data transfer number reduction\n  for automatic GPU offloading of IoT applications", "comments": "6 pages, 4 figures, in Japanese, IEICE Technical Report, SC2018-32", "journal-ref": "IEICE Technical Report, SC2018-32, Nov. 2018. (c) 2018 IEICE", "doi": null, "report-no": "IEICE Technical Report, SC2018-32, Nov. 2018", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Open IoT, we have proposed Tacit Computing technology to discover the\ndevices that have data users need on demand and use them dynamically and an\nautomatic GPU offloading technology as an elementary technology of Tacit\nComputing. However, it can improve limited applications because it only\noptimizes parallelizable loop statements extraction. Thus, in this paper, to\nimprove performances of more applications automatically, we propose an improved\nmethod with reduction of data transfer between CPU and GPU. We evaluate our\nproposed offloading method by applying it to Darknet and find that it can\nprocess it 3 times as quickly as only using CPU.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:38:37 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Yamato", "Yoji", ""], ["Noguchi", "Hirofumi", ""], ["Kataoka", "Misao", ""], ["Isoda", "Takuma", ""], ["Demizu", "Tatsuya", ""]]}, {"id": "1811.03968", "submitter": "Lili Su", "authors": "Lili Su, Martin Zubeldia, Nancy Lynch", "title": "Collaboratively Learning the Best Option on Graphs, Using Bounded Local\n  Memory", "comments": "arXiv admin note: text overlap with arXiv:1802.08159. Authors' note:\n  This work shares some overlap with our preliminary preprint arXiv:1802.08159\n  which focuses on complete graphs. arXiv:1802.08159 is combined with this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-armed bandit problems in social groups wherein each\nindividual has bounded memory and shares the common goal of learning the best\narm/option. We say an individual learns the best option if eventually (as $t\\to\n\\infty$) it pulls only the arm with the highest expected reward. While this\ngoal is provably impossible for an isolated individual due to bounded memory,\nwe show that, in social groups, this goal can be achieved easily with the aid\nof social persuasion (i.e., communication) as long as the communication\nnetworks/graphs satisfy some mild conditions. To deal with the interplay\nbetween the randomness in the rewards and in the social interaction, we employ\nthe {\\em mean-field approximation} method. Considering the possibility that the\nindividuals in the networks may not be exchangeable when the communication\nnetworks are not cliques, we go beyond the classic mean-field techniques and\napply a refined version of mean-field approximation:\n  (1) Using coupling we show that, if the communication graph is connected and\nis either regular or has doubly-stochastic degree-weighted adjacency matrix,\nwith probability $\\to 1$ as the social group size $N \\to \\infty$, every\nindividual in the social group learns the best option.\n  (2) If the minimum degree of the graph diverges as $N \\to \\infty$, over an\narbitrary but given finite time horizon, the sample paths describing the\nopinion evolutions of the individuals are asymptotically independent. In\naddition, the proportions of the population with different opinions converge to\nthe unique solution of a system of ODEs. In the solution of the obtained ODEs,\nthe proportion of the population holding the correct opinion converges to $1$\nexponentially fast in time.\n  Notably, our results hold even if the communication graphs are highly sparse.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 03:59:55 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 02:17:41 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 05:16:40 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Su", "Lili", ""], ["Zubeldia", "Martin", ""], ["Lynch", "Nancy", ""]]}, {"id": "1811.04078", "submitter": "Aniruddh Rao Kabbinale", "authors": "Aniruddh Rao Kabbinale, Emmanouil Dimogerontakis, Mennan Selimi,\n  Anwaar Ali, Leandro Navarro, Arjuna Sathiaseelan and Jon Crowcroft", "title": "Blockchain for Economically Sustainable Wireless Mesh Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.00561", "journal-ref": null, "doi": null, "report-no": "ARK54-1", "categories": "cs.NI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralization, in the form of mesh networking and blockchain, two\npromising technologies, is coming to the telecommunications industry. Mesh\nnetworking allows wider low cost Internet access with infrastructures built\nfrom routers contributed by diverse owners, while blockchain enables\ntransparency and accountability for investments, revenue or other forms of\neconomic compensations from sharing of network traffic, content and services.\nCrowdsourcing network coverage, combined with crowdfunding costs, can create\neconomically sustainable yet decentralized Internet access. This means every\nparticipant can invest in resources, and pay or be paid for usage to recover\nthe costs of network devices and maintenance. While mesh networks and mesh\nrouting protocols enable self-organized networks that expand organically,\ncryptocurrencies and smart contracts enable the economic coordination among\nnetwork providers and consumers. We explore and evaluate two existing\nblockchain software stacks, Hyperledger Fabric (HLF) and Ethereum geth with\nProof of Authority (PoA) intended as a local lightweight distributed ledger,\ndeployed in a real city-wide production mesh network and also in laboratory\nnetwork. We quantify the performance, bottlenecks and identify the current\nlimitations and opportunities for improvement to serve locally the needs of\nwireless mesh networks, without the privacy and economic cost of relying on\npublic blockchains.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 15:59:26 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 10:18:36 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 16:31:57 GMT"}, {"version": "v4", "created": "Tue, 2 Apr 2019 08:27:07 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Kabbinale", "Aniruddh Rao", ""], ["Dimogerontakis", "Emmanouil", ""], ["Selimi", "Mennan", ""], ["Ali", "Anwaar", ""], ["Navarro", "Leandro", ""], ["Sathiaseelan", "Arjuna", ""], ["Crowcroft", "Jon", ""]]}, {"id": "1811.04276", "submitter": "Leonid Sokolinsky", "authors": "Nadezhda A. Ezhova, Leonid B. Sokolinsky", "title": "Scalability Evaluation of Iterative Algorithms Used for Supercomputer\n  Simulation of Physical processes", "comments": "To be published in proceedings of the Global Smart Industry\n  Conference (GloSIC'2018) http://glosic.susu.ru/", "journal-ref": null, "doi": "10.1109/GloSIC.2018.8570107", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is devoted to the development of a methodology for evaluating the\nscalability of compute-intensive iterative algorithms used in simulating\ncomplex physical processes on supercomputer systems. The proposed methodology\nis based on the BSF (Bulk Synchronous Farm) parallel computation model, which\nmakes it possible to predict the upper scalability bound of an iterative\nalgorithm in early phases of its design. The BSF model assumes the\nrepresentation of the algorithm in the form of operations on lists using\nhigh-order functions. Two classes of representations are considered: BSF-M (Map\nBSF) and BSF-MR (Map-Reduce BSF). The proposed methodology is described by the\nexample of the solution of the system of linear equations by the Jacobi method.\nFor the Jacobi method, two iterative algorithms are constructed: Jacobi-M based\non the BSF-M representation and Jacobi-MR based on the BSF-MR representation.\nAnalytical estimations of the speedup, parallel efficiency and upper\nscalability bound are constructed for these algorithms using the BSF cost\nmetrics on multiprocessor computing systems with distributed memory. An\ninformation about the implementation of these algorithms in C++ language using\nthe BSF program skeleton and MPI parallel programming library are given. The\nresults of large-scale computational experiments performed on a cluster\ncomputing system are demonstrated. Based on the experimental results, an\nanalysis of the adequacy of estimations obtained analytically by using the cost\nmetrics of the BSF model is made.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 16:04:05 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Ezhova", "Nadezhda A.", ""], ["Sokolinsky", "Leonid B.", ""]]}, {"id": "1811.04481", "submitter": "Sakil Barbhuiya", "authors": "Sakil Barbhuiya, Zafeirios Papazachos, Peter Kilpatrick and Dimitrios\n  S. Nikolopoulos", "title": "RADS: Real-time Anomaly Detection System for Cloud Data Centres", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity attacks in Cloud data centres are increasing alongside the\ngrowth of the Cloud services market. Existing research proposes a number of\nanomaly detection systems for detecting such attacks. However, these systems\nencounter a number of challenges, specifically due to the unknown behaviour of\nthe attacks and the occurrence of genuine Cloud workload spikes, which must be\ndistinguished from attacks. In this paper, we discuss these challenges and\ninvestigate the issues with the existing Cloud anomaly detection approaches.\nThen, we propose a Real-time Anomaly Detection System (RADS) for Cloud data\ncentres, which uses a one class classification algorithm and a window-based\ntime series analysis to address the challenges. Specifically, RADS can detect\nVM-level anomalies occurring due to DDoS and cryptomining attacks. We evaluate\nthe performance of RADS by running lab-based experiments and by using\nreal-world Cloud workload traces. Evaluation results demonstrate that RADS can\nachieve 90-95% accuracy with a low false positive rate of 0-3%. The results\nfurther reveal that RADS experiences fewer false positives when using its\nwindow-based time series analysis in comparison to using state-of-the-art\naverage or entropy based analysis.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 21:13:08 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Barbhuiya", "Sakil", ""], ["Papazachos", "Zafeirios", ""], ["Kilpatrick", "Peter", ""], ["Nikolopoulos", "Dimitrios S.", ""]]}, {"id": "1811.04570", "submitter": "Zhinan Cheng", "authors": "Zhinan Cheng, Qun Huang, Patrick P. C. Lee", "title": "On the Performance and Convergence of Distributed Stream Processing via\n  Approximate Fault Tolerance", "comments": "25 pages. Accepted by The VLDB Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault tolerance is critical for distributed stream processing systems, yet\nachieving error-free fault tolerance often incurs substantial performance\noverhead. We present AF-Stream, a distributed stream processing system that\naddresses the trade-off between performance and accuracy in fault tolerance.\nAF-Stream builds on a notion called approximate fault tolerance, whose idea is\nto mitigate backup overhead by adaptively issuing backups, while ensuring that\nthe errors upon failures are bounded with theoretical guarantees. Specifically,\nAF-Stream allows users to specify bounds on both the state divergence and the\nloss of non-backup streaming items. It issues state and item backups only when\nthe bounds are reached. Our AF-Stream design provides an extensible programming\nmodel for incorporating general streaming algorithms as well as exports only\nfew threshold parameters for configuring approximation fault tolerance.\nFurthermore, we formally prove that AF-Stream preserves high algorithm-specific\naccuracy of streaming algorithms, and in particular the convergence guarantees\nof online learning. Experiments show that AF-Stream maintains high performance\n(compared to no fault tolerance) and high accuracy after multiple failures\n(compared to no failures) under various streaming algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 06:04:43 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 07:34:44 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 08:24:05 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 09:39:45 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Cheng", "Zhinan", ""], ["Huang", "Qun", ""], ["Lee", "Patrick P. C.", ""]]}, {"id": "1811.04731", "submitter": "Shengwei Chen", "authors": "Shengwei Chen, Yanyan Shen, and Yanmin Zhu", "title": "Modeling Conceptual Characteristics of Virtual Machines for CPU\n  Utilization Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud services have grown rapidly in recent years, which provide high\nflexibility for cloud users to fulfill their computing requirements on demand.\nTo wisely allocate computing resources in the cloud, it is inevitably important\nfor cloud service providers to be aware of the potential utilization of various\nresources in the future. This paper focuses on predicting CPU utilization of\nvirtual machines (VMs) in the cloud. We conduct empirical analysis on Microsoft\nAzure's VM workloads and identify important conceptual characteristics of CPU\nutilization among VMs, including locality, periodicity and tendency. We propose\na neural network method, named Time-aware Residual Networks (T-ResNet), to\nmodel the observed conceptual characteristics with expanded network depth for\nCPU utilization prediction. We conduct extensive experiments to evaluate the\neffectiveness of our proposed method and the results show that T-ResNet\nconsistently outperforms baseline approaches in various metrics including RMSE,\nMAE and MAPE.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:55:15 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chen", "Shengwei", ""], ["Shen", "Yanyan", ""], ["Zhu", "Yanmin", ""]]}, {"id": "1811.04740", "submitter": "Jay Lofstead", "authors": "Jay Lofstead, Joshua Baker, Andrew Younge", "title": "Data Pallets: Containerizing Storage For Reproducibility and\n  Traceability", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": "SAND2018-12861 J", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trusting simulation output is crucial for Sandia's mission objectives. We\nrely on these simulations to perform our high-consequence mission tasks given\nnational treaty obligations. Other science and modeling applications, while\nthey may have high-consequence results, still require the strongest levels of\ntrust to enable using the result as the foundation for both practical\napplications and future research. To this end, the computing community has\ndeveloped workflow and provenance systems to aid in both automating simulation\nand modeling execution as well as determining exactly how was some output was\ncreated so that conclusions can be drawn from the data.\n  Current approaches for workflows and provenance systems are all at the user\nlevel and have little to no system level support making them fragile, difficult\nto use, and incomplete solutions. The introduction of container technology is a\nfirst step towards encapsulating and tracking artifacts used in creating data\nand resulting insights, but their current implementation is focused solely on\nmaking it easy to deploy an application in an isolated \"sandbox\" and\nmaintaining a strictly read-only mode to avoid any potential changes to the\napplication. All storage activities are still using the system-level shared\nstorage.\n  This project explores extending the container concept to include storage as a\nnew container type we call \\emph{data pallets}. Data Pallets are potentially\nwriteable, auto generated by the system based on IO activities, and usable as a\nway to link the contained data back to the application and input deck used to\ncreate it.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:20:40 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Lofstead", "Jay", ""], ["Baker", "Joshua", ""], ["Younge", "Andrew", ""]]}, {"id": "1811.04749", "submitter": "Andrew Kennings", "authors": "Andrew Kennings", "title": "Simple FPGA routing graph compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern FPGAs continue to increase in capacity which requires more memory to\nrun the CAD flow. The routing resource graph, which is needed by the detailed\nrouter, is a memory hungry data structure which describes all of the physical\nresources and programmable connections within an FPGA. We propose a compression\nscheme to reduce the memory requirements of the routing resource graph. The\nscheme is simple to apply and requires only trivial changes to the FPGA\ndetailed routing algorithm. The approach does not require any assumptions about\nthe FPGA routing architecture. Numerical results show excellent compression (as\nmuch as 3.6X overall memory reduction) with only a slight increase (~20% on\naverage) on the router runtime as a consequence of the routing graph\ncompression.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 14:47:45 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kennings", "Andrew", ""]]}, {"id": "1811.04875", "submitter": "Junhao Li", "authors": "Junhao Li", "title": "Comparing Spark vs MPI/OpenMP On Word Count MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spark provides an in-memory implementation of MapReduce that is widely used\nin the big data industry. MPI/OpenMP is a popular framework for high\nperformance parallel computing. This paper presents a high performance\nMapReduce design in MPI/OpenMP and uses that to compare with Spark on the\nclassic word count MapReduce task. My result shows that the MPI/OpenMP\nMapReduce outperforms Apache Spark by about 300%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 17:43:53 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 20:52:11 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Li", "Junhao", ""]]}, {"id": "1811.05007", "submitter": "Yvonne Anne Pignolet", "authors": "David Kozhaya, Ognjen Maric, Yvonne-Anne Pignolet", "title": "You Only Live Multiple Times: A Blackbox Solution for Reusing Crash-Stop\n  Algorithms In Realistic Crash-Recovery Settings", "comments": "Published at OPODIS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed agreement-based algorithms are often specified in a crash-stop\nasynchronous model augmented by Chandra and Toueg's unreliable failure\ndetectors. In such models, correct nodes stay up forever, incorrect nodes\neventually crash and remain down forever, and failure detectors behave\ncorrectly forever eventually, However, in reality, nodes as well as\ncommunication links both crash and recover without deterministic guarantees to\nremain in some state forever.\n  In this paper, we capture this realistic temporary and probabilitic behaviour\nin a simple new system model. Moreover, we identify a large algorithm class for\nwhich we devis a property-preserving transformation. Using this transformation,\nmany algorithms written for the asynchronous crash-stop model run correctly and\nunchanged in real systems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 21:20:04 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kozhaya", "David", ""], ["Maric", "Ognjen", ""], ["Pignolet", "Yvonne-Anne", ""]]}, {"id": "1811.05077", "submitter": "Victor Eijkhout", "authors": "Victor Eijkhout", "title": "Task Graph Transformations for Latency Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Integrative Model for Parallelism (IMP) derives a task graph from a\nhigher level description of parallel algorithms. In this note we show how task\ngraph transformations can be used to achieve latency tolerance in the program\nexecution. We give a formal derivation of the graph transformation, and show\nthrough simulation how latency tolerant algorithms can be faster than the naive\nexecution in a strong scaling scenario.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:52:44 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Eijkhout", "Victor", ""]]}, {"id": "1811.05087", "submitter": "Yixin Tao", "authors": "Yun Kuen Cheung, Richard Cole and Yixin Tao", "title": "Parallel Stochastic Asynchronous Coordinate Descent: Tight Bounds on the\n  Possible Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several works have shown linear speedup is achieved by an asynchronous\nparallel implementation of stochastic coordinate descent so long as there is\nnot too much parallelism. More specifically, it is known that if all updates\nare of similar duration, then linear speedup is possible with up to\n$\\Theta(\\sqrt n L_{\\max}/L_{\\overline{\\mathrm{res}}})$ processors, where\n$L_{\\max}$ and $L_{\\overline{\\mathrm{res}}}$ are suitable Lipschitz parameters.\nThis paper shows the bound is tight for almost all possible values of these\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 03:30:14 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 07:59:57 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 19:05:07 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Cheung", "Yun Kuen", ""], ["Cole", "Richard", ""], ["Tao", "Yixin", ""]]}, {"id": "1811.05213", "submitter": "Guoping Long", "authors": "Guoping Long and Jun Yang and Kai Zhu and Wei Lin", "title": "FusionStitching: Deep Fusion and Code Generation for Tensorflow\n  Computations on GPUs", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is a surge on machine learning applications in\nindustry. Many of them are based on popular AI frameworks like Tensorflow,\nTorch, Caffe, or MxNet, etc, and are enpowered by accelerator platforms such as\nGPUs. One important challenge of running Tensorflow computations on GPUs is the\nfine granularity problem, namely, FLOPS of individual ops are far from enough\nto fully exploit the computing power of underlying accelerators. The XLA\nframework provides a solid foundation to explore this problem further. In this\npaper, we propose FusionStitching, a novel, comprehensive Op fusion and code\ngeneration system to stitch computations into large GPU kernels. Experimental\nresults on four public models and two of our large inhouse applications show\nanother 55% (geometric mean) reduction of GPU kernel launches, compared to the\nXLA fusion baseline. This increases the E2E performance of both of our latency\ncritical inhouse applications up to 20%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 11:03:16 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Long", "Guoping", ""], ["Yang", "Jun", ""], ["Zhu", "Kai", ""], ["Lin", "Wei", ""]]}, {"id": "1811.05407", "submitter": "Fernando Koch", "authors": "Kleber Vieira, Fernando Koch, Joao Bosco Mangueira Sobral, Carlos\n  Becker Westphall, and Jorge Lopes de Souza Leao", "title": "Autonomic Intrusion Response in Distributed Computing using Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for Intrusion Detection based on the classification,\nunderstanding and prediction of behavioural deviance and potential threats,\nissuing recommendations, and acting to address eminent issues. Our work seeks a\npractical solutions to automate the process of identification and response to\nCybersecurity threats in hybrid Distributed Computing environments through the\nanalysis of large datasets generated during operations. We are motivated by the\ngrowth in utilisation of Cloud Computing and Edge Computing as the technology\nfor business and social solutions. The technology mix and complex operation\nrender these environments target to attacks like hijacking, man-in-the-middle,\ndenial of service, phishing, and others. The Autonomous Intrusion Response\nSystem implements innovative models of data analysis and context-aware\nrecommendation systems to respond to attacks and self-healing. We introduce a\nproof-of-concept implementation and evaluate against datasets from\nexperimentation scenarios based on public and private clouds. The results\npresent significant improvement in response effectiveness and potential to\nscale to large environments.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 16:58:48 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Vieira", "Kleber", ""], ["Koch", "Fernando", ""], ["Sobral", "Joao Bosco Mangueira", ""], ["Westphall", "Carlos Becker", ""], ["Leao", "Jorge Lopes de Souza", ""]]}, {"id": "1811.05704", "submitter": "Denis Demidov", "authors": "Denis Demidov", "title": "AMGCL: an Efficient, Flexible, and Extensible Algebraic Multigrid\n  Implementation", "comments": null, "journal-ref": null, "doi": "10.1134/S1995080219050056", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents AMGCL -- an opensource C++ library implementing the\nalgebraic multigrid method (AMG) for solution of large sparse linear systems of\nequations, usually arising from discretization of partial differential\nequations on an unstructured grid. The library supports both shared and\ndistributed memory computation, allows to utilize modern massively parallel\nprocessors via OpenMP, OpenCL, or CUDA technologies, has minimal dependencies,\nand is easily extensible. The design principles behind AMGCL are discussed and\nit is shown that the code performance is on par with alternative\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:56:30 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Demidov", "Denis", ""]]}, {"id": "1811.05941", "submitter": "Bingqing Shen", "authors": "Bingqing Shen and Jingzhi Guo", "title": "Virtual Net: a Decentralized Architecture for Interaction in Mobile\n  Virtual Worlds", "comments": null, "journal-ref": "Wireless Communications and Mobile Computing, Volume 2018, Article\n  ID 9749187, 24 pages", "doi": "10.1155/2018/9749187", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of mobile technology, mobile virtual worlds have\nattracted massive users. To improve scalability, a peer-to-peer virtual world\nprovides the solution to accommodate more users without increasing hardware\ninvestment. In mobile settings, however, existing P2P solutions are not\napplicable due to the unreliability of mobile devices and the instability of\nmobile networks. To address the issue, a novel infrastructure model, called\nVirtual Net, is proposed to provide fault-tolerance in managing user content\nand object state. In this paper, the key problem, namely object state update,\nis resolved to maintain state consistency and high interaction responsiveness.\nThis work is important in implementing a scalable mobile virtual world.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:13:58 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Shen", "Bingqing", ""], ["Guo", "Jingzhi", ""]]}, {"id": "1811.05948", "submitter": "Anirban Das", "authors": "Anirban Das, Stacy Patterson, Mike P. Wittie", "title": "EdgeBench: Benchmarking Edge Computing Platforms", "comments": "6 pages, 5 figures, Fourth International Workshop on Serverless\n  Computing (WoSC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging trend of edge computing has led several cloud providers to\nrelease their own platforms for performing computation at the 'edge' of the\nnetwork. We compare two such platforms, Amazon AWS Greengrass and Microsoft\nAzure IoT Edge, using a new benchmark comprising a suite of performance\nmetrics. We also compare the performance of the edge frameworks to cloud-only\nimplementations available in their respective cloud ecosystems. Amazon AWS\nGreengrass and Azure IoT Edge use different underlying technologies, edge\nLambda functions vs. containers, and so we also elaborate on platform features\navailable to developers. Our study shows that both of these edge platforms\nprovide comparable performance, which nevertheless differs in important ways\nfor key types of workloads used in edge applications. Finally, we discuss\nseveral current issues and challenges we faced in deploying these platforms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:29:04 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Das", "Anirban", ""], ["Patterson", "Stacy", ""], ["Wittie", "Mike P.", ""]]}, {"id": "1811.06043", "submitter": "Martin Kong", "authors": "Martin Kong and Louis-No\\\"el Pouchet", "title": "A Performance Vocabulary for Affine Loop Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern polyhedral compilers excel at aggressively optimizing codes with\nstatic control parts, but the state-of-practice to find high-performance\npolyhedral transformations especially for different hardware targets still\nlargely involves auto-tuning. In this work we propose a novel polyhedral\nscheduling technique, with the aim to reduce the need for auto-tuning while\nallowing to build customizable and specific transformation strategies. We\ndesign constraints and objectives that model several crucial aspects of\nperformance such as stride optimization or the trade-off between parallelism\nand reuse, while taking into account important architectural features of the\ntarget machine. The developed set of objectives embody a Performance Vocabulary\nfor loop transformations. The goal is to use this vocabulary, consisting of\nperformance idioms, to construct transformation recipes adapted to a number of\nprogram classes. We evaluate our work using the PolyBench/C benchmark suite and\nexperimentally validate it against large optimization spaces generated with the\nPluto compiler on a 10-core Intel Core-i9 (Skylake-X). Our results show that we\ncan achieve comparable or superior performance to Pluto on the majority of\nbenchmarks, without implementing tiling in the source code nor using\nexperimental autotuning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:22:07 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:36:54 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Kong", "Martin", ""], ["Pouchet", "Louis-No\u00ebl", ""]]}, {"id": "1811.06099", "submitter": "Ron van der Meyden", "authors": "Ron van der Meyden", "title": "On the specification and verification of atomic swap smart contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain systems and smart contracts provide ways to securely implement\nmulti-party transactions without the use of trusted intermediaries, which\ncurrently underpin many commercial transactions. However, they do so by\ntransferring trust to computer systems, raising the question of whether code\ncan be trusted. Experience with high value losses resulting from incorrect code\nhas already shown that formal verification of smart contracts is likely to be\nbeneficial. This note investigates the specification and verification of a\nsimple form of multi-party transaction, atomic swaps. It is argued that logics\nwith the ability to express properties of strategies of players in a\nmulti-agent setting are conceptually useful for this purpose, although\nultimately, for our specific examples, the less expressive setting of temporal\nlogic suffices for verification of concrete implementations. This is\nillustrated through a number of examples of the use of a model checker to\nverify atomic swap smart contracts in on-chain and cross-chain settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:27:13 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["van der Meyden", "Ron", ""]]}, {"id": "1811.06279", "submitter": "Andrey Demichev", "authors": "Alexander Kryukov and Andrey Demichev", "title": "Decentralized Data Storages: Technologies of Construction", "comments": "19 pages, 1 figures, 87 references; in Russian", "journal-ref": "Programming and Computer Software, 2018, Vol. 44, No. 5, pp.\n  303-315", "doi": "10.1134/S0361768818050067", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a comparative overview of decentralized data storages of\nvarious types. It is shown that although they have a number of common\nproperties that are typical of all peer-to-peer (P2P) networks, the problems to\nbe solved and, accordingly, the technologies used to build different types of\nstorages differ significantly.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:27:43 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Kryukov", "Alexander", ""], ["Demichev", "Andrey", ""]]}, {"id": "1811.06383", "submitter": "Jeremy Ko", "authors": "Jeremy Ko", "title": "The Amortized Analysis of a Non-blocking Chromatic Tree", "comments": "arXiv admin note: text overlap with arXiv:1712.05406 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-blocking chromatic tree is a type of balanced binary search tree where\nmultiple processes can concurrently perform search and update operations. We\nprove that a certain implementation has amortized cost $O(\\dot{c} + \\log n)$\nfor each operation, where $\\dot{c}$ is the maximum number of concurrent\noperations during the execution and $n$ is the maximum number of keys in the\ntree during the operation. This amortized analysis presents new challenges\ncompared to existing analyses of other non-blocking data structures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 04:38:16 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ko", "Jeremy", ""]]}, {"id": "1811.06396", "submitter": "Shuheng Shen", "authors": "Shuheng Shen, Linli Xu, Jingchang Liu, Junliang Guo and Qing Ling", "title": "Asynchronous Stochastic Composition Optimization with Variance Reduction", "comments": "30 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composition optimization has drawn a lot of attention in a wide variety of\nmachine learning domains from risk management to reinforcement learning.\nExisting methods solving the composition optimization problem often work in a\nsequential and single-machine manner, which limits their applications in\nlarge-scale problems. To address this issue, this paper proposes two\nasynchronous parallel variance reduced stochastic compositional gradient\n(AsyVRSC) algorithms that are suitable to handle large-scale data sets. The two\nalgorithms are AsyVRSC-Shared for the shared-memory architecture and\nAsyVRSC-Distributed for the master-worker architecture. The embedded variance\nreduction techniques enable the algorithms to achieve linear convergence rates.\nFurthermore, AsyVRSC-Shared and AsyVRSC-Distributed enjoy provable linear\nspeedup, when the time delays are bounded by the data dimensionality or the\nsparsity ratio of the partial gradients, respectively. Extensive experiments\nare conducted to verify the effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 14:34:32 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Shen", "Shuheng", ""], ["Xu", "Linli", ""], ["Liu", "Jingchang", ""], ["Guo", "Junliang", ""], ["Ling", "Qing", ""]]}, {"id": "1811.06420", "submitter": "Andrzej Pelc", "authors": "Andrzej Pelc, Ram Narayan Yadav", "title": "Latecomers Help to Meet: Deterministic Anonymous Gathering in the Plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A team of anonymous mobile agents represented by points freely moving in the\nplane have to gather at a single point and stop. Agents start at different\npoints of the plane and at possibly different times chosen by the adversary.\nThey are equipped with compasses, a common unit of distance and clocks. They\nexecute the same deterministic algorithm and travel at speed 1. When agents are\nat distance at most $\\epsilon$, for some positive constant $\\epsilon$ unknown\nto them, they can exchange all information. Due to the anonymity of the agents\nand the symmetry of the plane, gathering is impossible, e.g., if agents start\nsimultaneously at distances larger than $\\epsilon$. However, if some agents\nstart with a delay with respect to others, gathering may become possible. In\nwhich situations such latecomers can enable gathering? To answer this question\nwe consider initial configurations formalized as sets of pairs $\\{(p_1,t_1),\n(p_2,t_2),\\dots , (p_n,t_n)\\}$, for $n\\geq 2$ where $p_i$ is the starting point\nof the $i$-th agent and $t_i$ is its starting time. An initial configuration is\ngatherable if agents starting at it can be gathered by some algorithm, even\ndedicated to this particular configuration. We characterize all gatherable\ninitial configurations. Is there a universal deterministic algorithm that can\ngather all gatherable configurations of a given size. It turns out that the\nanswer is no. We show that all gatherable configurations can be partitioned\ninto two sets: bad and good configurations. We show that bad gatherable\nconfigurations (even of size 2) cannot be gathered by a common gathering\nalgorithm, and we prove that there is a universal algorithm that gathers all\ngood configurations of a given size.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 15:09:31 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Pelc", "Andrzej", ""], ["Yadav", "Ram Narayan", ""]]}, {"id": "1811.06494", "submitter": "Shreyas Pai", "authors": "Tanmay Inamdar, Shreyas Pai, Sriram V. Pemmaraju", "title": "Large-Scale Distributed Algorithms for Facility Location with Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents fast, distributed, $O(1)$-approximation algorithms for\nmetric facility location problems with outliers in the Congested Clique model,\nMassively Parallel Computation (MPC) model, and in the $k$-machine model. The\npaper considers Robust Facility Location and Facility Location with Penalties,\ntwo versions of the facility location problem with outliers proposed by\nCharikar et al. (SODA 2001). The paper also considers two alternatives for\nspecifying the input: the input metric can be provided explicitly (as an $n\n\\times n$ matrix distributed among the machines) or implicitly as the shortest\npath metric of a given edge-weighted graph. The results in the paper are:\n  - Implicit metric: For both problems, $O(1)$-approximation algorithms running\nin $O(\\mbox{poly}(\\log n))$ rounds in the Congested Clique and the MPC model\nand $O(1)$-approximation algorithms running in $\\tilde{O}(n/k)$ rounds in the\n$k$-machine model.\n  - Explicit metric: For both problems, $O(1)$-approximation algorithms running\nin $O(\\log\\log\\log n)$ rounds in the Congested Clique and the MPC model and\n$O(1)$-approximation algorithms running in $\\tilde{O}(n/k)$ rounds in the\n$k$-machine model.\n  Our main contribution is to show the existence of Mettu-Plaxton-style\n$O(1)$-approximation algorithms for both Facility Location with outlier\nproblems. As shown in our previous work (Berns et al., ICALP 2012,\nBandyapadhyay et al., ICDCN 2018) Mettu-Plaxton style algorithms are more\neasily amenable to being implemented efficiently in distributed and large-scale\nmodels of computation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:40:30 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Inamdar", "Tanmay", ""], ["Pai", "Shreyas", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1811.06667", "submitter": "Wenbo Wang", "authors": "Zhengwei Ni and Wenbo Wang and Dong In Kim and Ping Wang and Dusit\n  Niyato", "title": "Evolutionary Game for Consensus Provision in Permissionless Blockchain\n  Networks with Shard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of decentralized consensus protocols, permissionless\nblockchains have been envisioned as a promising enabler for the general-purpose\ntransaction-driven, autonomous systems. However, most of the prevalent\nblockchain networks are built upon the consensus protocols under the\ncrypto-puzzle framework known as proof-of-work. Such protocols face the\ninherent problem of transaction-processing bottleneck, as the networks achieve\nthe decentralized consensus for transaction confirmation at the cost of very\nhigh latency. In this paper, we study the problem of consensus formation in a\nsystem of multiple throughput-scalable blockchains with sharded consensus.\nSpecifically, the protocol design of sharded consensus not only enables\nparallelizing the process of transaction validation with sub-groups of\nprocessors, but also introduces the Byzantine consensus protocols for\naccelerating the consensus processes. By allowing different blockchains to\nimpose different levels of processing fees and to have different\ntransaction-generating rate, we aim to simulate the multi-service provision\neco-systems based on blockchains in real world. We focus on the dynamics of\nblockchain-selection in the condition of a large population of consensus\nprocessors. Hence, we model the evolution of blockchain selection by the\nindividual processors as an evolutionary game. Both the theoretical and the\nnumerical analysis are provided regarding the evolutionary equilibria and the\nstability of the processors' strategies in a general case.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:25:08 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Ni", "Zhengwei", ""], ["Wang", "Wenbo", ""], ["Kim", "Dong In", ""], ["Wang", "Ping", ""], ["Niyato", "Dusit", ""]]}, {"id": "1811.06672", "submitter": "Haruna Isah", "authors": "Sazia Mahfuz, Haruna Isah, Farhana Zulkernine, Peter Nicholls", "title": "Detecting Irregular Patterns in IoT Streaming Data for Fall Detection", "comments": "7 pages", "journal-ref": null, "doi": "10.1109/IEMCON.2018.8614822", "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting patterns in real time streaming data has been an interesting and\nchallenging data analytics problem. With the proliferation of a variety of\nsensor devices, real-time analytics of data from the Internet of Things (IoT)\nto learn regular and irregular patterns has become an important machine\nlearning problem to enable predictive analytics for automated notification and\ndecision support. In this work, we address the problem of learning an irregular\nhuman activity pattern, fall, from streaming IoT data from wearable sensors. We\npresent a deep neural network model for detecting fall based on accelerometer\ndata giving 98.75 percent accuracy using an online physical activity monitoring\ndataset called \"MobiAct\", which was published by Vavoulas et al. The initial\nmodel was developed using IBM Watson studio and then later transferred and\ndeployed on IBM Cloud with the streaming analytics service supported by IBM\nStreams for monitoring real-time IoT data. We also present the systems\narchitecture of the real-time fall detection framework that we intend to use\nwith mbientlabs wearable health monitoring sensors for real time patient\nmonitoring at retirement homes or rehabilitation clinics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:59:22 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Mahfuz", "Sazia", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""], ["Nicholls", "Peter", ""]]}, {"id": "1811.06751", "submitter": "Zhiniang Peng", "authors": "Zhiniang Peng and Yuki Chen", "title": "All roads lead to Rome: Many ways to double spend your cryptocurrency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2008, Satoshi Nakamoto proposed an electronic cash system (bitcoin) that\nis completely realized by peer-to-peer technology. The core value of this\nscheme is that it proposes a solution based on Proof-of Work, so that the cash\nsystem can run in a peer-to-peer environment and be able to prevent\ndouble-spend attacks. Bitcoin has been developed for ten years, and since then\ncountless digital currencies have been created. But the discussion of\ndouble-spend attacks seems to still concentrate on 51% Attacks. In fact, our\nresearch has found that there are many other way to achieve double-spend\nattacks. In this paper, by introducing a number of double-spend attack\nvulnerabilities that we have found in EOS, NEO and other large blockchain\nplatforms, we summarized various reasons for causing double-spend attacks, and\npropose an efficient mitigation measure against them.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 11:02:56 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Peng", "Zhiniang", ""], ["Chen", "Yuki", ""]]}, {"id": "1811.06901", "submitter": "Rui Ren", "authors": "Rui Ren and Jinheng Li and Lei Wang and Jianfeng Zhan and Zheng Cao", "title": "Anomaly Analysis for Co-located Datacenter Workloads in the Alibaba\n  Cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In warehouse-scale cloud datacenters, co-locating online services and offline\nbatch jobs is an efficient approach to improving datacenter utilization. To\nbetter facilitate the understanding of interactions among the co-located\nworkloads and their real-world operational demands, Alibaba recently released a\ncluster usage and co-located workload dataset, which is the first publicly\ndataset with precise information about the category of each job. In this paper,\nwe perform a deep analysis on the released Alibaba workload dataset, from the\nperspective of anomaly analysis and diagnosis. Through data preprocessing, node\nsimilarity analysis based on Dynamic Time Warping (DTW), co-located workloads\ncharacteristics analysis and anomaly analysis based on iForest, we reveals\nseveral insights including: (1) The performance discrepancy of machines in\nAlibaba's production cluster is relatively large, for the distribution and\nresource utilization of co-located workloads is not balanced. For instance, the\nresource utilization (especially memory utilization) of batch jobs is\nfluctuating and not as stable as that of online containers, and the reason is\nthat online containers are long-running jobs with more memory-demanding and\nmost batch jobs are short jobs, (2) Based on the distribution of co-located\nworkload instance numbers, the machines can be classified into 8 workload\ndistribution categories1. And most patterns of machine resource utilization\ncurves are similar in the same workload distribution category. (3) In addition\nto the system failures, unreasonable scheduling and workload imbalance are the\nmain causes of anomalies in Alibaba's cluster.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 03:37:40 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Ren", "Rui", ""], ["Li", "Jinheng", ""], ["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""], ["Cao", "Zheng", ""]]}, {"id": "1811.06992", "submitter": "Chris Ying", "authors": "Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, Youlong Cheng", "title": "Image Classification at Supercomputer Scale", "comments": "Presented as part of Systems for ML Workshop @ NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning is extremely computationally intensive, and hardware vendors\nhave responded by building faster accelerators in large clusters. Training deep\nlearning models at petaFLOPS scale requires overcoming both algorithmic and\nsystems software challenges. In this paper, we discuss three systems-related\noptimizations: (1) distributed batch normalization to control per-replica batch\nsizes, (2) input pipeline optimizations to sustain model throughput, and (3)\n2-D torus all-reduce to speed up gradient summation. We combine these\noptimizations to train ResNet-50 on ImageNet to 76.3% accuracy in 2.2 minutes\non a 1024-chip TPU v3 Pod with a training throughput of over 1.05 million\nimages/second and no accuracy drop.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 19:01:40 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 01:30:42 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ying", "Chris", ""], ["Kumar", "Sameer", ""], ["Chen", "Dehao", ""], ["Wang", "Tao", ""], ["Cheng", "Youlong", ""]]}, {"id": "1811.07088", "submitter": "Ruisheng Shi", "authors": "Ruisheng Shi, Lina Lan, Peng Liu, Di Ao, Yueming Lu", "title": "Towards Scalable Subscription Aggregation and Real Time Event Matching\n  in a Large-Scale Content-Based Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many scalable event matching algorithms have been proposed to\nachieve scalability for large-scale content-based networks, content-based\npublish/subscribe networks (especially for large-scale real time systems) still\nsuffer performance deterioration when subscription scale increases. While\nsubscription aggregation techniques can be useful to reduce the amount of\nsubscription dissemination traffic and the subscription table size by\nexploiting the similarity among subscriptions, efficient subscription\naggregation is not a trivial task to accomplish. Previous research works have\nproved that it is either a NP-Complete or a co-NP complete problem. In this\npaper, we propose DLS (Discrete Label Set), a novel subscription representation\nmodel, and design algorithms to achieve the mapping from traditional Boolean\npredicate model to the DLS model. Based on the DLS model, we propose a\nsubscription aggregation algorithm with O(1) time complexity in most cases, and\nan event matching algorithm with O(1) time complexity. The significant\nperformance improvement is at the cost of memory consumption and controllable\nfalse positive rate. Our theoretical analysis shows that these algorithms are\ninherently scalable and can achieve real time event matching in a large-scale\ncontent-based publish/subscribe network. We discuss the tradeoff between\nmemory, false positive rate and partition granules of content space.\nExperimental results show that proposed algorithms achieve expected\nperformance. With the increasing of computer memory capacity and the dropping\nof memory price, more and more large-scale real time applications can benefit\nfrom our proposed DLS model, such as stock quote distribution, earthquake\nmonitoring, and severe weather alert.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 03:29:53 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 15:04:40 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Shi", "Ruisheng", ""], ["Lan", "Lina", ""], ["Liu", "Peng", ""], ["Ao", "Di", ""], ["Lu", "Yueming", ""]]}, {"id": "1811.07325", "submitter": "Chandan Misra", "authors": "Chandan Misra, Sourangshu Bhattacharya, Soumya K. Ghosh", "title": "Stark: Fast and Scalable Strassen's Matrix Multiplication using Apache\n  Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new fast, highly scalable distributed matrix\nmultiplication algorithm on Apache Spark, called Stark, based on Strassen's\nmatrix multiplication algorithm. Stark preserves Strassen's 7 multiplications\nscheme in a distributed environment and thus achieves faster execution. It is\nbased on two new ideas; it creates a recursion tree of computation where each\nlevel of such tree corresponds to division and combination of distributed\nmatrix blocks in the form of Resilient Distributed Datasets(RDDs); It processes\neach divide and combine step in parallel and memorize the sub-matrices by\nintelligently tagging matrix blocks in it. To the best of our knowledge, Stark\nis the first Strassen's implementation in Spark platform. We show\nexperimentally that Stark has a strong scalability with increasing matrix size\nenabling us to multiply two (16384 x 16384) matrices with 28% and 36% less wall\nclock time than Marlin and MLLib respectively, state-of-the-art matrix\nmultiplication approaches based on Spark.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 13:08:19 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 08:12:15 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Misra", "Chandan", ""], ["Bhattacharya", "Sourangshu", ""], ["Ghosh", "Soumya K.", ""]]}, {"id": "1811.07525", "submitter": "Po-Chun Kuo", "authors": "Tai-Yuan Chen and Wei-Ning Huang and Po-Chun Kuo and Hao Chung and\n  Tzu-Wei Chao", "title": "DEXON: A Highly Scalable, Decentralized DAG-Based Consensus Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blockchain system is a replicated state machine that must be fault\ntolerant. When designing a blockchain system, there is usually a trade-off\nbetween decentralization, scalability, and security. In this paper, we propose\na novel blockchain system, DEXON, which achieves high scalability while\nremaining decentralized and robust in the real-world environment. We have two\nmain contributions. First, we present a highly scalable sharding framework for\nblockchain. This framework takes an arbitrary number of single chains and\ntransforms them into the \\textit{blocklattice} data structure, enabling\n\\textit{high scalability} and \\textit{low transaction confirmation latency}\nwith asymptotically optimal communication overhead. Second, we propose a\nsingle-chain protocol based on our novel verifiable random function and a new\nByzantine agreement that achieves high decentralization and low latency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:59:50 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chen", "Tai-Yuan", ""], ["Huang", "Wei-Ning", ""], ["Kuo", "Po-Chun", ""], ["Chung", "Hao", ""], ["Chao", "Tzu-Wei", ""]]}, {"id": "1811.08047", "submitter": "Chunhui Guo", "authors": "Chunhui Guo, Hao Wu, Xiayu Hua, Shangping Ren, Jerzy Nogiec", "title": "Optimizing System Quality of Service through Rejuvenation for\n  Long-Running Applications with Real-Time Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability, longevity, availability, and deadline guarantees are the four\nmost important metrics to measure the QoS of long-running safety-critical\nreal-time applications. Software aging is one of the major factors that impact\nthe safety of long-running real-time applications as the degraded performance\nand increased failure rate caused by software aging can lead to deadline\nmissing and catastrophic consequences. Software rejuvenation is one of the most\ncommonly used approaches to handle issues caused by software aging. In this\npaper, we study the optimal time when software rejuvenation shall take place so\nthat the system's reliability, longevity, and availability are maximized, and\napplication delays caused by software rejuvenation is minimized. In particular,\nwe formally analyze the relationships between software rejuvenation frequency\nand system reliability, longevity, and availability. Based on the theoretic\nanalysis, we develop approaches to maximizing system reliability, longevity,\nand availability, and use simulation to evaluate the developed approaches. In\naddition, we design the MIN-DELAY semi-priority-driven scheduling algorithm to\nminimize application delays caused by rejuvenation processes. The simulation\nexperiments show that the developed semi-priority-driven scheduling algorithm\nreduces application delays by 9.01% and 14.24% over the earliest deadline first\n(EDF) and least release time (LRT) scheduling algorithms, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 02:51:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Guo", "Chunhui", ""], ["Wu", "Hao", ""], ["Hua", "Xiayu", ""], ["Ren", "Shangping", ""], ["Nogiec", "Jerzy", ""]]}, {"id": "1811.08057", "submitter": "Xiaomeng Dong", "authors": "Xiaomeng Dong, EN Barnett, Sudarshan K.Dhall", "title": "Parallel Matrix Condensation for Calculating Log-Determinant of Large\n  Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculating the log-determinant of a matrix is useful for statistical\ncomputations used in machine learning, such as generative learning which uses\nthe log-determinant of the covariance matrix to calculate the log-likelihood of\nmodel mixtures. The log-determinant calculation becomes challenging as the\nnumber of variables becomes large. Therefore, finding a practical speedup for\nthis computation can be useful. In this study, we present a parallel matrix\ncondensation algorithm for calculating the log-determinant of a large matrix.\nWe demonstrate that in a distributed environment, Parallel Matrix Condensation\nhas several advantages over the well-known Parallel Gaussian Elimination. The\nadvantages include high data distribution efficiency and less data\ncommunication operations. We test our Parallel Matrix Condensation against\nself-implemented Parallel Gaussian Elimination as well as ScaLAPACK (Scalable\nLinear Algebra Package) on 1000 x1000 to 8000x8000 for 1,2,4,8,16,32,64 and 128\nprocessors. The results show that Matrix Condensation yields the best speed-up\namong all other tested algorithms. The code is available on\nhttps://github.com/vbvg2008/MatrixCondensation\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 03:52:32 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Dong", "Xiaomeng", ""], ["Barnett", "EN", ""], ["Dhall", "Sudarshan K.", ""]]}, {"id": "1811.08197", "submitter": "Fabian Reiter", "authors": "Benedikt Bollig, Patricia Bouyer, and Fabian Reiter", "title": "Identifiers in Registers - Describing Network Algorithms with Logic", "comments": "17 pages (+ 17 pages of appendices), 1 figure (+ 1 figure in the\n  appendix)", "journal-ref": null, "doi": "10.1007/978-3-030-17127-8_7", "report-no": null, "categories": "cs.FL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formal model of distributed computing based on register automata\nthat captures a broad class of synchronous network algorithms. The local memory\nof each process is represented by a finite-state controller and a fixed number\nof registers, each of which can store the unique identifier of some process in\nthe network. To underline the naturalness of our model, we show that it has the\nsame expressive power as a certain extension of first-order logic on graphs\nwhose nodes are equipped with a total order. Said extension lets us define new\nfunctions on the set of nodes by means of a so-called partial fixpoint\noperator. In spirit, our result bears close resemblance to a classical theorem\nof descriptive complexity theory that characterizes the complexity class PSPACE\nin terms of partial fixpoint logic (a proper superclass of the logic we\nconsider here).\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:56:53 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Bollig", "Benedikt", ""], ["Bouyer", "Patricia", ""], ["Reiter", "Fabian", ""]]}, {"id": "1811.08282", "submitter": "Kyle Niemeyer", "authors": "Daniel J. Magee, Anthony S. Walker, and Kyle E. Niemeyer", "title": "Applying the swept rule for solving explicit partial differential\n  equations on heterogeneous computing systems", "comments": "24 pages, 9 figures. Accepted for publication by the Journal of\n  Supercomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications that exploit the architectural details of high-performance\ncomputing (HPC) systems have become increasingly invaluable in academia and\nindustry over the past two decades. The most important hardware development of\nthe last decade in HPC has been the General Purpose Graphics Processing Unit\n(GPGPU), a class of massively parallel devices that now contributes the\nmajority of computational power in the top 500 supercomputers. As these systems\ngrow, small costs such as latency---due to the fixed cost of memory accesses\nand communication---accumulate in a large simulation and become a significant\nbarrier to performance. The swept time-space decomposition rule is a\ncommunication-avoiding technique for time-stepping stencil update formulas that\nattempts to reduce latency costs. This work extends the swept rule by targeting\nheterogeneous, CPU/GPU architectures representing current and future HPC\nsystems. We compare our approach to a naive decomposition scheme with two test\nequations using an MPI+CUDA pattern on 40 processes over two nodes containing\none GPU. The swept rule produces a factor of 1.9 to 23 speedup for the heat\nequation and a factor of 1.1 to 2.0 speedup for the Euler equations, using the\nsame processors and work distribution, and with the best possible\nconfigurations. These results show the potential effectiveness of the swept\nrule for different equations and numerical schemes on massively parallel\ncomputing systems that incur substantial latency costs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:22:04 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 18:43:27 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Magee", "Daniel J.", ""], ["Walker", "Anthony S.", ""], ["Niemeyer", "Kyle E.", ""]]}, {"id": "1811.08355", "submitter": "Mirsad Cosovic", "authors": "Mirsad Cosovic", "title": "Design and Analysis of Distributed State Estimation Algorithms Based on\n  Belief Propagation and Applications in Smart Grids", "comments": "A PhD dissertation submitted to the Department of Power, Electronics\n  and Communication Engineering, Faculty of Technical Sciences, University of\n  Novi Sad, Serbia (147 pages, 42 figures, 4 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a detailed study on application of factor graphs and the belief\npropagation (BP) algorithm to the power system state estimation (SE) problem.\nWe start from the BP solution for the linear DC model, for which we provide a\ndetailed convergence analysis. Using BP-based DC model we propose a fast\nreal-time state estimator for the power system SE. The proposed estimator is\neasy to distribute and parallelize, thus alleviating computational limitations\nand allowing for processing measurements in real time. The presented algorithm\nmay run as a continuous process.\n  Using insights from the DC model, we use two different approaches to derive\nthe BP algorithm for the non-linear model. The first method directly applies BP\nmethodology, however, providing only approximate BP solution for the non-linear\nmodel. In the second approach, we make a key further step by providing the\nsolution in which the BP is applied sequentially over the non-linear model,\nakin to what is done by the Gauss-Newton method. The resulting iterative\nGauss-Newton belief propagation (GN-BP) algorithm can be interpreted as a\ndistributed Gauss-Newton method with the same accuracy as the centralized SE.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:38:14 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Cosovic", "Mirsad", ""]]}, {"id": "1811.08535", "submitter": "Muhammad Samir Khan", "authors": "Syed Shalan Naqvi, Muhammad Samir Khan and Nitin H. Vaidya", "title": "Exact Byzantine Consensus Under Local-Broadcast Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of achieving exact Byzantine consensus in a\nsynchronous system under a local-broadcast communication model. The nodes\ncommunicate with each other via message-passing. The communication network is\nmodeled as an undirected graph, with each vertex representing a node in the\nsystem. Under the local-broadcast communication model, when any node transmits\na message, all its neighbors in the communication graph receive the message\nreliably. This communication model is motivated by wireless networks. In this\nwork, we present necessary and sufficient conditions on the underlying\ncommunication graph to achieve exact Byzantine consensus under the\nlocal-broadcast communication model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 00:36:08 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Naqvi", "Syed Shalan", ""], ["Khan", "Muhammad Samir", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1811.08596", "submitter": "Linnan Wang", "authors": "Linnan Wang, Wei Wu, Junyu Zhang, Hang Liu, George Bosilca, Maurice\n  Herlihy, Rodrigo Fonseca", "title": "SuperNeurons: FFT-based Gradient Sparsification in the Distributed\n  Training of Deep Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance and efficiency of distributed training of Deep Neural\nNetworks highly depend on the performance of gradient averaging among all\nparticipating nodes, which is bounded by the communication between nodes. There\nare two major strategies to reduce communication overhead: one is to hide\ncommunication by overlapping it with computation, and the other is to reduce\nmessage sizes. The first solution works well for linear neural architectures,\nbut latest networks such as ResNet and Inception offer limited opportunity for\nthis overlapping. Therefore, researchers have paid more attention to minimizing\ncommunication. In this paper, we present a novel gradient compression framework\nderived from insights of real gradient distributions, and which strikes a\nbalance between compression ratio, accuracy, and computational overhead. Our\nframework has two major novel components: sparsification of gradients in the\nfrequency domain, and a range-based floating point representation to quantize\nand further compress gradients frequencies. Both components are dynamic, with\ntunable parameters that achieve different compression ratio based on the\naccuracy requirement and systems' platforms, and achieve very high throughput\non GPUs. We prove that our techniques guarantee the convergence with a\ndiminishing compression ratio. Our experiments show that the proposed\ncompression framework effectively improves the scalability of most popular\nneural networks on a 32 GPU cluster to the baseline of no compression, without\ncompromising the accuracy and convergence speed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 04:27:54 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 03:08:11 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wang", "Linnan", ""], ["Wu", "Wei", ""], ["Zhang", "Junyu", ""], ["Liu", "Hang", ""], ["Bosilca", "George", ""], ["Herlihy", "Maurice", ""], ["Fonseca", "Rodrigo", ""]]}, {"id": "1811.08614", "submitter": "Jiajun Xu", "authors": "Jiajun Xu, Sam Huang", "title": "Tetris", "comments": "22 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:cs/0006009 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tetris is an Asynchronous Byzantine Fault Tolerance consensus algorithm\ndesigned for next generation high-throughput permission and permissionless\nblockchain. The core concept of Tetris is derived from Reasoning About\nKnowledge, which we believe to be the most appropriate tools for revealing and\nanalyzing the fundamental complexity of distributed systems. By analyzing the\nstates of knowledge that each participant attained in an unreliable system, we\ncan capture some of the basis underlying structure of the system, then help us\ndesigning effective & efficient protocols. Plus the adoption of Full\nInformation Protocol (FIP) with the optimized message traffic model, Tetris has\nfinally got high performance, with proved safety. Tetris achieve consensus\nfinality in seconds, means transactions can be confirmed greatly faster than\nother scheme like Pow/Dpos. Tetris also achieve fairness, which is critically\nimportant in some areas such as stock market etc.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 07:23:58 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 06:32:48 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Xu", "Jiajun", ""], ["Huang", "Sam", ""]]}, {"id": "1811.08801", "submitter": "Yohsuke Murase", "authors": "Yohsuke Murase, Hiroyasu Matsushima, Itsuki Noda, Tomio Kamada", "title": "CARAVAN: a framework for comprehensive simulations on massive parallel\n  machines", "comments": "14 pages, 5 figures, to appear in a Springer LNCS/LNAI proceedings\n  series", "journal-ref": "In: Lin D., Ishida T., Zambonelli F., Noda I. (eds) Massively\n  Multi-Agent Systems II. MMAS 2018. Lecture Notes in Computer Science, vol\n  11422. Springer, Cham", "doi": "10.1007/978-3-030-20937-7_9", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a software framework called CARAVAN, which was developed for\ncomprehensive simulations on massive parallel computers. The framework runs\nuser-developed simulators with various input parameters in parallel without\nrequiring the knowledge of parallel programming. The framework is useful for\nexploring high-dimensional parameter spaces, for which sampling points must be\ndynamically determined based on the previous results. Possible use cases\ninclude optimization, data assimilation, and Markov-chain Monte Carlo sampling\nin parameter spaces. As a demonstration, we applied CARAVAN to an evacuation\nplanning problem in an urban area. We formulated the problem as a\nmulti-objective optimization problem, and searched for solutions using\nmulti-agent simulations and a multi-objective evolutionary algorithm, which\nwere developed as modules of the framework.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 05:31:03 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 01:27:14 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Murase", "Yohsuke", ""], ["Matsushima", "Hiroyasu", ""], ["Noda", "Itsuki", ""], ["Kamada", "Tomio", ""]]}, {"id": "1811.08834", "submitter": "Shanjiang Tang", "authors": "Shanjiang Tang, Bingsheng He, Ce Yu, Yusen Li, Kun Li", "title": "A Survey on Spark Ecosystem for Big Data Processing", "comments": "21 pages, 11 figures, technique report. in IEEE Transactions on\n  Knowledge and Data Engineering (2020). arXiv admin note: text overlap with\n  arXiv:1302.2966 by other authors", "journal-ref": null, "doi": "10.1109/TKDE.2020.2975652", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosive increase of big data in industry and academic fields, it\nis necessary to apply large-scale data processing systems to analysis Big Data.\nArguably, Spark is state of the art in large-scale data computing systems\nnowadays, due to its good properties including generality, fault tolerance,\nhigh performance of in-memory data processing, and scalability. Spark adopts a\nflexible Resident Distributed Dataset (RDD) programming model with a set of\nprovided transformation and action operators whose operating functions can be\ncustomized by users according to their applications. It is originally\npositioned as a fast and general data processing system. A large body of\nresearch efforts have been made to make it more efficient (faster) and general\nby considering various circumstances since its introduction. In this survey, we\naim to have a thorough review of various kinds of optimization techniques on\nthe generality and performance improvement of Spark. We introduce Spark\nprogramming model and computing system, discuss the pros and cons of Spark, and\nhave an investigation and classification of various solving techniques in the\nliterature. Moreover, we also introduce various data management and processing\nsystems, machine learning algorithms and applications supported by Spark.\nFinally, we make a discussion on the open issues and challenges for large-scale\nin-memory data processing with Spark.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 14:40:37 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Tang", "Shanjiang", ""], ["He", "Bingsheng", ""], ["Yu", "Ce", ""], ["Li", "Yusen", ""], ["Li", "Kun", ""]]}, {"id": "1811.08932", "submitter": "Alberto Marchisio", "authors": "Alberto Marchisio, Muhammad Abdullah Hanif, and Muhammad Shafique", "title": "CapsAcc: An Efficient Hardware Accelerator for CapsuleNets with Data\n  Reuse", "comments": "Accepted for publication at Design, Automation and Test in Europe\n  (DATE 2019). Florence, Italy", "journal-ref": null, "doi": "10.23919/DATE.2019.8714922", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been widely deployed for many Machine\nLearning applications. Recently, CapsuleNets have overtaken traditional DNNs,\nbecause of their improved generalization ability due to the multi-dimensional\ncapsules, in contrast to the single-dimensional neurons. Consequently,\nCapsuleNets also require extremely intense matrix computations, making it a\ngigantic challenge to achieve high performance. In this paper, we propose\nCapsAcc, the first specialized CMOS-based hardware architecture to perform\nCapsuleNets inference with high performance and energy efficiency.\nState-of-the-art convolutional DNN accelerators would not work efficiently for\nCapsuleNets, as their designs do not account for key operations involved in\nCapsuleNets, like squashing and dynamic routing, as well as multi-dimensional\nmatrix processing. Our CapsAcc architecture targets this problem and achieves\nsignificant improvements, when compared to an optimized GPU implementation. Our\narchitecture exploits the massive parallelism by flexibly feeding the data to a\nspecialized systolic array according to the operations required in different\nlayers. It also avoids extensive load and store operations on the on-chip\nmemory, by reusing the data when possible. We further optimize the routing\nalgorithm to reduce the computations needed at this stage. We synthesized the\ncomplete CapsAcc architecture in a 32nm CMOS technology using Synopsys design\ntools, and evaluated it for the MNIST benchmark (as also done by the original\nCapsuleNet paper) to ensure consistent and fair comparisons. This work enables\nhighly-efficient CapsuleNets inference on embedded platforms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:56:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Marchisio", "Alberto", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.08933", "submitter": "Deval Shah", "authors": "Jonathan Lew, Deval Shah, Suchita Pati, Shaylin Cattell, Mengchi\n  Zhang, Amruth Sandhupatla, Christopher Ng, Negar Goli, Matthew D. Sinclair,\n  Timothy G. Rogers, Tor Aamodt", "title": "Analyzing Machine Learning Workloads Using a Detailed GPU Simulator", "comments": "Source code available at:\n  https://github.com/gpgpu-sim/gpgpu-sim_distribution/tree/dev", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep neural networks deployed today are trained using GPUs via\nhigh-level frameworks such as TensorFlow and PyTorch. This paper describes\nchanges we made to the GPGPU-Sim simulator to enable it to run PyTorch by\nrunning PTX kernels included in NVIDIA's cuDNN library. We use the resulting\nmodified simulator, which has been made available publicly with this paper, to\nstudy some simple deep learning workloads. With our changes to GPGPU-Sim's\nfunctional simulation model, we find GPGPU-Sim performance model running a\ncuDNN enabled implementation of LeNet for MNIST reports results within 30% of\nreal hardware. Using GPGPU-Sim's AerialVision performance analysis tool we\nobserve that cuDNN API calls contain many varying phases and appear to include\npotentially inefficient microarchitecture behaviour such as DRAM partition bank\ncamping, at least when executed on GPGPU-Sim's current performance model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 07:52:34 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 23:24:48 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Lew", "Jonathan", ""], ["Shah", "Deval", ""], ["Pati", "Suchita", ""], ["Cattell", "Shaylin", ""], ["Zhang", "Mengchi", ""], ["Sandhupatla", "Amruth", ""], ["Ng", "Christopher", ""], ["Goli", "Negar", ""], ["Sinclair", "Matthew D.", ""], ["Rogers", "Timothy G.", ""], ["Aamodt", "Tor", ""]]}, {"id": "1811.09018", "submitter": "Nasser Ghadiri", "authors": "Erfan Farhangi Maleki, Nasser Ghadiri, Maryam Lotfi Shahreza, Zeinab\n  Maleki", "title": "DHLP 1&2: Giraph based distributed label propagation algorithms on\n  heterogeneous drug-related networks", "comments": "Source code available for Apache Giraph on Hadoop", "journal-ref": "Expert Systems with Applications, 2020,113640", "doi": "10.1016/j.eswa.2020.113640", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background and Objective: Heterogeneous complex networks are large graphs\nconsisting of different types of nodes and edges. The knowledge extraction from\nthese networks is complicated. Moreover, the scale of these networks is\nsteadily increasing. Thus, scalable methods are required. Methods: In this\npaper, two distributed label propagation algorithms for heterogeneous networks,\nnamely DHLP-1 and DHLP-2 have been introduced. Biological networks are one type\nof the heterogeneous complex networks. As a case study, we have measured the\nefficiency of our proposed DHLP-1 and DHLP-2 algorithms on a biological network\nconsisting of drugs, diseases, and targets. The subject we have studied in this\nnetwork is drug repositioning but our algorithms can be used as general methods\nfor heterogeneous networks other than the biological network. Results: We\ncompared the proposed algorithms with similar non-distributed versions of them\nnamely MINProp and Heter-LP. The experiments revealed the good performance of\nthe algorithms in terms of running time and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:10:09 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 15:13:33 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Maleki", "Erfan Farhangi", ""], ["Ghadiri", "Nasser", ""], ["Shahreza", "Maryam Lotfi", ""], ["Maleki", "Zeinab", ""]]}, {"id": "1811.09047", "submitter": "Ranesh Kumar Naha", "authors": "Ranesh Kumar Naha, Saurabh Garg and Andrew Chan", "title": "Fog Computing Architecture: Survey and Challenges", "comments": "25 pages, 3 Figures", "journal-ref": null, "doi": "10.1049/PBPC025E_ch10", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging technologies that generate a huge amount of data such as the\nInternet of Things (IoT) services need latency aware computing platforms to\nsupport time-critical applications. Due to the on-demand services and\nscalability features of cloud computing, Big Data application processing is\ndone in the cloud infrastructure. Managing Big Data applications exclusively in\nthe cloud is not an efficient solution for latency-sensitive applications\nrelated to smart transportation systems, healthcare solutions, emergency\nresponse systems and content delivery applications. Thus, the Fog computing\nparadigm that allows applications to perform computing operations in-between\nthe cloud and the end devices has emerged. In Fog architecture, IoT devices and\nsensors are connected to the Fog devices which are located in close proximity\nto the users and it is also responsible for intermediate computation and\nstorage. Most computations will be done on the edge by eliminating full\ndependencies on the cloud resources. In this chapter, we investigate and survey\nFog computing architectures which have been proposed over the past few years.\nMoreover, we study the requirements of IoT applications and platforms, and the\nlimitations faced by cloud systems when executing IoT applications. Finally, we\nreview current research works that particularly focus on Big Data application\nexecution on Fog and address several open challenges as well as future research\ndirections.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 07:41:16 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Naha", "Ranesh Kumar", ""], ["Garg", "Saurabh", ""], ["Chan", "Andrew", ""]]}, {"id": "1811.09143", "submitter": "Simon Doherty", "authors": "Simon Doherty, Brijesh Dongol, Heike Wehrheim, John Derrick", "title": "Verifying C11 Programs Operationally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an operational semantics for a release-acquire fragment\nof the C11 memory model with relaxed accesses. We show that the semantics is\nboth sound and complete with respect to the axiomatic model. The semantics\nrelies on a per-thread notion of observability, which allows one to reason\nabout a weak memory C11 program in program order. On top of this, we develop a\nproof calculus for invariant-based reasoning, which we use to verify the\nrelease-acquire version of Peterson's mutual exclusion algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:47:18 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Doherty", "Simon", ""], ["Dongol", "Brijesh", ""], ["Wehrheim", "Heike", ""], ["Derrick", "John", ""]]}, {"id": "1811.09248", "submitter": "Martin Koehler", "authors": "Martin Koehler and Alex Bogatu and Cristina Civili and Nikolaos\n  Konstantinou and Edward Abel and Alvaro A. A. Fernandes and John Keane and\n  Leonid Libkin and Norman W. Paton", "title": "Data Context Informed Data Wrangling", "comments": null, "journal-ref": "2017 IEEE International Conference on Big Data (Big Data), pp.\n  956-963, Boston, MA, 11-14 December, 2017", "doi": "10.1109/BigData.2017.8258015", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of preparing potentially large and complex data sets for further\nanalysis or manual examination is often called data wrangling. In classical\nwarehousing environments, the steps in such a process have been carried out\nusing Extract-Transform-Load platforms, with significant manual involvement in\nspecifying, configuring or tuning many of them. Cost-effective data wrangling\nprocesses need to ensure that data wrangling steps benefit from automation\nwherever possible. In this paper, we define a methodology to fully automate an\nend-to-end data wrangling process incorporating data context, which associates\nportions of a target schema with potentially spurious extensional data of types\nthat are commonly available. Instance-based evidence together with data\nprofiling paves the way to inform automation in several steps within the\nwrangling process, specifically, matching, mapping validation, value format\ntransformation, and data repair. The approach is evaluated with real estate\ndata showing substantial improvements in the results of automated wrangling.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 17:34:35 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Koehler", "Martin", ""], ["Bogatu", "Alex", ""], ["Civili", "Cristina", ""], ["Konstantinou", "Nikolaos", ""], ["Abel", "Edward", ""], ["Fernandes", "Alvaro A. A.", ""], ["Keane", "John", ""], ["Libkin", "Leonid", ""], ["Paton", "Norman W.", ""]]}, {"id": "1811.09271", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura and Sennur Ulukus and Deniz Gunduz", "title": "Distributed Gradient Descent with Coded Partial Gradient Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded computation techniques provide robustness against straggling servers in\ndistributed computing, with the following limitations: First, they increase\ndecoding complexity. Second, they ignore computations carried out by straggling\nservers; and they are typically designed to recover the full gradient, and\nthus, cannot provide a balance between the accuracy of the gradient and\nper-iteration completion time. Here we introduce a hybrid approach, called\ncoded partial gradient computation (CPGC), that benefits from the advantages of\nboth coded and uncoded computation schemes, and reduces both the computation\ntime and decoding complexity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 18:39:40 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Ozfatura", "Emre", ""], ["Ulukus", "Sennur", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1811.09712", "submitter": "Clement Fung", "authors": "Clement Fung, Jamie Koerner, Stewart Grant, Ivan Beschastnikh", "title": "Dancing in the Dark: Private Multi-Party Machine Learning in an\n  Untrusted Setting", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning (ML) systems today use an unsophisticated threat\nmodel: data sources must trust a central ML process. We propose a brokered\nlearning abstraction that allows data sources to contribute towards a\nglobally-shared model with provable privacy guarantees in an untrusted setting.\nWe realize this abstraction by building on federated learning, the state of the\nart in multi-party ML, to construct TorMentor: an anonymous hidden service that\nsupports private multi-party ML.\n  We define a new threat model by characterizing, developing and evaluating new\nattacks in the brokered learning setting, along with new defenses for these\nattacks. We show that TorMentor effectively protects data providers against\nknown ML attacks while providing them with a tunable trade-off between model\naccuracy and privacy. We evaluate TorMentor with local and geo-distributed\ndeployments on Azure/Tor. In an experiment with 200 clients and 14 MB of data\nper client, our prototype trained a logistic regression model using stochastic\ngradient descent in 65s.\n  Code is available at: https://github.com/DistributedML/TorML\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 22:00:39 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 00:40:45 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Fung", "Clement", ""], ["Koerner", "Jamie", ""], ["Grant", "Stewart", ""], ["Beschastnikh", "Ivan", ""]]}, {"id": "1811.09721", "submitter": "Tarek Elgamal", "authors": "Tarek Elgamal, Atul Sandur, Klara Nahrstedt, Gul Agha", "title": "Costless: Optimizing Cost of Serverless Computing through Function\n  Fusion and Placement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has recently experienced significant adoption by several\napplications, especially Internet of Things (IoT) applications. In serverless\ncomputing, rather than deploying and managing dedicated virtual machines, users\nare able to deploy individual functions, and pay only for the time that their\ncode is actually executing. However, since serverless platforms are relatively\nnew, they have a completely different pricing model that depends on the memory,\nduration, and the number of executions of a sequence/workflow of functions. In\nthis paper we present an algorithm that optimizes the price of serverless\napplications in AWS Lambda. We first describe the factors affecting price of\nserverless applications which include: (1) fusing a sequence of functions, (2)\nsplitting functions across edge and cloud resources, and (3) allocating the\nmemory for each function. We then present an efficient algorithm to explore\ndifferent function fusion-placement solutions and find the solution that\noptimizes the application's price while keeping the latency under a certain\nthreshold. Our results on image processing workflows show that the algorithm\ncan find solutions optimizing the price by more than 35%-57% with only 5%-15%\nincrease in latency. We also show that our algorithm can find non-trivial\nmemory configurations that reduce both latency and price.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 22:38:19 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Elgamal", "Tarek", ""], ["Sandur", "Atul", ""], ["Nahrstedt", "Klara", ""], ["Agha", "Gul", ""]]}, {"id": "1811.09732", "submitter": "Abdul Dakkak", "authors": "Abdul Dakkak, Cheng Li, Simon Garcia de Gonzalo, Jinjun Xiong, Wen-mei\n  Hwu", "title": "TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep\n  LearningInference in Function as a Service Environments", "comments": "In Proceedings CLOUD 2019", "journal-ref": null, "doi": "10.1109/CLOUD.2019.00067", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) have become core computation components within\nlow latency Function as a Service (FaaS) prediction pipelines: including image\nrecognition, object detection, natural language processing, speech synthesis,\nand personalized recommendation pipelines. Cloud computing, as the de-facto\nbackbone of modern computing infrastructure for both enterprise and consumer\napplications, has to be able to handle user-defined pipelines of diverse DNN\ninference workloads while maintaining isolation and latency guarantees, and\nminimizing resource waste. The current solution for guaranteeing isolation\nwithin FaaS is suboptimal -- suffering from \"cold start\" latency. A major cause\nof such inefficiency is the need to move large amount of model data within and\nacross servers. We propose TrIMS as a novel solution to address these issues.\nOur proposed solution consists of a persistent model store across the GPU, CPU,\nlocal storage, and cloud storage hierarchy, an efficient resource management\nlayer that provides isolation, and a succinct set of application APIs and\ncontainer technologies for easy and transparent integration with FaaS, Deep\nLearning (DL) frameworks, and user code. We demonstrate our solution by\ninterfacing TrIMS with the Apache MXNet framework and demonstrate up to 24x\nspeedup in latency for image classification models and up to 210x speedup for\nlarge models. We achieve up to 8x system throughput improvement.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 00:52:11 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dakkak", "Abdul", ""], ["Li", "Cheng", ""], ["de Gonzalo", "Simon Garcia", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1811.09878", "submitter": "Karanbir Chahal", "authors": "Vaibhav Mathur and Karanbir Chahal", "title": "Hydra: A Peer to Peer Distributed Training & Data Collection Framework", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1611.01578 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world needs diverse and unbiased data to train deep learning models.\nCurrently data comes from a variety of sources that are unmoderated to a large\nextent. The outcomes of training neural networks with unverified data yields\nbiased models with various strains of homophobia, sexism and racism. Another\ntrend observed in the world of deep learning is the rise of distributed\ntraining. Although cloud companies provide high performance compute for\ntraining models in the form of GPU's connected with a low latency network,\nusing these services comes at a high cost. We propose Hydra, a system that\nseeks to solve both of these problems in a novel manner by proposing a\ndecentralized distributed framework which utilizes the substantial amount of\nidle compute of everyday electronic devices like smartphones and desktop\ncomputers for training and data collection purposes. Hydra couples a\nspecialized distributed training framework on a network of these low powered\ndevices with a reward scheme that incentivizes users to provide high quality\ndata to unleash the compute capability on this training framework. Such a\nsystem has the ability to capture data from a wide variety of diverse sources\nwhich has been an issue in the current scenario of deep learning. Hydra brings\nin several new innovations in training on low powered devices including a fault\ntolerant version of the All Reduce algorithm. Furthermore we introduce a\nreinforcement learning policy to decide the size of training jobs on different\nmachines on a heterogeneous cluster of devices with varying network latencies\nfor Synchronous SGD. The novel thing about such a network is the ability of\neach machine to shut down and resume training capabilities at any point of time\nwithout restarting the overall training. To enable such an asynchronous\nbehaviour we propose a communication framework inspired by the Bittorrent\nprotocol and the Kademlia DHT.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 19:11:41 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mathur", "Vaibhav", ""], ["Chahal", "Karanbir", ""]]}, {"id": "1811.09904", "submitter": "Muhammad Shayan", "authors": "Muhammad Shayan, Clement Fung, Chris J.M. Yoon, Ivan Beschastnikh", "title": "Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is the current state of the art in supporting secure\nmulti-party machine learning (ML): data is maintained on the owner's device and\nthe updates to the model are aggregated through a secure protocol. However,\nthis process assumes a trusted centralized infrastructure for coordination, and\nclients must trust that the central service does not use the byproducts of\nclient data. In addition to this, a group of malicious clients could also harm\nthe performance of the model by carrying out a poisoning attack.\n  As a response, we propose Biscotti: a fully decentralized peer to peer (P2P)\napproach to multi-party ML, which uses blockchain and cryptographic primitives\nto coordinate a privacy-preserving ML process between peering clients. Our\nevaluation demonstrates that Biscotti is scalable, fault tolerant, and defends\nagainst known attacks. For example, Biscotti is able to protect the privacy of\nan individual client's update and the performance of the global model at scale\nwhen 30% of adversaries are trying to poison the model.\n  The implementation can be found at: https://github.com/DistributedML/Biscotti\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 22:24:38 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 21:39:04 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 01:40:22 GMT"}, {"version": "v4", "created": "Thu, 12 Dec 2019 04:29:53 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Shayan", "Muhammad", ""], ["Fung", "Clement", ""], ["Yoon", "Chris J. M.", ""], ["Beschastnikh", "Ivan", ""]]}, {"id": "1811.10316", "submitter": "Emanuele Natale", "authors": "Luca Becchetti, Andrea Clementi, Emanuele Natale, Francesco Pasquale,\n  Luca Trevisan", "title": "Finding a Bounded-Degree Expander Inside a Dense One", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It follows from the Marcus-Spielman-Srivastava proof of the Kadison-Singer\nconjecture that if $G=(V,E)$ is a $\\Delta$-regular dense expander then there is\nan edge-induced subgraph $H=(V,E_H)$ of $G$ of constant maximum degree which is\nalso an expander. As with other consequences of the MSS theorem, it is not\nclear how one would explicitly construct such a subgraph.\n  We show that such a subgraph (although with quantitatively weaker expansion\nand near-regularity properties than those predicted by MSS) can be constructed\nwith high probability in linear time, via a simple algorithm. Our algorithm\nallows a distributed implementation that runs in $\\mathcal O(\\log n)$ rounds\nand does $\\mathcal O(n)$ total work with high probability.\n  The analysis of the algorithm is complicated by the complex dependencies that\narise between edges and between choices made in different rounds. We sidestep\nthese difficulties by following the combinatorial approach of counting the\nnumber of possible random choices of the algorithm which lead to failure. We do\nso by a compression argument showing that such random choices can be encoded\nwith a non-trivial compression.\n  Our algorithm bears some similarity to the way agents construct a\ncommunication graph in a peer-to-peer network, and, in the bipartite case, to\nthe way agents select servers in blockchain protocols.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 12:18:29 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 13:10:32 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 14:10:01 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Becchetti", "Luca", ""], ["Clementi", "Andrea", ""], ["Natale", "Emanuele", ""], ["Pasquale", "Francesco", ""], ["Trevisan", "Luca", ""]]}, {"id": "1811.10498", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita and Roshan G. Ragel and Dhammike Elkaduwe", "title": "An optimized Parallel Failure-less Aho-Corasick algorithm for DNA\n  sequence matching", "comments": "6 pages, 3 figures, 4 tables, 5 graphs, 2016 IEEE International\n  Conference on Information and Automation for Sustainability (ICIAfS)", "journal-ref": null, "doi": "10.1109/ICIAFS.2016.7946533", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Aho-Corasick algorithm is multiple patterns searching algorithm running\nsequentially in various applications like network intrusion detection and\nbioinformatics for finding several input strings within a given large input\nstring. The parallel version of the Aho-Corasick algorithm is called as\nParallel Failure-less Aho-Corasick algorithm because it doesn't need failure\nlinks like in the original Aho-Corasick algorithm. In this research, we\nimplemented an application specific parallel failureless Aho-Corasick algorithm\nto the general purpose graphics processing unit by applying several cache\noptimization techniques for matching DNA sequences. Our parallel Aho-Corasick\nalgorithm shows better performance than the available parallel Aho-Corasick\nalgorithm library due to its simplicity and optimized cache memory usage of\ngraphics processing units for matching DNA sequences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:45:30 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Thambawita", "Vajira", ""], ["Ragel", "Roshan G.", ""], ["Elkaduwe", "Dhammike", ""]]}, {"id": "1811.10577", "submitter": "Kishori Konwar", "authors": "Kishori M Konwar, Wyatt Lloyd, Haonan Lu, Nancy Lynch", "title": "SNOW Revisited: Understanding When Ideal READ Transactions Are Possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  READ transactions that read data distributed across servers dominate the\nworkloads of real-world distributed storage systems. The SNOW Theorem stated\nthat ideal READ transactions that have optimal latency and the strongest\nguarantees, i.e. \"SNOW\" READ transactions, are impossible in one specific\nsetting that requires three or more clients: at least two readers and one\nwriter. However, it left many open questions. We close all of these open\nquestions with new impossibility results and new algorithms. First, we prove\nrigorously the result from the The SNOW Theorem paper saying that it is\nimpossible to have a READ transactions system that satisfies SNOW properties\nwith three or more clients. The insight we gained from this proof led to\nteasing out the implicit assumptions that are required to state the results and\nalso, resolving the open question regarding the possibility of SNOW with two\nclients. We show that it is possible to design an algorithm, where SNOW is\npossible in a multi-writer, single-reader (MWSR) setting when a client can send\nmessages to other clients; on the other hand, we prove it is impossible to\nimplement SNOW in a multi-writer, single-reader (MWSR) setting, which is more\ngeneral than the two-client setting, when client-to-client communication is\ndisallowed. We also correct the previous claim in The SNOW Theorem paper that\nincorrectly identified one existing system, Eiger, as supporting the strongest\nguarantees (SW) and whose read-only transactions had bounded latency. Thus,\nthere were no previous algorithms that provided the strongest guarantees and\nhad bounded latency. Finally, we introduce the first two algorithms to provide\nthe strongest guarantees with bounded latency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:32:40 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 14:44:39 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Konwar", "Kishori M", ""], ["Lloyd", "Wyatt", ""], ["Lu", "Haonan", ""], ["Lynch", "Nancy", ""]]}, {"id": "1811.10751", "submitter": "Sanghamitra Dutta", "authors": "Sanghamitra Dutta, Ziqian Bai, Haewon Jeong, Tze Meng Low and Pulkit\n  Grover", "title": "A Unified Coded Deep Neural Network Training Strategy Based on\n  Generalized PolyDot Codes for Matrix Multiplication", "comments": "Presented in part at the IEEE International Symposium on Information\n  Theory 2018 (Submission Date: Jan 12 2018); Currently under review at the\n  IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/ISIT.2018.8437852", "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has two contributions. First, we propose a novel coded matrix\nmultiplication technique called Generalized PolyDot codes that advances on\nexisting methods for coded matrix multiplication under storage and\ncommunication constraints. This technique uses \"garbage alignment,\" i.e.,\naligning computations in coded computing that are not a part of the desired\noutput. Generalized PolyDot codes bridge between Polynomial codes and MatDot\ncodes, trading off between recovery threshold and communication costs. Second,\nwe demonstrate that Generalized PolyDot can be used for training large Deep\nNeural Networks (DNNs) on unreliable nodes prone to soft-errors. This requires\nus to address three additional challenges: (i) prohibitively large overhead of\ncoding the weight matrices in each layer of the DNN at each iteration; (ii)\nnonlinear operations during training, which are incompatible with linear\ncoding; and (iii) not assuming presence of an error-free master node, requiring\nus to architect a fully decentralized implementation without any \"single point\nof failure.\" We allow all primary DNN training steps, namely, matrix\nmultiplication, nonlinear activation, Hadamard product, and update steps as\nwell as the encoding/decoding to be error-prone. We consider the case of\nmini-batch size $B=1$, as well as $B>1$, leveraging coded matrix-vector\nproducts, and matrix-matrix products respectively. The problem of DNN training\nunder soft-errors also motivates an interesting, probabilistic error model\nunder which a real number $(P,Q)$ MDS code is shown to correct $P-Q-1$ errors\nwith probability $1$ as compared to $\\lfloor \\frac{P-Q}{2} \\rfloor$ for the\nmore conventional, adversarial error model. We also demonstrate that our\nproposed strategy can provide unbounded gains in error tolerance over a\ncompeting replication strategy and a preliminary MDS-code-based strategy for\nboth these error models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 00:06:47 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Bai", "Ziqian", ""], ["Jeong", "Haewon", ""], ["Low", "Tze Meng", ""], ["Grover", "Pulkit", ""]]}, {"id": "1811.10792", "submitter": "Mahmoud Assran", "authors": "Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, Michael Rabbat", "title": "Stochastic Gradient Push for Distributed Deep Learning", "comments": "ICML 2019", "journal-ref": "International Conference on Machine Learning 97 (2019) 344-353", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data-parallel algorithms aim to accelerate the training of deep\nneural networks by parallelizing the computation of large mini-batch gradient\nupdates across multiple nodes. Approaches that synchronize nodes using exact\ndistributed averaging (e.g., via AllReduce) are sensitive to stragglers and\ncommunication delays. The PushSum gossip algorithm is robust to these issues,\nbut only performs approximate distributed averaging. This paper studies\nStochastic Gradient Push (SGP), which combines PushSum with stochastic gradient\nupdates. We prove that SGP converges to a stationary point of smooth,\nnon-convex objectives at the same sub-linear rate as SGD, and that all nodes\nachieve consensus. We empirically validate the performance of SGP on image\nclassification (ResNet-50, ImageNet) and machine translation (Transformer,\nWMT'16 En-De) workloads. Our code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:47:26 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 02:58:36 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 19:59:00 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Assran", "Mahmoud", ""], ["Loizou", "Nicolas", ""], ["Ballas", "Nicolas", ""], ["Rabbat", "Michael", ""]]}, {"id": "1811.10835", "submitter": "Chen Yang", "authors": "Chen Yang, Zhihui Du, Xiaofeng Meng, Yongjie Du and Zhiqiang Duan", "title": "A Frequency Scaling based Performance Indicator Framework for Big Data\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important for big data systems to identify their performance\nbottleneck. However, the popular indicators such as resource utilizations, are\noften misleading and incomparable with each other. In this paper, a novel\nindicator framework which can directly compare the impact of different\nindicators with each other is proposed to identify and analyze the performance\nbottleneck efficiently. A methodology which can construct the indicator from\nthe performance change with the CPU frequency scaling is described. Spark is\nused as an example of a big data system and two typical SQL benchmarks are used\nas the workloads to evaluate the proposed method. Experimental results show\nthat the proposed method is accurate compared with the resource utilization\nmethod and easy to implement compared with some white-box method. Meanwhile,\nthe analysis with our indicators lead to some interesting findings and valuable\nperformance optimization suggestions for big data systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:34:57 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Yang", "Chen", ""], ["Du", "Zhihui", ""], ["Meng", "Xiaofeng", ""], ["Du", "Yongjie", ""], ["Duan", "Zhiqiang", ""]]}, {"id": "1811.10939", "submitter": "Chii Chang", "authors": "Chii Chang and Amnir Hadachi and Satish Srirama", "title": "Adaptive Edge Process Migration for IoT in Heterogeneous Cloud-Fog-Edge\n  Computing Environment", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The latency issue of the cloud-centric IoT management system has motivated\nFog and Edge Computing (FEC) architecture, which distributes the tasks from the\ncloud to the edge resources such as routers, switches or the IoT devices\nthemselves. Specifically, mobile sensors of IoT system can also carry certain\ntasks for FEC. Considering the need of dynamic process migration from the\nmobile sensors to other resources when the mobile sensors unable to continue\ntheir tasks, the IoT system needs to provide a flexible mechanism that allows\nthe mobile sensors dynamically migrate their tasks to the other FEC resource at\nruntime. However, it raises a question in what is the optimal approach when the\nmobile sensors intend to migrate their tasks to multiple heterogeneous FEC\nresource? In order to address the question, the authors propose REM scheme,\nwhich is capable of optimising the process migration decision. Further, in\norder to realise such a system and to validate the REM scheme, the authors have\ndeveloped EPIoT host framework. Finally, the authors have implemented and have\nevaluated the REM scheme and framework, the results have shown that the REM\nscheme is capable of enhancing the performance of the process migration in\nheterogeneous FEC environment.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 12:42:51 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Chang", "Chii", ""], ["Hadachi", "Amnir", ""], ["Srirama", "Satish", ""]]}, {"id": "1811.11141", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Xiaowen Chu and Bo Li", "title": "MG-WFBP: Efficient Data Communication for Distributed Synchronous SGD\n  Algorithms", "comments": "9 pages, INFOCOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed synchronous stochastic gradient descent has been widely used to\ntrain deep neural networks on computer clusters. With the increase of\ncomputational power, network communications have become one limiting factor on\nsystem scalability. In this paper, we observe that many deep neural networks\nhave a large number of layers with only a small amount of data to be\ncommunicated. Based on the fact that merging some short communication tasks\ninto a single one may reduce the overall communication time, we formulate an\noptimization problem to minimize the training iteration time. We develop an\noptimal solution named merged-gradient WFBP (MG-WFBP) and implement it in our\nopen-source deep learning platform B-Caffe. Our experimental results on an\n8-node GPU cluster with 10GbE interconnect and trace-based simulation results\non a 64-node cluster both show that the MG-WFBP algorithm can achieve much\nbetter scaling efficiency than existing methods WFBP and SyncEASGD.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:08:07 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 04:32:44 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Shi", "Shaohuai", ""], ["Chu", "Xiaowen", ""], ["Li", "Bo", ""]]}, {"id": "1811.11213", "submitter": "Ryan Chard", "authors": "Ryan Chard, Zhuozhao Li, Kyle Chard, Logan Ward, Yadu Babuji, Anna\n  Woodard, Steve Tuecke, Ben Blaiszik, Michael J. Franklin, and Ian Foster", "title": "DLHub: Model and Data Serving for Science", "comments": "10 pages, 8 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the Machine Learning (ML) landscape is evolving rapidly, there has been\na relative lag in the development of the \"learning systems\" needed to enable\nbroad adoption. Furthermore, few such systems are designed to support the\nspecialized requirements of scientific ML. Here we present the Data and\nLearning Hub for science (DLHub), a multi-tenant system that provides both\nmodel repository and serving capabilities with a focus on science applications.\nDLHub addresses two significant shortcomings in current systems. First, its\nselfservice model repository allows users to share, publish, verify, reproduce,\nand reuse models, and addresses concerns related to model reproducibility by\npackaging and distributing models and all constituent components. Second, it\nimplements scalable and low-latency serving capabilities that can leverage\nparallel and distributed computing resources to democratize access to published\nmodels through a simple web interface. Unlike other model serving frameworks,\nDLHub can store and serve any Python 3-compatible model or processing function,\nplus multiple-function pipelines. We show that relative to other model serving\nsystems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides\ngreater capabilities, comparable performance without memoization and batching,\nand significantly better performance when the latter two techniques can be\nemployed. We also describe early uses of DLHub for scientific applications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:31:29 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Chard", "Ryan", ""], ["Li", "Zhuozhao", ""], ["Chard", "Kyle", ""], ["Ward", "Logan", ""], ["Babuji", "Yadu", ""], ["Woodard", "Anna", ""], ["Tuecke", "Steve", ""], ["Blaiszik", "Ben", ""], ["Franklin", "Michael J.", ""], ["Foster", "Ian", ""]]}, {"id": "1811.11653", "submitter": "George Teodoro", "authors": "Willian de Oliveira Barreiros Junior and George Teodoro", "title": "Accelerating Sensitivity Analysis in Microscopy Image Segmentation\n  Workflows", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasingly availability of digital microscopy imagery equipments\nthere is a demand for efficient execution of whole slide tissue image\napplications. Through the process of sensitivity analysis it is possible to\nimprove the output quality of such applications, and thus, improve the desired\nanalysis quality. Due to the high computational cost of such analyses and the\nrecurrent nature of executed tasks from sensitivity analysis methods (i.e.,\nreexecution of tasks), the opportunity for computation reuse arises. By\nperforming computation reuse we can optimize the run time of sensitivity\nanalysis applications. This work focuses then on finding new ways to take\nadvantage of computation reuse opportunities on multiple task abstraction\nlevels. This is done by presenting the coarse-grain merging strategy and the\nnew fine-grain merging algorithms, implemented on top of the Region Templates\nFramework.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 16:18:24 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Junior", "Willian de Oliveira Barreiros", ""], ["Teodoro", "George", ""]]}, {"id": "1811.11811", "submitter": "Sanghamitra Dutta", "authors": "Utsav Sheth, Sanghamitra Dutta, Malhar Chaudhari, Haewon Jeong,\n  Yaoqing Yang, Jukka Kohonen, Teemu Roos, Pulkit Grover", "title": "An Application of Storage-Optimal MatDot Codes for Coded Matrix\n  Multiplication: Fast k-Nearest Neighbors Estimation", "comments": "Accepted for publication at the IEEE Big Data 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel application of coded computing to the problem of the\nnearest neighbor estimation using MatDot Codes [Fahim. et.al. 2017], that are\nknown to be optimal for matrix multiplication in terms of recovery threshold\nunder storage constraints. In approximate nearest neighbor algorithms, it is\ncommon to construct efficient in-memory indexes to improve query response time.\nOne such strategy is Multiple Random Projection Trees (MRPT), which reduces the\nset of candidate points over which Euclidean distance calculations are\nperformed. However, this may result in a high memory footprint and possibly\npaging penalties for large or high-dimensional data. Here we propose two\ntechniques to parallelize MRPT, that exploit data and model parallelism\nrespectively, by dividing both the data storage and the computation efforts\namong different nodes in a distributed computing cluster. This is especially\ncritical when a single compute node cannot hold the complete dataset in memory.\nWe also propose a novel coded computation strategy based on MatDot codes for\nthe model-parallel architecture that, in a straggler-prone environment,\nachieves the storage-optimal recovery threshold, i.e., the number of nodes that\nare required to serve a query. We experimentally demonstrate that, in the\nabsence of straggling, our distributed approaches require less query time than\nexecution on a single processing node, providing near-linear speedups with\nrespect to the number of worker nodes. Through our experiments on real systems\nwith simulated straggling, we also show that our strategy achieves a faster\nquery execution than the uncoded strategy in a straggler-prone environment.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:22:31 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Sheth", "Utsav", ""], ["Dutta", "Sanghamitra", ""], ["Chaudhari", "Malhar", ""], ["Jeong", "Haewon", ""], ["Yang", "Yaoqing", ""], ["Kohonen", "Jukka", ""], ["Roos", "Teemu", ""], ["Grover", "Pulkit", ""]]}, {"id": "1811.11953", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Anand P. Santhanam, Celina Imielinska, Sanford\n  Meeks and Jannick P. Rolland", "title": "Distributed Augmented Reality with 3D Lung Dynamics -- A Planning Tool\n  Concept", "comments": null, "journal-ref": "IEEE Transactions on Information Technology in Biomedicine (2007),\n  Vol. 11(1), pp. 40-46", "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) systems add visual information to the world by using\nadvanced display techniques. The advances in miniaturization and reduced costs\nmake some of these systems feasible for applications in a wide set of fields.\nWe present a potential component of the cyber infrastructure for the operating\nroom of the future; a distributed AR based software-hardware system that allows\nreal-time visualization of 3D lung dynamics superimposed directly on the\npatient's body. Several emergency events (e.g. closed and tension pneumothorax)\nand surgical procedures related to the lung (e.g. lung transplantation, lung\nvolume reduction surgery, surgical treatment of lung infections, lung cancer\nsurgery) could benefit from the proposed prototype.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 04:10:23 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Santhanam", "Anand P.", ""], ["Imielinska", "Celina", ""], ["Meeks", "Sanford", ""], ["Rolland", "Jannick P.", ""]]}, {"id": "1811.11978", "submitter": "Md. Redowan Mahmud", "authors": "Shreshth Tuli, Redowan Mahmud, Shikhar Tuli, Rajkumar Buyya", "title": "FogBus: A Blockchain-based Lightweight Framework for Edge and Fog\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The requirement of supporting both latency sensitive and computing intensive\nInternet of Things (IoT) applications is consistently boosting the necessity\nfor integrating Edge, Fog and Cloud infrastructure. Although there are a number\nof real-world frameworks attempt to support such integration, they have many\nlimitations from various perspectives including platform independence,\nsecurity, resource management and multi-application assistance. To address\nthese limitations, we propose a simplified but effective framework, named\nFogBus for facilitating end-to-end IoT-Fog(Edge)-Cloud integration. FogBus\noffers a platform independent interface to IoT applications and computing\ninstances for execution and interaction. It not only assists developers in\nbuilding applications but also helps users in running multiple applications at\na time and service providers to manage their resources. In addition, FogBus\napplies Blockchain, authentication and encryption techniques to secure\noperations on sensitive data. Because of its lightweight and cross platform\nsoftware systems, it is easy to deploy, scalable and cost e_cient. We\ndemonstrate the effectiveness of our framework by creating a computing\nenvironment with it that integrates finger pulse oximeter as IoT devices with\nSmartphone-based gateway and Raspberry Pi-based Fog nodes for Sleep Apnea\nanalysis. We also run several experiments on this computing environment varying\nFogBus settings. The experimental results show that different FogBus settings\ncan improve latency, energy, network and CPU usage of the computing\ninfrastructure.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 06:19:43 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Tuli", "Shreshth", ""], ["Mahmud", "Redowan", ""], ["Tuli", "Shikhar", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1811.12068", "submitter": "Yoshiaki Katayama", "authors": "Satoshi Terai, Koichi Wada and Yoshiaki Katayama", "title": "Gathering Problems for Autonomous Mobile Robots with Lights", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Gathering problem for n autonomous mobile robots in\nsemi-synchronous settings with persistent memory called light. It is well known\nthat Gathering is impossible in a basic model when robots have no lights, if\nthe system is semi-synchronous or even centralized (only one robot is active in\neach time). On the other hand, Rendezvous (Gathering when n = 2) is possible if\nrobots have lights of various types with a constant number of colors. In this\npaper, we extend the model of robots with lights so that Gathering algorithms\ncan be discussed properly. Then we show Gathering algorithms with three types\nof lights in the semi-synchronous settings and reveal relationship between the\npower of lights and other additional assumptions. The most algorithms shown\nhere are optimal in the number of colors they use.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 11:15:06 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Terai", "Satoshi", ""], ["Wada", "Koichi", ""], ["Katayama", "Yoshiaki", ""]]}, {"id": "1811.12174", "submitter": "Amit Juneja", "authors": "Samuel Matzek, Max Grossman, Minsik Cho, Anar Yusifov, Bryant Nelson,\n  Amit Juneja", "title": "Data-parallel distributed training of very large models beyond GPU\n  capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs have limited memory and it is difficult to train wide and/or deep models\nthat cause the training process to go out of memory. It is shown in this paper\nhow an open source tool called Large Model Support (LMS) can utilize a high\nbandwidth NVLink connection between CPUs and GPUs to accomplish training of\ndeep convolutional networks. LMS performs tensor swapping between CPU memory\nand GPU memory such that only a minimal number of tensors required in a\ntraining step are kept in the GPU memory. It is also shown how LMS can be\ncombined with an MPI based distributed deep learning module to train models in\na data-parallel fashion across multiple GPUs, such that each GPU is utilizing\nthe CPU memory for tensor swapping. The hardware architecture that enables the\nhigh bandwidth GPU link with the CPU is discussed as well as the associated set\nof software tools that are available as the PowerAI package.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:22:05 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Matzek", "Samuel", ""], ["Grossman", "Max", ""], ["Cho", "Minsik", ""], ["Yusifov", "Anar", ""], ["Nelson", "Bryant", ""], ["Juneja", "Amit", ""]]}, {"id": "1811.12341", "submitter": "Neil J. Gunther", "authors": "Neil J. Gunther and Mohit Chawla", "title": "Linux-Tomcat Application Performance on Amazon AWS", "comments": "10 pages, 25 figures. To appear in Linux Magazin (in German),\n  February 2, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for Linux system administrators to do performance management has\nreturned with a vengeance. Why? The cloud. Resource consumption in the cloud is\nall about pay-as-you-go. This article shows you how performance models can find\nthe most cost-effective deployment of an application on Amazon's cloud.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:41:05 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Gunther", "Neil J.", ""], ["Chawla", "Mohit", ""]]}, {"id": "1811.12578", "submitter": "Wenjing Wu", "authors": "Wenjing Wu, David Cameron, Qing Di", "title": "Using ATLAS@Home to exploit extra CPU from busy grid sites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid computing typically provides most of the data processing resources for\nlarge High Energy Physics experiments. However typical grid sites are not fully\nutilized by regular workloads. In order to increase the CPU utilization of\nthese grid sites, the ATLAS@Home volunteer computing framework can be used as a\nbackfilling mechanism. Results show an extra 15% to 42% of CPU cycles can be\nexploited by backfilling grid sites running regular workloads while the overall\nCPU utilization can remain over 90%. Backfilling has no impact on the failure\nrate of the grid jobs, and the impact on the CPU efficiency of grid jobs varies\nfrom 1% to 11% depending on the configuration of the site. In addition the\nthroughput of backfill jobs in terms of CPU time per simulated event is the\nsame as for resources dedicated to ATLAS@Home. This approach is sufficiently\ngeneric that it can easily be extended to other clusters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:01:07 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Wu", "Wenjing", ""], ["Cameron", "David", ""], ["Di", "Qing", ""]]}, {"id": "1811.12628", "submitter": "Ivica Nikolic", "authors": "Haifeng Yu and Ivica Nikolic and Ruomu Hou and Prateek Saxena", "title": "OHIE: Blockchain Scaling Made Simple", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many blockchain consensus protocols have been proposed recently to scale the\nthroughput of a blockchain with available bandwidth. However, these protocols\nare becoming increasingly complex, making it more and more difficult to produce\nproofs of their security guarantees. We propose a novel permissionless\nblockchain protocol OHIE which explicitly aims for simplicity. OHIE composes as\nmany parallel instances of Bitcoin's original (and simple) backbone protocol as\nneeded to achieve excellent throughput. We formally prove the safety and\nliveness properties of OHIE. We demonstrate its performance with a prototype\nimplementation and large-scale experiments with up to 50,000 nodes. In our\nexperiments, OHIE achieves linear scaling with available bandwidth, providing\nabout 4-10 Mbps transaction throughput (under 8-20 Mbps per-node available\nbandwidth configurations) and at least about 20x better decentralization over\nprior works.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:02:45 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 04:07:25 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 04:23:22 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Yu", "Haifeng", ""], ["Nikolic", "Ivica", ""], ["Hou", "Ruomu", ""], ["Saxena", "Prateek", ""]]}, {"id": "1811.12706", "submitter": "Andrey Demichev", "authors": "Andrey Demichev, Alexander Kryukov and Nikolai Prikhodko", "title": "The Approach to Managing Provenance Metadata and Data Access Rights in\n  Distributed Storage Using the Hyperledger Blockchain Platform", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper suggests a new approach based on blockchain technologies and smart\ncontracts to creation of a distributed system for managing provenance metadata,\nas well as access rights to data in distributed storages, which is\nfault-tolerant, safe and secure from the point of view of preservation of\nmetadata records from accidental or intentional distortions. The implementation\nof the proposed approach is based on the permissioned blockchains and on the\nHyperledger Fabric blockchain platform in conjunction with Hyperledger\nComposer.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 10:40:23 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Demichev", "Andrey", ""], ["Kryukov", "Alexander", ""], ["Prikhodko", "Nikolai", ""]]}, {"id": "1811.12742", "submitter": "Christoph Rettinger", "authors": "Christoph Rettinger, Ulrich R\\\"ude", "title": "Dynamic Load Balancing Techniques for Particulate Flow Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel multiphysics simulations often suffer from load imbalances\noriginating from the applied coupling of algorithms with spatially and\ntemporally varying workloads. It is thus desirable to minimize these imbalances\nto reduce the time to solution and to better utilize the available hardware\nresources. Taking particulate flows as an illustrating example application, we\npresent and evaluate load balancing techniques that tackle this challenging\ntask. This involves a load estimation step in which the currently generated\nworkload is predicted. We describe in detail how such a workload estimator can\nbe developed. In a second step, load distribution strategies like space-filling\ncurves or graph partitioning are applied to dynamically distribute the load\namong the available processes. To compare and analyze their performance, we\nemploy these techniques to a benchmark scenario and observe a reduction of the\nload imbalances by almost a factor of four. This results in a decrease of the\noverall runtime by 14% for space-filling curves.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 11:47:23 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Rettinger", "Christoph", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1811.12806", "submitter": "Mou Wu", "authors": "Mou Wu, Naixue Xiong, Liansheng Tan", "title": "Convergence Analysis of a Cooperative Diffusion Gauss-Newton Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the convergence performance of a cooperative\ndiffusion Gauss-Newton (GN) method, which is widely used to solve the nonlinear\nleast squares problems (NLLS) due to the low computation cost compared with\nNewton's method. This diffusion GN collects the diversity of temporalspatial\ninformation over the network, which is used on local updates. In order to\naddress the challenges on convergence analysis, we firstly consider to form a\nglobal recursion relation over spatial and temporal scales since the\ntraditional GN is a time iterative method and the network-wide NLLS need to be\nsolved. Secondly, the derived recursion related to the networkwide deviation\nbetween the successive two iterations is ambiguous due to the uncertainty of\ndescent discrepancy in GN update step between two versions of cooperation and\nnon-cooperation. Thus, an important work is to derive the boundedness\nconditions of this discrepancy. Finally, based on the temporal-spatial\nrecursion relation and the steady-state equilibria theory for discrete\ndynamical systems, we obtain the sufficient conditions for algorithm\nconvergence, which require the good initial guesses, reasonable step size\nvalues and network connectivity. Such analysis provides a guideline for the\napplications based on this diffusion GN method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:45:14 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 07:29:59 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Wu", "Mou", ""], ["Xiong", "Naixue", ""], ["Tan", "Liansheng", ""]]}, {"id": "1811.12815", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Jannick P. Rolland, Charles Hughes", "title": "A Distributed Augmented Reality System for Medical Training and\n  Simulation", "comments": "arXiv admin note: text overlap with arXiv:1111.2993 by other authors", "journal-ref": "Energy, Simulation-Training, Ocean Engineering and\n  Instrumentation: Research Papers of the Link Foundation Fellows (2004), Vol.\n  4, pp. 213-235", "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) systems describe the class of systems that use\ncomputers to overlay virtual information on the real world. AR environments\nallow the development of promising tools in several application domains. In\nmedical training and simulation the learning potential of AR is significantly\namplified by the capability of the system to present 3D medical models in\nreal-time at remote locations. Furthermore the simulation applicability is\nbroadened by the use of real-time deformable medical models. This work presents\na distributed medical training prototype designed to train medical\npractitioners' hand-eye coordination when performing endotracheal intubations.\nThe system we present accomplishes this task with the help of AR paradigms. An\nextension of this prototype to medical simulations by employing deformable\nmedical models is possible. The shared state maintenance of the collaborative\nAR environment is assured through a novel adaptive synchronization algorithm\n(ASA) that increases the sense of presence among participants and facilitates\ntheir interactivity in spite of infrastructure delays. The system will allow\nparamedics, pre-hospital personnel, and students to practice their skills\nwithout touching a real patient and will provide them with the visual feedback\nthey could not otherwise obtain. Such a distributed AR training tool has the\npotential to: allow an instructor to simultaneously train local and remotely\nlocated students and, allow students to actually \"see\" the internal anatomy and\ntherefore better understand their actions on a human patient simulator (HPS).\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 04:35:44 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Rolland", "Jannick P.", ""], ["Hughes", "Charles", ""]]}, {"id": "1811.12901", "submitter": "Vaneet Aggarwal", "authors": "Yang Zhang and Arnob Ghosh and Vaneet Aggarwal", "title": "Optimized Portfolio Contracts for Bidding the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amazon EC2 provides two most popular pricing schemes--i) the {\\em costly}\non-demand instance where the job is guaranteed to be completed, and ii) the\n{\\em cheap} spot instance where a job may be interrupted. We consider a user\ncan select a combination of on-demand and spot instances to finish a task. Thus\nhe needs to find the optimal bidding price for the spot-instance, and the\nportion of the job to be run on the on-demand instance. We formulate the\nproblem as an optimization problem and seek to find the optimal solution. We\nconsider three bidding strategies: one-time requests with expected guarantee\nand one-time requests with penalty for incomplete job and violating the\ndeadline, and persistent requests. Even without a penalty on incomplete jobs,\nthe optimization problem turns out to be non-convex. Nevertheless, we show that\nthe portion of the job to be run on the on-demand instance is at most half. If\nthe job has a higher execution time or smaller deadline, the bidding price is\nhigher and vice versa. Additionally, the user never selects the on-demand\ninstance if the execution time is smaller than the deadline.\n  The numerical results illustrate the sensitivity of the effective portfolio\nto several of the parameters involved in the model. Our empirical analysis on\nthe Amazon EC2 data shows that our strategies can be employed on the real\ninstances, where the expected total cost of the proposed scheme decreases over\n45\\% compared to the baseline strategy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 17:25:11 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Zhang", "Yang", ""], ["Ghosh", "Arnob", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1811.12924", "submitter": "Vaneet Aggarwal", "authors": "Abubakr Alabbasi and Vaneet Aggarwal", "title": "Joint Information Freshness and Completion Time Optimization for\n  Vehicular Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for real-time cloud applications has seen an unprecedented growth\nover the past decade. These applications require rapidly data transfer and fast\ncomputations. This paper considers a scenario where multiple IoT devices update\ninformation on the cloud, and request a computation from the cloud at certain\ntimes. The time required to complete the request for computation includes the\ntime to wait for computation to start on busy virtual machines, performing the\ncomputation, waiting and service in the networking stage for delivering the\noutput to the end user. In this context, the freshness of the information is an\nimportant concern and is different from the completion time. This paper\nproposes novel scheduling strategies for both computation and networking\nstages. Based on these strategies, the age-of-information (AoI) metric and the\ncompletion time are characterized. A convex combination of the two metrics is\noptimized over the scheduling parameters. The problem is shown to be convex and\nthus can be solved optimally. Moreover, based on the offline policy, an online\nalgorithm for job scheduling is developed. Numerical results demonstrate\nsignificant improvement as compared to the considered baselines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 18:23:45 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Alabbasi", "Abubakr", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1811.12941", "submitter": "Kai Rothauge", "authors": "Noah Golmant and Nikita Vemuri and Zhewei Yao and Vladimir Feinberg\n  and Amir Gholami and Kai Rothauge and Michael W. Mahoney and Joseph Gonzalez", "title": "On the Computational Inefficiency of Large Batch Sizes for Stochastic\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing the mini-batch size for stochastic gradient descent offers\nsignificant opportunities to reduce wall-clock training time, but there are a\nvariety of theoretical and systems challenges that impede the widespread\nsuccess of this technique. We investigate these issues, with an emphasis on\ntime to convergence and total computational cost, through an extensive\nempirical analysis of network training across several architectures and problem\ndomains, including image classification, image segmentation, and language\nmodeling. Although it is common practice to increase the batch size in order to\nfully exploit available computational resources, we find a substantially more\nnuanced picture. Our main finding is that across a wide range of network\narchitectures and problem domains, increasing the batch size beyond a certain\npoint yields no decrease in wall-clock time to convergence for \\emph{either}\ntrain or test loss. This batch size is usually substantially below the capacity\nof current systems. We show that popular training strategies for large batch\nsize optimization begin to fail before we can populate all available compute\nresources, and we show that the point at which these methods break down depends\nmore on attributes like model architecture and data complexity than it does\ndirectly on the size of the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 18:58:59 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Golmant", "Noah", ""], ["Vemuri", "Nikita", ""], ["Yao", "Zhewei", ""], ["Feinberg", "Vladimir", ""], ["Gholami", "Amir", ""], ["Rothauge", "Kai", ""], ["Mahoney", "Michael W.", ""], ["Gonzalez", "Joseph", ""]]}]