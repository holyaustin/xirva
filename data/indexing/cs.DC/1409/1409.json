[{"id": "1409.0085", "submitter": "Partha Sarathi Mandal Dr.", "authors": "Kaushik Mondal and Arindam Karmakar and Partha Sarathi Mandal", "title": "Designing Path Planning Algorithms for Mobile Anchor towards Range-Free\n  Localization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization is one of the most important factor in wireless sensor networks\nas many applications demand position information of sensors. Recently there is\nan increasing interest on the use of mobile anchors for localizing sensors.\nMost of the works available in the literature either looks into the aspect of\nreducing path length of mobile anchor or tries to increase localization\naccuracy. The challenge is to design a movement strategy for a mobile anchor\nthat reduces path length while meeting the requirements of a good range-free\nlocalization technique. In this paper we propose two cost-effective movement\nstrategies i.e., path planning for a mobile anchor so that localization can be\ndone using the localization scheme \\cite{Lee2009}. In one strategy we use a\nhexagonal movement pattern for the mobile anchor to localize all sensors inside\na bounded rectangular region with lesser movement compared to the existing\nworks in literature. In other strategy we consider a connected network in an\nunbounded region where the mobile anchor moves in the hexagonal pattern to\nlocalize the sensors. In this approach, we guarantee localization of all\nsensors within $r/2$ error-bound where $r$ is the communication range of the\nmobile anchor and sensors. Our simulation results support theoretical results\nalong with localization accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 05:08:28 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Mondal", "Kaushik", ""], ["Karmakar", "Arindam", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "1409.0325", "submitter": "Drazen Lucanin MSc", "authors": "Dra\\v{z}en Lu\\v{c}anin, Foued Jrad, Ivona Brandic, Achim Streit", "title": "Energy-Aware Cloud Management through Progressive SLA Specification", "comments": "14 pages, conference", "journal-ref": null, "doi": "10.1007/978-3-319-14609-6", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel energy-aware cloud management methods dynamically reallocate\ncomputation across geographically distributed data centers to leverage regional\nelectricity price and temperature differences. As a result, a managed VM may\nsuffer occasional downtimes. Current cloud providers only offer high\navailability VMs, without enough flexibility to apply such energy-aware\nmanagement. In this paper we show how to analyse past traces of dynamic cloud\nmanagement actions based on electricity prices and temperatures to estimate VM\navailability and price values. We propose a novel SLA specification approach\nfor offering VMs with different availability and price values guaranteed over\nmultiple SLAs to enable flexible energy-aware cloud management. We determine\nthe optimal number of such SLAs as well as their availability and price\nguaranteed values. We evaluate our approach in a user SLA selection simulation\nusing Wikipedia and Grid'5000 workloads. The results show higher customer\nconversion and 39% average energy savings per VM.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 08:35:08 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Lu\u010danin", "Dra\u017een", ""], ["Jrad", "Foued", ""], ["Brandic", "Ivona", ""], ["Streit", "Achim", ""]]}, {"id": "1409.0547", "submitter": "Bart Smets", "authors": "Bart Smets", "title": "Investigation on Demand Side Management Techniques in the Smart Grid\n  using Game Theory and ICT Concepts", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how concepts from game theory and ICT can contribute\nto solve challenges in demand side management, an important concept in the\nupcoming smart grid. Demand side management is about modifying the energy load\ndistribution on the demand side, for example in order to reduce peaks in energy\nusage. This can be done by shifting energy demands where possible. We start\nwith describing a number of smart grid concepts and assumptions (smart meters,\npricing, appliance scheduling) and explain the advantages demand side\nmanagement has. After the introduction of game theoretic concepts, it becomes\npossible to mathematically describe the demand side management problem. Next\nstep is to solve the mathematical formulation, and show how complex demand side\nmanagement becomes if the number of energy users increases. By means of\ndistributed ICT algorithms however, it is possible to still find a solution.\nBased on existing literature, different algorithms are studied. Though results\nin literature looked promising, several conclusions on convergence of the\nalgorithm in general, and convergence towards the most optimal results in\nparticular are challenged.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 20:01:05 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Smets", "Bart", ""]]}, {"id": "1409.0669", "submitter": "Karl Rupp", "authors": "Karl Rupp and Philippe Tillet and Florian Rudolf and Josef Weinbub and\n  Tibor Grasser and Ansgar J\\\"ungel", "title": "Performance Portability Study of Linear Algebra Kernels in OpenCL", "comments": "11 pages, 8 figures, 2 tables, International Workshop on OpenCL 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance portability of OpenCL kernel implementations for common\nmemory bandwidth limited linear algebra operations across different hardware\ngenerations of the same vendor as well as across vendors is studied. Certain\ncombinations of kernel implementations and work sizes are found to exhibit good\nperformance across compute kernels, hardware generations, and, to a lesser\ndegree, vendors. As a consequence, it is demonstrated that the optimization of\na single kernel is often sufficient to obtain good performance for a large\nclass of more complicated operations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 11:21:13 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Rupp", "Karl", ""], ["Tillet", "Philippe", ""], ["Rudolf", "Florian", ""], ["Weinbub", "Josef", ""], ["Grasser", "Tibor", ""], ["J\u00fcngel", "Ansgar", ""]]}, {"id": "1409.0820", "submitter": "George Kesidis", "authors": "G. Kesidis, B. Urgaonkar, Y. Shan, S. Kamarava, J. Liebeherr", "title": "Network calculus for parallel processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we present preliminary results on the use of \"network calculus\"\nfor parallel processing systems, specifically MapReduce.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 18:44:48 GMT"}, {"version": "v2", "created": "Sat, 31 Jan 2015 23:37:31 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Kesidis", "G.", ""], ["Urgaonkar", "B.", ""], ["Shan", "Y.", ""], ["Kamarava", "S.", ""], ["Liebeherr", "J.", ""]]}, {"id": "1409.0940", "submitter": "Haim Avron", "authors": "Vikas Sindhwani and Haim Avron", "title": "High-performance Kernel Machines with Implicit Distributed Optimization\n  and Randomization", "comments": "Work presented at MMDS 2014 (June 2014) and JSM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to fully utilize \"big data\", it is often required to use \"big\nmodels\". Such models tend to grow with the complexity and size of the training\ndata, and do not make strong parametric assumptions upfront on the nature of\nthe underlying statistical dependencies. Kernel methods fit this need well, as\nthey constitute a versatile and principled statistical methodology for solving\na wide range of non-parametric modelling problems. However, their high\ncomputational costs (in storage and time) pose a significant barrier to their\nwidespread adoption in big data applications.\n  We propose an algorithmic framework and high-performance implementation for\nmassive-scale training of kernel-based statistical models, based on combining\ntwo key technical ingredients: (i) distributed general purpose convex\noptimization, and (ii) the use of randomization to improve the scalability of\nkernel methods. Our approach is based on a block-splitting variant of the\nAlternating Directions Method of Multipliers, carefully reconfigured to handle\nvery large random feature matrices, while exploiting hybrid parallelism\ntypically found in modern clusters of multicore machines. Our implementation\nsupports a variety of statistical learning tasks by enabling several loss\nfunctions, regularization schemes, kernels, and layers of randomized\napproximations for both dense and sparse datasets, in a highly extensible\nframework. We evaluate the ability of our framework to learn models on data\nfrom applications, and provide a comparison against existing sequential and\nparallel libraries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 02:28:51 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 21:38:15 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 18:06:53 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Avron", "Haim", ""]]}, {"id": "1409.1510", "submitter": "Mathias Wagner", "authors": "O. Kaczmarek, C. Schmidt, P. Steinbrecher, Swagato Mukherjee, and M.\n  Wagner", "title": "HISQ inverter on Intel Xeon Phi and NVIDIA GPUs", "comments": "7 pages, proceedings, presented at the 32nd International Symposium\n  on Lattice Field Theory (Lattice 2014), June 23 to June 28, 2014, New York,\n  USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The runtime of a Lattice QCD simulation is dominated by a small kernel, which\ncalculates the product of a vector by a sparse matrix known as the \"Dslash\"\noperator. Therefore, this kernel is frequently optimized for various HPC\narchitectures. In this contribution we compare the performance of the Intel\nXeon Phi to current Kepler-based NVIDIA Tesla GPUs running a conjugate gradient\nsolver. By exposing more parallelism to the accelerator through inverting\nmultiple vectors at the same time we obtain a performance 250 GFlop/s on both\narchitectures. This more than doubles the performance of the inversions. We\ngive a short overview of both architectures, discuss some details of the\nimplementation and the effort required to obtain the achieved performance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 18:13:30 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Kaczmarek", "O.", ""], ["Schmidt", "C.", ""], ["Steinbrecher", "P.", ""], ["Mukherjee", "Swagato", ""], ["Wagner", "M.", ""]]}, {"id": "1409.1551", "submitter": "Sreechakra Goparaju", "authors": "Salim El Rouayheb and Sreechakra Goparaju and Han Mao Kiah and Olgica\n  Milenkovic", "title": "Synchronizing Edits in Distributed Storage Networks", "comments": "This draft contains 33 pages. The authors are listed according to the\n  Hardy-Littlewood rule (alphabetical in last name)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of synchronizing data in distributed storage networks\nunder an edit model that includes deletions and insertions. We present two\nmodifications of MDS, regenerating and locally repairable codes that allow\nupdates in the parity-check values to be performed with one round of\ncommunication at low bit rates and using small storage overhead. Our main\ncontributions are novel protocols for synchronizing both hot and semi-static\ndata and protocols for data deduplication applications, based on intermediary\npermutation, Vandermonde and Cauchy matrix coding.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 19:26:46 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Rouayheb", "Salim El", ""], ["Goparaju", "Sreechakra", ""], ["Kiah", "Han Mao", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1409.1654", "submitter": "Ashraf Shahin", "authors": "Ashraf A. Shahin", "title": "Polymorphic Worms Collection in Cloud Computing", "comments": "International Journal of Computer Science and Mobile Computing, Vol.3\n  Issue.8, August- 2014, pg. 645-652", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, computer worms are seen as one of significant\nchallenges of cloud computing. Worms are rapidly changing and getting more\nsophisticated to evade detection. One major issue to defend against computer\nworms is collecting worms' payloads to generate their signature and study their\nbehavior. To collect worms' payloads, we identified challenges for detecting\nand collecting worms' payloads and proposed high-interactive honeypot to\ncollect payloads of zero-day polymorphic worms in homogeneous and heterogeneous\ncloud computing platforms. Virtual machine (VM) memory and VM disk image are\ninspected from outside using open-source forensics tools and VMWare Virtual\nDisk Development Kit. Our experiments show that the proposed approach overcomes\nthe identified challenges.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 03:14:47 GMT"}, {"version": "v2", "created": "Sat, 13 Sep 2014 02:20:50 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Shahin", "Ashraf A.", ""]]}, {"id": "1409.1666", "submitter": "Nihar Shah", "authors": "Preetum Nakkiran, Nihar B. Shah, K. V. Rashmi", "title": "Fundamental Limits on Communication for Oblivious Updates in Storage\n  Networks", "comments": "IEEE Global Communications Conference (GLOBECOM) 2014", "journal-ref": null, "doi": "10.1109/GLOCOM.2014.7037161", "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed storage systems, storage nodes intermittently go offline for\nnumerous reasons. On coming back online, nodes need to update their contents to\nreflect any modifications to the data in the interim. In this paper, we\nconsider a setting where no information regarding modified data needs to be\nlogged in the system. In such a setting, a 'stale' node needs to update its\ncontents by downloading data from already updated nodes, while neither the\nstale node nor the updated nodes have any knowledge as to which data symbols\nare modified and what their value is. We investigate the fundamental limits on\nthe amount of communication necessary for such an \"oblivious\" update process.\n  We first present a generic lower bound on the amount of communication that is\nnecessary under any storage code with a linear encoding (while allowing\nnon-linear update protocols). This lower bound is derived under a set of\nextremely weak conditions, giving all updated nodes access to the entire\nmodified data and the stale node access to the entire stale data as side\ninformation. We then present codes and update algorithms that are optimal in\nthat they meet this lower bound. Next, we present a lower bound for an\nimportant subclass of codes, that of linear Maximum-Distance-Separable (MDS)\ncodes. We then present an MDS code construction and an associated update\nalgorithm that meets this lower bound. These results thus establish the\ncapacity of oblivious updates in terms of the communication requirements under\nthese settings.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 06:36:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Shah", "Nihar B.", ""], ["Rashmi", "K. V.", ""]]}, {"id": "1409.1914", "submitter": "Richard Lethin", "authors": "Nicolas Vasilache, Muthu Baskaran, Tom Henretty, Benoit Meister, M.\n  Harper Langston, Sanket Tavarageri, Richard Lethin", "title": "A Tale of Three Runtimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution discusses the automatic generation of event-driven,\ntuple-space based programs for task-oriented execution models from a sequential\nC specification. We developed a hierarchical mapping solution using\nauto-parallelizing compiler technology to target three different runtimes\nrelying on event-driven tasks (EDTs). Our solution benefits from the important\nobservation that loop types encode short, transitive relations among EDTs that\nare compact and efficiently evaluated at runtime. In this context, permutable\nloops are of particular importance as they translate immediately into\nconservative point-to-point synchronizations of distance 1. Our solution\ngenerates calls into a runtime-agnostic C++ layer, which we have retargeted to\nIntel's Concurrent Collections (CnC), ETI's SWARM, and the Open Community\nRuntime (OCR). Experience with other runtime systems motivates our introduction\nof support for hierarchical async-finishes in CnC. Experimental data is\nprovided to show the benefit of automatically generated code for EDT-based\nruntimes as well as comparisons across runtimes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 19:58:40 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Vasilache", "Nicolas", ""], ["Baskaran", "Muthu", ""], ["Henretty", "Tom", ""], ["Meister", "Benoit", ""], ["Langston", "M. Harper", ""], ["Tavarageri", "Sanket", ""], ["Lethin", "Richard", ""]]}, {"id": "1409.2088", "submitter": "Michael Kruse", "authors": "Michael Kruse (LRI, INRIA Saclay - Ile de France)", "title": "Introducing Molly: Distributed Memory Parallelization with LLVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming for distributed memory machines has always been a tedious task,\nbut necessary because compilers have not been sufficiently able to optimize for\nsuch machines themselves. Molly is an extension to the LLVM compiler toolchain\nthat is able to distribute and reorganize workload and data if the program is\norganized in statically determined loop control-flows. These are represented as\npolyhedral integer-point sets that allow program transformations applied on\nthem. Memory distribution and layout can be declared by the programmer as\nneeded and the necessary asynchronous MPI communication is generated\nautomatically. The primary motivation is to run Lattice QCD simulations on IBM\nBlue Gene/Q supercomputers, but since the implementation is not yet completed,\nthis paper shows the capabilities on Conway's Game of Life.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 06:41:30 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Kruse", "Michael", "", "LRI, INRIA Saclay - Ile de France"]]}, {"id": "1409.2156", "submitter": "Ashraf Shahin", "authors": "Ashraf A. Shahin", "title": "Variability Modeling for Customizable SaaS Applications", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": "10.5121/ijcsit.2014.6503", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of current Software-as-a-Service (SaaS) applications are developed as\ncustomizable service-oriented applications that serve a large number of tenants\n(users) by one application instance. The current rapid evolution of SaaS\napplications increases the demand to study the commonality and variability in\nsoftware product lines that produce customizable SaaS applications. During\nruntime, Customizability is required to achieve different tenants'\nrequirements. During the development process, defining and realizing commonalty\nand variability in SaaS applications' families is required to develop reusable,\nflexible, and customizable SaaS applications at lower costs, in shorter time,\nand with higher quality. In this paper, Orthogonal Variability Model (OVM) is\nused to model variability in a separated model, which is used to generate\nsimple and understandable customization model. Additionally, Service oriented\narchitecture Modeling Language (SoaML) is extended to define and realize\ncommonalty and variability during the development of SaaS applications.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 19:22:46 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Shahin", "Ashraf A.", ""]]}, {"id": "1409.2383", "submitter": "Athanasios Liavas", "authors": "Athanasios P. Liavas and Nicholas D. Sidiropoulos", "title": "Parallel Algorithms for Constrained Tensor Factorization via the\n  Alternating Direction Method of Multipliers", "comments": "Submitted to the IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2454476", "report-no": null, "categories": "cs.NA cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorization has proven useful in a wide range of applications, from\nsensor array processing to communications, speech and audio signal processing,\nand machine learning. With few recent exceptions, all tensor factorization\nalgorithms were originally developed for centralized, in-memory computation on\na single machine; and the few that break away from this mold do not easily\nincorporate practically important constraints, such as nonnegativity. A new\nconstrained tensor factorization framework is proposed in this paper, building\nupon the Alternating Direction method of Multipliers (ADMoM). It is shown that\nthis simplifies computations, bypassing the need to solve constrained\noptimization problems in each iteration; and it naturally leads to distributed\nalgorithms suitable for parallel implementation on regular high-performance\ncomputing (e.g., mesh) architectures. This opens the door for many emerging big\ndata-enabled applications. The methodology is exemplified using nonnegativity\nas a baseline constraint, but the proposed framework can more-or-less readily\nincorporate many other types of constraints. Numerical experiments are very\nencouraging, indicating that the ADMoM-based nonnegative tensor factorization\n(NTF) has high potential as an alternative to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 14:51:08 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 14:07:59 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Liavas", "Athanasios P.", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1409.2591", "submitter": "EPTCS", "authors": "R. Ramanujam (IMSc, Chennai), S. Sheerazuddin (SSNCE, Chennai)", "title": "A Local Logic for Realizability in Web Service Choreographies", "comments": "In Proceedings WWV 2014, arXiv:1409.2294", "journal-ref": "EPTCS 163, 2014, pp. 16-35", "doi": "10.4204/EPTCS.163.3", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web service choreographies specify conditions on observable interactions\namong the services. An important question in this regard is realizability:\ngiven a choreography C, does there exist a set of service implementations I\nthat conform to C ? Further, if C is realizable, is there an algorithm to\nconstruct implementations in I ? We propose a local temporal logic in which\nchoreographies can be specified, and for specifications in the logic, we solve\nthe realizability problem by constructing service implementations (when they\nexist) as communicating automata. These are nondeterministic finite state\nautomata with a coupling relation. We also report on an implementation of the\nrealizability algorithm and discuss experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 04:12:49 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Ramanujam", "R.", "", "IMSc, Chennai"], ["Sheerazuddin", "S.", "", "SSNCE, Chennai"]]}, {"id": "1409.2762", "submitter": "Thalia Karydi", "authors": "Efthalia Karydi and Konstantinos G. Margaritis", "title": "Parallel and Distributed Collaborative Filtering: A Survey", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Collaborative filtering is amongst the most preferred techniques when\nimplementing recommender systems. Recently, great interest has turned towards\nparallel and distributed implementations of collaborative filtering algorithms.\nThis work is a survey of the parallel and distributed collaborative filtering\nimplementations, aiming not only to provide a comprehensive presentation of the\nfield's development, but also to offer future research orientation by\nhighlighting the issues that need to be further developed.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 14:54:49 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Karydi", "Efthalia", ""], ["Margaritis", "Konstantinos G.", ""]]}, {"id": "1409.2864", "submitter": "Michael Lawrence", "authors": "Michael Lawrence, Martin Morgan", "title": "Scalable Genomics with R and Bioconductor", "comments": "Published in at http://dx.doi.org/10.1214/14-STS476 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 214-226", "doi": "10.1214/14-STS476", "report-no": "IMS-STS-STS476", "categories": "q-bio.GN cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews strategies for solving problems encountered when analyzing\nlarge genomic data sets and describes the implementation of those strategies in\nR by packages from the Bioconductor project. We treat the scalable processing,\nsummarization and visualization of big genomic data. The general ideas are well\nestablished and include restrictive queries, compression, iteration and\nparallel computing. We demonstrate the strategies by applying Bioconductor\npackages to the detection and analysis of genetic variants from a whole genome\nsequencing experiment.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 10:47:37 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Lawrence", "Michael", ""], ["Morgan", "Martin", ""]]}, {"id": "1409.2908", "submitter": "Austin Benson", "authors": "Austin R. Benson and Grey Ballard", "title": "A Framework for Practical Parallel Fast Matrix Multiplication", "comments": null, "journal-ref": "Proceedings of the 20th ACM SIGPLAN Symposium on Principles and\n  Practice of Parallel Programming (PPoPP), 2015", "doi": "10.1145/2858788.2688513", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication is a fundamental computation in many scientific\ndisciplines. In this paper, we show that novel fast matrix multiplication\nalgorithms can significantly outperform vendor implementations of the classical\nalgorithm and Strassen's fast algorithm on modest problem sizes and shapes.\nFurthermore, we show that the best choice of fast algorithm depends not only on\nthe size of the matrices but also the shape. We develop a code generation tool\nto automatically implement multiple sequential and shared-memory parallel\nvariants of each fast algorithm, including our novel parallelization scheme.\nThis allows us to rapidly benchmark over 20 fast algorithms on several problem\nsizes. Furthermore, we discuss a number of practical implementation issues for\nthese algorithms on shared-memory machines that can direct further research on\nmaking fast algorithms practical.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 22:28:36 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Benson", "Austin R.", ""], ["Ballard", "Grey", ""]]}, {"id": "1409.3367", "submitter": "Gabriel Muller", "authors": "Gabriel L. Muller", "title": "HTML5 WebSocket protocol and its application to distributed computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTML5 WebSocket protocol brings real time communication in web browsers to a\nnew level. Daily, new products are designed to stay permanently connected to\nthe web. WebSocket is the technology enabling this revolution. WebSockets are\nsupported by all current browsers, but it is still a new technology in constant\nevolution.\n  WebSockets are slowly replacing older client-server communication\ntechnologies. As opposed to comet-like technologies WebSockets' remarkable\nperformances is a result of the protocol's fully duplex nature and because it\ndoesn't rely on HTTP communications.\n  To begin with this paper studies the WebSocket protocol and different\nWebSocket servers implementations. This first theoretic part focuses more\ndeeply on heterogeneous implementations and OpenCL. The second part is a\nbenchmark of a new promising library.\n  The real-time engine used for testing purposes is SocketCluster.\nSocketCluster provides a highly scalable WebSocket server that makes use of all\navailable cpu cores on an instance. The scope of this work is reduced to\nvertical scaling of SocketCluster.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 09:36:46 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Muller", "Gabriel L.", ""]]}, {"id": "1409.3463", "submitter": "Yousi Zheng", "authors": "Yousi Zheng and Ness Shroff and Prasun Sinha", "title": "Heavy Traffic Limits for GI/H/n Queues: Theory and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a GI/H/n queueing system. In this system, there are multiple\nservers in the queue. The inter-arrival time is general and independent, and\nthe service time follows hyper-exponential distribution. Instead of stochastic\ndifferential equations, we propose two heavy traffic limits for this system,\nwhich can be easily applied in practical systems. In applications, we show how\nto use these heavy traffic limits to design a power efficient cloud computing\nenvironment based on different QoS requirements.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 14:53:47 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Zheng", "Yousi", ""], ["Shroff", "Ness", ""], ["Sinha", "Prasun", ""]]}, {"id": "1409.3651", "submitter": "Dushyant Vaghela Mr.", "authors": "Dushyant Vaghela", "title": "An Advanced Approach On Load Balancing in Grid Computing", "comments": "We have applied our Research work on various servers, NGIX performs\n  better, VPS Hosting Godadday servers Representative for\n  http://explorequotes.com/ working fine, finally we have concluded that all\n  the experiments were satisfactory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development in wide area networks and low cost, powerful\ncomputational resources, grid computing has gained its popularity. With the\nadvent of grid computing, space limitations of conventional distributed systems\ncan be overcome and underutilized computing resources at different locations\naround the world can be put to distributed jobs. Workload and resource\nmanagement is the main key grid services at the service level of grid\ninfrastructures, out of which load balancing in the main concern for grid\ndevelopers. It has been found that load is the major problem which server\nfaces, especially when the number of users increases. A lot of research is\nbeing done in the area of load management. This paper presents the various\nmechanisms of load balancing in grid computing so that the readers will get an\nidea of which algorithm would be suitable in different situations. Keywords:\nwide area network, distributed computing, load balancing.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 05:40:34 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Vaghela", "Dushyant", ""]]}, {"id": "1409.4078", "submitter": "Boris Burshteyn", "authors": "Boris Burshteyn", "title": "The distributed Language Hello White Paper", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hello is a general-purpose, object-oriented, protocol-agnostic distributed\nprogramming language. This paper explains the ideas that guided design of\nHello. It shows the spirit of Hello using two brief expressive programs and\nprovides a summary of language features. In addition, it explores historical\nparallels between the binary programming of early computers and the distributed\nprogramming of modern networks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 17:26:34 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Burshteyn", "Boris", ""]]}, {"id": "1409.4082", "submitter": "Evgeny Nikulchev", "authors": "Evgeniy Pluzhnik, Evgeny Nikulchev", "title": "Virtual Laboratories in Cloud Infrastructure of Educational Institutions", "comments": "3 pages, Published in: 2014 2nd International Conference on Emission\n  Electronics (ICEE), Saint-Petersburg, Russia", "journal-ref": null, "doi": "10.1109/Emission.2014.6893974", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern educational institutions widely used virtual laboratories and cloud\ntechnologies. In practice must deal with security, processing speed and other\ntasks. The paper describes the experience of the construction of an\nexperimental stand cloud computing and network management. Models and control\nprinciples set forth herein.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 17:53:49 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Pluzhnik", "Evgeniy", ""], ["Nikulchev", "Evgeny", ""]]}, {"id": "1409.4256", "submitter": "Stefan Engblom", "authors": "Tomas Ekeberg, Stefan Engblom, and Jing Liu", "title": "Machine learning for ultrafast X-ray diffraction patterns on large-scale\n  GPU clusters", "comments": null, "journal-ref": "Int. J. High Perf. Comput. Appl. 29(2):233--243 (2015)", "doi": "10.1177/1094342015572030", "report-no": null, "categories": "q-bio.BM cs.DC cs.LG physics.bio-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical method of determining the atomic structure of complex molecules\nby analyzing diffraction patterns is currently undergoing drastic developments.\nModern techniques for producing extremely bright and coherent X-ray lasers\nallow a beam of streaming particles to be intercepted and hit by an ultrashort\nhigh energy X-ray beam. Through machine learning methods the data thus\ncollected can be transformed into a three-dimensional volumetric intensity map\nof the particle itself. The computational complexity associated with this\nproblem is very high such that clusters of data parallel accelerators are\nrequired.\n  We have implemented a distributed and highly efficient algorithm for\ninversion of large collections of diffraction patterns targeting clusters of\nhundreds of GPUs. With the expected enormous amount of diffraction data to be\nproduced in the foreseeable future, this is the required scale to approach real\ntime processing of data at the beam site. Using both real and synthetic data we\nlook at the scaling properties of the application and discuss the overall\ncomputational viability of this exciting and novel imaging technique.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 20:26:23 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 12:53:13 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Ekeberg", "Tomas", ""], ["Engblom", "Stefan", ""], ["Liu", "Jing", ""]]}, {"id": "1409.4626", "submitter": "Evgeny Nikulchev", "authors": "Evgeniy Pluzhnik, Evgeny Nikulchev, Simon Payain", "title": "Laboratory Test Bench for Research Network and Cloud Computing", "comments": "5 pages", "journal-ref": "Int'l J. of Communications, Network and System Sciences, 7:7,\n  243-247", "doi": "10.4236/ijcns.2014.77026", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present moment, there is a great interest in development of information\nsystems operating in cloud infrastructures. Generally, many of tasks remain\nunresolved such as tasks of optimization of large databases in a hybrid cloud\ninfrastructure, quality of service (QoS) at different levels of cloud services,\ndynamic control of distribution of cloud resources in application systems and\nmany others. Research and development of new solutions can be limited in case\nof using emulators or international commercial cloud services, due to the\nclosed architecture and limited opportunities for experimentation. Article\nprovides answers to questions on the establishment of a pilot cloud practically\n\"at home\" with the ability to adjust the width of the emulation channel and\ndelays in data transmission. It also describes architecture and configuration\nof the experimental setup. The proposed modular structure can be expanded by\navailable computing power.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 18:00:13 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Pluzhnik", "Evgeniy", ""], ["Nikulchev", "Evgeny", ""], ["Payain", "Simon", ""]]}, {"id": "1409.4711", "submitter": "Bogdan Chlebus", "authors": "Bogdan S. Chlebus and Leszek G\\k{a}sieniec and Dariusz R. Kowalski and\n  Alexander A. Schwarzmann", "title": "Doing-it-All with Bounded Work and Communication", "comments": null, "journal-ref": "Doing-it-All with Bounded Work and Communication. Information and\n  Computation, 254: 1 - 40, 2017", "doi": "10.1016/j.ic.2017.02.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Do-All problem, where $p$ cooperating processors need to\ncomplete $t$ similar and independent tasks in an adversarial setting. Here we\ndeal with a synchronous message passing system with processors that are subject\nto crash failures. Efficiency of algorithms in this setting is measured in\nterms of work complexity (also known as total available processor steps) and\ncommunication complexity (total number of point-to-point messages). When work\nand communication are considered to be comparable resources, then the overall\nefficiency is meaningfully expressed in terms of effort defined as work +\ncommunication. We develop and analyze a constructive algorithm that has work\n$O( t + p \\log p\\, (\\sqrt{p\\log p}+\\sqrt{t\\log t}\\, ) )$ and a nonconstructive\nalgorithm that has work $O(t +p \\log^2 p)$. The latter result is close to the\nlower bound $\\Omega(t + p \\log p/ \\log \\log p)$ on work. The effort of each of\nthese algorithms is proportional to its work when the number of crashes is\nbounded above by $c\\,p$, for some positive constant $c < 1$. We also present a\nnonconstructive algorithm that has effort $O(t + p ^{1.77})$.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 17:39:27 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 21:26:58 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 20:05:21 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Chlebus", "Bogdan S.", ""], ["G\u0105sieniec", "Leszek", ""], ["Kowalski", "Dariusz R.", ""], ["Schwarzmann", "Alexander A.", ""]]}, {"id": "1409.4988", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Enrico Maiorino, Lorenzo Livi, Antonello Rizzi\n  and Alireza Sadeghian", "title": "An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for\n  Clusters Mining and Knowledge Discovery", "comments": null, "journal-ref": null, "doi": "10.1007/s00500-015-1876-1", "report-no": null, "categories": "cs.LG cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-agent algorithm able to automatically discover relevant\nregularities in a given dataset, determining at the same time the set of\nconfigurations of the adopted parametric dissimilarity measure yielding compact\nand separated clusters. Each agent operates independently by performing a\nMarkovian random walk on a suitable weighted graph representation of the input\ndataset. Such a weighted graph representation is induced by the specific\nparameter configuration of the dissimilarity measure adopted by the agent,\nwhich searches and takes decisions autonomously for one cluster at a time.\nResults show that the algorithm is able to discover parameter configurations\nthat yield a consistent and interpretable collection of clusters. Moreover, we\ndemonstrate that our algorithm shows comparable performances with other similar\nstate-of-the-art algorithms when facing specific clustering problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 14:39:37 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Maiorino", "Enrico", ""], ["Livi", "Lorenzo", ""], ["Rizzi", "Antonello", ""], ["Sadeghian", "Alireza", ""]]}, {"id": "1409.4991", "submitter": "Alexander Setzer", "authors": "Martina Eikel, Christian Scheideler, Alexander Setzer", "title": "RoBuSt: A Crash-Failure-Resistant Distributed Storage System", "comments": "Revised full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the first distributed storage system that is provably\nrobust against crash failures issued by an adaptive adversary, i.e., for each\nbatch of requests the adversary can decide based on the entire system state\nwhich servers will be unavailable for that batch of requests. Despite up to\n$\\gamma n^{1/\\log\\log n}$ crashed servers, with $\\gamma>0$ constant and $n$\ndenoting the number of servers, our system can correctly process any batch of\nlookup and write requests (with at most a polylogarithmic number of requests\nissued at each non-crashed server) in at most a polylogarithmic number of\ncommunication rounds, with at most polylogarithmic time and work at each server\nand only a logarithmic storage overhead.\n  Our system is based on previous work by Eikel and Scheideler (SPAA 2013), who\npresented IRIS, a distributed information system that is provably robust\nagainst the same kind of crash failures. However, IRIS is only able to serve\nlookup requests. Handling both lookup and write requests has turned out to\nrequire major changes in the design of IRIS.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 13:38:30 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 11:05:37 GMT"}, {"version": "v3", "created": "Fri, 6 Feb 2015 16:11:11 GMT"}, {"version": "v4", "created": "Mon, 23 Feb 2015 08:13:37 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Eikel", "Martina", ""], ["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""]]}, {"id": "1409.5313", "submitter": "Holger Machens", "authors": "Holger Machens", "title": "Sandboxing for Software Transactional Memory with Deferred Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Software transactional memory implementations which allow transactions to\nwork on inconsistent states of shared data, risk to cause application visible\nerrors such as memory access violations or endless loops. Hence, many\nimplementations rely on repeated incremental validation of every read of the\ntransaction to always guarantee for a consistent view of shared data. Because\nthis eager validation technique generates significant processing costs several\nproposals have been published to establish a sandbox for transactions, which\ntransparently prevents or suppresses those errors and thereby allows to reduce\nthe frequency of in-flight validations.\n  The most comprehensive sandboxing concept of transactions in software\ntransactional memory based on deferred updates and considering unmanaged\nlanguages, integrates multiple techniques such as signal interposition,\nout-of-band validation and static and dynamic instrumentation. The latter\ncomprises the insertion of a validation barrier in front of every direct write\nwhich addresses the execution stack of the thread and potentially results from\nunvalidated reads.\n  This paper basically results from a review of this sandboxing approach, which\nrevealed some improvements for sandboxing on C/C++. Based on knowledge about\nthe runtime environment and the compiler an error model has been developed to\nidentify critical paths to application visible errors. This analysis lead to a\nconcept for stack protection with less frequent validation, an alternative\nout-of-band validation technique and revealed additional risks of so-called\nwaivered regions without instrumentation inside transactions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 14:26:22 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 17:02:39 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Machens", "Holger", ""]]}, {"id": "1409.5546", "submitter": "Asif Imran", "authors": "Asif Imran, Nadia Nahar and Kazi Sakib", "title": "Watchword-Oriented and Time-Stamped Algorithms for Tamper-Proof Cloud\n  Provenance Cognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance is derivative journal information about the origin and activities\nof system data and processes. For a highly dynamic system like the cloud,\nprovenance can be accurately detected and securely used in cloud digital\nforensic investigation activities. This paper proposes watchword oriented\nprovenance cognition algorithm for the cloud environment. Additionally\ntime-stamp based buffer verifying algorithm is proposed for securing the access\nto the detected cloud provenance. Performance analysis of the novel algorithms\nproposed here yields a desirable detection rate of 89.33% and miss rate of\n8.66%. The securing algorithm successfully rejects 64% of malicious requests,\nyielding a cumulative frequency of 21.43 for MR.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 08:33:15 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Imran", "Asif", ""], ["Nahar", "Nadia", ""], ["Sakib", "Kazi", ""]]}, {"id": "1409.5552", "submitter": "Asif Imran", "authors": "Asif Imran, Emon Kumar Dey and Kazi Sakib", "title": "Active-Threaded Algorithms for Provenance Cognition in the Cloud\n  preserving Low Overhead and Fault Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance is the derivation history of information about the origin of data\nand processes. For a highly dynamic system such as the cloud, provenance must\nbe effectively detected to be used as proves to ensure accountability during\ndigital forensic investigations. This paper proposes active-threaded provenance\ncognition algorithms that ensure effective and high speed detection of\nprovenance information in the activity layer of the cloud. The algorithms also\nsupport encapsulation of the provenance information on specific targets.\nPerformance evaluation of the proposed algorithms reveal mean delay of 8.198\nseconds that is below the pre-defined benchmark of 10 seconds. Standard\ndeviation and cumulative frequencies for delays are found to be 1.434 and 45.1%\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 08:47:51 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Imran", "Asif", ""], ["Dey", "Emon Kumar", ""], ["Sakib", "Kazi", ""]]}, {"id": "1409.5705", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang\n  Yu, Eric Xing", "title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-parametrized models, including multiclass logistic regression and\nsparse coding, are used in machine learning (ML) applications ranging from\ncomputer vision to computational biology. When these models are applied to\nlarge-scale ML problems starting at millions of samples and tens of thousands\nof classes, their parameter matrix can grow at an unexpected rate, resulting in\nhigh parameter synchronization costs that greatly slow down distributed\nlearning. To address this issue, we propose a Sufficient Factor Broadcasting\n(SFB) computation model for efficient distributed learning of a large family of\nmatrix-parameterized models, which share the following property: the parameter\nupdate computed on each data sample is a rank-1 matrix, i.e., the outer product\nof two \"sufficient factors\" (SFs). By broadcasting the SFs among worker\nmachines and reconstructing the update matrices locally at each worker, SFB\nimproves communication efficiency --- communication costs are linear in the\nparameter matrix's dimensions, rather than quadratic --- without affecting\ncomputational correctness. We present a theoretical convergence analysis of\nSFB, and empirically corroborate its efficiency on four different\nmatrix-parametrized ML models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 15:42:28 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 12:14:30 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Xie", "Pengtao", ""], ["Kim", "Jin Kyu", ""], ["Zhou", "Yi", ""], ["Ho", "Qirong", ""], ["Kumar", "Abhimanu", ""], ["Yu", "Yaoliang", ""], ["Xing", "Eric", ""]]}, {"id": "1409.5715", "submitter": "Stefan Schulte", "authors": "Stefan Schulte, Christian Janiesch, Srikumar Venugopal, Ingo Weber,\n  Philipp Hoenisch", "title": "Elastic Business Process Management: State of the Art and Open\n  Challenges for BPM in the Cloud", "comments": "Please cite as: S. Schulte, C. Janiesch, S. Venugopal, I. Weber, and\n  P. Hoenisch (2015). Elastic Business Process Management: State of the Art and\n  Open Challenges for BPM in the Cloud. Future Generation Computer Systems,\n  Volume NN, Number N, NN-NN., http://dx.doi.org/10.1016/j.future.2014.09.005", "journal-ref": "Future Generation Computer Systems, Volume 46, 36-50 (2015)", "doi": "10.1016/j.future.2014.09.005", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of cloud computing, organizations are nowadays able to react\nrapidly to changing demands for computational resources. Not only individual\napplications can be hosted on virtual cloud infrastructures, but also complete\nbusiness processes. This allows the realization of so-called elastic processes,\ni.e., processes which are carried out using elastic cloud resources. Despite\nthe manifold benefits of elastic processes, there is still a lack of solutions\nsupporting them.\n  In this paper, we identify the state of the art of elastic Business Process\nManagement with a focus on infrastructural challenges. We conceptualize an\narchitecture for an elastic Business Process Management System and discuss\nexisting work on scheduling, resource allocation, monitoring, decentralized\ncoordination, and state management for elastic processes. Furthermore, we\npresent two representative elastic Business Process Management Systems which\nare intended to counter these challenges. Based on our findings, we identify\nopen issues and outline possible research directions for the realization of\nelastic processes and elastic Business Process Management.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 16:36:49 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 10:56:55 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Schulte", "Stefan", ""], ["Janiesch", "Christian", ""], ["Venugopal", "Srikumar", ""], ["Weber", "Ingo", ""], ["Hoenisch", "Philipp", ""]]}, {"id": "1409.5757", "submitter": "Andrey Vladimirov", "authors": "Ryo Asai and Andrey Vladimirov", "title": "Intel Cilk Plus for Complex Parallel Algorithms: \"Enormous Fast Fourier\n  Transform\" (EFFT) Library", "comments": "17 pages. Submitted to Parallel Computing", "journal-ref": null, "doi": "10.1016/j.parco.2015.05.004", "report-no": null, "categories": "cs.MS cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate the methodology for parallelizing the\ncomputation of large one-dimensional discrete fast Fourier transforms (DFFTs)\non multi-core Intel Xeon processors. DFFTs based on the recursive Cooley-Tukey\nmethod have to control cache utilization, memory bandwidth and vector hardware\nusage, and at the same time scale across multiple threads or compute nodes. Our\nmethod builds on single-threaded Intel Math Kernel Library (MKL) implementation\nof DFFT, and uses the Intel Cilk Plus framework for thread parallelism. We\ndemonstrate the ability of Intel Cilk Plus to handle parallel recursion with\nnested loop-centric parallelism without tuning the code to the number of cores\nor cache metrics. The result of our work is a library called EFFT that performs\n1D DFTs of size 2^N for N>=21 faster than the corresponding Intel MKL parallel\nDFT implementation by up to 1.5x, and faster than FFTW by up to 2.5x. The code\nof EFFT is available for free download under the GPLv3 license. This work\nprovides a new efficient DFFT implementation, and at the same time demonstrates\nan educational example of how computer science problems with complex parallel\npatterns can be optimized for high performance using the Intel Cilk Plus\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 18:48:58 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Asai", "Ryo", ""], ["Vladimirov", "Andrey", ""]]}, {"id": "1409.6679", "submitter": "AashihaPriyadarshni LakshmiKumar", "authors": "Aashiha Priyadarshni.L", "title": "Heterogeneous Multi core processors for improving the efficiency of\n  Market basket analysis algorithm in data mining", "comments": "4 pages, 2 figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V15(1):16-19, Sep 2014. ISSN:2231-2803. www.ijcttjournal.org. Published by\n  Seventh Sense Research Group", "doi": "10.14445/22312803/IJCTT-V15P103", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous multi core processors can offer diverse computing capabilities.\nThe efficiency of Market Basket Analysis Algorithm can be improved with\nheterogeneous multi core processors. Market basket analysis algorithm utilises\napriori algorithm and is one of the popular data mining algorithms which can\nutilise Map/Reduce framework to perform analysis. The algorithm generates\nassociation rules based on transactional data and Map/Reduce motivates to\nredesign and convert the existing sequential algorithms for efficiency. Hadoop\nis the parallel programming platform built on Hadoop Distributed File\nSystems(HDFS) for Map/Reduce computation that process data as (key, value)\npairs. In Hadoop map/reduce, the sequential jobs are parallelised and the Job\nTracker assigns parallel tasks to the Task Tracker. Based on single threaded or\nmultithreaded parallel tasks in the task tracker, execution is carried out in\nthe appropriate cores. For this, a new scheduler called MB Scheduler can be\ndeveloped. Switching between the cores can be made static or dynamic. The use\nof heterogeneous multi core processors optimizes processing capabilities and\npower requirements for a processor and improves the performance of the system.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 17:44:59 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["L", "Aashiha Priyadarshni.", ""]]}, {"id": "1409.6771", "submitter": "Dmitry Zinoviev", "authors": "Dmitry Zinoviev and Dan Stefanescu and Hamid Benbrahim and Greta\n  Meszoely", "title": "Mitigation of Delayed Management Costs in Transaction-Oriented Systems", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abundant examples of complex transaction-oriented networks (TONs) can be\nfound in a variety of disciplines, including information and communication\ntechnology, finances, commodity trading, and real estate. A transaction in a\nTON is executed as a sequence of subtransactions associated with the network\nnodes, and is committed if every subtransaction is committed. A subtransaction\nincurs a two-fold overhead on the host node: the fixed transient operational\ncost and the cost of long-term management (e.g. archiving and support) that\npotentially grows exponentially with the transaction length. If the overall\ncost exceeds the node capacity, the node fails and all subtransaction incident\nto the node, and their parent distributed transactions, are aborted. A TON\nresilience can be measured in terms of either external workloads or intrinsic\nnode fault rates that cause the TON to partially or fully choke. We demonstrate\nthat under certain conditions, these two measures are equivalent. We further\nshow that the exponential growth of the long-term management costs can be\nmitigated by adjusting the effective operational cost: in other words, that the\nfuture maintenance costs could be absorbed into the transient operational\ncosts.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 23:07:02 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Zinoviev", "Dmitry", ""], ["Stefanescu", "Dan", ""], ["Benbrahim", "Hamid", ""], ["Meszoely", "Greta", ""]]}, {"id": "1409.6828", "submitter": "Shang Shang", "authors": "Shang Shang, Paul Cuff, Pan Hui, Sanjeev Kulkarni", "title": "An Upper Bound on the Convergence Time for Quantized Consensus of\n  Arbitrary Static Graphs", "comments": "to appear in IEEE Trans. on Automatic Control, January, 2015. arXiv\n  admin note: substantial text overlap with arXiv:1208.0788", "journal-ref": "IEEE Trans. on Automatic Control, 60(4):1127-32, April, 2015", "doi": "10.1109/TAC.2014.2342071", "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a class of distributed quantized consensus algorithms for\narbitrary static networks. In the initial setting, each node in the network has\nan integer value. Nodes exchange their current estimate of the mean value in\nthe network, and then update their estimation by communicating with their\nneighbors in a limited capacity channel in an asynchronous clock setting.\nEventually, all nodes reach consensus with quantized precision. We analyze the\nexpected convergence time for the general quantized consensus algorithm\nproposed by Kashyap et al \\cite{Kashyap}. We use the theory of electric\nnetworks, random walks, and couplings of Markov chains to derive an $O(N^3\\log\nN)$ upper bound for the expected convergence time on an arbitrary graph of size\n$N$, improving on the state of art bound of $O(N^5)$ for quantized consensus\nalgorithms. Our result is not dependent on graph topology. Example of complete\ngraphs is given to show how to extend the analysis to graphs of given topology.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 05:03:58 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Shang", "Shang", ""], ["Cuff", "Paul", ""], ["Hui", "Pan", ""], ["Kulkarni", "Sanjeev", ""]]}, {"id": "1409.7286", "submitter": "Vinay Vaishampayan", "authors": "Antonio Campello and Vinay A. Vaishampayan", "title": "Reliability of Erasure Coded Storage Systems: A Geometric Approach", "comments": "28 pages. 8 figures. Presented in part at IEEE International\n  Conference on BigData 2013, Santa Clara, CA, Oct. 2013 and to be presented in\n  part at 2014 IEEE Information Theory Workshop, Tasmania, Australia, Nov.\n  2014. New analysis added May 2015. Further Update Aug. 2015", "journal-ref": null, "doi": "10.1109/TIT.2015.2477401", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the probability of data loss, or equivalently, the reliability\nfunction for an erasure coded distributed data storage system under worst case\nconditions. Data loss in an erasure coded system depends on probability\ndistributions for the disk repair duration and the disk failure duration. In\nprevious works, the data loss probability of such systems has been studied\nunder the assumption of exponentially distributed disk failure and disk repair\ndurations, using well-known analytic methods from the theory of Markov\nprocesses. These methods lead to an estimate of the integral of the reliability\nfunction.\n  Here, we address the problem of directly calculating the data loss\nprobability for general repair and failure duration distributions. A closed\nlimiting form is developed for the probability of data loss and it is shown\nthat the probability of the event that a repair duration exceeds a failure\nduration is sufficient for characterizing the data loss probability.\n  For the case of constant repair duration, we develop an expression for the\nconditional data loss probability given the number of failures experienced by a\neach node in a given time window. We do so by developing a geometric approach\nthat relies on the computation of volumes of a family of polytopes that are\nrelated to the code. An exact calculation is provided and an upper bound on the\ndata loss probability is obtained by posing the problem as a set avoidance\nproblem. Theoretical calculations are compared to simulation results.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 15:12:15 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 12:50:56 GMT"}, {"version": "v3", "created": "Thu, 21 May 2015 21:47:39 GMT"}, {"version": "v4", "created": "Thu, 20 Aug 2015 03:37:26 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Campello", "Antonio", ""], ["Vaishampayan", "Vinay A.", ""]]}, {"id": "1409.7764", "submitter": "Saad Quader", "authors": "Saad Quader", "title": "A (Somewhat Dated) Comparative Study of Betweenness Centrality\n  Algorithms on GPU", "comments": "This study was done as a class project on the HPC course CSE 5304\n  (Fall 2012) at the University of Connecticut, and hence it does not cover any\n  advances since January 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of computing the Betweenness Centrality (BC) is important in\nanalyzing graphs in many practical applications like social networks,\nbiological networks, transportation networks, electrical circuits, etc. Since\nthis problem is computation intensive, researchers have been developing\nalgorithms using high performance computing resources like supercomputers,\nclusters, and Graphics Processing Units (GPUs). Current GPU algorithms for\ncomputing BC employ Brandes' sequential algorithm with different trade-offs for\nthread scheduling, data structures, and atomic operations. In this paper, we\nstudy three GPU algorithms for computing BC of unweighted, directed, scale-free\nnetworks. We discuss and measure the trade-offs of their design choices about\nbalanced thread scheduling, atomic operations, synchronizations and latency\nhiding. Our program is written in NVIDIA CUDA C and was tested on an NVIDIA\nTesla M2050 GPU.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 04:57:30 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Quader", "Saad", ""]]}, {"id": "1409.7771", "submitter": "Chinmoy Dutta", "authors": "Chinmoy Dutta and Gopal Pandurangan and Rajmohan Rajaraman and Zhifeng\n  Sun and Emanuele Viola", "title": "Global Information Sharing under Network Dynamics", "comments": "arXiv admin note: substantial text overlap with arXiv:1112.0384", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to spread $k$ tokens of information to every node on an $n$-node\ndynamic network, the edges of which are changing at each round. This basic {\\em\ngossip problem} can be completed in $O(n + k)$ rounds in any static network,\nand determining its complexity in dynamic networks is central to understanding\nthe algorithmic limits and capabilities of various dynamic network models. Our\nfocus is on token-forwarding algorithms, which do not manipulate tokens in any\nway other than storing, copying and forwarding them.\n  We first consider the {\\em strongly adaptive} adversary model where in each\nround, each node first chooses a token to broadcast to all its neighbors\n(without knowing who they are), and then an adversary chooses an arbitrary\nconnected communication network for that round with the knowledge of the tokens\nchosen by each node. We show that $\\Omega(nk/\\log n + n)$ rounds are needed for\nany randomized (centralized or distributed) token-forwarding algorithm to\ndisseminate the $k$ tokens, thus resolving an open problem raised\nin~\\cite{kuhn+lo:dynamic}. The bound applies to a wide class of initial token\ndistributions, including those in which each token is held by exactly one node\nand {\\em well-mixed} ones in which each node has each token independently with\na constant probability.\n  We also show several upper bounds in varying models.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 06:36:19 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Dutta", "Chinmoy", ""], ["Pandurangan", "Gopal", ""], ["Rajaraman", "Rajmohan", ""], ["Sun", "Zhifeng", ""], ["Viola", "Emanuele", ""]]}, {"id": "1409.7916", "submitter": "Sruthi priya kulasekaran Sivakumar", "authors": "Nallakumar R., Sruthi Priya K.S", "title": "A Survey on Deadline Constrained Workflow Scheduling Algorithms in Cloud\n  Environment", "comments": "6 pages,2 figures,Published with International Journal of Computer\n  Science Trends and Technology(IJCST)", "journal-ref": "IJCST V2(5): Page(44-50) Sep 2014", "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Cloud Computing is the latest blooming technology in the era of Computer\nScience and Information Technology domain. There is an enormous pool of data\ncentres, which are termed as Clouds where the services and associated data are\nbeing deployed and users need a constant Internet connection to access them.\nOne of the highlights in Cloud is the delivering of applications or services in\nan on-demand environment. One of the most promising areas in Cloud scheduling\nis Scheduling of workflows which is intended to match the request of the user\nto the appropriate resources. There are several algorithms to automate the\nworkflows in a way to satisfy the Quality of service (QoS) of the user among\nwhich deadline is considered as a major criterion, i.e. Satisfying the needs of\nthe user with minimized cost and within the minimum stipulated time. This paper\nsurveys various workflow scheduling algorithms that have a deadline as its\ncriterion.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 14:43:46 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["R.", "Nallakumar", ""], ["S", "Sruthi Priya K.", ""]]}, {"id": "1409.8098", "submitter": "Ward Jaradat", "authors": "Ward Jaradat, Alan Dearle, Adam Barker", "title": "Workflow Partitioning and Deployment on the Cloud using Orchestra", "comments": "To appear in Proceedings of the IEEE/ACM 7th International Conference\n  on Utility and Cloud Computing (UCC 2014)", "journal-ref": null, "doi": "10.1109/UCC.2014.34", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orchestrating service-oriented workflows is typically based on a design model\nthat routes both data and control through a single point - the centralised\nworkflow engine. This causes scalability problems that include the unnecessary\nconsumption of the network bandwidth, high latency in transmitting data between\nthe services, and performance bottlenecks. These problems are highly prominent\nwhen orchestrating workflows that are composed from services dispersed across\ndistant geographical locations. This paper presents a novel workflow\npartitioning approach, which attempts to improve the scalability of\norchestrating large-scale workflows. It permits the workflow computation to be\nmoved towards the services providing the data in order to garner optimal\nperformance results. This is achieved by decomposing the workflow into smaller\nsub workflows for parallel execution, and determining the most appropriate\nnetwork locations to which these sub workflows are transmitted and subsequently\nexecuted. This paper demonstrates the efficiency of our approach using a set of\nexperimental workflows that are orchestrated over Amazon EC2 and across several\ngeographic network regions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 12:39:13 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Jaradat", "Ward", ""], ["Dearle", "Alan", ""], ["Barker", "Adam", ""]]}, {"id": "1409.8324", "submitter": "Ittay Eyal", "authors": "Ittay Eyal, Ken Birman, Robbert van Renesse", "title": "Cache Serializability: Reducing Inconsistency in Edge Transactions", "comments": "Ittay Eyal, Ken Birman, Robbert van Renesse, \"Cache Serializability:\n  Reducing Inconsistency in Edge Transactions,\" Distributed Computing Systems\n  (ICDCS), IEEE 35th International Conference on, June~29 2015--July~2 2015", "journal-ref": null, "doi": "10.1109/ICDCS.2015.75", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Read-only caches are widely used in cloud infrastructures to reduce access\nlatency and load on backend databases. Operators view coherent caches as\nimpractical at genuinely large scale and many client-facing caches are updated\nin an asynchronous manner with best-effort pipelines. Existing solutions that\nsupport cache consistency are inapplicable to this scenario since they require\na round trip to the database on every cache transaction.\n  Existing incoherent cache technologies are oblivious to transactional data\naccess, even if the backend database supports transactions. We propose T-Cache,\na novel caching policy for read-only transactions in which inconsistency is\ntolerable (won't cause safety violations) but undesirable (has a cost). T-Cache\nimproves cache consistency despite asynchronous and unreliable communication\nbetween the cache and the database. We define cache-serializability, a variant\nof serializability that is suitable for incoherent caches, and prove that with\nunbounded resources T-Cache implements this new specification. With limited\nresources, T-Cache allows the system manager to choose a trade-off between\nperformance and consistency.\n  Our evaluation shows that T-Cache detects many inconsistencies with only\nnominal overhead. We use synthetic workloads to demonstrate the efficacy of\nT-Cache when data accesses are clustered and its adaptive reaction to workload\nchanges. With workloads based on the real-world topologies, T-Cache detects\n43-70% of the inconsistencies and increases the rate of consistent transactions\nby 33-58%.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 20:53:34 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 13:25:47 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2015 19:28:17 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Eyal", "Ittay", ""], ["Birman", "Ken", ""], ["van Renesse", "Robbert", ""]]}, {"id": "1409.8563", "submitter": "Daniel Ruprecht", "authors": "Andrea Arteaga, Daniel Ruprecht, Rolf Krause", "title": "A stencil-based implementation of Parareal in the C++ domain specific\n  embedded language STELLA", "comments": null, "journal-ref": "Applied Mathematics and Computation 267, pp. 727-741, 2015", "doi": "10.1016/j.amc.2014.12.055", "report-no": null, "categories": "cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the rapid rise of the number of cores in modern supercomputers,\ntime-parallel methods that introduce concurrency along the temporal axis are\nbecoming increasingly popular. For the solution of time-dependent partial\ndifferential equations, these methods can add another direction for concurrency\non top of spatial parallelization. The paper presents an implementation of the\ntime-parallel Parareal method in a C++ domain specific language for stencil\ncomputations (STELLA). STELLA provides both an OpenMP and a CUDA backend for a\nshared memory parallelization, using the CPU or GPU inside a node for the\nspatial stencils. Here, we intertwine this node-wise spatial parallelism with\nthe time-parallel Parareal. This is done by adding an MPI-based implementation\nof Parareal, which allows us to parallelize in time across nodes. The\nperformance of Parareal with both backends is analyzed in terms of speedup,\nparallel efficiency and energy-to-solution for an advection-diffusion problem\nwith a time-dependent diffusion coefficient.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 14:34:13 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 12:55:41 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Arteaga", "Andrea", ""], ["Ruprecht", "Daniel", ""], ["Krause", "Rolf", ""]]}]