[{"id": "1710.00073", "submitter": "Diman Zad Tootaghaj", "authors": "Farshid Farhat, Diman Zad Tootaghaj", "title": "CARMA: Contention-aware Auction-based Resource Management in\n  Architecture", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of resources on chip multiprocessors (CMPs) increases, the\ncomplexity of how to best allocate these resources increases drastically.\nBecause the higher number of applications makes the interaction and impacts of\nvarious memory levels more complex. Also, the selection of the objective\nfunction to define what \\enquote{best} means for all applications is\nchallenging. Memory-level parallelism (MLP) aware replacement algorithms in\nCMPs try to maximize the overall system performance or equalize each\napplication's performance degradation due to sharing. However, depending on the\nselected \\enquote{performance} metric, these algorithms are not efficiently\nimplemented, because these centralized approaches mostly need some further\ninformation regarding about applications' need. In this paper, we propose a\ncontention-aware game-theoretic resource management approach (CARMA) using\nmarket auction mechanism to find an optimal strategy for each application in a\nresource competition game. The applications learn through repeated interactions\nto choose their action on choosing the shared resources. Specifically, we\nconsider two cases: (i) cache competition game, and (ii) main processor and\nco-processor congestion game. We enforce costs for each resource and derive\nbidding strategy. Accurate evaluation of the proposed approach show that our\ndistributed allocation is scalable and outperforms the static and traditional\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 20:03:41 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 15:38:22 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 18:36:59 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 14:04:21 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Farhat", "Farshid", ""], ["Tootaghaj", "Diman Zad", ""]]}, {"id": "1710.00100", "submitter": "Burt Holzman", "authors": "Burt Holzman, Lothar A. T. Bauerdick, Brian Bockelman, Dave Dykstra,\n  Ian Fisk, Stuart Fuess, Gabriele Garzoglio, Maria Girone, Oliver Gutsche,\n  Dirk Hufnagel, Hyunwoo Kim, Robert Kennedy, Nicolo Magini, David Mason,\n  Panagiotis Spentzouris, Anthony Tiradani, Steve Timm, Eric W. Vaandering", "title": "HEPCloud, a New Paradigm for HEP Facilities: CMS Amazon Web Services\n  Investigation", "comments": "15 pages, 9 figures", "journal-ref": "Comput Softw Big Sci (2017) 1:1", "doi": "10.1007/s41781-017-0001-9", "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, high energy physics computing has been performed on large\npurpose-built computing systems. These began as single-site compute facilities,\nbut have evolved into the distributed computing grids used today. Recently,\nthere has been an exponential increase in the capacity and capability of\ncommercial clouds. Cloud resources are highly virtualized and intended to be\nable to be flexibly deployed for a variety of computing tasks. There is a\ngrowing nterest among the cloud providers to demonstrate the capability to\nperform large-scale scientific computing. In this paper, we discuss results\nfrom the CMS experiment using the Fermilab HEPCloud facility, which utilized\nboth local Fermilab resources and virtual machines in the Amazon Web Services\nElastic Compute Cloud. We discuss the planning, technical challenges, and\nlessons learned involved in performing physics workflows on a large-scale set\nof virtualized resources. In addition, we will discuss the economics and\noperational efficiencies when executing workflows both in the cloud and on\ndedicated resources.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 21:39:27 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Holzman", "Burt", ""], ["Bauerdick", "Lothar A. T.", ""], ["Bockelman", "Brian", ""], ["Dykstra", "Dave", ""], ["Fisk", "Ian", ""], ["Fuess", "Stuart", ""], ["Garzoglio", "Gabriele", ""], ["Girone", "Maria", ""], ["Gutsche", "Oliver", ""], ["Hufnagel", "Dirk", ""], ["Kim", "Hyunwoo", ""], ["Kennedy", "Robert", ""], ["Magini", "Nicolo", ""], ["Mason", "David", ""], ["Spentzouris", "Panagiotis", ""], ["Tiradani", "Anthony", ""], ["Timm", "Steve", ""], ["Vaandering", "Eric W.", ""]]}, {"id": "1710.00112", "submitter": "Sayed Hadi Hashemi", "authors": "Faraz Faghri, Sayed Hadi Hashemi, Mohammad Babaeizadeh, Mike A. Nalls,\n  Saurabh Sinha, Roy H. Campbell", "title": "Toward Scalable Machine Learning and Data Mining: the Bioinformatics\n  Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to overcome the data deluge in computational biology and\nbioinformatics and to facilitate bioinformatics research in the era of big\ndata, we identify some of the most influential algorithms that have been widely\nused in the bioinformatics community. These top data mining and machine\nlearning algorithms cover classification, clustering, regression, graphical\nmodel-based learning, and dimensionality reduction. The goal of this study is\nto guide the focus of scalable computing experts in the endeavor of applying\nnew storage and scalable computation designs to bioinformatics algorithms that\nmerit their attention most, following the engineering maxim of \"optimize the\ncommon case\".\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 22:29:19 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Faghri", "Faraz", ""], ["Hashemi", "Sayed Hadi", ""], ["Babaeizadeh", "Mohammad", ""], ["Nalls", "Mike A.", ""], ["Sinha", "Saurabh", ""], ["Campbell", "Roy H.", ""]]}, {"id": "1710.00122", "submitter": "Osama Talaat Ibrahim Eng", "authors": "Osama Talaat Ibrahim and Ahmed El-Mahdy", "title": "An Efficient Load Balancing Method for Tree Algorithms", "comments": "IEEE International Conferences on Scalable Computing and\n  Communications (ScalCom) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, multiprocessing is mainstream with exponentially increasing number\nof processors. Load balancing is, therefore, a critical operation for the\nefficient execution of parallel algorithms. In this paper we consider the\nfundamental class of tree-based algorithms that are notoriously irregular, and\nhard to load-balance with existing static techniques. We propose a hybrid load\nbalancing method using the utility of statistical random sampling in estimating\nthe tree depth and node count distributions to uniformly partition an input\ntree. To conduct an initial performance study, we implemented the method on an\nIntel Xeon Phi accelerator system. We considered the tree traversal operation\non both regular and irregular unbalanced trees manifested by Fibonacci and\nunbalanced (biased) randomly generated trees, respectively. The results show\nscalable performance for up to the 60 physical processors of the accelerator,\nas well as an extrapolated 128 processors case.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 00:12:05 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Ibrahim", "Osama Talaat", ""], ["El-Mahdy", "Ahmed", ""]]}, {"id": "1710.00267", "submitter": "Abhishek Dubey", "authors": "Gabor Karsai, Daniel Balasubramanian, Abhishek Dubey, and William R.\n  Otte", "title": "Distributed and Managed: Research Challenges and Opportunities of the\n  Next Generation Cyber-Physical Systems", "comments": "ISORC 2014, Object/Component/Service-Oriented Real-Time Distributed\n  Computing (ISORC), 2014 IEEE 17th International Symposium on", "journal-ref": null, "doi": "10.1109/ISORC.2014.36", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems increasingly rely on distributed computing platforms\nwhere sensing, computing, actuation, and communication resources are shared by\na multitude of applications. Such `cyber-physical cloud computing platforms'\npresent novel challenges because the system is built from mobile embedded\ndevices, is inherently distributed, and typically suffers from highly\nfluctuating connectivity among the modules. Architecting software for these\nsystems raises many challenges not present in traditional cloud computing.\nEffective management of constrained resources and application isolation without\nadversely affecting performance are necessary. Autonomous fault management and\nreal-time performance requirements must be met in a verifiable manner. It is\nalso both critical and challenging to support multiple end-users whose diverse\nsoftware applications have changing demands for computational and communication\nresources, while operating on different levels and in separate domains of\nsecurity.\n  The solution presented in this paper is based on a layered architecture\nconsisting of a novel operating system, a middleware layer, and\ncomponent-structured applications. The component model facilitates the\nconstruction of software applications from modular and reusable components that\nare deployed in the distributed system and interact only through well-defined\nmechanisms. The complexity of creating applications and performing system\nintegration is mitigated through the use of a domain-specific model-driven\ndevelopment process that relies on a domain-specific modeling language and its\naccompanying graphical modeling tools, software generators for synthesizing\ninfrastructure code, and the extensive use of model-based analysis for\nverification and validation.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 22:28:24 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Karsai", "Gabor", ""], ["Balasubramanian", "Daniel", ""], ["Dubey", "Abhishek", ""], ["Otte", "William R.", ""]]}, {"id": "1710.00268", "submitter": "Abhishek Dubey", "authors": "Abhishek Dubey, Gabor Karsai, Aniruddha Gokhale, William Emfinger,\n  Pranav Kumar", "title": "DREMS-OS: An Operating System for Managed Distributed Real-time Embedded\n  Systems", "comments": "Conference, International Conference on Space Mission Challenges for\n  Information Technology (SMC-IT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed real-time and embedded (DRE) systems executing mixed criticality\ntask sets are increasingly being deployed in mobile and embedded cloud\ncomputing platforms, including space applications. These DRE systems must not\nonly operate over a range of temporal and spatial scales, but also require\nstringent assurances for secure interactions between the system's tasks without\nviolating their individual timing constraints. To address these challenges,\nthis paper describes a novel distributed operating system focusing on the\nscheduler design to support the mixed criticality task sets. Empirical results\nfrom experiments involving a case study of a cluster of satellites emulated in\na laboratory testbed validate our claims.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 22:41:11 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Dubey", "Abhishek", ""], ["Karsai", "Gabor", ""], ["Gokhale", "Aniruddha", ""], ["Emfinger", "William", ""], ["Kumar", "Pranav", ""]]}, {"id": "1710.00414", "submitter": "Mehmet Aktas", "authors": "Mehmet Fatih Aktas, Pei Peng, Emina Soljanin", "title": "Straggler Mitigation by Delayed Relaunch of Tasks", "comments": "Accepted for IFIP WG 7.3 Performance 2017. Nov. 14-16, 2017, New\n  York, NY USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy for straggler mitigation, originally in data download and more\nrecently in distributed computing context, has been shown to be effective both\nin theory and practice. Analysis of systems with redundancy has drawn\nsignificant attention and numerous papers have studied pain and gain of\nredundancy under various service models and assumptions on the straggler\ncharacteristics. We here present a cost (pain) vs. latency (gain) analysis of\nusing simple replication or erasure coding for straggler mitigation in\nexecuting jobs with many tasks. We quantify the effect of the tail of task\nexecution times and discuss tail heaviness as a decisive parameter for the cost\nand latency of using redundancy. Specifically, we find that coded redundancy\nachieves better cost vs. latency tradeoff than simple replication and can yield\nreduction in both cost and latency under less heavy tailed execution times. We\nshow that delaying redundancy is not effective in reducing cost and that\ndelayed relaunch of stragglers can yield significant reduction in cost and\nlatency. We validate these observations by comparing with the simulations that\nuse empirical distributions extracted from Google cluster data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 21:09:29 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Peng", "Pei", ""], ["Soljanin", "Emina", ""]]}, {"id": "1710.00466", "submitter": "Huda Chuangpishit", "authors": "Huda Chuangpishit, Jurek Czyzowicz, Leszek Gasieniec, Konstantinos\n  Georgiou, Tomasz Jurdzinski, Evangelos Kranakis", "title": "Patrolling a Path Connecting a Set of Points with Unbalanced Frequencies\n  of Visits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patrolling consists of scheduling perpetual movements of a collection of\nmobile robots, so that each point of the environment is regularly revisited by\nany robot in the collection. In previous research, it was assumed that all\npoints of the environment needed to be revisited with the same minimal\nfrequency. In this paper we study efficient patrolling protocols for points\nlocated on a path, where each point may have a different constraint on\nfrequency of visits. The problem of visiting such divergent points was recently\nposed by Gasieniec et al. in [13], where the authors study protocols using a\nsingle robot patrolling a set of $n$ points located in nodes of a complete\ngraph and in Euclidean spaces. The focus in this paper is on patrolling with\ntwo robots. We adopt a scenario in which all points to be patrolled are located\non a line. We provide several approximation algorithms concluding with the best\ncurrently known $\\sqrt 3$-approximation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 03:04:21 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chuangpishit", "Huda", ""], ["Czyzowicz", "Jurek", ""], ["Gasieniec", "Leszek", ""], ["Georgiou", "Konstantinos", ""], ["Jurdzinski", "Tomasz", ""], ["Kranakis", "Evangelos", ""]]}, {"id": "1710.00610", "submitter": "Zheng Wang", "authors": "Vicent Sanz Marco, Ben Taylor, Barry Porter, Zheng Wang", "title": "Improving Spark Application Throughput Via Memory Aware Task\n  Co-location: A Mixture of Experts Approach", "comments": "ACM/IFIP/USENIX Middleware 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data analytic applications built upon big data processing frameworks such as\nApache Spark are an important class of applications. Many of these applications\nare not latency-sensitive and thus can run as batch jobs in data centers. By\nrunning multiple applications on a computing host, task co-location can\nsignificantly improve the server utilization and system throughput. However,\neffective task co-location is a non-trivial task, as it requires an\nunderstanding of the computing resource requirement of the co-running\napplications, in order to determine what tasks, and how many of them, can be\nco-located.\n  In this paper, we present a mixture-of-experts approach to model the memory\nbehavior of Spark applications. We achieve this by learning, off-line, a range\nof specialized memory models on a range of typical applications; we then\ndetermine at runtime which of the memory models, or experts, best describes the\nmemory behavior of the target application. We show that by accurately\nestimating the resource level that is needed, a co-location scheme can\neffectively determine how many applications can be co-located on the same host\nto improve the system throughput, by taking into consideration the memory and\nCPU requirements of co-running application tasks. Our technique is applied to a\nset of representative data analytic applications built upon the Apache Spark\nframework. We evaluated our approach for system throughput and average\nnormalized turnaround time on a multi-core cluster. Our approach achieves over\n83.9% of the performance delivered using an ideal memory predictor. We obtain,\non average, 8.69x improvement on system throughput and a 49% reduction on\nturnaround time over executing application tasks in isolation, which translates\nto a 1.28x and 1.68x improvement over a state-of-the-art co-location scheme for\nsystem throughput and turnaround time respectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 12:41:41 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Marco", "Vicent Sanz", ""], ["Taylor", "Ben", ""], ["Porter", "Barry", ""], ["Wang", "Zheng", ""]]}, {"id": "1710.00692", "submitter": "Vladimir Savic Dr", "authors": "Vladimir Savic, Elad M. Schiller, Marina Papatriantafilou", "title": "Aiding Autonomous Vehicles with Fault-tolerant V2V Communication", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.02641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle-to-vehicle (V2V) communication is a key component of the future\nautonomous driving systems. V2V can provide an improved awareness of the\nsurrounding environment, and the knowledge about the future actions of nearby\nvehicles. However, V2V communication is subject to different kind of failures\nand delays, so a distributed fault-tolerant approach is required for safe and\nefficient transportation. This work considers fully autonomous vehicles that\noperates using local sensory information, and aided with fault-tolerant V2V\ncommunication. The sensors provide all basic functionality, but are overridden\nby V2V whenever is possible to increase the efficiency. As an example scenario,\nwe consider intersection crossing (IC) with autonomous vehicles that cooperate\nvia V2V communication, and propose a fully distributed and a fault-tolerant\nalgorithm for this problem. According to our numerical results, based on a real\ndata set, we show the crossing delay is only slightly increased in the presence\nof a burst of V2V failures, and that V2V can be successfully used in most\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:15:28 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Savic", "Vladimir", ""], ["Schiller", "Elad M.", ""], ["Papatriantafilou", "Marina", ""]]}, {"id": "1710.00714", "submitter": "Ata Turk", "authors": "Ata Turk, Mayank Varia, Georgios Kellaris", "title": "Revealing the Unseen: How to Expose Cloud Usage While Protecting User\n  Privacy", "comments": "6 pages, ICDM PAIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud users have little visibility into the performance characteristics and\nutilization of the physical machines underpinning the virtualized cloud\nresources they use. This uncertainty forces users and researchers to reverse\nengineer the inner workings of cloud systems in order to understand and\noptimize the conditions their applications operate. At Massachusetts Open Cloud\n(MOC), as a public cloud operator, we'd like to expose the utilization of our\nphysical infrastructure to stop this wasteful effort. Mindful that such\nexposure can be used maliciously for gaining insight into other users\nworkloads, in this position paper we argue for the need for an approach that\nbalances openness of the cloud overall with privacy for each tenant inside of\nit. We believe that this approach can be instantiated via a novel combination\nof several security and privacy technologies. We discuss the potential\nbenefits, implications of transparency for cloud systems and users, and\ntechnical challenges/possibilities.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 15:05:25 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Turk", "Ata", ""], ["Varia", "Mayank", ""], ["Kellaris", "Georgios", ""]]}, {"id": "1710.00734", "submitter": "Ata Turk", "authors": "Rudolph Pienaar, Ata Turk, Jorge Bernal-Rusiel, Nicolas Rannou, Daniel\n  Haehn, P. Ellen Grant, and Orran Krieger", "title": "CHIPS: A Service for Collecting, Organizing, Processing, and Sharing\n  Medical Image Data in the Cloud", "comments": "5 pages, VLDB-DMAH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web browsers are increasingly used as middleware platforms offering a central\naccess point for service provision. Using backend containerization, RESTful\nAPIs, and distributed computing allows for complex systems to be realized that\naddress the needs of modern compute intense environments. In this paper, we\npresent a web-based medical image data and information management software\nplatform called CHIPS (Cloud Healthcare Image Processing Service). This\ncloud-based services allows for authenticated and secure retrieval of medical\nimage data from resources typically found in hospitals, organizes and presents\ninformation in a modern feed-like interface, provides access to a growing\nlibrary of plugins that process these data, allows for easy data sharing\nbetween users and provides powerful 3D visualization and real-time\ncollaboration. Image processing is orchestrated across additional cloud-based\nresources using containerization technologies.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 15:49:49 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Pienaar", "Rudolph", ""], ["Turk", "Ata", ""], ["Bernal-Rusiel", "Jorge", ""], ["Rannou", "Nicolas", ""], ["Haehn", "Daniel", ""], ["Grant", "P. Ellen", ""], ["Krieger", "Orran", ""]]}, {"id": "1710.00748", "submitter": "Mehmet Aktas", "authors": "Mehmet Fatih Aktas, Pei Peng, Emina Soljanin", "title": "Effective Straggler Mitigation: Which Clones Should Attack and When?", "comments": "Published at MAMA Workshop in conjunction with ACM Sigmetrics, June\n  5, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy for straggler mitigation, originally in data download and more\nrecently in distributed computing context, has been shown to be effective both\nin theory and practice. Analysis of systems with redundancy has drawn\nsignificant attention and numerous papers have studied pain and gain of\nredundancy under various service models and assumptions on the straggler\ncharacteristics. We here present a cost (pain) vs. latency (gain) analysis of\nusing simple replication or erasure coding for straggler mitigation in\nexecuting jobs with many tasks. We quantify the effect of the tail of task\nexecution times and discuss tail heaviness as a decisive parameter for the cost\nand latency of using redundancy. Specifically, we find that coded redundancy\nachieves better cost vs. latency and allows for greater achievable latency and\ncost tradeoff region compared to replication and can yield reduction in both\ncost and latency under less heavy tailed execution times. We show that delaying\nredundancy is not effective in reducing cost.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 16:04:27 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Peng", "Pei", ""], ["Soljanin", "Emina", ""]]}, {"id": "1710.00778", "submitter": "Jian Du", "authors": "Jian Du, Xue Liu and Lei Rao", "title": "Proactive Doppler Shift Compensation in Vehicular Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vehicular cyber-physical systems (CPS), safety information, including\nvehicular speed and location information, is shared among vehicles via wireless\nwaves at specific frequency. This helps control vehicle to alleviate traffic\ncongestion and road accidents. However, Doppler shift existing between vehicles\nwith high relative speed causes an apparent frequency shift for the received\nwireless wave, which consequently decreases the reliability of the recovered\nsafety information and jeopardizes the safety of vehicular CPS. Passive\nconfrontation of Doppler shift at the receiver side is not applicable due to\nmultiple Doppler shifts at each receiver. In this paper, we provide a proactive\nDoppler shift compensation algorithm based on the probabilistic graphical\nmodel. Each vehicle pre-compensates its carrier frequency individually so that\nthere is no frequency shift from the desired carrier frequency between each\npair of transceiver. The pre-compensated offset for each vehicle is computed in\na distributed fashion in order to be adaptive to the distributed and dynamic\ntopology of vehicular CPS. Besides, the updating procedure is designed in a\nbroadcasting fashion to reduce communication burden. It is rigorously proved\nthat the proposed algorithm is convergence guaranteed even for systems with\npacket drops and random communication delays. Simulations based on real map and\ntransportation data verify the accuracy and convergence property of the\nproposed algorithm. It is shown that this method achieves almost the optimal\nfrequency compensation accuracy with an error approaching the Cram\\'{e}r-Rao\nlower bound.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 02:46:31 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Du", "Jian", ""], ["Liu", "Xue", ""], ["Rao", "Lei", ""]]}, {"id": "1710.01429", "submitter": "Hao Qian", "authors": "Hao Qian", "title": "Improving Scientific Workflow with Cloud Offloading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific workflow is a powerful tool to streamline and organize\ncomputational steps of scientific application. This paper presents Emerald, a\nsystem that adds sophisticated cloud offloading capabilities to scientific\nworkflows. Emerald automatically offloads computation intensive steps of\nscientific workflow to the cloud in order to enhance workflow performance.\nEmerald provides easy-to-use APIs to help developers build cloud offloading\nenabled scientific workflows. Evaluation showed that Emerald can effectively\nreduce up to 55% of execution time for scientific applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 00:38:39 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Qian", "Hao", ""]]}, {"id": "1710.01431", "submitter": "Adithya Vadapalli", "authors": "Grigory Yaroslavtsev, Adithya Vadapalli", "title": "Massively Parallel Algorithms and Hardness for Single-Linkage Clustering\n  Under $\\ell_p$-Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present massively parallel (MPC) algorithms and hardness of approximation\nresults for computing Single-Linkage Clustering of $n$ input $d$-dimensional\nvectors under Hamming, $\\ell_1, \\ell_2$ and $\\ell_\\infty$ distances. All our\nalgorithms run in $O(\\log n)$ rounds of MPC for any fixed $d$ and achieve\n$(1+\\epsilon)$-approximation for all distances (except Hamming for which we\nshow an exact algorithm). We also show constant-factor inapproximability\nresults for $o(\\log n)$-round algorithms under standard MPC hardness\nassumptions (for sufficiently large dimension depending on the distance used).\nEfficiency of implementation of our algorithms in Apache Spark is demonstrated\nthrough experiments on a variety of datasets exhibiting speedups of several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 00:48:54 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 04:08:40 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yaroslavtsev", "Grigory", ""], ["Vadapalli", "Adithya", ""]]}, {"id": "1710.01476", "submitter": "Dimitrios Sikeridis", "authors": "Dimitrios Sikeridis, Ioannis Papapanagiotou, Bhaskar Prasad Rimal,\n  Michael Devetsikiotis", "title": "A Comparative Taxonomy and Survey of Public Cloud Infrastructure Vendors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of technology enterprises are adopting cloud-native\narchitectures to offer their web-based products, by moving away from\nprivately-owned data-centers and relying exclusively on cloud service\nproviders. As a result, cloud vendors have lately increased, along with the\nestimated annual revenue they share. However, in the process of selecting a\nprovider's cloud service over the competition, we observe a lack of universal\ncommon ground in terms of terminology, functionality of services and billing\nmodels. This is an important gap especially under the new reality of the\nindustry where each cloud provider has moved towards his own service taxonomy,\nwhile the number of specialized services has grown exponentially. This work\ndiscusses cloud services offered by four dominant, in terms of their current\nmarket share, cloud vendors. We provide a taxonomy of their services and\nsub-services that designates major service families namely computing, storage,\ndatabases, analytics, data pipelines, machine learning, and networking. The aim\nof such clustering is to indicate similarities, common design approaches and\nfunctional differences of the offered services. The outcomes are essential both\nfor individual researchers, and bigger enterprises in their attempt to identify\nthe set of cloud services that will utterly meet their needs without\ncompromises. While we acknowledge the fact that this is a dynamic industry,\nwhere new services arise constantly, and old ones experience important updates,\nthis study paints a solid image of the current offerings and gives prominence\nto the directions that cloud service providers are following.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 06:35:02 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2018 23:33:00 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Sikeridis", "Dimitrios", ""], ["Papapanagiotou", "Ioannis", ""], ["Rimal", "Bhaskar Prasad", ""], ["Devetsikiotis", "Michael", ""]]}, {"id": "1710.01800", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Varsha Dani, Thomas P. Hayes, Qizheng He, Wenzheng Li,\n  Seth Pettie", "title": "The Energy Complexity of Broadcast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy is often the most constrained resource in networks of battery-powered\ndevices, and as devices become smaller, they spend a larger fraction of their\nenergy on communication (transceiver usage) not computation. As an imperfect\nproxy for true energy usage, we define energy complexity to be the number of\ntime slots a device transmits/listens; idle time and computation are free.\n  In this paper we investigate the energy complexity of fundamental\ncommunication primitives such as broadcast in multi-hop radio networks. We\nconsider models with collision detection (CD) and without (No-CD), as well as\nboth randomized and deterministic algorithms. Some take-away messages from this\nwork include:\n  1. The energy complexity of broadcast in a multi-hop network is intimately\nconnected to the time complexity of leader election in a single-hop (clique)\nnetwork. Many existing lower bounds on time complexity immediately transfer to\nenergy complexity. For example, in the CD and No-CD models, we need\n$\\Omega(\\log n)$ and $\\Omega(\\log^2 n)$ energy, respectively.\n  2. The energy lower bounds above can almost be achieved, given sufficient\n($\\Omega(n)$) time. In the CD and No-CD models we can solve broadcast using\n$O(\\frac{\\log n\\log\\log n}{\\log\\log\\log n})$ energy and $O(\\log^3 n)$ energy,\nrespectively.\n  3. The complexity measures of Energy and Time are in conflict, and it is an\nopen problem whether both can be minimized simultaneously. We give a tradeoff\nshowing it is possible to be nearly optimal in both measures simultaneously.\nFor any constant $\\epsilon>0$, broadcast can be solved in\n$O(D^{1+\\epsilon}\\log^{O(1/\\epsilon)} n)$ time with $O(\\log^{O(1/\\epsilon)} n)$\nenergy, where $D$ is the diameter of the network.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 20:54:42 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Dani", "Varsha", ""], ["Hayes", "Thomas P.", ""], ["He", "Qizheng", ""], ["Li", "Wenzheng", ""], ["Pettie", "Seth", ""]]}, {"id": "1710.01801", "submitter": "Mohammad Shojafar", "authors": "Paola G. Vinueza Naranjo, Zahra Pooranian, Mohammad Shojafar, Mauro\n  Conti, Rajkumar Buyya", "title": "FOCAN: A Fog-supported Smart City Network Architecture for Management of\n  Applications in the Internet of Everything Environments", "comments": "Sapienza University of Rome, 3 Oct. 2017, in review, 16 pages, 6\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart city vision brings emerging heterogeneous communication technologies\nsuch as Fog Computing (FC) together to substantially reduce the latency and\nenergy consumption of Internet of Everything (IoE) devices running various\napplications. The key feature that distinguishes the FC paradigm for smart\ncities is that it spreads communication and computing resources over the\nwired/wireless access network (e.g., proximate access points and base stations)\nto provide resource augmentation (e.g., cyberforaging) for resource and\nenergy-limited wired/wireless (possibly mobile) things. Moreover, smart city\napplications are developed with the goal of improving the management of urban\nflows and allowing real-time responses to challenges that can arise in users'\ntransactional relationships. This article presents a Fog-supported smart city\nnetwork architecture called Fog Computing Architecture Network (FOCAN), a\nmulti-tier structure in which the applications running on things jointly\ncompute, route, and communicate with one another through the smart city\nenvironment to decrease latency and improve energy provisioning and the\nefficiency of services among things with different capabilities. An important\nconcern that arises with the introduction of FOCAN is the need to avoid\ntransferring data to/from distant things and instead to cover the nearest\nregion for an IoT application. We define three types of communications between\nFOCAN devices (e.g., interprimary, primary, and secondary communication) to\nmanage applications in a way that meets the quality of service standards for\nthe IoE. One of the main advantages of FOCAN is that the devices can provide\nthe services with low energy usage and in an efficient manner. Simulation\nresults for a selected case study demonstrate the tremendous impact of the\nFOCAN energy-efficient solution on the communication performance of various\ntypes of things in smart cities.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 20:55:39 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Naranjo", "Paola G. Vinueza", ""], ["Pooranian", "Zahra", ""], ["Shojafar", "Mohammad", ""], ["Conti", "Mauro", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1710.01986", "submitter": "Evan Berkowitz", "authors": "Evan Berkowitz, Gustav R. Jansen, Kenneth McElvain, Andr\\'e\n  Walker-Loud", "title": "Job Management and Task Bundling", "comments": "8 pages, 3 figures, LATTICE 2017 proceedings", "journal-ref": null, "doi": "10.1051/epjconf/201817509007", "report-no": null, "categories": "hep-lat cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Performance Computing is often performed on scarce and shared computing\nresources. To ensure computers are used to their full capacity, administrators\noften incentivize large workloads that are not possible on smaller systems.\nMeasurements in Lattice QCD frequently do not scale to machine-size workloads.\nBy bundling tasks together we can create large jobs suitable for gigantic\npartitions. We discuss METAQ and mpi_jm, software developed to dynamically\ngroup computational tasks together, that can intelligently backfill to consume\nidle time without substantial changes to users' current workflows or\nexecutables.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 12:39:49 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Berkowitz", "Evan", ""], ["Jansen", "Gustav R.", ""], ["McElvain", "Kenneth", ""], ["Walker-Loud", "Andr\u00e9", ""]]}, {"id": "1710.02260", "submitter": "Roopal Nahar", "authors": "Roopal Nahar, Akanksha Baranwal, K.Madhava Krishna", "title": "FPGA based Parallelized Architecture of Efficient Graph based Image\n  Segmentation Algorithm", "comments": "6 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and real time segmentation of color images has a variety of\nimportance in many fields of computer vision such as image compression, medical\nimaging, mapping and autonomous navigation. Being one of the most\ncomputationally expensive operation, it is usually done through software imple-\nmentation using high-performance processors. In robotic systems, however, with\nthe constrained platform dimensions and the need for portability, low power\nconsumption and simultaneously the need for real time image segmentation, we\nenvision hardware parallelism as the way forward to achieve higher\nacceleration. Field-programmable gate arrays (FPGAs) are among the best suited\nfor this task as they provide high computing power in a small physical area.\nThey exceed the computing speed of software based implementations by breaking\nthe paradigm of sequential execution and accomplishing more per clock cycle\noperations by enabling hardware level parallelization at an architectural\nlevel. In this paper, we propose three novel architectures of a well known\nEfficient Graph based Image Segmentation algorithm. These proposed\nimplementations optimizes time and power consumption when compared to software\nimplementations. The hybrid design proposed, has notable furtherance of\nacceleration capabilities delivering atleast 2X speed gain over other implemen-\ntations, which henceforth allows real time image segmentation that can be\ndeployed on Mobile Robotic systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 02:48:26 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Nahar", "Roopal", ""], ["Baranwal", "Akanksha", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1710.02282", "submitter": "Gabriele D'Angelo", "authors": "Stefano Ferretti, Gabriele D'Angelo, Vittorio Ghini, Moreno Marzolla", "title": "The Quest for Scalability and Accuracy in the Simulation of the Internet\n  of Things: an Approach based on Multi-Level Simulation", "comments": "Proceedings of the IEEE/ACM International Symposium on Distributed\n  Simulation and Real Time Applications (DS-RT 2017)", "journal-ref": null, "doi": "10.1109/DISTRA.2017.8167672", "report-no": null, "categories": "cs.PF cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a methodology for simulating the Internet of Things (IoT)\nusing multi-level simulation models. With respect to conventional simulators,\nthis approach allows us to tune the level of detail of different parts of the\nmodel without compromising the scalability of the simulation. As a use case, we\nhave developed a two-level simulator to study the deployment of smart services\nover rural territories. The higher level is base on a coarse grained,\nagent-based adaptive parallel and distributed simulator. When needed, this\nsimulator spawns OMNeT++ model instances to evaluate in more detail the issues\nconcerned with wireless communications in restricted areas of the simulated\nworld. The performance evaluation confirms the viability of multi-level\nsimulations for IoT environments.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 06:05:58 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 07:12:41 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""], ["Ghini", "Vittorio", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1710.02368", "submitter": "Joeri Hermans", "authors": "Joeri Hermans, Gerasimos Spanakis and Rico M\\\"ockel", "title": "Accumulated Gradient Normalization", "comments": "16 pages, 12 figures, ACML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work addresses the instability in asynchronous data parallel\noptimization. It does so by introducing a novel distributed optimizer which is\nable to efficiently optimize a centralized model under communication\nconstraints. The optimizer achieves this by pushing a normalized sequence of\nfirst-order gradients to a parameter server. This implies that the magnitude of\na worker delta is smaller compared to an accumulated gradient, and provides a\nbetter direction towards a minimum compared to first-order gradients, which in\nturn also forces possible implicit momentum fluctuations to be more aligned\nsince we make the assumption that all workers contribute towards a single\nminima. As a result, our approach mitigates the parameter staleness problem\nmore effectively since staleness in asynchrony induces (implicit) momentum, and\nachieves a better convergence rate compared to other optimizers such as\nasynchronous EASGD and DynSGD, which we show empirically.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 12:32:16 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Hermans", "Joeri", ""], ["Spanakis", "Gerasimos", ""], ["M\u00f6ckel", "Rico", ""]]}, {"id": "1710.02553", "submitter": "Juan Juli\\'an Merelo-Guerv\\'os Pr.", "authors": "Juan-Juli\\'an Merelo-Guerv\\'os", "title": "Artificial life, complex systems and cloud computing: a short review", "comments": "Short paper to support a Evolution in the cloud tutorial in ECAL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cloud computing is the prevailing mode of designing, creating and deploying\ncomplex applications nowadays. Its underlying assumptions include distributed\ncomputing, but also new concepts that need to be incorporated in the different\nfields. In this short paper we will make a review of how the world of cloud\ncomputing has intersected the complex systems and artificial life field, and\nhow it has been used as inspiration for new models or implementation of new and\npowerful algorithms\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 18:05:13 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Merelo-Guerv\u00f3s", "Juan-Juli\u00e1n", ""]]}, {"id": "1710.02627", "submitter": "Saurabh Hukerikar", "authors": "Saurabh Hukerikar and Christian Engelmann", "title": "Pattern-based Modeling of High-Performance Computing Resilience", "comments": "International European Conference on Parallel and Distributed\n  Computing (Euro-Par) 2017 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing scale and complexity of high-performance computing (HPC)\nsystems, resilience solutions that ensure continuity of service despite\nfrequent errors and component failures must be methodically designed to balance\nthe reliability requirements with the overheads to performance and power.\nDesign patterns enable a structured approach to the development of resilience\nsolutions, providing hardware and software designers with the building block\nelements for the rapid development of novel solutions and for adapting existing\ntechnologies for emerging, extreme-scale HPC environments. In this paper, we\ndevelop analytical models that enable designers to evaluate the reliability and\nperformance characteristics of the design patterns. These models are\nparticularly useful in building a unified framework that analyzes and compares\nvarious resilience solutions built using a combination of patterns.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 03:13:20 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Hukerikar", "Saurabh", ""], ["Engelmann", "Christian", ""]]}, {"id": "1710.02722", "submitter": "Wiktor Daszczuk", "authors": "Wiktor B. Daszczuk, Maciej Bielecki, Jan Michalski", "title": "Rybu: Imperative-style Preprocessor for Verification of Distributed\n  Systems in the Dedan Environment", "comments": "16 pages, 1 figure", "journal-ref": "KKIO 2017 - Software Engineering Conference, Rzesz\\'ow, Poland,\n  14-16 Sept 2017, Software Engineering Research for the Practice, Polish\n  Information Processing Society, pp. 135-150", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Model of Distributed Systems (IMDS) is developed for specification\nand verification of distributed systems, and verification against deadlocks. On\nthe basis of IMDS, Dedan verification environment was prepared. Universal\ndeadlock detection formulas allow for automatic verification, without any\nknowledge of a temporal logic, which simplifies the verification process.\nHowever, the input language, following the rules of IMDS, seems to be exotic\nfor many users. For this reason Rybu preprocessor was created. Its purpose is\nto build large models in imperative-style language, on much higher abstraction\nlevel.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 18:57:50 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Daszczuk", "Wiktor B.", ""], ["Bielecki", "Maciej", ""], ["Michalski", "Jan", ""]]}, {"id": "1710.03013", "submitter": "Marco Jacopo Ferrarotti", "authors": "Marco Jacopo Ferrarotti, Sergio Decherchi and Walter Rocchia", "title": "Distributed Kernel K-Means for Large Scale Clustering", "comments": "Conference paper", "journal-ref": "3rd International Conference on Artificial Intelligence and Soft\n  Computing, Computer Science & Information Technology, AIRCC, Vol 7, Number\n  10, August 2017", "doi": "10.5121/csit.2017.71015", "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering samples according to an effective metric and/or vector space\nrepresentation is a challenging unsupervised learning task with a wide spectrum\nof applications. Among several clustering algorithms, k-means and its\nkernelized version have still a wide audience because of their conceptual\nsimplicity and efficacy. However, the systematic application of the kernelized\nversion of k-means is hampered by its inherent square scaling in memory with\nthe number of samples. In this contribution, we devise an approximate strategy\nto minimize the kernel k-means cost function in which the trade-off between\naccuracy and velocity is automatically ruled by the available system memory.\nMoreover, we define an ad-hoc parallelization scheme well suited for hybrid\ncpu-gpu state-of-the-art parallel architectures. We proved the effectiveness\nboth of the approximation scheme and of the parallelization method on standard\nUCI datasets and on molecular dynamics (MD) data in the realm of computational\nchemistry. In this applicative domain, clustering can play a key role for both\nquantitively estimating kinetics rates via Markov State Models or to give\nqualitatively a human compatible summarization of the underlying chemical\nphenomenon under study. For these reasons, we selected it as a valuable\nreal-world application scenario.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 09:55:24 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Ferrarotti", "Marco Jacopo", ""], ["Decherchi", "Sergio", ""], ["Rocchia", "Walter", ""]]}, {"id": "1710.03040", "submitter": "Ricardo Morla", "authors": "Eduardo Rodrigues, Ricardo Morla", "title": "Run Time Prediction for Big Data Iterative ML Algorithms: a KMeans case\n  study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science and machine learning algorithms running on big data\ninfrastructure are increasingly important in activities ranging from business\nintelligence and analytics to cybersecurity, smart city management, and many\nfields of science and engineering. As these algorithms are further integrated\ninto daily operations, understanding how long they take to run on a big data\ninfrastructure is paramount to controlling costs and delivery times. In this\npaper we discuss the issues involved in understanding the run time of iterative\nmachine learning algorithms and provide a case study of such an algorithm -\nincluding a statistical characterization and model of the run time of an\nimplementation of K-Means for the Spark big data engine using the Edward\nprobabilistic programming language.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 11:40:36 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Rodrigues", "Eduardo", ""], ["Morla", "Ricardo", ""]]}, {"id": "1710.03178", "submitter": "Avery Miller", "authors": "Faith Ellen, Barun Gorain, Avery Miller, Andrzej Pelc", "title": "Constant-Length Labeling Schemes for Deterministic Radio Broadcast", "comments": "updated version of the paper that appeared in SPAA 2019: now contains\n  a solution for the case where the source node is not designated when the\n  labeling scheme is applied", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadcast is one of the fundamental network communication primitives. One\nnode of a network, called the $\\mathit{source}$, has a message that has to be\nlearned by all other nodes. We consider the feasibility of deterministic\nbroadcast in radio networks. If nodes of the network do not have any labels,\ndeterministic broadcast is impossible even in the four-cycle. On the other\nhand, if all nodes have distinct labels, then broadcast can be carried out,\ne.g., in a round-robin fashion, and hence $O(\\log n)$-bit labels are sufficient\nfor this task in $n$-node networks. In fact, $O(\\log \\Delta)$-bit labels, where\n$\\Delta$ is the maximum degree, are enough to broadcast successfully. Hence, it\nis natural to ask if very short labels are sufficient for broadcast. Our main\nresult is a positive answer to this question. We show that every radio network\ncan be labeled using 2 bits in such a way that broadcast can be accomplished by\nsome universal deterministic algorithm that does not know the network topology\nnor any bound on its size. Moreover, at the expense of an extra bit in the\nlabels, we get the additional strong property that there exists a common round\nin which all nodes know that broadcast has been completed. Finally, we show\nthat 3-bit labels are also sufficient to solve both versions of broadcast in\nthe case where the labeling scheme does not know which node is the source.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 16:24:10 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 20:37:18 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Ellen", "Faith", ""], ["Gorain", "Barun", ""], ["Miller", "Avery", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1710.03197", "submitter": "Jyh-Miin Lin", "authors": "Jyh-Miin Lin", "title": "Python Non-Uniform Fast Fourier Transform (PyNUFFT): multi-dimensional\n  non-Cartesian image reconstruction package for heterogeneous platforms and\n  applications to MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the development of a Python Non-Uniform Fast Fourier\nTransform (PyNUFFT) package, which accelerates non-Cartesian image\nreconstruction on heterogeneous platforms. Scientific computing with Python\nencompasses a mature and integrated environment. The NUFFT algorithm has been\nextensively used for non-Cartesian image reconstruction but previously there\nwas no native Python NUFFT library. The current PyNUFFT software enables\nmulti-dimensional NUFFT on heterogeneous platforms. The PyNUFFT also provides\nseveral solvers, including the conjugate gradient method, $\\ell$1\ntotal-variation regularized ordinary least square (L1TV-OLS) and $\\ell$1\ntotal-variation regularized least absolute deviation (L1TV-LAD).\nMetaprogramming libraries were employed to accelerate PyNUFFT. The PyNUFFT\npackage has been tested on multi-core CPU and GPU, with acceleration factors of\n6.3 - 9.5$\\times$ on a 32 thread CPU platform and 5.4 - 13$\\times$ on the GPU.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 17:12:46 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Lin", "Jyh-Miin", ""]]}, {"id": "1710.03439", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Jianxun Liu, Mengying Guo, Yungang Bao, Wenlong Ma,\n  Zhuoyue Liu, Kunpeng Song, Yingchun Yang", "title": "BestConfig: Tapping the Performance Potential of Systems via Automatic\n  Configuration Tuning", "comments": null, "journal-ref": "ACM SoCC 2017", "doi": "10.1145/3127479.3128605", "report-no": null, "categories": "cs.PF cs.DB cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ever increasing number of configuration parameters are provided to system\nusers. But many users have used one configuration setting across different\nworkloads, leaving untapped the performance potential of systems. A good\nconfiguration setting can greatly improve the performance of a deployed system\nunder certain workloads. But with tens or hundreds of parameters, it becomes a\nhighly costly task to decide which configuration setting leads to the best\nperformance. While such task requires the strong expertise in both the system\nand the application, users commonly lack such expertise.\n  To help users tap the performance potential of systems, we present\nBestConfig, a system for automatically finding a best configuration setting\nwithin a resource limit for a deployed system under a given application\nworkload. BestConfig is designed with an extensible architecture to automate\nthe configuration tuning for general systems. To tune system configurations\nwithin a resource limit, we propose the divide-and-diverge sampling method and\nthe recursive bound-and-search algorithm. BestConfig can improve the throughput\nof Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce\nthe running time of Hive join job by about 50% and that of Spark join job by\nabout 80%, solely by configuration adjustment.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 08:10:06 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Zhu", "Yuqing", ""], ["Liu", "Jianxun", ""], ["Guo", "Mengying", ""], ["Bao", "Yungang", ""], ["Ma", "Wenlong", ""], ["Liu", "Zhuoyue", ""], ["Song", "Kunpeng", ""], ["Yang", "Yingchun", ""]]}, {"id": "1710.03559", "submitter": "Zheng Wang", "authors": "Jie Ren, Ling Gao, Hai Wang, Zheng Wang", "title": "Energy-aware Web Browsing on Heterogeneous Mobile Platforms", "comments": "Appeared at IEEE INFOCOM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Web browsing is an activity that billions of mobile users perform on a daily\nbasis. Battery life is a primary concern to many mobile users who often find\ntheir phone has died at most inconvenient times. The heterogeneous multi-core\narchitecture is a solution for energy-efficient processing. However, the\ncurrent mobile web browsers rely on the operating system to exploit the\nunderlying hardware, which has no knowledge of individual web contents and\noften leads to poor energy efficiency. This paper describes an automatic\napproach to render mobile web workloads for performance and energy efficiency.\nIt achieves this by developing a machine learning based approach to predict\nwhich processor to use to run the web rendering engine and at what frequencies\nthe processors should operate. Our predictor learns offline from a set of\ntraining web workloads. The built predictor is then integrated into the browser\nto predict the optimal processor configuration at runtime, taking into account\nthe web workload characteristics and the optimisation goal: whether it is load\ntime, energy consumption or a trade-off between them. We evaluate our approach\non a representative ARM big.LITTLE mobile architecture using the hottest 500\nwebpages. Our approach achieves 80% of the performance delivered by an ideal\npredictor. We obtain, on average, 45%, 63.5% and 81% improvement respectively\nfor load time, energy consumption and the energy delay product, when compared\nto the Linux heterogeneous multi-processing scheduler.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 09:33:01 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Ren", "Jie", ""], ["Gao", "Ling", ""], ["Wang", "Hai", ""], ["Wang", "Zheng", ""]]}, {"id": "1710.03647", "submitter": "Andrea Formisano", "authors": "Andrea Formisano, Raffaella Gentilini, Flavio Vella", "title": "Accelerating Energy Games Solvers on Modern Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative games, where quantitative objectives are defined on weighted\ngame arenas, provide natural tools for designing faithful models of embedded\ncontrollers. Instances of these games that recently gained interest are the so\ncalled Energy Games. The fast-known algorithm solves Energy Games in O(EVW)\nwhere W is the maximum weight. Starting from a sequential baseline\nimplementation, we investigate the use of massively data computation\ncapabilities supported by modern Graphics Processing Units to solve the\n`initial credit problem' for Energy Games. We present four different parallel\nimplementations on multi-core CPU and GPU systems. Our solution outperforms the\nbaseline implementation by up to 36x speedup and obtains a faster convergence\ntime on real-world graphs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 15:15:32 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Formisano", "Andrea", ""], ["Gentilini", "Raffaella", ""], ["Vella", "Flavio", ""]]}, {"id": "1710.03649", "submitter": "Javad Zarrin", "authors": "Javad Zarrin, Rui L. Aguiar, Joao Paulo Barraca", "title": "Decentralized Resource Discovery and Management for Future Manycore\n  Systems", "comments": "46 pages, 18 figures", "journal-ref": null, "doi": "10.1016/j.micpro.2016.06.007", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation of many-core enabled large-scale computing systems relies\non thousands of billions of heterogeneous processing cores connected to form a\nsingle computing unit. In such large-scale computing environments, resource\nmanagement is one of the most challenging, and complex issues for efficient\nresource sharing and utilization, particularly as we move toward Future\nManyCore Systems (FMCS). This work proposes a novel resource management scheme\nfor future peta-scale many-core-enabled computing systems, based on hybrid\nadaptive resource discovery, called ElCore. The proposed architecture contains\na set of modules which will dynamically be instantiated on the nodes in the\ndistributed system on demand. Our approach provides flexibility to allocate the\nrequired set of resources for various types of processes/applications. It can\nalso be considered as a generic solution (with respect to the general\nrequirements of large scale computing environments) which brings a set of\ninteresting features (such as auto-scaling, multitenancy, multi-dimensional\nmapping, etc,.) to facilitate its easy adaptation to any distributed technology\n(such as SOA, Grid and HPC many-core). The achieved evaluation results assured\nthe significant scalability and the high quality resource mapping of the\nproposed resource discovery and management over highly heterogeneous,\nhierarchical and dynamic computing environments with respect to several\nscalability and efficiency aspects while supporting flexible and complex\nqueries with guaranteed discovery results accuracy. The simulation results\nprove that, using our approach, the mapping between processes and resources can\nbe done with high level of accuracy which potentially leads to a significant\nenhancement in the overall system performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 15:18:06 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Zarrin", "Javad", ""], ["Aguiar", "Rui L.", ""], ["Barraca", "Joao Paulo", ""]]}, {"id": "1710.03732", "submitter": "Ketan Date", "authors": "Ketan Date and Rakesh Nagi", "title": "RLT2-based Parallel Algorithms for Solving Large Quadratic Assignment\n  Problems on Graphics Processing Unit Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses efficient parallel algorithms for obtaining strong lower\nbounds and exact solutions for large instances of the Quadratic Assignment\nProblem (QAP). Our parallel architecture is comprised of both multi-core\nprocessors and Compute Unified Device Architecture (CUDA) enabled NVIDIA\nGraphics Processing Units (GPUs) on the Blue Waters Supercomputing Facility at\nthe University of Illinois at Urbana-Champaign. We propose novel\nparallelization of the Lagrangian Dual Ascent algorithm on the GPUs, which is\nused for solving a QAP formulation based on Level-2 Refactorization\nLinearization Technique (RLT2). The Linear Assignment sub-problems (LAPs) in\nthis procedure are solved using our accelerated Hungarian algorithm [Date,\nKetan, Rakesh Nagi. 2016. GPU-accelerated Hungarian algorithms for the Linear\nAssignment Problem. Parallel Computing 57 52-72]. We embed this accelerated\ndual ascent algorithm in a parallel branch-and-bound scheme and conduct\nextensive computational experiments on single and multiple GPUs, using problem\ninstances with up to 42 facilities from the QAPLIB. The experiments suggest\nthat our GPU-based approach is scalable and it can be used to obtain tight\nlower bounds on large QAP instances. Our accelerated branch-and-bound scheme is\nable to comfortably solve Nugent and Taillard instances (up to 30 facilities)\nfrom the QAPLIB, using modest number of GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 17:22:04 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Date", "Ketan", ""], ["Nagi", "Rakesh", ""]]}, {"id": "1710.03928", "submitter": "Christopher M. Poskitt", "authors": "Claudio Corrodi, Alexander Heu{\\ss}ner, Christopher M. Poskitt", "title": "A Semantics Comparison Workbench for a Concurrent, Asynchronous,\n  Distributed Programming Language", "comments": "Accepted by Formal Aspects of Computing", "journal-ref": "Formal Asp. Comput. 30(1): 163-192 (2018)", "doi": "10.1007/s00165-017-0443-1", "report-no": null, "categories": "cs.SE cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of high-level languages and libraries have been proposed that offer\nnovel and simple to use abstractions for concurrent, asynchronous, and\ndistributed programming. The execution models that realise them, however, often\nchange over time---whether to improve performance, or to extend them to new\nlanguage features---potentially affecting behavioural and safety properties of\nexisting programs. This is exemplified by SCOOP, a message-passing approach to\nconcurrent object-oriented programming that has seen multiple changes proposed\nand implemented, with demonstrable consequences for an idiomatic usage of its\ncore abstraction. We propose a semantics comparison workbench for SCOOP with\nfully and semi-automatic tools for analysing and comparing the state spaces of\nprograms with respect to different execution models or semantics. We\ndemonstrate its use in checking the consistency of properties across semantics\nby applying it to a set of representative programs, and highlighting a\ndeadlock-related discrepancy between the principal execution models of SCOOP.\nFurthermore, we demonstrate the extensibility of the workbench by generalising\nthe formalisation of an execution model to support recently proposed extensions\nfor distributed programming. Our workbench is based on a modular and\nparameterisable graph transformation semantics implemented in the GROOVE tool.\nWe discuss how graph transformations are leveraged to atomically model\nintricate language abstractions, how the visual yet algebraic nature of the\nmodel can be used to ascertain soundness, and highlight how the approach could\nbe applied to similar languages.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 06:36:58 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Corrodi", "Claudio", ""], ["Heu\u00dfner", "Alexander", ""], ["Poskitt", "Christopher M.", ""]]}, {"id": "1710.03940", "submitter": "Denis Demidov", "authors": "Denis Demidov and Riccardo Rossi", "title": "Subdomain Deflation Combined with Local AMG: a Case Study Using AMGCL\n  Library", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": "10.1134/S1995080220040071", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a combination of the subdomain deflation method and local\nalgebraic multigrid as a scalable distributed memory preconditioner that is\nable to solve large linear systems of equations. The implementation of the\nalgorithm is made available for the community as part of an open source AMGCL\nlibrary. The solution targets both homogeneous (CPU-only) and heterogeneous\n(CPU/GPU) systems, employing hybrid MPI/OpenMP approach in the former and a\ncombination of MPI, OpenMP, and CUDA in the latter cases. The use of OpenMP\nminimizes the number of MPI processes, thus reducing the communication overhead\nof the deflation method and improving both weak and strong scalability of the\npreconditioner. The examples of scalar, Poisson-like, systems as well as\nnon-scalar problems, stemming out of the discretization of the Navier-Stokes\nequations, are considered in order to estimate performance of the implemented\nalgorithm. A comparison with a traditional global AMG preconditioner based on a\nwell-established Trilinos ML package is provided.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 07:25:16 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 10:54:14 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 08:19:38 GMT"}, {"version": "v4", "created": "Fri, 26 Oct 2018 06:20:18 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Demidov", "Denis", ""], ["Rossi", "Riccardo", ""]]}, {"id": "1710.04049", "submitter": "Nane Kratzke", "authors": "Nane Kratzke", "title": "About Microservices, Containers and their Underestimated Impact on\n  Network Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservices are used to build complex applications composed of small,\nindependent and highly decoupled processes. Recently, microservices are often\nmentioned in one breath with container technologies like Docker. That is why\noperating system virtualization experiences a renaissance in cloud computing.\nThese approaches shall provide horizontally scalable, easily deployable systems\nand a high-performance alternative to hypervisors. Nevertheless, performance\nimpacts of containers on top of hypervisors are hardly investigated.\nFurthermore, microservice frameworks often come along with software defined\nnetworks. This contribution presents benchmark results to quantify the impacts\nof container, software defined networking and encryption on network\nperformance. Even containers, although postulated to be lightweight, show a\nnoteworthy impact to network performance. These impacts can be minimized on\nseveral system layers. Some design recommendations for cloud deployed systems\nfollowing the microservice architecture pattern are derived.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 16:41:04 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Kratzke", "Nane", ""]]}, {"id": "1710.04062", "submitter": "Alec Koppel", "authors": "Alec Koppel, Santiago Paternain, Cedric Richard, Alejandro Ribeiro", "title": "Decentralized Online Learning with Kernels", "comments": "Submitted to IEEE TSP. Partial results appear in 2017 IEEE GlobalSIP.\n  arXiv admin note: text overlap with arXiv:1612.04111", "journal-ref": null, "doi": "10.1109/TSP.2018.2830299", "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-agent stochastic optimization problems over reproducing\nkernel Hilbert spaces (RKHS). In this setting, a network of interconnected\nagents aims to learn decision functions, i.e., nonlinear statistical models,\nthat are optimal in terms of a global convex functional that aggregates data\nacross the network, with only access to locally and sequentially observed\nsamples. We propose solving this problem by allowing each agent to learn a\nlocal regression function while enforcing consensus constraints. We use a\npenalized variant of functional stochastic gradient descent operating\nsimultaneously with low-dimensional subspace projections. These subspaces are\nconstructed greedily by applying orthogonal matching pursuit to the sequence of\nkernel dictionaries and weights. By tuning the projection-induced bias, we\npropose an algorithm that allows for each individual agent to learn, based upon\nits locally observed data stream and message passing with its neighbors only, a\nregression function that is close to the globally optimal regression function.\nThat is, we establish that with constant step-size selections agents' functions\nconverge to a neighborhood of the globally optimal one while satisfying the\nconsensus constraints as the penalty parameter is increased. Moreover, the\ncomplexity of the learned regression functions is guaranteed to remain finite.\nOn both multi-class kernel logistic regression and multi-class kernel support\nvector classification with data generated from class-dependent Gaussian mixture\nmodels, we observe stable function estimation and state of the art performance\nfor distributed online multi-class classification. Experiments on the Brodatz\ntextures further substantiate the empirical validity of this approach.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 13:49:28 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Koppel", "Alec", ""], ["Paternain", "Santiago", ""], ["Richard", "Cedric", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1710.04094", "submitter": "Thomas R\\\"ohl", "authors": "Thomas R\\\"ohl, Jan Eitzinger, Georg Hager and Gerhard Wellein", "title": "Validation of hardware events for successful performance pattern\n  identification in High Performance Computing", "comments": null, "journal-ref": "Tools for High Performance Computing 2015", "doi": "10.1007/978-3-319-39589-0_2", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware performance monitoring (HPM) is a crucial ingredient of performance\nanalysis tools. While there are interfaces like LIKWID, PAPI or the kernel\ninterface perf\\_event which provide HPM access with some additional features,\nmany higher level tools combine event counts with results retrieved from other\nsources like function call traces to derive (semi-)automatic performance\nadvice. However, although HPM is available for x86 systems since the early 90s,\nonly a small subset of the HPM features is used in practice. Performance\npatterns provide a more comprehensive approach, enabling the identification of\nvarious performance-limiting effects. Patterns address issues like bandwidth\nsaturation, load imbalance, non-local data access in ccNUMA systems, or false\nsharing of cache lines. This work defines HPM event sets that are best suited\nto identify a selection of performance patterns on the Intel Haswell processor.\nWe validate the chosen event sets for accuracy in order to arrive at a reliable\npattern detection mechanism and point out shortcomings that cannot be easily\ncircumvented due to bugs or limitations in the hardware.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 14:40:04 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["R\u00f6hl", "Thomas", ""], ["Eitzinger", "Jan", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1710.04162", "submitter": "Adam Stooke", "authors": "Adam Stooke and Pieter Abbeel", "title": "Synkhronos: a Multi-GPU Theano Extension for Data Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Synkhronos, an extension to Theano for multi-GPU computations\nleveraging data parallelism. Our framework provides automated execution and\nsynchronization across devices, allowing users to continue to write serial\nprograms without risk of race conditions. The NVIDIA Collective Communication\nLibrary is used for high-bandwidth inter-GPU communication. Further\nenhancements to the Theano function interface include input slicing (with\naggregation) and input indexing, which perform common data-parallel computation\npatterns efficiently. One example use case is synchronous SGD, which has\nrecently been shown to scale well for a growing set of deep learning problems.\nWhen training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA\nDGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in\nisolation. Yet Synkhronos remains general to any data-parallel computation\nprogrammable in Theano. By implementing parallelism at the level of individual\nTheano functions, our framework uniquely addresses a niche between manual\nmulti-device programming and prescribed multi-GPU training routines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 16:38:58 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Stooke", "Adam", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1710.04252", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Vittorio Ghini", "title": "Distributed Hybrid Simulation of the Internet of Things and Smart\n  Territories", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.04876", "journal-ref": "Concurrency and Computation: Practice and Experience, Wiley, vol.\n  30, issue 9 (May 2018). ISSN: 1532-0634", "doi": "10.1002/cpe.4370", "report-no": null, "categories": "cs.CY cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the use of hybrid simulation to build and compose\nheterogeneous simulation scenarios that can be proficiently exploited to model\nand represent the Internet of Things (IoT). Hybrid simulation is a methodology\nthat combines multiple modalities of modeling/simulation. Complex scenarios are\ndecomposed into simpler ones, each one being simulated through a specific\nsimulation strategy. All these simulation building blocks are then synchronized\nand coordinated. This simulation methodology is an ideal one to represent IoT\nsetups, which are usually very demanding, due to the heterogeneity of possible\nscenarios arising from the massive deployment of an enormous amount of sensors\nand devices. We present a use case concerned with the distributed simulation of\nsmart territories, a novel view of decentralized geographical spaces that,\nthanks to the use of IoT, builds ICT services to manage resources in a way that\nis sustainable and not harmful to the environment. Three different simulation\nmodels are combined together, namely, an adaptive agent-based parallel and\ndistributed simulator, an OMNeT++ based discrete event simulator and a\nscript-language simulator based on MATLAB. Results from a performance analysis\nconfirm the viability of using hybrid simulation to model complex IoT\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 08:40:02 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 09:14:45 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Ghini", "Vittorio", ""]]}, {"id": "1710.04352", "submitter": "Hao Qian", "authors": "Hao Qian", "title": "Enhanced Mobile Computing Experience with Cloud Offloading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for increased performance of mobile device directly conflicts with\nthe desire for longer battery life. Offloading compu-tation to multiple devices\nis an effective method to reduce energy consumption and enhance performance for\nmobile applications. Android provides mechanisms for creating mobile\napplications but lacks a native scheduling system for determining where code\nshould be executed. This paper presents Jade, a system that adds sophisticated\nenergy-aware computation offloading capabilities to Android apps. Jade monitors\ndevice and application status and automatically decides where code should be\nexecuted. Jade dynamically adjusts offloading strategy by adapting to workload\nvariation, communication costs, and energy status in a distributed network of\nAndroid and non-Android devices. Jade minimizes the burden on developers to\nbuild applications with computation offloading ability by providing easy-to-use\nJade API. Evaluation shows that Jade can effectively reduce up to 39% of\naverage power consumption for mobile application while improving application\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 03:27:00 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Qian", "Hao", ""]]}, {"id": "1710.04357", "submitter": "Xingyu Zhou", "authors": "Xingyu Zhou, Fei Wu, Jian Tan, Yin Sun and Ness Shroff", "title": "Designing Low-Complexity Heavy-Traffic Delay-Optimal Load Balancing\n  Schemes: Theory to Algorithms", "comments": "42 pages, 34 figure, this is the technical report for a paper\n  accepted by ACM Sigmetrics 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We establish a unified analytical framework for load balancing systems, which\nallows us to construct a general class $\\Pi$ of policies that are both\nthroughput optimal and heavy-traffic delay optimal. This general class $\\Pi$\nincludes as special cases popular policies such as join-shortest-queue and\npower-of-$d$, but not the join-idle-queue (JIQ) policy. In fact, we show that\nJIQ, which is not in $\\Pi$, is actually not heavy-traffic delay optimal. Owing\nto the significant flexibility offered by class $\\Pi$, we are able to design a\nnew policy called join-below-threshold (JBT-d), which maintains the simplicity\nof pull-based policies such as JIQ, but updates its threshold dynamically. We\nprove that JBT-$d$ belongs to the class $\\Pi$ when the threshold is picked\nappropriately and thus it is heavy-traffic delay optimal. Extensive simulations\nshow that the new policy not only has a low complexity in message rates, but\nalso achieves excellent delay performance, comparable to the optimal\njoin-shortest-queue in various system settings.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 03:56:50 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Zhou", "Xingyu", ""], ["Wu", "Fei", ""], ["Tan", "Jian", ""], ["Sun", "Yin", ""], ["Shroff", "Ness", ""]]}, {"id": "1710.04469", "submitter": "Ali Shoker", "authors": "Carlos Baquero, Paulo Sergio Almeida and Ali Shoker", "title": "Pure Operation-Based Replicated Data Types", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems designed to serve clients across the world often make use\nof geo-replication to attain low latency and high availability. Conflict-free\nReplicated Data Types (CRDTs) allow the design of predictable multi-master\nreplication and support eventual consistency of replicas that are allowed to\ntransiently diverge. CRDTs come in two flavors: state-based, where a state is\nchanged locally and shipped and merged into other replicas; operation-based,\nwhere operations are issued locally and reliably causal broadcast to all other\nreplicas. However, the standard definition of op-based CRDTs is very\nencompassing, allowing even sending the full-state, and thus imposing storage\nand dissemination overheads as well as blurring the distinction from\nstate-based CRDTs. We introduce pure op-based CRDTs, that can only send\noperations to other replicas, drawing a clear distinction from state-based\nones. Data types with commutative operations can be trivially implemented as\npure op-based CRDTs using standard reliable causal delivery; whereas data types\nhaving non-commutative operations are implemented using a PO-Log, a partially\nordered log of operations, and making use of an extended API, i.e., a Tagged\nCausal Stable Broadcast (TCSB), that provides extra causality information upon\ndelivery and later informs when delivered messages become causally stable,\nallowing further PO-Log compaction. The framework is illustrated by a catalog\nof pure op-based specifications for classic CRDTs, including counters,\nmulti-value registers, add-wins and remove-wins sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:18:30 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Baquero", "Carlos", ""], ["Almeida", "Paulo Sergio", ""], ["Shoker", "Ali", ""]]}, {"id": "1710.04685", "submitter": "Ismail Akturk", "authors": "Ismail Akturk, Ulya R. Karpuzcu", "title": "Recomputation Enabled Efficient Checkpointing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic checkpointing of the machine state makes restart of execution from\na safe state possible upon detection of an error. The time and energy overhead\nof checkpointing, however, grows with the frequency of checkpointing.\nAmortizing this overhead becomes especially challenging, considering the growth\nof expected error rates, as checkpointing frequency tends to increase with\nincreasing error rates. Based on the observation that due to imbalanced\ntechnology scaling, recomputing a data value can be more energy efficient than\nretrieving (i.e., loading) a stored copy, this paper explores how recomputation\nof data values (which otherwise would be read from a checkpoint from memory or\nsecondary storage) can reduce the machine state to be checkpointed, and thereby\nreduce the checkpointing overhead. Specifically, the resulting amnesic\ncheckpointing framework AmnesiCHK can reduce the storage overhead by up to\n23.91%; time overhead, by 11.92%; and energy overhead, by 12.53%, respectively,\neven in a relatively small scale system.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 21:35:09 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 22:20:32 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Akturk", "Ismail", ""], ["Karpuzcu", "Ulya R.", ""]]}, {"id": "1710.04839", "submitter": "John Wickerson", "authors": "Nathan Chong, Tyler Sorensen, John Wickerson", "title": "The Semantics of Transactions and Weak Memory in x86, Power, ARM, and\n  C++", "comments": null, "journal-ref": "Proceedings of 39th ACM SIGPLAN Conference on Programming Language\n  Design and Implementation (PLDI'18), ACM, New York, NY, USA. 2018", "doi": "10.1145/3192366.3192373", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak memory models provide a complex, system-centric semantics for concurrent\nprograms, while transactional memory (TM) provides a simpler,\nprogrammer-centric semantics. Both have been studied in detail, but their\ncombined semantics is not well understood. This is problematic because such\nwidely-used architectures and languages as x86, Power, and C++ all support TM,\nand all have weak memory models.\n  Our work aims to clarify the interplay between weak memory and TM by\nextending existing axiomatic weak memory models (x86, Power, ARMv8, and C++)\nwith new rules for TM. Our formal models are backed by automated tooling that\nenables (1) the synthesis of tests for validating our models against existing\nimplementations and (2) the model-checking of TM-related transformations, such\nas lock elision and compiling C++ transactions to hardware. A key finding is\nthat a proposed TM extension to ARMv8 currently being considered within ARM\nResearch is incompatible with lock elision without sacrificing portability or\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 08:36:16 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 21:19:34 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Chong", "Nathan", ""], ["Sorensen", "Tyler", ""], ["Wickerson", "John", ""]]}, {"id": "1710.04890", "submitter": "Ruben Mayer", "authors": "Thomas Bach, Muhammad Adnan Tariq, Ruben Mayer and Kurt Rothermel", "title": "Knowledge is at the Edge! How to Search in Distributed Machine Learning\n  Models", "comments": "Published in CoopIS 2017", "journal-ref": null, "doi": "10.1007/978-3-319-69462-7_27", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the Internet of Things and Industry 4.0 an enormous amount\nof data is produced at the edge of the network. Due to a lack of computing\npower, this data is currently send to the cloud where centralized machine\nlearning models are trained to derive higher level knowledge. With the recent\ndevelopment of specialized machine learning hardware for mobile devices, a new\nera of distributed learning is about to begin that raises a new research\nquestion: How can we search in distributed machine learning models? Machine\nlearning at the edge of the network has many benefits, such as low-latency\ninference and increased privacy. Such distributed machine learning models can\nalso learn personalized for a human user, a specific context, or application\nscenario. As training data stays on the devices, control over possibly\nsensitive data is preserved as it is not shared with a third party. This new\nform of distributed learning leads to the partitioning of knowledge between\nmany devices which makes access difficult. In this paper we tackle the problem\nof finding specific knowledge by forwarding a search request (query) to a\ndevice that can answer it best. To that end, we use a entropy based quality\nmetric that takes the context of a query and the learning quality of a device\ninto account. We show that our forwarding strategy can achieve over 95%\naccuracy in a urban mobility scenario where we use data from 30 000 people\ncommuting in the city of Trento, Italy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 12:22:03 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Bach", "Thomas", ""], ["Tariq", "Muhammad Adnan", ""], ["Mayer", "Ruben", ""], ["Rothermel", "Kurt", ""]]}, {"id": "1710.04919", "submitter": "Carla Mouradian", "authors": "Carla Mouradian, Sami Yangui, Roch H. Glitho", "title": "Robots as-a-Service in Cloud Computing: Search and Rescue in Large-scale\n  Disasters Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) is expected to enable a myriad of applications by\ninterconnecting objects - such as sensors and robots - over the Internet. IoT\napplications range from healthcare to autonomous vehicles and include disaster\nmanagement. Enabling these applications in cloud environments requires the\ndesign of appropriate IoT Infrastructure-as-a-Service (IoT IaaS) to ease the\nprovisioning of the IoT objects as cloud services. This paper discusses a case\nstudy on search and rescue IoT applications in large-scale disaster scenarios.\nIt proposes an IoT IaaS architecture that virtualizes robots (IaaS for robots)\nand provides them to the upstream applications as-a-Service. Node- and\nNetwork-level robots virtualization are supported. The proposed architecture\nmeets a set of identified requirements, such as the need for a unified\ndescription model for heterogeneous robots, publication/discovery mechanism,\nand federation with other IaaS for robots when needed. A validating proof of\nconcept is built and experiments are made to evaluate its performance. Lessons\nlearned and prospective research directions are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 13:36:36 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 14:45:42 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 17:20:01 GMT"}, {"version": "v4", "created": "Sat, 4 Nov 2017 00:02:26 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Mouradian", "Carla", ""], ["Yangui", "Sami", ""], ["Glitho", "Roch H.", ""]]}, {"id": "1710.05246", "submitter": "Don Krieger", "authors": "Don Krieger, Paul Shepard, Ben Zusman, Anirban Jana, David O. Okonkwo", "title": "Shared High Value Research Resources: The CamCAN Human Lifespan\n  Neuroimaging Dataset Processed on the Open Science Grid", "comments": "8 pages, 7 figures; Proceedings of the 2017 IEEE International\n  Conference on Bioinformatics and Biomedicine; Keynote to The International\n  Workshop on High Throughput Computing in Bioinformatics and Biomedicine using\n  the Open Science Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CamCAN Lifespan Neuroimaging Dataset, Cambridge (UK) Centre for Ageing\nand Neuroscience, was acquired and processed beginning in December, 2016. The\nreferee consensus solver deployed to the Open Science Grid was used for this\ntask. The dataset includes demographic and screening measures, a\nhigh-resolution MRI scan of the brain, and whole-head magnetoencephalographic\n(MEG) recordings during eyes closed rest (560 sec), a simple task (540 sec),\nand passive listening/viewing (140 sec). The data were collected from 619\nneurologically normal individuals, ages 18-87. The processed results from the\nresting recordings are completed and available online. These constitute 1.7\nTBytes of data including the location within the brain (1 mm resolution), time\nstamp (1 msec resolution), and 80 msec time course for each of 3.7 billion\nvalidated neuroelectric events, i.e. mean 6.1 million events for each of the\n619 participants.\n  The referee consensus solver provides high yield (mean 11,000 neuroelectric\ncurrents/sec; standard deviation (sd): 3500/sec) high confidence (p < 10-12 for\neach identified current) measures of the neuroelectric currents whose magnetic\nfields are detected in the MEG recordings. We describe the solver, the\nimplementation of the solver deployed on the Open Science Grid, the workflow\nmanagement system, the opportunistic use of high performance computing (HPC)\nresources to add computing capacity to the Open Science Grid reserved for this\nproject, and our initial findings from the recently completed processing of the\nresting recordings. This required 14 million core hours, i.e. 40 core hours per\nsecond of data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 23:16:17 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 00:38:06 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 14:11:10 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Krieger", "Don", ""], ["Shepard", "Paul", ""], ["Zusman", "Ben", ""], ["Jana", "Anirban", ""], ["Okonkwo", "David O.", ""]]}, {"id": "1710.05615", "submitter": "Hyegyeong Park", "authors": "Hyegyeong Park, Dongwon Lee and Jaekyun Moon", "title": "LDPC Code Design for Distributed Storage: Balancing Repair Bandwidth,\n  Reliability and Storage Overhead", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage systems suffer from significant repair traffic generated\ndue to frequent storage node failures. This paper shows that properly designed\nlow-density parity-check (LDPC) codes can substantially reduce the amount of\nrequired block downloads for repair thanks to the sparse nature of their factor\ngraph representation. In particular, with a careful construction of the factor\ngraph, both low repair-bandwidth and high reliability can be achieved for a\ngiven code rate. First, a formula for the average repair bandwidth of LDPC\ncodes is developed. This formula is then used to establish that the minimum\nrepair bandwidth can be achieved by forcing a regular check node degree in the\nfactor graph. Moreover, it is shown that given a fixed code rate, the variable\nnode degree should also be regular to yield minimum repair bandwidth, under\nsome reasonable minimum variable node degree constraint. It is also shown that\nfor a given repair-bandwidth requirement, LDPC codes can yield substantially\nhigher reliability than currently utilized Reed-Solomon (RS) codes. Our\nreliability analysis is based on a formulation of the general equation for the\nmean-time-to-data-loss (MTTDL) associated with LDPC codes. The formulation\nreveals that the stopping number is closely related to the MTTDL. It is further\nshown that LDPC codes can be designed such that a small loss of\nrepair-bandwidth optimality may be traded for a large improvement in\nerasure-correction capability and thus the MTTDL.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 10:49:25 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Park", "Hyegyeong", ""], ["Lee", "Dongwon", ""], ["Moon", "Jaekyun", ""]]}, {"id": "1710.05656", "submitter": "Sarod Yatawatta", "authors": "Sarod Yatawatta, Faruk Diblen, Hanno Spreeuw", "title": "Adaptive ADMM in Distributed Radio Interferometric Calibration", "comments": "Draft, to be published in the Proceedings of the 7th International\n  Workshop on Computational Advances in Multi-Sensor Adaptive Processing\n  (CAMSAP) (IEEE CAMSAP 2017), published by IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed radio interferometric calibration based on consensus optimization\nhas been shown to improve the estimation of systematic errors in radio\nastronomical observations. The intrinsic continuity of systematic errors across\nfrequency is used by a consensus polynomial to penalize traditional\ncalibration. Consensus is achieved via the use of alternating direction method\nof multipliers (ADMM) algorithm. In this paper, we extend the existing\ndistributed calibration algorithms to use ADMM with an adaptive penalty\nparameter update. Compared to a fixed penalty, its adaptive update has been\nshown to perform better in diverse applications of ADMM. In this paper, we\ncompare two such popular penalty parameter update schemes: residual balance\npenalty update and spectral penalty update (Barzilai-Borwein). We apply both\nschemes to distributed radio interferometric calibration and compare their\nperformance against ADMM with a fixed penalty parameter. Simulations show that\nboth methods of adaptive penalty update improve the convergence of ADMM but the\nspectral penalty parameter update shows more stability.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 12:45:08 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Yatawatta", "Sarod", ""], ["Diblen", "Faruk", ""], ["Spreeuw", "Hanno", ""]]}, {"id": "1710.05785", "submitter": "Yanfeng Zhang", "authors": "Yanfeng Zhang, Qixin Gao, Lixin Gao, Cuirong Wang", "title": "Maiter: An Asynchronous Graph Processing Framework for Delta-based\n  Accumulative Iterative Computation", "comments": "ScienceCloud 2012, TKDE 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myriad of graph-based algorithms in machine learning and data mining require\nparsing relational data iteratively. These algorithms are implemented in a\nlarge-scale distributed environment in order to scale to massive data sets. To\naccelerate these large-scale graph-based iterative computations, we propose\ndelta-based accumulative iterative computation (DAIC). Different from\ntraditional iterative computations, which iteratively update the result based\non the result from the previous iteration, DAIC updates the result by\naccumulating the \"changes\" between iterations. By DAIC, we can process only the\n\"changes\" to avoid the negligible updates. Furthermore, we can perform DAIC\nasynchronously to bypass the high-cost synchronous barriers in heterogeneous\ndistributed environments. Based on the DAIC model, we design and implement an\nasynchronous graph processing framework, Maiter. We evaluate Maiter on local\ncluster as well as on Amazon EC2 Cloud. The results show that Maiter achieves\nas much as 60x speedup over Hadoop and outperforms other state-of-the-art\nframeworks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 15:42:21 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhang", "Yanfeng", ""], ["Gao", "Qixin", ""], ["Gao", "Lixin", ""], ["Wang", "Cuirong", ""]]}, {"id": "1710.06089", "submitter": "Hamed Shah-Mansouri", "authors": "Hamed Shah-Mansouri and Vincent W.S. Wong", "title": "Hierarchical Fog-Cloud Computing for IoT Systems: A Computation\n  Offloading Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing, which provides low-latency computing services at the network\nedge, is an enabler for the emerging Internet of Things (IoT) systems. In this\npaper, we study the allocation of fog computing resources to the IoT users in a\nhierarchical computing paradigm including fog and remote cloud computing\nservices. We formulate a computation offloading game to model the competition\nbetween IoT users and allocate the limited processing power of fog nodes\nefficiently. Each user aims to maximize its own quality of experience (QoE),\nwhich reflects its satisfaction of using computing services in terms of the\nreduction in computation energy and delay. Utilizing a potential game approach,\nwe prove the existence of a pure Nash equilibrium and provide an upper bound\nfor the price of anarchy. Since the time complexity to reach the equilibrium\nincreases exponentially in the number of users, we further propose a\nnear-optimal resource allocation mechanism and prove that in a system with $N$\nIoT users, it can achieve an $\\epsilon$-Nash equilibrium in $O(N/\\epsilon)$\ntime. Through numerical studies, we evaluate the users' QoE as well as the\nequilibrium efficiency. Our results reveal that by utilizing the proposed\nmechanism, more users benefit from computing services in comparison to an\nexisting offloading mechanism. We further show that our proposed mechanism\nsignificantly reduces the computation delay and enables low-latency fog\ncomputing services for delay-sensitive IoT applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 04:31:38 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Shah-Mansouri", "Hamed", ""], ["Wong", "Vincent W. S.", ""]]}, {"id": "1710.06316", "submitter": "Bo Zhang", "authors": "B. Zhang and J. DeBuhr and D. Niedzielski and S. Mayolo and B. Lu and\n  T. Sterling", "title": "DASHMM Accelerated Adaptive Fast Multipole Poisson-Boltzmann Solver on\n  Distributed Memory Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an updated version of the AFMPB package for fast calculation of\nmolecular solvation-free energy. The main feature of the new version is the\nsuccessful adoption of the DASHMM library, which enables AFMPB to operate on\ndistributed memory computers. As a result, the new version can easily handle\nlarger molecules or situations with higher accuracy requirements. To\ndemonstrate the updated code, we applied the new version to a dengue virus\nsystem with more than one million atoms and a mesh with approximately 20\nmillion triangles, and were able to reduce the time-to-solution from 10 hours\nreported in the previous release on a shared memory computer to less than 30\nseconds on a Cray XC30 cluster using 12, 288 cores.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 14:35:35 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Zhang", "B.", ""], ["DeBuhr", "J.", ""], ["Niedzielski", "D.", ""], ["Mayolo", "S.", ""], ["Lu", "B.", ""], ["Sterling", "T.", ""]]}, {"id": "1710.06331", "submitter": "Wiktor Daszczuk", "authors": "Wiktor B. Daszczuk, Jerzy Mie\\'scicki, Waldemar Grabski", "title": "Distributed algorithm for empty vehicles management in personal rapid\n  transit (PRT) network", "comments": "22 pages, 6 figures, 5 tables", "journal-ref": "Journal of Advanced Transportation, vol. 50 (2016), No.4,\n  pp.608-629", "doi": "10.1002/atr.1365", "report-no": null, "categories": "cs.DC cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an original heuristic algorithm of empty vehicles management\nin personal rapid transit network is presented. The algorithm is used for the\ndelivery of empty vehicles for waiting passengers, for balancing the\ndistribution of empty vehicles within the network, and for providing an empty\nspace for vehicles approaching a station. Each of these tasks involves a\ndecision on the trip that has to be done by a selected empty vehicle from its\nactual location to some determined destination. The decisions are based on a\nmulti-parameter function involving a set of factors and thresholds. An\nimportant feature of the algorithm is that it does not use any central database\nof passenger input (demand) and locations of free vehicles. Instead, it is\nbased on the local exchange of data between stations: on their states and on\nthe vehicles they expect. Therefore, it seems well-tailored for a distributed\nimplementation. The algorithm is uniform, meaning that the same basic procedure\nis used for multiple tasks using a task-specific set of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 15:03:22 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Daszczuk", "Wiktor B.", ""], ["Mie\u015bcicki", "Jerzy", ""], ["Grabski", "Waldemar", ""]]}, {"id": "1710.06471", "submitter": "Qian Yu", "authors": "Qian Yu, Mohammad Ali Maddah-Ali, and A. Salman Avestimehr", "title": "Coded Fourier Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the Fourier transform of\nhigh-dimensional vectors, distributedly over a cluster of machines consisting\nof a master node and multiple worker nodes, where the worker nodes can only\nstore and process a fraction of the inputs. We show that by exploiting the\nalgebraic structure of the Fourier transform operation and leveraging concepts\nfrom coding theory, one can efficiently deal with the straggler effects. In\nparticular, we propose a computation strategy, named as coded FFT, which\nachieves the optimal recovery threshold, defined as the minimum number of\nworkers that the master node needs to wait for in order to compute the output.\nThis is the first code that achieves the optimum robustness in terms of\ntolerating stragglers or failures for computing Fourier transforms.\nFurthermore, the reconstruction process for coded FFT can be mapped to MDS\ndecoding, which can be solved efficiently. Moreover, we extend coded FFT to\nsettings including computing general $n$-dimensional Fourier transforms, and\nprovide the optimal computing strategy for those settings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 18:57:52 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Yu", "Qian", ""], ["Maddah-Ali", "Mohammad Ali", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1710.06957", "submitter": "Shafinaz Islam", "authors": "Shafinaz Islam", "title": "Network Load Balancing Methods: Experimental Comparisons and Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load balancing algorithms play critical roles in systems where the workload\nhas to be distributed across multiple resources, such as cores in\nmultiprocessor system, computers in distributed computing, and network links.\nIn this paper, we study and evaluate four load balancing methods: random, round\nrobin, shortest queue, and shortest queue with stale load information. We build\na simulation model and compare mean delay of the systems for the load balancing\nmethods. We also provide a method to improve shortest queue with stale load\ninformation load balancing. A performance analysis for the improvement is also\npresented in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 22:55:43 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Islam", "Shafinaz", ""]]}, {"id": "1710.07040", "submitter": "Parikshit Saikia", "authors": "Parikshit Saikia, Sushanta Karmakar, and Aris T. Pagourtzis", "title": "A Primal-Dual based Distributed Approximation Algorithm for\n  Prize-Collecting Steiner Tree", "comments": "Distributed algorithm, 32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Prize-Collecting Steiner Tree (PCST) problem is a generalization of the\nSteiner Tree problem that has applications in network design, content\ndistribution networks, and many more. There are a few centralized approximation\nalgorithms \\cite{DB_MG_DS_DW_1993, GW_1995, DJ_MM_SP_2000, AA_MB_MH_2011} for\nsolving the PCST problem. However no distributed algorithm is known that solves\nPCST with a guaranteed approximation factor. In this work we present an\nasynchronous distributed $(2 - \\frac{1}{n - 1})$-approximation algorithm that\nconstructs a PCST for a given connected undirected graph with non-negative edge\nweights and a non-negative prize value for each node. Our algorithm is an\nadaptation of the centralized algorithm proposed by Goemans and Williamson\n\\cite{GW_1995} to the distributed setting, and is based on the primal-dual\nmethod. The message complexity of the algorithm with input graph having node\nset $V$ and edge set $E$ is $O(|V||E|)$. Initially each node knows only its own\nprize value and the weight of each incident edge. The algorithm is\nspontaneously initiated at a special node called the \\emph{root node} and when\nit terminates each node knows whether it is in the PCST or not. To the best of\nour knowledge this is the first distributed constant approximation algorithm\nfor PCST.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 08:41:38 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 09:55:04 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Saikia", "Parikshit", ""], ["Karmakar", "Sushanta", ""], ["Pagourtzis", "Aris T.", ""]]}, {"id": "1710.07041", "submitter": "Tilo Wettig", "authors": "Peter Georg, Daniel Richtmann, Tilo Wettig", "title": "DD-$\\alpha$AMG on QPACE 3", "comments": "12 pages, 6 figures, Proceedings of Lattice 2017", "journal-ref": null, "doi": "10.1051/epjconf/201817502007", "report-no": null, "categories": "hep-lat cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our experience porting the Regensburg implementation of the\nDD-$\\alpha$AMG solver from QPACE 2 to QPACE 3. We first review how the code was\nported from the first generation Intel Xeon Phi processor (Knights Corner) to\nits successor (Knights Landing). We then describe the modifications in the\ncommunication library necessitated by the switch from InfiniBand to Omni-Path.\nFinally, we present the performance of the code on a single processor as well\nas the scaling on many nodes, where in both cases the speedup factor is close\nto the theoretical expectations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 08:44:48 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Georg", "Peter", ""], ["Richtmann", "Daniel", ""], ["Wettig", "Tilo", ""]]}, {"id": "1710.07191", "submitter": "Oded Padon", "authors": "Oded Padon, Giuliano Losa, Mooly Sagiv, Sharon Shoham", "title": "Paxos Made EPR: Decidable Reasoning about Distributed Protocols", "comments": "61 pages. Full version of paper by the same title presented in OOPSLA\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed protocols such as Paxos play an important role in many computer\nsystems. Therefore, a bug in a distributed protocol may have tremendous\neffects. Accordingly, a lot of effort has been invested in verifying such\nprotocols. However, checking invariants of such protocols is undecidable and\nhard in practice, as it requires reasoning about an unbounded number of nodes\nand messages. Moreover, protocol actions and invariants involve both quantifier\nalternations and higher-order concepts such as set cardinalities and\narithmetic.\n  This paper makes a step towards automatic verification of such protocols. We\naim at a technique that can verify correct protocols and identify bugs in\nincorrect protocols. To this end, we develop a methodology for deductive\nverification based on effectively propositional logic (EPR)---a decidable\nfragment of first-order logic (also known as the Bernays-Sch\\\"onfinkel-Ramsey\nclass). In addition to decidability, EPR also enjoys the finite model property,\nallowing to display violations as finite structures which are intuitive for\nusers. Our methodology involves modeling protocols using general\n(uninterpreted) first-order logic, and then systematically transforming the\nmodel to obtain a model and an inductive invariant that are decidable to check.\nThe steps of the transformations are also mechanically checked, ensuring the\nsoundness of the method. We have used our methodology to verify the safety of\nPaxos, and several of its variants, including Multi-Paxos, Vertical Paxos, Fast\nPaxos, Flexible Paxos and Stoppable Paxos. To the best of our knowledge, this\nwork is the first to verify these protocols using a decidable logic, and the\nfirst formal verification of Vertical Paxos, Fast Paxos and Stoppable Paxos.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 15:37:42 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Padon", "Oded", ""], ["Losa", "Giuliano", ""], ["Sagiv", "Mooly", ""], ["Shoham", "Sharon", ""]]}, {"id": "1710.07234", "submitter": "Long Gong", "authors": "Long Gong, Liang Liu, Sen Yang, Jun Xu, Yi Xie, Xinbing Wang", "title": "SERENADE: A Parallel Randomized Algorithm Suite for Crossbar Scheduling\n  in Input-Queued Switches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of today's high-speed switches and routers adopt an input-queued\ncrossbar switch architecture. Such a switch needs to compute a matching\n(crossbar schedule) between the input ports and output ports during each\nswitching cycle (time slot). A key research challenge in designing large (in\nnumber of input/output ports $N$) input-queued crossbar switches is to develop\ncrossbar scheduling algorithms that can compute \"high quality\" matchings --\ni.e., those that result in high switch throughput (ideally $100\\%$) and low\nqueueing delays for packets -- at line rates. SERENA is one such algorithm: it\noutputs excellent matching decisions that result in $100\\%$ switch throughput\nand reasonably good queueing delays. However, since SERENA is a centralized\nalgorithm with $O(N)$ computational complexity, it cannot support switches that\nboth are large and have a very high line rate per port. In this work, we\npropose SERENADE (SERENA, the Distributed Edition), a parallel iterative\nalgorithm that emulates SERENA in only $O(\\log N)$ iterations between input\nports and output ports, and hence has a time complexity of only $O(\\log N)$ per\nport. We prove that SERENADE can exactly emulate SERENA. We also propose an\nearly-stop version of SERENADE, called O-SERENADE, to only approximately\nemulate SERENA. Through extensive simulations, we show that O-SERENADE can\nachieve 100\\% throughput and that it has similar as or slightly better delay\nperformance than SERENA under various load conditions and traffic patterns.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 16:26:51 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 03:15:53 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 06:41:57 GMT"}, {"version": "v4", "created": "Mon, 18 Mar 2019 14:32:54 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Gong", "Long", ""], ["Liu", "Liang", ""], ["Yang", "Sen", ""], ["Xu", "Jun", ""], ["Xie", "Yi", ""], ["Wang", "Xinbing", ""]]}, {"id": "1710.07358", "submitter": "Walid Jradi", "authors": "Walid Jradi, Hugo do Nascimento, Wellington Martins", "title": "A Fast and Generic GPU-Based Parallel Reduction Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduction operations are extensively employed in many computational problems.\nA reduction consists of, given a finite set of numeric elements, combining into\na single value all elements in that set, using for this a combiner function. A\nparallel reduction, in turn, is the reduction operation concurrently performed\nwhen multiple execution units are available. The current work reports an\ninvestigation on this subject and depicts a GPU-based parallel approach for it.\nEmploying techniques like Loop Unrolling, Persistent Threads and Algebraic\nExpressions to avoid thread divergence, the presented approach was able to\nachieve a 2.8x speedup when compared to the work of Catanzaro, using a generic,\nsimple and easily portable code. Experiments conducted to evaluate the approach\nshow that the strategy is able to perform efficiently in AMD and NVidia's\nhardware, as well as in OpenCL and CUDA.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 21:29:52 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Jradi", "Walid", ""], ["Nascimento", "Hugo do", ""], ["Martins", "Wellington", ""]]}, {"id": "1710.07380", "submitter": "Jaros{\\l}aw Mirek", "authors": "Marek Klonowski, Dariusz R. Kowalski, Jaros{\\l}aw Mirek and Prudence\n  W.H. Wong", "title": "Fault-tolerant parallel scheduling of arbitrary length jobs on a shared\n  channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of scheduling jobs on fault-prone machines communicating\nvia a shared channel, also known as multiple-access channel. We have $n$\narbitrary length jobs to be scheduled on $m$ identical machines, $f$ of which\nare prone to crashes by an adversary. A machine can inform other machines when\na job is completed via the channel without collision detection. Performance is\nmeasured by the total number of available machine steps during the whole\nexecution. Our goal is to study the impact of preemption (i.e., interrupting\nthe execution of a job and resuming later in the same or different machine) and\nfailures on the work performance of job processing. The novelty is the ability\nto identify the features that determine the complexity (difficulty) of the\nproblem. We show that the problem becomes difficult when preemption is not\nallowed, by showing corresponding lower and upper bounds, the latter with\nalgorithms reaching them. We also prove that randomization helps even more, but\nonly against a non-adaptive adversary; in the presence of more severe adaptive\nadversary, randomization does not help in any setting. Our work has extended\nfrom previous work that focused on settings including: scheduling on\nmultiple-access channel without machine failures, complete information about\nfailures, or incomplete information about failures (like in this work) but with\nunit length jobs and, hence, without considering preemption.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 00:30:04 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 12:24:36 GMT"}, {"version": "v3", "created": "Sat, 18 Nov 2017 00:20:47 GMT"}, {"version": "v4", "created": "Tue, 24 Jul 2018 22:56:00 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Klonowski", "Marek", ""], ["Kowalski", "Dariusz R.", ""], ["Mirek", "Jaros\u0142aw", ""], ["Wong", "Prudence W. H.", ""]]}, {"id": "1710.07543", "submitter": "Gustavo Maciel Dias Vieira", "authors": "Rodrigo R. Barbieri, Gustavo M. D. Vieira", "title": "Hardened Paxos Through Consistency Validation", "comments": "6 pages, fixed metadata", "journal-ref": "SBESC '15: Proceedings of the V Brazilian Symposium on Computing\n  Systems Engineering, IEEE Computer Society, 2015, 13-18", "doi": "10.1109/SBESC.2015.10", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the emergent adoption of distributed systems when building\napplications, demand for reliability and availability has increased. These\nproperties can be achieved through replication techniques using middleware\nalgorithms that must be capable of tolerating faults. Certain faults such as\narbitrary faults, however, may be more difficult to tolerate, resulting in more\ncomplex and resource intensive algorithms that end up being not so practical to\nuse. We propose and experiment with the use of consistency validation\ntechniques to harden a benign fault-tolerant Paxos, thus being able to detect\nand tolerate non-malicious arbitrary faults.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 14:22:10 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 17:22:45 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Barbieri", "Rodrigo R.", ""], ["Vieira", "Gustavo M. D.", ""]]}, {"id": "1710.07565", "submitter": "Sebastian Lamm", "authors": "Daniel Funke, Sebastian Lamm, Ulrich Meyer, Peter Sanders, Manuel\n  Penschuck, Christian Schulz, Darren Strash, Moritz von Looz", "title": "Communication-free Massively Distributed Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing massive complex networks yields promising insights about our\neveryday lives. Building scalable algorithms to do so is a challenging task\nthat requires a careful analysis and an extensive evaluation. However,\nengineering such algorithms is often hindered by the scarcity of\npublicly~available~datasets.\n  Network generators serve as a tool to alleviate this problem by providing\nsynthetic instances with controllable parameters. However, many network\ngenerators fail to provide instances on a massive scale due to their sequential\nnature or resource constraints. Additionally, truly scalable network generators\nare few and often limited in their realism.\n  In this work, we present novel generators for a variety of network models\nthat are frequently used as benchmarks. By making use of pseudorandomization\nand divide-and-conquer schemes, our generators follow a communication-free\nparadigm. The resulting generators are thus embarrassingly parallel and have a\nnear optimal scaling behavior. This allows us to generate instances of up to\n$2^{43}$ vertices and $2^{47}$ edges in less than 22 minutes on 32768 cores.\nTherefore, our generators allow new graph families to be used on an\nunprecedented scale.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 15:10:59 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 09:46:00 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 10:12:06 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Funke", "Daniel", ""], ["Lamm", "Sebastian", ""], ["Meyer", "Ulrich", ""], ["Sanders", "Peter", ""], ["Penschuck", "Manuel", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""], ["von Looz", "Moritz", ""]]}, {"id": "1710.07588", "submitter": "Vitaly Aksenov", "authors": "Vitaly Aksenov, Petr Kuznetsov, Anatoly Shalyto", "title": "Parallel Combining: Benefits of Explicit Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parallel batched data structures are designed to process synchronized batches\nof operations in a parallel computing model. In this paper, we propose parallel\ncombining, a technique that implements a concurrent data structure from a\nparallel batched one. The idea is that we explicitly synchronize concurrent\noperations into batches: one of the processes becomes a combiner which collects\nconcurrent requests and initiates a parallel batched algorithm involving the\nowners (clients) of the collected requests. Intuitively, the cost of\nsynchronizing the concurrent calls can be compensated by running the parallel\nbatched algorithm.\n  We validate the intuition via two applications of parallel combining. First,\nwe use our technique to design a concurrent data structure optimized for\nread-dominated workloads, taking a dynamic graph data structure as an example.\nSecond, we use a novel parallel batched priority queue to build a concurrent\none. In both cases, we obtain performance gains with respect to the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 16:07:18 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 17:47:24 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 18:34:42 GMT"}, {"version": "v4", "created": "Tue, 13 Nov 2018 10:10:08 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Aksenov", "Vitaly", ""], ["Kuznetsov", "Petr", ""], ["Shalyto", "Anatoly", ""]]}, {"id": "1710.07623", "submitter": "Gustavo Maciel Dias Vieira", "authors": "Fellipe A. Ugliara, Gustavo M. D. Vieira, Jos\\'e de O. Guimar\\~aes", "title": "Transparent Replication Using Metaprogramming in Cyan", "comments": "24 pages, revised and expanded journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication can be used to increase the availability of a service by creating\nmany operational copies of its data called replicas. Active replication is a\nform of replication that has strong consistency semantics, easier to reason\nabout and program. However, creating replicated services using active\nreplication still demands from the programmer the knowledge of subtleties of\nthe replication mechanism. In this paper we show how to use the metaprogramming\ninfrastructure of the Cyan language to shield the application programmer from\nthese details, allowing easier creation of fault-tolerant replicated\napplications through simple annotations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 17:45:37 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 19:31:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ugliara", "Fellipe A.", ""], ["Vieira", "Gustavo M. D.", ""], ["Guimar\u00e3es", "Jos\u00e9 de O.", ""]]}, {"id": "1710.07628", "submitter": "Shu Wang", "authors": "Shu Wang, Chi Li, William Sentosa, Henry Hoffmann, Shan Lu", "title": "Understanding and Auto-Adjusting Performance-Related Configurations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software systems are often equipped with hundreds to thousands of\nconfiguration options, many of which greatly affect performance. Unfortunately,\nproperly setting these configurations is challenging for developers due to the\ncomplex and dynamic nature of system workload and environment. In this paper,\nwe first conduct an empirical study to understand performance-related\nconfigurations and the challenges of setting them in the real-world. Guided by\nour study, we design a systematic and general control-theoretic framework,\nSmartConf, to automatically set and dynamically adjust performance-related\nconfigurations to meet required operating constraints while optimizing other\nperformance metrics. Evaluation shows that SmartConf is effective in solving\nreal-world configuration problems, often providing better performance than even\nthe best static configuration developers can choose under existing\nconfiguration systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 17:56:46 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Wang", "Shu", ""], ["Li", "Chi", ""], ["Sentosa", "William", ""], ["Hoffmann", "Henry", ""], ["Lu", "Shan", ""]]}, {"id": "1710.07745", "submitter": "Hope Mogale Mr", "authors": "Hope Mogale", "title": "High Performance Canny Edge Detector using Parallel Patterns for\n  Scalability on Modern Multicore Processors", "comments": "8 Pages, 13 figures, 4 Sections, Algorithm listing, Mathematical\n  models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canny Edge Detector (CED) is an edge detection operator commonly used by most\nImage Feature Extraction (IFE) Algorithms and Image Processing Applications.\nThis operator involves the use of a multi-stage algorithm to detect edges in a\nwide range of images. Edge detection is at the forefront of image processing\nand hence, it is crucial to have at an up to scale level. Multicore Processors\nhave emerged as the next solution for tackling compute intensive tasks that\nhave a high demand for computational power. Having significant changes that\nrestructured the microprocessor industry, it is evident that the best way to\npromote efficiency and improve performance is no longer by increasing the clock\nspeeds on traditional monolithic processors but by adopting and utilizing\nProcessors with Multicore architectures. In this paper we provide a high\nperformance implementation of Canny Edge Detector using parallel patterns for\nimproved performance and Scalability on Multicore Processors. The results show\nsignificant improvements in overall performance and this proves that our\nimplementation using parallel patterns does not under utilize resources but\nscales well for multicore processors.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 02:52:36 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Mogale", "Hope", ""]]}, {"id": "1710.07845", "submitter": "Gustavo Maciel Dias Vieira", "authors": "Gustavo M. D. Vieira, Islene C. Garcia, Luiz E. Buzato", "title": "Seamless Paxos Coordinators", "comments": "11 pages, final published version, with correct experimental data", "journal-ref": "Cluster Computing (2014) 17:463-473", "doi": "10.1007/s10586-013-0264-9", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Paxos algorithm requires a single correct coordinator process to operate.\nAfter a failure, the replacement of the coordinator may lead to a temporary\nunavailability of the application implemented atop Paxos. So far, this\nunavailability has been addressed by reducing the coordinator replacement rate\nthrough the use of stable coordinator selection algorithms. We have observed\nthat the cost of recovery of the newly elected coordinator's state is at the\ncore of this unavailability problem. In this paper we present a new technique\nto manage coordinator replacement that allows the recovery to occur\nconcurrently with new consensus rounds. Experimental results show that our\nseamless approach effectively solves the temporary unavailability problem, its\nadoption entails uninterrupted execution of the application. Our solution\nremoves the restriction that the occurrence of coordinator replacements is\nsomething to be avoided, allowing the decoupling of the application execution\nfrom the accuracy of the mechanism used to choose a coordinator. This result\nincreases the performance of the application even in the presence of failures,\nit is of special importance to the autonomous operation of replicated\napplications that have to adapt to varying network conditions and partial\nfailures.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 19:12:48 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Vieira", "Gustavo M. D.", ""], ["Garcia", "Islene C.", ""], ["Buzato", "Luiz E.", ""]]}, {"id": "1710.07898", "submitter": "Yue Fu", "authors": "Dagang Li, Rong Du, Man Ho Au, Yue Fu", "title": "Meta-Key: A Secure Data-Sharing Protocol under Blockchain-Based\n  Decentralised Storage Architecture", "comments": null, "journal-ref": "IEEE Networking Letters (2019) 30-33", "doi": "10.1109/LNET.2019.2891998", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter we propose Meta-key, a data-sharing mechanism that enables\nusers share their encrypted data under a blockchain-based decentralized storage\narchitecture. All the data-encryption keys are encrypted by the owner's public\nkey and put onto the blockchain for safe and secure storage and easy\nkey-management. Encrypted data are stored in dedicated storage nodes and proxy\nre-encryption mechanism is used to ensure secure data-sharing in the untrusted\nenvironment. Security analysis of our model shows that the proxy re-encryption\nadopted in our system is naturally free from collusion-attack due to the\nspecific architecture of Meta-key.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 07:14:32 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 06:49:20 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Li", "Dagang", ""], ["Du", "Rong", ""], ["Au", "Man Ho", ""], ["Fu", "Yue", ""]]}, {"id": "1710.07907", "submitter": "Wiktor Daszczuk", "authors": "Stanis{\\l}aw Chrobot, Wiktor B. Daszczuk", "title": "Communication Dualism in Distributed Systems with Petri Net\n  Interpretation", "comments": "14 pages, 4 figures, Appendix with proofs of some lemmas", "journal-ref": "Theoretical and Applied Informatics vol. 4/2006, pp. 261-278,\n  ISSN: 1896-5334", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper notion of communication dualism id formalized and explained in\nPetri net interpretation. We consider communication dualism a basic property of\ncommunication in distributed systems. The formalization is done in the\nIntegrated Model of Distributed Systems (IMDS) where synchronous communication,\nas well as asynchronous message-passing and variable-sharing are modeled in a\ncommon framework. In the light of this property, communication in distributed\nsystems can be seen as a two-dimensional phenomenon with passing being its\nspatial dimension and sharing its temporal dimension. Any distributed system\ncan be modeled as a composition of message-passes asynchronous processes or as\na composition of variable-sharing asynchronous processes. A method of automatic\nprocess extraction in Petri net interpretation of IMDS is presented.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 08:29:03 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chrobot", "Stanis\u0142aw", ""], ["Daszczuk", "Wiktor B.", ""]]}, {"id": "1710.07997", "submitter": "Wiktor Daszczuk", "authors": "Wiktor B. Daszczuk", "title": "Timed Concurrent State Machines", "comments": "13 pages, 4 figures", "journal-ref": "Computer Science, vol. 8, 2007, pp. 23-36", "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timed Concurrent State Machines are an application of Alur's Timed Automata\nconcept to coincidence-based (rather than interleaving) CSM modeling technique.\nTCSM support the idea of testing automata, allowing to specify time properties\neasier than temporal formulas. Also, calculation of a global state space in\nreal-time domain (Region Concurrent State Machines) is defined, allowing to\nstore a verified system in ready-to-verification form, and to multiply it by\nvarious testing automata.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 18:41:28 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Daszczuk", "Wiktor B.", ""]]}, {"id": "1710.08027", "submitter": "Michael Axtmann", "authors": "Michael Axtmann, Armin Wiebigke and Peter Sanders", "title": "Lightweight MPI Communicators with Applications to Perfectly Balanced\n  Quicksort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MPI uses the concept of communicators to connect groups of processes. It\nprovides nonblocking collective operations on communicators to overlap\ncommunication and computation. Flexible algorithms demand flexible\ncommunicators. E.g., a process can work on different subproblems within\ndifferent process groups simultaneously, new process groups can be created, or\nthe members of a process group can change. Depending on the number of\ncommunicators, the time for communicator creation can drastically increase the\nrunning time of the algorithm. Furthermore, a new communicator synchronizes all\nprocesses as communicator creation routines are blocking collective operations.\n  We present RBC, a communication library based on MPI, that creates\nrange-based communicators in constant time without communication. These RBC\ncommunicators support (non)blocking point-to-point communication as well as\n(non)blocking collective operations. Our experiments show that the library\nreduces the time to create a new communicator by a factor of more than 400\nwhereas the running time of collective operations remains about the same. We\npropose Janus Quicksort, a distributed sorting algorithm that avoids any load\nimbalances. We improved the performance of this algorithm by a factor of 15 for\nmoderate inputs by using RBC communicators. Finally, we discuss different\napproaches to bring nonblocking (local) communicator creation of lightweight\n(range-based) communicators into MPI.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 22:09:33 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 10:29:05 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Axtmann", "Michael", ""], ["Wiebigke", "Armin", ""], ["Sanders", "Peter", ""]]}, {"id": "1710.08047", "submitter": "Gustavo Maciel Dias Vieira", "authors": "Gustavo M. D. Vieira, Luiz E. Buzato", "title": "On the Coordinator's Rule for Fast Paxos", "comments": "6 pages", "journal-ref": "Information Processing Letters (2008) 107:183-187", "doi": "10.1016/j.ipl.2008.03.001", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Paxos is an algorithm for consensus that works by a succession of\nrounds, where each round tries to decide a value $v$ that is consistent with\nall past rounds. Rounds are started by a coordinator process and consistency is\nguaranteed by the rule used by this process for the selection of $v$ and by the\nproperties of process sets called quorums. We show a simplified version of this\nrule for the specific case where the quorums are defined by the cardinality of\nthese process sets. This rule is of special interest for implementors of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 00:26:11 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Vieira", "Gustavo M. D.", ""], ["Buzato", "Luiz E.", ""]]}, {"id": "1710.08101", "submitter": "Lican Huang", "authors": "Lican Huang", "title": "Directory Service Provided by DSCloud Platform", "comments": "4 pages, ICITEL2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there are huge volumes of information dispersing in the various\nmachines, global directory services are required for the users. DSCloud\nPlatform provides the global directory service, in which the directories are\ncreated and maintained by the users themselves. In this paper, we describe the\nDSCloud Platform directory service's functions, authorization, mounting users'\nlocal file systems, and usage scenery for education.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 06:11:54 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Huang", "Lican", ""]]}, {"id": "1710.08128", "submitter": "Michael Feldmann M. Sc.", "authors": "Michael Feldmann, Christina Kolb, Christian Scheideler and Thim\n  Strothmann", "title": "Self-Stabilizing Supervised Publish-Subscribe Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two major results: First, we introduce the first\nself-stabilizing version of a supervised overlay network by presenting a\nself-stabilizing supervised skip ring. Secondly, we show how to use the\nself-stabilizing supervised skip ring to construct an efficient\nself-stabilizing publish-subscribe system. That is, in addition to stabilizing\nthe overlay network, every subscriber of a topic will eventually know all of\nthe publications that have been issued so far for that topic. The communication\nwork needed to processes a subscribe or unsubscribe operation is just a\nconstant in a legitimate state, and the communication work of checking whether\nthe system is still in a legitimate state is just a constant on expectation for\nthe supervisor as well as any process in the system.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 07:53:12 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 15:32:52 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Feldmann", "Michael", ""], ["Kolb", "Christina", ""], ["Scheideler", "Christian", ""], ["Strothmann", "Thim", ""]]}, {"id": "1710.08255", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Lorenz H\\\"ubschle-Schneider and Peter Sanders", "title": "Communication Efficient Checking of Big Data Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose fast probabilistic algorithms with low (i.e., sublinear in the\ninput size) communication volume to check the correctness of operations in Big\nData processing frameworks and distributed databases. Our checkers cover many\nof the commonly used operations, including sum, average, median, and minimum\naggregation, as well as sorting, union, merge, and zip. An experimental\nevaluation of our implementation in Thrill (Bingmann et al., 2016) confirms the\nlow overhead and high failure detection rate predicted by theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 13:15:53 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 10:34:38 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Sanders", "Peter", ""]]}, {"id": "1710.08281", "submitter": "Ethel Ethel", "authors": "Obinna Ethelbert, Faraz Fatemi Moghaddam, Philipp Wieder, Ramin\n  Yahyapour", "title": "A JSON Token-Based Authentication and Access Management Schema for Cloud\n  SaaS Applications", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF cs.SE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cloud computing is significantly reshaping the computing industry built\naround core concepts such as virtualization, processing power, connectivity and\nelasticity to store and share IT resources via a broad network. It has emerged\nas the key technology that unleashes the potency of Big Data, Internet of\nThings, Mobile and Web Applications, and other related technologies, but it\nalso comes with its challenges - such as governance, security, and privacy.\nThis paper is focused on the security and privacy challenges of cloud computing\nwith specific reference to user authentication and access management for cloud\nSaaS applications. The suggested model uses a framework that harnesses the\nstateless and secure nature of JWT for client authentication and session\nmanagement. Furthermore, authorized access to protected cloud SaaS resources\nhave been efficiently managed. Accordingly, a Policy Match Gate (PMG) component\nand a Policy Activity Monitor (PAM) component have been introduced. In\naddition, other subcomponents such as a Policy Validation Unit (PVU) and a\nPolicy Proxy DB (PPDB) have also been established for optimized service\ndelivery. A theoretical analysis of the proposed model portrays a system that\nis secure, lightweight and highly scalable for improved cloud resource security\nand management.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 14:01:29 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Ethelbert", "Obinna", ""], ["Moghaddam", "Faraz Fatemi", ""], ["Wieder", "Philipp", ""], ["Yahyapour", "Ramin", ""]]}, {"id": "1710.08291", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e, Andrzej Pelc, Franck Petit", "title": "Deterministic Rendezvous at a Node of Agents with Arbitrary Velocities", "comments": "arXiv admin note: text overlap with arXiv:1704.08880", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of rendezvous in networks modeled as undirected graphs.\nTwo mobile agents with different labels, starting at different nodes of an\nanonymous graph, have to meet. This task has been considered in the literature\nunder two alternative scenarios: weak and strong. Under the weak scenario,\nagents may meet either at a node or inside an edge. Under the strong scenario,\nthey have to meet at a node, and they do not even notice meetings inside an\nedge. Rendezvous algorithms under the strong scenario are known for synchronous\nagents. For asynchronous agents, rendezvous under the strong scenario is\nimpossible even in the two-node graph, and hence only algorithms under the weak\nscenario were constructed. In this paper we show that rendezvous under the\nstrong scenario is possible for agents with restricted asynchrony: agents have\nthe same measure of time but the adversary can arbitrarily impose the speed of\ntraversing each edge by each of the agents. We construct a deterministic\nrendezvous algorithm for such agents, working in time polynomial in the size of\nthe graph, in the length of the smaller label, and in the largest edge\ntraversal time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 13:46:08 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 13:45:25 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Pelc", "Andrzej", ""], ["Petit", "Franck", ""]]}, {"id": "1710.08296", "submitter": "Muktikanta Sa", "authors": "Sathya Peri, Muktikanta Sa and Nandini Singhal", "title": "Building Efficient Concurrent Graph Object through Composition of\n  List-based Set", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generic concurrent directed graph (for shared\nmemory architecture) that is concurrently being updated by threads\nadding/deleting vertices and edges. The graph is constructed by the composition\nof the well known concurrent list-based set data-structure from the literature.\nOur construction is generic, in the sense that it can be used to obtain various\nprogress guarantees, depending on the granularity of the underlying concurrent\nset implementation - either blocking or non-blocking. We prove that the\nproposed construction is linearizable by identifying its linearization points.\nFinally, we compare the performance of all the variants of the concurrent graph\ndata-structure along with its sequential implementation. We observe that our\nconcurrent graph data-structure mimics the performance of the concurrent list\nbased set.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 16:02:36 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 04:08:15 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Peri", "Sathya", ""], ["Sa", "Muktikanta", ""], ["Singhal", "Nandini", ""]]}, {"id": "1710.08332", "submitter": "Robert Atkey", "authors": "Robert Atkey, Michel Steuwer, Sam Lindley, Christophe Dubach", "title": "Strategy Preserving Compilation for Parallel Functional Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphics Processing Units (GPUs) and other parallel devices are widely\navailable and have the potential for accelerating a wide class of algorithms.\nHowever, expert programming skills are required to achieving maximum\nperformance. hese devices expose low-level hardware details through imperative\nprogramming interfaces where programmers explicity encode device-specific\noptimisation strategies. This inevitably results in non-performance-portable\nprograms delivering suboptimal performance on other devices.\n  Functional programming models have recently seen a renaissance in the systems\ncommunity as they offer possible solutions for tackling the performance\nportability challenge. Recent work has shown how to automatically choose\nhigh-performance parallelisation strategies for a wide range of hardware\narchitectures encoded in a functional representation. However, the translation\nof such functional representations to the imperative program expected by the\nhardware interface is typically performed ad hoc with no correctness guarantees\nand no guarantees to preserve the intended parallelisation strategy.\n  In this paper, we present a formalised strategy-preserving translation from\nhigh-level functional code to low-level data race free parallel imperative\ncode. This translation is formulated and proved correct within a language we\ncall Data Parallel Idealised Algol (DPIA), a dialect of Reynolds' Idealised\nAlgol. Performance results on GPUs and a multicore CPU show that the formalised\ntranslation process generates low-level code with performance on a par with\ncode generated from ad hoc approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 15:24:30 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Atkey", "Robert", ""], ["Steuwer", "Michel", ""], ["Lindley", "Sam", ""], ["Dubach", "Christophe", ""]]}, {"id": "1710.08381", "submitter": "Shreyas Pai", "authors": "Sayan Bandyapadhyay and Tanmay Inamdar and Shreyas Pai and Sriram V.\n  Pemmaraju", "title": "Near-Optimal Clustering in the $k$-machine model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering problem, in its many variants, has numerous applications in\noperations research and computer science (e.g., in applications in\nbioinformatics, image processing, social network analysis, etc.). As sizes of\ndata sets have grown rapidly, researchers have focused on designing algorithms\nfor clustering problems in models of computation suited for large-scale\ncomputation such as MapReduce, Pregel, and streaming models. The $k$-machine\nmodel (Klauck et al., SODA 2015) is a simple, message-passing model for\nlarge-scale distributed graph processing. This paper considers three of the\nmost prominent examples of clustering problems: the uncapacitated facility\nlocation problem, the $p$-median problem, and the $p$-center problem and\npresents $O(1)$-factor approximation algorithms for these problems running in\n$\\tilde{O}(n/k)$ rounds in the $k$-machine model. These algorithms are optimal\nup to polylogarithmic factors because this paper also shows\n$\\tilde{\\Omega}(n/k)$ lower bounds for obtaining polynomial-factor\napproximation algorithms for these problems. These are the first results for\nclustering problems in the $k$-machine model.\n  We assume that the metric provided as input for these clustering problems in\nonly implicitly provided, as an edge-weighted graph and in a nutshell, our main\ntechnical contribution is to show that constant-factor approximation algorithms\nfor all three clustering problems can be obtained by learning only a small\nportion of the input metric.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 16:57:50 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Inamdar", "Tanmay", ""], ["Pai", "Shreyas", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1710.08460", "submitter": "Vatche Ishakian", "authors": "Vatche Ishakian, Vinod Muthusamy, Aleksander Slominski", "title": "Serving deep learning models in a serverless platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has emerged as a compelling paradigm for the development\nand deployment of a wide range of event based cloud applications. At the same\ntime, cloud providers and enterprise companies are heavily adopting machine\nlearning and Artificial Intelligence to either differentiate themselves, or\nprovide their customers with value added services. In this work we evaluate the\nsuitability of a serverless computing environment for the inferencing of large\nneural network models. Our experimental evaluations are executed on the AWS\nLambda environment using the MxNet deep learning framework. Our experimental\nresults show that while the inferencing latency can be within an acceptable\nrange, longer delays due to cold starts can skew the latency distribution and\nhence risk violating more stringent SLAs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 19:01:10 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 21:41:18 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ishakian", "Vatche", ""], ["Muthusamy", "Vinod", ""], ["Slominski", "Aleksander", ""]]}, {"id": "1710.08471", "submitter": "Edward Hutter", "authors": "Edward Hutter, Edgar Solomonik", "title": "Communication-avoiding Cholesky-QR2 for rectangular matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable QR factorization algorithms for solving least squares and eigenvalue\nproblems are critical given the increasing parallelism within modern machines.\nWe introduce a more general parallelization of the CholeskyQR2 algorithm and\nshow its effectiveness for a wide range of matrix sizes. Our algorithm executes\nover a 3D processor grid, the dimensions of which can be tuned to trade-off\ncosts in synchronization, interprocessor communication, computational work, and\nmemory footprint. We implement this algorithm, yielding a code that can achieve\na factor of $\\Theta(P^{1/6})$ less interprocessor communication on $P$\nprocessors than any previous parallel QR implementation. Our performance study\non Intel Knights-Landing and Cray XE supercomputers demonstrates the\neffectiveness of this CholeskyQR2 parallelization on a large number of nodes.\nSpecifically, relative to ScaLAPACK's QR, on 1024 nodes of Stampede2, our\nCholeskyQR2 implementation is faster by 2.6x-3.3x in strong scaling tests and\nby 1.1x-1.9x in weak scaling tests.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 19:35:07 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 03:36:15 GMT"}, {"version": "v3", "created": "Sat, 25 Aug 2018 23:36:20 GMT"}, {"version": "v4", "created": "Thu, 18 Oct 2018 00:54:26 GMT"}, {"version": "v5", "created": "Wed, 9 Jan 2019 03:16:52 GMT"}, {"version": "v6", "created": "Sat, 15 Jun 2019 18:52:13 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Hutter", "Edward", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1710.08491", "submitter": "Vivekanandan Balasubramanian", "authors": "Vivek Balasubramanian, Matteo Turilli, Weiming Hu, Matthieu Lefebvre,\n  Wenjie Lei, Guido Cervone, Jeroen Tromp, Shantenu Jha", "title": "Harnessing the Power of Many: Extensible Toolkit for Scalable Ensemble\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific problems require multiple distinct computational tasks to be\nexecuted in order to achieve a desired solution. We introduce the Ensemble\nToolkit (EnTK) to address the challenges of scale, diversity and reliability\nthey pose. We describe the design and implementation of EnTK, characterize its\nperformance and integrate it with two distinct exemplar use cases: seismic\ninversion and adaptive analog ensembles. We perform nine experiments,\ncharacterizing EnTK overheads, strong and weak scalability, and the performance\nof two use case implementations, at scale and on production infrastructures. We\nshow how EnTK meets the following general requirements: (i) implementing\ndedicated abstractions to support the description and execution of ensemble\napplications; (ii) support for execution on heterogeneous computing\ninfrastructures; (iii) efficient scalability up to O(10^4) tasks; and (iv)\nfault tolerance. We discuss novel computational capabilities that EnTK enables\nand the scientific advantages arising thereof. We propose EnTK as an important\naddition to the suite of tools in support of production scientific computing.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 20:18:30 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 20:07:05 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 15:24:32 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Balasubramanian", "Vivek", ""], ["Turilli", "Matteo", ""], ["Hu", "Weiming", ""], ["Lefebvre", "Matthieu", ""], ["Lei", "Wenjie", ""], ["Cervone", "Guido", ""], ["Tromp", "Jeroen", ""], ["Jha", "Shantenu", ""]]}, {"id": "1710.08616", "submitter": "Michel M\\\"uller", "authors": "Michel M\\\"uller, Takayuki Aoki", "title": "Hybrid Fortran: High Productivity GPU Porting Framework Applied to\n  Japanese Weather Prediction Model", "comments": "Preprint as accepted for WACCPD 2017, final version to appear in\n  Springer LNCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we use the GPU porting task for the operative Japanese weather\nprediction model \"ASUCA\" as an opportunity to examine productivity issues with\nOpenACC when applied to structured grid problems. We then propose \"Hybrid\nFortran\", an approach that combines the advantages of directive based methods\n(no rewrite of existing code necessary) with that of stencil DSLs (memory\nlayout is abstracted). This gives the ability to define multiple\nparallelizations with different granularities in the same code. Without\ncompromising on performance, this approach enables a major reduction in the\ncode changes required to achieve a hybrid GPU/CPU parallelization - as\ndemonstrated with our ASUCA implementation using Hybrid Fortran.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 06:34:46 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 09:48:39 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["M\u00fcller", "Michel", ""], ["Aoki", "Takayuki", ""]]}, {"id": "1710.08632", "submitter": "Paolo Frasca", "authors": "Chiara Ravazzi, Nelson P. K. Chan, Paolo Frasca", "title": "Distributed estimation from relative measurements of heterogeneous and\n  uncertain quality", "comments": "Submitted to IEEE transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of estimation from relative measurements in a\ngraph, in which a vector indexed over the nodes has to be reconstructed from\npairwise measurements of differences between its components associated to nodes\nconnected by an edge. In order to model heterogeneity and uncertainty of the\nmeasurements, we assume them to be affected by additive noise distributed\naccording to a Gaussian mixture. In this original setup, we formulate the\nproblem of computing the Maximum-Likelihood (ML) estimates and we design two\nnovel algorithms, based on Least Squares regression and\nExpectation-Maximization (EM). The first algorithm (LS- EM) is centralized and\nperforms the estimation from relative measurements, the soft classification of\nthe measurements, and the estimation of the noise parameters. The second\nalgorithm (Distributed LS-EM) is distributed and performs estimation and soft\nclassification of the measurements, but requires the knowledge of the noise\nparameters. We provide rigorous proofs of convergence of both algorithms and we\npresent numerical experiments to evaluate and compare their performance with\nclassical solutions. The experiments show the robustness of the proposed\nmethods against different kinds of noise and, for the Distributed LS-EM,\nagainst errors in the knowledge of noise parameters.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 07:29:51 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 10:10:20 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Ravazzi", "Chiara", ""], ["Chan", "Nelson P. K.", ""], ["Frasca", "Paolo", ""]]}, {"id": "1710.08679", "submitter": "Takuma Yamaguchi", "authors": "Takuma Yamaguchi (1), Kohei Fujita (1 and 2), Tsuyoshi Ichimura (1 and\n  2), Muneo Hori (1 and 2), Maddegedara Lalith (1 and 2) and Kengo Nakajima (3)\n  ((1) Earthquake Research Institute and Department of Civil Engineering, The\n  University of Tokyo, (2) Advanced Institute for Computational Science, RIKEN,\n  (3) Information Technology Center, The University of Tokyo)", "title": "Implicit Low-Order Unstructured Finite-Element Multiple Simulation\n  Enhanced by Dense Computation using OpenACC", "comments": "18 pages, 10 figures, accepted for WACCPD2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a low-order three-dimensional finite-element solver\nfor fast multiple-case crust deformation analysis on GPU-based systems. Based\non a high-performance solver designed for massively parallel CPU based systems,\nwe modify the algorithm to reduce random data access, and then insert OpenACC\ndirectives. The developed solver on ten Reedbush-H nodes (20 P100 GPUs)\nattained speedup of 14.2 times from 20 K computer nodes, which is high\nconsidering the peak memory bandwidth ratio of 11.4 between the two systems. On\nthe newest Volta generation V100 GPUs, the solver attained a further 2.45 times\nspeedup from P100 GPUs. As a demonstrative example, we computed 368 cases of\ncrustal deformation analyses of northeast Japan with 400 million degrees of\nfreedom. The total procedure of algorithm modification and porting\nimplementation took only two weeks; we can see that high performance\nimprovement was achieved with low development cost. With the developed solver,\nwe can expect improvement in reliability of crust-deformation analyses by\nmany-case analyses on a wide range of GPU-based systems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 09:51:31 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Yamaguchi", "Takuma", "", "1 and 2"], ["Fujita", "Kohei", "", "1 and 2"], ["Ichimura", "Tsuyoshi", "", "1 and\n  2"], ["Hori", "Muneo", "", "1 and 2"], ["Lalith", "Maddegedara", "", "1 and 2"], ["Nakajima", "Kengo", ""]]}, {"id": "1710.08731", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Marco A. S. Netto, Rodrigo N. Calheiros, Eduardo R. Rodrigues, Renato\n  L. F. Cunha, Rajkumar Buyya", "title": "HPC Cloud for Scientific and Business Applications: Taxonomy, Vision,\n  and Research Challenges", "comments": "29 pages, 5 figures, Published in ACM Computing Surveys (CSUR)", "journal-ref": "ACM Computing Surveys (CSUR) Volume 51 Issue 1, January 2018\n  Article No. 8", "doi": "10.1145/3150224", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Performance Computing (HPC) clouds are becoming an alternative to\non-premise clusters for executing scientific applications and business\nanalytics services. Most research efforts in HPC cloud aim to understand the\ncost-benefit of moving resource-intensive applications from on-premise\nenvironments to public cloud platforms. Industry trends show hybrid\nenvironments are the natural path to get the best of the on-premise and cloud\nresources---steady (and sensitive) workloads can run on on-premise resources\nand peak demand can leverage remote resources in a pay-as-you-go manner.\nNevertheless, there are plenty of questions to be answered in HPC cloud, which\nrange from how to extract the best performance of an unknown underlying\nplatform to what services are essential to make its usage easier. Moreover, the\ndiscussion on the right pricing and contractual models to fit small and large\nusers is relevant for the sustainability of HPC clouds. This paper brings a\nsurvey and taxonomy of efforts in HPC cloud and a vision on what we believe is\nahead of us, including a set of research challenges that, once tackled, can\nhelp advance businesses and scientific discoveries. This becomes particularly\nrelevant due to the fast increasing wave of new HPC applications coming from\nbig data and artificial intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 12:25:06 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 13:09:33 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 13:42:19 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Netto", "Marco A. S.", ""], ["Calheiros", "Rodrigo N.", ""], ["Rodrigues", "Eduardo R.", ""], ["Cunha", "Renato L. F.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1710.08774", "submitter": "Jason Sewall", "authors": "Jason Sewall, Simon J. Pennycook", "title": "High-Performance Code Generation though Fusion and Vectorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for automatically transforming kernel-based\ncomputations in disparate, nested loops into a fused, vectorized form that can\nreduce intermediate storage needs and lead to improved performance on\ncontemporary hardware.\n  We introduce representations for the abstract relationships and data\ndependencies of kernels in loop nests and algorithms for manipulating them into\nmore efficient form; we similarly introduce techniques for determining data\naccess patterns for stencil-like array accesses and show how this can be used\nto elide storage and improve vectorization.\n  We discuss our prototype implementation of these ideas---named HFAV---and its\nuse of a declarative, inference-based front-end to drive transformations, and\nwe present results for some prominent codes in HPC.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 13:51:36 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Sewall", "Jason", ""], ["Pennycook", "Simon J.", ""]]}, {"id": "1710.08842", "submitter": "Wiktor Daszczuk", "authors": "Wiktor B. Daszczuk", "title": "Deadlock and Termination Detection using IMDS Formalism and Model\n  Checking. Version 2", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": "ICS WUT Research Report No. 2/2008", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern model checking techniques concentrate on global properties of verified\nsystems, because the methods base on global state space. Local features like\npartial deadlock or process termination are not easy to express and check. In\nthe paper a description of distributed system in an Integrated Model of\nDistributed Systems (IMDS) combined with model checking is presented. IMDS\nexpresses a dualism in distributed systems: server view and agent view. The\nformalism uses server states and messages. A progress in computations is\ndefined in terms of actions consuming and producing states and messages.\nDistributed actions are totally independent and they do not depend on global\nstate. Therefore, IMDS allows the designer to express local features of\nsubsystems. In this model it is easy to describe various kinds of deadlock\n(including partial deadlock) and to differentiate deadlock from termination.\nThe integration of IMDS with model checking is presented. Temporal formulas\ntesting various kinds of deadlock (in communication or over resources) and\ntermination are proposed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 15:23:07 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Daszczuk", "Wiktor B.", ""]]}, {"id": "1710.08883", "submitter": "Saeed Soori", "authors": "Saeed Soori, Aditya Devarakonda, James Demmel, Mert Gurbuzbalaban,\n  Maryam Mehri Dehnavi", "title": "Avoiding Communication in Proximal Methods for Convex Optimization\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast iterative soft thresholding algorithm (FISTA) is used to solve\nconvex regularized optimization problems in machine learning. Distributed\nimplementations of the algorithm have become popular since they enable the\nanalysis of large datasets. However, existing formulations of FISTA communicate\ndata at every iteration which reduces its performance on modern distributed\narchitectures. The communication costs of FISTA, including bandwidth and\nlatency costs, is closely tied to the mathematical formulation of the\nalgorithm. This work reformulates FISTA to communicate data at every k\niterations and reduce data communication when operating on large data sets. We\nformulate the algorithm for two different optimization methods on the Lasso\nproblem and show that the latency cost is reduced by a factor of k while\nbandwidth and floating-point operation costs remain the same. The convergence\nrates and stability properties of the reformulated algorithms are similar to\nthe standard formulations. The performance of communication-avoiding FISTA and\nProximal Newton methods is evaluated on 1 to 1024 nodes for multiple benchmarks\nand demonstrate average speedups of 3-10x with scaling properties that\noutperform the classical algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 16:47:23 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Soori", "Saeed", ""], ["Devarakonda", "Aditya", ""], ["Demmel", "James", ""], ["Gurbuzbalaban", "Mert", ""], ["Dehnavi", "Maryam Mehri", ""]]}, {"id": "1710.08938", "submitter": "Junyao Guo", "authors": "Junyao Guo, Gabriela Hug, Ozan Tonguz", "title": "Asynchronous ADMM for Distributed Non-Convex Optimization in Power\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale, non-convex optimization problems arising in many complex\nnetworks such as the power system call for efficient and scalable distributed\noptimization algorithms. Existing distributed methods are usually iterative and\nrequire synchronization of all workers at each iteration, which is hard to\nscale and could result in the under-utilization of computation resources due to\nthe heterogeneity of the subproblems. To address those limitations of\nsynchronous schemes, this paper proposes an asynchronous distributed\noptimization method based on the Alternating Direction Method of Multipliers\n(ADMM) for non-convex optimization. The proposed method only requires local\ncommunications and allows each worker to perform local updates with information\nfrom a subset of but not all neighbors. We provide sufficient conditions on the\nproblem formulation, the choice of algorithm parameter and network delay, and\nshow that under those mild conditions, the proposed asynchronous ADMM method\nasymptotically converges to the KKT point of the non-convex problem. We\nvalidate the effectiveness of asynchronous ADMM by applying it to the Optimal\nPower Flow problem in multiple power systems and show that the convergence of\nthe proposed asynchronous scheme could be faster than its synchronous\ncounterpart in large-scale applications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 18:12:52 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Guo", "Junyao", ""], ["Hug", "Gabriela", ""], ["Tonguz", "Ozan", ""]]}, {"id": "1710.08951", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Statistical considerations on limitations of supercomputers", "comments": "11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supercomputer building is a many sceene, many authors game, comprising a lot\nof different technologies, manufacturers and ideas. Checking data available in\nthe public database in a systematic way, some general tendencies and\nlimitations can be concluded, both for the past and the future. The feasibility\nof building exa-scale computers as well as their limitations and utilization\nare also discussed. The statistical considerations provide a strong support for\nthe conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 18:58:09 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 18:49:53 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 07:55:56 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1710.08961", "submitter": "Milad Makkie", "authors": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming\n  Liu", "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI\n  Big Data Analytics", "comments": "This work is submitted to SIGKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, analyzing task-based fMRI (tfMRI) data has become an\nessential tool for understanding brain function and networks. However, due to\nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of\nground truth of underlying neural activities, modeling tfMRI data is hard and\nchallenging. Previously proposed data-modeling methods including Independent\nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly\nestablished model based on blind source separation under the strong assumption\nthat original fMRI signals could be linearly decomposed into time series\ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a\nlarge amount of tfMRI data from a variety of subjects has been shown to be very\ndemanding but yet challenging even with technological advances in computational\nhardware. Given the Convolutional Neural Network (CNN), a robust method for\nlearning high-level abstractions from low-level data such as tfMRI time series,\nin this work we propose a fast and scalable novel framework for distributed\ndeep Convolutional Autoencoder model. This model aims to both learn the complex\nhierarchical structure of the tfMRI data and to leverage the processing power\nof multiple GPUs in a distributed fashion. To implement such a model, we have\ncreated an enhanced processing pipeline on the top of Apache Spark and\nTensorflow library, leveraging from a very large cluster of GPU machines.\nExperimental data from applying the model on the Human Connectome Project (HCP)\nshow that the proposed model is efficient and scalable toward tfMRI big data\nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific\ninformation from massive fMRI big data in the future.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 19:35:51 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 07:46:58 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 21:31:55 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Makkie", "Milad", ""], ["Huang", "Heng", ""], ["Zhao", "Yu", ""], ["Vasilakos", "Athanasios V.", ""], ["Liu", "Tianming", ""]]}, {"id": "1710.09001", "submitter": "Jingge Zhu", "authors": "Jingge Zhu, Ye Pu, Vipul Gupta, Claire Tomlin, Kannan Ramchandran", "title": "A Sequential Approximation Framework for Coded Distributed Optimization", "comments": "presented in 55th Annual Allerton Conference on Communication,\n  Control, and Computing, Oct. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on the previous work of Lee et al. and Ferdinand et al. on coded\ncomputation, we propose a sequential approximation framework for solving\noptimization problems in a distributed manner. In a distributed computation\nsystem, latency caused by individual processors (\"stragglers\") usually causes a\nsignificant delay in the overall process. The proposed method is powered by a\nsequential computation scheme, which is designed specifically for systems with\nstragglers. This scheme has the desirable property that the user is guaranteed\nto receive useful (approximate) computation results whenever a processor\nfinishes its subtask, even in the presence of uncertain latency. In this paper,\nwe give a coding theorem for sequentially computing matrix-vector\nmultiplications, and the optimality of this coding scheme is also established.\nAs an application of the results, we demonstrate solving optimization problems\nusing a sequential approximation approach, which accelerates the algorithm in a\ndistributed system with stragglers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 21:53:21 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Zhu", "Jingge", ""], ["Pu", "Ye", ""], ["Gupta", "Vipul", ""], ["Tomlin", "Claire", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1710.09025", "submitter": "Helmut Katzgraber", "authors": "Amin Barzegar, Christopher Pattison, Wenlong Wang, Helmut G.\n  Katzgraber", "title": "Optimization of population annealing Monte Carlo for large-scale\n  spin-glass simulations", "comments": "14 pages, 11 figures, 1 table. See also arXiv:1711.02146 by Amey and\n  Machta", "journal-ref": "Phys. Rev. E 98, 053308 (2018)", "doi": "10.1103/PhysRevE.98.053308", "report-no": null, "categories": "cond-mat.dis-nn cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population annealing Monte Carlo is an efficient sequential algorithm for\nsimulating k-local Boolean Hamiltonians. Because of its structure, the\nalgorithm is inherently parallel and therefore well suited for large-scale\nsimulations of computationally hard problems. Here we present various ways of\noptimizing population annealing Monte Carlo using 2-local spin-glass\nHamiltonians as a case study. We demonstrate how the algorithm can be optimized\nfrom an implementation, algorithmic accelerator, as well as scalable\nparallelization points of view. This makes population annealing Monte Carlo\nperfectly suited to study other frustrated problems such as pyrochlore\nlattices, constraint-satisfaction problems, as well as higher-order\nHamiltonians commonly found in, e.g., topological color codes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 00:10:12 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 02:02:18 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 09:08:55 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Barzegar", "Amin", ""], ["Pattison", "Christopher", ""], ["Wang", "Wenlong", ""], ["Katzgraber", "Helmut G.", ""]]}, {"id": "1710.09052", "submitter": "Jiechao Cheng", "authors": "Jiechao Cheng, Rui Ren, Lei Wang and Jianfeng Zhan", "title": "Deep Convolutional Neural Networks for Anomaly Event Classification on\n  Distributed Systems", "comments": "There was an error in the Results and Discussion parts of Experiments\n  section, figure 7 and table VII have some number confusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of server usage has brought a plenty of anomaly log\nevents, which have threatened a vast collection of machines. Recognizing and\ncategorizing the anomalous events thereby is a much salient work for our\nsystems, especially the ones generate the massive amount of data and harness it\nfor technology value creation and business development. To assist in focusing\non the classification and the prediction of anomaly events, and gaining\ncritical insights from system event records, we propose a novel log\npreprocessing method which is very effective to filter abundant information and\nretain critical characteristics. Additionally, a competitive approach for\nautomated classification of anomalous events detected from the distributed\nsystem logs with the state-of-the-art deep (Convolutional Neural Network)\narchitectures is proposed in this paper. We measure a series of deep CNN\nalgorithms with varied hyper-parameter combinations by using standard\nevaluation metrics, the results of our study reveals the advantages and\npotential capabilities of the proposed deep CNN models for anomaly event\nclassification tasks on real-world systems. The optimal classification\nprecision of our approach is 98.14%, which surpasses the popular traditional\nmachine learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 02:10:34 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 01:48:22 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Cheng", "Jiechao", ""], ["Ren", "Rui", ""], ["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""]]}, {"id": "1710.09074", "submitter": "Saurabh Hukerikar", "authors": "Saurabh Hukerikar and Christian Engelmann", "title": "A Pattern Language for High-Performance Computing Resilience", "comments": "Proceedings of the 22nd European Conference on Pattern Languages of\n  Programs", "journal-ref": null, "doi": "10.1145/3147704.3147718", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance computing systems (HPC) provide powerful capabilities for\nmodeling, simulation, and data analytics for a broad class of computational\nproblems. They enable extreme performance of the order of quadrillion\nfloating-point arithmetic calculations per second by aggregating the power of\nmillions of compute, memory, networking and storage components. With the\nrapidly growing scale and complexity of HPC systems for achieving even greater\nperformance, ensuring their reliable operation in the face of system\ndegradations and failures is a critical challenge. System fault events often\nlead the scientific applications to produce incorrect results, or may even\ncause their untimely termination. The sheer number of components in modern\nextreme-scale HPC systems and the complex interactions and dependencies among\nthe hardware and software components, the applications, and the physical\nenvironment makes the design of practical solutions that support fault\nresilience a complex undertaking. To manage this complexity, we developed a\nmethodology for designing HPC resilience solutions using design patterns. We\ncodified the well-known techniques for handling faults, errors and failures\nthat have been devised, applied and improved upon over the past three decades\nin the form of design patterns. In this paper, we present a pattern language to\nenable a structured approach to the development of HPC resilience solutions.\nThe pattern language reveals the relations among the resilience patterns and\nprovides the means to explore alternative techniques for handling a specific\nfault model that may have different efficiency and complexity characteristics.\nUsing the pattern language enables the design and implementation of\ncomprehensive resilience solutions as a set of interconnected resilience\npatterns that can be instantiated across layers of the system stack.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 04:44:28 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 03:19:21 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Hukerikar", "Saurabh", ""], ["Engelmann", "Christian", ""]]}, {"id": "1710.09183", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Terry Ferrett, Matthew C. Valenti, Arun Ross", "title": "Biometrics-as-a-Service: A Framework to Promote Innovative Biometric\n  Recognition in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric recognition, or simply biometrics, is the use of biological\nattributes such as face, fingerprints or iris in order to recognize an\nindividual in an automated manner. A key application of biometrics is\nauthentication; i.e., using said biological attributes to provide access by\nverifying the claimed identity of an individual. This paper presents a\nframework for Biometrics-as-a-Service (BaaS) that performs biometric matching\noperations in the cloud, while relying on simple and ubiquitous consumer\ndevices such as smartphones. Further, the framework promotes innovation by\nproviding interfaces for a plurality of software developers to upload their\nmatching algorithms to the cloud. When a biometric authentication request is\nsubmitted, the system uses a criteria to automatically select an appropriate\nmatching algorithm. Every time a particular algorithm is selected, the\ncorresponding developer is rendered a micropayment. This creates an innovative\nand competitive ecosystem that benefits both software developers and the\nconsumers. As a case study, we have implemented the following: (a) an ocular\nrecognition system using a mobile web interface providing user access to a\nbiometric authentication service, and (b) a Linux-based virtual machine\nenvironment used by software developers for algorithm development and\nsubmission.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 11:46:52 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Talreja", "Veeru", ""], ["Ferrett", "Terry", ""], ["Valenti", "Matthew C.", ""], ["Ross", "Arun", ""]]}, {"id": "1710.09209", "submitter": "Jad Hamza", "authors": "Alain Girault, Gregor G\\\"ossler, Rachid Guerraoui, Jad Hamza,\n  Dragos-Adrian Seredinschi", "title": "Monotonic Prefix Consistency in Distributed Systems", "comments": "Submitted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the issue of data consistency in distributed systems. Specifically,\nwe consider a distributed system that replicates its data at multiple sites,\nwhich is prone to partitions, and which is assumed to be available (in the\nsense that queries are always eventually answered). In such a setting, strong\nconsistency, where all replicas of the system apply synchronously every\noperation, is not possible to implement. However, many weaker consistency\ncriteria that allow a greater number of behaviors than strong consistency, are\nimplementable in available distributed systems. We focus on determining the\nstrongest consistency criterion that can be implemented in a convergent and\navailable distributed system that tolerates partitions. We focus on objects\nwhere the set of operations can be split into updates and queries. We show that\nno criterion stronger than Monotonic Prefix Consistency (MPC) can be\nimplemented.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 13:02:17 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 10:17:16 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Girault", "Alain", ""], ["G\u00f6ssler", "Gregor", ""], ["Guerraoui", "Rachid", ""], ["Hamza", "Jad", ""], ["Seredinschi", "Dragos-Adrian", ""]]}, {"id": "1710.09280", "submitter": "Jannik Sundermeier", "authors": "Daniel Jung, Christina Kolb, Christian Scheideler, Jannik Sundermeier", "title": "Competitive Routing in Hybrid Communication Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routing is a challenging problem for wireless ad hoc networks, especially\nwhen the nodes are mobile and spread so widely that in most cases multiple hops\nare needed to route a message from one node to another. In fact, it is known\nthat any online routing protocol has a poor performance in the worst case, in a\nsense that there is a distribution of nodes resulting in bad routing paths for\nthat protocol, even if the nodes know their geographic positions and the\ngeographic position of the destination of a message is known. The reason for\nthat is that radio holes in the ad hoc network may require messages to take\nlong detours in order to get to a destination, which are hard to find in an\nonline fashion.\n  In this paper, we assume that the wireless ad hoc network can make limited\nuse of long-range links provided by a global communication infrastructure like\na cellular infrastructure or a satellite in order to compute an abstraction of\nthe wireless ad hoc network that allows the messages to be sent along\nnear-shortest paths in the ad hoc network. We present distributed algorithms\nthat compute an abstraction of the ad hoc network in $\\mathcal{O}\\left(\\log ^2\nn\\right)$ time using long-range links, which results in $c$-competitive routing\npaths between any two nodes of the ad hoc network for some constant $c$ if the\nconvex hulls of the radio holes do not intersect. We also show that the storage\nneeded for the abstraction just depends on the number and size of the radio\nholes in the wireless ad hoc network and is independent on the total number of\nnodes, and this information just has to be known to a few nodes for the routing\nto work.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 14:55:48 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 09:23:54 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Jung", "Daniel", ""], ["Kolb", "Christina", ""], ["Scheideler", "Christian", ""], ["Sundermeier", "Jannik", ""]]}, {"id": "1710.09398", "submitter": "Sruti Gan Chaudhuri", "authors": "Arijit Sil and Sruti Gan Chaudhuri", "title": "Line Formation by Fat Robots under Limited Visibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a distributed algorithm for a set of tiny unit disc\nshaped robot to form a straight line. The robots are homoge- neous, autonomous,\nanonymous. They observe their surrounding up to a certain distance, compute\ndestinations to move to and move there. They do not have any explicit message\nsending or receiving capability. They forget their past observed or computed\ndata. The robots do not have any global coordinate system or origin. Each robot\nconsiders its position as its origin. However, they agree on the X and Y axis.\nThe robots are not aware of the total number of robots in the system. The\nalgorithm presented in this paper assures collision free movements of the\nrobots. To the best of our knowledge this paper is the first reported result on\nline formation by fat robots under limited visibility.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 18:07:42 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Sil", "Arijit", ""], ["Chaudhuri", "Sruti Gan", ""]]}, {"id": "1710.09423", "submitter": "Sruti Gan Chaudhuri", "authors": "Moumita Mondal and Sruti Gan Chaudhuri", "title": "Uniform Circle Formation by Transparent Fat Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of Uniform Circle Formation by n > 1\ntransparent disc robots (fat robots). The robots execute repetitive cycles of\nthe states look-compute-move in semi-synchronous manner where a set of robots\nexecute the cycle simultaneously. They do not communicate by any explicit\nmessage passing. However, they can sense or observes the positions of other\nrobots around themselves through sensors or camera. The robots are unable to\nrecover the past actions and observations. They have no unique identity. The\nrobots do not have any global coordinate system. They agree upon only y-axis\n(South and North direction). However, they do not have chirality or common\norientation of Y axis with respect to X axis. Being transparent the robots do\nnot cause any visual obstructions for other robots. But, they act as physical\nobstacles for other robots. This paper proposes a collision free movement\nstrategy for the robots to form a uniform circle (in other words convex regular\npolygon) executing finite number of cycles. To the best of our knowledge this\nis the first reported results on uniform circle formation for fat robots under\nthe considered model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 18:53:50 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Mondal", "Moumita", ""], ["Chaudhuri", "Sruti Gan", ""]]}, {"id": "1710.09593", "submitter": "Nhien-An Le-Khac", "authors": "Malika Bendechache, Nhien-An Le-Khac, M-Tahar Kechadi", "title": "Distributed Spatial Data Clustering as a New Approach for Big Data\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new approach for Big Data mining and analysis.\nThis new approach works well on distributed datasets and deals with data\nclustering task of the analysis. The approach consists of two main phases, the\nfirst phase executes a clustering algorithm on local data, assuming that the\ndatasets was already distributed among the system processing nodes. The second\nphase deals with the local clusters aggregation to generate global clusters.\nThis approach not only generates local clusters on each processing node in\nparallel, but also facilitates the formation of global clusters without prior\nknowledge of the number of the clusters, which many partitioning clustering\nalgorithm require. In this study, this approach was applied on spatial\ndatasets. The proposed aggregation phase is very efficient and does not involve\nthe exchange of large amounts of data between the processing nodes. The\nexperimental results show that the approach has super linear speed up, scales\nup very well, and can take advantage of the recent programming models, such as\nMapReduce model, as its results are not affected by the types of\ncommunications.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 08:52:12 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 21:29:41 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Bendechache", "Malika", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1710.09605", "submitter": "Tim Zeitz", "authors": "Michael Hamann, Ben Strasser, Dorothea Wagner and Tim Zeitz", "title": "Distributed Graph Clustering using Modularity and Map Equation", "comments": "14 pages, 3 figures; v3: Camera ready for Euro-Par 2018, more\n  details, more results; v2: extended experiments to include comparison with\n  competing algorithms, shortened for submission to Euro-Par 2018", "journal-ref": null, "doi": "10.1007/978-3-319-96983-1_49", "report-no": null, "categories": "cs.DS cs.DC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale, distributed graph clustering. Given an undirected\ngraph, our objective is to partition the nodes into disjoint sets called\nclusters. A cluster should contain many internal edges while being sparsely\nconnected to other clusters. In the context of a social network, a cluster\ncould be a group of friends. Modularity and map equation are established\nformalizations of this internally-dense-externally-sparse principle. We present\ntwo versions of a simple distributed algorithm to optimize both measures. They\nare based on Thrill, a distributed big data processing framework that\nimplements an extended MapReduce model. The algorithms for the two measures,\nDSLM-Mod and DSLM-Map, differ only slightly. Adapting them for similar quality\nmeasures is straight-forward. We conduct an extensive experimental study on\nreal-world graphs and on synthetic benchmark graphs with up to 68 billion\nedges. Our algorithms are fast while detecting clusterings similar to those\ndetected by other sequential, parallel and distributed clustering algorithms.\nCompared to the distributed GossipMap algorithm, DSLM-Map needs less memory, is\nup to an order of magnitude faster and achieves better quality.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 09:24:40 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 15:55:27 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 13:09:41 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hamann", "Michael", ""], ["Strasser", "Ben", ""], ["Wagner", "Dorothea", ""], ["Zeitz", "Tim", ""]]}, {"id": "1710.09921", "submitter": "Seo Jin Park", "authors": "Seo Jin Park and John Ousterhout", "title": "Exploiting Commutativity For Practical Fast Replication", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to replication require client requests to be ordered\nbefore making them durable by copying them to replicas. As a result, clients\nmust wait for two round-trip times (RTTs) before updates complete. In this\npaper, we show that this entanglement of ordering and durability is unnecessary\nfor strong consistency. Consistent Unordered Replication Protocol (CURP) allows\nclients to replicate requests that have not yet been ordered, as long as they\nare commutative. This strategy allows most operations to complete in 1 RTT (the\nsame as an unreplicated system). We implemented CURP in the Redis and RAMCloud\nstorage systems. In RAMCloud, CURP improved write latency by ~2x (13.8 us ->\n7.3 us) and write throughput by 4x. Compared to unreplicated RAMCloud, CURP's\nlatency overhead for 3-way replication is just 0.4 us (6.9 us vs 7.3 us). CURP\ntransformed a non-durable Redis cache into a consistent and durable storage\nsystem with only a small performance overhead.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 21:34:34 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Park", "Seo Jin", ""], ["Ousterhout", "John", ""]]}, {"id": "1710.09990", "submitter": "Songze Li", "authors": "Songze Li, Seyed Mohammadreza Mousavi Kalan, A. Salman Avestimehr,\n  Mahdi Soltanolkotabi", "title": "Near-Optimal Straggler Mitigation for Distributed Gradient Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern learning algorithms use gradient descent updates to train inferential\nmodels that best explain data. Scaling these approaches to massive data sizes\nrequires proper distributed gradient descent schemes where distributed worker\nnodes compute partial gradients based on their partial and local data sets, and\nsend the results to a master node where all the computations are aggregated\ninto a full gradient and the learning model is updated. However, a major\nperformance bottleneck that arises is that some of the worker nodes may run\nslow. These nodes a.k.a. stragglers can significantly slow down computation as\nthe slowest node may dictate the overall computational time. We propose a\ndistributed computing scheme, called Batched Coupon's Collector (BCC) to\nalleviate the effect of stragglers in gradient methods. We prove that our BCC\nscheme is robust to a near optimal number of random stragglers. We also\nempirically demonstrate that our proposed BCC scheme reduces the run-time by up\nto 85.4% over Amazon EC2 clusters when compared with other straggler mitigation\nstrategies. We also generalize the proposed BCC scheme to minimize the\ncompletion time when implementing gradient descent-based algorithms over\nheterogeneous worker nodes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 04:46:44 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Li", "Songze", ""], ["Kalan", "Seyed Mohammadreza Mousavi", ""], ["Avestimehr", "A. Salman", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1710.10090", "submitter": "Blesson Varghese", "authors": "Blesson Varghese, Nan Wang, Jianyu Li, Dimitrios S. Nikolopoulos", "title": "Edge-as-a-Service: Towards Distributed Cloud Architectures", "comments": "10 pages; presented at the EdgeComp Symposium 2017; will appear in\n  Proceedings of the International Conference on Parallel Computing, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an Edge-as-a-Service (EaaS) platform for realising distributed\ncloud architectures and integrating the edge of the network in the computing\necosystem. The EaaS platform is underpinned by (i) a lightweight discovery\nprotocol that identifies edge nodes and make them publicly accessible in a\ncomputing environment, and (ii) a scalable resource provisioning mechanism for\noffloading workloads from the cloud on to the edge for servicing multiple user\nrequests. We validate the feasibility of EaaS on an online game use-case to\nhighlight the improvement in the QoS of the application hosted on our\ncloud-edge platform. On this platform we demonstrate (i) low overheads of less\nthan 6%, (ii) reduced data traffic to the cloud by up to 95% and (iii)\nminimised application latency between 40%-60%.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:48:28 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Varghese", "Blesson", ""], ["Wang", "Nan", ""], ["Li", "Jianyu", ""], ["Nikolopoulos", "Dimitrios S.", ""]]}, {"id": "1710.10091", "submitter": "Mathias Rav", "authors": "Lars Arge, Mathias Rav, Svend C. Svendsen, Jakob Truelsen", "title": "External Memory Pipelining Made Easy With TPIE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When handling large datasets that exceed the capacity of the main memory,\nmovement of data between main memory and external memory (disk), rather than\nactual (CPU) computation time, is often the bottleneck in the computation.\nSince data is moved between disk and main memory in large contiguous blocks,\nthis has led to the development of a large number of I/O-efficient algorithms\nthat minimize the number of such block movements.\n  TPIE is one of two major libraries that have been developed to support\nI/O-efficient algorithm implementations. TPIE provides an interface where list\nstream processing and sorting can be implemented in a simple and modular way\nwithout having to worry about memory management or block movement. However, if\ncare is not taken, such streaming-based implementations can lead to practically\ninefficient algorithms since lists of data items are typically written to (and\nread from) disk between components.\n  In this paper we present a major extension of the TPIE library that includes\na pipelining framework that allows for practically efficient streaming-based\nimplementations while minimizing I/O-overhead between streaming components. The\nframework pipelines streaming components to avoid I/Os between components, that\nis, it processes several components simultaneously while passing output from\none component directly to the input of the next component in main memory. TPIE\nautomatically determines which components to pipeline and performs the required\nmain memory management, and the extension also includes support for\nparallelization of internal memory computation and progress tracking across an\nentire application. The extended library has already been used to evaluate\nI/O-efficient algorithms in the research literature and is heavily used in\nI/O-efficient commercial terrain processing applications by the Danish startup\nSCALGO.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:52:14 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Arge", "Lars", ""], ["Rav", "Mathias", ""], ["Svendsen", "Svend C.", ""], ["Truelsen", "Jakob", ""]]}, {"id": "1710.10490", "submitter": "Leonid Sokolinsky", "authors": "Leonid B. Sokolinsky", "title": "Analytical Estimation of the Scalability of Iterative Numerical\n  Algorithms on Distributed Memory Multiprocessors", "comments": "Submitted to a special issue of Lobachevskii Journal of Mathematics\n  on \"Parallel Structure of Algorithms\"", "journal-ref": null, "doi": "10.1134/S1995080218040121", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new high-level parallel computational model named BSF\n- Bulk Synchronous Farm. The BSF model extends the BSP model to deal with the\ncompute-intensive iterative numerical methods executed on distributed-memory\nmultiprocessor systems. The BSF model is based on the master-worker paradigm\nand the SPMD programming model. The BSF model makes it possible to predict the\nupper scalability bound of a BSF-program with great accuracy. The BSF model\nalso provides equations for estimating the speedup and parallel efficiency of a\nBSF-program.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 16:17:23 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 16:06:56 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Sokolinsky", "Leonid B.", ""]]}, {"id": "1710.10756", "submitter": "Ond\\v{r}ej Leng\\'al", "authors": "Ondrej Lengal and Anthony W. Lin and Rupak Majumdar and Philipp\n  Ruemmer", "title": "Fair Termination for Parameterized Probabilistic Concurrent Systems\n  (Technical Report)", "comments": "A technical report of a TACAS'17 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automatically verifying that a parameterized\nfamily of probabilistic concurrent systems terminates with probability one for\nall instances against adversarial schedulers. A parameterized family defines an\ninfinite-state system: for each number n, the family consists of an instance\nwith n finite-state processes. In contrast to safety, the parameterized\nverification of liveness is currently still considered extremely challenging\nespecially in the presence of probabilities in the model. One major challenge\nis to provide a sufficiently powerful symbolic framework. One well-known\nsymbolic framework for the parameterized verification of non-probabilistic\nconcurrent systems is regular model checking. Although the framework was\nrecently extended to probabilistic systems, incorporating fairness in the\nframework - often crucial for verifying termination - has been especially\ndifficult due to the presence of an infinite number of fairness constraints\n(one for each process). Our main contribution is a systematic,\nregularity-preserving, encoding of finitary fairness (a realistic notion of\nfairness proposed by Alur & Henzinger) in the framework of regular model\nchecking for probabilistic parameterized systems. Our encoding reduces\ntermination with finitary fairness to verifying parameterized termination\nwithout fairness over probabilistic systems in regular model checking (for\nwhich a verification framework already exists). We show that our algorithm\ncould verify termination for many interesting examples from distributed\nalgorithms (Herman's protocol) and evolutionary biology (Moran process, cell\ncycle switch), which do not hold under the standard notion of fairness. To the\nbest of our knowledge, our algorithm is the first fully-automatic method that\ncan prove termination for these examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 03:33:46 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Lengal", "Ondrej", ""], ["Lin", "Anthony W.", ""], ["Majumdar", "Rupak", ""], ["Ruemmer", "Philipp", ""]]}, {"id": "1710.10769", "submitter": "Penporn Koanantakool", "authors": "Penporn Koanantakool, Alnur Ali, Ariful Azad, Aydin Buluc, Dmitriy\n  Morozov, Leonid Oliker, Katherine Yelick, Sang-Yun Oh", "title": "Communication-Avoiding Optimization Methods for Distributed\n  Massive-Scale Sparse Inverse Covariance Estimation", "comments": "Main paper: 15 pages, appendix: 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across a variety of scientific disciplines, sparse inverse covariance\nestimation is a popular tool for capturing the underlying dependency\nrelationships in multivariate data. Unfortunately, most estimators are not\nscalable enough to handle the sizes of modern high-dimensional data sets (often\non the order of terabytes), and assume Gaussian samples. To address these\ndeficiencies, we introduce HP-CONCORD, a highly scalable optimization method\nfor estimating a sparse inverse covariance matrix based on a regularized\npseudolikelihood framework, without assuming Gaussianity. Our parallel proximal\ngradient method uses a novel communication-avoiding linear algebra algorithm\nand runs across a multi-node cluster with up to 1k nodes (24k cores), achieving\nparallel scalability on problems with up to ~819 billion parameters (1.28\nmillion dimensions); even on a single node, HP-CONCORD demonstrates\nscalability, outperforming a state-of-the-art method. We also use HP-CONCORD to\nestimate the underlying dependency structure of the brain from fMRI data, and\nuse the result to identify functional regions automatically. The results show\ngood agreement with a clustering from the neuroscience literature.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 04:32:41 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 16:06:51 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Koanantakool", "Penporn", ""], ["Ali", "Alnur", ""], ["Azad", "Ariful", ""], ["Buluc", "Aydin", ""], ["Morozov", "Dmitriy", ""], ["Oliker", "Leonid", ""], ["Yelick", "Katherine", ""], ["Oh", "Sang-Yun", ""]]}, {"id": "1710.10835", "submitter": "Nadezhda Ezhova", "authors": "Nadezhda Ezhova", "title": "Verification of BSF Parallel Computational Model", "comments": "Accepted at the 3rd Ural Workshop on Parallel, Distributed, and Cloud\n  Computing for Young Scientists (Ural-PDC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article is devoted to the verification of the BSF parallel computing\nmodel. The BSF-model is an evolution of the \"master-slave\" model and\nSPMD-model. The BSF-model is oriented to iterative algorithms that are\nimplemented in cluster computing systems. The article briefly describes the\nbasics of the BSF-model and its cost metrics. The structure of the BSF program\nis shown in the form of a UML activity diagram. The simulator of BSF-programs,\nimplemented in C++ language using the MPI-library, is described. The results of\ncomputational experiments confirming the adequacy of the cost metrics of the\nBSF-model are presented.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 09:57:10 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ezhova", "Nadezhda", ""]]}, {"id": "1710.10899", "submitter": "Michael Lass", "authors": "Michael Lass and Stephan Mohr and Hendrik Wiebeler and Thomas D.\n  K\\\"uhne and Christian Plessl", "title": "A Massively Parallel Algorithm for the Approximate Calculation of\n  Inverse p-th Roots of Large Sparse Matrices", "comments": null, "journal-ref": null, "doi": "10.1145/3218176.3218231", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the submatrix method, a highly parallelizable method for the\napproximate calculation of inverse p-th roots of large sparse symmetric\nmatrices which are required in different scientific applications. We follow the\nidea of Approximate Computing, allowing imprecision in the final result in\norder to be able to utilize the sparsity of the input matrix and to allow\nmassively parallel execution. For an n x n matrix, the proposed algorithm\nallows to distribute the calculations over n nodes with only little\ncommunication overhead. The approximate result matrix exhibits the same\nsparsity pattern as the input matrix, allowing for efficient reuse of allocated\ndata structures.\n  We evaluate the algorithm with respect to the error that it introduces into\ncalculated results, as well as its performance and scalability. We demonstrate\nthat the error is relatively limited for well-conditioned matrices and that\nresults are still valuable for error-resilient applications like\npreconditioning even for ill-conditioned matrices. We discuss the execution\ntime and scaling of the algorithm on a theoretical level and present a\ndistributed implementation of the algorithm using MPI and OpenMP. We\ndemonstrate the scalability of this implementation by running it on a\nhigh-performance compute cluster comprised of 1024 CPU cores, showing a speedup\nof 665x compared to single-threaded execution.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 12:33:35 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 13:18:53 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 13:50:40 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Lass", "Michael", ""], ["Mohr", "Stephan", ""], ["Wiebeler", "Hendrik", ""], ["K\u00fchne", "Thomas D.", ""], ["Plessl", "Christian", ""]]}, {"id": "1710.10911", "submitter": "Shreya Tayade", "authors": "Shreya Tayade and Peter Rost and Andreas Maeder and Hans D. Schotten", "title": "Device-centric Energy Optimization for Edge Cloud Offloading", "comments": "7 pages, 4 figures, Accepted for IEEE Globecom 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wireless system is considered, where, computationally complex algorithms\nare offloaded from user devices to an edge cloud server, for the purpose of\nefficient battery usage. The main focus of this paper is to characterize and\nanalyze, the trade-off between the energy consumed for processing the data\nlocally, and for offloading. An analytical framework is presented, that\nminimizes the in-device energy consumption, by providing an optimal offloading\ndecision for multiple user devices. A closed form solution is obtained for the\noffloading decision. The solution also provides the amount of computational\ndata that should be offloaded, for the given computational and communication\nresources. Consequently, reduction in the energy consumption is observed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 12:54:00 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Tayade", "Shreya", ""], ["Rost", "Peter", ""], ["Maeder", "Andreas", ""], ["Schotten", "Hans D.", ""]]}, {"id": "1710.11001", "submitter": "Carla Mouradian", "authors": "Carla Mouradian, Diala Naboulsi, Sami Yangui, Roch H. Glitho, Monique\n  J. Morrow, and Paul A. Polakos", "title": "A Comprehensive Survey on Fog Computing: State-of-the-art and Research\n  Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing with its three key facets (i.e., IaaS, PaaS, and SaaS) and\nits inherent advantages (e.g., elasticity and scalability) still faces several\nchallenges. The distance between the cloud and the end devices might be an\nissue for latency-sensitive applications such as disaster management and\ncontent delivery applications. Service Level Agreements (SLAs) may also impose\nprocessing at locations where the cloud provider does not have data centers.\nFog computing is a novel paradigm to address such issues. It enables\nprovisioning resources and services outside the cloud, at the edge of the\nnetwork, closer to end devices or eventually, at locations stipulated by SLAs.\nFog computing is not a substitute for cloud computing but a powerful\ncomplement. It enables processing at the edge while still offering the\npossibility to interact with the cloud. This article presents a comprehensive\nsurvey on fog computing. It critically reviews the state of the art in the\nlight of a concise set of evaluation criteria. We cover both the architectures\nand the algorithms that make fog systems. Challenges and research directions\nare also introduced. In addition, the lessons learned are reviewed and the\nprospects are discussed in terms of the key role fog is likely to play in\nemerging technologies such as Tactile Internet.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 15:12:43 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 14:28:31 GMT"}, {"version": "v3", "created": "Wed, 1 Nov 2017 18:21:01 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Mouradian", "Carla", ""], ["Naboulsi", "Diala", ""], ["Yangui", "Sami", ""], ["Glitho", "Roch H.", ""], ["Morrow", "Monique J.", ""], ["Polakos", "Paul A.", ""]]}, {"id": "1710.11057", "submitter": "Christian Weilbach", "authors": "Christian Weilbach, Annette Bieniusa", "title": "Techreport: Time-sensitive probabilistic inference for the edge", "comments": "11 pages, techreport from research in the Lightkone H2020 Project for\n  edge computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the two trends of edge computing and artificial intelligence\nbecame both crucial for information processing infrastructures. While the\ncentralized analysis of massive amounts of data seems to be at odds with\ncomputation on the outer edge of distributed systems, we explore the properties\nof eventually consistent systems and statistics to identify sound formalisms\nfor probabilistic inference on the edge. In particular we treat time itself as\na random variable that we incorporate into statistical models through\nprobabilistic programming.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:42:19 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 12:20:17 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Weilbach", "Christian", ""], ["Bieniusa", "Annette", ""]]}, {"id": "1710.11222", "submitter": "Saurabh Bagchi", "authors": "Paul Wood, Heng Zhang, Muhammad-Bilal Siddiqui, Saurabh Bagchi", "title": "Dependability in Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is the practice of placing computing resources at the edges of\nthe Internet in close proximity to devices and information sources. This, much\nlike a cache on a CPU, increases bandwidth and reduces latency for applications\nbut at a potential cost of dependability and capacity. This is because these\nedge devices are often not as well maintained, dependable, powerful, or robust\nas centralized server-class cloud resources. This article explores\ndependability and deployment challenges in the field of edge computing, what\naspects are solvable with today's technology, and what aspects call for new\nsolutions.\n  The first issue addressed is failures, both hard (crash, hang, etc.) and soft\n(performance-related), and real-time constraint violation. In this domain, edge\ncomputing bolsters real-time system capacity through reduced end-to-end\nlatency. However, much like cache misses, overloaded or malfunctioning edge\ncomputers can drive latency beyond tolerable limits. Second, decentralized\nmanagement and device tampering can lead to chain of trust and security or\nprivacy violations. Authentication, access control, and distributed intrusion\ndetection techniques have to be extended from current cloud deployments and\nneed to be customized for the edge ecosystem. The third issue deals with\nhandling multi-tenancy in the typically resource-constrained edge devices and\nthe need for standardization to allow for interoperability across vendor\nproducts.\n  We explore the key challenges in each of these three broad issues as they\nrelate to dependability of edge computing and then hypothesize about promising\navenues of work in this area.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 04:03:46 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Wood", "Paul", ""], ["Zhang", "Heng", ""], ["Siddiqui", "Muhammad-Bilal", ""], ["Bagchi", "Saurabh", ""]]}, {"id": "1710.11246", "submitter": "Saman Ashkiani", "authors": "Saman Ashkiani, Martin Farach-Colton, John D. Owens", "title": "A Dynamic Hash Table for the GPU", "comments": "11 pages, accepted to appear on the Proceedings of IEEE International\n  Parallel and Distributed Processing Symposium (IPDPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We design and implement a fully concurrent dynamic hash table for GPUs with\ncomparable performance to the state of the art static hash tables. We propose a\nwarp-cooperative work sharing strategy that reduces branch divergence and\nprovides an efficient alternative to the traditional way of per-thread (or\nper-warp) work assignment and processing. By using this strategy, we build a\ndynamic non-blocking concurrent linked list, the slab list, that supports\nasynchronous, concurrent updates (insertions and deletions) as well as search\nqueries. We use the slab list to implement a dynamic hash table with chaining\n(the slab hash). On an NVIDIA Tesla K40c GPU, the slab hash performs updates\nwith up to 512 M updates/s and processes search queries with up to 937 M\nqueries/s. We also design a warp-synchronous dynamic memory allocator,\nSlabAlloc, that suits the high performance needs of the slab hash. SlabAlloc\ndynamically allocates memory at a rate of 600 M allocations/s, which is up to\n37x faster than alternative methods in similar scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 21:20:39 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 00:46:39 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Ashkiani", "Saman", ""], ["Farach-Colton", "Martin", ""], ["Owens", "John D.", ""]]}, {"id": "1710.11351", "submitter": "Keisuke Fukuda", "authors": "Takuya Akiba and Keisuke Fukuda and Shuji Suzuki", "title": "ChainerMN: Scalable Distributed Deep Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the keys for deep learning to have made a breakthrough in various\nfields was to utilize high computing powers centering around GPUs. Enabling the\nuse of further computing abilities by distributed processing is essential not\nonly to make the deep learning bigger and faster but also to tackle unsolved\nchallenges. We present the design, implementation, and evaluation of ChainerMN,\nthe distributed deep learning framework we have developed. We demonstrate that\nChainerMN can scale the learning process of the ResNet-50 model to the ImageNet\ndataset up to 128 GPUs with the parallel efficiency of 90%.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 07:13:29 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Akiba", "Takuya", ""], ["Fukuda", "Keisuke", ""], ["Suzuki", "Shuji", ""]]}, {"id": "1710.11590", "submitter": "Ashraf Shahin", "authors": "Amal S. Alzahrani and Ashraf A. Shahin", "title": "Energy-Aware Virtual Network Embedding Approach for Distributed Cloud", "comments": "International Journal of Advanced Computer Science and\n  Applications(IJACSA)", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), 8(10), 2017", "doi": "10.14569/IJACSA.2017.081031", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network virtualization has caught the attention of many researchers in recent\nyears. It facilitates the process of creating several virtual networks over a\nsingle physical network. Despite this advantage, however, network\nvirtualization suffers from the problem of mapping virtual links and nodes to\nphysical network in most efficient way. This problem is called virtual network\nembedding (\"VNE\"). Many researches have been proposed in an attempt to solve\nthis problem, which have many optimization aspects, such as improving embedding\nstrategies in a way that preserves energy, reducing embedding cost and\nincreasing embedding revenue. Moreover, some researchers have extended their\nalgorithms to be more compatible with the distributed clouds instead of a\nsingle infrastructure provider (\"ISP\"). This paper proposes energy aware\nparticle swarm optimization algorithm for distributed clouds. This algorithm\naims to partition each virtual network request (\"VNR\") to subgraphs, using the\nHeavy Clique Matching technique (\"HCM\") to generate a coarsened graph. Each\ncoarsened node in the coarsened graph is assigned to a suitable data center\n(\"DC\"). Inside each DC, a modified particle swarm optimization algorithm is\ninitiated to find the near optimal solution for the VNE problem. The proposed\nalgorithm was tested and evaluated against existing algorithms using extensive\nsimulations, which shows that the proposed algorithm outperforms other\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:08:44 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Alzahrani", "Amal S.", ""], ["Shahin", "Ashraf A.", ""]]}]