[{"id": "1912.00131", "submitter": "Keith Bonawitz", "authors": "Keith Bonawitz, Fariborz Salehi, Jakub Kone\\v{c}n\\'y, Brendan McMahan,\n  Marco Gruteser", "title": "Federated Learning with Autotuned Communication-Efficient Secure\n  Aggregation", "comments": "5 pages, 3 figures. To appear at the IEEE Asilomar Conference on\n  Signals, Systems, and Computers 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning enables mobile devices to collaboratively learn a shared\ninference model while keeping all the training data on a user's device,\ndecoupling the ability to do machine learning from the need to store the data\nin the cloud. Existing work on federated learning with limited communication\ndemonstrates how random rotation can enable users' model updates to be\nquantized much more efficiently, reducing the communication cost between users\nand the server. Meanwhile, secure aggregation enables the server to learn an\naggregate of at least a threshold number of device's model contributions\nwithout observing any individual device's contribution in unaggregated form. In\nthis paper, we highlight some of the challenges of setting the parameters for\nsecure aggregation to achieve communication efficiency, especially in the\ncontext of the aggressively quantized inputs enabled by random rotation. We\nthen develop a recipe for auto-tuning communication-efficient secure\naggregation, based on specific properties of random rotation and secure\naggregation -- namely, the predictable distribution of vector entries\npost-rotation and the modular wrapping inherent in secure aggregation. We\npresent both theoretical results and initial experiments.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 04:27:27 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bonawitz", "Keith", ""], ["Salehi", "Fariborz", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["McMahan", "Brendan", ""], ["Gruteser", "Marco", ""]]}, {"id": "1912.00245", "submitter": "Christian Schulz", "authors": "Christian Schulz", "title": "Scalable Graph Algorithms", "comments": "Habilitation thesis of Christian Schulz", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing large complex networks recently attracted considerable interest.\nComplex graphs are useful in a wide range of applications from technological\nnetworks to biological systems like the human brain. Sometimes these networks\nare composed of billions of entities that give rise to emerging properties and\nstructures. Analyzing these structures aids us in gaining new insights about\nour surroundings. As huge networks become abundant, there is a need for\nscalable algorithms to perform analysis. A prominent example is the PageRank\nalgorithm, which is one of the measures used by web search engines such as\nGoogle to rank web pages displayed to the user. In order to find these\npatterns, massive amounts of data have to be acquired and processed. Designing\nand evaluating scalable graph algorithms to handle these data sets is a crucial\ntask on the road to understanding the underlying systems.\n  This habilitation thesis is a summary a broad spectrum of scalable graph\nalgorithms that I developed over the last six years with many coauthors. In\ngeneral, this research is based on four pillars: multilevel algorithms,\npractical kernelization, parallelization and memetic algorithms that are highly\ninterconnected. Experiments conducted indicate that our algorithms find better\nsolutions and/or are much more scalable than the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 18:01:43 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Schulz", "Christian", ""]]}, {"id": "1912.00264", "submitter": "Chenglong Fu", "authors": "Chenglong Fu, Qiang Zeng, Xiaojiang Du", "title": "Towards Efficient Integration of Blockchain for IoT Security: The Case\n  Study of IoT Remote Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The booming Internet of Things (IoT) market has drawn tremendous interest\nfrom cyber attackers. The centralized cloud-based IoT service architecture has\nserious limitations in terms of security, availability, and scalability, and is\nsubject to single points of failure (SPOF). Recently, accommodating IoT\nservices on blockchains has become a trend for better security, privacy, and\nreliability. However, blockchain's shortcomings of high cost, low throughput,\nand long latency make it unsuitable for IoT applications. In this paper, we\ntake a retrospection of existing blockchain-based IoT solutions and propose a\nframework for efficient blockchain and IoT integration. Following the\nframework, we design a novel blockchain-assisted decentralized IoT remote\naccessing system, RS-IoT, which has the advantage of defending IoT devices\nagainst zero-day attacks without relying on any trusted third-party. By\nintroducing incentives and penalties enforced by smart contracts, our work\nenables \"an economic approach\" to thwarting the majority of attackers who aim\nto achieve monetary gains. Our work presents an example of how blockchain can\nbe used to ensure the fairness of service trading in a decentralized\nenvironment and punish misbehaviors objectively. We show the security of RS-IoT\nvia detailed security analyses. Finally, we demonstrate its scalability,\nefficiency, and usability through a proof-of-concept implementation on the\nEthereum testnet blockchain.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 21:13:58 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Fu", "Chenglong", ""], ["Zeng", "Qiang", ""], ["Du", "Xiaojiang", ""]]}, {"id": "1912.00539", "submitter": "Aditya Kashi", "authors": "Aditya Kashi and Siva Nadarajah", "title": "An asynchronous incomplete block LU preconditioner for computational\n  fluid dynamics on unstructured grids", "comments": "Accepted by SIAM SISC", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a study of the effectiveness of asynchronous incomplete LU\nfactorization preconditioners for the time-implicit solution of compressible\nflow problems while exploiting thread-parallelism within a compute node. A\nblock variant of the asynchronous fine-grain parallel preconditioner adapted to\na finite volume discretization of the compressible Navier-Stokes equations on\nunstructured grids is presented, and convergence theory is extended to the new\nvariant. Experimental (numerical) results on the performance of these\npreconditioners on inviscid and viscous laminar two-dimensional steady-state\ntest cases are reported. It is found, for these compressible flow problems,\nthat the block variant performs much better in terms of convergence, parallel\nscalability and reliability than the original scalar asynchronous ILU\npreconditioner. For viscous flow, it is found that the ordering of unknowns may\ndetermine the success or failure of asynchronous block-ILU preconditioning, and\nan ordering of grid cells suitable for solving viscous problems is presented.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 01:45:19 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 10:36:42 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kashi", "Aditya", ""], ["Nadarajah", "Siva", ""]]}, {"id": "1912.00547", "submitter": "Alexey Svyatkovskiy", "authors": "Alexey Svyatkovskiy, Kosuke Imai, Mary Kroeger, Yuki Shiraito", "title": "Large-scale text processing pipeline with Apache Spark", "comments": null, "journal-ref": "Published in Proceedings of Big NLP workshop at the IEEE Big Data\n  Conference 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate Apache Spark for a data-intensive machine learning\nproblem. Our use case focuses on policy diffusion detection across the state\nlegislatures in the United States over time. Previous work on policy diffusion\nhas been unable to make an all-pairs comparison between bills due to\ncomputational intensity. As a substitute, scholars have studied single topic\nareas.\n  We provide an implementation of this analysis workflow as a distributed text\nprocessing pipeline with Spark dataframes and Scala application programming\ninterface. We discuss the challenges and strategies of unstructured data\nprocessing, data formats for storage and efficient access, and graph processing\nat scale.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 02:12:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Svyatkovskiy", "Alexey", ""], ["Imai", "Kosuke", ""], ["Kroeger", "Mary", ""], ["Shiraito", "Yuki", ""]]}, {"id": "1912.00549", "submitter": "Vatche Ishakian", "authors": "Vatche Ishakian and Azer Bestavros and Assaf Kfoury", "title": "MORPHOSYS: Efficient Colocation of QoS-Constrained Workloads in the\n  Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hosting environments such as IaaS clouds, desirable application\nperformance is usually guaranteed through the use of Service Level Agreements\n(SLAs), which specify minimal fractions of resource capacities that must be\nallocated for use for proper operation. Arbitrary colocation of applications\nwith different SLAs on a single host may result in inefficient utilization of\nthe host's resources. In this paper, we propose that periodic resource\nallocation and consumption models be used for a more granular expression of\nSLAs. Our proposed SLA model has the salient feature that it exposes\nflexibilities that enable the IaaS provider to safelya transform SLAs from one\nform to another for the purpose of achieving more efficient colocation. Towards\nthat goal, we present MorphoSys: a framework for a service that allows the\nmanipulation of SLAs to enable efficient colocation of workloads. We present\nresults from extensive trace-driven simulations of colocated Video-on-Demand\nservers in a cloud setting. The results show that potentially-significant\nreduction in wasted resources (by as much as 60%) are possible using MorphoSys.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 02:15:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ishakian", "Vatche", ""], ["Bestavros", "Azer", ""], ["Kfoury", "Assaf", ""]]}, {"id": "1912.00642", "submitter": "Yongrae Jo", "authors": "Yongrae Jo, Chanik Park", "title": "BlockLot: Blockchain based Verifiable Lottery", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose BlockLot, a blockchain based verifiable lottery. BlockLot provides\ntransparent, immutable, fair, and verifiable lottery services enhanced by\nrecent blockchain technologies such as append-only (replicated) distributed\nledger and smart contract. In addition, BlockLot allows all participants to\nperform various verification to ensure that the system is actually working as\nexpected. We implement BlockLot services which includes open, query, subscribe,\nand draw in smart contracts. We also develop webbased user interface for using\nthe lottery services provided by BlockLot. The web interface allows the user to\nverify the lottery as well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:10:56 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Jo", "Yongrae", ""], ["Park", "Chanik", ""]]}, {"id": "1912.00695", "submitter": "Vitor Hugo Mickus Rodrigues", "authors": "Vitor Hugo Mickus Rodrigues, Lucas Cavalcante, Maelso Bruno Pereira,\n  Fabio Luporini, Istv\\'an Reguly, Gerard Gorman, Samuel Xavier de Souza", "title": "GPU Support for Automatic Generation of Finite-Differences Stencil\n  Kernels", "comments": "This work was accepted and presented to Latin America High\n  Performance Computing (CARLA 2019)", "journal-ref": null, "doi": "10.1007/978-3-030-41005-6_16", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of data to be processed in the Oil & Gas industry matches the\nrequirements imposed by evolving algorithms based on stencil computations, such\nas Full Waveform Inversion and Reverse Time Migration. Graphical processing\nunits (GPUs) are an attractive architectural target for stencil computations\nbecause of its high degree of data parallelism. However, the rapid\narchitectural and technological progression makes it difficult for even the\nmost proficient programmers to remain up-to-date with the technological\nadvances at a micro-architectural level. In this work, we present an extension\nfor an open source compiler designed to produce highly optimized finite\ndifference kernels for use in inversion methods named Devito. We embed it with\nthe Oxford Parallel Domain Specific Language (OP-DSL) in order to enable\nautomatic code generation for GPU architectures from a high-level\nrepresentation. We aim to enable users coding in a symbolic representation\nlevel to effortlessly get their implementations leveraged by the processing\ncapacities of GPU architectures. The implemented backend is evaluated on a\nNVIDIA GTX Titan Z, and on a NVIDIA Tesla V100 in terms of operational\nintensity through the roof-line model for varying space-order discretization\nlevels of 3D acoustic isotropic wave propagation stencil kernels with and\nwithout symbolic optimizations. It achieves approximately 63% of V100's peak\nperformance and 24% of Titan Z's peak performance for stencil kernels over\ngrids with 256 points. Our study reveals that improving memory usage should be\nthe most efficient strategy for leveraging the performance of the implemented\nsolution on the evaluated architectures.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:45:50 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Rodrigues", "Vitor Hugo Mickus", ""], ["Cavalcante", "Lucas", ""], ["Pereira", "Maelso Bruno", ""], ["Luporini", "Fabio", ""], ["Reguly", "Istv\u00e1n", ""], ["Gorman", "Gerard", ""], ["de Souza", "Samuel Xavier", ""]]}, {"id": "1912.00702", "submitter": "Robert Speck", "authors": "Ruth Sch\\\"obel, Robert Speck", "title": "PFASST-ER: Combining the Parallel Full Approximation Scheme in Space and\n  Time with parallelization across the method", "comments": "12 pages, 12 figures, CVS PinT Workshop Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To extend prevailing scaling limits when solving time-dependent partial\ndifferential equations, the parallel full approximation scheme in space and\ntime (PFASST) has been shown to be a promising parallel-in-time integrator.\nSimilar to a space-time multigrid, PFASST is able to compute multiple\ntime-steps simultaneously and is therefore in particular suitable for\nlarge-scale applications on high performance computing systems. In this work we\ncouple PFASST with a parallel spectral deferred correction (SDC) method,\nforming an unprecedented doubly time-parallel integrator. While PFASST provides\nglobal, large-scale \"parallelization across the step\", the inner parallel SDC\nmethod allows to integrate each individual time-step \"parallel across the\nmethod\" using a diagonalized local Quasi-Newton solver. This new method, which\nwe call \"PFASST with Enhanced concuRrency\" (PFASST-ER), therefore exposes even\nmore temporal parallelism. For two challenging nonlinear reaction-diffusion\nproblems, we show that PFASST-ER works more efficiently than the classical\nvariants of PFASST and can be used to run parallel-in-time beyond the number of\ntime-steps.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 12:00:03 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sch\u00f6bel", "Ruth", ""], ["Speck", "Robert", ""]]}, {"id": "1912.00816", "submitter": "Qinmeng Zou", "authors": "Qinmeng Zou and Frederic Magoules", "title": "Recent Developments in Iterative Methods for Reducing Synchronization", "comments": null, "journal-ref": "18th International Symposium on Distributed Computing and\n  Applications for Business Engineering and Science (DCABES), 2019, IEEE", "doi": "10.1109/DCABES48411.2019.00048", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On modern parallel architectures, the cost of synchronization among\nprocessors can often dominate the cost of floating-point computation. Several\nmodifications of the existing methods have been proposed in order to keep the\ncommunication cost as low as possible. This paper aims at providing a brief\noverview of recent advances in parallel iterative methods for solving\nlarge-scale problems. We refer the reader to the related references for more\ndetails on the derivation, implementation, performance, and analysis of these\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:22:42 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zou", "Qinmeng", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.00818", "submitter": "Manoj Ghuhan Arivazhagan", "authors": "Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, Sunav\n  Choudhary", "title": "Federated Learning with Personalization Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The emerging paradigm of federated learning strives to enable collaborative\ntraining of machine learning models on the network edge without centrally\naggregating raw data and hence, improving data privacy. This sharply deviates\nfrom traditional machine learning and necessitates the design of algorithms\nrobust to various sources of heterogeneity. Specifically, statistical\nheterogeneity of data across user devices can severely degrade the performance\nof standard federated averaging for traditional machine learning applications\nlike personalization with deep learning. This paper pro-posesFedPer, a base +\npersonalization layer approach for federated training of deep feedforward\nneural networks, which can combat the ill-effects of statistical heterogeneity.\nWe demonstrate effectiveness ofFedPerfor non-identical data partitions\nofCIFARdatasetsand on a personalized image aesthetics dataset from Flickr.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:29:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Arivazhagan", "Manoj Ghuhan", ""], ["Aggarwal", "Vinay", ""], ["Singh", "Aaditya Kumar", ""], ["Choudhary", "Sunav", ""]]}, {"id": "1912.00842", "submitter": "Veronica Quintuna Rodriguez", "authors": "Veronica Quintuna Rodriguez and Fabrice Guillemin", "title": "Contribution to the design and the implementation of a Cloud Radio\n  Access Network", "comments": "Cloud-RAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation paper presents the main contributions to the design and the\nimplementation of a Cloud-RAN solution. We concretely address the two main\nchallenges of Cloud-RAN systems: real-time processing of radio signals and\nreduced fronthaul capacity. We propose a multi-threading model to achieve\nlatency reduction of critical RAN functions as well as an adapted functional\nsplit for optimizing the transmission of radio signals. We model the\nperformance of the proposed solution by means of stochastic service systems\nwhich reflect the behavior of high performance computing architectures based on\nparallel processing and yield dimensioning rules for the required computing\ncapacity. Finally, we validate the accuracy of the theoretical proposals by a\nCloud-RAN testbed implemented on the basis of open source solutions, namely\nOpen Air Interface (OAI).\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:01:43 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Rodriguez", "Veronica Quintuna", ""], ["Guillemin", "Fabrice", ""]]}, {"id": "1912.00937", "submitter": "Ingo M\\\"uller", "authors": "Ingo M\\\"uller and Renato Marroqu\\'in and Gustavo Alonso", "title": "Lambada: Interactive Data Analytics on Cold Data using Serverless Cloud\n  Infrastructure", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3389758", "report-no": "https://doi.org/10.3929/ethz-b-000413183", "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The promise of ultimate elasticity and operational simplicity of serverless\ncomputing has recently lead to an explosion of research in this area. In the\ncontext of data analytics, the concept sounds appealing, but due to the\nlimitations of current offerings, there is no consensus yet on whether or not\nthis approach is technically and economically viable. In this paper, we\nidentify interactive data analytics on cold data as a use case where serverless\ncomputing excels. We design and implement Lambada, a system following a purely\nserverless architecture, in order to illustrate when and how serverless\ncomputing should be employed for data analytics. We propose several system\ncomponents that overcome the previously known limitations inherent in the\nserverless paradigm as well as additional ones we identify in this work. We can\nshow that, thanks to careful design, a serverless query processing system can\nbe at the same time one order of magnitude faster and two orders of magnitude\ncheaper compared to commercial Query-as-a-Service systems, the only alternative\nwith similar operational simplicity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:07:43 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["M\u00fcller", "Ingo", ""], ["Marroqu\u00edn", "Renato", ""], ["Alonso", "Gustavo", ""]]}, {"id": "1912.00966", "submitter": "Chirayu Anant Haryan", "authors": "Chirayu Anant Haryan, G. Ramakrishna, Rupesh Nasre and Allam Dinesh\n  Reddy", "title": "GPU Algorithm for Earliest Arrival Time Problem in Public Transport\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a temporal graph G, a source vertex s, and a departure time at source\nvertex t_s, the earliest arrival time problem EAT is to start from s on or\nafter t_s and reach all the vertices in G as early as possible. Ni et al. have\nproposed a parallel algorithm for EAT and obtained a speedup up to 9.5 times on\nreal-world graphs with respect to the connection-scan serial algorithm by using\nmulti-core processors.\n  We propose a topology-driven parallel algorithm for EAT on public transport\nnetworks and implement using general-purpose programming on the graphics\nprocessing unit GPU. A temporal edge or connection in a temporal graph for a\npublic transport network is associated with a departure time and a duration\ntime, and many connections exist from u to v for an edge (u,v). We propose two\npruning techniques connection-type and clustering, and use arithmetic\nprogression technique appropriately to process many connections of an edge,\nwithout scanning all of them. In the connection-type technique, the connections\nof an edge with the same duration are grouped together. In the clustering\ntechnique, we follow 24-hour format and the connections of an edge are\npartitioned into 24 clusters so that the departure time of connections in the\ni^{th} cluster is at least i-hour and at most i+1-hour. The arithmetic\nprogression technique helps to store a sequence of departure times of various\nconnections in a compact way. We propose a hybrid approach to combine the three\ntechniques connection-type, clustering and arithmetic progression in an\nappropriate way. Our techniques achieve an average speedup up to 59.09 times\nwhen compared to the existing connection-scan serial algorithm running on CPU.\nAlso, the average speedup of our algorithm is 12.48 times against the parallel\nedge-scan-dependency graph algorithm running on GPU.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:55:02 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Haryan", "Chirayu Anant", ""], ["Ramakrishna", "G.", ""], ["Nasre", "Rupesh", ""], ["Reddy", "Allam Dinesh", ""]]}, {"id": "1912.01146", "submitter": "Basilis Mamalis", "authors": "Basilis Mamalis", "title": "Prolonging Network Lifetime in Wireless Sensor Networks with\n  Path-Constrained Mobile Sink", "comments": "10 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Vol. 5, No. 10, 2014", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies in recent years have considered the use of mobile sinks (MS) for\ndata gathering in wireless sensor networks (WSN), so as to reduce the need for\ndata forwarding among the sensor nodes (SN) and thereby prolong the network\nlifetime. Moreover, in practice, often the MS tour length has to be kept below\na threshold, usually due to timeliness constraints on the sensors data\n(delay-critical applications). This paper presents a modified clustering and\ndata forwarding protocol combined with a MS solution for efficient data\ngathering in wireless sensor networks (WSNs) with delay constraints. The\nadopted cluster formation method is based in the 'residual energy' of the SNs\nand it is appropriately modified in order to fit properly to the requirement of\nlength-constrained MS tour, which involves, among else, the need for\ninter-cluster communication and increased data forwarding. In addition, a\nsuitable data gathering protocol is designed, based on an approximated TSP\nroute that satisfies the given length constraint, whereas the proper\napplication of reclustering phases guarantees the effective handling of the\n'energy holes' caused around the CHs involved in the MS route. Extended\nsimulation experiments show the stable and energy-efficient behavior of the\nproposed scheme (thus leading to increased network lifetime) as well as its\nhigher performance in comparison to other competent approaches from the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 01:30:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Mamalis", "Basilis", ""]]}, {"id": "1912.01365", "submitter": "Andr\\'e Gaul", "authors": "Andr\\'e Gaul, Ismail Khoffi, J\\\"org Liesen, Torsten St\\\"uber", "title": "Mathematical Analysis and Algorithms for Federated Byzantine Agreement\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give an introduction to federated Byzantine agreement systems (FBAS) with\nmany examples ranging from small \"academic\" cases to the current Stellar\nnetwork. We then analyze the main concepts from a mathematical and an\nalgorithmic point of view. Based on work of Lachowski we derive algorithms for\nquorum enumeration, checking quorum intersection, and computing the intact\nnodes with respect to a given set of ill-behaved (Byzantine) nodes. We also\nshow that from the viewpoint of the intactness probability of nodes, which we\nintroduce in this paper, a hierarchical setup of nodes is inferior to an\narrangement that we call a symmetric simple FBAS. All algorithms described in\nthis paper are implemented in the Python package Stellar Observatory, which is\nalso used in some of the computed examples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:53:29 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Gaul", "Andr\u00e9", ""], ["Khoffi", "Ismail", ""], ["Liesen", "J\u00f6rg", ""], ["St\u00fcber", "Torsten", ""]]}, {"id": "1912.01367", "submitter": "Christian Menard", "authors": "Christian Menard, Andres Goens, Marten Lohstroh and Jeronimo\n  Castrillon", "title": "Achieving Determinism in Adaptive AUTOSAR", "comments": "Preprint, final version to appear in DATE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AUTOSAR Adaptive Platform is an emerging industry standard that tackles the\nchallenges of modern automotive software design, but does not provide adequate\nmechanisms to enforce deterministic execution. This poses profound challenges\nto testing and maintenance of the application software, which is particularly\nproblematic for safety-critical applications. In this paper, we analyze the\nproblem of nondeterminism in AP and propose a framework for the design of\ndeterministic automotive software that transparently integrates with the AP\ncommunication mechanisms. We illustrate our approach in a case study based on\nthe brake assistant demonstrator application that is provided by the AUTOSAR\nconsortium. We show that the original implementation is nondeterministic and\ndiscuss a deterministic solution based on our framework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:56:51 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:18:35 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Menard", "Christian", ""], ["Goens", "Andres", ""], ["Lohstroh", "Marten", ""], ["Castrillon", "Jeronimo", ""]]}, {"id": "1912.01478", "submitter": "Rohit M P", "authors": "Shanthanu S Rai, Rohit M P, Sreepathi Pai", "title": "A Hybrid Graph Coloring Algorithm for GPUs", "comments": "5 pages, 4 figures, 4 tables, 2 listings, accepted at 26th IEEE\n  International Conference on High Performance Computing, Data, and Analytics\n  (Student Research Symposium)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms mainly belong to two categories, topology-driven and\ndata-driven. Data-driven approach maintains a worklist of active nodes, the\nnodes on which work has to be done. Topology-driven approach sweeps over the\nentire graph to find active nodes.\n  Hybridization is an optimization technique where in each iteration, the\ncomputation is done in a topology-driven or data-driven manner based on\nworklist size. In hybrid implementations, there is a need to switch between\ntopology-driven and data-driven approaches. Typically, a worklist is maintained\njust in the data-driven part of the algorithm and discarded in the\ntopology-driven part. We propose a variant of hybridization, wherein a worklist\nis maintained throughout all iterations of the algorithm and still show it to\nbe faster than both, topology-driven and data-driven approaches.\n  We consider a graph coloring algorithm called IPGC (Iterative Parallel Graph\nColoring) and implement a hybrid version for the same in a graph domain\nspecific language called IrGL. We observe a mean speedup of 2.13x over a\ndata-driven implementation of IPGC on a suite of 10 large graphs on a NVIDIA\nGPU.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 15:43:40 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Rai", "Shanthanu S", ""], ["P", "Rohit M", ""], ["Pai", "Sreepathi", ""]]}, {"id": "1912.01491", "submitter": "Iosif Meyerov", "authors": "Iosif Meyerov, Evgeny Kozinov, Alexey Liniov, Valentin Volokitin, Igor\n  Yusipov, Mikhail Ivanchenko and Sergey Denisov", "title": "Transforming the Lindblad Equation into a System of Linear Equations:\n  Performance Optimization and Parallelization of an Algorithm", "comments": null, "journal-ref": null, "doi": "10.3390/e22101133", "report-no": null, "categories": "physics.comp-ph cs.DC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With their constantly increasing peak performance and memory capacity, modern\nsupercomputers offer new perspectives on numerical studies of open many-body\nquantum systems. These systems are often modeled by using Markovian quantum\nmaster equations describing the evolution of the system density operators. In\nthis paper we address master equations of the Lindblad form, which are a\npopular theoretical tool in quantum optics, cavity quantum electrodynamics, and\noptomechanics. By using the generalized Gell-Mann matrices as a basis, any\nLindblad equation can be transformed into a system of ordinary differential\nequations with real coefficients. This allows us to use standard\nhigh-performance parallel algorithms to integrate the equations and thus to\nemulate open quantum dynamics in a computationally efficient way. Recently we\npresented an implementation of the transform with the computational complexity\nscaling as $O(N^5 log N)$ for dense Lindbaldians and $O(N^3 log N)$ for sparse\nones. However, infeasible memory costs remain a serious obstacle on the way to\nlarge models. Here we present a parallel cluster-based implementation of the\nalgorithm and demonstrate that it allows us to integrate a sparse Lindbladian\nmodel of the dimension $N=2000$ and a dense random Lindbladian model of the\ndimension $N=200$ by using $25$ nodes with $64$ GB RAM per node.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:06:52 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 14:38:54 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 18:11:12 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Meyerov", "Iosif", ""], ["Kozinov", "Evgeny", ""], ["Liniov", "Alexey", ""], ["Volokitin", "Valentin", ""], ["Yusipov", "Igor", ""], ["Ivanchenko", "Mikhail", ""], ["Denisov", "Sergey", ""]]}, {"id": "1912.01555", "submitter": "Saba Ahmadian", "authors": "Saba Ahmadian, Farhad Taheri, Hossein Asadi", "title": "Evaluating Reliability of SSD-Based I/O Caches in Enterprise Storage\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a comprehensive analysis investigating the\nreliability of SSD-based I/O caching architectures used in enterprise storage\nsystems under power failure and high-operating temperature. We explore variety\nof SSDs from top vendors and investigate the cache reliability in mirrored\nconfiguration. To this end, we first develop a physical fault injection and\nfailure detection platform and then investigate the impact of workload\ndependent parameters on the reliability of I/O cache in the presence of two\ncommon failure types in data centers, power outage and high temperature faults.\nWe implement an I/O cache scheme using an open-source I/O cache module in Linux\noperating system. The experimental results obtained by conducting more than\ntwenty thousand of physical fault injections on the implemented I/O cache with\ndifferent write policies reveal that the failure rate of the I/O cache is\nsignificantly affected by workload dependent parameters. Our results show that\nunlike workload requests access pattern, the other workload dependent\nparameters such as request size, Working Set Size (WSS), and sequence of the\naccesses have considerable impact on the I/O cache failure rate. We observe a\nsignificant growth in the failure rate in the workloads by decreasing the size\nof the requests (by more than 14X). Furthermore, we observe that in addition to\nwrites, the read accesses to the I/O cache are subjected to failure in presence\nof sudden power outage (the failure mainly occurs during promoting data to the\ncache). In addition, we observe that I/O cache experiences no data failure upon\nhigh temperature faults.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 11:00:35 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ahmadian", "Saba", ""], ["Taheri", "Farhad", ""], ["Asadi", "Hossein", ""]]}, {"id": "1912.01556", "submitter": "Behzad Salami", "authors": "Oyku Melikoglu, Oguz Ergin, Behzad Salami, Julian Pavon, Osman Unsal,\n  Adrian Cristal", "title": "A Novel FPGA-Based High Throughput Accelerator For Binary Search Trees", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deeply pipelined and massively parallel Binary Search\nTree (BST) accelerator for Field Programmable Gate Arrays (FPGAs). Our design\nrelies on the extremely parallel on-chip memory, or Block RAMs (BRAMs)\narchitecture of FPGAs. To achieve significant throughput for the search\noperation on BST, we present several novel mechanisms including tree\nduplication as well as horizontal, duplicated, and hybrid (horizontal-vertical)\ntree partitioning. Also, we present efficient techniques to decrease the\nstalling rates that can occur during the parallel tree search. By combining\nthese techniques and implementations on Xilinx Virtex-7 VC709 platform, we\nachieve up to 8X throughput improvement gain in comparison to the baseline\nimplementation, i.e., a fully-pipelined FPGA-based accelerator.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 14:03:36 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Melikoglu", "Oyku", ""], ["Ergin", "Oguz", ""], ["Salami", "Behzad", ""], ["Pavon", "Julian", ""], ["Unsal", "Osman", ""], ["Cristal", "Adrian", ""]]}, {"id": "1912.01560", "submitter": "Michel Kinsy", "authors": "Novak Boskov, Mihailo Isakov, Michel A. Kinsy", "title": "Drndalo: Lightweight Control Flow Obfuscation Through Minimal\n  Processor/Compiler Co-Design", "comments": null, "journal-ref": null, "doi": null, "report-no": "BU - ECE - ASCS Laboratory 2019 Report v5", "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary analysis is traditionally used in the realm of malware detection.\nHowever, the same technique may be employed by an attacker to analyze the\noriginal binaries in order to reverse engineer them and extract exploitable\nweaknesses. When a binary is distributed to end users, it becomes a common\nremotely exploitable attack point. Code obfuscation is used to hinder reverse\nengineering of executable programs. In this paper, we focus on securing binary\ndistribution, where attackers gain access to binaries distributed to end\ndevices, in order to reverse engineer them and find potential vulnerabilities.\nAttackers do not however have means to monitor the execution of said devices.\nIn particular, we focus on the control flow obfuscation --- a technique that\nprevents an attacker from restoring the correct reachability conditions for the\nbasic blocks of a program. By doing so, we thwart attackers in their effort to\ninfer the inputs that cause the program to enter a vulnerable state (e.g.,\nbuffer overrun). We propose a compiler extension for obfuscation and a minimal\nhardware modification for dynamic deobfuscation that takes advantage of a\nsecret key stored in hardware. We evaluate our experiments on the LLVM compiler\ntoolchain and the BRISC-V open source processor. On PARSEC benchmarks, our\ndeobfuscation technique incurs only a 5\\% runtime overhead. We evaluate the\nsecurity of Drndalo by training classifiers on pairs of obfuscated and\nunobfuscated binaries. Our results shine light on the difficulty of producing\nobfuscated binaries of arbitrary programs in such a way that they are\nstatistically indistinguishable from plain binaries.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 05:22:47 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Boskov", "Novak", ""], ["Isakov", "Mihailo", ""], ["Kinsy", "Michel A.", ""]]}, {"id": "1912.01561", "submitter": "Maksim Jenihhin", "authors": "Maksim Jenihhin, Said Hamdioui, Matteo Sonza Reorda, Milos Krstic,\n  Peter Langendoerfer, Christian Sauer, Anton Klotz, Michael Huebner, Joerg\n  Nolte, Heinrich Theodor Vierhaus, Georgios Selimis, Dan Alexandrescu,\n  Mottaqiallah Taouil, Geert-Jan Schrijen, Jaan Raik, Luca Sterpone, Giovanni\n  Squillero and Zoya Dyka", "title": "RESCUE: Interdependent Challenges of Reliability, Security and Quality\n  in Nanoelectronic Systems", "comments": "2020 Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), Grenoble, France, 09 - 13 March 2020 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent trends for nanoelectronic computing systems include\nmachine-to-machine communication in the era of Internet-of-Things (IoT) and\nautonomous systems, complex safety-critical applications, extreme\nminiaturization of implementation technologies and intensive interaction with\nthe physical world. These set tough requirements on mutually dependent\nextra-functional design aspects. The H2020 MSCA ITN project RESCUE is focused\non key challenges for reliability, security and quality, as well as related\nelectronic design automation tools and methodologies. The objectives include\nboth research advancements and cross-sectoral training of a new generation of\ninterdisciplinary researchers. Notable interdisciplinary collaborative research\nresults for the first half-period include novel approaches for test generation,\nsoft-error and transient faults vulnerability analysis, cross-layer\nfault-tolerance and error-resilience, functional safety validation, reliability\nassessment and run-time management, HW security enhancement and initial\nimplementation of these into holistic EDA tools.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 09:04:27 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Jenihhin", "Maksim", ""], ["Hamdioui", "Said", ""], ["Reorda", "Matteo Sonza", ""], ["Krstic", "Milos", ""], ["Langendoerfer", "Peter", ""], ["Sauer", "Christian", ""], ["Klotz", "Anton", ""], ["Huebner", "Michael", ""], ["Nolte", "Joerg", ""], ["Vierhaus", "Heinrich Theodor", ""], ["Selimis", "Georgios", ""], ["Alexandrescu", "Dan", ""], ["Taouil", "Mottaqiallah", ""], ["Schrijen", "Geert-Jan", ""], ["Raik", "Jaan", ""], ["Sterpone", "Luca", ""], ["Squillero", "Giovanni", ""], ["Dyka", "Zoya", ""]]}, {"id": "1912.01562", "submitter": "Piotr Dziurzanski", "authors": "Shuai Zhao, Piotr Dziurzanski, Leandro Soares Indrusiak", "title": "Value-driven Manufacturing Planning using Cloud-based Evolutionary\n  Optimisation", "comments": "This paper is an extended version of a paper published at\n  International Conference on Manufacturing Science and Technology (ICMST) in\n  2019. In this version, we have updated the evaluation section to provide\n  experimental setup and results that are more realistic and suitable to the\n  proposed approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers manufacturing planning and scheduling of manufacturing\norders whose value decreases over time. The value decrease is modelled with a\nso-called value curve. Two genetic-algorithm-based methods for multi-objective\noptimisation have been proposed, implemented and deployed to a cloud. The first\nproposed method allocates and schedules manufacturing of all the ordered\nelements optimising both the makespan and the total value, whereas the second\nmethod selects only the profitable orders for manufacturing. The proposed\nevolutionary optimisation has been performed for a set of real-world-inspired\nmanufacturing orders. Both the methods yield a similar total value, but the\nlatter method leads to a shorter makespan.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:18:26 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 15:17:52 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhao", "Shuai", ""], ["Dziurzanski", "Piotr", ""], ["Indrusiak", "Leandro Soares", ""]]}, {"id": "1912.01563", "submitter": "Behzad Salami", "authors": "B. Salami, K. Parasyris, A. Cristal, O. Unsal, X. Martorell, P.\n  Carpenter, R. De La Cruz, L. Bautista, D. Jimenez, C. Alvarez, S. Nabavi, S.\n  Madonar, M. Pericas, P. Trancoso, M. Abduljabbar, J. Chen, P. N. Soomro, M\n  Manivannan, M. Berge, S. Krupop, F. Klawonn, Al Mekhlafi, S. May, T. Becker,\n  G. Gaydadjiev, D. Odman, H. Salomonsson, D. Dubhashi, O. Port, Y. Etsion, Le\n  Quoc Do, Christof Fetzer, M. Kaiser, N. Kucza, J. Hagemeyer, R. Griessl, L.\n  Tigges, K. Mika, A. Huffmeier, Th. Jungeblu, M. Pasin, V. Schiavoni, I.\n  Rocha, C. Gottel and P. Felber", "title": "LEGaTO: Low-Energy, Secure, and Resilient Toolset for Heterogeneous\n  Computing", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LEGaTO project leverages task-based programming models to provide a\nsoftware ecosystem for Made in-Europe heterogeneous hardware composed of CPUs,\nGPUs, FPGAs and dataflow engines. The aim is to attain one order of magnitude\nenergy savings from the edge to the converged cloud/HPC, balanced with the\nsecurity and resilience challenges. LEGaTO is an ongoing three-year EU H2020\nproject started in December 2017.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 09:24:06 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Salami", "B.", ""], ["Parasyris", "K.", ""], ["Cristal", "A.", ""], ["Unsal", "O.", ""], ["Martorell", "X.", ""], ["Carpenter", "P.", ""], ["De La Cruz", "R.", ""], ["Bautista", "L.", ""], ["Jimenez", "D.", ""], ["Alvarez", "C.", ""], ["Nabavi", "S.", ""], ["Madonar", "S.", ""], ["Pericas", "M.", ""], ["Trancoso", "P.", ""], ["Abduljabbar", "M.", ""], ["Chen", "J.", ""], ["Soomro", "P. N.", ""], ["Manivannan", "M", ""], ["Berge", "M.", ""], ["Krupop", "S.", ""], ["Klawonn", "F.", ""], ["Mekhlafi", "Al", ""], ["May", "S.", ""], ["Becker", "T.", ""], ["Gaydadjiev", "G.", ""], ["Odman", "D.", ""], ["Salomonsson", "H.", ""], ["Dubhashi", "D.", ""], ["Port", "O.", ""], ["Etsion", "Y.", ""], ["Do", "Le Quoc", ""], ["Fetzer", "Christof", ""], ["Kaiser", "M.", ""], ["Kucza", "N.", ""], ["Hagemeyer", "J.", ""], ["Griessl", "R.", ""], ["Tigges", "L.", ""], ["Mika", "K.", ""], ["Huffmeier", "A.", ""], ["Jungeblu", "Th.", ""], ["Pasin", "M.", ""], ["Schiavoni", "V.", ""], ["Rocha", "I.", ""], ["Gottel", "C.", ""], ["Felber", "P.", ""]]}, {"id": "1912.01684", "submitter": "Zirui Xu", "authors": "Zirui Xu, Fuxun Yu, Jinjun Xiong, Xiang Chen", "title": "Helios: Heterogeneity-Aware Federated Learning with Dynamically Balanced\n  Collaboration", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Helios, a heterogeneity-aware FL framework to\ntackle the straggler issue. Helios identifies individual devices' heterogeneous\ntraining capability, and therefore the expected neural network model training\nvolumes regarding the collaborative training pace. For straggling devices, a\n\"soft-training\" method is proposed to dynamically compress the original\nidentical training model into the expected volume through a rotating neuron\ntraining approach. With extensive algorithm analysis and optimization schemes,\nthe stragglers can be accelerated while retaining the convergence for local\ntraining as well as federated collaboration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 21:08:53 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 17:23:20 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Zirui", ""], ["Yu", "Fuxun", ""], ["Xiong", "Jinjun", ""], ["Chen", "Xiang", ""]]}, {"id": "1912.01697", "submitter": "Abdul Wahab Mr", "authors": "Abdul Wahab and Phil Maguire", "title": "Smart Parking: IoT and Blockchain", "comments": "Submitted at National University of Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed ledger technology and IoT has revolutionized the world by finding\nits application in all the domains. It promises to transform the digital\ninfrastructure which powers extensive evolutions and impacts a lot of areas.\nVehicle parking is a major problem in major cities around the world in both\ndeveloped and developing countries. The common problems are unavailability or\nshortage of parking spaces, no information about tariff and no mean of\nsearching availability of parking space online. The struggle doesn't end even\nif an individual finds a spot, he is required to pay in cash. This traditional\nand manual process takes a lot of time and causes a lot of hassle. In this\npaper, we provide a novel solution to the parking problem using IoT and\ndistributed ledger technology. This system is based on pervasive computing and\nprovides auto check-in and check-out. The user can control the system and their\nprofile using the app on their smartphone. The major advantage of the system is\nan easy and online payment method. Users can pay for their parking tickets\nusing their credit cards from their smartphone app. This decreases their hassle\nof carrying cash and coins for purchasing parking tickets. Smart Parking will\noptimize the parking mechanism, save time, reduce traffic and pollution, and\nprovide an enhanced user experience. It is robust, secure, scalable and\nautomated using the combination of cutting-edge technologies.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 21:49:25 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Wahab", "Abdul", ""], ["Maguire", "Phil", ""]]}, {"id": "1912.02050", "submitter": "Ali Mohammed", "authors": "Ali Mohammed and Florina M. Ciorba", "title": "SimAS: A Simulation-assisted Approach for the Scheduling Algorithm\n  Selection under Perturbations", "comments": "arXiv admin note: text overlap with arXiv:1807.03577", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific applications consist of large and computationally-intensive\nloops. Dynamic loop self-scheduling (DLS) techniques are used to parallelize\nand to balance the load during the execution of such applications. Load\nimbalance arises from variations in the loop iteration (or tasks) execution\ntimes, caused by problem, algorithmic, or systemic characteristics. The\nvariations in systemic characteristics are referred to as perturbations, and\ncan be caused by other applications or processes that share the same resources,\nor a temporary system fault or malfunction. Therefore, the selection of the\nmost efficient DLS technique is critical to achieve the best application\nperformance. The following question motivates this work: Given an application,\nan HPC system, and their characteristics and interplay, which DLS technique\nwill achieve improved performance under unpredictable perturbations? Existing\nstudies focus on variations in the delivered computational speed only as the\nsource of perturbations in the system. However, perturbations in available\nnetwork bandwidth or latency are inevitable on production HPC systems. A\nSimulator-assisted scheduling (SimAS) is introduced as a new\ncontrol-theoretic-inspired approach to dynamically select DLS techniques that\nimprove the performance of applications executing on heterogeneous HPC systems\nunder perturbations. The present work examines the performance of seven\napplications on a heterogeneous system under all the above system\nperturbations. SimAS is evaluated as a proof of concept using native and\nsimulative experiments. The performance results confirm the original hypothesis\nthat no single DLS technique can deliver the absolute best performance in all\nscenarios, whereas the SimAS-based DLS selection resulted in improved\napplication performance in most experiments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:17:58 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Mohammed", "Ali", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1912.02165", "submitter": "Rati Gelashvili", "authors": "Rati Gelashvili, Nir Shavit, Aleksandar Zlateski", "title": "L3 Fusion: Fast Transformed Convolutions on CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast convolutions via transforms, either Winograd or FFT, had emerged as a\npreferred way of performing the computation of convolutional layers, as it\ngreatly reduces the number of required operations. Recent work shows that, for\nmany layer structures, a well--designed implementation of fast convolutions can\ngreatly utilize modern CPUs, significantly reducing the compute time. However,\nthe generous amount of shared L3 cache present on modern CPUs is often\nneglected, and the algorithms are optimized solely for the private L2 cache. In\nthis paper we propose an efficient `L3 Fusion` algorithm that is specifically\ndesigned for CPUs with significant amount of shared L3 cache. Using the\nhierarchical roofline model, we show that in many cases, especially for layers\nwith fewer channels, the `L3 fused` approach can greatly outperform standard 3\nstage one provided by big vendors such as Intel. We validate our theoretical\nfindings, by benchmarking our `L3 fused` implementation against publicly\navailable state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:34:58 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Gelashvili", "Rati", ""], ["Shavit", "Nir", ""], ["Zlateski", "Aleksandar", ""]]}, {"id": "1912.02322", "submitter": "Shijian Li", "authors": "Matthew LeMay and Shijian Li and Tian Guo", "title": "Perseus: Characterizing Performance and Cost of Multi-Tenant Serving for\n  CNN Models", "comments": "8 pages, 5 figures, and 6 tables. In proceedings of International\n  Conference on Cloud Engineering (IC2E) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are increasingly used for end-user applications,\nsupporting both novel features such as facial recognition, and traditional\nfeatures, e.g. web search. To accommodate high inference throughput, it is\ncommon to host a single pre-trained Convolutional Neural Network (CNN) in\ndedicated cloud-based servers with hardware accelerators such as Graphics\nProcessing Units (GPUs). However, GPUs can be orders of magnitude more\nexpensive than traditional Central Processing Unit (CPU) servers. These\nresources could also be under-utilized facing dynamic workloads, which may\nresult in inflated serving costs. One potential way to alleviate this problem\nis by allowing hosted models to share the underlying resources, which we refer\nto as multi-tenant inference serving. One of the key challenges is maximizing\nthe resource efficiency for multi-tenant serving given hardware with diverse\ncharacteristics, models with unique response time Service Level Agreement\n(SLA), and dynamic inference workloads. In this paper, we present Perseus, a\nmeasurement framework that provides the basis for understanding the performance\nand cost trade-offs of multi-tenant model serving. We implemented Perseus in\nPython atop a popular cloud inference server called Nvidia TensorRT Inference\nServer. Leveraging Perseus, we evaluated the inference throughput and cost for\nserving various models and demonstrated that multi-tenant model serving led to\nup to 12% cost reduction.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 00:35:43 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 22:02:51 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["LeMay", "Matthew", ""], ["Li", "Shijian", ""], ["Guo", "Tian", ""]]}, {"id": "1912.02531", "submitter": "Florian Hofer", "authors": "Florian Hofer and Martin A. Sehr and Alberto Sangiovanni-Vincentelli\n  and Barbara Russo", "title": "ODRE Workshop: Probabilistic Dynamic Hard Real-Time Scheduling in HPC", "comments": "6 page, 5 figures ISORC 2020 Workshop (ODRE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 is changing fundamentally the way data is collected, stored and\nanalyzed in industrial processes. While this change enables novel application\nsuch as flexible manufacturing of highly customized products, the real-time\ncontrol of these processes, however, has not yet realized its full potential.\nWe believe that modern virtualization techniques, specifically application\ncontainers, present a unique opportunity to decouple control functionality from\nassociated hardware. Through it, we can fully realize the potential for highly\ndistributed and transferable industrial processes even with real-time\nconstraints arising from time-critical sub-processes. In this paper, we present\na specifically developed orchestration tool to manage the challenges and\nopportunities of shifting industrial control software from dedicated hardware\nto bare-metal servers or (edge) cloud computing platforms. Using off-the-shelf\ntechnology, the proposed tool can manage the execution of containerized\napplications on shared resources without compromising hard real-time execution\ndeterminism. Through first experimental results, we confirm the viability and\nanalyzed the behavior of resource shared systems with strict real-time\nrequirements. We then describe experiments set out to deliver expected results\nand gather performance, application scope and limits of the presented approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 12:24:56 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 04:43:54 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 00:11:31 GMT"}, {"version": "v4", "created": "Sat, 9 May 2020 23:25:14 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hofer", "Florian", ""], ["Sehr", "Martin A.", ""], ["Sangiovanni-Vincentelli", "Alberto", ""], ["Russo", "Barbara", ""]]}, {"id": "1912.02607", "submitter": "H{\\aa}vard Heitlo Holm", "authors": "H{\\aa}vard H. Holm, Andr\\'e R. Brodtkorb and Martin L. S{\\ae}tra", "title": "GPU Computing with Python: Performance, Energy Efficiency and Usability", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": "10.3390/computation8010004", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we examine the performance, energy efficiency and usability\nwhen using Python for developing HPC codes running on the GPU. We investigate\nthe portability of performance and energy efficiency between CUDA and OpenCL;\nbetween GPU generations; and between low-end, mid-range and high-end GPUs. Our\nfindings show that the impact of using Python is negligible for our\napplications, and furthermore, CUDA and OpenCL applications tuned to an\nequivalent level can in many cases obtain the same computational performance.\nOur experiments show that performance in general varies more between different\nGPUs than between using CUDA and OpenCL. We also show that tuning for\nperformance is a good way of tuning for energy efficiency, but that specific\ntuning is needed to obtain optimal energy efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:46:06 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Holm", "H\u00e5vard H.", ""], ["Brodtkorb", "Andr\u00e9 R.", ""], ["S\u00e6tra", "Martin L.", ""]]}, {"id": "1912.02802", "submitter": "Alcardo Alex Barakabitze", "authors": "Alcardo Alex Barakabitze and Arslan Ahmad and Rashid Mijumbi and\n  Andrew Hines", "title": "5G network slicing using SDN and NFV- A survey of taxonomy,\n  architectures and future challenges", "comments": "40 Pages, 22 figures, published in computer networks (Open Access)", "journal-ref": "2019", "doi": "10.1016/j.comnet.2019.106984", "report-no": null, "categories": "cs.NI cs.DC cs.MM cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we provide a comprehensive review and updated solutions\nrelated to 5G network slicing using SDN and NFV. Firstly, we present 5G service\nquality and business requirements followed by a description of 5G network\nsoftwarization and slicing paradigms including essential concepts, history and\ndifferent use cases. Secondly, we provide a tutorial of 5G network slicing\ntechnology enablers including SDN, NFV, MEC, cloud/Fog computing, network\nhypervisors, virtual machines & containers. Thidly, we comprehensively survey\ndifferent industrial initiatives and projects that are pushing forward the\nadoption of SDN and NFV in accelerating 5G network slicing. A comparison of\nvarious 5G architectural approaches in terms of practical implementations,\ntechnology adoptions and deployment strategies is presented. Moreover, we\nprovide a discussion on various open source orchestrators and proof of concepts\nrepresenting industrial contribution. The work also investigates the\nstandardization efforts in 5G networks regarding network slicing and\nsoftwarization. Additionally, the article presents the management and\norchestration of network slices in a single domain followed by a comprehensive\nsurvey of management and orchestration approaches in 5G network slicing across\nmultiple domains while supporting multiple tenants. Furthermore, we highlight\nthe future challenges and research directions regarding network softwarization\nand slicing using SDN and NFV in 5G networks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:50:31 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Barakabitze", "Alcardo Alex", ""], ["Ahmad", "Arslan", ""], ["Mijumbi", "Rashid", ""], ["Hines", "Andrew", ""]]}, {"id": "1912.02814", "submitter": "Yannic Maus", "authors": "Philipp Bamberger, Fabian Kuhn, Yannic Maus", "title": "Efficient Deterministic Distributed Coloring with Small Bandwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the $(degree+1)$-list coloring problem can be solved\ndeterministically in $O(D \\cdot \\log n \\cdot\\log^2\\Delta)$ rounds in the\n\\CONGEST model, where $D$ is the diameter of the graph, $n$ the number of\nnodes, and $\\Delta$ the maximum degree. Using the recent polylogarithmic-time\ndeterministic network decomposition algorithm by Rozho\\v{n} and Ghaffari [STOC\n2020], this implies the first efficient (i.e., $\\poly\\log n$-time)\ndeterministic \\CONGEST algorithm for the $(\\Delta+1)$-coloring and the\n$(\\mathit{degree}+1)$-list coloring problem. Previously the best known\nalgorithm required $2^{O(\\sqrt{\\log n})}$ rounds and was not based on network\ndecompositions.\n  Our techniques also lead to deterministic $(\\mathit{degree}+1)$-list coloring\nalgorithms for the congested clique and the massively parallel computation\n(MPC) model. For the congested clique, we obtain an algorithm with time\ncomplexity $O(\\log\\Delta\\cdot\\log\\log\\Delta)$, for the MPC model, we obtain\nalgorithms with round complexity $O(\\log^2\\Delta)$ for the linear-memory regime\nand $O(\\log^2\\Delta + \\log n)$ for the sublinear memory regime.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:57:01 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 16:09:22 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 07:10:34 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bamberger", "Philipp", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "1912.02892", "submitter": "Jayson Luc Peterson", "authors": "J. Luc Peterson, Ben Bay, Joe Koning, Peter Robinson, Jessica Semler,\n  Jeremy White, Rushil Anirudh, Kevin Athey, Peer-Timo Bremer, Francesco Di\n  Natale, David Fox, Jim A. Gaffney, Sam A. Jacobs, Bhavya Kailkhura, Bogdan\n  Kustowski, Steven Langer, Brian Spears, Jayaraman Thiagarajan, Brian Van\n  Essen, Jae-Seung Yeom", "title": "Enabling Machine Learning-Ready HPC Ensembles with Merlin", "comments": "28 pages, 9 figures; Submitted to FGCS", "journal-ref": null, "doi": null, "report-no": "LLNL-JRNL-821884", "categories": "cs.DC cs.LG physics.comp-ph physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing complexity of computational and experimental facilities,\nmany scientific researchers are turning to machine learning (ML) techniques to\nanalyze large scale ensemble data. With complexities such as multi-component\nworkflows, heterogeneous machine architectures, parallel file systems, and\nbatch scheduling, care must be taken to facilitate this analysis in a high\nperformance computing (HPC) environment. In this paper, we present Merlin, a\nworkflow framework to enable large ML-friendly ensembles of scientific HPC\nsimulations. By augmenting traditional HPC with distributed compute\ntechnologies, Merlin aims to lower the barrier for scientific subject matter\nexperts to incorporate ML into their analysis. In addition to its design, we\ndescribe some example applications that Merlin has enabled on leadership-class\nHPC resources, such as the ML-augmented optimization of nuclear fusion\nexperiments and the calibration of infectious disease models to study the\nprogression of and possible mitigation strategies for COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 21:40:07 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 21:52:14 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Peterson", "J. Luc", ""], ["Bay", "Ben", ""], ["Koning", "Joe", ""], ["Robinson", "Peter", ""], ["Semler", "Jessica", ""], ["White", "Jeremy", ""], ["Anirudh", "Rushil", ""], ["Athey", "Kevin", ""], ["Bremer", "Peer-Timo", ""], ["Di Natale", "Francesco", ""], ["Fox", "David", ""], ["Gaffney", "Jim A.", ""], ["Jacobs", "Sam A.", ""], ["Kailkhura", "Bhavya", ""], ["Kustowski", "Bogdan", ""], ["Langer", "Steven", ""], ["Spears", "Brian", ""], ["Thiagarajan", "Jayaraman", ""], ["Van Essen", "Brian", ""], ["Yeom", "Jae-Seung", ""]]}, {"id": "1912.02924", "submitter": "Allison Irvin", "authors": "Allison Irvin, Isabell Kiral", "title": "Designing for Privacy and Confidentiality on Distributed Ledgers for\n  Enterprise (Industry Track)", "comments": "Middleware 2019", "journal-ref": "In Proceedings of Middleware 2019: 20th ACM/IFIP International\n  Middleware Conference (Middleware 2019). ACM, New York, NY, USA", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed ledger technology offers numerous desirable attributes to\napplications in the enterprise context. However, with distributed data and\ndecentralized computation on a shared platform, privacy and confidentiality\nchallenges arise. Any design for an enterprise system needs to carefully cater\nfor use case specific privacy and confidentiality needs. With the goal to\nfacilitate the design of enterprise solutions, this paper aims to provide a\nguide to navigate and aid in decisions around common requirements and\nmechanisms that prevent the leakage of private and confidential information. To\nfurther contextualize key concepts, the design guide is then applied to three\nenterprise DLT protocols: Hyperledger Fabric, Corda, and Quorum.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:51:58 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Irvin", "Allison", ""], ["Kiral", "Isabell", ""]]}, {"id": "1912.02927", "submitter": "Manoj Penmetcha", "authors": "Manoj Penmetcha, Shyam Sundar Kannan, Byung-Cheol Min", "title": "Smart Cloud: Scalable Cloud Robotic Architecture for Web-powered\n  Multi-Robot Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots have inherently limited onboard processing, storage, and power\ncapabilities. Cloud computing resources have the potential to provide\nsignificant advantages for robots in many applications. However, to make use of\nthese resources, frameworks must be developed that facilitate robot\ninteractions with cloud services. In this paper, we propose a cloud-based\narchitecture called Smart Cloud that intends to overcome the physical\nlimitations of single- or multi-robot systems through massively parallel\ncomputation, provided on demand by cloud services. Smart Cloud is implemented\non Amazon Web Services (AWS) and available for robots running on the Robot\nOperating System (ROS) and on the non-ROS systems. Smart Cloud features a\nfirst-of-its-kind architecture that incorporates JavaScript-based libraries to\nrun various robotic applications related to machine learning and other methods.\nThis paper presents the architecture and its performance in terms of CPU usage\nand latency, and finally validates it for navigation and machine learning\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:12:50 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 14:00:47 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 08:24:19 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Penmetcha", "Manoj", ""], ["Kannan", "Shyam Sundar", ""], ["Min", "Byung-Cheol", ""]]}, {"id": "1912.03107", "submitter": "Jonathan Hasenburg", "authors": "Jonathan Hasenburg and Martin Grambow and David Bermbach", "title": "FBase: A Replication Service for Data-Intensive Fog Applications", "comments": "Technical Report; extended version", "journal-ref": null, "doi": null, "report-no": "MCC.2019.1", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of edge and cloud in the fog computing paradigm enables a new\nbreed of data-intensive applications. These applications, however, have to face\na number of fog-specific challenges which developers have to repetitively\naddress for every single application.\n  In this paper, we propose a replication service specifically tailored to the\nneeds of data-intensive fog applications that aims to ease or eliminate\nchallenges caused by the highly distributed and heterogeneous environment fog\napplications operate in. Furthermore, we present our prototypical\nproof-of-concept implementation FBase that we have made available as open\nsource.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 13:37:46 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:31:16 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Hasenburg", "Jonathan", ""], ["Grambow", "Martin", ""], ["Bermbach", "David", ""]]}, {"id": "1912.03208", "submitter": "Xin Zhang", "authors": "Xin Zhang, Jia Liu, Zhengyuan Zhu, Elizabeth S. Bentley", "title": "Communication-Efficient Network-Distributed Optimization with\n  Differential-Coded Compressors", "comments": "10 pages, 15 figures, IEEE INFOCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network-distributed optimization has attracted significant attention in\nrecent years due to its ever-increasing applications. However, the classic\ndecentralized gradient descent (DGD) algorithm is communication-inefficient for\nlarge-scale and high-dimensional network-distributed optimization problems. To\naddress this challenge, many compressed DGD-based algorithms have been\nproposed. However, most of the existing works have high complexity and assume\ncompressors with bounded noise power. To overcome these limitations, in this\npaper, we propose a new differential-coded compressed DGD (DC-DGD) algorithm.\nThe key features of DC-DGD include: i) DC-DGD works with general\nSNR-constrained compressors, relaxing the bounded noise power assumption; ii)\nThe differential-coded design entails the same convergence rate as the original\nDGD algorithm; and iii) DC-DGD has the same low-complexity structure as the\noriginal DGD due to a {\\em self-noise-reduction effect}. Moreover, the above\nfeatures inspire us to develop a hybrid compression scheme that offers a\nsystematic mechanism to minimize the communication cost. Finally, we conduct\nextensive experiments to verify the efficacy of the proposed DC-DGD and hybrid\ncompressor.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:19:52 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Zhang", "Xin", ""], ["Liu", "Jia", ""], ["Zhu", "Zhengyuan", ""], ["Bentley", "Elizabeth S.", ""]]}, {"id": "1912.03312", "submitter": "H. Rittich", "authors": "Hannah Rittich and Robert Speck", "title": "Time-parallel simulation of the Schr\\\"odinger Equation", "comments": "29 pages, 4 figures, 7 tables", "journal-ref": "Computer Physics Communications, 2020, 255, 107363", "doi": "10.1016/j.cpc.2020.107363", "report-no": null, "categories": "math.NA cs.CE cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical simulation of the time-dependent Schr\\\"odinger equation for\nquantum systems is a very active research topic. Yet, resolving the solution\nsufficiently in space and time is challenging and mandates the use of modern\nhigh-performance computing systems. While classical parallelization techniques\nin space can reduce the runtime per time-step, novel parallel-in-time\nintegrators expose parallelism in the temporal domain. They work, however, not\nvery well for wave-type problems such as the Schr\\\"odinger equation. One\nnotable exception is the rational approximation of exponential integrators. In\nthis paper we derive an efficient variant of this approach suitable for the\ncomplex-valued Schr\\\"odinger equation. Using the Faber-Carath\\'eodory-Fej\\'er\napproximation, this variant is already a fast serial and in particular an\nefficient time-parallel integrator. It can be used to augment classical\nparallelization in space and we show the efficiency and effectiveness of our\nmethod along the lines of two challenging, realistic examples.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:59:39 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 10:42:10 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 14:29:02 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Rittich", "Hannah", ""], ["Speck", "Robert", ""]]}, {"id": "1912.03348", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far and Emina Soljanin", "title": "Scheduling in the Presence of Data Intensive Compute Jobs", "comments": "Accepted in IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of non-adaptive scheduling policies in computing\nsystems with multiple servers. Compute jobs are mostly regular, with modest\nservice requirements. However, there are sporadic data intensive jobs, whose\nexpected service time is much higher than that of the regular jobs. Forthis\nmodel, we are interested in the effect of scheduling policieson the average\ntime a job spends in the system. To this end, we introduce two performance\nindicators in a simplified, only-arrival system. We believe that these\nperformance indicators are good predictors of the relative performance of the\npolicies in the queuing system, which is supported by simulations results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:38:55 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 16:40:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""]]}, {"id": "1912.03349", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far and Emina Soljanin", "title": "Data Replication for Reducing Computing Time in Distributed Systems with\n  Stragglers", "comments": "Accepted in IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed computing systems with stragglers, various forms of redundancy\ncan improve the average delay performance. We study the optimal replication of\ndata in systems where the job execution time is a stochastically decreasing and\nconvex random variable. We show that in such systems, the optimum assignment\npolicy is the balanced replication of disjoint batches of data. Furthermore,\nfor Exponential and Shifted-Exponential service times, we derive the optimum\nredundancy levels for minimizing both expected value and the variance of the\njob completion time. Our analysis shows that, the optimum redundancy level may\nnot be the same for the two metrics, thus there is a trade-off between reducing\nthe expected value of the completion time and reducing its variance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:40:43 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 16:37:44 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""]]}, {"id": "1912.03413", "submitter": "Daniele Scarpazza", "authors": "Zhe Jia and Blake Tillman and Marco Maggioni and Daniele Paolo\n  Scarpazza", "title": "Dissecting the Graphcore IPU Architecture via Microbenchmarking", "comments": "91 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report focuses on the architecture and performance of the Intelligence\nProcessing Unit (IPU), a novel, massively parallel platform recently introduced\nby Graphcore and aimed at Artificial Intelligence/Machine Learning (AI/ML)\nworkloads. We dissect the IPU's performance behavior using microbenchmarks that\nwe crafted for the purpose. We study the IPU's memory organization and\nperformance. We study the latency and bandwidth that the on-chip and off-chip\ninterconnects offer, both in point-to-point transfers and in a spectrum of\ncollective operations, under diverse loads. We evaluate the IPU's compute power\nover matrix multiplication, convolution, and AI/ML primitives. We discuss\nactual performance in comparison with its theoretical limits. Our findings\nreveal how the IPU's architectural design affects its performance. Moreover,\nthey offer simple mental models to predict an application's performance on the\nIPU, on the basis of the computation and communication steps it involves. This\nreport is the natural extension to a novel architecture of a continuing effort\nof ours that focuses on the microbenchmark-based discovery of massively\nparallel architectures.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 02:10:19 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jia", "Zhe", ""], ["Tillman", "Blake", ""], ["Maggioni", "Marco", ""], ["Scarpazza", "Daniele Paolo", ""]]}, {"id": "1912.03506", "submitter": "Srivatsan Ravi Mr", "authors": "Bo Sang and Gustavo Petri and Masoud Saeida Ardekani and Srivatsan\n  Ravi and Patrick Eugster", "title": "Programming Scalable Cloud Services with AEON", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing low-latency cloud-based applications that are adaptable to\nunpredictable workloads and efficiently utilize modern cloud computing\nplatforms is hard. The actor model is a popular paradigm that can be used to\ndevelop distributed applications: actors encapsulate state and communicate with\neach other by sending events. Consistency is guaranteed if each event only\naccesses a single actor, thus eliminating potential data races and deadlocks.\nHowever it is nontrivial to provide consistency for concurrent events spanning\nacross multiple actors. This paper addresses this problem by introducing AEON:\na framework that provides the following properties: (i) Programmability:\nprogrammers only need to reason about sequential semantics when reasoning about\nconcurrency resulting from multi-actor events; (ii) Scalability: AEON runtime\nprotocol guarantees serializable and starvation-free execution of multi-actor\nevents, while maximizing parallel execution; (iii) Elasticity: AEON supports\nfine-grained elasticity enabling the programmer to transparently migrate\nindividual actors without violating the consistency or entailing significant\nperformance overheads. Our empirical results show that it is possible to\ncombine the best of all the above three worlds without compromising on the\napplication performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 13:58:59 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Sang", "Bo", ""], ["Petri", "Gustavo", ""], ["Ardekani", "Masoud Saeida", ""], ["Ravi", "Srivatsan", ""], ["Eugster", "Patrick", ""]]}, {"id": "1912.03507", "submitter": "Asif Ali Khan", "authors": "Asif Ali Khan, Andres Goens, Fazal Hameed and Jeronimo Castrillon", "title": "Generalized Data Placement Strategies for Racetrack Memories", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra-dense non-volatile racetrack memories (RTMs) have been investigated at\nvarious levels in the memory hierarchy for improved performance and reduced\nenergy consumption. However, the innate shift operations in RTMs hinder their\napplicability to replace low-latency on-chip memories. Recent research has\ndemonstrated that intelligent placement of memory objects in RTMs can\nsignificantly reduce the amount of shifts with no hardware overhead, albeit for\nspecific system setups. However, existing placement strategies may lead to\nsub-optimal performance when applied to different architectures. In this paper\nwe look at generalized data placement mechanisms that improve upon existing\nones by taking into account the underlying memory architecture and the timing\nand liveliness information of memory objects. We propose a novel heuristic and\na formulation using genetic algorithms that optimize key performance\nparameters. We show that, on average, our generalized approach improves the\nnumber of shifts, performance and energy consumption by 4.3x, 46% and 55%\nrespectively compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 14:09:07 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Khan", "Asif Ali", ""], ["Goens", "Andres", ""], ["Hameed", "Fazal", ""], ["Castrillon", "Jeronimo", ""]]}, {"id": "1912.03523", "submitter": "Tan N. Le", "authors": "Tan N. Le, Xiao Sun, Mosharaf Chowdhury, Zhenhua Liu", "title": "BoPF: Mitigating the Burstiness-Fairness Tradeoff in Multi-Resource\n  Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously supporting latency- and throughout-sensitive workloads in a\nshared environment is an increasingly more common challenge in big data\nclusters. Despite many advances, existing cluster schedulers force the same\nperformance goal - fairness in most cases - on all jobs. Latency-sensitive jobs\nsuffer, while throughput-sensitive ones thrive. Using prioritization does the\nopposite: it opens up a path for latency-sensitive jobs to dominate. In this\npaper, we tackle the challenges in supporting both short-term performance and\nlong-term fairness simultaneously with high resource utilization by proposing\nBounded Priority Fairness (BoPF). BoPF provides short-term resource guarantees\nto latency-sensitive jobs and maintains long-term fairness for\nthroughput-sensitive jobs. BoPF is the first scheduler that can provide\nlong-term fairness, burst guarantee, and Pareto efficiency in a strategyproof\nmanner for multi-resource scheduling. Deployments and large-scale simulations\nshow that BoPF closely approximates the performance of Strict Priority as well\nas the fairness characteristics of DRF. In deployments, BoPF speeds up\nlatency-sensitive jobs by 5.38 times compared to DRF, while still maintaining\nlong-term fairness. In the meantime, BoPF improves the average completion times\nof throughput-sensitive jobs by up to 3.05 times compared to Strict Priority.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 16:36:42 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Le", "Tan N.", ""], ["Sun", "Xiao", ""], ["Chowdhury", "Mosharaf", ""], ["Liu", "Zhenhua", ""]]}, {"id": "1912.03715", "submitter": "Sharvari Thippanna", "authors": "Sharvari T, Sowmya Nag K", "title": "A study on Modern Messaging Systems- Kafka, RabbitMQ and NATS Streaming", "comments": "5 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed messaging systems form the core of big data streaming, cloud\nnative applications and microservice architecture. With real-time critical\napplications there is a growing need for well-built messaging platform that is\nscalable, fault tolerant and has low latency. There are multiple modern\nmessaging systems that have come up in the recent past, all with their own pros\nand cons. This has become problematic for the industry to decide which\nmessaging system is the most suitable for a specific application. An in-depth\nstudy is required to decide which features of a messaging system meet the needs\nof the application. This survey paper outlines the modern messaging\ntechnologies and delves deep on three popular publisher/subscriber systems-\nApache Kafka, RabbitMQ and NATS Streaming. The paper provides information about\nmessaging systems, the use cases, similarities and differences of features to\nfacilitate users to make an informed decision and also pave way for future\nresearch and development.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 17:12:22 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["T", "Sharvari", ""], ["K", "Sowmya Nag", ""]]}, {"id": "1912.03824", "submitter": "Enric Boix Adser\\`a", "authors": "Enric Boix-Adser\\`a, Lior Eldar, Saeed Mehraban", "title": "Approximating the Determinant of Well-Conditioned Matrices by Shallow\n  Circuits", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determinant can be computed by classical circuits of depth $O(\\log^2 n)$,\nand therefore it can also be computed in classical space $O(\\log^2 n)$. Recent\nprogress by Ta-Shma [Ta13] implies a method to approximate the determinant of\nHermitian matrices with condition number $\\kappa$ in quantum space $O(\\log n +\n\\log \\kappa)$. However, it is not known how to perform the task in less than\n$O(\\log^2 n)$ space using classical resources only. In this work, we show that\nthe condition number of a matrix implies an upper bound on the depth complexity\n(and therefore also on the space complexity) for this task: the determinant of\nHermitian matrices with condition number $\\kappa$ can be approximated to\ninverse polynomial relative error with classical circuits of depth $\\tilde\nO(\\log n \\cdot \\log \\kappa)$, and in particular one can approximate the\ndeterminant for sufficiently well-conditioned matrices in depth $\\tilde{O}(\\log\nn)$. Our algorithm combines Barvinok's recent complex-analytic approach for\napproximating combinatorial counting problems [Bar16] with the\nValiant-Berkowitz-Skyum-Rackoff depth-reduction theorem for low-degree\narithmetic circuits [Val83].\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 02:57:12 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Boix-Adser\u00e0", "Enric", ""], ["Eldar", "Lior", ""], ["Mehraban", "Saeed", ""]]}, {"id": "1912.03887", "submitter": "Huijun Wu", "authors": "Wenzhe Zhang, Kai Lu, Ruibo Wang, Wanqing Chi, Mingtian Shao, Huijun\n  Wu, Mikel Luj\\'an, Xiaoping Wang", "title": "Lightweight Container-based User Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern operating systems all support multi-users that users could share a\ncomputer simultaneously and not affect each other. However, there are some\nlimitations. For example, privacy problem exists that users are visible to each\nother in terms of running processes and files. Moreover, users have little\nfreedom to customize the system environment. Last, it is a burden for system\nadministrator to safely manage and update system environment while satisfying\nmultiple users. Facing the above problems, this paper proposes CUE, a\nLightweight Container-based User Environment. CUE proposes a new notion that\nstands in between application container and operating system container:user\ncontainer. CUE is able to give users more flexibility to customize their\nenvironment, achieve privacy isolation, and make system update easier and\nsafer. Its goal is to optimize and enhance the multi-user notion of current\noperating system and being lightweight. Moreover, it is able to facilitate\napplication deployment in high performance clusters. It is currently deployed\nin NUDT's Tianhe E prototype supercomputer. Experiment results show that it\nintroduces negligible overhead.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 07:55:25 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhang", "Wenzhe", ""], ["Lu", "Kai", ""], ["Wang", "Ruibo", ""], ["Chi", "Wanqing", ""], ["Shao", "Mingtian", ""], ["Wu", "Huijun", ""], ["Luj\u00e1n", "Mikel", ""], ["Wang", "Xiaoping", ""]]}, {"id": "1912.03920", "submitter": "Gabriel Moreau", "authors": "Cyrille Bonamy (LEGI), Laurent Lef\\`evre (AVALON), Gabriel Moreau\n  (LEGI)", "title": "High performance computing and energy efficiency: focus on OpenFOAM", "comments": "Vid{\\'e}o\n  https://replay.jres.org/videos/watch/811bb2fe-582e-4996-9643-89401d2c0d2c ,\n  in French, Congr\\`es JRES : Les Journ\\'ees R\\'eseaux de l'Enseignement et de\n  la Recherche, RENATER, Dec 2019, Dijon, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance calculation is increasingly used within society. Previously\nreserved for an elite, based on large computing and storage infrastructures, it\nis now a core module for many companies. Indeed, high performance calculation\nmakes it possible to design and optimize many elements for a limited cost\n,compared to the production of prototypes or tests in situ. It is also widely\nused in big data and artificial intelligence.It seems essential to ask about\ntheenvironmental impact of these digital practices. A number of actions have\nalready been initiated in this community: GREEN500; European CoC\neco-responsibility label for Data centres... but these actions generally look\nat specific or even idealised situations and/or software.The software\nqualification process in the field of high performance calculation consists in\nlooking at the scalability of the software. The originality of this study is to\nfocus on energy scalability (calculation return time depending on the power\nconsumed), by considering several architectures (three TOP500 machines and a\nlaboratory cluster).The energy cost of an example calculation could be\nestimated, which shows that the most efficient machine in terms of calculation\ntime is not necessarily the most energy efficient, and depending on the number\nof cores/processes chosen, it is not always the same architecture that is the\nmost energy-efficient. It was therefore possible to show that the longer the\nuser is prepared to wait, the less energy is used by the calculation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 09:35:56 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Bonamy", "Cyrille", "", "LEGI"], ["Lef\u00e8vre", "Laurent", "", "AVALON"], ["Moreau", "Gabriel", "", "LEGI"]]}, {"id": "1912.03923", "submitter": "Gabriel Moreau", "authors": "Nicolas Gibelin (GRICAD), R\\'emi Cailletaud (OSUG), Gabriel Moreau\n  (LEGI), Jean-Fran\\c{c}ois Scariot, Gabrielle Feltin (GRICAD), Anthony Defize\n  (GRICAD)", "title": "Nova -- A rainbow cloud over the Alps", "comments": "Vid{\\'e}o\n  https://replay.jres.org/videos/watch/c0ce8c10-fc41-4cf7-9069-9d2c225f9e0a ,\n  in French, Congr\\`es JRES : Les Journ\\'ees R\\'eseaux de l'Enseignement et de\n  la Recherche, RENATER, Dec 2019, Dijon, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pooled and shared on-demand Infrastructure as a Service (IaaS), based on\nthe Openstack software suite, was rolled out on the Grenoble university campus\nin 2018 and updated in 2019.We present the methods used to deploy and manage\nthe infrastructure: racadm and preseed for basic system installation, then\nKolla for Openstack deployment. This latter solution, based on containers for\neach service, enables a centralised and logged configuration (GitLab) of\ncontrollers and calculation nodes. The solution is the benchmark solution for a\nreproducible deployment of Openstack. We have been able to expand our cloud\neasily with new nodes. The change in version of the basic OS was also\nsuccessfully tested despite a few small hitches... As security is a key element\nin the proper operation of this type of shared service, each project has been\nmade watertight and its data perfectly isolated from other projects, thanks to\nthe encryption of all network flows in VXLANs.This OpenStack infotainment\nplatform is operational. What is it all for? For example, our first users use\nthe Jupyter Notebook through the provision of Jupyterhub servers (web portal);\nthe Distributed Health Assessment IT System (SIDES project); the continuous\nintegration in connection with the GitLab platform; the test for the Kubernetes\ncontainer scheduler or the calculation and visualisation software, etc. Highly\nvaried uses that other platforms had difficulty offering.Nova, a new platform,\nwas born.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 09:39:31 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Gibelin", "Nicolas", "", "GRICAD"], ["Cailletaud", "R\u00e9mi", "", "OSUG"], ["Moreau", "Gabriel", "", "LEGI"], ["Scariot", "Jean-Fran\u00e7ois", "", "GRICAD"], ["Feltin", "Gabrielle", "", "GRICAD"], ["Defize", "Anthony", "", "GRICAD"]]}, {"id": "1912.03942", "submitter": "Nico H\\\"ubner", "authors": "Nico H\\\"ubner, Yannick Rink, Michael Suriyah, Thomas Leibfried", "title": "Distributed AC-DC Optimal Power Flow in the European Transmission Grid\n  with ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed or multi-area optimal power flow appears to be promising in order\nto cope with computational burdens in large-scale grids and without the\nregional system operators losing control over their respective areas. However,\nalgorithms are usually tested either in small test cases or single countries.\nWe present a realistic case study in the interconnected European transmission\ngrid with over 5000 buses and 315 GW load. The grid is partitioned into 24\nareas which correspond to the respective single countries. We use a full\nalternating current model and integrate multi-terminal direct current systems.\nThe decomposed problem is solved via a modified Alternating Direction of\nMultipliers Method (ADMM), in which the single countries only exchange border\nnode information with each neighbor. In terms of generation costs, the solution\nof the distributed optimal power flow problem deviates only 0.02% from a\ncentrally computed one. Consensus between all regions is reached before 200\niterations, which is remarkable in such a large system.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 10:11:23 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["H\u00fcbner", "Nico", ""], ["Rink", "Yannick", ""], ["Suriyah", "Michael", ""], ["Leibfried", "Thomas", ""]]}, {"id": "1912.03998", "submitter": "Guillaume Gbikpi-Benissan", "authors": "Guillaume Gbikpi-Benissan, Frederic Magoules", "title": "Beam-tracing domain decomposition method for urban acoustic pollution", "comments": null, "journal-ref": null, "doi": "10.1109/DCABES.2014.34", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the fast solution of large acoustic problems on\nlow-resources parallel platforms. A domain decomposition method is coupled with\na dynamic load balancing scheme to efficiently accelerate a geometrical\nacoustic method. The geometrical method studied implements a beam-tracing\nmethod where intersections are handled as in a ray-tracing method. Beyond the\ndistribution of the global processing upon multiple sub-domains, a second\nparallelization level is operated by means of multi-threading and shared memory\nmechanisms.\n  Numerical experiments show that this method allows to handle large scale open\ndomains for parallel computing purposes on few machines. Urban acoustic\npollution arrising from car traffic was simulated on a large model of the\nShinjuku district of Tokyo, Japan. The good speed-up results illustrate the\nperformance of this new domain decomposition method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:30:05 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Gbikpi-Benissan", "Guillaume", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.04000", "submitter": "Guillaume Gbikpi-Benissan", "authors": "Guillaume Gbikpi-Benissan, Patrick Callet, Frederic Magoules", "title": "Spectral domain decomposition method for physically-based rendering of\n  Royaumont abbey", "comments": null, "journal-ref": null, "doi": "10.1109/CSE-EUC-DCABES.2016.212", "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a virtual reconstitution of the destroyed Royaumont abbey\nchurch, this paper investigates computer sciences issues intrinsic to the\nphysically-based image rendering. First, a virtual model was designed from\nhistorical sources and archaeological descriptions. Then some materials\nphysical properties were measured on remains of the church and on pieces from\nsimilar ancient churches. We specify the properties of our lighting source\nwhich is a representation of the sun, and present the rendering algorithm\nimplemented in our software Virtuelium. In order to accelerate the computation\nof the interactions between light-rays and objects, this ray-tracing algorithm\nis parallelized by means of domain decomposition techniques. Numerical\nexperiments show that the computational time saved by a classic parallelization\nis much less significant than that gained with our approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:36:39 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Gbikpi-Benissan", "Guillaume", ""], ["Callet", "Patrick", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.04050", "submitter": "Gang Chen", "authors": "Gang Chen, Shengyu He, Haitao Meng, Kai Huang", "title": "PhoneBit: Efficient GPU-Accelerated Binary Neural Network Inference\n  Engine for Mobile Phones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, a great success of deep neural networks (DNNs) has been\nwitnessed in computer vision and other fields. However, performance and power\nconstraints make it still challenging to deploy DNNs on mobile devices due to\ntheir high computational complexity. Binary neural networks (BNNs) have been\ndemonstrated as a promising solution to achieve this goal by using bit-wise\noperations to replace most arithmetic operations. Currently, existing\nGPU-accelerated implementations of BNNs are only tailored for desktop\nplatforms. Due to architecture differences, mere porting of such\nimplementations to mobile devices yields suboptimal performance or is\nimpossible in some cases. In this paper, we propose PhoneBit, a GPU-accelerated\nBNN inference engine for Android-based mobile devices that fully exploits the\ncomputing power of BNNs on mobile GPUs. PhoneBit provides a set of\noperator-level optimizations including locality-friendly data layout, bit\npacking with vectorization and layers integration for efficient binary\nconvolution. We also provide a detailed implementation and parallelization\noptimization for PhoneBit to optimally utilize the memory bandwidth and\ncomputing power of mobile GPUs. We evaluate PhoneBit with AlexNet, YOLOv2 Tiny\nand VGG16 with their binary version. Our experiment results show that PhoneBit\ncan achieve significant speedup and energy efficiency compared with\nstate-of-the-art frameworks for mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:52:24 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chen", "Gang", ""], ["He", "Shengyu", ""], ["Meng", "Haitao", ""], ["Huang", "Kai", ""]]}, {"id": "1912.04239", "submitter": "Krzysztof Nowicki", "authors": "Krzysztof Nowicki", "title": "A Deterministic Algorithm for the MST Problem in Constant Rounds of\n  Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the Minimum Spanning Tree problem can be solved\n\\emph{deterministically}, in $\\mathcal{O}(1)$ rounds of the\n$\\mathsf{Congested}$ $\\mathsf{Clique}$ model.\n  In the $\\mathsf{Congested}$ $\\mathsf{Clique}$ model, there are $n$ players\nthat perform computation in synchronous rounds. Each round consist of a phase\nof local computation and a phase of communication, in which each pair of\nplayers is allowed to exchange $\\mathcal{O}(\\log n)$ bit messages.\n  The studies of this model began with the MST problem: in the paper by Lotker\net al.[SPAA'03, SICOMP'05] that defines the $\\mathsf{Congested}$\n$\\mathsf{Clique}$ model the authors give a deterministic $\\mathcal{O}(\\log \\log\nn)$ round algorithm that improved over a trivial $\\mathcal{O}(\\log n)$ round\nadaptation of Bor\\r{u}vka's algorithm.\n  There was a sequence of gradual improvements to this result: an\n$\\mathcal{O}(\\log \\log \\log n)$ round algorithm by Hegeman et al. [PODC'15], an\n$\\mathcal{O}(\\log^* n)$ round algorithm by Ghaffari and Parter, [PODC'16] and\nan $\\mathcal{O}(1)$ round algorithm by Jurdzi\\'nski and Nowicki, [SODA'18], but\nall those algorithms were randomized, which left the question about the\nexistence of any deterministic $o(\\log \\log n)$ round algorithms for the\nMinimum Spanning Tree problem open.\n  Our result resolves this question and establishes that $\\mathcal{O}(1)$\nrounds is enough to solve the MST problem in the $\\mathsf{Congested}$\n$\\mathsf{Clique}$ model, even if we are not allowed to use any randomness.\nFurthermore, the amount of communication needed by the algorithm makes it\napplicable to some variants of the $\\mathsf{MPC}$ model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:27:50 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 15:19:05 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 16:50:59 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Nowicki", "Krzysztof", ""]]}, {"id": "1912.04269", "submitter": "Vero Estrada-Gali\\~nanes", "authors": "Vero Estrada-Galinanes, Racin Nygaard, Viktor Tron, Rodrigo Saramago,\n  Leander Jehl and Hein Meling", "title": "[Invited talk] Building a Disaster-resilient Storage Layer for Next\n  Generation Networks: The Role of Redundancy", "comments": "6 pages, Invited talk at Technical Committee on Network Systems\n  Workshop 2019-10-10 - 2019-10-11, Nagoya Institute of Technology, Japan", "journal-ref": "IEICE Technical Report, 2019, vol. 119, no. 221, Print edition\n  ISSN 0913-5685, Online edition ISSN 2432-6380", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is the driving force behind a myriad of decentralized applications\n(dapps) that promise to transform the Internet. The next generation Internet,\nor web3, introduces a \"universal state layer\" to store data in p2p networks.\nSwarm, a native layer of the Ethereum web3 stack, aims at providing redundant\nstorage for dapp code, data, as well as, blockchain and state data. Based on a\ndiploma verification dapp use case, we share insights on the role of redundancy\nstrategies in designing a reliable storage layer. Our proof-of-concept improves\nSwarm's resilience to failures by balancing repairs and storage, with a\nslightly added latency.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 19:45:44 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Estrada-Galinanes", "Vero", ""], ["Nygaard", "Racin", ""], ["Tron", "Viktor", ""], ["Saramago", "Rodrigo", ""], ["Jehl", "Leander", ""], ["Meling", "Hein", ""]]}, {"id": "1912.04352", "submitter": "Mengchen Wang", "authors": "Mengchen Wang, Nicolas Ferey, Patrick Bourdot, Frederic Magoules", "title": "Using asynchronous simulation approach for interactive simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses about the advantage of using asynchronous simulation in\nthe case of interactive simulation in which user can steer and control\nparameters during a simulation in progress. synchronous models allow to compute\neach iteration faster to address the issues of performance needed in an highly\ninteractive context, and our hypothesis is that get partial results faster is\nbetter than getting synchronized and final results to take a decision, in a\ninteractive simulation context.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 20:07:36 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Mengchen", ""], ["Ferey", "Nicolas", ""], ["Bourdot", "Patrick", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.04466", "submitter": "Jiaming Ye", "authors": "Yinxing Xue (1), Jiaming Ye (1), Mingliang Ma (1), Lei Ma (2), Yi Li\n  (3), Haijun Wang (3), Yun Lin (4), Tianyong Peng (1), Yang Liu (3) ((1)\n  University of Science and Technology of China, (2) Kyushu University, (3)\n  Nanyang Technological University, (4) National University of Singapore)", "title": "Doublade: Unknown Vulnerability Detection in Smart Contracts Via\n  Abstract Signature Matching and Refined Detection Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prosperity of smart contracts and the blockchain technology, various\nsecurity analyzers have been proposed from both the academia and industry to\naddress the associated risks. Yet, there does not exist a high-quality\nbenchmark of smart contract vulnerability for security research. In this study,\nwe propose an approach towards building a high-quality vulnerability benchmark.\nOur approach consists of two parts. First, to improve recall, we propose to\nsearch for similar vulnerabilities in an automated way by leveraging the\nabstract vulnerability signature (AVS). Second, to remove the false positives\n(FPs) due to AVS-based matching, we summarize the detection rules of existing\ntools and apply the refined rules by considering various defense mechanisms\n(DMs). By integrating AVS-based code matching and the refined detection rules\n(RDR), our approach achieves higher precision and recall. On the collected\n76,354 contracts, we build a benchmark consisting of 1,219 vulnerabilities\ncovering five different vulnerability types identified together by our tool\n(DOUBLADE) and other three scanners. Additionally, we conduct a comparison\nbetween DOUBLADE and the others, on an additional 17,770 contracts. Results\nshow that DOUBLADE can yield a better detection accuracy with similar execution\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 03:09:57 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Xue", "Yinxing", ""], ["Ye", "Jiaming", ""], ["Ma", "Mingliang", ""], ["Ma", "Lei", ""], ["Li", "Yi", ""], ["Wang", "Haijun", ""], ["Lin", "Yun", ""], ["Peng", "Tianyong", ""], ["Liu", "Yang", ""]]}, {"id": "1912.04481", "submitter": "Yuan Yao", "authors": "Sam Likun Xi, Yuan Yao, Kshitij Bhardwaj, Paul Whatmough, Gu-Yeon Wei,\n  David Brooks", "title": "SMAUG: End-to-End Full-Stack Simulation Infrastructure for Deep Learning\n  Workloads", "comments": "14 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been tremendous advances in hardware acceleration\nof deep neural networks. However, most of the research has focused on\noptimizing accelerator microarchitecture for higher performance and energy\nefficiency on a per-layer basis. We find that for overall single-batch\ninference latency, the accelerator may only make up 25-40%, with the rest spent\non data movement and in the deep learning software framework. Thus far, it has\nbeen very difficult to study end-to-end DNN performance during early stage\ndesign (before RTL is available) because there are no existing DNN frameworks\nthat support end-to-end simulation with easy custom hardware accelerator\nintegration. To address this gap in research infrastructure, we present SMAUG,\nthe first DNN framework that is purpose-built for simulation of end-to-end deep\nlearning applications. SMAUG offers researchers a wide range of capabilities\nfor evaluating DNN workloads, from diverse network topologies to easy\naccelerator modeling and SoC integration. To demonstrate the power and value of\nSMAUG, we present case studies that show how we can optimize overall\nperformance and energy efficiency for up to 1.8-5x speedup over a baseline\nsystem, without changing any part of the accelerator microarchitecture, as well\nas show how SMAUG can tune an SoC for a camera-powered deep learning pipeline.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 03:46:59 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:18:02 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Xi", "Sam Likun", ""], ["Yao", "Yuan", ""], ["Bhardwaj", "Kshitij", ""], ["Whatmough", "Paul", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""]]}, {"id": "1912.04513", "submitter": "Pierre Tholoniat", "authors": "Rob van Glabbeek, Vincent Gramoli, Pierre Tholoniat", "title": "Cross-Chain Payment Protocols with Success Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of cross-chain payment whereby\ncustomers of different escrows -- implemented by a bank or a blockchain smart\ncontract -- successfully transfer digital assets without trusting each other.\nPrior to this work, cross-chain payment problems did not require this success\nor any form of progress. We introduce a new specification formalism called\nAsynchronous Networks of Timed Automata (ANTA) to formalise such protocols.\n  We present the first cross-chain payment protocol that ensures termination in\na bounded amount of time and works correctly in the presence of clock skew. We\nthen demonstrate that it is impossible to solve this problem without assuming\nsynchrony, in the sense that each message is guaranteed to arrive within a\nknown amount of time. We also offer a protocol that solves an eventually\nterminating variant of this cross-chain payment problem without synchrony, and\neven in the presence of Byzantine failures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 05:56:55 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["van Glabbeek", "Rob", ""], ["Gramoli", "Vincent", ""], ["Tholoniat", "Pierre", ""]]}, {"id": "1912.04526", "submitter": "Ence Zhou", "authors": "Ence Zhou, Haoli Sun, Bingfeng Pi, Jun Sun, Kazuhiro Yamashita,\n  Yoshihide Nomura", "title": "Ledgerdata Refiner: A Powerful Ledger Data Query Platform for\n  Hyperledger Fabric", "comments": "8 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blockchain is one of the most popular distributed ledger technologies. It can\nsolve the trust issue among enterprises. Hyperledger Fabric is a permissioned\nblockchain aiming at enterprise-grade business applications. However, compared\nto traditional distributed database solutions, one issue of blockchain based\napplication development is the limited data access. For Fabric, the ledger data\ncan only be retrieved by limited interfaces provided by Fabric SDKs or\nchaincode. In order to meet the requirements of data query and provide flexible\nquery functions for real applications built on Fabric, this paper proposed a\nledger data query platform called Ledgerdata Refiner. With ledger data analysis\nmiddleware, we provide sufficient interfaces for users to retrieve block or\ntransaction efficiently. It is also able to track historical operations for any\nspecific state. In addition, schemas of ledger state have been analyzed and\nclustered, which enable users to perform rich queries against ledger data.\nFinally, we validate the effectiveness of our query platform on a real\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:24:01 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Zhou", "Ence", ""], ["Sun", "Haoli", ""], ["Pi", "Bingfeng", ""], ["Sun", "Jun", ""], ["Yamashita", "Kazuhiro", ""], ["Nomura", "Yoshihide", ""]]}, {"id": "1912.04531", "submitter": "Prashant Khanduri", "authors": "Prashant Khanduri, Saikiran Bulusu, Pranay Sharma, and Pramod K.\n  Varshney", "title": "Byzantine Resilient Non-Convex SVRG with Distributed Batch Gradient\n  Computations", "comments": "Optimization for Machine Learning, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the distributed stochastic optimization problem of\nminimizing a non-convex function $f(x) = \\mathbb{E}_{\\xi \\sim \\mathcal{D}} f(x;\n\\xi)$ in an adversarial setting, where the individual functions $f(x; \\xi)$ can\nalso be potentially non-convex. We assume that at most $\\alpha$-fraction of a\ntotal of $K$ nodes can be Byzantines. We propose a robust stochastic\nvariance-reduced gradient (SVRG) like algorithm for the problem, where the\nbatch gradients are computed at the worker nodes (WNs) and the stochastic\ngradients are computed at the server node (SN). For the non-convex optimization\nproblem, we show that we need $\\tilde{O}\\left( \\frac{1}{\\epsilon^{5/3} K^{2/3}}\n+ \\frac{\\alpha^{4/3}}{\\epsilon^{5/3}} \\right)$ gradient computations on average\nat each node (SN and WNs) to reach an $\\epsilon$-stationary point. The proposed\nalgorithm guarantees convergence via the design of a novel Byzantine filtering\nrule which is independent of the problem dimension. Importantly, we capture the\neffect of the fraction of Byzantine nodes $\\alpha$ present in the network on\nthe convergence performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:42:52 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Khanduri", "Prashant", ""], ["Bulusu", "Saikiran", ""], ["Sharma", "Pranay", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1912.04648", "submitter": "Jonas Traub", "authors": "Jonas Traub (1 and 2), Julius H\\\"ulsmann (1), Sebastian Bre{\\ss} (1),\n  Tilmann Rabl (3), Volker Markl (1 and 2) ((1) Technische Universit\\\"at\n  Berlin, (2) German Research Center for Artificial Intelligence (DFKI), (3)\n  Hasso-Plattner-Institut Potsdam (HPI))", "title": "SENSE: Scalable Data Acquisition from Distributed Sensors with\n  Guaranteed Time Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis in the Internet of Things (IoT) requires us to combine event\nstreams from a huge amount of sensors. This combination (join) of events is\nusually based on the time stamps associated with the events. We address two\nchallenges in environments which acquire and join events in the IoT: First, due\nto the growing number of sensors, we are facing the performance limits of\ncentral joins with respect to throughput, latency, and network utilization.\nSecond, in the IoT, diverse sensor nodes are operated by different\norganizations and use different time synchronization techniques. Thus, events\nwith the same timestamps are not necessarily recorded at the exact same time\nand joined data tuples have an unknown time incoherence. This can cause\nundetected failures, such as false correlations and wrong predictions. We\npresent SENSE, a system for scalable data acquisition from distributed sensors.\nSENSE introduces time coherence measures as a fundamental data characteristic\nin addition to common time synchronization techniques. The time coherence of a\ndata tuple is the time span in which all values contained in the tuple have\nbeen read from sensors. We explore concepts and algorithms to quantify and\noptimize time coherence and show that SENSE scales to thousands of sensors,\noperates efficiently under latency and coherence constraints, and adapts to\nchanging network conditions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 11:17:15 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Traub", "Jonas", "", "1 and 2"], ["H\u00fclsmann", "Julius", "", "1 and 2"], ["Bre\u00df", "Sebastian", "", "1 and 2"], ["Rabl", "Tilmann", "", "1 and 2"], ["Markl", "Volker", "", "1 and 2"]]}, {"id": "1912.04753", "submitter": "Zhipeng Gui", "authors": "Yuan Wang, Zhipeng Gui, Huayi Wu, Dehua Peng, Jinghang Wu, Zousen Cui", "title": "Optimizing and accelerating space-time Ripley's K function based on\n  Apache Spark for distributed spatiotemporal point pattern analysis", "comments": "35 pages, 23 figures, Future Generation Computer Systems", "journal-ref": "Future Generation Computer Systems, 2020", "doi": "10.1016/j.future.2019.11.036", "report-no": null, "categories": "stat.CO cs.CG cs.DC cs.DS cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With increasing point of interest (POI) datasets available with fine-grained\nspatial and temporal attributes, space-time Ripley's K function has been\nregarded as a powerful approach to analyze spatiotemporal point process.\nHowever, space-time Ripley's K function is computationally intensive for\npoint-wise distance comparisons, edge correction and simulations for\nsignificance testing. Parallel computing technologies like OpenMP, MPI and CUDA\nhave been leveraged to accelerate the K function, and related experiments have\ndemonstrated the substantial acceleration. Nevertheless, previous works have\nnot extended optimization of Ripley's K function from space dimension to\nspace-time dimension. Without sophisticated spatiotemporal query and\npartitioning mechanisms, extra computational overhead can be problematic.\nMeanwhile, these researches were limited by the restricted scalability and\nrelative expensive programming cost of parallel frameworks and impeded their\napplications for large POI dataset and Ripley's K function variations. This\npaper presents a distributed computing method to accelerate space-time Ripley's\nK function upon state-of-the-art distributed computing framework Apache Spark,\nand four strategies are adopted to simplify calculation procedures and\naccelerate distributed computing respectively. Based on the optimized method, a\nweb-based visual analytics framework prototype has been developed. Experiments\nprove the feasibility and time efficiency of the proposed method, and also\ndemonstrate its value on promoting applications of space-time Ripley's K\nfunction in ecology, geography, sociology, economics, urban transportation and\nother fields.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:15:37 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Yuan", ""], ["Gui", "Zhipeng", ""], ["Wu", "Huayi", ""], ["Peng", "Dehua", ""], ["Wu", "Jinghang", ""], ["Cui", "Zousen", ""]]}, {"id": "1912.04859", "submitter": "Anudit Nagar", "authors": "Anudit Nagar", "title": "Privacy-Preserving Blockchain Based Federated Learning with Differential\n  Data Sharing", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the modern world where data is becoming one of the most valuable assets,\nrobust data privacy policies rooted in the fundamental infrastructure of\nnetworks and applications are becoming an even bigger necessity to secure\nsensitive user data. In due course with the ever-evolving nature of newer\nstatistical techniques infringing user privacy, machine learning models with\nalgorithms built with respect for user privacy can offer a dynamically adaptive\nsolution to preserve user privacy against the exponentially increasing\nmultidimensional relationships that datasets create. Using these privacy aware\nML Models at the core of a Federated Learning Ecosystem can enable the entire\nnetwork to learn from data in a decentralized manner. By harnessing the\never-increasing computational power of mobile devices, increasing network\nreliability and IoT devices revolutionizing the smart devices industry, and\ncombining it with a secure and scalable, global learning session backed by a\nblockchain network with the ability to ensure on-device privacy, we allow any\nInternet enabled device to participate and contribute data to a global privacy\npreserving, data sharing network with blockchain technology even allowing the\nnetwork to reward quality work. This network architecture can also be built on\ntop of existing blockchain networks like Ethereum and Hyperledger, this lets\neven small startups build enterprise ready decentralized solutions allowing\nanyone to learn from data across different departments of a company, all the\nway to thousands of devices participating in a global synchronized learning\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:58:10 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Nagar", "Anudit", ""]]}, {"id": "1912.05160", "submitter": "Amirhossein Esmaili", "authors": "Amirhossein Esmaili, Massoud Pedram", "title": "Energy-aware Scheduling of Jobs in Heterogeneous Cluster Systems Using\n  Deep Reinforcement Learning", "comments": "Accepted in International Symposium on Quality Electronic Design\n  (ISQED), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption is one of the most critical concerns in designing\ncomputing devices, ranging from portable embedded systems to computer cluster\nsystems. Furthermore, in the past decade, cluster systems have increasingly\nrisen as popular platforms to run computing-intensive real-time applications in\nwhich the performance is of great importance. However, due to different\ncharacteristics of real-time workloads, developing general job scheduling\nsolutions that efficiently address both energy consumption and performance in\nreal-time cluster systems is a challenging problem. In this paper, inspired by\nrecent advances in applying deep reinforcement learning for resource management\nproblems, we present the Deep-EAS scheduler that learns efficient energy-aware\nscheduling strategies for workloads with different characteristics without\ninitially knowing anything about the scheduling task at hand. Results show that\nDeep-EAS converges quickly, and performs better compared to standard\nmanually-tuned heuristics, especially in heavy load conditions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:07:50 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Esmaili", "Amirhossein", ""], ["Pedram", "Massoud", ""]]}, {"id": "1912.05208", "submitter": "Kazuyuki Shudo", "authors": "Ryunosuke Nagayama, Kazuyuki Shudo, Ryohei Banno", "title": "Identifying Impacts of Protocol and Internet Development on the Bitcoin\n  Network", "comments": "Proc. 25th IEEE Symposium on Computers and Communications (IEEE ISCC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving transaction throughput is an important challenge for Bitcoin.\nHowever, shortening the block generation interval or increasing the block size\nto improve throughput makes it sharing blocks within the network slower and\nincreases the number of orphan blocks. Consequently, the security of the\nblockchain is sacrificed. To mitigate this, it is necessary to reduce the block\npropagation delay. Because of the contribution of new Bitcoin protocols and the\nimprovements of the Internet, the block propagation delay in the Bitcoin\nnetwork has been shortened in recent years. In this study, we identify impacts\nof compact block relay---an up-to-date Bitcoin protocol---and Internet\nimprovement on the block propagation delay and fork rate in the Bitcoin network\nfrom 2015 to 2019. Existing measurement studies could not identify them but our\nsimulation enables it. The experimental results reveal that compact block relay\ncontributes to shortening the block propagation delay more than Internet\nimprovements. The block propagation delay is reduced by 64.5% for the 50th\npercentile and 63.7% for the 90th percentile due to Internet improvements, and\nby 90.1% for the 50th percentile and by 87.6% for the 90th percentile due to\ncompact block relay.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:59:54 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 20:24:33 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Nagayama", "Ryunosuke", ""], ["Shudo", "Kazuyuki", ""], ["Banno", "Ryohei", ""]]}, {"id": "1912.05241", "submitter": "Jiashuo Zhang", "authors": "Jiashuo Zhang, Jianbo Gao, Zhenhao Wu, Wentian Yan, Qize Wu, Qingshan\n  Li, Zhong Chen", "title": "Performance Analysis of the Libra Blockchain: An Experimental Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies\nhave been proposed based on blockchain. However, the performance of\npermissionless blockchains restricts the widespread of cryptocurrency.\nRecently, Libra was proposed by Facebook based on a permissioned blockchain,\ni.e. the Libra blockchain. The vision of Libra is to become a global currency\nsupporting financial applications, but it is doubted whether the performance of\nthe Libra blockchain is able to support frequent micropayment scenarios. In\nthis paper, we propose a methodology to evaluate the performance of blockchain\nplatforms and conducted an experimental study on the Libra blockchain. The\nresults show that the Libra blockchain can only process about one thousand\ntransactions per second at most, and the performance drops significantly as the\nnumber of validators increases. Although it outperforms permissionless\nblockchain platforms, the performance of the Libra blockchain is still\nunsatisfactory compared to other permissioned blockchains like Hyperledger\nFabric and needs to make effective improvements in order to support global\nmicropayment in the future.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:33:36 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zhang", "Jiashuo", ""], ["Gao", "Jianbo", ""], ["Wu", "Zhenhao", ""], ["Yan", "Wentian", ""], ["Wu", "Qize", ""], ["Li", "Qingshan", ""], ["Chen", "Zhong", ""]]}, {"id": "1912.05381", "submitter": "Tom Cornebize", "authors": "Tom Cornebize (UGA, POLARIS), Arnaud Legrand (CNRS, POLARIS)", "title": "DGEMM performance is data-dependent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DGEMM function is a widely used implementation of the matrix product.\nWhile the asymptotic complexity of the algorithm only depends on the sizes of\nthe matrices, we show that the performance is significantly impacted by the\nmatrices content. Our experiments show that this may be due to bit flips in the\nCPU causing an energy consumption overhead.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 15:18:07 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Cornebize", "Tom", "", "UGA, POLARIS"], ["Legrand", "Arnaud", "", "CNRS, POLARIS"]]}, {"id": "1912.05390", "submitter": "Merav Parter", "authors": "Artur Czumaj, Peter Davies and Merav Parter", "title": "Graph Sparsification for Derandomizing Massively Parallel Computation\n  with Low Space", "comments": "he coloring part was omitted from the current version, and will\n  appear soon on a separate arXiv manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massively Parallel Computation (MPC) model is an emerging model which\ndistills core aspects of distributed and parallel computation. It has been\ndeveloped as a tool to solve (typically graph) problems in systems where the\ninput is distributed over many machines with limited space. Recent work has\nfocused on the regime in which machines have sublinear (in $n$, the number of\nnodes in the input graph) memory, with randomized algorithms presented for\nfundamental graph problems of Maximal Matching and Maximal Independent Set.\nHowever, there have been no prior corresponding \\emph{deterministic}\nalgorithms.\n  A major challenge underlying the sublinear space setting is that the local\nspace of each machine might be too small to store all the edges incident to a\nsingle node. This poses a considerable obstacle compared to the classical\nmodels in which each node is assumed to know and have easy access to its\nincident edges. To overcome this barrier we introduce a new \\emph{graph\nsparsification technique} that \\emph{deterministically} computes a low-degree\nsubgraph with additional desired properties. Using this framework to\nderandomize the well-known randomized algorithm of Luby [SICOMP'86], we obtain\n$O(\\log \\Delta+\\log\\log n)$-round \\emph{deterministic} MPC algorithms for\nsolving the fundamental problems of \\emph{Maximal Matching} and \\emph{Maximal\nIndependent Set} with $O(n^{\\epsilon})$ space on each machine for any constant\n$\\epsilon > 0$. Based on the recent work of Ghaffari et al. [FOCS'18], this\nadditive $O(\\log\\log n)$ factor is \\emph{conditionally} essential. These\nalgorithms can also be shown to run in $O(\\log \\Delta)$ rounds in the closely\nrelated model of \\congc, improving upon the state-of-the-art bound of $O(\\log^2\n\\Delta)$ rounds by Censor-Hillel et al. [DISC'17].\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 15:31:32 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 09:14:58 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 13:27:13 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""], ["Parter", "Merav", ""]]}, {"id": "1912.05416", "submitter": "Xiaolong Ma", "authors": "Geng Yuan, Xiaolong Ma, Sheng Lin, Zhengang Li, Caiwen Ding", "title": "A SOT-MRAM-based Processing-In-Memory Engine for Highly Compressed DNN\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.DC cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computing wall and data movement challenges of deep neural networks\n(DNNs) have exposed the limitations of conventional CMOS-based DNN\naccelerators. Furthermore, the deep structure and large model size will make\nDNNs prohibitive to embedded systems and IoT devices, where low power\nconsumption are required. To address these challenges, spin orbit torque\nmagnetic random-access memory (SOT-MRAM) and SOT-MRAM based\nProcessing-In-Memory (PIM) engines have been used to reduce the power\nconsumption of DNNs since SOT-MRAM has the characteristic of near-zero standby\npower, high density, none-volatile. However, the drawbacks of SOT-MRAM based\nPIM engines such as high writing latency and requiring low bit-width data\ndecrease its popularity as a favorable energy efficient DNN accelerator. To\nmitigate these drawbacks, we propose an ultra energy efficient framework by\nusing model compression techniques including weight pruning and quantization\nfrom the software level considering the architecture of SOT-MRAM PIM. And we\nincorporate the alternating direction method of multipliers (ADMM) into the\ntraining phase to further guarantee the solution feasibility and satisfy\nSOT-MRAM hardware constraints. Thus, the footprint and power consumption of\nSOT-MRAM PIM can be reduced, while increasing the overall system throughput at\nthe meantime, making our proposed ADMM-based SOT-MRAM PIM more energy\nefficiency and suitable for embedded systems or IoT devices. Our experimental\nresults show the accuracy and compression rate of our proposed framework is\nconsistently outperforming the reference works, while the efficiency (area \\&\npower) and throughput of SOT-MRAM PIM engine is significantly improved.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 22:03:26 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Li", "Zhengang", ""], ["Ding", "Caiwen", ""]]}, {"id": "1912.05517", "submitter": "Maria Salama", "authors": "Maria Salama, Rami Bahsoon, Rajkumar Buyya", "title": "A Reference Architecture and Modelling Principles for Architectural\n  Stability based on Self-Awareness: Case of Cloud Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased dependence on software, there is a pressing need for\nengineering long-lived software. As architectures have a profound effect on the\nlife-span of the software and the provisioned quality of service, stable\narchitectures are significant assets. Architectural stability tends to reflect\nthe success of the system in supporting continuous changes without phasing-out.\nThe \\textit{behavioural} aspect of stability is essential for seamless\noperation, to continuously keep the provision of quality requirements stable\nand prevent architecture's drifting and phasing-out. In this paper, we\nintroduce a reference architecture and model for stability. Specifically, we\nleverage on the self-awareness principles and runtime goals modelling to\nexplicitly support architectural stability. To illustrate the applicability and\nevaluate the proposed approach, we consider the case of cloud architectures.\nThe experimental results show that our approach increases the efficiency of the\narchitecture in keeping the expected behaviour stable during runtime operation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:31:17 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Salama", "Maria", ""], ["Bahsoon", "Rami", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1912.05529", "submitter": "Ahmad Shawahna", "authors": "Ahmad Shawahna, Syed Abdul Salam, and Mayez Al-Mouhamed", "title": "Seismic Imaging: An Overview and Parallel Implementation of Poststack\n  Depth Migration", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.DC cs.PF eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seismic migration is the core step of seismic data processing which is\nimportant for oil exploration. Poststack depth migration in frequency-space\n(f-x) domain is one of commonly used algorithms. The wave-equation solution can\nbe approximated as FIR filtering process to extrapolate the raw data and\nextract the subsurface image. Because of its computational complexity, its\nparallel implementation is encouraged. For calculating the next depth level,\nprevious depth level is required. So, this part cannot be parallelized because\nof data dependence. But at each depth level there is plenty of roam for\nparallelism and can be parallelized. In case of CUDA programming, each thread\ncalculate a single pixel on the next depth plan. After calculating the next\ndepth plan, we can calculate the depth row by summing over all the frequencies\nand calculating all the depth rows results in the final migrated image. The\npoststack depth migration is implemented in CUDA and its performance is\nevaluated with the sequential code with different problem sizes.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 01:33:03 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Shawahna", "Ahmad", ""], ["Salam", "Syed Abdul", ""], ["Al-Mouhamed", "Mayez", ""]]}, {"id": "1912.05571", "submitter": "Iman Tabrizian", "authors": "Saeedeh Parsaeefard, Iman Tabrizian, Alberto Leon Garcia", "title": "Representation of Federated Learning via Worst-Case Robust Optimization\n  Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a distributed learning approach where a set of\nend-user devices participate in the learning process by acting on their\nisolated local data sets. Here, we process local data sets of users where\nworst-case optimization theory is used to reformulate the FL problem where the\nimpact of local data sets in training phase is considered as an uncertain\nfunction bounded in a closed uncertainty region. This representation allows us\nto compare the performance of FL with its centralized counterpart, and to\nreplace the uncertain function with a concept of protection functions leading\nto more tractable formulation. The latter supports applying a regularization\nfactor in each user cost function in FL to reach a better performance. We\nevaluated our model using the MNIST data set versus the protection function\nparameters, e.g., regularization factors.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:02:49 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Parsaeefard", "Saeedeh", ""], ["Tabrizian", "Iman", ""], ["Garcia", "Alberto Leon", ""]]}, {"id": "1912.05662", "submitter": "Roger Immich", "authors": "Diego O. Rodrigues, Frances A. Santos, Geraldo P. Rocha Filho, Ademar\n  T. Akabane, Raquel Cabral, Roger Immich, Wellington L. Junior, Felipe D.\n  Cunha, Daniel L. Guidoni, Thiago H. Silva, Denis Ros\\'ario, Eduardo\n  Cerqueira, Antonio A. F. Loureiro, Leandro A. Villas", "title": "Computa\\c{c}\\~ao Urbana da Teoria \\`a Pr\\'atica: Fundamentos,\n  Aplica\\c{c}\\~oes e Desafios", "comments": "in Portuguese. Simp\\'osio Brasileiro de Redes de Computadores e\n  Sistemas Distribu\\'idos (SBRC) 2019 - Minicursos", "journal-ref": "Simposio Brasileiro de Redes de Computadores e Sistemas\n  Distribuidos (SBRC), 2019", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DC cs.HC cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing of cities has resulted in innumerable technical and managerial\nchallenges for public administrators such as energy consumption, pollution,\nurban mobility and even supervision of private and public spaces in an\nappropriate way. Urban Computing emerges as a promising paradigm to solve such\nchallenges, through the extraction of knowledge, from a large amount of\nheterogeneous data existing in urban space. Moreover, Urban Computing\ncorrelates urban sensing, data management, and analysis to provide services\nthat have the potential to improve the quality of life of the citizens of large\nurban centers. Consider this context, this chapter aims to present the\nfundamentals of Urban Computing and the steps necessary to develop an\napplication in this area. To achieve this goal, the following questions will be\ninvestigated, namely: (i) What are the main research problems of Urban\nComputing?; (ii) What are the technological challenges for the implementation\nof services in Urban Computing?; (iii) What are the main methodologies used for\nthe development of services in Urban Computing?; and (iv) What are the\nrepresentative applications in this field?\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:01:58 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Rodrigues", "Diego O.", ""], ["Santos", "Frances A.", ""], ["Filho", "Geraldo P. Rocha", ""], ["Akabane", "Ademar T.", ""], ["Cabral", "Raquel", ""], ["Immich", "Roger", ""], ["Junior", "Wellington L.", ""], ["Cunha", "Felipe D.", ""], ["Guidoni", "Daniel L.", ""], ["Silva", "Thiago H.", ""], ["Ros\u00e1rio", "Denis", ""], ["Cerqueira", "Eduardo", ""], ["Loureiro", "Antonio A. F.", ""], ["Villas", "Leandro A.", ""]]}, {"id": "1912.05716", "submitter": "Stefan Henneking", "authors": "Stefan Henneking and Leszek Demkowicz", "title": "A numerical study of the pollution error and DPG adaptivity for long\n  waveguide simulations", "comments": null, "journal-ref": null, "doi": "10.1016/j.camwa.2020.03.024", "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-frequency wave propagation has many important applications in acoustics,\nelastodynamics, and electromagnetics. Unfortunately, the finite element\ndiscretization for these problems suffers from significant numerical pollution\nerrors that increase with the wavenumber. It is critical to control these\nerrors to obtain a stable and accurate method. We study the effect of pollution\nfor very long waveguide problems in the context of robust discontinuous\nPetrov-Galerkin (DPG) finite element discretizations. Our numerical experiments\nshow that the pollution primarily has a diffusive effect causing energy loss in\nthe DPG method while phase errors appear less significant. We report results\nfor 3D vectorial time-harmonic Maxwell problems in waveguides with more than\n8000 wavelengths. Our results corroborate previous analysis for the Galerkin\ndiscretization of the Helmholtz operator by Melenk and Sauter (2011).\nAdditionally, we discuss adaptive refinement strategies for multi-mode fiber\nwaveguides where the propagating transverse modes must be resolved\nsufficiently. Our study shows the applicability of the DPG error indicator to\nthis class of problems. Finally, we illustrate the importance of load balancing\nin these simulations for distributed-memory parallel computing.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 01:00:44 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 01:36:59 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Henneking", "Stefan", ""], ["Demkowicz", "Leszek", ""]]}, {"id": "1912.05848", "submitter": "Magnus Sj\\\"alander", "authors": "Magnus Sj\\\"alander and Magnus Jahre and Gunnar Tufte and Nico\n  Reissmann", "title": "EPIC: An Energy-Efficient, High-Performance GPGPU Computing Research\n  Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pursuit of many research questions requires massive computational\nresources. State-of-the-art research in physical processes using simulations,\nthe training of neural networks for deep learning, or the analysis of big data\nare all dependent on the availability of sufficient and performant\ncomputational resources. For such research, access to a high-performance\ncomputing infrastructure is indispensable. Many scientific workloads from such\nresearch domains are inherently parallel and can benefit from the data-parallel\narchitecture of general purpose graphics processing units (GPGPUs). However,\nGPGPU resources are scarce at Norway's national infrastructure. EPIC is a GPGPU\nenabled computing research infrastructure at NTNU. It enables NTNU's\nresearchers to perform experiments that otherwise would be impossible, as\ntime-to-solution would simply take too long.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 09:40:37 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 09:06:10 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 16:45:35 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Sj\u00e4lander", "Magnus", ""], ["Jahre", "Magnus", ""], ["Tufte", "Gunnar", ""], ["Reissmann", "Nico", ""]]}, {"id": "1912.05895", "submitter": "Trafim Lasy", "authors": "Trafim Lasy", "title": "From Hashgraph to a Family of Atomic Broadcast Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this article is to extend the ideas concerning Bracha-Toueg\nasynchronous Byzantine Fault Tolerant consensus algorithm and Baird's Hashgraph\nconsensus. We propose a family of atomic broadcast algorithms, which Hashgraph\nconsensus is closely related to. We also do preliminary comparative algorithm\nspeed analysis which shows that some members of the family seriously outperform\nHashgraph consensus. These algorithms can also be readily used as a base of\nproof-of-stake consensuses. In appendix we provide an extension of Hashgraph\ngossip protocol, which efficiently handles byzantine fault information exchange\nbetween nodes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 12:31:36 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Lasy", "Trafim", ""]]}, {"id": "1912.06036", "submitter": "Pranay Sharma", "authors": "Pranay Sharma, Swatantra Kafle, Prashant Khanduri, Saikiran Bulusu,\n  Ketan Rajawat, and Pramod K. Varshney", "title": "Parallel Restarted SPIDER -- Communication Efficient Distributed\n  Nonconvex Optimization with Optimal Computation Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a distributed algorithm for stochastic smooth,\nnon-convex optimization. We assume a worker-server architecture where $N$\nnodes, each having $n$ (potentially infinite) number of samples, collaborate\nwith the help of a central server to perform the optimization task. The global\nobjective is to minimize the average of local cost functions available at\nindividual nodes. The proposed approach is a non-trivial extension of the\npopular parallel-restarted SGD algorithm, incorporating the optimal\nvariance-reduction based SPIDER gradient estimator into it. We prove\nconvergence of our algorithm to a first-order stationary solution. The proposed\napproach achieves the best known communication complexity $O(\\epsilon^{-1})$\nalong with the optimal computation complexity. For finite-sum problems (finite\n$n$), we achieve the optimal computation (IFO) complexity\n$O(\\sqrt{Nn}\\epsilon^{-1})$. For online problems ($n$ unknown or infinite), we\nachieve the optimal IFO complexity $O(\\epsilon^{-3/2})$. In both the cases, we\nmaintain the linear speedup achieved by existing methods. This is a massive\nimprovement over the $O(\\epsilon^{-2})$ IFO complexity of the existing\napproaches. Additionally, our algorithm is general enough to allow\nnon-identical distributions of data across workers, as in the recently proposed\nfederated learning paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:36:22 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 06:03:32 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Sharma", "Pranay", ""], ["Kafle", "Swatantra", ""], ["Khanduri", "Prashant", ""], ["Bulusu", "Saikiran", ""], ["Rajawat", "Ketan", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1912.06096", "submitter": "David Bermbach", "authors": "David Bermbach and Setareh Maghsudi and Jonathan Hasenburg and Tobias\n  Pfandzelter", "title": "Towards Auction-Based Function Placement in Serverless Fog Platforms", "comments": "preprint, authors' camera-ready version of a conference paper\n  accepted in the 2020 IEEE International Conference on Fog Computing (ICFC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Function-as-a-Service (FaaS) paradigm has a lot of potential as a\ncomputing model for fog environments comprising both cloud and edge nodes. When\nthe request rate exceeds capacity limits at the edge, some functions need to be\noffloaded from the edge towards the cloud.\n  In this position paper, we propose an auction-based approach in which\napplication developers bid on resources. This allows fog nodes to make a local\ndecision about which functions to offload while maximizing revenue. For a first\nevaluation of our approach, we use simulation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 17:49:27 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 14:51:30 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Bermbach", "David", ""], ["Maghsudi", "Setareh", ""], ["Hasenburg", "Jonathan", ""], ["Pfandzelter", "Tobias", ""]]}, {"id": "1912.06243", "submitter": "Minghui LiWang", "authors": "Minghui Liwang and Zhibin Gao and Seyyedali Hosseinalipour and Huaiyu\n  Dai", "title": "Multi-Task Offloading over Vehicular Clouds under Graph-based\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicular cloud computing has emerged as a promising paradigm for realizing\nuser requirements in computation-intensive tasks in modern driving\nenvironments. In this paper, a novel framework of multi-task offloading over\nvehicular clouds (VCs) is introduced where tasks and VCs are modeled as\nundirected weighted graphs. Aiming to achieve a trade-off between minimizing\ntask completion time and data exchange costs, task components are efficiently\nmapped to available virtual machines in the related VCs. The problem is\nformulated as a non-linear integer programming problem, mainly under\nconstraints of limited contact between vehicles as well as available resources,\nand addressed in low-traffic and rush-hour scenarios. In low-traffic cases, we\ndetermine optimal solutions; in rush-hour cases, a connection-restricted\nrandommatching-based subgraph isomorphism algorithm is proposed that presents\nlow computational complexity. Evaluations of the proposed algorithms against\ngreedy-based baseline methods are conducted via extensive simulations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 22:25:39 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Liwang", "Minghui", ""], ["Gao", "Zhibin", ""], ["Hosseinalipour", "Seyyedali", ""], ["Dai", "Huaiyu", ""]]}, {"id": "1912.06255", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Yan Gu, Julian Shun", "title": "Theoretically-Efficient and Practical Parallel DBSCAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DBSCAN method for spatial clustering has received significant attention\ndue to its applicability in a variety of data analysis tasks. There are fast\nsequential algorithms for DBSCAN in Euclidean space that take $O(n\\log n)$ work\nfor two dimensions, sub-quadratic work for three or more dimensions, and can be\ncomputed approximately in linear work for any constant number of dimensions.\nHowever, existing parallel DBSCAN algorithms require quadratic work in the\nworst case, making them inefficient for large datasets. This paper bridges the\ngap between theory and practice of parallel DBSCAN by presenting new parallel\nalgorithms for Euclidean exact DBSCAN and approximate DBSCAN that match the\nwork bounds of their sequential counterparts, and are highly parallel\n(polylogarithmic depth). We present implementations of our algorithms along\nwith optimizations that improve their practical performance. We perform a\ncomprehensive experimental evaluation of our algorithms on a variety of\ndatasets and parameter settings. Our experiments on a 36-core machine with\nhyper-threading show that we outperform existing parallel DBSCAN\nimplementations by up to several orders of magnitude, and achieve speedups by\nup to 33x over the best sequential algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:09:20 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 19:41:12 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 21:10:23 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 23:55:26 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wang", "Yiqiu", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "1912.06296", "submitter": "Shripad Gade", "authors": "Shripad Gade and Anna Winnicki and Subhonmesh Bose", "title": "On Privatizing Equilibrium Computation in Aggregate Games over Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed algorithm to compute an equilibrium in aggregate\ngames where players communicate over a fixed undirected network. Our algorithm\nexploits correlated perturbation to obfuscate information shared over the\nnetwork. We prove that our algorithm does not reveal private information of\nplayers to an honest-but-curious adversary who monitors several nodes in the\nnetwork. In contrast with differential privacy based algorithms, our method\ndoes not sacrifice accuracy of equilibrium computation to provide privacy\nguarantees.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 02:20:40 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Gade", "Shripad", ""], ["Winnicki", "Anna", ""], ["Bose", "Subhonmesh", ""]]}, {"id": "1912.06415", "submitter": "Sudhakar Singh", "authors": "Pankaj Singh, Sudhakar Singh, P. K. Mishra, Rakhi Garg", "title": "RDD-Eclat: Approaches to Parallelize Eclat Algorithm on Spark RDD\n  Framework", "comments": "16 pages, 6 figures, ICCNCT 2019", "journal-ref": "ICCNCT 2019, LNDECT 44", "doi": "10.1007/978-3-030-37051-0_85", "report-no": "ICCNCT-171", "categories": "cs.DC cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initially, a number of frequent itemset mining (FIM) algorithms have been\ndesigned on the Hadoop MapReduce, a distributed big data processing framework.\nBut, due to heavy disk I/O, MapReduce is found to be inefficient for such\nhighly iterative algorithms. Therefore, Spark, a more efficient distributed\ndata processing framework, has been developed with in-memory computation and\nresilient distributed dataset (RDD) features to support the iterative\nalgorithms. On the Spark RDD framework, Apriori and FP-Growth based FIM\nalgorithms have been designed, but Eclat-based algorithm has not been explored\nyet. In this paper, RDD-Eclat, a parallel Eclat algorithm on the Spark RDD\nframework is proposed with its five variants. The proposed algorithms are\nevaluated on the various benchmark datasets, which shows that RDD-Eclat\noutperforms the Spark-based Apriori by many times. Also, the experimental\nresults show the scalability of the proposed algorithms on increasing the\nnumber of cores and size of the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:23:47 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Singh", "Pankaj", ""], ["Singh", "Sudhakar", ""], ["Mishra", "P. K.", ""], ["Garg", "Rakhi", ""]]}, {"id": "1912.06469", "submitter": "Maria Salama", "authors": "Maria Salama, Rami Bahsoon, Rajkumar Buyya", "title": "Architectural Stability Reasoning using Self-Awareness Principles: Case\n  of Self-Adaptive Cloud Architectures", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.05517", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased dependence on software, there is a pressing need for\nengineering long-lived software. As architectures have a profound effect on the\nlife-span of the software and the provisioned quality of service, stable\narchitectures are significant assets. Architectural stability tends to reflect\nthe success of the system in supporting continuous changes without phasing-out.\nThe \\textit{behavioural} aspect of stability is essential for seamless\noperation, to continuously keep the provision of quality requirements stable\nand prevent architecture's drifting and phasing-out. In this paper, we present\na framework for reasoning about stability during runtime, leveraging on\nself-awareness principles. Specifically, we employ runtime goals for managing\nstability goals, online learning for reasoning about stability on the long-run,\nand stochastic games for managing associated trade-offs. We evaluate the\nproposed work using the case of cloud architectures for its highly dynamics\nduring runtime. The experimental results have shown the efficiency of\nself-awareness techniques in realising the expected behaviour stable during\nruntime operation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:44:47 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Salama", "Maria", ""], ["Bahsoon", "Rami", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1912.06474", "submitter": "Guillaume Gbikpi-Benissan", "authors": "Guillaume Gbikpi-Benissan, Patrick Callet, Frederic Magoules", "title": "Spectral domain decomposition method for physically-based rendering of\n  photochromic/electrochromic glass windows", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.05494", "journal-ref": null, "doi": "10.1109/DCABES.2014.27", "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the time consuming issues intrinsic to physically-based\nimage rendering algorithms. First, glass materials optical properties were\nmeasured on samples of real glasses and other objects materials inside an hotel\nroom were characterized by deducing spectral data from multiple trichromatic\nimages. We then present the rendering model and ray-tracing algorithm\nimplemented in Virtuelium, an open source software. In order to accelerate the\ncomputation of the interactions between light rays and objects, the ray-tracing\nalgorithm is parallelized by means of domain decomposition method techniques.\nNumerical experiments show that the speedups obtained with classical\nparallelization techniques are significantly less significant than those\nachieved with parallel domain decomposition methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:31:50 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Gbikpi-Benissan", "Guillaume", ""], ["Callet", "Patrick", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.06485", "submitter": "Hong-Ning Dai Prof.", "authors": "Zibin Zheng and Hong-Ning Dai and Jiajing Wu", "title": "Blockchain Intelligence: When Blockchain Meets Artificial Intelligence", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain is gaining extensive attention due to its provision of secure and\ndecentralized resource sharing manner. However, the incumbent blockchain\nsystems also suffer from a number of challenges in operational maintenance,\nquality assurance of smart contracts and malicious behaviour detection of\nblockchain data. The recent advances in artificial intelligence bring the\nopportunities in overcoming the above challenges. The integration of blockchain\nwith artificial intelligence can be beneficial to enhance current blockchain\nsystems. This article presents an introduction of the convergence of blockchain\nand artificial intelligence (namely blockchain intelligence). This article also\ngives a case study to further demonstrate the feasibility of blockchain\nintelligence and point out the future directions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 02:56:45 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 03:57:05 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 22:41:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zheng", "Zibin", ""], ["Dai", "Hong-Ning", ""], ["Wu", "Jiajing", ""]]}, {"id": "1912.06526", "submitter": "Johannes de Fine Licht", "authors": "Johannes de Fine Licht, Grzegorz Kwasniewski, Torsten Hoefler", "title": "Flexible Communication Avoiding Matrix Multiplication on FPGA with\n  High-Level Synthesis", "comments": null, "journal-ref": "In Proceedings of the 2020 ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays (FPGA'20), February 23-25, 2020, Seaside, CA,\n  USA", "doi": "10.1145/3373087.3375296", "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data movement is the dominating factor affecting performance and energy in\nmodern computing systems. Consequently, many algorithms have been developed to\nminimize the number of I/O operations for common computing patterns. Matrix\nmultiplication is no exception, and lower bounds have been proven and\nimplemented both for shared and distributed memory systems. Reconfigurable\nhardware platforms are a lucrative target for I/O minimizing algorithms, as\nthey offer full control of memory accesses to the programmer. While bounds\ndeveloped in the context of fixed architectures still apply to these platforms,\nthe spatially distributed nature of their computational and memory resources\nrequires a decentralized approach to optimize algorithms for maximum hardware\nutilization. We present a model to optimize matrix multiplication for FPGA\nplatforms, simultaneously targeting maximum performance and minimum off-chip\ndata movement, within constraints set by the hardware. We map the model to a\nconcrete architecture using a high-level synthesis tool, maintaining a high\nlevel of abstraction, allowing us to support arbitrary data types, and enables\nmaintainability and portability across FPGA devices. Kernels generated from our\narchitecture are shown to offer competitive performance in practice, scaling\nwith both compute and memory resources. We offer our design as an open source\nproject to encourage the open development of linear algebra and I/O minimizing\nalgorithms on reconfigurable hardware platforms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 14:28:42 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 13:39:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Licht", "Johannes de Fine", ""], ["Kwasniewski", "Grzegorz", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.06545", "submitter": "Steven Finch", "authors": "Steven Finch", "title": "Resolving Conflicts and Electing Leaders", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review distributed algorithms for transmitting data ($n$ real numbers)\nunder a broadcast communication model, as well as for maximum finding and for\nsorting. Our interest is in the basics of recursive formulas and corresponding\nasymptotics as ${n\\to\\infty}$. The emphasis is on concrete examples rather than\ngeneral theory.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 13:58:15 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Finch", "Steven", ""]]}, {"id": "1912.06547", "submitter": "Marc Armstrong", "authors": "Marc P. Armstrong", "title": "High Performance Computing for Geospatial Applications: A Prospective\n  View", "comments": "Forthcoming in W. Tang and S. Wang (eds.) High-Performance Computing\n  for Geospatial Applications, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pace of improvement in the performance of conventional computer hardware\nhas slowed significantly during the past decade, largely as a consequence of\nreaching the physical limits of manufacturing processes. To offset this\nslowdown, new approaches to HPC are now undergoing rapid development. This\nchapter describes current work on the development of cutting-edge exascale\ncomputing systems that are intended to be in place in 2021 and then turns to\naddress several other important developments in HPC, some of which are only in\nthe early stage of development. Domain-specific heterogeneous processing\napproaches use hardware that is tailored to specific problem types.\nNeuromorphic systems are designed to mimic brain function and are well suited\nto machine learning. And then there is quantum computing, which is the subject\nof some controversy despite the enormous funding initiatives that are in place\nto ensure that systems continue to scale-up from current small demonstration\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 20:05:26 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Armstrong", "Marc P.", ""]]}, {"id": "1912.06548", "submitter": "Marc Armstrong", "authors": "Marc P. Armstrong", "title": "High Performance Computing for Geospatial Applications: A Retrospective\n  View", "comments": "Forthcoming in W. Tang and S. Wang (eds.) High-Performance Computing\n  for Geospatial Applications, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many types of geospatial analyses are computationally complex, involving, for\nexample, solution processes that require numerous iterations or combinatorial\ncomparisons. This complexity has motivated the application of high performance\ncomputing (HPC) to a variety of geospatial problems. In many instances, HPC\nassumes even greater importance because complexity interacts with rapidly\ngrowing volumes of geospatial information to further impede analysis and\ndisplay. This chapter briefly reviews the underlying need for HPC in geospatial\napplications and describes different approaches to past implementations. Many\nof these applications were developed using hardware systems that had a\nrelatively short life-span and were implemented in software that was not easily\nportable. More promising recent approaches have turned to the use of\ndistributed resources that includes cyberinfrastructure as well as cloud and\nfog computing.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:28:17 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Armstrong", "Marc P.", ""]]}, {"id": "1912.06567", "submitter": "Eduard Gibert Renart", "authors": "Eduard Gibert Renart, Daniel Balouek-thomert and Manish Parashar", "title": "Challenges in designing edge-based middlewares for the Internet of\n  Things: A survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things paradigm connects edge devices via the Internet\nenabling them to be seamlessly integrated with a wide variety of applications.\nIn recent years, the number of connected devices has grown significantly, along\nwith the volume and variety of data that is being generated by these devices at\nthe edge of the network. An edge-based middleware is defined as a software that\nserves as an interface between the computational resources and the IoT devices,\nmaking communication possible among elements. Such middleware is required to\nprovide the necessary functional components for sensor registration, discovery,\nworkflow composition, and data pre-processing. In recent years, the landscape\nof the edge middleware platforms has grown exponentially, each of them with\ndifferent platform requirements, architectures, and features. The core of this\nsurvey is a comprehensive review of existing edge middleware solutions. In this\nregard, we propose a four-layer architecture for the design of edge-based\nmiddleware, along with some design goals for each of the proposed layer. The\npaper concludes with some open challenges and possible future research\ndirections.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 15:42:28 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Renart", "Eduard Gibert", ""], ["Balouek-thomert", "Daniel", ""], ["Parashar", "Manish", ""]]}, {"id": "1912.06776", "submitter": "Rusheng Zhang", "authors": "Rusheng Zhang, Baptiste Jacquemot, Kagan Bakirci, Sacha Bartholme,\n  Killian Kaempf, Baptiste Freydt, Loic Montandon, Shenqi Zhang, Ozan Tonguz", "title": "Leader selection in Vehicular Ad-hoc Networks: a Proactive Approach", "comments": "5 pages, 2 figures, 1 table, accepted by Vehicular Technology\n  Conference 2020 (VTC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advancement in vehicle-to-vehicle (V2V) communications, there\nare growing interests in related fields to leverage V2V communications for\ndifferent applications. A robust leader selection algorithm is required in a\nnumber of V2V communications related applications. In this paper, a distributed\nleader selection algorithm is introduced for vehicular ad-hoc network (VANET)\nover unreliable V2V communications. The algorithm is simple, light-weighted and\ntightly integrated with SAE 2735 protocol in vehicular networks. The goal of\nthe algorithm is to provide a robust leader selection method for a group of\nvehicles to determine a temporary leader locally in very short time, which\ncould benefit a lot of newly proposed applications based on V2V in traffic\ncontrol and autonomous driving related areas. Theoretical analysis as well as\nsimulation results has verified the convergence time, robustness and other\naspects of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 03:12:58 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 03:49:26 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Zhang", "Rusheng", ""], ["Jacquemot", "Baptiste", ""], ["Bakirci", "Kagan", ""], ["Bartholme", "Sacha", ""], ["Kaempf", "Killian", ""], ["Freydt", "Baptiste", ""], ["Montandon", "Loic", ""], ["Zhang", "Shenqi", ""], ["Tonguz", "Ozan", ""]]}, {"id": "1912.06802", "submitter": "Arindam Saha Dr.", "authors": "Arindam Saha, James A. R. Marshall and Andreagiovanni Reina", "title": "A memory and communication efficient algorithm for decentralized\n  counting of nodes in networks", "comments": "20 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node counting on a graph is subject to some fundamental theoretical\nlimitations, yet a solution to such problems is necessary in many applications\nof graph theory to real-world systems, such as collective robotics and\ndistributed sensor networks. Thus several stochastic and na{\\\"i}ve\ndeterministic algorithms for distributed graph size estimation or calculation\nhave been provided. Here we present a deterministic and distributed algorithm\nthat allows every node of a connected graph to determine the graph size in\nfinite time, if an upper bound on the graph size is provided. The algorithm\nconsists in the iterative aggregation of information in local hubs which then\nbroadcast it throughout the whole graph. The proposed node-counting algorithm\nis on average more efficient in terms of node memory and communication cost\nthan its previous deterministic counterpart for node counting, and appears\ncomparable or more efficient in terms of average-case time complexity. As well\nas node counting, the algorithm is more broadly applicable to problems such as\nsummation over graphs, quorum sensing, and spontaneous hierarchy creation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 08:12:44 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 21:28:48 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 08:30:22 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Saha", "Arindam", ""], ["Marshall", "James A. R.", ""], ["Reina", "Andreagiovanni", ""]]}, {"id": "1912.06912", "submitter": "Shahrzad Kianidehkordi", "authors": "Shahrzad Kiani, Nuwan Ferdinand, and Stark C. Draper", "title": "Hierarchical Coded Matrix Multiplication", "comments": null, "journal-ref": "IEEE Transactions on Information Theory, vol. 67, no. 2, pp.\n  726-754, Feb. 2021", "doi": "10.1109/TIT.2020.3036763", "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed computing systems slow working nodes, known as stragglers, can\ngreatly extend finishing times. Coded computing is a technique that enables\nstraggler-resistant computation. Most coded computing techniques presented to\ndate provide robustness by ensuring that the time to finish depends only on a\nset of the fastest nodes. However, while stragglers do compute less work than\nnon-stragglers, in real-world commercial cloud computing systems (e.g.,\nAmazon's Elastic Compute Cloud (EC2)) the distinction is often a soft one. In\nthis paper, we develop hierarchical coded computing that exploits the work\ncompleted by all nodes, both fast and slow, automatically integrating the\npotential contribution of each. We first present a conceptual framework to\nrepresent the division of work amongst nodes in coded matrix multiplication as\na cuboid partitioning problem. This framework allows us to unify existing\nmethods and motivates new techniques. We then develop three methods of\nhierarchical coded computing that we term bit-interleaved coded computation\n(BICC), multilevel coded computation (MLCC), and hybrid hierarchical coded\ncomputation (HHCC). In this paradigm, each worker is tasked with completing a\nsequence (a hierarchy) of ordered subtasks. The sequence of subtasks, and the\ncomplexity of each, is designed so that partial work completed by stragglers\ncan be used, rather than ignored. We note that our methods can be used in\nconjunction with any coded computing method. We illustrate this by showing how\nwe can use our methods to accelerate all previously developed coded computing\ntechniques by enabling them to exploit stragglers. Under a widely studied\nstatistical model of completion time, our approach realizes a $66\\%$\nimprovement in the expected finishing time. On Amazon EC2, the gain was $27\\%$\nwhen stragglers are simulated.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 19:43:21 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 19:26:24 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kiani", "Shahrzad", ""], ["Ferdinand", "Nuwan", ""], ["Draper", "Stark C.", ""]]}, {"id": "1912.07162", "submitter": "Sachini Jayasekara", "authors": "Sachini Jayasekara, Aaron Harwood, Shanika Karunasekera", "title": "Optimal Multi-Level Interval-based Checkpointing for Exascale Stream\n  Processing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art stream processing platforms make use of checkpointing to\nsupport fault tolerance, where a \"checkpoint tuple\" flows through the topology\nto all operators, indicating a checkpoint and triggering a checkpoint\noperation. The checkpoint will enable recovering from any kind of failure, be\nit as localized as a process fault or as wide spread as power supply loss to an\nentire rack of machines. As we move towards Exascale computing, it is becoming\nclear that this kind of \"single-level\" checkpointing is too inefficient to\nscale. Some HPC researchers are now investigating multi-level checkpointing,\nwhere checkpoint operations at each level are tailored to specific kinds of\nfailure to address the inefficiencies of single-level checkpointing.\nMulti-level checkpointing has been shown in practice to be superior, giving\ngreater efficiency in operation over single-level checkpointing. However, to\ndate there is no theoretical basis that provides optimal parameter settings for\nan interval-based coordinated multi-level checkpointing approach. This paper\npresents a theoretical framework for determining optimal parameter settings in\nan interval-based multi-level periodic checkpointing system, that is applicable\nto stream processing. Our approach is stochastic, where at a given checkpoint\ninterval, a level is selected with some probability for checkpointing. We\nderive the optimal checkpoint interval and associated optimal checkpoint\nprobabilities for a multi-level checkpointing system, that considers failure\nrates, checkpoint costs, restart costs and possible failure during restarting,\nat every level. We confirm our results with stochastic simulation and practical\nexperimentation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 02:31:50 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Jayasekara", "Sachini", ""], ["Harwood", "Aaron", ""], ["Karunasekera", "Shanika", ""]]}, {"id": "1912.07284", "submitter": "Kingshuk Majumder", "authors": "Kingshuk Majumder, Uday Bondhugula", "title": "A flexible FPGA accelerator for convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though CNNs are highly parallel workloads, in the absence of efficient\non-chip memory reuse techniques, an accelerator for them quickly becomes memory\nbound. In this paper, we propose a CNN accelerator design for inference that is\nable to exploit all forms of reuse available to minimize off-chip memory access\nwhile increasing utilization of available resources. The proposed design is\ncomposed of cores, each of which contains a one-dimensional array of processing\nelements. These cores can exploit different types of reuse available in CNN\nlayers of varying shapes without requiring any reconfiguration; in particular,\nour design minimizes underutilization due to problem sizes that are not perfect\nmultiples of the underlying hardware array dimensions. A major obstacle in the\nadoption of FPGAs as a platform for CNN inference is the difficulty to program\nthese devices using hardware description languages. Our end goal is to also\naddress this, and we develop preliminary software support via a codesign in\norder to leverage the accelerator through TensorFlow, a dominant high-level\nprogramming model. Our framework takes care of tiling and scheduling of neural\nnetwork layers and generates necessary low-level commands to execute the CNN.\nExperimental evaluation on a real system with a PCI-express based Xilinx VC709\nboard demonstrates the effectiveness of our approach. As a result of an\neffective interconnection, the design maintains a high frequency when we scale\nthe number of PEs. The sustained performance overall is a good fraction of the\naccelerator's theoretical peak performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 10:34:44 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 11:46:50 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Majumder", "Kingshuk", ""], ["Bondhugula", "Uday", ""]]}, {"id": "1912.07365", "submitter": "Ali Dorosty", "authors": "M. Ali Dorosty, Fathiyeh Faghih, Ehsan Khamespanah", "title": "Decentralized Runtime Verification for LTL Properties Using Global Clock", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime verification is the process of verifying critical behavioral\nproperties in big complex systems, where formal verification is not possible\ndue to state space explosion. There have been several attempts to design\nefficient algorithms for runtime verification. Most of these algorithms have a\nformally defined correctness property as a reference and check whether the\nsystem consistently meets the demands of the property or it fails to satisfy\nthe property at some point in runtime. LTL is a commonly used language for\ndefining these kinds of properties and is also the language of focus in this\npaper. One of the main target systems for runtime verification are distributed\nsystems, where the system consists of a number of processes connecting to each\nother using asynchronous message passing. There are two approaches for runtime\nverification in distributed systems. The first one consists of centralized\nalgorithms, where all processes send their events to a specific decision-making\nprocess, which keeps track of all the events to evaluate the specified\nproperty. The second approach consists of distributed algorithms, where\nprocesses check the specified property collaboratively. Centralized algorithms\nare simple, but usually involve sending a large number of messages to the\ndecision-making process. They also suffer from the problem of single point of\nfailure, as well as high traffic loads towards one process. Distributed\nalgorithms, on the other hand, are usually more complicated, but once\nimplemented, offer more efficiency. In this paper, we focus on a class of\nasynchronous distributed systems, where each process can change its own local\nstate at any arbitrary time and completely independent of others, while all\nprocesses share a global clock. We propose a sound and complete algorithm for\ndecentralized runtime verification of LTL properties in these systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 13:54:17 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 11:09:21 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 15:10:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Dorosty", "M. Ali", ""], ["Faghih", "Fathiyeh", ""], ["Khamespanah", "Ehsan", ""]]}, {"id": "1912.07479", "submitter": "Olasupo Ajayi", "authors": "Bagula Antoine, Tuyishimire Emmanuel, Ajayi Olasupo", "title": "Cyber Physical Systems (CPS) Surveillance Using An Epidemic Model", "comments": "25 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vast investments have recently been made worldwide in developing the\nCyber-Physical System (CPS) technology with the expectations of improving\neconomical and societal structures. However, great care must be paid to the\nCPS' complexity, the impact of emerging IoT (Internet of Things) protocols on\nthe CPS infrastructure as well as the impact of information dissemination by\nthese protocols on the safety of these infrastructures. This paper addresses\nthe issue of CPS safety by proposing and evaluating the performance of a CPS\nmanagement framework and the analysis of the dynamics of the underlining IoT\nnetwork in the cyber-space. The main contributions of this paper are in\nthreefold. Firstly, a new CPS framework is proposed; that: 1) builds around a\nlayered architecture to compartmentalise the CPS functionalities into different\nmodules for efficiency and scalability and 2) uses an inner feedback loop for\nthe efficient management of CPS infrastructure. Secondly, building upon this\nframework, a novel diffusion model that uses the epidemic (interference) sets\nto produce accurate diffusion patterns across the CPS IoT subsystem is\nproposed. Finally, the proposed diffusion model is numerically analysed to show\nhow it can be used to achieve efficient CPS surveillance in order to trigger\nreconfiguration to re-optimise the CPS when it is under stress. in IoT\nsettings. The numerical analysis of the diffusion model shows that interference\npropagates in pairwise disjoint sets, with IoT nodes migrating from\n\"susceptible\" to \"attacked\" statuses and finally reaching the \"removed\" state\nat a predictable time. Deployment considerations on some of the current social\nand public networks are also onsidered\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:22:48 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Antoine", "Bagula", ""], ["Emmanuel", "Tuyishimire", ""], ["Olasupo", "Ajayi", ""]]}, {"id": "1912.07860", "submitter": "Sicong Zhou", "authors": "Sicong Zhou, Huawei Huang, Wuhui Chen, Zibin Zheng, and Song Guo", "title": "PIRATE: A Blockchain-based Secure Framework of Distributed Machine\n  Learning in 5G Networks", "comments": "8 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fifth-generation (5G) networks and the beyond, communication latency\nand network bandwidth will be no more bottleneck to mobile users. Thus, almost\nevery mobile device can participate in the distributed learning. That is, the\navailability issue of distributed learning can be eliminated. However, the\nmodel safety will become a challenge. This is because the distributed learning\nsystem is prone to suffering from byzantine attacks during the stages of\nupdating model parameters and aggregating gradients amongst multiple learning\nparticipants. Therefore, to provide the byzantine-resilience for distributed\nlearning in 5G era, this article proposes a secure computing framework based on\nthe sharding-technique of blockchain, namely PIRATE. A case-study shows how the\nproposed PIRATE contributes to the distributed learning. Finally, we also\nenvision some open issues and challenges based on the proposed\nbyzantine-resilient learning framework.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 08:01:36 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Zhou", "Sicong", ""], ["Huang", "Huawei", ""], ["Chen", "Wuhui", ""], ["Zheng", "Zibin", ""], ["Guo", "Song", ""]]}, {"id": "1912.08063", "submitter": "Fantine Huot", "authors": "Fantine Huot, Yi-Fan Chen, Robert Clapp, Carlos Boneti and John\n  Anderson", "title": "High-resolution imaging on TPUs", "comments": "12 pages, 6 figures, submitted to ISC High Performance 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC physics.comp-ph physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid evolution of artificial intelligence (AI) is leading to a new\ngeneration of hardware accelerators optimized for deep learning. Some of the\ndesigns of these accelerators are general enough to allow their use for other\ncomputationally intensive tasks beyond AI. Cloud tensor processing units (TPUs)\nare one such example. Here, we demonstrate a novel approach using TensorFlow on\nCloud TPUs to implement a high-resolution imaging technique called\nfull-waveform inversion. Higher-order numerical stencils leverage the efficient\nmatrix multiplication offered by the Cloud TPU, and the halo exchange benefits\nfrom the dedicated high-speed interchip connection. The performance is\ncompetitive when compared with Tesla V100 graphics processing units and shows\npromise for future computation- and memory-intensive imaging applications.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 22:13:39 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Huot", "Fantine", ""], ["Chen", "Yi-Fan", ""], ["Clapp", "Robert", ""], ["Boneti", "Carlos", ""], ["Anderson", "John", ""]]}, {"id": "1912.08319", "submitter": "Ranesh Kumar Naha", "authors": "Ranesh Kumar Naha and Saurabh Garg", "title": "Multi-Criteria-based Dynamic User Behaviour Aware Resource Allocation in\n  Fog Computing", "comments": "31 Pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing is a promising computing paradigm in which IoT data can be\nprocessed near the edge to support time-sensitive applications. However, the\navailability of the resources in the computation device is not stable since\nthey may not be exclusively dedicated to the processing in the Fog environment.\nThis, combined with dynamic user behaviour, can affect the execution of\napplications. To address dynamic changes in user behaviour in a resource\nlimited Fog device, this paper proposes a Multi-Criteria-based resource\nallocation policy with resource reservation in order to minimise overall delay,\nprocessing time and SLA violation which considers Fog computing-related\ncharacteristics, such as device heterogeneity, resource constraint and\nmobility, as well as dynamic changes in user requirements. We employ multiple\nobjective functions to find appropriate resources for execution of\ntime-sensitive tasks in the Fog environment. Experimental results show that our\nproposed policy performs better than the existing one, reducing the total delay\nby 51%. The proposed algorithm also reduces processing time and SLA violation\nwhich is beneficial to run time-sensitive applications in the Fog environment.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:48:12 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Naha", "Ranesh Kumar", ""], ["Garg", "Saurabh", ""]]}, {"id": "1912.08392", "submitter": "Mutaz Barika", "authors": "Mutaz Barika, Saurabh Garg, Andrew Chan and Rodrigo N. Calheiros", "title": "Scheduling Algorithms for Efficient Execution of Stream Workflow\n  Applications in Multicloud Environments", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data processing applications are becoming more and more complex. They are\nno more monolithic in nature but instead they are composed of decoupled\nanalytical processes in the form of a workflow. One type of such workflow\napplications is stream workflow application, which integrates multiple\nstreaming big data applications to support decision making. Each analytical\ncomponent of these applications runs continuously and processes data streams\nwhose velocity will depend on several factors such as network bandwidth and\nprocessing rate of parent analytical component. As a consequence, the execution\nof these applications on cloud environments requires advanced scheduling\ntechniques that adhere to end user's requirements in terms of data processing\nand deadline for decision making. In this paper, we propose two Multicloud\nscheduling and resource allocation techniques for efficient execution of stream\nworkflow applications on Multicloud environments while adhering to workflow\napplication and user performance requirements and reducing execution cost.\nResults showed that the proposed genetic algorithm is an adequate and effective\nfor all experiments.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:48:19 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Barika", "Mutaz", ""], ["Garg", "Saurabh", ""], ["Chan", "Andrew", ""], ["Calheiros", "Rodrigo N.", ""]]}, {"id": "1912.08397", "submitter": "Mutaz Barika", "authors": "Mutaz Barika, Saurabh Garg and Rajiv Ranjan", "title": "Adaptive Scheduling for Efficient Execution of Dynamic Stream Workflows", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream workflow application such as online anomaly detection or online\ntraffic monitoring, integrates multiple streaming big data applications into\ndata analysis pipeline. This application can be highly dynamic in nature, where\nthe data velocity may change at runtime and therefore the resources should be\nmanaged overtime. To manage these changes, the orchestration of this\napplication requires a dynamic execution environment and dynamic scheduling\ntechnique. For the former requirement, Multicloud environment is a visible\nsolution to cope with the dynamic aspects of this workflow application. While\nfor the latter requirement, dynamic scheduling technique not only need to\nadhere to end user's requirements in terms of data processing and deadline for\ndecision making, and data stream sources location constraints, but also adjust\nprovisioning and scheduling plan at runtime to cope with dynamic variations of\nstream data rates. Therefore, we propose a two-phase adaptive scheduling\ntechnique to efficiently schedule dynamic workflow application in Multicloud\nenvironment that can respond to changes in the velocity of data at runtime. The\nexperimental results showed that the proposed technique is close to the lower\nbound and effective for different experiment scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:59:57 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Barika", "Mutaz", ""], ["Garg", "Saurabh", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1912.08453", "submitter": "Tahsin Reza", "authors": "Tahsin Reza, Hassan Halawa, Matei Ripeanu, Geoffrey Sanders, Roger\n  Pearce", "title": "Scalable Pattern Matching in Metadata Graphs via Constraint Checking", "comments": null, "journal-ref": "ACM Transactions on Parallel Computing (TOPC) 2020", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pattern matching is a fundamental tool for answering complex graph queries.\nUnfortunately, existing solutions have limited capabilities: they do not scale\nto process large graphs and/or support only a restricted set of search\ntemplates or usage scenarios. We present an algorithmic pipeline that bases\npattern matching on constraint checking. The key intuition is that each vertex\nor edge participating in a match has to meet a set of constrains implicitly\nspecified by the search template. The pipeline we propose, generates these\nconstraints and iterates over them to eliminate all the vertices and edges that\ndo not participate in any match, and reduces the background graph to a subgraph\nwhich is the union of all matches. Additional analysis can be performed on this\nannotated, reduced graph, such as full match enumeration. Furthermore, a\nvertex-centric formulation for constraint checking algorithms exists, and this\nmakes it possible to harness existing high-performance, vertex-centric graph\nprocessing frameworks. The key contribution of this work is a design following\nthe constraint checking approach for exact matching and its experimental\nevaluation. We show that the proposed technique: (i) enables highly scalable\npattern matching in labeled graphs, (ii) supports arbitrary patterns with 100%\nprecision, (iii) always selects all vertices and edges that participate in\nmatches, thus offering 100% recall, and (iv) supports a set of popular data\nanalysis scenarios. We implement our approach on top of HavoqGT, an open-source\nasynchronous graph processing framework, and demonstrate its advantages through\nstrong and weak scaling experiments on massive scale real-world (up to 257\nbillion edges) and synthetic (up to 4.4 trillion edges) labeled graphs\nrespectively, and at scales (1,024 nodes / 36,864 cores), orders of magnitude\nlarger than used in the past for similar problems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:54:32 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 03:50:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Reza", "Tahsin", ""], ["Halawa", "Hassan", ""], ["Ripeanu", "Matei", ""], ["Sanders", "Geoffrey", ""], ["Pearce", "Roger", ""]]}, {"id": "1912.08454", "submitter": "Yaxing Chen", "authors": "Yaxing Chen, Qinghua Zheng, Dan Liu, Zheng Yan, Wenhai Sun, Ning\n  Zhang, Wenjing Lou, Y. Thomas Hou", "title": "Enjoy the Untrusted Cloud: A Secure, Scalable and Efficient SQL-like\n  Query Framework for Outsourcing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the security of the cloud remains a concern, a common practice is to\nencrypt data before outsourcing them for utilization. One key challenging issue\nis how to efficiently perform queries over the ciphertext. Conventional\ncrypto-based solutions, e.g. partially/fully homomorphic encryption and\nsearchable encryption, suffer from low performance, poor expressiveness and\nweak compatibility. An alternative method that utilizes hardware-assisted\ntrusted execution environment, i.e., Intel SGX, has emerged recently. On one\nhand, such work lacks of supporting scalable access control over multiple data\nusers. On the other hand, existing solutions are subjected to the key\nrevocation problem and knowledge extractor vulnerability. In this work, we\nleverage the newly hardware-assisted methodology and propose a secure, scalable\nand efficient SQL-like query framework named QShield. Building upon Intel SGX,\nQShield can guarantee the confidentiality and integrity of sensitive data when\nbeing processed on an untrusted cloud platform. Moreover, we present a novel\nlightweight secret sharing method to enable multi-user access control in\nQShield, while tackling the key revocation problem. Furthermore, with an\nadditional trust proof mechanism, QShield guarantees the correctness of queries\nand significantly alleviates the possibility to build a knowledge extractor. We\nimplemented a prototype for QShield and show that QShield incurs minimum\nperformance cost.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:54:51 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Chen", "Yaxing", ""], ["Zheng", "Qinghua", ""], ["Liu", "Dan", ""], ["Yan", "Zheng", ""], ["Sun", "Wenhai", ""], ["Zhang", "Ning", ""], ["Lou", "Wenjing", ""], ["Hou", "Y. Thomas", ""]]}, {"id": "1912.08464", "submitter": "Pedro Martinez", "authors": "Jan Ciesko, Pedro J. Mart\\'inez-Ferrer, Ra\\'ul Pe\\~nacoba Veigas,\n  Xavier Teruel, Vicen\\c{c} Beltran", "title": "HDOT -- an Approach Towards Productive Programming of Hybrid\n  Applications", "comments": "Accepted manuscript", "journal-ref": "Journal of Parallel and Distributed Computing 137 (2020) 104-118", "doi": "10.1016/j.jpdc.2019.11.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MPI applications matter. However, with the advent of many-core processors,\ntraditional MPI applications are challenged to achieve satisfactory\nperformance. This is due to the inability of these applications to respond to\nload imbalances, to reduce serialization imposed by synchronous communication\npatterns, to overlap communication with computation and finally to deal with\nincreasing memory overheads. The MPI specification provides asynchronous calls\nto mitigate some of these factors. However, application developers rarely make\nthe effort to apply them efficiently. In this work, we present a methodology to\ndevelop hybrid applications called Hierarchical Domain Over-decomposition with\nTasking (HDOT), that reduces programming effort by emphasizing the reuse of\ndata partition schemes from process-level and applying them on task-level,\nallowing a top-down approach to express concurrency and allowing a natural\ncoexistence between MPI and shared-memory programming models. Our integration\nof MPI and OmpSs-2 shows promising results in terms of programmability and\nperformance measured on a set of applications.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:09:19 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 13:23:41 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ciesko", "Jan", ""], ["Mart\u00ednez-Ferrer", "Pedro J.", ""], ["Veigas", "Ra\u00fal Pe\u00f1acoba", ""], ["Teruel", "Xavier", ""], ["Beltran", "Vicen\u00e7", ""]]}, {"id": "1912.08530", "submitter": "Alexander Willner", "authors": "Andrea Hamm, Alexander Willner, Ina Schieferdecker", "title": "Edge Computing: A Comprehensive Survey of Current Initiatives and a\n  Roadmap for a Sustainable Edge Computing Development", "comments": "15 pages", "journal-ref": "15th International Conference on Wirtschaftsinformatik (2020)", "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge Computing is a new distributed Cloud Computing paradigm in which\ncomputing and storage capabilities are pushed to the topological edge of a\nnetwork. However, various standards and implementations are promoted by\ndifferent initiatives. Lead by a reference architecture model for Edge\nComputing, current initiatives are analyzed by explorative content analysis.\nProviding two main contributions to the field, we present, first, how current\ninitiatives are characterized, and second, a roadmap for sustainable Edge\nComputing relating three dimensions of sustainable development to four\ncross-concerns of Edge Computing. Findings show that most initiatives are\ninternationally organized software development projects; important branches are\ncurrently telecom and industrial sectors; most addressed is the network\nvirtualization layer. The roadmap reveals numerous chances and risks of Edge\nComputing related to sustainable development; such as the use of renewable\nenergies, biases, new business models, increase and decrease of energy\nconsumption, responsiveness, monitoring and traceability.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:26:22 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Hamm", "Andrea", ""], ["Willner", "Alexander", ""], ["Schieferdecker", "Ina", ""]]}, {"id": "1912.08810", "submitter": "Alexandros Nikolaos Ziogas", "authors": "Alexandros Nikolaos Ziogas, Tal Ben-Nun, Guillermo Indalecio\n  Fern\\'andez, Timo Schneider, Mathieu Luisier, Torsten Hoefler", "title": "Optimizing the Data Movement in Quantum Transport Simulations via\n  Data-Centric Parallel Programming", "comments": "12 pages, 18 figures, SC19", "journal-ref": null, "doi": "10.1145/3295500.3356200", "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing efficient cooling systems for integrated circuits (ICs) relies on a\ndeep understanding of the electro-thermal properties of transistors. To shed\nlight on this issue in currently fabricated FinFETs, a quantum mechanical\nsolver capable of revealing atomically-resolved electron and phonon transport\nphenomena from first-principles is required. In this paper, we consider a\nglobal, data-centric view of a state-of-the-art quantum transport simulator to\noptimize its execution on supercomputers. The approach yields coarse- and\nfine-grained data-movement characteristics, which are used for performance and\ncommunication modeling, communication-avoidance, and data-layout\ntransformations. The transformations are tuned for the Piz Daint and Summit\nsupercomputers, where each platform requires different caching and fusion\nstrategies to perform optimally. The presented results make ab initio device\nsimulation enter a new era, where nanostructures composed of over 10,000 atoms\ncan be investigated at an unprecedented level of accuracy, paving the way for\nbetter heat management in next-generation ICs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:47:39 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ziogas", "Alexandros Nikolaos", ""], ["Ben-Nun", "Tal", ""], ["Fern\u00e1ndez", "Guillermo Indalecio", ""], ["Schneider", "Timo", ""], ["Luisier", "Mathieu", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.08950", "submitter": "Maciej Besta", "authors": "Maciej Besta, Simon Weber, Lukas Gianinazzi, Robert Gerstenberger,\n  Andrey Ivanov, Yishai Oltchik, Torsten Hoefler", "title": "Slim Graph: Practical Lossy Graph Compression for Approximate Graph\n  Processing, Storage, and Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Slim Graph: the first programming model and framework for\npractical lossy graph compression that facilitates high-performance approximate\ngraph processing, storage, and analytics. Slim Graph enables the developer to\nexpress numerous compression schemes using small and programmable compression\nkernels that can access and modify local parts of input graphs. Such kernels\nare executed in parallel by the underlying engine, isolating developers from\ncomplexities of parallel programming. Our kernels implement novel graph\ncompression schemes that preserve numerous graph properties, for example\nconnected components, minimum spanning trees, or graph spectra. Finally, Slim\nGraph uses statistical divergences and other metrics to analyze the accuracy of\nlossy graph compression. We illustrate both theoretically and empirically that\nSlim Graph accelerates numerous graph algorithms, reduces storage used by graph\ndatasets, and ensures high accuracy of results. Slim Graph may become the\ncommon ground for developing, executing, and analyzing emerging lossy graph\ncompression schemes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:48:29 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Besta", "Maciej", ""], ["Weber", "Simon", ""], ["Gianinazzi", "Lukas", ""], ["Gerstenberger", "Robert", ""], ["Ivanov", "Andrey", ""], ["Oltchik", "Yishai", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.09022", "submitter": "Akito Suzuki", "authors": "Akito Suzuki, Ryoichi Kawahara, Masahiro Kobayashi, Shigeaki Harada,\n  Yousuke Takahashi and Keisuke Ishibashi", "title": "Extendable NFV-Integrated Control Method Using Reinforcement Learning", "comments": "17 pages, 8 figures, 7 tables, accepted for publication in IEICE\n  Transactions on Communications. copyright(c) 2019 IEICE,\n  https://search.ieice.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network functions virtualization (NFV) enables telecommunications service\nproviders to realize various network services by flexibly combining multiple\nvirtual network functions (VNFs). To provide such services, an NFV control\nmethod should optimally allocate such VNFs into physical networks and servers\nby taking account of the combination(s) of objective functions and constraints\nfor each metric defined for each VNF type, e.g., VNF placements and routes\nbetween the VNFs. The NFV control method should also be extendable for adding\nnew metrics or changing the combination of metrics. One approach for NFV\ncontrol to optimize allocations is to construct an algorithm that\nsimultaneously solves the combined optimization problem. However, this approach\nis not extendable because the problem needs to be reformulated every time a new\nmetric is added or a combination of metrics is changed. Another approach\ninvolves using an extendable network-control architecture that coordinates\nmultiple control algorithms specified for individual metrics. However, to the\nbest of our knowledge, no method has been developed that can optimize\nallocations through this kind of coordination. In this paper, we propose an\nextendable NFV-integrated control method by coordinating multiple control\nalgorithms. We also propose an efficient coordination algorithm based on\nreinforcement learning. Finally, we evaluate the effectiveness of the proposed\nmethod through simulations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 05:23:40 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Suzuki", "Akito", ""], ["Kawahara", "Ryoichi", ""], ["Kobayashi", "Masahiro", ""], ["Harada", "Shigeaki", ""], ["Takahashi", "Yousuke", ""], ["Ishibashi", "Keisuke", ""]]}, {"id": "1912.09083", "submitter": "Anton Akusok", "authors": "Anton Akusok, Kaj-Mikael Bj\\\"ork, Leonardo Espinosa Leal, Yoan Miche,\n  Renjie Hu and Amaury Lendasse", "title": "Spiking Networks for Improved Cognitive Abilities of Edge Computing\n  Devices", "comments": null, "journal-ref": "Proceedings of the 12th ACM International Conference on PErvasive\n  Technologies Related to Assistive Environments (PETRA '19). ACM, New York,\n  NY, USA, 307-308. 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This concept paper highlights a recently opened opportunity for large scale\nanalytical algorithms to be trained directly on edge devices. Such approach is\na response to the arising need of processing data generated by natural person\n(a human being), also known as personal data. Spiking Neural networks are the\ncore method behind it: suitable for a low latency energy-constrained hardware,\nenabling local training or re-training, while not taking advantage of\nscalability available in the Cloud.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:36:00 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Akusok", "Anton", ""], ["Bj\u00f6rk", "Kaj-Mikael", ""], ["Leal", "Leonardo Espinosa", ""], ["Miche", "Yoan", ""], ["Hu", "Renjie", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1912.09088", "submitter": "Ben Blamey", "authors": "Ben Blamey, Ida-Maria Sintorn, Andreas Hellander, Salman Toor", "title": "Resource- and Message Size-Aware Scheduling of Stream Processing at the\n  Edge with application to Realtime Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst computational resources at the cloud edge can be leveraged to improve\nlatency and reduce the costs of cloud services for a wide variety mobile, web,\nand IoT applications; such resources are naturally constrained. For distributed\nstream processing applications, there are clear advantages to offloading some\nprocessing work to the cloud edge. Many state of the art stream processing\napplications such as Flink and Spark Streaming, being designed to run\nexclusively in the cloud, are a poor fit for such hybrid edge/cloud deployment\nsettings, not least because their schedulers take limited consideration of the\nheterogeneous hardware in such deployments. In particular, their schedulers\nbroadly assume a homogeneous network topology (aside from data locality\nconsideration in, e.g., HDFS/Spark). Specialized stream processing frameworks\nintended for such hybrid deployment scenarios, especially IoT applications,\nallow developers to manually allocate specific operators in the pipeline to\nnodes at the cloud edge. In this paper, we investigate scheduling stream\nprocessing in hybrid cloud/edge deployment settings with sensitivity to CPU\ncosts and message size, with the aim of maximizing throughput with respect to\nlimited edge resources. We demonstrate real-time edge processing of a stream of\nelectron microscopy images, and measure a consistent reduction in end-to-end\nlatency under our approach versus a resource-agnostic baseline scheduler, under\nbenchmarking.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:44:18 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Blamey", "Ben", ""], ["Sintorn", "Ida-Maria", ""], ["Hellander", "Andreas", ""], ["Toor", "Salman", ""]]}, {"id": "1912.09170", "submitter": "Bertrand Simon", "authors": "Bertrand Simon, Joachim Falk, Nicole Megow, and J\\\"urgen Teich", "title": "Energy Minimization in DAG Scheduling on MPSoCs at Run-Time: Theory and\n  Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static (offline) techniques for mapping applications given by task graphs to\nMPSoC systems often deliver overly pessimistic and thus suboptimal results\nw.r.t. exploiting time slack in order to minimize the energy consumption. This\nholds true in particular in case computation times of tasks may be\nworkload-dependent and becoming known only at runtime or in case of\nconditionally executed tasks or scenarios. This paper studies and\nquantitatively evaluates different classes of algorithms for scheduling\nperiodic applications given by task graphs (i.e., DAGs) with precedence\nconstraints and a global deadline on homogeneous MPSoCs purely at runtime on a\nper-instance base. We present and analyze algorithms providing provably optimal\nresults as well as approximation algorithms with proven guarantees on the\nachieved energy savings. For problem instances taken from realistic embedded\nsystem benchmarks as well as synthetic scalable problems, we provide results on\nthe computation time and quality of each algorithm to perform a) scheduling and\nb) voltage/speed assignments for each task at runtime. In our portfolio, we\ndistinguish as well continuous and discrete speed (e.g., DVFS-related)\nassignment problems. In summary, the presented ties between theory (algorithmic\ncomplexity and optimality) and execution time analysis deliver important\ninsights on the practical usability of the presented algorithms for runtime\noptimization of task scheduling and speed assignment on MPSoCs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 13:04:53 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Simon", "Bertrand", ""], ["Falk", "Joachim", ""], ["Megow", "Nicole", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "1912.09230", "submitter": "Markus Levonyak", "authors": "Markus Levonyak, Christina Pacher, Wilfried N. Gansterer", "title": "Scalable Resilience Against Node Failures for Communication-Hiding\n  Preconditioned Conjugate Gradient and Conjugate Residual Methods", "comments": "12 pages, 2 figures, 2 tables", "journal-ref": "Proceedings of the 2020 SIAM Conference on Parallel Processing for\n  Scientific Computing (2020) 81-92", "doi": "10.1137/1.9781611976137.8", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The observed and expected continued growth in the number of nodes in\nlarge-scale parallel computers gives rise to two major challenges: global\ncommunication operations are becoming major bottlenecks due to their limited\nscalability, and the likelihood of node failures is increasing. We study an\napproach for addressing these challenges in the context of solving large sparse\nlinear systems. In particular, we focus on the pipelined preconditioned\nconjugate gradient (PPCG) method, which has been shown to successfully deal\nwith the first of these challenges. In this paper, we address the second\nchallenge. We present extensions to the PPCG solver and two of its variants\nwhich make them resilient against the failure of a compute node while fully\npreserving their communication-hiding properties and thus their scalability.\nThe basic idea is to efficiently communicate a few redundant copies of local\nvector elements to neighboring nodes with very little overhead. In case a node\nfails, these redundant copies are gathered at a replacement node, which can\nthen accurately reconstruct the lost parts of the solver's state. After that,\nthe parallel solver can continue as in the failure-free scenario. Experimental\nevaluations of our approach illustrate on average very low runtime overheads\ncompared to the standard non-resilient algorithms. This shows that scalable\nalgorithmic resilience can be achieved at low extra cost.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:41:05 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Levonyak", "Markus", ""], ["Pacher", "Christina", ""], ["Gansterer", "Wilfried N.", ""]]}, {"id": "1912.09256", "submitter": "Alexandru Uta", "authors": "Alexandru Uta, Alexandru Custura, Dmitry Duplyakin, Ivo Jimenez, Jan\n  Rellermeyer, Carlos Maltzahn, Robert Ricci, Alexandru Iosup", "title": "Is Big Data Performance Reproducible in Modern Cloud Networks?", "comments": "12 pages paper, 3 pages references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance variability has been acknowledged as a problem for over a decade\nby cloud practitioners and performance engineers. Yet, our survey of top\nsystems conferences reveals that the research community regularly disregards\nvariability when running experiments in the cloud. Focusing on networks, we\nassess the impact of variability on cloud-based big-data workloads by gathering\ntraces from mainstream commercial clouds and private research clouds. Our data\ncollection consists of millions of datapoints gathered while transferring over\n9 petabytes of data. We characterize the network variability present in our\ndata and show that, even though commercial cloud providers implement mechanisms\nfor quality-of-service enforcement, variability still occurs, and is even\nexacerbated by such mechanisms and service provider policies. We show how\nbig-data workloads suffer from significant slowdowns and lack predictability\nand replicability, even when state-of-the-art experimentation techniques are\nused. We provide guidelines for practitioners to reduce the volatility of big\ndata performance, making experiments more repeatable.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:05:30 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Uta", "Alexandru", ""], ["Custura", "Alexandru", ""], ["Duplyakin", "Dmitry", ""], ["Jimenez", "Ivo", ""], ["Rellermeyer", "Jan", ""], ["Maltzahn", "Carlos", ""], ["Ricci", "Robert", ""], ["Iosup", "Alexandru", ""]]}, {"id": "1912.09268", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Xiaowen Chu, Bo Li", "title": "MG-WFBP: Merging Gradients Wisely for Efficient Communication in\n  Distributed Deep Learning", "comments": "Accepted by IEEE TPDS. 15 pages. arXiv admin note: substantial text\n  overlap with arXiv:1811.11141", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed synchronous stochastic gradient descent has been widely used to\ntrain deep neural networks (DNNs) on computer clusters. With the increase of\ncomputational power, network communications generally limit the system\nscalability. Wait-free backpropagation (WFBP) is a popular solution to overlap\ncommunications with computations during the training process. In this paper, we\nobserve that many DNNs have a large number of layers with only a small amount\nof data to be communicated at each layer in distributed training, which could\nmake WFBP inefficient. Based on the fact that merging some short communication\ntasks into a single one can reduce the overall communication time, we formulate\nan optimization problem to minimize the training time in pipelining\ncommunications and computations. We derive an optimal solution that can be\nsolved efficiently without affecting the training performance. We then apply\nthe solution to propose a distributed training algorithm named merged-gradient\nWFBP (MG-WFBP) and implement it in two platforms Caffe and PyTorch. Extensive\nexperiments in three GPU clusters are conducted to verify the effectiveness of\nMG-WFBP. We further exploit trace-based simulations of 4 to 2048 GPUs to\nexplore the potential scaling efficiency of MG-WFBP. Experimental results show\nthat MG-WFBP achieves much better scaling performance than existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 02:59:14 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 11:59:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Shi", "Shaohuai", ""], ["Chu", "Xiaowen", ""], ["Li", "Bo", ""]]}, {"id": "1912.09500", "submitter": "Hannah Atmer", "authors": "Hannah Atmer, Kilian Rausch, Daniel McNally", "title": "Front-Running Protection for Distributed Exchanges using\n  Tamper-Resistant Round Trip Time Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present ODIN, a front-running protection system that uses a\nnovel algorithm to measure Round-Trip-Time (RTT) to untrusted servers. ODIN is\nthe decentralized equivalent of THOR, a RTT-aware front-running protection\nsystem for trading on centralized exchanges. Unlike centralized exchanges, P2P\nexchanges have potentially malicious peers which makes reliable direct RTT\nmeasurement impossible. In order to prevent tampering by an arbitrarily\nmalicious peer, ODIN performs an indirect RTT measurement that never interacts\ndirectly with the target machine. The RTT to the target is estimated by\nmeasuring the RTT to a randomized IP address that is known to be close to the\ntarget's IP address in the global routing network. We find that ODIN's RTT\nestimation algorithm provides an accurate, practical, and generic solution for\ncollecting network latency data in a hostile network environment.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:06:57 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 18:02:10 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 21:08:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Atmer", "Hannah", ""], ["Rausch", "Kilian", ""], ["McNally", "Daniel", ""]]}, {"id": "1912.09512", "submitter": "Chien-Sheng Yang", "authors": "Chien-Sheng Yang, Ramtin Pedarsani and A. Salman Avestimehr", "title": "Edge Computing in the Dark: Leveraging Contextual-Combinatorial Bandit\n  and Coded Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advancements in edge computing capabilities, there has been a\nsignificant increase in utilizing the edge cloud for event-driven and\ntime-sensitive computations. However, large-scale edge computing networks can\nsuffer substantially from unpredictable and unreliable computing resources\nwhich can result in high variability of service quality. Thus, it is crucial to\ndesign efficient task scheduling policies that guarantee quality of service and\nthe timeliness of computation queries. In this paper, we study the problem of\ncomputation offloading over unknown edge cloud networks with a sequence of\ntimely computation jobs. Motivated by the MapReduce computation paradigm, we\nassume each computation job can be partitioned to smaller Map functions that\nare processed at the edge, and the Reduce function is computed at the user\nafter the Map results are collected from the edge nodes. We model the service\nquality (success probability of returning result back to the user within\ndeadline) of each edge device as function of context (collection of factors\nthat affect edge devices). The user decides the computations to offload to each\ndevice with the goal of receiving a recoverable set of computation results in\nthe given deadline. Our goal is to design an efficient edge computing policy in\nthe dark without the knowledge of the context or computation capabilities of\neach device. By leveraging the \\emph{coded computing} framework in order to\ntackle failures or stragglers in computation, we formulate this problem using\ncontextual-combinatorial multi-armed bandits (CC-MAB), and aim to maximize the\ncumulative expected reward. We propose an online learning policy called\n\\emph{online coded edge computing policy}, which provably achieves\nasymptotically-optimal performance in terms of regret loss compared with the\noptimal offline policy for the proposed CC-MAB problem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:27:15 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 14:34:45 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yang", "Chien-Sheng", ""], ["Pedarsani", "Ramtin", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1912.09528", "submitter": "Nirupam Gupta", "authors": "Nirupam Gupta and Nitin H. Vaidya", "title": "Randomized Reactive Redundancy for Byzantine Fault-Tolerance in\n  Parallelized Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report considers the problem of Byzantine fault-tolerance in synchronous\nparallelized learning that is founded on the parallelized stochastic gradient\ndescent (parallelized-SGD) algorithm. The system comprises a master, and $n$\nworkers, where up to $f$ of the workers are Byzantine faulty. Byzantine workers\nneed not follow the master's instructions correctly, and might send malicious\nincorrect (or faulty) information. The identity of the Byzantine workers\nremains fixed throughout the learning process, and is unknown a priori to the\nmaster. We propose two coding schemes, a deterministic scheme and a randomized\nscheme, for guaranteeing exact fault-tolerance if $2f < n$. The coding schemes\nuse the concept of reactive redundancy for isolating Byzantine workers that\neventually send faulty information. We note that the computation efficiencies\nof the schemes compare favorably with other (deterministic or randomized)\ncoding schemes, for exact fault-tolerance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:15:28 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Gupta", "Nirupam", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1912.09536", "submitter": "Subru Krishnan", "authors": "Fotis Psallidas, Yiwen Zhu, Bojan Karlas, Matteo Interlandi, Avrilia\n  Floratou, Konstantinos Karanasos, Wentao Wu, Ce Zhang, Subru Krishnan, Carlo\n  Curino, Markus Weimer", "title": "Data Science through the looking glass and what we found there", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent success of machine learning (ML) has led to an explosive growth\nboth in terms of new systems and algorithms built in industry and academia, and\nnew applications built by an ever-growing community of data science (DS)\npractitioners. This quickly shifting panorama of technologies and applications\nis challenging for builders and practitioners alike to follow. In this paper,\nwe set out to capture this panorama through a wide-angle lens, by performing\nthe largest analysis of DS projects to date, focusing on questions that can\nhelp determine investments on either side. Specifically, we download and\nanalyze: (a) over 6M Python notebooks publicly available on GITHUB, (b) over 2M\nenterprise DS pipelines developed within COMPANYX, and (c) the source code and\nmetadata of over 900 releases from 12 important DS libraries. The analysis we\nperform ranges from coarse-grained statistical characterizations to analysis of\nlibrary imports, pipelines, and comparative studies across datasets and time.\nWe report a large number of measurements for our readers to interpret, and dare\nto draw a few (actionable, yet subjective) conclusions on (a) what systems\nbuilders should focus on to better serve practitioners, and (b) what\ntechnologies should practitioners bet on given current trends. We plan to\nautomate this analysis and release associated tools and results periodically.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:29:44 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Psallidas", "Fotis", ""], ["Zhu", "Yiwen", ""], ["Karlas", "Bojan", ""], ["Interlandi", "Matteo", ""], ["Floratou", "Avrilia", ""], ["Karanasos", "Konstantinos", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""], ["Krishnan", "Subru", ""], ["Curino", "Carlo", ""], ["Weimer", "Markus", ""]]}, {"id": "1912.09601", "submitter": "Walter Mayor", "authors": "Walter M. Mayor Toro, Juan C. Perafan Villota, Oscar H. Mondragon,\n  Johan S. Obando Ceron", "title": "Divide and Conquer: an Accurate Machine Learning Algorithm to Process\n  Split Videos on a Parallel Processing Infrastructure", "comments": "3 pages, 2 figures, LatinX in AI Research at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every day the number of traffic cameras in cities rapidly increase and huge\namount of video data are generated. Parallel processing infrastruture, such as\nHadoop, and programming models, such as MapReduce, are being used to promptly\nprocess that amount of data. The common approach for video processing by using\nHadoop MapReduce is to process an entire video on only one node, however, in\norder to avoid parallelization problems, such as load imbalance, we propose to\nprocess videos by splitting it into equal parts and processing each resulting\nchunk on a different node. We used some machine learning techniques to detect\nand track the vehicles. However, video division may produce inaccurate results.\nTo solve this problem we proposed a heuristic algorithm to avoid process a\nvehicle in more than one chunk.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:51:32 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Toro", "Walter M. Mayor", ""], ["Villota", "Juan C. Perafan", ""], ["Mondragon", "Oscar H.", ""], ["Ceron", "Johan S. Obando", ""]]}, {"id": "1912.09623", "submitter": "Rui Duan", "authors": "Rui Duan, Yang Ning, Yong Chen", "title": "Heterogeneity-aware and communication-efficient distributed statistical\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multicenter research, individual-level data are often protected against\nsharing across sites. To overcome the barrier of data sharing, many distributed\nalgorithms, which only require sharing aggregated information, have been\ndeveloped. The existing distributed algorithms usually assume the data are\nhomogeneously distributed across sites. This assumption ignores the important\nfact that the data collected at different sites may come from various\nsub-populations and environments, which can lead to heterogeneity in the\ndistribution of the data. Ignoring the heterogeneity may lead to erroneous\nstatistical inference. In this paper, we propose distributed algorithms which\naccount for the heterogeneous distributions by allowing site-specific nuisance\nparameters. The proposed methods extend the surrogate likelihood approach to\nthe heterogeneous setting by applying a novel density ratio tilting method to\nthe efficient score function. The proposed algorithms maintain the same\ncommunication cost as the existing communication-efficient algorithms. We\nestablish a non-asymptotic risk bound for the proposed distributed estimator\nand its limiting distribution in the two-index asymptotic setting which allows\nboth sample size per site and the number of sites to go to infinity. In\naddition, we show that the asymptotic variance of the estimator attains the\nCram\\'er-Rao lower bound when the number of sites is in rate smaller than the\nsample size at each site. Finally, we use simulation studies and a real data\napplication to demonstrate the validity and feasibility of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:14:07 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 21:01:58 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Duan", "Rui", ""], ["Ning", "Yang", ""], ["Chen", "Yong", ""]]}, {"id": "1912.09644", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao", "title": "The Blockchain Game: Synthesis of Byzantine Systems and Nash Equilibria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper presents a synthesis viewpoint of blockchains from two\northogonal perspectives: fault-tolerant distributed systems and game theory.\nSpecifically, we formulate a new game-theoretical problem in the context of\nblockchains and sketch a closed-form Nash equilibrium to the problem.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 05:14:20 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Zhao", "Dongfang", ""]]}, {"id": "1912.09722", "submitter": "Shujie Han", "authors": "Shujie Han, Jun Wu, Erci Xu, Cheng He, Patrick P. C. Lee, Yi Qiang,\n  Qixing Zheng, Tao Huang, Zixi Huang, Rui Li", "title": "Robust Data Preprocessing for Machine-Learning-Based Disk Failure\n  Prediction in Cloud Production Environments", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide proactive fault tolerance for modern cloud data centers, extensive\nstudies have proposed machine learning (ML) approaches to predict imminent disk\nfailures for early remedy and evaluated their approaches directly on public\ndatasets (e.g., Backblaze SMART logs). However, in real-world production\nenvironments, the data quality is imperfect (e.g., inaccurate labeling, missing\ndata samples, and complex failure types), thereby degrading the prediction\naccuracy. We present RODMAN, a robust data preprocessing pipeline that refines\ndata samples before feeding them into ML models. We start with a large-scale\ntrace-driven study of over three million disks from Alibaba Cloud's data\ncenters, and motivate the practical challenges in ML-based disk failure\nprediction. We then design RODMAN with three data preprocessing echniques,\nnamely failure-type filtering, spline-based data filling, and automated\npre-failure backtracking, that are applicable for general ML models. Evaluation\non both the Alibaba and Backblaze datasets shows that RODMAN improves the\nprediction accuracy compared to without data preprocessing under various\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 09:43:25 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Han", "Shujie", ""], ["Wu", "Jun", ""], ["Xu", "Erci", ""], ["He", "Cheng", ""], ["Lee", "Patrick P. C.", ""], ["Qiang", "Yi", ""], ["Zheng", "Qixing", ""], ["Huang", "Tao", ""], ["Huang", "Zixi", ""], ["Li", "Rui", ""]]}, {"id": "1912.09747", "submitter": "Malte Sandstede", "authors": "Malte Sandstede", "title": "Online Analysis of Distributed Dataflows with Timely Dataflow", "comments": "92 pages, 44 figures, contact and further information: visit\n  https://www.maltesandstede.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ST2, an end-to-end solution to analyze distributed dataflows in an\nonline setting. It is powered by Timely Dataflow, a low-latency, distributed\ndata-parallel dataflow computational framework, and expands on its predecessor\nSnailTrail 1, a system to run online critical path analysis on program activity\ngraphs derived from dataflow execution traces. ST2 connects to a running Timely\ncomputation, creates the program activity graph representation, and runs\nmultiple analyses on top of it. Analyses include aggregate metrics, progress\nand temporal invariant checking, and graph pattern matching. Through a\ncommand-line interface and a real-time dashboard, users are able to interact\nwith and visualize ST2's analysis results.\n  For ST2's implementation, we discuss Differential Dataflow, a framework that\nuses differential computation to incrementalize even complex relational\ndataflow operators, as an alternative to Timely Dataflow, but ultimately settle\non using Timely. In our performance evaluations, we are able to show that ST2\nis able to comfortably keep up with common streaming computations in offline\nand online settings, even exceeding SnailTrail 1's performance. We also\nshowcase and evaluate ST2 from a functional standpoint in a case study. Using\nthe dashboard to profile a faulty source computation, we manage to successfully\ndetect the issues' root cause. We argue that ST2 is an extendable system that\npaves the way for users to debug, monitor, and optimize online distributed\ndataflows.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 10:42:18 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Sandstede", "Malte", ""]]}, {"id": "1912.09765", "submitter": "Swanand Kadhe", "authors": "Mehmet Fatih Aktas, Swanand Kadhe, Emina Soljanin, Alex Sprintson", "title": "Download Time Analysis for Distributed Storage Codes with Locality and\n  Availability", "comments": "Accepted for a publication in IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper presents techniques for analyzing the expected download time in\ndistributed storage systems that employ systematic availability codes. These\ncodes provide access to hot data through the systematic server containing the\nobject and multiple recovery groups. When a request for an object is received,\nit can be replicated (forked) to the systematic server and all recovery groups.\nWe first consider the low-traffic regime and present the close-form expression\nfor the download time. By comparison across systems with availability, maximum\ndistance separable (MDS), and replication codes, we demonstrate that\navailability codes can reduce download time in some settings but are not always\noptimal. In the high-traffic regime, the system consists of multiple\ninter-dependent Fork-Join queues, making exact analysis intractable.\nAccordingly, we present upper and lower bounds on the download time, and an\nM/G/1 queue approximation for several cases of interest. Via extensive\nnumerical simulations, we evaluate our bounds and demonstrate that the M/G/1\nqueue approximation has a high degree of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:24:30 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 04:39:41 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 17:31:01 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 18:29:44 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Kadhe", "Swanand", ""], ["Soljanin", "Emina", ""], ["Sprintson", "Alex", ""]]}, {"id": "1912.09789", "submitter": "Jan S. Rellermeyer", "authors": "Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen\n  Kloppenburg, Tim Verbelen and Jan S. Rellermeyer", "title": "A Survey on Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for artificial intelligence has grown significantly over the last\ndecade and this growth has been fueled by advances in machine learning\ntechniques and the ability to leverage hardware acceleration. However, in order\nto increase the quality of predictions and render machine learning solutions\nfeasible for more complex applications, a substantial amount of training data\nis required. Although small machine learning models can be trained with modest\namounts of data, the input for training larger models such as neural networks\ngrows exponentially with the number of parameters. Since the demand for\nprocessing training data has outpaced the increase in computation power of\ncomputing machinery, there is a need for distributing the machine learning\nworkload across multiple machines, and turning the centralized into a\ndistributed system. These distributed systems present new challenges, first and\nforemost the efficient parallelization of the training process and the creation\nof a coherent model. This article provides an extensive overview of the current\nstate-of-the-art in the field by outlining the challenges and opportunities of\ndistributed machine learning over conventional (centralized) machine learning,\ndiscussing the techniques used for distributed machine learning, and providing\nan overview of the systems that are available.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 12:24:25 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Verbraeken", "Joost", ""], ["Wolting", "Matthijs", ""], ["Katzy", "Jonathan", ""], ["Kloppenburg", "Jeroen", ""], ["Verbelen", "Tim", ""], ["Rellermeyer", "Jan S.", ""]]}, {"id": "1912.09844", "submitter": "Rajiv Nishtala Dr", "authors": "Rajiv Nishtala (1) and Vinicius Petrucci (2) and Paul Carpenter (3)\n  and Xavier Martorell (3) ((1) Norwegian University of Science and Technology,\n  (2) Federal University of Bahia and University of Pittsburgh (3) Barcelona\n  Supercomputing center)", "title": "Hurry-up: Scaling Web Search on Big/Little Multi-core Architectures", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous multi-core systems such as big/little architectures have been\nintroduced as an attractive server design option with the potential to improve\nperformance under power constraints in data centres. Since both big\nhigh-performing and little power-efficient cores can run on the same system\nsharing the workload processing, thread mapping/scheduling turns out to be much\nmore challenging. This is particularly hard when considering the different\ntrade-offs shaped by the heterogeneous cores on the quality-of-service\n(expressed as tail latency) experienced by user-facing applications, such as\nWeb Search.\n  In this work, we present Hurry-up, a runtime thread mapping solution designed\nto select individual requests to run on the most appropriate heterogeneous\ncores to improve tail latency. Hurry-up accelerates compute-intensive requests\non big cores, while letting less intensive threads to execute on little cores.\nWe implement and deploy Hurry-up on a real 64-bit big/little architecture (ARM\nJuno), and show that, compared to a conservative policy on Linux, Hurry-up\nreduces the server tail latency by 39.5% (mean).\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:24:38 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Nishtala", "Rajiv", ""], ["Petrucci", "Vinicius", ""], ["Carpenter", "Paul", ""], ["Martorell", "Xavier", ""]]}, {"id": "1912.09925", "submitter": "Peter Richt\\'arik", "authors": "S\\'elim Chraibi and Ahmed Khaled and Dmitry Kovalev and Peter\n  Richt\\'arik and Adil Salim and Martin Tak\\'a\\v{c}", "title": "Distributed Fixed Point Methods with Compressed Iterates", "comments": "15 pages, 4 algorithms, 4 Theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose basic and natural assumptions under which iterative optimization\nmethods with compressed iterates can be analyzed. This problem is motivated by\nthe practice of federated learning, where a large model stored in the cloud is\ncompressed before it is sent to a mobile device, which then proceeds with\ntraining based on local data. We develop standard and variance reduced methods,\nand establish communication complexity bounds. Our algorithms are the first\ndistributed methods with compressed iterates, and the first fixed point methods\nwith compressed iterates.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:36:03 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Chraibi", "S\u00e9lim", ""], ["Khaled", "Ahmed", ""], ["Kovalev", "Dmitry", ""], ["Richt\u00e1rik", "Peter", ""], ["Salim", "Adil", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1912.10024", "submitter": "Alexandros Nikolaos Ziogas", "authors": "Alexandros Nikolaos Ziogas, Tal Ben-Nun, Guillermo Indalecio\n  Fern\\'andez, Timo Schneider, Mathieu Luisier, Torsten Hoefler", "title": "A Data-Centric Approach to Extreme-Scale Ab initio Dissipative Quantum\n  Transport Simulations", "comments": "13 pages, 13 figures, SC19", "journal-ref": null, "doi": "10.1145/3295500.3357156", "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational efficiency of a state of the art ab initio quantum\ntransport (QT) solver, capable of revealing the coupled electro-thermal\nproperties of atomically-resolved nano-transistors, has been improved by up to\ntwo orders of magnitude through a data centric reorganization of the\napplication. The approach yields coarse-and fine-grained data-movement\ncharacteristics that can be used for performance and communication modeling,\ncommunication-avoidance, and dataflow transformations. The resulting code has\nbeen tuned for two top-6 hybrid supercomputers, reaching a sustained\nperformance of 85.45 Pflop/s on 4,560 nodes of Summit (42.55% of the peak) in\ndouble precision, and 90.89 Pflop/s in mixed precision. These computational\nachievements enable the restructured QT simulator to treat realistic\nnanoelectronic devices made of more than 10,000 atoms within a 14$\\times$\nshorter duration than the original code needs to handle a system with 1,000\natoms, on the same number of CPUs/GPUs and with the same physical accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:38:44 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ziogas", "Alexandros Nikolaos", ""], ["Ben-Nun", "Tal", ""], ["Fern\u00e1ndez", "Guillermo Indalecio", ""], ["Schneider", "Timo", ""], ["Luisier", "Mathieu", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.10344", "submitter": "Lu Xu", "authors": "Lu Xu and Yating Wang", "title": "XCloud: Design and Implementation of AI Cloud Platform with RESTful API\n  Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, artificial intelligence (AI) has aroused much attention\namong both industrial and academic areas. However, building and maintaining\nefficient AI systems are quite difficult for many small business companies and\nresearchers if they are not familiar with machine learning and AI. In this\npaper, we first evaluate the difficulties and challenges in building AI\nsystems. Then an cloud platform termed XCloud, which provides several common AI\nservices in form of RESTful APIs, is constructed. Technical details are\ndiscussed in Section 2. This project is released as open-source software and\ncan be easily accessed for late research. Code is available at\nhttps://github.com/lucasxlu/XCloud.git.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 15:09:43 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Xu", "Lu", ""], ["Wang", "Yating", ""]]}, {"id": "1912.10357", "submitter": "Yu Chen", "authors": "Ronghua Xu, Yu Chen", "title": "Microchain: a Light Hierarchical Consensus Protocol for IoT System", "comments": "chapter contributed to \"Blockchain Consensus Algorithms\", edited by\n  D. Hyland-Wood, S. Khatchadourian, and S. Johnson, to be published by\n  Springer Nature in 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the large-scale Internet of Things (IoT) makes many new applications\nfeasible, like Smart Cities, IoT also brings new concerns on data reliability,\nsecurity, and privacy. The rapid evolution in blockchain technologies, which\nrelied on a decentralized, immutable and distributed ledger system for\ntransaction data auditing, provides a prospective solution to address the\nissues in IoT. The blockchain and smart contract enabled security mechanism for\nIoT applications have attracted increasing interests from both academia and\nindustry. However, integrating cryptocurrency-oriented blockchain technologies\ninto IoT systems meets tremendous challenges on scalability, storage capacity,\nsecurity, and privacy. Particularly, the performance of blockchain networks\nsignificantly relies on the performance of consensus mechanisms, e.g., in terms\nof data confidentiality, transaction throughput, and network scalability. In\nthis chapter, given an in-depth review of state-of-the-art blockchain networks,\nthe key matrix of designing consensus mechanism for IoT networks are identified\nin terms of throughput, scalability, and security. To demonstrate a case study\non designing scalable, lightweight blockchain protocols for IoT systems, a\nMicrochain framework is introduced and a proof-of-concept prototype is\nimplemented in a physical network environment. The experimental results verify\nthe feasibility of integrating the Microchain into IoT systems.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 23:39:00 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Xu", "Ronghua", ""], ["Chen", "Yu", ""]]}, {"id": "1912.10367", "submitter": "Vincent Gramoli", "authors": "Gauthier Voron and Vincent Gramoli", "title": "Dispel: Byzantine SMR with Distributed Pipelining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Byzantine State Machine Replication (SMR) is a long studied topic that\nreceived increasing attention recently with the advent of blockchains as\ncompanies are trying to scale them to hundreds of nodes. Byzantine SMRs try to\nincrease throughput by either reducing the latency of consensus instances that\nthey run sequentially or by reducing the number of replicas that send messages\nto others in order to reduce the network usage. Unfortunately, the former\napproach makes use of resources in burst whereas the latter requires\nCPU-intensive authentication mechanisms.\n  In this paper, we propose a new Byzantine SMR called Dispel (Distributed\nPipeline) that allows any node to distributively start new consensus instances\nwhenever they detect sufficient resources locally. We evaluate the performance\nof Dispel within a single datacenter and across up to 380 machines over 3\ncontinents by comparing it against four other SMRs. On 128 nodes, Dispel speeds\nup HotStuff, the Byzantine fault tolerant SMR being integrated within\nFacebook's blockchain, by more than 12 times. In addition, we also test Dispel\nunder isolated and correlated failures and show that the Dispel distributed\ndesign is more robust than HotStuff. Finally, we evaluate Dispel in a\ncryptocurrency application with Bitcoin transactions and show that this SMR is\nnot the bottleneck.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 00:58:54 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 09:57:12 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Voron", "Gauthier", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1912.10370", "submitter": "Hong-Ning Dai Prof.", "authors": "Zibin Zheng, Shaoan Xie, Hong-Ning Dai, Weili Chen, Xiangping Chen,\n  Jian Weng, Muhammad Imran", "title": "An Overview on Smart Contracts: Challenges, Advances and Platforms", "comments": "19 pages, 8 figures", "journal-ref": "Future Generation Computer Systems, 2019", "doi": "10.1016/j.future.2019.12.019", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Smart contract technology is reshaping conventional industry and business\nprocesses. Being embedded in blockchains, smart contracts enable the\ncontractual terms of an agreement to be enforced automatically without the\nintervention of a trusted third party. As a result, smart contracts can cut\ndown administration and save services costs, improve the efficiency of business\nprocesses and reduce the risks. Although smart contracts are promising to drive\nthe new wave of innovation in business processes, there are a number of\nchallenges to be tackled.This paper presents a survey on smart contracts. We\nfirst introduce blockchains and smart contracts. We then present the challenges\nin smart contracts as well as recent technical advances. We also compare\ntypical smart contract platforms and give a categorization of smart contract\napplications along with some representative examples.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 01:52:54 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zheng", "Zibin", ""], ["Xie", "Shaoan", ""], ["Dai", "Hong-Ning", ""], ["Chen", "Weili", ""], ["Chen", "Xiangping", ""], ["Weng", "Jian", ""], ["Imran", "Muhammad", ""]]}, {"id": "1912.10596", "submitter": "Vahid Noormofidi", "authors": "Vahid Noormofidi, Susan R. Atlas, Huaiyu Duan", "title": "Simulating collective neutrinos oscillations on the Intel Many\n  Integrated Core (MIC) architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the second-generation Intel Xeon Phi coprocessor based on the\nIntel Many Integrated Core (MIC) architecture, aka the Knights Landing or KNL,\nfor simulating neutrino oscillations in (core-collapse) supernovae. For this\npurpose we have developed a numerical code XFLAT which is optimized for the MIC\narchitecture and which can run on both the homogeneous HPC platform with CPUs\nor Xeon Phis only and the hybrid platform with both CPUs and Xeon Phis. To\nefficiently utilize the SIMD (vector) units of the MIC architecture we\nimplemented a design of Structure of Array (SoA) in the low-level module of the\ncode. We benchmarked the code on the NERSC Cori supercomputer which is equipped\nwith dual 68-core 7250 Xeon Phis. We find that compare to the first generation\nof the Xeon Phi (Knights Corner a.k.a KNC) the performance improves by many\nfolds. Some of the problems that we encountered in this work may be solved with\nthe advent of the new supernova model for neutrino oscillations and the\nnext-generation Xeon Phi.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 02:58:45 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Noormofidi", "Vahid", ""], ["Atlas", "Susan R.", ""], ["Duan", "Huaiyu", ""]]}, {"id": "1912.10643", "submitter": "Pradipta Ghosh", "authors": "Pradipta Ghosh, Quynh Nguyen, Pranav K Sakulkar, Aleksandra Knezevic,\n  Jason A. Tran, Jiatong Wang, Zhifeng Lin, Bhaskar Krishnamachari, Murali\n  Annavaram, and Salman Avestimehr", "title": "Jupiter: A Networked Computing Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Internet of Things, there is an increasing demand for networked\ncomputing to support the requirements of the time-constrained,\ncompute-intensive distributed applications such as multi-camera video\nprocessing and data fusion for security. We present Jupiter, an open source\nnetworked computing system that inputs a Directed Acyclic Graph (DAG)-based\ncomputational task graph to efficiently distribute the tasks among a set of\nnetworked compute nodes regardless of their geographical separations and\norchestrates the execution of the DAG thereafter. This Kubernetes\ncontainer-orchestration-based system supports both centralized and\ndecentralized scheduling algorithms for optimally mapping the tasks based on\ninformation from a range of profilers: network profilers, resource profilers,\nand execution time profilers. While centralized scheduling algorithms with\nglobal knowledge have been popular among the grid/cloud computing community, we\nargue that a distributed scheduling approach is better suited for networked\ncomputing due to lower communication and computation overhead in the face of\nnetwork dynamics. To this end, we propose and implement a new class of\ndistributed scheduling algorithms called WAVE on the Jupiter system. We present\na set of real world experiments on two separate testbeds - one a world-wide\nnetwork of 90 cloud computers across 8 cities and the other a cluster of 30\nRaspberry pi nodes, over a simple networked computing application called\nDistributed Network Anomaly Detector (DNAD). We show that despite using more\nlocalized knowledge, a distributed WAVE greedy algorithm can achieve similar\nperformance as a classical centralized scheduling algorithm called\nHeterogeneous Earliest Finish Time (HEFT), suitably enhanced for the Jupiter\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 06:39:18 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ghosh", "Pradipta", ""], ["Nguyen", "Quynh", ""], ["Sakulkar", "Pranav K", ""], ["Knezevic", "Aleksandra", ""], ["Tran", "Jason A.", ""], ["Wang", "Jiatong", ""], ["Lin", "Zhifeng", ""], ["Krishnamachari", "Bhaskar", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "1912.10808", "submitter": "Mingxi Cheng", "authors": "Mingxi Cheng, Ji Li, Paul Bogdan, Shahin Nazarian", "title": "H2O-Cloud: A Resource and Quality of Service-Aware Task Scheduling\n  Framework for Warehouse-Scale Data Centers -- A Hierarchical Hybrid DRL (Deep\n  Reinforcement Learning) based Approach", "comments": "12 pages, 5 figures. IEEE Transactions on Computer-Aided Design of\n  Integrated Circuits and Systems. 2019 Jul 23", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has attracted both end-users and Cloud Service Providers\n(CSPs) in recent years. Improving resource utilization rate (RUtR), such as CPU\nand memory usages on servers, while maintaining Quality-of-Service (QoS) is one\nkey challenge faced by CSPs with warehouse-scale data centers. Prior works\nproposed various algorithms to reduce energy cost or to improve RUtR, which\neither lack the fine-grained task scheduling capabilities, or fail to take a\ncomprehensive system model into consideration. This article presents H2O-Cloud,\na Hierarchical and Hybrid Online task scheduling framework for warehouse-scale\nCSPs, to improve resource usage effectiveness while maintaining QoS. H2O-Cloud\nis highly scalable and considers comprehensive information such as various\nworkload scenarios, cloud platform configurations, user request information and\ndynamic pricing model. The hierarchy and hybridity of the framework, combined\nwith its deep reinforcement learning (DRL) engines, enable H2O-Cloud to\nefficiently start on-the-go scheduling and learning in an unpredictable\nenvironment without pre-training. Our experiments confirm the high efficiency\nof the proposed H2O-Cloud when compared to baseline approaches, in terms of\nenergy and cost while maintaining QoS. Compared with a state-of-the-art\nDRL-based algorithm, H2O-Cloud achieves up to 201.17% energy cost efficiency\nimprovement, 47.88% energy efficiency improvement and 551.76% reward rate\nimprovement.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:18:54 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 07:04:49 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 01:38:04 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Cheng", "Mingxi", ""], ["Li", "Ji", ""], ["Bogdan", "Paul", ""], ["Nazarian", "Shahin", ""]]}, {"id": "1912.10821", "submitter": "Manoj Muniswamaiah", "authors": "Manoj Muniswamaiah, Tilak Agerwala and Charles Tappert", "title": "Big Data in Cloud Computing Review and Opportunities", "comments": null, "journal-ref": null, "doi": "10.5121/ijcsit.2019.11404", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data is used in decision making process to gain useful insights hidden in\nthe data for business and engineering. At the same time it presents challenges\nin processing, cloud computing has helped in advancement of big data by\nproviding computational, networking and storage capacity. This paper presents\nthe review, opportunities and challenges of transforming big data using cloud\ncomputing resources.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 17:23:36 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Muniswamaiah", "Manoj", ""], ["Agerwala", "Tilak", ""], ["Tappert", "Charles", ""]]}, {"id": "1912.10823", "submitter": "Luca Piccolboni", "authors": "Luca Piccolboni, Paolo Mantovani, Giuseppe Di Guglielmo, Luca P.\n  Carloni", "title": "COSMOS: Coordination of High-Level Synthesis and Memory Optimization for\n  Hardware Accelerators", "comments": "Published in ACM Transactions on Embedded Computing Systems (TECS)", "journal-ref": "ACM Trans. Embed. Comput. Syst. 16, 5s, Article 150 (October 2017)", "doi": "10.1145/3126566", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware accelerators are key to the efficiency and performance of\nsystem-on-chip (SoC) architectures. With high-level synthesis (HLS), designers\ncan easily obtain several performance-cost trade-off implementations for each\ncomponent of a complex hardware accelerator. However, navigating this design\nspace in search of the Pareto-optimal implementations at the system level is a\nhard optimization task. We present COSMOS, an automatic methodology for the\ndesign-space exploration (DSE) of complex accelerators, that coordinates both\nHLS and memory optimization tools in a compositional way. First, thanks to the\nco-design of datapath and memory, COSMOS produces a large set of Pareto-optimal\nimplementations for each component of the accelerator. Then, COSMOS leverages\ncompositional design techniques to quickly converge to the desired trade-off\npoint between cost and performance at the system level. When applied to the\nsystem-level design (SLD) of an accelerator for wide-area motion imagery\n(WAMI), COSMOS explores the design space as completely as an exhaustive search,\nbut it reduces the number of invocations to the HLS tool by up to 14.6x.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:18:00 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Piccolboni", "Luca", ""], ["Mantovani", "Paolo", ""], ["Di Guglielmo", "Giuseppe", ""], ["Carloni", "Luca P.", ""]]}, {"id": "1912.10959", "submitter": "Waqar Ali", "authors": "Waqar Ali and Rodolfo Pellizzoni and Heechul Yun", "title": "Virtual Gang based Scheduling of Real-Time Tasks on Multicore Platforms", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a virtual-gang based parallel real-time task scheduling approach\nfor multicore platforms. Our approach is based on the notion of a virtual-gang,\nwhich is a group of parallel real-time tasks that are statically linked and\nscheduled together by a gang scheduler. We present a light-weight intra-gang\nsynchronization framework, called RTG-Sync, and virtual gang formation\nalgorithms that provide strong temporal isolation and high real-time\nschedulability in scheduling real-time tasks on multicore. We evaluate our\napproach both analytically, with generated tasksets against state-of-the-art\napproaches, and empirically with a case-study involving real-world workloads on\na real embedded multicore platform. The results show that our approach provides\nsimple but powerful compositional analysis framework, achieves better analytic\nschedulability, especially when the effect of interference is considered, and\nis a practical solution for COTS multicore platforms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 16:36:23 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 19:48:02 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Ali", "Waqar", ""], ["Pellizzoni", "Rodolfo", ""], ["Yun", "Heechul", ""]]}, {"id": "1912.11153", "submitter": "Luca Piccolboni", "authors": "Luca Piccolboni, Giuseppe Di Guglielmo, Luca P. Carloni", "title": "PAGURUS: Low-Overhead Dynamic Information Flow Tracking on Loosely\n  Coupled Accelerators", "comments": "Published in IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems (TCAD)", "journal-ref": null, "doi": "10.1109/TCAD.2018.2857321", "report-no": "IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. Volume 37\n  Number 11 (November 2018)", "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-based attacks exploit bugs or vulnerabilities to get unauthorized\naccess or leak confidential information. Dynamic information flow tracking\n(DIFT) is a security technique to track spurious information flows and provide\nstrong security guarantees against such attacks. To secure heterogeneous\nsystems, the spurious information flows must be tracked through all their\ncomponents, including processors, accelerators (i.e., application-specific\nhardware components) and memories. We present PAGURUS, a flexible methodology\nto design a low-overhead shell circuit that adds DIFT support to accelerators.\nThe shell uses a coarse-grain DIFT approach, thus not requiring to make\nmodifications to the accelerator's implementation. We analyze the performance\nand area overhead of the DIFT shell on FPGAs and we propose a metric, called\ninformation leakage, to measure its security guarantees. We perform a\ndesign-space exploration to show that we can synthesize accelerators with\ndifferent characteristics in terms of performance, cost and security\nguarantees. We also present a case study where we use the DIFT shell to secure\nan accelerator running on a embedded platform with a DIFT-enhanced RISC-V core.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:28:15 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Piccolboni", "Luca", ""], ["Di Guglielmo", "Giuseppe", ""], ["Carloni", "Luca P.", ""]]}, {"id": "1912.11401", "submitter": "Vidal Attias", "authors": "Vidal Attias, Luigi Vigneri, Vassil Dimitrov", "title": "On the Decentralized Generation of theRSA Moduli in Multi-Party Settings", "comments": "The submission contains 14 pages and 12 figures. The conference to\n  submit is not determined yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RSA cryptography is still widely used. Some of its applications (e.g.,\ndistributed signature schemes, cryptosystems) do not allow the RSA modulus to\nbe generated by a centralized trusted entity. Instead, the factorization must\nremain unknown to all the network participants. To this date, the existing\nalgorithms are either computationally expensive, or limited to two-party\nsettings. In this work, we design a decentralized multi-party computation\nalgorithm able to generate efficiently the RSA modulus.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 14:57:43 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Attias", "Vidal", ""], ["Vigneri", "Luigi", ""], ["Dimitrov", "Vassil", ""]]}, {"id": "1912.11417", "submitter": "Sergio Sainz", "authors": "Sergio Sainz-Palacios", "title": "Flat combined Red Black Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flat combining is a concurrency threaded technique whereby one thread\nperforms all the operations in batch by scanning a queue of operations\nto-be-done and performing them together. Flat combining makes sense as long as\nk operations each taking O(n) separately can be batched together and done in\nless than O(k*n). Red black tree is a balanced binary search tree with\npermanent balancing warranties. Operations in red black tree are hard to batch\ntogether: for example inserting nodes in two different branches of the tree\naffect different areas of the tree. In this paper we investigate alternatives\nto making a flat combine approach work for red black trees.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 15:39:46 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Sainz-Palacios", "Sergio", ""]]}, {"id": "1912.11456", "submitter": "Andrew Sutton", "authors": "Grant Chung, Luc Desrosiers, Manav Gupta, Andrew Sutton, Kaushik\n  Venkatadri, Ontak Wong, and Goran Zugic", "title": "Performance Tuning and Scaling Enterprise Blockchain Applications", "comments": "49 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain scalability can be complicated and costly. As enterprises begin to\nadopt blockchain technology to solve business problems, there are valid\nconcerns if blockchain applications can support the transactional demands of\nproduction systems. In fact, the multiple distributed components and protocols\nthat underlie blockchain applications makes performance optimization a\nnon-trivial task. Blockchain performance optimization and scalability require a\nmethodology to reduce complexity and cost. Furthermore, existing performance\nresults often lack the requirements, load, and infrastructure of a production\napplication. In this paper, we first develop a methodical approach to\nperformance tuning enterprise blockchain applications to increase performance\nand transaction capacity. The methodology is applied to an enterprise\nblockchain-based application (leveraging Hyperledger Fabric) for performance\ntuning and optimization with the goal of bridging the gap between laboratory\nand production deployed system performance. We then present extensive results\nand analysis of our performance testing for on-premise and cloud deployments,\nin which we were able to scale the application from 30 to 3000 TPS without\nforking the Hyperledger Fabric source code and maintaining a reasonable\ninfrastructure footprint. We also provide blockchain application and platform\nrecommendations for performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:22:21 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Chung", "Grant", ""], ["Desrosiers", "Luc", ""], ["Gupta", "Manav", ""], ["Sutton", "Andrew", ""], ["Venkatadri", "Kaushik", ""], ["Wong", "Ontak", ""], ["Zugic", "Goran", ""]]}, {"id": "1912.11477", "submitter": "Shizhan Lu", "authors": "Shizhan Lu", "title": "Self-adaption grey DBSCAN clustering", "comments": "8 pages, 4 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:1906.11416", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering analysis, a classical issue in data mining, is widely used in\nvarious research areas. This article aims at proposing a self-adaption grey\nDBSCAN clustering (SAG-DBSCAN) algorithm. First, the grey relational matrix is\nused to obtain the grey local density indicator, and then this indicator is\napplied to make self-adapting noise identification for obtaining a dense subset\nof clustering dataset, finally, the DBSCAN which automatically selects\nparameters is utilized to cluster the dense subset. Several frequently-used\ndatasets were used to demonstrate the performance and effectiveness of the\nproposed clustering algorithm and to compare the results with those of other\nstate-of-the-art algorithms. The comprehensive comparisons indicate that our\nmethod has advantages over other compared methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 02:46:15 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lu", "Shizhan", ""]]}, {"id": "1912.11516", "submitter": "Aayush Ankit", "authors": "Aayush Ankit, Izzat El Hajj, Sai Rahul Chalamalasetti, Sapan Agarwal,\n  Matthew Marinella, Martin Foltin, John Paul Strachan, Dejan Milojicic,\n  Wen-mei Hwu, Kaushik Roy", "title": "PANTHER: A Programmable Architecture for Neural Network Training\n  Harnessing Energy-efficient ReRAM", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.ET eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide adoption of deep neural networks has been accompanied by\never-increasing energy and performance demands due to the expensive nature of\ntraining them. Numerous special-purpose architectures have been proposed to\naccelerate training: both digital and hybrid digital-analog using resistive RAM\n(ReRAM) crossbars. ReRAM-based accelerators have demonstrated the effectiveness\nof ReRAM crossbars at performing matrix-vector multiplication operations that\nare prevalent in training. However, they still suffer from inefficiency due to\nthe use of serial reads and writes for performing the weight gradient and\nupdate step. A few works have demonstrated the possibility of performing outer\nproducts in crossbars, which can be used to realize the weight gradient and\nupdate step without the use of serial reads and writes. However, these works\nhave been limited to low precision operations which are not sufficient for\ntypical training workloads. Moreover, they have been confined to a limited set\nof training algorithms for fully-connected layers only. To address these\nlimitations, we propose a bit-slicing technique for enhancing the precision of\nReRAM-based outer products, which is substantially different from bit-slicing\nfor matrix-vector multiplication only. We incorporate this technique into a\ncrossbar architecture with three variants catered to different training\nalgorithms. To evaluate our design on different types of layers in neural\nnetworks (fully-connected, convolutional, etc.) and training algorithms, we\ndevelop PANTHER, an ISA-programmable training accelerator with compiler\nsupport. Our evaluation shows that PANTHER achieves up to $8.02\\times$,\n$54.21\\times$, and $103\\times$ energy reductions as well as $7.16\\times$,\n$4.02\\times$, and $16\\times$ execution time reductions compared to digital\naccelerators, ReRAM-based accelerators, and GPUs, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 20:24:32 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ankit", "Aayush", ""], ["Hajj", "Izzat El", ""], ["Chalamalasetti", "Sai Rahul", ""], ["Agarwal", "Sapan", ""], ["Marinella", "Matthew", ""], ["Foltin", "Martin", ""], ["Strachan", "John Paul", ""], ["Milojicic", "Dejan", ""], ["Hwu", "Wen-mei", ""], ["Roy", "Kaushik", ""]]}, {"id": "1912.11818", "submitter": "Chao Guo", "authors": "Chao Guo, Kai Xu, Gangxiang Shen, Moshe Zukerman", "title": "Temperature-Aware Virtual Data Center Embedding to Avoid Hot Spots in\n  Data Centers", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Data Center (VDC) embedding has drawn significant attention recently\nbecause of growing need for efficient and flexible means of Data Center (DC)\nresource allocation. Existing studies on VDC embedding mainly focus on\nimproving DCs' resource utilization. However, an important problem that has not\nbeen considered in VDC embedding solutions is the creation of hot spots by\nexcessive heat dissipation and hot air generation from racks in DCs, which have\nsignificant adverse effect on energy consumption of the cooling system and IT\nequipment lifespan. To address this issue, we propose a temperature-aware VDC\nembedding scheme to avoid hot spots by minimizing the maximum temperature of\nhot air emitted from each rack. Meanwhile, we also aim to reduce the total\npower consumption of IT equipment in this scheme. A Mixed Integer Linear\nProgramming (MILP) model and a heuristic algorithm are developed to implement\nthe proposed VDC embedding scheme. Numerical results show that the proposed\ntemperature-aware embedding scheme can significantly outperforms a\nload-balanced embedding scheme in terms of maximum rack temperature, total\npower consumption of IT equipment, and VDC rejection ratio.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 09:47:12 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Guo", "Chao", ""], ["Xu", "Kai", ""], ["Shen", "Gangxiang", ""], ["Zukerman", "Moshe", ""]]}, {"id": "1912.11963", "submitter": "Ningxin Zheng", "authors": "Ningxin Zheng, Quan Chen, Yong Yang, Wei Zhang, Jin Li, Wenli Zheng,\n  Minyi Guo", "title": "URSA: Precise Capacity Planning and Contention-aware Scheduling for\n  Public Clouds", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database platform-as-a-service (dbPaaS) is developing rapidly and a large\nnumber of databases have been migrated to run on the Clouds for the low cost\nand flexibility. Emerging Clouds rely on the tenants to provide the resource\nspecification for their database workloads. However, they tend to over-estimate\nthe resource requirement of their databases, resulting in the unnecessarily\nhigh cost and low Cloud utilization. A methodology that automatically suggests\nthe \"just-enough\" resource specification that fulfills the performance\nrequirement of every database workload is profitable. To this end, we propose\nURSA, a capacity planning and workload scheduling system for dbPaaS Clouds.\nUSRA is comprised by an online capacity planner, a performance interference\nestimator, and a contention-aware scheduling engine. The capacity planner\nidentifies the most cost-efficient resource specification for a database\nworkload to achieve the required performance online. The interference estimator\nquantifies the pressure on the shared resource and the sensitivity to the\nshared resource contention of each database workload. The scheduling engine\nschedules the workloads across Cloud nodes carefully to eliminate unfair\nperformance interference between the co-located workloads. Our real system\nexperimental results show that URSA reduces 27.5% of CPU usage and 53.4% of\nmemory usage for database workloads while satisfying their performance\nrequirements. Meanwhile, URSA reduces the performance unfairness between the\nco-located workloads by 42.8% compared with the Kubernetes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 02:39:48 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zheng", "Ningxin", ""], ["Chen", "Quan", ""], ["Yang", "Yong", ""], ["Zhang", "Wei", ""], ["Li", "Jin", ""], ["Zheng", "Wenli", ""], ["Guo", "Minyi", ""]]}, {"id": "1912.12039", "submitter": "Ashutosh Bhatia Dr.", "authors": "Ashutosh Bhatia, R. C. Hansdah", "title": "A Two-Phase Scheme for Distributed TDMA Scheduling in WSNs with\n  Flexibility to Trade-off between Schedule Length and Scheduling Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing distributed TDMA-scheduling techniques can be classified as\neither static or dynamic. The primary purpose of static TDMA-scheduling\nalgorithms is to improve the channel utilization by generating a schedule of\nshorter length. But, they usually take a longer time to schedule, and hence,\nare not suitable for WSNs, in which the network topology changes dynamically.\nOn the other hand, dynamic TDMA-scheduling algorithms generate a schedule\nquickly, but they are not efficient in terms of generated schedule length. In\nthis paper, we propose a new approach to TDMA scheduling for WSNs, that bridges\nthe gap between the above two extreme types of TDMA-scheduling techniques, by\nproviding the flexibility to trade-off between the schedule length and the time\nrequired to generate the schedule (scheduling time). The proposed TDMA\nscheduling works in two phases. In the first phase, we generate a TDMA schedule\nquickly, which need not have to be very efficient in terms of schedule length.\nIn the second phase, we iteratively reduce the schedule length in a manner,\nsuch that the process of schedule length reduction can be terminated after the\nexecution of an arbitrary number of iterations, and still be left with a valid\nschedule. This step provides the capability to trade-off between schedule\nlength and scheduling time. We have used Castalia network simulator to evaluate\nthe performance of proposed TDMA-scheduling scheme. The simulation result\ntogether with theoretical analysis shows that in addition to the advantage of\ntrading-off the schedule length with scheduling time, the proposed TDMA\nscheduling approach achieves better performance than existing algorithms in\nterms of schedule length and scheduling time.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 09:38:31 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Bhatia", "Ashutosh", ""], ["Hansdah", "R. C.", ""]]}, {"id": "1912.12115", "submitter": "Praneeth Vepakomma", "authors": "Maarten G.Poirot, Praneeth Vepakomma, Ken Chang, Jayashree\n  Kalpathy-Cramer, Rajiv Gupta, Ramesh Raskar", "title": "Split Learning for collaborative deep learning in healthcare", "comments": "Workshop paper: 8 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortage of labeled data has been holding the surge of deep learning in\nhealthcare back, as sample sizes are often small, patient information cannot be\nshared openly, and multi-center collaborative studies are a burden to set up.\nDistributed machine learning methods promise to mitigate these problems. We\nargue for a split learning based approach and apply this distributed learning\nmethod for the first time in the medical field to compare performance against\n(1) centrally hosted and (2) non collaborative configurations for a range of\nparticipants. Two medical deep learning tasks are used to compare split\nlearning to conventional single and multi center approaches: a binary\nclassification problem of a data set of 9000 fundus photos, and multi-label\nclassification problem of a data set of 156,535 chest X-rays. The several\ndistributed learning setups are compared for a range of 1-50 distributed\nparticipants. Performance of the split learning configuration remained constant\nfor any number of clients compared to a single center study, showing a marked\ndifference compared to the non collaborative configuration after 2 clients (p <\n0.001) for both sets. Our results affirm the benefits of collaborative training\nof deep neural networks in health care. Our work proves the significant benefit\nof distributed learning in healthcare, and paves the way for future real-world\nimplementations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 14:39:58 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Poirot", "Maarten G.", ""], ["Vepakomma", "Praneeth", ""], ["Chang", "Ken", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Gupta", "Rajiv", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1912.12370", "submitter": "Josh Payne", "authors": "Josh Payne and Ashish Kundu", "title": "Towards Deep Federated Defenses Against Malware in Cloud Ecosystems", "comments": "IEEE International Conference on Trust, Privacy and Security in\n  Intelligent Systems, and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud computing environments with many virtual machines, containers, and\nother systems, an epidemic of malware can be highly threatening to business\nprocesses. In this vision paper, we introduce a hierarchical approach to\nperforming malware detection and analysis using several recent advances in\nmachine learning on graphs, hypergraphs, and natural language. We analyze\nindividual systems and their logs, inspecting and understanding their behavior\nwith attentional sequence models. Given a feature representation of each\nsystem's logs using this procedure, we construct an attributed network of the\ncloud with systems and other components as vertices and propose an analysis of\nmalware with inductive graph and hypergraph learning models. With this\nfoundation, we consider the multicloud case, in which multiple clouds with\ndiffering privacy requirements cooperate against the spread of malware,\nproposing the use of federated learning to perform inference and training while\npreserving privacy. Finally, we discuss several open problems that remain in\ndefending cloud computing environments against malware related to designing\nrobust ecosystems, identifying cloud-specific optimization problems for\nresponse strategy, action spaces for malware containment and eradication, and\ndeveloping priors and transfer learning tasks for machine learning models in\nthis area.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 23:46:06 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Payne", "Josh", ""], ["Kundu", "Ashish", ""]]}, {"id": "1912.12456", "submitter": "Donghua Chen", "authors": "Donghua Chen", "title": "An Open-Source Project for MapReduce Performance Self-Tuning", "comments": "3 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Hadoop configuration parameters have significant influence in the\nperformance of running MapReduce jobs on Hadoop. It is time-consuming and\ntedious for general users to manually tune the parameters for optimal MapReduce\nperformance. Besides, most of existing self-tuning system have opaque\nimplementation, making it difficult to use in practice. This study presents an\nopen-source project that hosts the developing self-tuning system called Catla\nto address the issues. Catla integrates multiple direct search and\nderivative-free optimization-based techniques to facilitate tuning efficiency\nfor users. An overview of the system and its usage are illustrated in this\nstudy. We also reported a simple example demonstrating the benefits of this\nongoing project. Although this project is still developing and far from\ncomprehensive, it is dedicated to contributing Hadoop ecosystem in terms of\nimproving performance in big data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 13:26:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Donghua", ""]]}, {"id": "1912.12559", "submitter": "Baoqian Wang", "authors": "Baoqian Wang, Junfei Xie, Kejie Lu, Yan Wan, Shengli Fu", "title": "On Batch-Processing Based Coded Computing for Heterogeneous Distributed\n  Computing Systems", "comments": "Accepted by IEEE Transactions on Network Science and Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, coded distributed computing (CDC) has attracted significant\nattention, because it can efficiently facilitate many delay-sensitive\ncomputation tasks against unexpected latencies in distributed computing\nsystems. Despite such a salient feature, many design challenges and\nopportunities remain. In this paper, we focus on practical computing systems\nwith heterogeneous computing resources, and design a novel CDC approach, called\nbatch-processing based coded computing (BPCC), which exploits the fact that\nevery computing node can obtain some coded results before it completes the\nwhole task. To this end, we first describe the main idea of the BPCC framework,\nand then formulate an optimization problem for BPCC to minimize the task\ncompletion time by configuring the computation load. Through formal theoretical\nanalyses, extensive simulation studies, and comprehensive real experiments on\nthe Amazon EC2 computing clusters, we demonstrate promising performance of the\nproposed BPCC scheme, in terms of high computational efficiency and robustness\nto uncertain disturbances.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 00:57:45 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 05:36:10 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 16:46:32 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wang", "Baoqian", ""], ["Xie", "Junfei", ""], ["Lu", "Kejie", ""], ["Wan", "Yan", ""], ["Fu", "Shengli", ""]]}, {"id": "1912.12675", "submitter": "Lifu Zhang", "authors": "Lifu Zhang, Tarek S. Abdelrahman", "title": "Pipelined Training with Stale Weights of Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth in the complexity of Convolutional Neural Networks (CNNs) is\nincreasing interest in partitioning a network across multiple accelerators\nduring training and pipelining the backpropagation computations over the\naccelerators. Existing approaches avoid or limit the use of stale weights\nthrough techniques such as micro-batching or weight stashing. These techniques\neither underutilize of accelerators or increase memory footprint. We explore\nthe impact of stale weights on the statistical efficiency and performance in a\npipelined backpropagation scheme that maximizes accelerator utilization and\nkeeps memory overhead modest. We use 4 CNNs (LeNet-5, AlexNet, VGG and ResNet)\nand show that when pipelining is limited to early layers in a network, training\nwith stale weights converges and results in models with comparable inference\naccuracies to those resulting from non-pipelined training on MNIST and CIFAR-10\ndatasets; a drop in accuracy of 0.4%, 4%, 0.83% and 1.45% for the 4 networks,\nrespectively. However, when pipelining is deeper in the network, inference\naccuracies drop significantly. We propose combining pipelined and non-pipelined\ntraining in a hybrid scheme to address this drop. We demonstrate the\nimplementation and performance of our pipelined backpropagation in PyTorch on 2\nGPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with\na small drop in inference accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 15:28:13 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhang", "Lifu", ""], ["Abdelrahman", "Tarek S.", ""]]}, {"id": "1912.12681", "submitter": "Bowen Xiao", "authors": "Bowen Xiao, Xiaoyi Fan, Sheng Gao and Wei Cai", "title": "EdgeToll: A Blockchain-based Toll Collection System for Public Sharing\n  of Heterogeneous Edges", "comments": null, "journal-ref": "The 3rd International Workshop on Integrating Edge Computing,\n  Caching, and Offloading in Next Generation Networks (INFOCOM workshop IECCO\n  2019), At Paris, France", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is a novel paradigm designed toimprove the quality of service\nfor latency sensitive cloud applications. However, the state-of-the-art edge\nservices are designedfor specific applications, which are isolated from each\nother.To better improve the utilization level of edge nodes, publicresource\nsharing among edges from distinct service providersshould be encouraged\neconomically. In this work, we employ thepayment channel techniques to design\nand implement EdgeToll,a blockchain-based toll collection system for\nheterogeneous public edge sharing. Test-bed has been developed to validate\ntheproposal and preliminary experiments have been conducted todemonstrate the\ntime and cost efficiency of the system.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 16:02:57 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Xiao", "Bowen", ""], ["Fan", "Xiaoyi", ""], ["Gao", "Sheng", ""], ["Cai", "Wei", ""]]}, {"id": "1912.12700", "submitter": "Jeremie Lagraviere", "authors": "J\\'er\\'emie Lagravi\\`ere, Johannes Langguth, Mohammed Sourouri, Phuong\n  H. Ha, Xing Cai", "title": "On the Performance and Energy Efficiency of the PGAS Programming Model\n  on Multicore Architectures", "comments": null, "journal-ref": "Published in: 2016 International Conference on High Performance\n  Computing & Simulation (HPCS) Date of Conference: 18-22 July 2016 Conference\n  Location: Innsbruck, Austria", "doi": "10.1109/HPCSim.2016.7568416", "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Using large-scale multicore systems to get the maximum performance and energy\nefficiency with manageable programmability is a major challenge. The\npartitioned global address space (PGAS) programming model enhances\nprogrammability by providing a global address space over large-scale computing\nsystems. However, so far the performance and energy efficiency of the PGAS\nmodel on multicore-based parallel architectures have not been investigated\nthoroughly. In this paper we use a set of selected kernels from the well-known\nNAS Parallel Benchmarks to evaluate the performance and energy efficiency of\nthe UPC programming language, which is a widely used implementation of the PGAS\nmodel. In addition, the MPI and OpenMP versions of the same parallel kernels\nare used for comparison with their UPC counterparts. The investigated hardware\nplatforms are based on multicore CPUs, both within a single 16-core node and\nacross multiple nodes involving up to 1024 physical cores. On the multi-node\nplatform we used the hardware measurement solution called High definition\nEnergy Efficiency Monitoring tool in order to measure energy. On the\nsingle-node system we used the hybrid measurement solution to make an effort\ninto understanding the observed performance differences, we use the Intel\nPerformance Counter Monitor to quantify in detail the communication time, cache\nhit/miss ratio and memory usage. Our experiments show that UPC is competitive\nwith OpenMP and MPI on single and multiple nodes, with respect to both the\nperformance and energy efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 17:51:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lagravi\u00e8re", "J\u00e9r\u00e9mie", ""], ["Langguth", "Johannes", ""], ["Sourouri", "Mohammed", ""], ["Ha", "Phuong H.", ""], ["Cai", "Xing", ""]]}, {"id": "1912.12701", "submitter": "Jeremie Lagraviere", "authors": "J\\'er\\'emie Lagravi\\`ere, Johannes Langguth, Martina Prugger, Lukas\n  Einkemmer, Phuong H. Ha, Xing Cai", "title": "Performance optimization and modeling of fine-grained irregular\n  communication in UPC", "comments": null, "journal-ref": "Scientific Programming Volume 2019, Article ID 6825728, 20 pages.\n  Hindawi", "doi": "10.1155/2019/6825728", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The UPC programming language offers parallelism via logically partitioned\nshared memory, which typically spans physically disjoint memory sub-systems.\nOne convenient feature of UPC is its ability to automatically execute\nbetween-thread data movement, such that the entire content of a shared data\narray appears to be freely accessible by all the threads. The programmer\nfriendliness, however, can come at the cost of substantial performance\npenalties. This is especially true when indirectly indexing the elements of a\nshared array, for which the induced between-thread data communication can be\nirregular and have a fine-grained pattern. In this paper we study performance\nenhancement strategies specifically targeting such fine-grained irregular\ncommunication in UPC. Starting from explicit thread privatization, continuing\nwith block-wise communication, and arriving at message condensing and\nconsolidation, we obtained considerable performance improvement of UPC programs\nthat originally require fine-grained irregular communication. Besides the\nperformance enhancement strategies, the main contribution of the present paper\nis to propose performance models for the different scenarios, in form of\nquantifiable formulas that hinge on the actual volumes of various data\nmovements plus a small number of easily obtainable hardware characteristic\nparameters. These performance models help to verify the enhancements obtained,\nwhile also providing insightful predictions of similar parallel\nimplementations, not limited to UPC, that also involve between-thread or\nbetween-process irregular communication. As a further validation, we also apply\nour performance modeling methodology and hardware characteristic parameters to\nan existing UPC code for solving a 2D heat equation on a uniform mesh.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 17:51:59 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lagravi\u00e8re", "J\u00e9r\u00e9mie", ""], ["Langguth", "Johannes", ""], ["Prugger", "Martina", ""], ["Einkemmer", "Lukas", ""], ["Ha", "Phuong H.", ""], ["Cai", "Xing", ""]]}, {"id": "1912.12740", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marc Fischer, Vasiliki Kalavri, Michael Kapralov,\n  Torsten Hoefler", "title": "Practice of Streaming Processing of Dynamic Graphs: Concepts, Models,\n  and Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing has become an important part of various areas of computing,\nincluding machine learning, medical applications, social network analysis,\ncomputational sciences, and others. A growing amount of the associated graph\nprocessing workloads are dynamic, with millions of edges added or removed per\nsecond. Graph streaming frameworks are specifically crafted to enable the\nprocessing of such highly dynamic workloads. Recent years have seen the\ndevelopment of many such frameworks. However, they differ in their general\narchitectures (with key details such as the support for the concurrent\nexecution of graph updates and queries, or the incorporated graph data\norganization), the types of updates and workloads allowed, and many others. To\nfacilitate the understanding of this growing field, we provide the first\nanalysis and taxonomy of dynamic and streaming graph processing. We focus on\nidentifying the fundamental system designs and on understanding their support\nfor concurrency, and for different graph updates as well as analytics\nworkloads. We also crystallize the meaning of different concepts associated\nwith streaming graph processing, such as dynamic, temporal, online, and\ntime-evolving graphs, edge-centric processing, models for the maintenance of\nupdates, and graph databases. Moreover, we provide a bridge with the very rich\nlandscape of graph streaming theory by giving a broad overview of recent\ntheoretical related advances, and by discussing which graph streaming models\nand settings could be helpful in developing more powerful streaming frameworks\nand designs. We also outline graph streaming workloads and research challenges.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 21:41:14 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:24:01 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 13:00:50 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Besta", "Maciej", ""], ["Fischer", "Marc", ""], ["Kalavri", "Vasiliki", ""], ["Kapralov", "Michael", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.12795", "submitter": "Qinghe Jing", "authors": "Qinghe Jing, Weiyan Wang, Junxue Zhang, Han Tian, Kai Chen", "title": "Quantifying the Performance of Federated Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of data and isolated data islands encourage different\norganizations to share data with each other to train machine learning models.\nHowever, there are increasing concerns on the problems of data privacy and\nsecurity, which urges people to seek a solution like Federated Transfer\nLearning (FTL) to share training data without violating data privacy. FTL\nleverages transfer learning techniques to utilize data from different sources\nfor training, while achieving data privacy protection without significant\naccuracy loss. However, the benefits come with a cost of extra computation and\ncommunication consumption, resulting in efficiency problems. In order to\nefficiently deploy and scale up FTL solutions in practice, we need a deep\nunderstanding on how the infrastructure affects the efficiency of FTL. Our\npaper tries to answer this question by quantitatively measuring a real-world\nFTL implementation FATE on Google Cloud. According to the results of carefully\ndesigned experiments, we verified that the following bottlenecks can be further\noptimized: 1) Inter-process communication is the major bottleneck; 2) Data\nencryption adds considerable computation overhead; 3) The Internet networking\ncondition affects the performance a lot when the model is large.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 03:10:00 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jing", "Qinghe", ""], ["Wang", "Weiyan", ""], ["Zhang", "Junxue", ""], ["Tian", "Han", ""], ["Chen", "Kai", ""]]}, {"id": "1912.12844", "submitter": "Xianfeng Liang", "authors": "Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen,\n  Yifei Cheng", "title": "Variance Reduced Local SGD with Lower Communication Complexity", "comments": "25 pages, 6 figures. The paper presents a novel variance reduction\n  algorithm for Local SGD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate the training of machine learning models, distributed stochastic\ngradient descent (SGD) and its variants have been widely adopted, which apply\nmultiple workers in parallel to speed up training. Among them, Local SGD has\ngained much attention due to its lower communication cost. Nevertheless, when\nthe data distribution on workers is non-identical, Local SGD requires\n$O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its\n\\emph{linear iteration speedup} property, where $T$ is the total number of\niterations and $N$ is the number of workers. In this paper, we propose Variance\nReduced Local SGD (VRL-SGD) to further reduce the communication complexity.\nBenefiting from eliminating the dependency on the gradient variance among\nworkers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration\nspeedup} with a lower communication complexity $O(T^{\\frac{1}{2}}\nN^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct\nexperiments on three machine learning tasks, and the experimental results\ndemonstrate that VRL-SGD performs impressively better than Local SGD when the\ndata among workers are quite diverse.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 08:15:21 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Liang", "Xianfeng", ""], ["Shen", "Shuheng", ""], ["Liu", "Jingchang", ""], ["Pan", "Zhen", ""], ["Chen", "Enhong", ""], ["Cheng", "Yifei", ""]]}, {"id": "1912.12918", "submitter": "Masatoshi Hanai", "authors": "Masatoshi Hanai and Georgios Theodoropoulos", "title": "Performance Evaluation of Dynamic Scaling on MPI", "comments": "Code is in https://github.com/masatoshihanai/MPIDynamicScaling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic scaling aims to elastically change the number of processes during\nruntime to tune the performance of the distributed applications. This report\nbriefly presents a performance evaluation of MPI process provisioning /\nde-provisioning for dynamic scaling by using 16 to 128 cores. Our dynamic\nscaling implementation allows the new MPI processes from new hosts to\ncommunicate with the original ones immediately. Moreover, it forbids the\nremoving MPI processes to communicate with others as well as gets the\ninformation whether the host node can be terminated or not. Such a simple\nfeature is not supported as a single-line API in MPI-2 such as\nMPI_Comm_spawn(). We provide our implementation as a simple library to extend a\nnon-dynamic-scalable MPI program into a dynamic-scalable one by adding only\nseveral lines of codes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 13:20:43 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Hanai", "Masatoshi", ""], ["Theodoropoulos", "Georgios", ""]]}, {"id": "1912.12953", "submitter": "Liu Ke", "authors": "Liu Ke, Udit Gupta, Carole-Jean Wu, Benjamin Youngjae Cho, Mark\n  Hempstead, Brandon Reagen, Xuan Zhang, David Brooks, Vikas Chandra, Utku\n  Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng\n  Li, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail\n  Smelyanskiy, Xiaodong Wang", "title": "RecNMP: Accelerating Personalized Recommendation with Near-Memory\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendation systems leverage deep learning models and account\nfor the majority of data center AI cycles. Their performance is dominated by\nmemory-bound sparse embedding operations with unique irregular memory access\npatterns that pose a fundamental challenge to accelerate. This paper proposes a\nlightweight, commodity DRAM compliant, near-memory processing solution to\naccelerate personalized recommendation inference. The in-depth characterization\nof production-grade recommendation models shows that embedding operations with\nhigh model-, operator- and data-level parallelism lead to memory bandwidth\nsaturation, limiting recommendation inference performance. We propose RecNMP\nwhich provides a scalable solution to improve system throughput, supporting a\nbroad range of sparse embedding models. RecNMP is specifically tailored to\nproduction environments with heavy co-location of operators on a single server.\nSeveral hardware/software co-optimization techniques such as memory-side\ncaching, table-aware packet scheduling, and hot entry profiling are studied,\nresulting in up to 9.8x memory latency speedup over a highly-optimized\nbaseline. Overall, RecNMP offers 4.2x throughput improvement and 45.8% memory\nenergy savings.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 15:08:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Ke", "Liu", ""], ["Gupta", "Udit", ""], ["Wu", "Carole-Jean", ""], ["Cho", "Benjamin Youngjae", ""], ["Hempstead", "Mark", ""], ["Reagen", "Brandon", ""], ["Zhang", "Xuan", ""], ["Brooks", "David", ""], ["Chandra", "Vikas", ""], ["Diril", "Utku", ""], ["Firoozshahian", "Amin", ""], ["Hazelwood", "Kim", ""], ["Jia", "Bill", ""], ["Lee", "Hsien-Hsin S.", ""], ["Li", "Meng", ""], ["Maher", "Bert", ""], ["Mudigere", "Dheevatsa", ""], ["Naumov", "Maxim", ""], ["Schatz", "Martin", ""], ["Smelyanskiy", "Mikhail", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1912.13163", "submitter": "Stefano Savazzi", "authors": "Stefano Savazzi, Monica Nicoli, Vittorio Rampa", "title": "Federated Learning with Cooperating Devices: A Consensus Approach for\n  Massive IoT Networks", "comments": "This work received support from the CHIST-ERA III Grant RadioSense\n  (Big Data and Process Modelling for the Smart Industry - BDSI). The paper has\n  been accepted for publication in the IEEE Internet of Things Journal. The\n  current arXiv contains an additional Appendix C that describes the database\n  and the Python scripts. Published version:\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8950073&isnumber=6702522", "journal-ref": "IEEE Internet of Things Journal, vol. 7, no. 5, pp. 4641-4654, May\n  2020", "doi": "10.1109/JIOT.2020.2964162", "report-no": null, "categories": "eess.SP cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is emerging as a new paradigm to train machine\nlearning models in distributed systems. Rather than sharing, and disclosing,\nthe training dataset with the server, the model parameters (e.g. neural\nnetworks weights and biases) are optimized collectively by large populations of\ninterconnected devices, acting as local learners. FL can be applied to\npower-constrained IoT devices with slow and sporadic connections. In addition,\nit does not need data to be exported to third parties, preserving privacy.\nDespite these benefits, a main limit of existing approaches is the centralized\noptimization which relies on a server for aggregation and fusion of local\nparameters; this has the drawback of a single point of failure and scaling\nissues for increasing network size. The paper proposes a fully distributed (or\nserver-less) learning approach: the proposed FL algorithms leverage the\ncooperation of devices that perform data operations inside the network by\niterating local computations and mutual interactions via consensus-based\nmethods. The approach lays the groundwork for integration of FL within 5G and\nbeyond networks characterized by decentralized connectivity and computing, with\nintelligence distributed over the end-devices. The proposed methodology is\nverified by experimental datasets collected inside an industrial IoT\nenvironment.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 15:16:04 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Savazzi", "Stefano", ""], ["Nicoli", "Monica", ""], ["Rampa", "Vittorio", ""]]}, {"id": "1912.13451", "submitter": "Olin Shivers Third", "authors": "Olin Shivers, Justin Slepak and Panagiotis Manolios", "title": "Introduction to Rank-polymorphic Programming in Remora (Draft)", "comments": "52 pages; fixes some errors in previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remora is a higher-order, rank-polymorphic array-processing programming\nlanguage, in the same general class of languages as APL and J. It is intended\nfor writing programs to be executed on parallel hardware.\n  We provide an example-driven introduction to the language, and its general\ncomputational model, originally developed by Iverson for APL. We begin with\nDynamic Remora, a variant of the language with a dynamic type system (as in\nScheme or Lisp), to introduce the fundamental computational mechanisms of the\nlanguage, then shift to Explicitly Typed Remora, a variant of the language with\na static, dependent type system that permits the shape of the arrays being\ncomputed to be captured at compile time.\n  This article can be considered an introduction to the general topic of the\nrank-polymorphic array-processing computational model, above and beyond the\nspecific details of the Remora language.\n  We do not address the details of type inference in Remora, that is, the\nassignment of explicit types to programs written without such annotations; this\nis ongoing research.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:45:43 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 06:33:42 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Shivers", "Olin", ""], ["Slepak", "Justin", ""], ["Manolios", "Panagiotis", ""]]}]