[{"id": "2104.00087", "submitter": "Chinmay Soman", "authors": "Yupeng Fu, Chinmay Soman", "title": "Real-time Data Infrastructure at Uber", "comments": "To be published in Proceedings of the 2021 International Conference\n  on Management of Data (SIGMOD '21)", "journal-ref": null, "doi": "10.1145/3448016.3457552", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uber's business is highly real-time in nature. PBs of data is continuously\nbeing collected from the end users such as Uber drivers, riders, restaurants,\neaters and so on everyday. There is a lot of valuable information to be\nprocessed and many decisions must be made in seconds for a variety of use cases\nsuch as customer incentives, fraud detection, machine learning model\nprediction. In addition, there is an increasing need to expose this ability to\ndifferent user categories, including engineers, data scientists, executives and\noperations personnel which adds to the complexity. In this paper, we present\nthe overall architecture of the real-time data infrastructure and identify\nthree scaling challenges that we need to continuously address for each\ncomponent in the architecture. At Uber, we heavily rely on open source\ntechnologies for the key areas of the infrastructure. On top of those\nopen-source software, we add significant improvements and customizations to\nmake the open-source solutions fit in Uber's environment and bridge the gaps to\nmeet Uber's unique scale and requirements. We then highlight several important\nuse cases and show their real-time solutions and tradeoffs. Finally, we reflect\non the lessons we learned as we built, operated and scaled these systems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:02:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fu", "Yupeng", ""], ["Soman", "Chinmay", ""]]}, {"id": "2104.00131", "submitter": "Mario Figueiredo", "authors": "Francisco L. Andrade, M\\'ario A. T. Figueiredo, Jo\\~ao Xavier", "title": "Distributed Picard Iteration", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Picard iteration is widely used to find fixed points of locally\ncontractive (LC) maps. This paper extends the Picard iteration to distributed\nsettings; specifically, we assume the map of which the fixed point is sought to\nbe the average of individual (not necessarily LC) maps held by a set of agents\nlinked by a sparse communication network. An additional difficulty is that the\nLC map is not assumed to come from an underlying optimization problem, which\nprevents exploiting strong global properties such as convexity or\nLipschitzianity. Yet, we propose a distributed algorithm and prove its\nconvergence, in fact showing that it maintains the linear rate of the standard\nPicard iteration for the average LC map. As another contribution, our proof\nimports tools from perturbation theory of linear operators, which, to the best\nof our knowledge, had not been used before in the theory of distributed\ncomputation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 21:50:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Andrade", "Francisco L.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""], ["Xavier", "Jo\u00e3o", ""]]}, {"id": "2104.00178", "submitter": "Dingwen Tao", "authors": "Sian Jin, Jesus Pulido, Pascal Grosset, Jiannan Tian, Dingwen Tao,\n  James Ahrens", "title": "Adaptive Configuration of In Situ Lossy Compression for Cosmology\n  Simulations via Fine-Grained Rate-Quality Modeling", "comments": "13 pages, 19 figures, 2 tables, published by HPDC'21", "journal-ref": null, "doi": "10.1145/3431379.3460653", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extreme-scale cosmological simulations have been widely used by today's\nresearchers and scientists on leadership supercomputers. A new generation of\nerror-bounded lossy compressors has been used in workflows to reduce storage\nrequirements and minimize the impact of throughput limitations while saving\nlarge snapshots of high-fidelity data for post-hoc analysis. In this paper, we\npropose to adaptively provide compression configurations to compute partitions\nof cosmological simulations with newly designed post-analysis aware\nrate-quality modeling. The contribution is fourfold: (1) We propose a novel\nadaptive approach to select feasible error bounds for different partitions,\nshowing the possibility and efficiency of adaptively configuring lossy\ncompression for each partition individually. (2) We build models to estimate\nthe overall loss of post-analysis result due to lossy compression and to\nestimate compression ratio, based on the property of each partition. (3) We\ndevelop an efficient optimization guideline to determine the best-fit\nconfiguration of error bounds combination in order to maximize the compression\nratio under acceptable post-analysis quality loss. (4) Our approach introduces\nnegligible overheads for feature extraction and error-bound optimization for\neach partition, enabling post-analysis-aware in situ lossy compression for\ncosmological simulations. Experiments show that our proposed models are highly\naccurate and reliable. Our fine-grained adaptive configuration approach\nimproves the compression ratio of up to 73% on the tested datasets with the\nsame post-analysis distortion with only 1% performance overhead.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 00:51:05 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 16:07:08 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 22:59:07 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Jin", "Sian", ""], ["Pulido", "Jesus", ""], ["Grosset", "Pascal", ""], ["Tian", "Jiannan", ""], ["Tao", "Dingwen", ""], ["Ahrens", "James", ""]]}, {"id": "2104.00282", "submitter": "Akshay Agrawal", "authors": "Akshay Agrawal, Stephen Boyd, Deepak Narayanan, Fiodar Kazhamiaka,\n  Matei Zaharia", "title": "Allocation of Fungible Resources via a Fast, Scalable Price Discovery\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of assigning or allocating resources to a set of\njobs. We consider the case when the resources are fungible, that is, the job\ncan be done with any mix of the resources, but with different efficiencies. In\nour formulation we maximize a total utility subject to a given limit on the\nresource usage, which is a convex optimization problem and so is tractable. In\nthis paper we develop a custom, parallelizable algorithm for solving the\nresource allocation problem that scales to large problems, with millions of\njobs. Our algorithm is based on the dual problem, in which the dual variables\nassociated with the resource usage limit can be interpreted as resource prices.\nOur method updates the resource prices in each iteration, ultimately\ndiscovering the optimal resource prices, from which an optimal allocation is\nobtained. We provide an open-source implementation of our method, which can\nsolve problems with millions of jobs in a few seconds on CPU, and under a\nsecond on a GPU; our software can solve smaller problems in milliseconds. On\nlarge problems, our implementation is up to three orders of magnitude faster\nthan a commerical solver for convex optimization.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:40:49 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 17:29:25 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 16:05:50 GMT"}, {"version": "v4", "created": "Sun, 18 Apr 2021 15:19:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Agrawal", "Akshay", ""], ["Boyd", "Stephen", ""], ["Narayanan", "Deepak", ""], ["Kazhamiaka", "Fiodar", ""], ["Zaharia", "Matei", ""]]}, {"id": "2104.00352", "submitter": "Akihito Taya", "authors": "Akihito Taya, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto", "title": "Decentralized and Model-Free Federated Learning: Consensus-Based\n  Distillation in Function Space", "comments": "submitted to IEEE TSIPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a decentralized FL scheme for IoE devices connected via\nmulti-hop networks. FL has gained attention as an enabler of privacy-preserving\nalgorithms, but it is not guaranteed that FL algorithms converge to the optimal\npoint because of non-convexity when using decentralized parameter averaging\nschemes. Therefore, a distributed algorithm that converges to the optimal\nsolution should be developed. The key idea of the proposed algorithm is to\naggregate the local prediction functions, not in a parameter space but in a\nfunction space. Since machine learning tasks can be regarded as convex\nfunctional optimization problems, a consensus-based optimization algorithm\nachieves the global optimum if it is tailored to work in a function space. This\npaper at first analyzes the convergence of the proposed algorithm in a function\nspace, which is referred to as a meta-algorithm. It is shown that spectral\ngraph theory can be applied to the function space in a similar manner as that\nof numerical vectors. Then, a CMFD is developed for NN as an implementation of\nthe meta-algorithm. CMFD leverages knowledge distillation to realize function\naggregation among adjacent devices without parameter averaging. One of the\nadvantages of CMFD is that it works even when NN models are different among the\ndistributed learners. This paper shows that CMFD achieves higher accuracy than\nparameter aggregation under weakly-connected networks. The stability of CMFD is\nalso higher than that of parameter aggregation methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:17:20 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 09:32:12 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Taya", "Akihito", ""], ["Nishio", "Takayuki", ""], ["Morikura", "Masahiro", ""], ["Yamamoto", "Koji", ""]]}, {"id": "2104.00486", "submitter": "Qiang Wang", "authors": "Xinxin Mei, Qiang Wang, Xiaowen Chu, Hai Liu, Yiu-Wing Leung, Zongpeng\n  Li", "title": "Energy-aware Task Scheduling with Deadline Constraint in DVFS-enabled\n  Heterogeneous Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy conservation of large data centers for high-performance computing\nworkloads, such as deep learning with big data, is of critical significance,\nwhere cutting down a few percent of electricity translates into million-dollar\nsavings. This work studies energy conservation on emerging CPU-GPU hybrid\nclusters through dynamic voltage and frequency scaling (DVFS). We aim at\nminimizing the total energy consumption of processing a batch of offline tasks\nor a sequence of real-time tasks under deadline constraints. We derive a fast\nand accurate analytical model to compute the appropriate voltage/frequency\nsetting for each task and assign multiple tasks to the cluster with heuristic\nscheduling algorithms. In particular, our model stresses the nonlinear\nrelationship between task execution time and processor speed for\nGPU-accelerated applications, for more accurately capturing real-world GPU\nenergy consumption. In performance evaluation driven by real-world power\nmeasurement traces, our scheduling algorithm shows comparable energy savings to\nthe theoretical upper bound. With a GPU scaling interval where analytically at\nmost 36% of energy can be saved, we record 33-35% of energy savings. Our\nresults are applicable to energy management on modern heterogeneous clusters.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:14:46 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Mei", "Xinxin", ""], ["Wang", "Qiang", ""], ["Chu", "Xiaowen", ""], ["Liu", "Hai", ""], ["Leung", "Yiu-Wing", ""], ["Li", "Zongpeng", ""]]}, {"id": "2104.00489", "submitter": "Pavlos Papadopoulos", "authors": "Daniele Romanini, Adam James Hall, Pavlos Papadopoulos, Tom Titcombe,\n  Abbas Ismail, Tudor Cebere, Robert Sandmann, Robin Roehm, Michael A. Hoeh", "title": "PyVertical: A Vertical Federated Learning Framework for Multi-headed\n  SplitNN", "comments": "ICLR 2021 Workshop on Distributed and Private Machine Learning (DPML\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce PyVertical, a framework supporting vertical federated learning\nusing split neural networks. The proposed framework allows a data scientist to\ntrain neural networks on data features vertically partitioned across multiple\nowners while keeping raw data on an owner's device. To link entities shared\nacross different datasets' partitions, we use Private Set Intersection on IDs\nassociated with data points. To demonstrate the validity of the proposed\nframework, we present the training of a simple dual-headed split neural network\nfor a MNIST classification task, with data samples vertically distributed\nacross two data owners and a data scientist.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:21:33 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 18:15:08 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 08:05:04 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Romanini", "Daniele", ""], ["Hall", "Adam James", ""], ["Papadopoulos", "Pavlos", ""], ["Titcombe", "Tom", ""], ["Ismail", "Abbas", ""], ["Cebere", "Tudor", ""], ["Sandmann", "Robert", ""], ["Roehm", "Robin", ""], ["Hoeh", "Michael A.", ""]]}, {"id": "2104.00501", "submitter": "Alexander Renz-Wieland", "authors": "Alexander Renz-Wieland, Rainer Gemulla, Zoi Kaoudi, Volker Markl", "title": "Replicate or Relocate? Non-Uniform Access in Parameter Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Parameter servers (PSs) facilitate the implementation of distributed training\nfor large machine learning tasks. A key challenge for PS performance is that\nparameter access is non-uniform in many real-world machine learning tasks,\ni.e., different parameters exhibit drastically different access patterns. We\nidentify skew and nondeterminism as two major sources for non-uniformity.\nExisting PSs are ill-suited for managing such non-uniform access because they\nuniformly apply the same parameter management technique to all parameters. As\nconsequence, the performance of existing PSs is negatively affected and may\neven fall behind that of single node baselines. In this paper, we explore how\nPSs can manage non-uniform access efficiently. We find that it is key for PSs\nto support multiple management techniques and to leverage a well-suited\nmanagement technique for each parameter. We present Lapse2, a PS that\nreplicates hot spot parameters, relocates less frequently accessed parameters,\nand employs specialized techniques to manage nondeterminism that arises from\nrandom sampling. In our experimental study, Lapse2 outperformed existing,\nsingle-technique PSs by up to one order of magnitude and provided near-linear\nscalability across multiple machine learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:52:32 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Renz-Wieland", "Alexander", ""], ["Gemulla", "Rainer", ""], ["Kaoudi", "Zoi", ""], ["Markl", "Volker", ""]]}, {"id": "2104.00547", "submitter": "Michael Kuperberg", "authors": "Michael Kuperberg, Matthias Geipel", "title": "Blockchain and BIM (Building Information Modeling): Progress in Academia\n  and Industry", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In construction, BIM (Building Information Modeling) promises to increase\nquality of data and to provide a shared, uniform view to all parties. While BIM\ntools and exchange formats exist, the distribution and safeguarding of data is\nan ongoing challenge. Distributed Ledger Technology and Blockchains offer a\npossible solution to this task, and they promise quality attributes such as\ntamper resistance, traceability/auditability and safe digitalization of assets\nand intellectual property. However, the practical application and adoption of\nDistributed Ledger Technology in the built environment requires a good\nunderstanding of tool maturity, performance and standardization. Also,\nuser-oriented integration of BIM tools with the blockchain backend needs\nattention. The contribution of this paper is an overview over both industrial\nand academic progress at the intersection of BIM and blockchains/DLT.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:34:20 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 06:12:10 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kuperberg", "Michael", ""], ["Geipel", "Matthias", ""]]}, {"id": "2104.00792", "submitter": "Alok Tripathy", "authors": "Alok Tripathy, Oded Green", "title": "Scalable Hash Table for NUMA Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are used in a plethora of applications, including database\noperations, DNA sequencing, string searching, and many more. As such, there are\nmany parallelized hash tables targeting multicore, distributed, and\naccelerator-based systems. We present in this work a multi-GPU hash table\nimplementation that can process keys at a throughput comparable to that of\ndistributed hash tables. Distributed CPU hash tables have received\nsignificantly more attention than GPU-based hash tables. We show that a single\nnode with multiple GPUs offers roughly the same performance as a 500-1,000-core\nCPU-based cluster. Our algorithm's key component is our use of multiple\nsparse-graph data structures and binning techniques to build the hash table. As\nhas been shown individually, these components can be written with massive\nparallelism that is amenable to GPU acceleration. Since we focus on an\nindividual node, we also leverage communication primitives that are typically\nprohibitive in distributed environments. We show that our new multi-GPU\nalgorithm shares many of the same features of the single GPU algorithm -- thus\nwe have efficient collision management capabilities and can deal with a large\nnumber of duplicates. We evaluate our algorithm on two multi-GPU compute nodes:\n1) an NVIDIA DGX2 server with 16 GPUs and 2) an IBM Power 9 Processor with 6\nNVIDIA GPUs. With 32-bit keys, our implementation processes 8B keys per second,\ncomparable to some 500-1,000-core CPU-based clusters and 4X faster than prior\nsingle-GPU implementations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:34:48 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Tripathy", "Alok", ""], ["Green", "Oded", ""]]}, {"id": "2104.00813", "submitter": "Asmae Benali", "authors": "Asmae Benali and Bouchra El Asri", "title": "A Context Aware and Self Adaptation Strategy for Cloud Service Selection\n  and Configuration in Run Time", "comments": "URL: https://doi.org/10.5539/cis.v13n1p10", "journal-ref": "Vol. 13, No. 1; 2020", "doi": "10.5539/cis.v13n1p10", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Day after day, the number of mobile applications deployed on cloud computing\ncontinues in increasing because o f smartphone capabilities improvement. Cloud\ncomputing has already succeeded in the web based application, for that reason,\nthe demand for context aware services provided by cloud computing increases. To\ncustomize a cloud service that takes into account th e consumer requirements,\nwhich depend on information change, it brings to light many recent challenges\nto cloud computing about environment aware, location aware, time aware. The\ncloud provider, moreover, has to manage personalized applications and the con\nstraints of mobile devices in matters of interaction abilities and\ncommunication restrictions. This paper proposes a strategy for selecting\nautomatically an appropriate cloud environment that runs out whole\nrequirements, defines a configuration for the ass ociated cloud environment and\nable to easily adapt to the change of the environment on either the user or the\ncloud side or both. This process builds on the principles of dynamic software\nproduct lines, Agent oriented software engineering, and the MAPE k m odel to\nselect and configure cloud environments according to the consumer needs and the\ncontext change.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:49:33 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Benali", "Asmae", ""], ["Asri", "Bouchra El", ""]]}, {"id": "2104.00815", "submitter": "Asmae Benali", "authors": "Asmae Benali, Bouchra El Asri and Houda Kriouile", "title": "Toward Sensor and Software Product Line Based Context Aware Cloud\n  Environment Assignment", "comments": "https://doi.org/10.20943/01201605.7685", "journal-ref": "Volume 13, Issue 5, September 2016", "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the growing interest for mobile device and pervasive applications\ndeployed on cloud computing, the providing of intelligent and ubiquitous\ncontext-aware applications that take into account the user's context is one of\nthe main challenges in future applications. In this article we consider how to\naugment applications aware context used by mobiles device and deployed on cloud\ncomputing. The behavior of these applications should depend not only on their\ninternal state and user interactions but also on the context sensed during\ntheir execution. Indeed, our approach based on two essential mechanisms,\ncontext sensoring and context reasoning. We consider the information acquired\nby the context sensoring as a product line and we use feature models to\nrepresent this information received, the services provided by cloud provider,\nthe available resources and constraints. At the context reasoning step, the\ncontext provisioning feature model (CSCAFM) algorithm permits to generate\nservice context-aware that fulfils the requirements of use ensures a certain\nlevel of performance of resources.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:59:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Benali", "Asmae", ""], ["Asri", "Bouchra El", ""], ["Kriouile", "Houda", ""]]}, {"id": "2104.00819", "submitter": "Asmae Benali", "authors": "Asmae Benali and Bouchra El Asri", "title": "Towards Rigorous Selection and Configuration of Cloud Services: Research\n  Methodology", "comments": "https://doi.org/10.5281/zenodo.4431076", "journal-ref": "Volume 17, Issue 6, November 2020", "doi": "10.5281/zenodo.4431076", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has recently emerged as a major trend in distributed\ncomputing. We proposed a platform for selecting and configuring automatically\nan appropriate cloud environment that meets a set of consumer and provider\nrequirements. It can easily adapt its behavior, either at design-time or\nruntime, to the change of the environment in matters of location, time,\nactivity, interaction abilities, and communication restrictions. The platform\nbased on the principles of dynamic software product lines (SPL), Agent-oriented\nsoftware engineering, and the MAPE-k reference model. We based on the Design\nScience Research Methodology to conduct this work. In this article, we present\nthe steps of our research following this methodology's guidelines.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:09:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Benali", "Asmae", ""], ["Asri", "Bouchra El", ""]]}, {"id": "2104.00828", "submitter": "Yifan Sun", "authors": "Yifan Sun, Yixuan Zhang, Ali Mosallaei, Michael D. Shah, Cody Dunne,\n  David Kaeli", "title": "Daisen: A Framework for Visualizing Detailed GPU Execution", "comments": "EuroVis Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphics Processing Units (GPUs) have been widely used to accelerate\nartificial intelligence, physics simulation, medical imaging, and information\nvisualization applications. To improve GPU performance, GPU hardware designers\nneed to identify performance issues by inspecting a huge amount of\nsimulator-generated traces. Visualizing the execution traces can reduce the\ncognitive burden of users and facilitate making sense of behaviors of GPU\nhardware components. In this paper, we first formalize the process of GPU\nperformance analysis and characterize the design requirements of visualizing\nexecution traces based on a survey study and interviews with GPU hardware\ndesigners. We contribute data and task abstraction for GPU performance\nanalysis. Based on our task analysis, we propose Daisen, a framework that\nsupports data collection from GPU simulators and provides visualization of the\nsimulator-generated GPU execution traces. Daisen features a data abstraction\nand trace format that can record simulator-generated GPU execution traces.\nDaisen also includes a web-based visualization tool that helps GPU hardware\ndesigners examine GPU execution traces, identify performance bottlenecks, and\nverify performance improvement. Our qualitative evaluation with GPU hardware\ndesigners demonstrates that the design of Daisen reflects the typical workflow\nof GPU hardware designers. Using Daisen, participants were able to effectively\nidentify potential performance bottlenecks and opportunities for performance\nimprovement. The open-sourced implementation of Daisen can be found at\ngitlab.com/akita/vis. Supplemental materials including a demo video, survey\nquestions, evaluation study guide, and post-study evaluation survey are\navailable at osf.io/j5ghq.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:52:37 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sun", "Yifan", ""], ["Zhang", "Yixuan", ""], ["Mosallaei", "Ali", ""], ["Shah", "Michael D.", ""], ["Dunne", "Cody", ""], ["Kaeli", "David", ""]]}, {"id": "2104.00897", "submitter": "Yujia Zhai", "authors": "Yujia Zhai, Elisabeth Giem, Quan Fan, Kai Zhao, Jinyang Liu, Zizhong\n  Chen", "title": "FT-BLAS: A High Performance BLAS Implementation With Online Fault\n  Tolerance", "comments": "camera-ready version at ICS'21: International Conference on\n  Supercomputing 2021 with ISBN updated", "journal-ref": null, "doi": "10.1145/3447818.3460364", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Basic Linear Algebra Subprograms (BLAS) is a core library in scientific\ncomputing and machine learning. This paper presents FT-BLAS, a new\nimplementation of BLAS routines that not only tolerates soft errors on the fly,\nbut also provides comparable performance to modern state-of-the-art BLAS\nlibraries on widely-used processors such as Intel Skylake and Cascade Lake. To\naccommodate the features of BLAS, which contains both memory-bound and\ncomputing-bound routines, we propose a hybrid strategy to incorporate fault\ntolerance into our brand-new BLAS implementation: duplicating computing\ninstructions for memory-bound Level-1 and Level-2 BLAS routines and\nincorporating an Algorithm-Based Fault Tolerance mechanism for computing-bound\nLevel-3 BLAS routines. Our high performance and low overhead are obtained from\ndelicate assembly-level optimization and a kernel-fusion approach to the\ncomputing kernels. Experimental results demonstrate that FT-BLAS offers high\nreliability and high performance -- faster than Intel MKL, OpenBLAS, and BLIS\nby up to 3.50%, 22.14% and 21.70%, respectively, for routines spanning all\nthree levels of BLAS we benchmarked, even under hundreds of errors injected per\nminute.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:02:58 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 08:10:07 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 21:29:55 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhai", "Yujia", ""], ["Giem", "Elisabeth", ""], ["Fan", "Quan", ""], ["Zhao", "Kai", ""], ["Liu", "Jinyang", ""], ["Chen", "Zizhong", ""]]}, {"id": "2104.01082", "submitter": "Adeyinka K. Akanbi MR", "authors": "Adeyinka Akanbi", "title": "ESTemd: A Distributed Processing Framework for Environmental Monitoring\n  based on Apache Kafka Streaming Engine", "comments": "12 pages", "journal-ref": null, "doi": "10.1145/3445945.3445949", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed networks and real-time systems are becoming the most important\ncomponents for the new computer age, the Internet of Things (IoT), with huge\ndata streams or data sets generated from sensors and data generated from\nexisting legacy systems. The data generated offers the ability to measure,\ninfer and understand environmental indicators, from delicate ecologies and\nnatural resources to urban environments. This can be achieved through the\nanalysis of the heterogeneous data sources (structured and unstructured). In\nthis paper, we propose a distributed framework Event STream Processing Engine\nfor Environmental Monitoring Domain (ESTemd) for the application of stream\nprocessing on heterogeneous environmental data. Our work in this area\ndemonstrates the useful role big data techniques can play in an environmental\ndecision support system, early warning and forecasting systems. The proposed\nframework addresses the challenges of data heterogeneity from heterogeneous\nsystems and real time processing of huge environmental datasets through a\npublish/subscribe method via a unified data pipeline with the application of\nApache Kafka for real time analytics.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:04:15 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Akanbi", "Adeyinka", ""]]}, {"id": "2104.01126", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Shangdi Yu, Yan Gu, Julian Shun", "title": "Fast Parallel Algorithms for Euclidean Minimum Spanning Tree and\n  Hierarchical Spatial Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new parallel algorithms for generating Euclidean minimum\nspanning trees and spatial clustering hierarchies (known as HDBSCAN$^*$). Our\napproach is based on generating a well-separated pair decomposition followed by\nusing Kruskal's minimum spanning tree algorithm and bichromatic closest pair\ncomputations. We introduce a new notion of well-separation to reduce the work\nand space of our algorithm for HDBSCAN$^*$. We also present a parallel\napproximate algorithm for OPTICS based on a recent sequential algorithm by Gan\nand Tao. Finally, we give a new parallel divide-and-conquer algorithm for\ncomputing the dendrogram and reachability plots, which are used in visualizing\nclusters of different scale that arise for both EMST and HDBSCAN$^*$. We show\nthat our algorithms are theoretically efficient: they have work (number of\noperations) matching their sequential counterparts, and polylogarithmic depth\n(parallel time).\n  We implement our algorithms and propose a memory optimization that requires\nonly a subset of well-separated pairs to be computed and materialized, leading\nto savings in both space (up to 10x) and time (up to 8x). Our experiments on\nlarge real-world and synthetic data sets using a 48-core machine show that our\nfastest algorithms outperform the best serial algorithms for the problems by\n11.13--55.89x, and existing parallel algorithms by at least an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:05:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Yiqiu", ""], ["Yu", "Shangdi", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "2104.01142", "submitter": "Vitor Enes", "authors": "Vitor Enes, Carlos Baquero, Alexey Gotsman, Pierre Sutra", "title": "Efficient Replication via Timestamp Stability (Extended Version)", "comments": "Extended version of a EuroSys'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern web applications replicate their data across the globe and require\nstrong consistency guarantees for their most critical data. These guarantees\nare usually provided via state-machine replication (SMR). Recent advances in\nSMR have focused on leaderless protocols, which improve the availability and\nperformance of traditional Paxos-based solutions. We propose Tempo - a\nleaderless SMR protocol that, in comparison to prior solutions, achieves\nsuperior throughput and offers predictable performance even in contended\nworkloads. To achieve these benefits, Tempo timestamps each application command\nand executes it only after the timestamp becomes stable, i.e., all commands\nwith a lower timestamp are known. Both the timestamping and stability detection\nmechanisms are fully decentralized, thus obviating the need for a leader\nreplica. Our protocol furthermore generalizes to partial replication settings,\nenabling scalability in highly parallel workloads. We evaluate the protocol in\nboth real and simulated geo-distributed environments and demonstrate that it\noutperforms state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:44:47 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 08:06:05 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Enes", "Vitor", ""], ["Baquero", "Carlos", ""], ["Gotsman", "Alexey", ""], ["Sutra", "Pierre", ""]]}, {"id": "2104.01238", "submitter": "Leila Namvari-TazehKand", "authors": "Leila Namvari-Tazehkand, Saeid Pashazadeh", "title": "Investigating the Reliability in Three RAID Storage Models and Effect of\n  Ordering Replicas on Disks", "comments": "7 pages, This paper accepted in 6th International Reliability and\n  Safety Engineering Conference, Iran, Shiraz, 2021/2/18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the most important parts of cloud computing is storage devices, and\nRedundant Array of Independent Disks (RAID) systems are well known and\nfrequently used storage devices. With the increasing production of data in\ncloud environments, we need high-reliable storage, given the importance of\ndata. RAID system's reliability analysis is of particular significance in the\narea of cloud storage. Generally, data redundancy is used to create fault\ntolerance and increases the reliability of storage. This study has considered\nthree examples of the simple RAID storage models and analyzed their\nreliability. All of which have a replication factor of two and have the same\nnumber of disks and reliabilities. The only difference is the model that they\nused for generating the redundancy. To compare these three models' reliability,\nwe examined the degree of fault tolerance (FT) and calculated the models'\nreliability using the reliability block diagram (RBD). In this paper, the\neffect of redundancy's type and the blocks' arrangement on the system's\nreliability was investigated.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 05:53:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Namvari-Tazehkand", "Leila", ""], ["Pashazadeh", "Saeid", ""]]}, {"id": "2104.01327", "submitter": "Antonios Katsarakis", "authors": "Antonios Katsarakis (1), Yijun Ma (2), Zhaowei Tan (3), Andrew\n  Bainbridge (4), Matthew Balkwill (4), Aleksandar Dragojevic (4), Boris Grot\n  (1), Bozidar Radunovic (4), Yongguang Zhang (4) ((1) University of Edinburgh,\n  (2) Fudan University, (3) UCLA, (4) Microsoft Research)", "title": "Zeus: Locality-aware Distributed Transactions", "comments": "Sixteenth European Conference on Computer Systems", "journal-ref": null, "doi": "10.1145/3447786.3456234", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art distributed in-memory datastores (FaRM, FaSST, DrTM) provide\nstrongly-consistent distributed transactions with high performance and\navailability. Transactions in those systems are fully general; they can\natomically manipulate any set of objects in the store, regardless of their\nlocation. To achieve this, these systems use complex distributed transactional\nprotocols. Meanwhile, many workloads have a high degree of locality. For such\nworkloads, distributed transactions are an overkill as most operations only\naccess objects located on the same server -- if sharded appropriately.\n  In this paper, we show that for these workloads, a single-node transactional\nprotocol combined with dynamic object re-sharding and asynchronously pipelined\nreplication can provide the same level of generality with better performance,\nsimpler protocols, and lower developer effort. We present Zeus, an in-memory\ndistributed datastore that provides general transactions by acquiring all\nobjects involved in the transaction to the same server and executing a\nsingle-node transaction on them. Zeus is fault-tolerant and\nstrongly-consistent. At the heart of Zeus is a reliable dynamic object sharding\nprotocol that can move 250K objects per second per server, allowing Zeus to\nprocess millions of transactions per second and outperform more traditional\ndistributed transactions on a wide range of workloads that exhibit locality.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 07:10:57 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 13:28:53 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Katsarakis", "Antonios", ""], ["Ma", "Yijun", ""], ["Tan", "Zhaowei", ""], ["Bainbridge", "Andrew", ""], ["Balkwill", "Matthew", ""], ["Dragojevic", "Aleksandar", ""], ["Grot", "Boris", ""], ["Radunovic", "Bozidar", ""], ["Zhang", "Yongguang", ""]]}, {"id": "2104.01618", "submitter": "Haijin Wang", "authors": "Haijin Wang, Caomingzhe Si, Junhua Zhao", "title": "A Federated Learning Framework for Non-Intrusive Load Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive load monitoring (NILM) aims at decomposing the total reading of\nthe household power consumption into appliance-wise ones, which is beneficial\nfor consumer behavior analysis as well as energy conservation. NILM based on\ndeep learning has been a focus of research. To train a better neural network,\nit is necessary for the network to be fed with massive data containing various\nappliances and reflecting consumer behavior habits. Therefore, data cooperation\namong utilities and DNOs (distributed network operators) who own the NILM data\nhas been increasingly significant. During the cooperation, however, risks of\nconsumer privacy leakage and losses of data control rights arise. To deal with\nthe problems above, a framework to improve the performance of NILM with\nfederated learning (FL) has been set up. In the framework, model weights\ninstead of the local data are shared among utilities. The global model is\ngenerated by weighted averaging the locally-trained model weights to gather the\nlocally-trained model information. Optimal model selection help choose the\nmodel which adapts to the data from different domains best. Experiments show\nthat this proposal improves the performance of local NILM runners. The\nperformance of this framework is close to that of the centrally-trained model\nobtained by the convergent data without privacy protection.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 14:24:50 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Haijin", ""], ["Si", "Caomingzhe", ""], ["Zhao", "Junhua", ""]]}, {"id": "2104.01688", "submitter": "Anthony Boulmier", "authors": "Anthony Boulmier, Nabil Abdennadher, Bastien Chopard", "title": "Optimal Load Balancing and Assessment of Existing Load Balancing\n  Criteria", "comments": "Preprint submitted to the Journal of Parallel and Distributed\n  Computing (JPDC), 43 pages, 11 figures, 2 algorithms, 4 tables, 14 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel iterative applications often suffer from load imbalance, one of the\nmost critical performance degradation factors. Hence, load balancing techniques\nare used to distribute the workload evenly to maximize performance. A key\nchallenge is to know \\textit{when} to use load balancing techniques. In\ngeneral, this is done through load balancing criteria, which trigger load\nbalancing based on runtime application data and/or user-defined information. In\nthe first part of this paper, we introduce a novel, automatic load balancing\ncriterion derived from a simple mathematical model. In the second part, we\npropose a branch-and-bound algorithm to find the load balancing iterations that\nlead to the optimal application performance. This algorithm finds the optimal\nload balancing scenario in quadratic time while, to the best of our knowledge,\nthis has never been addressed in less than an exponential time. Finally, we\ncompare the performance of the scenarios produced by state-of-the-art load\nbalancing criteria relative to the optimal load balancing scenario in synthetic\nbenchmarks and parallel N-body simulations. In the synthetic benchmarks, we\nobserve that the proposed criterion outperforms the other automatic criteria.\nIn the numerical experiments, we show that our new criterion is, on average,\n$4.9\\%$ faster than state-of-the-art load balancing criteria and can outperform\nthem by up to $17.6\\%$. Moreover, we see in the numerical study that the\nstate-of-the-art automatic criteria are at worst $47.4\\%$ slower than the\noptimum and at best $16.5\\%$ slower.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 20:32:04 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Boulmier", "Anthony", ""], ["Abdennadher", "Nabil", ""], ["Chopard", "Bastien", ""]]}, {"id": "2104.01789", "submitter": "Guannan Lou", "authors": "Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, Qing-Long\n  Han", "title": "Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and\n  Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of artificial intelligence, especially deep learning\ntechnology, has advanced autonomous driving systems (ADSs) by providing precise\ncontrol decisions to counterpart almost any driving event, spanning from\nanti-fatigue safe driving to intelligent route planning. However, ADSs are\nstill plagued by increasing threats from different attacks, which could be\ncategorized into physical attacks, cyberattacks and learning-based adversarial\nattacks. Inevitably, the safety and security of deep learning-based autonomous\ndriving are severely challenged by these attacks, from which the\ncountermeasures should be analyzed and studied comprehensively to mitigate all\npotential risks. This survey provides a thorough analysis of different attacks\nthat may jeopardize ADSs, as well as the corresponding state-of-the-art defense\nmechanisms. The analysis is unrolled by taking an in-depth overview of each\nstep in the ADS workflow, covering adversarial attacks for various deep\nlearning models and attacks in both physical and cyber context. Furthermore,\nsome promising research directions are suggested in order to improve deep\nlearning-based autonomous driving safety, including model robustness training,\nmodel testing and verification, and anomaly detection based on cloud/edge\nservers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 06:31:47 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 02:28:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deng", "Yao", ""], ["Zhang", "Tiehua", ""], ["Lou", "Guannan", ""], ["Zheng", "Xi", ""], ["Jin", "Jiong", ""], ["Han", "Qing-Long", ""]]}, {"id": "2104.01864", "submitter": "Rakshit Naidu", "authors": "Aman Priyanshu, Rakshit Naidu", "title": "FedPandemic: A Cross-Device Federated Learning Approach Towards\n  Elementary Prognosis of Diseases During a Pandemic", "comments": "4+10 pages. To be presented at the DPML and MLPCP workshops at\n  ICLR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of data, manpower and capital required to understand, evaluate and\nagree on a group of symptoms for the elementary prognosis of pandemic diseases\nis enormous. In this paper, we present FedPandemic, a novel noise\nimplementation algorithm integrated with cross-device Federated learning for\nElementary symptom prognosis during a pandemic, taking COVID-19 as a case\nstudy. Our results display consistency and enhance robustness in recovering the\ncommon symptoms displayed by the disease, paving a faster and cheaper path\ntowards symptom retrieval while also preserving the privacy of patient's\nsymptoms via Federated learning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 12:01:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Priyanshu", "Aman", ""], ["Naidu", "Rakshit", ""]]}, {"id": "2104.01929", "submitter": "Nik Sultana", "authors": "Andr\\'e DeHon, Hans Giesen, Nik Sultana, Yuanlong Xiao", "title": "Meta-level issues in Offloading: Scoping, Composition, Development, and\n  their Automation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.NI cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper argues for an accelerator development toolchain that takes into\naccount the whole system containing the accelerator. With whole-system\nvisibility, the toolchain can better assist accelerator scoping and composition\nin the context of the expected workloads and intended performance objectives.\nDespite being focused on the 'meta-level' of accelerators, this would build on\nexisting and ongoing DSLs and toolchains for accelerator design. Basing this on\nour experience in programmable networking and reconfigurable-hardware\nprogramming, we propose an integrative approach that relies on three\nactivities: (i) generalizing the focus of acceleration to offloading to\naccommodate a broader variety of non-functional needs -- such as security and\npower use -- while using similar implementation approaches, (ii) discovering\nwhat to offload, and to what hardware, through semi-automated analysis of a\nwhole system that might compose different offload choices that changeover time,\n(iii) connecting with research and state-of-the-art approaches for using\ndomain-specific languages (DSLs) and high-level synthesis (HLS) systems for\ncustom offload development. We outline how this integration can drive new\ndevelopment tooling that accepts models of programs and resources to assist\nsystem designers through design-space exploration for the accelerated system.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:16:54 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["DeHon", "Andr\u00e9", ""], ["Giesen", "Hans", ""], ["Sultana", "Nik", ""], ["Xiao", "Yuanlong", ""]]}, {"id": "2104.01981", "submitter": "Jack Kosaian", "authors": "Kaige Liu, Jack Kosaian, K. V. Rashmi", "title": "ECRM: Efficient Fault Tolerance for Recommendation Model Training via\n  Erasure Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning-based recommendation models (DLRMs) are widely deployed to\nserve personalized content to users. DLRMs are large in size due to their use\nof large embedding tables, and are trained by distributing the model across the\nmemory of tens or hundreds of servers. Server failures are common in such large\ndistributed systems and must be mitigated to enable training to progress.\nCheckpointing is the primary approach used for fault tolerance in these\nsystems, but incurs significant training-time overhead both during normal\noperation and when recovering from failures. As these overheads increase with\nDLRM size, checkpointing is slated to become an even larger overhead for future\nDLRMs, which are expected to grow in size. This calls for rethinking fault\ntolerance in DLRM training.\n  We present ECRM, a DLRM training system that achieves efficient fault\ntolerance using erasure coding. ECRM chooses which DLRM parameters to encode,\ncorrectly and efficiently updates parities, and enables training to proceed\nwithout any pauses, while maintaining consistency of the recovered parameters.\nWe implement ECRM atop XDL, an open-source, industrial-scale DLRM training\nsystem. Compared to checkpointing, ECRM reduces training-time overhead for\nlarge DLRMs by up to 88%, recovers from failures up to 10.3$\\times$ faster, and\nallows training to proceed during recovery. These results show the promise of\nerasure coding in imparting efficient fault tolerance to training current and\nfuture DLRMs.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:16:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liu", "Kaige", ""], ["Kosaian", "Jack", ""], ["Rashmi", "K. V.", ""]]}, {"id": "2104.02188", "submitter": "Yaosheng Fu", "authors": "Yaosheng Fu, Evgeny Bolotin, Niladrish Chatterjee, David Nellans,\n  Stephen W. Keckler", "title": "GPU Domain Specialization via Composable On-Package Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As GPUs scale their low precision matrix math throughput to boost deep\nlearning (DL) performance, they upset the balance between math throughput and\nmemory system capabilities. We demonstrate that converged GPU design trying to\naddress diverging architectural requirements between FP32 (or larger) based HPC\nand FP16 (or smaller) based DL workloads results in sub-optimal configuration\nfor either of the application domains. We argue that a Composable On-PAckage\nGPU (COPAGPU) architecture to provide domain-specialized GPU products is the\nmost practical solution to these diverging requirements. A COPA-GPU leverages\nmulti-chip-module disaggregation to support maximal design reuse, along with\nmemory system specialization per application domain. We show how a COPA-GPU\nenables DL-specialized products by modular augmentation of the baseline GPU\narchitecture with up to 4x higher off-die bandwidth, 32x larger on-package\ncache, 2.3x higher DRAM bandwidth and capacity, while conveniently supporting\nscaled-down HPC-oriented designs. This work explores the microarchitectural\ndesign necessary to enable composable GPUs and evaluates the benefits\ncomposability can provide to HPC, DL training, and DL inference. We show that\nwhen compared to a converged GPU design, a DL-optimized COPA-GPU featuring a\ncombination of 16x larger cache capacity and 1.6x higher DRAM bandwidth scales\nper-GPU training and inference performance by 31% and 35% respectively and\nreduces the number of GPU instances by 50% in scale-out training scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 23:06:50 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Fu", "Yaosheng", ""], ["Bolotin", "Evgeny", ""], ["Chatterjee", "Niladrish", ""], ["Nellans", "David", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "2104.02396", "submitter": "Tobias Pfandzelter", "authors": "Tobias Pfandzelter and Jonathan Hasenburg and David Bermbach", "title": "Towards a Computing Platform for the LEO Edge", "comments": "Accepted for Publication at 4th International Workshop on Edge\n  Systems, Analytics and Networking (EdgeSys '21), April 26, 2021, Online,\n  United Kingdom", "journal-ref": null, "doi": "10.1145/3434770.3459736", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new space race is heating up as private companies such as SpaceX and\nAmazon are building large satellite constellations in low-earth orbit (LEO) to\nprovide global broadband internet access. As the number of subscribers\nconnected to this access network grows, it becomes necessary to investigate if\nand how edge computing concepts can be applied to LEO satellite networks.\n  In this paper, we discuss the unique characteristics of the LEO edge and\nanalyze the suitability of three organization paradigms for applications\nconsidering developer requirements. We conclude that the serverless approach is\nthe most promising solution, opening up the field for future research.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 10:04:13 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Pfandzelter", "Tobias", ""], ["Hasenburg", "Jonathan", ""], ["Bermbach", "David", ""]]}, {"id": "2104.02421", "submitter": "Xiangqiang Gao", "authors": "Xiangqiang Gao, Rongke Liu (Senior Member, IEEE), Aryan Kaushik\n  (Member, IEEE)", "title": "A Distributed Virtual Network Function Placement Approach in Satellite\n  Edge and Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite edge computing has become a promising way to provide computing\nservices for Internet of Things (IoT) devices in remote areas, which are out of\nthe coverage of terrestrial networks, nevertheless, it is not suitable for\nlarge-scale IoT devices due to the resource limitation of satellites. Cloud\ncomputing can provide sufficient available resources for IoT devices but it\ndoes not meet the service requirements of delay sensitive users as high network\nlatency. Collaborative edge and cloud computing is to facilitate flexible\nservice provisioning for numerous IoT devices by incorporating the advantages\nof edge and cloud computing. In this paper, we investigate the virtual network\nfunction (VNF) placement problem in collaborative satellite edge and cloud\ncomputing to minimize the satellite network bandwidth usage and the service\nend-to-end delay. We formulate the VNF placement problem as an integer\nnon-linear programming problem and propose a distributed VNF placement (D-VNFP)\nalgorithm to address it, as the VNF placement problem is NP-hard. The\nexperiments are conducted to evaluate the effectiveness of the proposed D-VNFP\nalgorithm. The results show that the proposed D-VNFP algorithm outperforms the\nexisting baseline algorithms of Greedy and Viterbi for solving the VNF\nplacement problem in satellite edge and cloud computing.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 10:54:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gao", "Xiangqiang", "", "Senior Member, IEEE"], ["Liu", "Rongke", "", "Senior Member, IEEE"], ["Kaushik", "Aryan", "", "Member, IEEE"]]}, {"id": "2104.02423", "submitter": "Andrew Jeffery", "authors": "Andrew Jeffery, Heidi Howard, Richard Mortier", "title": "Rearchitecting Kubernetes for the Edge", "comments": "6 pages. Accepted in EdgeSys '21", "journal-ref": "Proceedings of the 4th International Workshop on Edge Systems,\n  Analytics and Networking (2021) 7-12", "doi": "10.1145/3434770.3459730", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent years have seen Kubernetes emerge as a primary choice for container\norchestration. Kubernetes largely targets the cloud environment but new use\ncases require performant, available and scalable orchestration at the edge.\nKubernetes stores all cluster state in etcd, a strongly consistent key-value\nstore. We find that at larger etcd cluster sizes, offering higher availability,\nwrite request latency significantly increases and throughput decreases\nsimilarly. Coupled with approximately 30% of Kubernetes requests being writes,\nthis directly impacts the request latency and availability of Kubernetes,\nreducing its suitability for the edge. We revisit the requirement of strong\nconsistency and propose an eventually consistent approach instead. This enables\nhigher performance, availability and scalability whilst still supporting the\nbroad needs of Kubernetes. This aims to make Kubernetes much more suitable for\nperformance-critical, dynamically-scaled edge solutions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 10:56:42 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jeffery", "Andrew", ""], ["Howard", "Heidi", ""], ["Mortier", "Richard", ""]]}, {"id": "2104.02426", "submitter": "Hanhui Deng", "authors": "Di Wu, Xiang Nie, Hanhui Deng, and Zhijin Qin", "title": "Software Defined Edge Computing for Distributed Management and Scalable\n  Control in IoT Multinetworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, edge computing has emerged as a promising paradigm to support\nmobile access in IoT multinetworks. However, coexistence of heterogeneous\nwireless communication schemes brings about new challenges to the mobility\nmanagement and access control in such networks. To maintain its computing\ncontinuum, Software-Defined Networking (SDN) architecture can be applied to the\ndistributed edge networks, which provides consistent frameworks for mobility\nmanagement and creates secure mechanisms for access control. In this paper, we\nfirst review the state-of-the-art SDN technology and its applications in edge\ncomputing. In particular, we highlight the prominent issues related to mobility\nmanagement and access control for SDN in the edge computing environment. We\nalso introduce a number of effective solutions to these issues by presenting\nreal-world testbed experiments, prototypes and numerical analysis of adopting\nSDN based edge computing paradigms for IoT multinetworks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:10:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wu", "Di", ""], ["Nie", "Xiang", ""], ["Deng", "Hanhui", ""], ["Qin", "Zhijin", ""]]}, {"id": "2104.02463", "submitter": "William T\\\"arneberg", "authors": "Lars Larsson, William T\\\"arneberg, Cristian Klein, Maria Kihl, and\n  Erik Elmroth", "title": "Towards Soft Circuit Breaking in Service Meshes via Application-agnostic\n  Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service meshes factor out code dealing with inter-micro-service\ncommunication, such as circuit breaking. Circuit breaking actuation is\ncurrently limited to an \"on/off\" switch, i.e., a tripped circuit breaker will\nreturn an application-level error indicating service unavailability to the\ncalling micro-service. This paper proposes a soft circuit breaker actuator,\nwhich returns cached data instead of an error. The overall resilience of a\ncloud application is improved if constituent micro-services return stale data,\ninstead of no data at all. While caching is widely employed for serving web\nservice traffic, its usage in inter-micro-service communication is lacking.\nMicro-services responses are highly dynamic, which requires carefully choosing\nadaptive time-to-life caching algorithms. We evaluate our approach through two\nexperiments. First, we quantify the trade-off between traffic reduction and\ndata staleness using a purpose-build service, thereby identifying algorithm\nconfigurations that keep data staleness at about 3% or less while reducing\nnetwork load by up to 30%. Second, we quantify the network load reduction with\nthe micro-service benchmark by Google Cloud called Hipster Shop. Our approach\nresults in caching of about 80% of requests. Results show the feasibility and\nefficiency of our approach, which encourages implementing caching as a circuit\nbreaking actuator in service meshes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:46:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Larsson", "Lars", ""], ["T\u00e4rneberg", "William", ""], ["Klein", "Cristian", ""], ["Kihl", "Maria", ""], ["Elmroth", "Erik", ""]]}, {"id": "2104.02494", "submitter": "Nils-Arne Dreier", "authors": "Nils-Arne Dreier", "title": "Hardware-Oriented Krylov Methods for High-Performance Computing", "comments": "PhD thesis (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Krylov subspace methods are an essential building block in numerical\nsimulation software. The efficient utilization of modern hardware is a\nchallenging problem in the development of these methods. In this work, we\ndevelop Krylov subspace methods to solve linear systems with multiple\nright-hand sides, tailored to modern hardware in high-performance computing. To\nthis end, we analyze an innovative block Krylov subspace framework that allows\nto balance the computational and data-transfer costs to the hardware. Based on\nthe framework, we formulate commonly used Krylov methods. For the CG and\nBiCGStab methods, we introduce a novel stabilization approach as an alternative\nto a deflation strategy. This helps us to retain the block size, thus leading\nto a simpler and more efficient implementation. In addition, we optimize the\nmethods further for distributed memory systems and the communication overhead.\nFor the CG method, we analyze approaches to overlap the communication and\ncomputation and present multiple variants of the CG method, which differ in\ntheir communication properties. Furthermore, we present optimizations of the\northogonalization procedure in the GMRes method. Beside introducing a pipelined\nGram-Schmidt variant that overlaps the global communication with the\ncomputation of inner products, we present a novel orthonormalization method\nbased on the TSQR algorithm, which is communication-optimal and stable. For all\noptimized method, we present tests that show their superiority in a distributed\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:25:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dreier", "Nils-Arne", ""]]}, {"id": "2104.02759", "submitter": "Ivan Geffner", "authors": "Ivan Geffner, Joseph Y. Halpern", "title": "Lower Bounds Implementing Mediators in Asynchronous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abraham, Dolev, Geffner, and Halpern proved that, in asynchronous systems, a\n$(k,t)$-robust equilibrium for $n$ players and a trusted mediator can be\nimplemented without the mediator as long as $n > 4(k+t)$, where an equilibrium\nis $(k,t)$-robust if, roughly speaking, no coalition of $t$ players can\ndecrease the payoff of any of the other players, and no coalition of $k$\nplayers can increase their payoff by deviating. We prove that this bound is\ntight, in the sense that if $n \\le 4(k+t)$ there exist $(k,t)$-robust\nequilibria with a mediator that cannot be implemented by the players alone.\nEven though implementing $(k,t)$-robust mediators seems closely related to\nimplementing asynchronous multiparty $(k+t)$-secure computation \\cite{BCG93},\nto the best of our knowledge there is no known straightforward reduction from\none problem to another. Nevertheless, we show that there is a non-trivial\nreduction from a slightly weaker notion of $(k+t)$-secure computation, which we\ncall $(k+t)$-strict secure computation, to implementing $(k,t)$-robust\nmediators. We prove the desired lower bound by showing that there are functions\non $n$ variables that cannot be $(k+t)$-strictly securely computed if $n \\le\n4(k+t)$. This also provides a simple alternative proof for the well-known lower\nbound of $4t+1$ on asynchronous secure computation in the presence of up to $t$\nmalicious agents.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 19:45:20 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Geffner", "Ivan", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "2104.02783", "submitter": "Orhan Dagdeviren", "authors": "Vahid Khalilpour Akram, Orhan Dagdeviren (Member), Bulent Tavli", "title": "A Coverage-Aware Distributed k-Connectivity Maintenance Algorithm for\n  Arbitrarily Large k in Mobile Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile sensor networks (MSNs) have emerged from the interaction between\nmobile robotics and wireless sensor networks. MSNs can be deployed in harsh\nenvironments, where failures in some nodes can partition MSNs into disconnected\nnetwork segments or reduce the coverage area. A k-connected network can\ntolerate at least k-1 arbitrary node failures without losing its connectivity.\nIn this study, we present a coverage-aware distributed k-connectivity\nmaintenance (restoration) algorithm that generates minimum-cost movements of\nactive nodes after a node failure to preserve a persistent k value subject to a\ncoverage conservation criterion. The algorithm accepts a coverage conservation\nratio (as a trade-off parameter between coverage and movements) and facilitates\ncoverage with the generated movements according to this value. Extensive\nsimulations and testbed experiments reveal that the proposed algorithm restores\nk-connectivity more efficiently than the existing restoration algorithms.\nFurthermore, our algorithm can be utilized to maintain k-connectivity without\nsacrificing the coverage, significantly.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:00:21 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Akram", "Vahid Khalilpour", "", "Member"], ["Dagdeviren", "Orhan", "", "Member"], ["Tavli", "Bulent", ""]]}, {"id": "2104.02880", "submitter": "Rui Yao", "authors": "Rui Yao, Feng Qiu, Kai Sun", "title": "Contingency Analysis Based on Partitioned and Parallel Holomorphic\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the steady-state contingency analysis, the traditional Newton-Raphson\nmethod suffers from non-convergence issues when solving post-outage power flow\nproblems, which hinders the integrity and accuracy of security assessment. In\nthis paper, we propose a novel robust contingency analysis approach based on\nholomorphic embedding (HE). The HE-based simulator guarantees convergence if\nthe true power flow solution exists, which is desirable because it avoids the\ninfluence of numerical issues and provides a credible security assessment\nconclusion. In addition, based on the multi-area characteristics of real-world\npower systems, a partitioned HE (PHE) method is proposed with an\ninterface-based partitioning of HE formulation. The PHE method does not\nundermine the numerical robustness of HE and significantly reduces the\ncomputation burden in large-scale contingency analysis. The PHE method is\nfurther enhanced by parallel or distributed computation to become parallel PHE\n(P${}^\\mathrm{2}$HE). Tests on a 458-bus system, a synthetic 419-bus system and\na large-scale 21447-bus system demonstrate the advantages of the proposed\nmethods in robustness and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 03:08:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yao", "Rui", ""], ["Qiu", "Feng", ""], ["Sun", "Kai", ""]]}, {"id": "2104.02937", "submitter": "Miguel A. Mosteiro", "authors": "Dariusz R. Kowalski and Miguel A. Mosteiro", "title": "Polynomial Anonymous Dynamic Distributed Computing without a Unique\n  Leader", "comments": "arXiv admin note: text overlap with arXiv:1707.04282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Counting the number of nodes in Anonymous Dynamic Networks is enticing from\nan algorithmic perspective: an important computation in a restricted platform\nwith promising applications. Starting with Michail, Chatzigiannakis, and\nSpirakis [19], a flurry of papers sped up the running time guarantees from\ndoubly-exponential to polynomial [16]. There is a common theme across all those\nworks: a distinguished node is assumed to be present, because Counting cannot\nbe solved deterministically without at least one. In the present work we study\nchallenging questions that naturally follow: how to efficiently count with more\nthan one distinguished node, or how to count without any distinguished node.\nMore importantly, what is the minimal information needed about these\ndistinguished nodes and what is the best we can aim for (count precision,\nstochastic guarantees, etc.) without any. We present negative and positive\nresults to answer these questions. To the best of our knowledge, this is the\nfirst work that addresses them.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:12:52 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Kowalski", "Dariusz R.", ""], ["Mosteiro", "Miguel A.", ""]]}, {"id": "2104.03042", "submitter": "Akhil Mathur", "authors": "Akhil Mathur, Daniel J. Beutel, Pedro Porto Buarque de Gusm\\~ao,\n  Javier Fernandez-Marques, Taner Topal, Xinchi Qiu, Titouan Parcollet, Yan\n  Gao, Nicholas D. Lane", "title": "On-device Federated Learning with Flower", "comments": "Accepted at the 2nd On-device Intelligence Workshop @ MLSys 2021.\n  arXiv admin note: substantial text overlap with arXiv:2007.14390", "journal-ref": "On-device Intelligence Workshop at the Fourth Conference on\n  Machine Learning and Systems (MLSys), April 9, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) allows edge devices to collaboratively learn a shared\nprediction model while keeping their training data on the device, thereby\ndecoupling the ability to do machine learning from the need to store data in\nthe cloud. Despite the algorithmic advancements in FL, the support for\non-device training of FL algorithms on edge devices remains poor. In this\npaper, we present an exploration of on-device FL on various smartphones and\nembedded devices using the Flower framework. We also evaluate the system costs\nof on-device FL and discuss how this quantification could be used to design\nmore efficient FL algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 10:42:14 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Mathur", "Akhil", ""], ["Beutel", "Daniel J.", ""], ["de Gusm\u00e3o", "Pedro Porto Buarque", ""], ["Fernandez-Marques", "Javier", ""], ["Topal", "Taner", ""], ["Qiu", "Xinchi", ""], ["Parcollet", "Titouan", ""], ["Gao", "Yan", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2104.03075", "submitter": "Pedro Garcia Lopez", "authors": "Pedro Garcia Lopez, Aleksander Slominski, Michael Behrendt, Bernard\n  Metzler", "title": "Serverless Predictions: 2021-2030", "comments": "arXiv admin note: text overlap with arXiv:2006.01251", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within the next 10 years, advances on resource disaggregation will enable\nfull transparency for most Cloud applications: to run unmodified single-machine\napplications over effectively unlimited remote computing resources. In this\narticle, we present five serverless predictions for the next decade that will\nrealize this vision of transparency -- equivalent to Tim Wagner's Serverless\nSuperComputer or AnyScale's Infinite Laptop proposals.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:46:29 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lopez", "Pedro Garcia", ""], ["Slominski", "Aleksander", ""], ["Behrendt", "Michael", ""], ["Metzler", "Bernard", ""]]}, {"id": "2104.03126", "submitter": "Andreas Grammenos", "authors": "Apostolos I. Rikos, Andreas Grammenos, Evangelia Kalyvianaki,\n  Christoforos N. Hadjicostis, Themistoklis Charalambous, Karl H. Johansson", "title": "Optimal CPU Scheduling in Data Centers via a Finite-Time Distributed\n  Quantized Coordination Mechanism", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the problem of optimal task scheduling for data\ncenters. Given the available resources and tasks, we propose a fast distributed\niterative algorithm which operates over a large scale network of nodes and\nallows each of the interconnected nodes to reach agreement to an optimal\nsolution in a finite number of time steps. More specifically, the algorithm (i)\nis guaranteed to converge to the exact optimal scheduling plan in a finite\nnumber of time steps and, (ii) once the goal of task scheduling is achieved, it\nexhibits distributed stopping capabilities (i.e., it allows the nodes to\ndistributely determine whether they can terminate the operation of the\nalgorithm). Furthermore, the proposed algorithm operates exclusively with\nquantized values (i.e., the information stored, processed and exchanged between\nneighboring agents is subject to deterministic uniform quantization) and relies\non event-driven updates (e.g., to reduce energy consumption, communication\nbandwidth, network congestion, and/or processor usage). We also provide\nexamples to illustrate the operation, performance, and potential advantages of\nthe proposed algorithm. Finally, by using extensive empirical evaluations\nthrough simulations we show that the proposed algorithm exhibits\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:56:19 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Rikos", "Apostolos I.", ""], ["Grammenos", "Andreas", ""], ["Kalyvianaki", "Evangelia", ""], ["Hadjicostis", "Christoforos N.", ""], ["Charalambous", "Themistoklis", ""], ["Johansson", "Karl H.", ""]]}, {"id": "2104.03129", "submitter": "Elad Michael Schiller (PhD)", "authors": "Oskar Lundstr\\\"om and Michel Raynal and Elad Michael Schiller", "title": "Self-stabilizing Multivalued Consensus in Asynchronous Crash-prone\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of multivalued consensus is fundamental in the area of\nfault-tolerant distributed computing since it abstracts a very broad set of\nagreement problems in which processes have to uniformly decide on a specific\nvalue v in V, where |V| >1. Existing solutions (that tolerate process failures)\nreduce the multivalued consensus problem to the one of binary consensus, e.g.,\nMostefaoui-Raynal-Tronel and Zhang-Chen.\n  Our study aims at the design of an even more reliable solution. We do so\nthrough the lenses of self-stabilization -- a very strong notion of\nfault-tolerance. In addition to node and communication failures,\nself-stabilizing algorithms can recover after the occurrence of arbitrary\ntransient-faults; these faults represent any violation of the assumptions\naccording to which the system was designed to operate (as long as the algorithm\ncode stays intact).\n  This work proposes the first (to the best of our knowledge) self-stabilizing\nalgorithm for multivalued consensus for asynchronous message-passing systems\nprone to process failures and arbitrary transient-faults. Our solution is also\nthe first (to the best of our knowledge) to support wait-freedom. Moreover,\nusing piggybacking techniques, our solution can invoke n binary consensus\nobjects concurrently. Thus, the proposed self-stabilizing wait-free solution\ncan terminate using fewer resources than earlier non-self-stabilizing solutions\nby Mostefaoui, Raynal, and Tronel, which uses an unbounded number of binary\nconsensus objects, or Zhang and Chen, which is not wait-free.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:59:49 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lundstr\u00f6m", "Oskar", ""], ["Raynal", "Michel", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "2104.03186", "submitter": "Simo S\\\"arkk\\\"a", "authors": "Simo S\\\"arkk\\\"a, \\'Angel F. Garc\\'ia-Fern\\'andez", "title": "Temporal Parallelisation of Dynamic Programming and Linear Quadratic\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for temporal parallelisation of dynamic\nprogramming solutions of optimal control problems. We also derive the temporal\nparallelisation of the linear quadratic tracking control problem. For these two\nproblems, we derive the elements and associative operators to be able to use\nparallel scans to solve these problems with logarithmic time complexity rather\nthan linear time complexity. The computational benefits of the parallel methods\nare demonstrated via numerical simulations run on a multi-core processor and a\ngraphics processing unit.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:25:50 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["S\u00e4rkk\u00e4", "Simo", ""], ["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""]]}, {"id": "2104.03187", "submitter": "Pierangelo Di Sanzo", "authors": "Pierangelo Di Sanzo", "title": "A Preliminary Proposal for an Analytical Model for Evaluating the Impact\n  on Performance of Data Access Patterns in Transaction Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a preliminary proposal for an analytical model for evaluating the\nimpact on performance of data access patterns in concurrent transaction\nexecution. We consider the case of concurrency control protocols that use\nlocking to ensure isolation in the execution of transactions. We analyse\nscenarios where transactions access one or more sets of data items in the same\norder or in different order.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:29:12 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 17:42:19 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 10:49:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Di Sanzo", "Pierangelo", ""]]}, {"id": "2104.03277", "submitter": "Venkatraman Ramakrishna", "authors": "Bishakh Chandra Ghosh, Venkatraman Ramakrishna, Chander Govindarajan,\n  Dushyant Behl, Dileban Karunamoorthy, Ermyas Abebe, Sandip Chakraborty", "title": "Decentralized Cross-Network Identity Management for Blockchain\n  Interoperation", "comments": "9 pages, 5 figures, accepted for publication in the proceedings of\n  the IEEE International Conference on Blockchain and Cryptocurrency (ICBC)\n  2021", "journal-ref": null, "doi": "10.1109/ICBC51069.2021.9461064", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interoperation for data sharing between permissioned blockchain networks\nrelies on networks' abilities to independently authenticate requests and\nvalidate proofs accompanying the data; these typically contain digital\nsignatures. This requires counterparty networks to know the identities and\ncertification chains of each other's members, establishing a common trust basis\nrooted in identity. But permissioned networks are ad hoc consortia of existing\norganizations, whose network affiliations may not be well-known or\nwell-established even though their individual identities are. In this paper, we\ndescribe an architecture and set of protocols for distributed identity\nmanagement across permissioned blockchain networks to establish a trust basis\nfor data sharing. Networks wishing to interoperate can associate with one or\nmore distributed identity registries that maintain credentials on shared\nledgers managed by groups of reputed identity providers. A network's\nparticipants possess self-sovereign decentralized identities (DIDs) on these\nregistries and can obtain privacy-preserving verifiable membership credentials.\nDuring interoperation, networks can securely and dynamically discover each\nothers' latest membership lists and members' credentials. We implement a\nsolution based on Hyperledger Indy and Aries, and demonstrate its viability and\nusefulness by linking a trade finance network with a trade logistics network,\nboth built on Hyperledger Fabric. We also analyze the extensibility, security,\nand trustworthiness of our system.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:31:45 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ghosh", "Bishakh Chandra", ""], ["Ramakrishna", "Venkatraman", ""], ["Govindarajan", "Chander", ""], ["Behl", "Dushyant", ""], ["Karunamoorthy", "Dileban", ""], ["Abebe", "Ermyas", ""], ["Chakraborty", "Sandip", ""]]}, {"id": "2104.03354", "submitter": "Shantanu Sharma", "authors": "Yin Li, Dhrubajyoti Ghosh, Peeyush Gupta, Sharad Mehrotra, Nisha\n  Panwar, Shantanu Sharma", "title": "Prism: Private Verifiable Set Computation over Multi-Owner Outsourced\n  Databases", "comments": "This paper has been accepted in ACM SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Prism, a secret sharing based approach to compute private\nset operations (i.e., intersection and union), as well as aggregates over\noutsourced databases belonging to multiple owners. Prism enables data owners to\npre-load the data onto non-colluding servers and exploits the additive and\nmultiplicative properties of secret-shares to compute the above-listed\noperations in (at most) two rounds of communication between the servers\n(storing the secret-shares) and the querier, resulting in a very efficient\nimplementation. Also, Prism does not require communication among the servers\nand supports result verification techniques for each operation to detect\nmalicious adversaries. Experimental results show that Prism scales both in\nterms of the number of data owners and database sizes, to which prior\napproaches do not scale.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:08:15 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Yin", ""], ["Ghosh", "Dhrubajyoti", ""], ["Gupta", "Peeyush", ""], ["Mehrotra", "Sharad", ""], ["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""]]}, {"id": "2104.03368", "submitter": "Andre Luckow", "authors": "Andre Luckow and Kartik Rattan and Shantenu Jha", "title": "Exploring Task Placement for Edge-to-Cloud Applications using Emulation", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vast and growing number of IoT applications connect physical devices, such\nas scientific instruments, technical equipment, machines, and cameras, across\nheterogenous infrastructure from the edge to the cloud to provide responsive,\nintelligent services while complying with privacy and security requirements.\nHowever, the integration of heterogeneous IoT, edge, and cloud technologies and\nthe design of end-to-end applications that seamlessly work across multiple\nlayers and types of infrastructures is challenging. A significant issue is\nresource management and the need to ensure that the right type and scale of\nresources is allocated on every layer to fulfill the application's processing\nneeds. As edge and cloud layers are increasingly tightly integrated, imbalanced\nresource allocations and sub-optimally placed tasks can quickly deteriorate the\noverall system performance. This paper proposes an emulation approach for the\ninvestigation of task placements across the edge-to-cloud continuum. We\ndemonstrate that emulation can address the complexity and many\ndegrees-of-freedom of the problem, allowing us to investigate essential\ndeployment patterns and trade-offs. We evaluate our approach using a machine\nlearning-based workload, demonstrating the validity by comparing emulation and\nreal-world experiments. Further, we show that the right task placement strategy\nhas a significant impact on performance -- in our experiments, between 5% and\n65% depending on the scenario.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:42:59 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Luckow", "Andre", ""], ["Rattan", "Kartik", ""], ["Jha", "Shantenu", ""]]}, {"id": "2104.03374", "submitter": "Andre Luckow", "authors": "Andre Luckow, Kartik Rattan, and Shantenu Jha", "title": "Pilot-Edge: Distributed Resource Management Along the Edge-to-Cloud\n  Continuum", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many science and industry IoT applications necessitate data processing across\nthe edge-to-cloud continuum to meet performance, security, cost, and privacy\nrequirements. However, diverse abstractions and infrastructures for managing\nresources and tasks across the edge-to-cloud scenario are required. We propose\nPilot-Edge as a common abstraction for resource management across the\nedge-to-cloud continuum. Pilot-Edge is based on the pilot abstraction, which\ndecouples resource and workload management, and provides a\nFunction-as-a-Service (FaaS) interface for application-level tasks. The\nabstraction allows applications to encapsulate common functions in high-level\ntasks that can then be configured and deployed across the continuum. We\ncharacterize Pilot-Edge on geographically distributed infrastructures using\nmachine learning workloads (e.g., k-means and auto-encoders). Our experiments\ndemonstrate how Pilot-Edge manages distributed resources and allows\napplications to evaluate task placement based on multiple factors (e.g., model\ncomplexities, throughput, and latency).\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 20:04:41 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Luckow", "Andre", ""], ["Rattan", "Kartik", ""], ["Jha", "Shantenu", ""]]}, {"id": "2104.03490", "submitter": "Xin Fan", "authors": "Xin Fan, Yue Wang, Yan Huo, and Zhi Tian", "title": "Joint Optimization of Communications and Federated Learning Over the Air", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is an attractive paradigm for making use of rich\ndistributed data while protecting data privacy. Nonetheless, nonideal\ncommunication links and limited transmission resources have become the\nbottleneck of the implementation of fast and accurate FL. In this paper, we\nstudy joint optimization of communications and FL based on analog aggregation\ntransmission in realistic wireless networks. We first derive a closed-form\nexpression for the expected convergence rate of FL over the air, which\ntheoretically quantifies the impact of analog aggregation on FL. Based on the\nanalytical result, we develop a joint optimization model for accurate FL\nimplementation, which allows a parameter server to select a subset of workers\nand determine an appropriate power scaling factor. Since the practical setting\nof FL over the air encounters unobservable parameters, we reformulate the joint\noptimization of worker selection and power allocation using controlled\napproximation. Finally, we efficiently solve the resulting mixed-integer\nprogramming problem via a simple yet optimal finite-set search method by\nreducing the search space. Simulation results show that the proposed solutions\ndeveloped for realistic wireless analog channels outperform a benchmark method,\nand achieve comparable performance of the ideal case where FL is implemented\nover noise-free wireless channels.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 03:38:31 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Fan", "Xin", ""], ["Wang", "Yue", ""], ["Huo", "Yan", ""], ["Tian", "Zhi", ""]]}, {"id": "2104.03673", "submitter": "J\\'er\\'emie Decouchant", "authors": "Silvia Bonomi and J\\'er\\'emie Decouchant and Giovanni Farina and\n  Vincent Rahli and S\\'ebastien Tixeuil", "title": "Practical Byzantine Reliable Broadcast on Partially Connected Networks", "comments": "This is a preprint of a paper that will appear at the IEEE ICDCS 2021\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we consider the Byzantine reliable broadcast problem on\nauthenticated and partially connected networks. The state-of-the-art method to\nsolve this problem consists in combining two algorithms from the literature.\nHandling asynchrony and faulty senders is typically done thanks to Gabriel\nBracha's authenticated double-echo broadcast protocol, which assumes an\nasynchronous fully connected network. Danny Dolev's algorithm can then be used\nto provide reliable communications between processes in the global fault model,\nwhere up to f processes among N can be faulty in a communication network that\nis at least 2f+1-connected. Following recent works that showed that Dolev's\nprotocol can be made more practical thanks to several optimizations, we show\nthat the state-of-the-art methods to solve our problem can be optimized thanks\nto layer-specific and cross-layer optimizations. Our simulations with the\nOmnet++ network simulator show that these optimizations can be efficiently\ncombined to decrease the total amount of information transmitted or the\nprotocol's latency (e.g., respectively, -25% and -50% with a 16B payload, N=31\nand f=4) compared to the state-of-the-art combination of Bracha's and Dolev's\nprotocols.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:43:32 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bonomi", "Silvia", ""], ["Decouchant", "J\u00e9r\u00e9mie", ""], ["Farina", "Giovanni", ""], ["Rahli", "Vincent", ""], ["Tixeuil", "S\u00e9bastien", ""]]}, {"id": "2104.03731", "submitter": "Christian G\\\"ottel", "authors": "Christian G\\\"ottel, Rafael Pires, Isabelly Rocha, S\\'ebastien Vaucher,\n  Pascal Felber, Marcelo Pasin, Valerio Schiavoni", "title": "Security, Performance and Energy Implications of Hardware-assisted\n  Memory Protection Mechanisms on Event-based Streaming Systems", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": "2018 IEEE 37th Symposium on Reliable Distributed Systems (SRDS),\n  Salvador, Brazil, 2018, pp. 264-266", "doi": "10.1109/SRDS.2018.00042", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major cloud providers such as Amazon, Google and Microsoft provide nowadays\nsome form of infrastructure as a service (IaaS) which allows deploying services\nin the form of virtual machines, containers or bare-metal instances. Although\nsoftware-based solutions like homomorphic encryption exit, privacy concerns\ngreatly hinder the deployment of such services over public clouds. It is\nparticularly difficult for homomorphic encryption to match performance\nrequirements of modern workloads. Evaluating simple operations on basic data\ntypes with HElib, a homomorphic encryption library, against their unencrypted\ncounter part reveals that homomorphic encryption is still impractical under\nrealistic workloads.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:29:15 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 06:26:16 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["G\u00f6ttel", "Christian", ""], ["Pires", "Rafael", ""], ["Rocha", "Isabelly", ""], ["Vaucher", "S\u00e9bastien", ""], ["Felber", "Pascal", ""], ["Pasin", "Marcelo", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2104.03834", "submitter": "Jinu Gong", "authors": "Jinu Gong, Osvaldo Simeone, Joonhyuk Kang", "title": "Bayesian Variational Federated Learning and Unlearning in Decentralized\n  Networks", "comments": "Submitted for conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Bayesian learning offers a principled framework for the definition\nof collaborative training algorithms that are able to quantify epistemic\nuncertainty and to produce trustworthy decisions. Upon the completion of\ncollaborative training, an agent may decide to exercise her legal \"right to be\nforgotten\", which calls for her contribution to the jointly trained model to be\ndeleted and discarded. This paper studies federated learning and unlearning in\na decentralized network within a Bayesian framework. It specifically develops\nfederated variational inference (VI) solutions based on the decentralized\nsolution of local free energy minimization problems within exponential-family\nmodels and on local gossip-driven communication. The proposed protocols are\ndemonstrated to yield efficient unlearning mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:18:35 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Gong", "Jinu", ""], ["Simeone", "Osvaldo", ""], ["Kang", "Joonhyuk", ""]]}, {"id": "2104.04060", "submitter": "Brice Ekane Apah", "authors": "Brice Ekane, Yohan Pipereau, Boris Teabe, Alain Tchana, Gael Thomas,\n  Noel de palma, Daniel Hagimont", "title": "Network in Disaggregated Datacenters", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, datacenters lean on a computer-centric approach based on monolithic\nservers which include all necessary hardware resources (mainly CPU, RAM,\nnetwork and disks) to run applications. Such an architecture comes with two\nmain limitations: (1) difficulty to achieve full resource utilization and (2)\ncoarse granularity for hardware maintenance. Recently, many works investigated\na resource-centric approach called disaggregated architecture where the\ndatacenter is composed of self-content resource boards interconnected using\nfast interconnection technologies, each resource board including instances of\none resource type. The resource-centric architecture allows each resource to be\nmanaged (maintenance, allocation) independently. LegoOS is the first work which\nstudied the implications of disaggregation on the operating system, proposing\nto disaggregate the operating system itself. They demonstrated the suitability\nof this approach, considering mainly CPU and RAM resources. However, they\ndidn't study the implication of disaggregation on network resources. We\nreproduced a LegoOS infrastructure and extended it to support disaggregated\nnetworking. We show that networking can be disaggregated following the same\nprinciples, and that classical networking optimizations such as DMA, DDIO or\nloopback can be reproduced in such an environment. Our evaluations show the\nviability of the approach and the potential of future disaggregated\ninfrastructures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 23:27:02 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ekane", "Brice", ""], ["Pipereau", "Yohan", ""], ["Teabe", "Boris", ""], ["Tchana", "Alain", ""], ["Thomas", "Gael", ""], ["de palma", "Noel", ""], ["Hagimont", "Daniel", ""]]}, {"id": "2104.04102", "submitter": "Michael Whittaker", "authors": "Michael Whittaker, Aleksey Charapko, Joseph M. Hellerstein, Heidi\n  Howard, Ion Stoica", "title": "Read-Write Quorum Systems Made Practical", "comments": "To be published in PaPoC 2021\n  (https://papoc-workshop.github.io/2021/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quorum systems are a powerful mechanism for ensuring the consistency of\nreplicated data. Production systems usually opt for majority quorums due to\ntheir simplicity and fault tolerance, but majority quorum systems provide poor\nthroughput and scalability. Alternatively, researchers have invented a number\nof theoretically \"optimal\" quorum systems, but the underlying theory ignores\nmany practical complexities such as machine heterogeneity and workload skew. In\nthis paper, we conduct a pragmatic re-examination of quorum systems. We enrich\nthe current theory on quorum systems with a number of practical refinements to\nfind quorum systems that provide higher throughput, lower latency, and lower\nnetwork load. We also develop a library Quoracle that precisely quantifies the\navailable trade-offs between quorum systems to empower engineers to find\noptimal quorum systems, given a set of objectives for specific deployments and\nworkloads. Our tool is available online at:\nhttps://github.com/mwhittaker/quoracle.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 22:45:16 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Whittaker", "Michael", ""], ["Charapko", "Aleksey", ""], ["Hellerstein", "Joseph M.", ""], ["Howard", "Heidi", ""], ["Stoica", "Ion", ""]]}, {"id": "2104.04141", "submitter": "Chunnnan Wang", "authors": "Chunnan Wang, Bozhou Chen, Geng Li, Hongzhi Wang", "title": "FL-AGCNS: Federated Learning Framework for Automatic Graph Convolutional\n  Network Search", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, some Neural Architecture Search (NAS) techniques are proposed for\nthe automatic design of Graph Convolutional Network (GCN) architectures. They\nbring great convenience to the use of GCN, but could hardly apply to the\nFederated Learning (FL) scenarios with distributed and private datasets, which\nlimit their applications. Moreover, they need to train many candidate GCN\nmodels from scratch, which is inefficient for FL. To address these challenges,\nwe propose FL-AGCNS, an efficient GCN NAS algorithm suitable for FL scenarios.\nFL-AGCNS designs a federated evolutionary optimization strategy to enable\ndistributed agents to cooperatively design powerful GCN models while keeping\npersonal information on local devices. Besides, it applies the GCN SuperNet and\na weight sharing strategy to speed up the evaluation of GCN models.\nExperimental results show that FL-AGCNS can find better GCN models in short\ntime under the FL framework, surpassing the state-of-the-arts NAS methods and\nGCN models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:42:06 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wang", "Chunnan", ""], ["Chen", "Bozhou", ""], ["Li", "Geng", ""], ["Wang", "Hongzhi", ""]]}, {"id": "2104.04447", "submitter": "Ramyad Hadidi", "authors": "Ramyad Hadidi, Jiashen Cao, Hyesoon Kim", "title": "Creating Robust Deep Neural Networks With Coded Distributed Computing\n  for IoT Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing interest in serverless computation and ubiquitous wireless\nnetworks has led to numerous connected devices in our surroundings. Among such\ndevices, IoT devices have access to an abundance of raw data, but their\ninadequate resources in computing limit their capabilities. Specifically, with\nthe emergence of deep neural networks (DNNs), not only is the demand for the\ncomputing power of IoT devices increasing but also privacy concerns are pushing\ncomputations towards the edge. To overcome inadequate resources, several\nstudies have proposed the distribution of work among IoT devices. These\npromising methods harvest the aggregated computing power of the idle IoT\ndevices in an environment. However, since such a distributed system strongly\nrelies on each device, unstable latencies, and intermittent failures, the\ncommon characteristics of IoT devices and wireless networks, cause high\nrecovery overheads. To reduce this overhead, we propose a novel robustness\nmethod with a close-to-zero recovery latency for DNN computations. Our solution\nnever loses a request or spends time recovering from a failure. To do so,\nfirst, we analyze the underlying matrix-matrix computations affected by\ndistribution. Then, we introduce a new coded distributed computing (CDC) method\nthat has a constant cost with the increasing number of devices, unlike the\nlinear cost of modular redundancies. Moreover, our method is applied in the\nlibrary level, without requiring extensive changes to the program, while still\nensuring a balanced work assignment during distribution. To illustrate our\nmethod, we perform experiments with distributed systems comprising up to 12\nRaspberry Pis.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:52:35 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hadidi", "Ramyad", ""], ["Cao", "Jiashen", ""], ["Kim", "Hyesoon", ""]]}, {"id": "2104.04473", "submitter": "Deepak Narayanan", "authors": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,\n  Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, Prethvi\n  Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia", "title": "Efficient Large-Scale Language Model Training on GPU Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these large models efficiently is challenging for\ntwo reasons: a) GPU memory capacity is limited, making it impossible to fit\nlarge models on a single GPU or even on a multi-GPU server; and b) the number\nof compute operations required to train these models can result in\nunrealistically long training times. New methods of model parallelism such as\ntensor and pipeline parallelism have been proposed to address these challenges.\nUnfortunately, naive usage leads to fundamental scaling issues at thousands of\nGPUs due to various reasons, e.g., expensive cross-node communication or idle\nperiods waiting on other devices.\n  In this work, we show how to compose different types of parallelism methods\n(tensor, pipeline, and data parallelism) to scale to thousands of GPUs,\nachieving a two-order-of-magnitude increase in the sizes of models we can\nefficiently train compared to existing systems. We survey techniques for\npipeline parallelism and propose a novel interleaved pipeline parallelism\nschedule that can improve throughput by more than 10% with comparable memory\nfootprint compared to previously-proposed approaches. We quantitatively study\nthe trade-offs between tensor, pipeline, and data parallelism, and provide\nintuition as to how to configure distributed training of a large model. Our\napproach allows us to perform training iterations on a model with 1 trillion\nparameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of\n52% of peak; previous efforts to train similar-sized models achieve much lower\nthroughput (36% of theoretical peak). Our code is open sourced at\nhttps://github.com/nvidia/megatron-lm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:43:11 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:44:52 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Narayanan", "Deepak", ""], ["Shoeybi", "Mohammad", ""], ["Casper", "Jared", ""], ["LeGresley", "Patrick", ""], ["Patwary", "Mostofa", ""], ["Korthikanti", "Vijay Anand", ""], ["Vainbrand", "Dmitri", ""], ["Kashinkunti", "Prethvi", ""], ["Bernauer", "Julie", ""], ["Catanzaro", "Bryan", ""], ["Phanishayee", "Amar", ""], ["Zaharia", "Matei", ""]]}, {"id": "2104.04474", "submitter": "Chavit Denninnart", "authors": "Chavit Denninnart, Mohsen Amini Salehi", "title": "Harnessing the Potential of Function-Reuse in Multimedia Cloud Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud-based computing systems can get oversubscribed due to the budget\nconstraints of their users or limitations in certain resource types. The\noversubscription can, in turn, degrade the users perceived Quality of Service\n(QoS). The approach we investigate to mitigate both the oversubscription and\nthe incurred cost is based on smart reusing of the computation needed to\nprocess the service requests (i.e., tasks). We propose a reusing paradigm for\nthe tasks that are waiting for execution. This paradigm can be particularly\nimpactful in serverless platforms where multiple users can request similar\nservices simultaneously. Our motivation is a multimedia streaming engine that\nprocesses the media segments in an on-demand manner. We propose a mechanism to\nidentify various types of \"mergeable\" tasks and aggregate them to improve the\nQoS and mitigate the incurred cost. We develop novel approaches to determine\nwhen and how to perform task aggregation such that the QoS of other tasks is\nnot affected. Evaluation results show that the proposed mechanism can improve\nthe QoS by significantly reducing the percentage of tasks missing their\ndeadlines %. In addition, it can and reduce the overall time (and subsequently\nthe incurred cost) of utilizing cloud services by more than 9%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:45:53 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Denninnart", "Chavit", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2104.04512", "submitter": "Caleb Stanford", "authors": "Konstantinos Kallas, Filip Niksic, Caleb Stanford, Rajeev Alur", "title": "Stream Processing With Dependency-Guided Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time data processing applications with low latency requirements have led\nto the increasing popularity of stream processing systems. While such systems\noffer convenient APIs that can be used to achieve data parallelism\nautomatically, they offer limited support for computations that require\nsynchronization between parallel nodes. In this paper, we propose\n*dependency-guided synchronization (DGS)*, an alternative programming model and\nstream processing API for stateful streaming computations with complex\nsynchronization requirements. In the proposed model, the input is viewed as\npartially ordered, and the program consists of a set of parallelization\nconstructs which are applied to decompose the partial order and process events\nindependently. Our API maps to an execution model called *synchronization\nplans* which supports synchronization between parallel nodes. Our evaluation\nshows that APIs offered by two widely used systems -- Flink and Timely Dataflow\n-- cannot suitably expose parallelism in some representative applications. In\ncontrast, DGS enables implementations with scalable performance, the resulting\nsynchronization plans offer throughput improvements when implemented manually\nin existing systems, and the programming overhead is small compared to writing\nsequential code.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:50:53 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 05:12:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kallas", "Konstantinos", ""], ["Niksic", "Filip", ""], ["Stanford", "Caleb", ""], ["Alur", "Rajeev", ""]]}, {"id": "2104.04579", "submitter": "Iosif Meyerov", "authors": "Valentin Volokitin, Alexey Bashinov, Evgeny Efimenko, Arkady Gonoskov,\n  Iosif Meyerov", "title": "High Performance Implementation of Boris Particle Pusher on DPC++. A\n  First Look at oneAPI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New hardware architectures open up immense opportunities for supercomputer\nsimulations. However, programming techniques for different architectures vary\nsignificantly, which leads to the necessity of developing and supporting\nmultiple code versions, each being optimized for specific hardware features.\nThe oneAPI framework, recently introduced by Intel, contains a set of\nprogramming tools for the development of portable codes that can be compiled\nand fine-tuned for CPUs, GPUs, FPGAs, and accelerators. In this paper, we\nreport on the experience of porting the implementation of Boris particle pusher\nto oneAPI. Boris particle pusher is one of the most demanding computational\nstages of the Particle-in-Cell method, which, in particular, is used for\nsupercomputer simulations of laser-plasma interactions. We show how to adapt\nthe C++ implementation of the particle push algorithm from the Hi-Chi project\nto the DPC++ programming language and report the performance of the code on\nhigh-end Intel CPUs (Xeon Platinum 8260L) and Intel GPUs (P630 and Iris Xe\nMax). It turned out that our C++ code can be easily ported to DPC++. We found\nthat on CPUs the resulting DPC++ code is only ~10% on average inferior to the\noptimized C++ code. Moreover, the code is compiled and run on new Intel GPUs\nwithout any specific optimizations and shows the expected performance, taking\ninto account the parameters of the hardware.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:27:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Volokitin", "Valentin", ""], ["Bashinov", "Alexey", ""], ["Efimenko", "Evgeny", ""], ["Gonoskov", "Arkady", ""], ["Meyerov", "Iosif", ""]]}, {"id": "2104.04649", "submitter": "Velma Jones", "authors": "Velma Jones, Kendra Keyse, Alfredo Melgoza, Karen Perez, Tammy Qamar,\n  Jason Villalpando, Jongwook Woo", "title": "Avocado Buying Trends in the United States Using SAC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of our paper is to analyze the dataset from Hass Avocado Board\n(HAB). The data features historical data on avocado prices and sales volume in\nmultiple cities, states, and regions of the United States, ranging from 2015 to\n2020. The paper consists of a mapped and calculated statistical analysis of the\ndata over an established period. Using the cloud visualization tool SAP\nAnalytics Cloud (SAC) to import and clean the data, we will complete\ncomprehensive investigations and employ class lecture information as needed.\nThen, we will present insights using visualization, and time-series analysis.\nOur research intends to reveal insights into consumer buying statistics, such\nas the most popular type of avocado, preferences of organic to conventional and\nseasonality trends. The research is relevant as health-conscious trends have\nbecome increasingly popular, and avocado purchases indicate this.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 00:03:12 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jones", "Velma", ""], ["Keyse", "Kendra", ""], ["Melgoza", "Alfredo", ""], ["Perez", "Karen", ""], ["Qamar", "Tammy", ""], ["Villalpando", "Jason", ""], ["Woo", "Jongwook", ""]]}, {"id": "2104.04731", "submitter": "Dennis Rieber", "authors": "Dennis Rieber, Axel Acosta, Holger Fr\\\"oning", "title": "Joint Program and Layout Transformations to enable DNN Operators on\n  Specialized Hardware based on Constraint Programming", "comments": "25 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Deep Artificial Neural Networks (DNNs) in many domains created\na rich body of research concerned with hardwareaccelerators for\ncompute-intensive DNN operators. However, implementing such operators\nefficiently with complex hardwareintrinsics such as matrix multiply is a task\nnot yet automated gracefully. Solving this task often requires joint program\nand data layouttransformations. First solutions to this problem have been\nproposed, such as TVM, UNIT or ISAMIR, which work on a loop-levelrepresentation\nof operators and specify data layout and possible program transformations\nbefore the embedding into the operator isperformed. This top-down approach\ncreates a tension between exploration range and search space complexity,\nespecially when alsoexploring data layout transformations such as im2col,\nchannel packing or padding.In this work, we propose a new approach to this\nproblem. We created a bottom-up method that allows the joint transformation\nofboth compuation and data layout based on the found embedding. By formulating\nthe embedding as a constraint satisfaction problemover the scalar dataflow,\nevery possible embedding solution is contained in the search space. Adding\nadditional constraints andoptmization targets to the solver generates the\nsubset of preferable solutions.An evaluation using the VTA hardware accelerator\nwith the Baidu DeepBench inference benchmark shows that our approach\ncanautomatically generate code competitive to reference implementations.\nFurther, we show that dynamically determining the data layoutbased on intrinsic\nand workload is beneficial for hardware utilization and performance. In cases\nwhere the reference implementationhas low hardware utilization due to its fixed\ndeployment strategy, we achieve a geomean speedup of up to x2.813, while\nindividualoperators can improve as much as x170.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 10:39:47 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 06:16:45 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 06:42:29 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Rieber", "Dennis", ""], ["Acosta", "Axel", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2104.04797", "submitter": "Anda Trifan", "authors": "Alexander Brace, Hyungro Lee, Heng Ma, Anda Trifan, Matteo Turilli,\n  Igor Yakushin, Todd Munson, Ian Foster, Shantenu Jha and Arvind Ramanathan", "title": "Achieving 100X faster simulations of complex biological phenomena by\n  coupling ML to HPC ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of ML methods to dynamically steer ensemble-based simulations\npromises significant improvements in the performance of scientific\napplications. We present DeepDriveMD, a tool for a range of prototypical\nML-driven HPC simulation scenarios, and use it to quantify improvements in the\nscientific performance of ML-driven ensemble-based applications. We discuss its\ndesign and characterize its performance. Motivated by the potential for further\nscientific improvements and applicability to more sophisticated physical\nsystems, we extend the design of DeepDriveMD to support stream-based\ncommunication between simulations and learning methods. It demonstrates a 100x\nspeedup to fold proteins, and performs 1.6x more simulations per unit time,\nimproving resource utilization compared to the sequential framework.\nExperiments are performed on leadership-class platforms, at scales of up to\nO(1000) nodes, and for production workloads. We establish DeepDriveMD as a\nhigh-performance framework for ML-driven HPC simulation scenarios, that\nsupports diverse simulation and ML back-ends, and which enables new scientific\ninsights by improving length- and time-scale accessed.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 15:52:39 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 16:35:20 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Brace", "Alexander", ""], ["Lee", "Hyungro", ""], ["Ma", "Heng", ""], ["Trifan", "Anda", ""], ["Turilli", "Matteo", ""], ["Yakushin", "Igor", ""], ["Munson", "Todd", ""], ["Foster", "Ian", ""], ["Jha", "Shantenu", ""], ["Ramanathan", "Arvind", ""]]}, {"id": "2104.04955", "submitter": "R. Baghdadi", "authors": "Riyadh Baghdadi, Massinissa Merouani, Mohamed-Hicham Leghettas, Kamel\n  Abdous, Taha Arbaoui, Karima Benatchba, Saman Amarasinghe", "title": "A Deep Learning Based Cost Model for Automatic Code Optimization", "comments": null, "journal-ref": "Proceedings of the 4th MLSys Conference, San Jose, CA, USA, 2021", "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling compilers to automatically optimize code has been a longstanding\ngoal for the compiler community. Efficiently solving this problem requires\nusing precise cost models. These models predict whether applying a sequence of\ncode transformations reduces the execution time of the program. Building an\nanalytical cost model to do so is hard in modern x86 architectures due to the\ncomplexity of the microarchitecture. In this paper, we present a novel deep\nlearning based cost model for automatic code optimization. This model was\nintegrated in a search method and implemented in the Tiramisu compiler to\nselect the best code transformations. The input of the proposed model is a set\nof simple features representing the unoptimized code and a sequence of code\ntransformations. The model predicts the speedup expected when the code\ntransformations are applied. Unlike previous models, the proposed one works on\nfull programs and does not rely on any heavy feature engineering. The proposed\nmodel has only 16% of mean absolute percentage error in predicting speedups on\nfull programs. The proposed model enables Tiramisu to automatically find code\ntransformations that match or are better than state-of-the-art compilers\nwithout requiring the same level of heavy feature engineering required by those\ncompilers.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 08:32:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Baghdadi", "Riyadh", ""], ["Merouani", "Massinissa", ""], ["Leghettas", "Mohamed-Hicham", ""], ["Abdous", "Kamel", ""], ["Arbaoui", "Taha", ""], ["Benatchba", "Karima", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "2104.04996", "submitter": "Ran Tamir (Averbuch)", "authors": "Ran Tamir (Averbuch), Ariel Livshits, and Yonatan Shadmi", "title": "Simple Majority Consensus in Networks with Unreliable Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze the performance of a simple majority-rule protocol\nsolving a fundamental coordination problem in distributed systems -\n\\emph{binary majority consensus}, in the presence of probabilistic message\nloss. Using probabilistic analysis for a large scale, fully-connected, network\nof $2n$ agents, we prove that the Simple Majority Protocol (SMP) reaches\nconsensus in only three communication rounds with probability approaching $1$\nas $n$ grows to infinity. Moreover, if the difference between the numbers of\nagents that hold different opinions grows at a rate of $\\sqrt{n}$, then the SMP\nwith only two communication rounds attains consensus on the majority opinion of\nthe network, and if this difference grows faster than $\\sqrt{n}$, then the SMP\nreaches consensus on the majority opinion of the network in a single round,\nwith probability converging to $1$ exponentially fast as $n \\rightarrow\n\\infty$. We also provide some converse results, showing that these requirements\nare not only sufficient, but also necessary.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 11:36:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tamir", "Ran", "", "Averbuch"], ["Livshits", "Ariel", ""], ["Shadmi", "Yonatan", ""]]}, {"id": "2104.05035", "submitter": "Samson Akintoye Dr.", "authors": "Samson B. Akintoye, Liangxiu Han, Xin Zhang, Haoming Chen, Daoqiang\n  Zhang", "title": "A Hybrid Parallelization Approach for Distributed and Scalable Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep Neural Networks (DNNs) have recorded great success in handling\nmedical and other complex classification tasks. However, as the sizes of a DNN\nmodel and the available dataset increase, the training process becomes more\ncomplex and computationally intensive, which usually takes a longer time to\ncomplete. In this work, we have proposed a generic full end-to-end hybrid\nparallelization approach combining both model and data parallelism for\nefficiently distributed and scalable training of DNN models. We have also\nproposed a Genetic Algorithm based heuristic resources allocation mechanism\n(GABRA) for optimal distribution of partitions on the available GPUs for\ncomputing performance optimization. We have applied our proposed approach to a\nreal use case based on 3D Residual Attention Deep Neural Network (3D-ResAttNet)\nfor efficient Alzheimer Disease (AD) diagnosis on multiple GPUs. The\nexperimental evaluation shows that the proposed approach is efficient and\nscalable, which achieves almost linear speedup with little or no differences in\naccuracy performance when compared with the existing non-parallel DNN models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 15:42:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Akintoye", "Samson B.", ""], ["Han", "Liangxiu", ""], ["Zhang", "Xin", ""], ["Chen", "Haoming", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "2104.05040", "submitter": "Zhenlong Li Dr.", "authors": "Zhenlong Li, Xiao Huang, Tao Hu, Huan Ning, Xinyue Ye, Xiaoming Li", "title": "ODT FLOW: A Scalable Platform for Extracting, Analyzing, and Sharing\n  Multi-source Multi-scale Human Mobility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response to the soaring needs of human mobility data, especially during\ndisaster events such as the COVID-19 pandemic, and the associated big data\nchallenges, we develop a scalable online platform for extracting, analyzing,\nand sharing multi-source multi-scale human mobility flows. Within the platform,\nan origin-destination-time (ODT) data model is proposed to work with scalable\nquery engines to handle heterogenous mobility data in large volumes with\nextensive spatial coverage, which allows for efficient extraction, query, and\naggregation of billion-level origin-destination (OD) flows in parallel at the\nserver-side. An interactive spatial web portal, ODT Flow Explorer, is developed\nto allow users to explore multi-source mobility datasets with user-defined\nspatiotemporal scales. To promote reproducibility and replicability, we further\ndevelop ODT Flow REST APIs that provide researchers with the flexibility to\naccess the data programmatically via workflows, codes, and programs.\nDemonstrations are provided to illustrate the potential of the APIs integrating\nwith scientific workflows and with the Jupyter Notebook environment. We believe\nthe platform coupled with the derived multi-scale mobility data can assist\nhuman mobility monitoring and analysis during disaster events such as the\nongoing COVID-19 pandemic and benefit both scientific communities and the\ngeneral public in understanding human mobility dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:07:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Zhenlong", ""], ["Huang", "Xiao", ""], ["Hu", "Tao", ""], ["Ning", "Huan", ""], ["Ye", "Xinyue", ""], ["Li", "Xiaoming", ""]]}, {"id": "2104.05102", "submitter": "Atanu Barai", "authors": "Atanu Barai and Yehia Arafa and Abdel-Hameed Badawy and Gopinath\n  Chennupati and Nandakishore Santhi and Stephan Eidenbenz", "title": "PPT-Multicore: Performance Prediction of OpenMP applications using Reuse\n  Profiles and Analytical Modeling", "comments": "arXiv admin note: text overlap with arXiv:2103.10635", "journal-ref": null, "doi": null, "report-no": "LA-UR-21-22749", "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PPT-Multicore, an analytical model embedded in the Performance\nPrediction Toolkit (PPT) to predict parallel application performance running on\na multicore processor. PPT-Multicore builds upon our previous work towards a\nmulticore cache model. We extract LLVM basic block labeled memory trace using\nan architecture-independent LLVM-based instrumentation tool only once in an\napplication's lifetime. The model uses the memory trace and other parameters\nfrom an instrumented sequentially executed binary. We use a probabilistic and\ncomputationally efficient reuse profile to predict the cache hit rates and\nruntimes of OpenMP programs' parallel sections. We model Intel's Broadwell,\nHaswell, and AMD's Zen2 architectures and validate our framework using\ndifferent applications from PolyBench and PARSEC benchmark suites. The results\nshow that PPT-Multicore can predict cache hit rates with an overall average\nerror rate of 1.23% while predicting the runtime with an error rate of 9.08%.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:49:55 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Barai", "Atanu", ""], ["Arafa", "Yehia", ""], ["Badawy", "Abdel-Hameed", ""], ["Chennupati", "Gopinath", ""], ["Santhi", "Nandakishore", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "2104.05125", "submitter": "Evgeny Toropov", "authors": "Evgeny Toropov, Paola A. Buitrago, Jose M. F. Moura", "title": "Shuffler: A Large Scale Data Management Tool for ML in Computer Vision", "comments": null, "journal-ref": "PEARC 2019 Article No 23", "doi": "10.1145/3332186.3333046", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets in the computer vision academic research community are primarily\nstatic. Once a dataset is accepted as a benchmark for a computer vision task,\nresearchers working on this task will not alter it in order to make their\nresults reproducible. At the same time, when exploring new tasks and new\napplications, datasets tend to be an ever changing entity. A practitioner may\ncombine existing public datasets, filter images or objects in them, change\nannotations or add new ones to fit a task at hand, visualize sample images, or\nperhaps output statistics in the form of text or plots. In fact, datasets\nchange as practitioners experiment with data as much as with algorithms, trying\nto make the most out of machine learning models. Given that ML and deep\nlearning call for large volumes of data to produce satisfactory results, it is\nno surprise that the resulting data and software management associated to\ndealing with live datasets can be quite complex. As far as we know, there is no\nflexible, publicly available instrument to facilitate manipulating image data\nand their annotations throughout a ML pipeline. In this work, we present\nShuffler, an open source tool that makes it easy to manage large computer\nvision datasets. It stores annotations in a relational, human-readable\ndatabase. Shuffler defines over 40 data handling operations with annotations\nthat are commonly useful in supervised learning applied to computer vision and\nsupports some of the most well-known computer vision datasets. Finally, it is\neasily extensible, making the addition of new operations and datasets a task\nthat is fast and easy to accomplish.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 22:27:28 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Toropov", "Evgeny", ""], ["Buitrago", "Paola A.", ""], ["Moura", "Jose M. F.", ""]]}, {"id": "2104.05158", "submitter": "Dheevatsa Mudigere", "authors": "Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas\n  Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie\n  Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan\n  K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat\n  Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie\n  Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna\n  Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi\n  Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar\n  Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr\n  Lapukhov, Maxim Naumov, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao", "title": "High-performance, Distributed Training of Large-scale Deep Learning\n  Recommendation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning recommendation models (DLRMs) are used across many\nbusiness-critical services at Facebook and are the single largest AI\napplication in terms of infrastructure demand in its data-centers. In this\npaper we discuss the SW/HW co-designed solution for high-performance\ndistributed training of large-scale DLRMs. We introduce a high-performance\nscalable software stack based on PyTorch and pair it with the new evolution of\nZion platform, namely ZionEX. We demonstrate the capability to train very large\nDLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup\nin terms of time to solution over previous systems. We achieve this by (i)\ndesigning the ZionEX platform with dedicated scale-out network, provisioned\nwith high bandwidth, optimal topology and efficient transport (ii) implementing\nan optimized PyTorch-based training stack supporting both model and data\nparallelism (iii) developing sharding algorithms capable of hierarchical\npartitioning of the embedding tables along row, column dimensions and load\nbalancing them across multiple workers; (iv) adding high-performance core\noperators while retaining flexibility to support optimizers with fully\ndeterministic updates (v) leveraging reduced precision communications,\nmulti-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we\ndevelop and briefly comment on distributed data ingestion and other supporting\nservices that are required for the robust and efficient end-to-end training in\nproduction environments.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 02:15:55 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 01:30:23 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 22:58:58 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mudigere", "Dheevatsa", ""], ["Hao", "Yuchen", ""], ["Huang", "Jianyu", ""], ["Tulloch", "Andrew", ""], ["Sridharan", "Srinivas", ""], ["Liu", "Xing", ""], ["Ozdal", "Mustafa", ""], ["Nie", "Jade", ""], ["Park", "Jongsoo", ""], ["Luo", "Liang", ""], ["Yang", "Jie Amy", ""], ["Gao", "Leon", ""], ["Ivchenko", "Dmytro", ""], ["Basant", "Aarti", ""], ["Hu", "Yuxi", ""], ["Yang", "Jiyan", ""], ["Ardestani", "Ehsan K.", ""], ["Wang", "Xiaodong", ""], ["Komuravelli", "Rakesh", ""], ["Chu", "Ching-Hsiang", ""], ["Yilmaz", "Serhat", ""], ["Li", "Huayu", ""], ["Qian", "Jiyuan", ""], ["Feng", "Zhuobo", ""], ["Ma", "Yinbin", ""], ["Yang", "Junjie", ""], ["Wen", "Ellie", ""], ["Li", "Hong", ""], ["Yang", "Lin", ""], ["Sun", "Chonglin", ""], ["Zhao", "Whitney", ""], ["Melts", "Dimitry", ""], ["Dhulipala", "Krishna", ""], ["Kishore", "KR", ""], ["Graf", "Tyler", ""], ["Eisenman", "Assaf", ""], ["Matam", "Kiran Kumar", ""], ["Gangidi", "Adi", ""], ["Chen", "Guoqiang Jerry", ""], ["Krishnan", "Manoj", ""], ["Nayak", "Avinash", ""], ["Nair", "Krishnakumar", ""], ["Muthiah", "Bharath", ""], ["khorashadi", "Mahmoud", ""], ["Bhattacharya", "Pallab", ""], ["Lapukhov", "Petr", ""], ["Naumov", "Maxim", ""], ["Qiao", "Lin", ""], ["Smelyanskiy", "Mikhail", ""], ["Jia", "Bill", ""], ["Rao", "Vijay", ""]]}, {"id": "2104.05245", "submitter": "Ce Zhang", "authors": "Ji Liu, Ce Zhang", "title": "Distributed Learning Systems with First-order Methods", "comments": "Foundations and Trends in Databases: Vol. 9: No. 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable and efficient distributed learning is one of the main driving forces\nbehind the recent rapid advancement of machine learning and artificial\nintelligence. One prominent feature of this topic is that recent progresses\nhave been made by researchers in two communities: (1) the system community such\nas database, data management, and distributed systems, and (2) the machine\nlearning and mathematical optimization community. The interaction and knowledge\nsharing between these two communities has led to the rapid development of new\ndistributed learning systems and theory.\n  In this work, we hope to provide a brief introduction of some distributed\nlearning techniques that have recently been developed, namely lossy\ncommunication compression (e.g., quantization and sparsification), asynchronous\ncommunication, and decentralized communication. One special focus in this work\nis on making sure that it can be easily understood by researchers in both\ncommunities -- On the system side, we rely on a simplified system model hiding\nmany system details that are not necessary for the intuition behind the system\nspeedups; while, on the theory side, we rely on minimal assumptions and\nsignificantly simplify the proof of some recent work to achieve comparable\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:27:07 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liu", "Ji", ""], ["Zhang", "Ce", ""]]}, {"id": "2104.05286", "submitter": "Georgios Mylonas", "authors": "Dimitrios Amaxilatis, Georgios Mylonas, Evangelos Theodoridis, Luis\n  Diez, Katerina Deligiannidou", "title": "LearningCity: Knowledge Generation for Smart Cities", "comments": "Preprint of chapter submitted to \"Smart Cities Performability,\n  Cognition, & Security\". EAI/Springer Innovations in Communication and\n  Computing. Springer, Cham. arXiv admin note: text overlap with\n  arXiv:2103.16998", "journal-ref": null, "doi": "10.1007/978-3-030-14718-1_2", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although we have reached new levels in smart city installations and systems,\nefforts so far have focused on providing diverse sources of data to smart city\nservices consumers while neglecting to provide ways to simplify making good use\nof them. In this context, one first step that will bring added value to smart\ncities is knowledge creation in smart cities through anomaly detection and data\nannotation, supported in both an automated and a crowdsourced manner. We\npresent here LearningCity, our solution that has been validated over an\nexisting smart city deployment in Santander, and the OrganiCity\nexperimentation-as-a-service ecosystem. We discuss key challenges along with\ncharacteristic use cases, and report on our design and implementation, together\nwith some preliminary results derived from combining large smart city datasets\nwith machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:31:10 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Amaxilatis", "Dimitrios", ""], ["Mylonas", "Georgios", ""], ["Theodoridis", "Evangelos", ""], ["Diez", "Luis", ""], ["Deligiannidou", "Katerina", ""]]}, {"id": "2104.05313", "submitter": "Sebastian M\\\"uller", "authors": "Serguei Popov, Sebastian M\\\"uller", "title": "Voting-based probabilistic consensuses and their applications in\n  distributed ledgers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review probabilistic models known as majority dynamics (also known as\nthreshold Voter Models) and discuss their possible applications for achieving\nconsensus in cryptocurrency systems. In particular, we show that using this\napproach straightforwardly for practical consensus in Byzantine setting can be\nproblematic and requires extensive further research. We then discuss the FPC\nconsensus protocol which circumvents the problems mentioned above by using\nexternal randomness.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:44:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Popov", "Serguei", ""], ["M\u00fcller", "Sebastian", ""]]}, {"id": "2104.05343", "submitter": "Yang You", "authors": "Qifan Xu and Shenggui Li and Chaoyu Gong and Yang You", "title": "An Efficient 2D Method for Training Super-Large Deep Learning Models", "comments": "Mr. Qifan Xu finished this work when he was an intern in Dr. Yang\n  You's group at NUS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge neural network models have shown unprecedented performance in real-world\napplications. However, due to memory constraints, model parallelism must be\nutilized to host large models that would otherwise not fit into the memory of a\nsingle device. Previous methods like Megatron partition the parameters of the\nentire model among multiple devices, while each device has to accommodate the\nredundant activations in forward and backward pass. In this work, we propose\nOptimus, a highly efficient and scalable 2D-partition paradigm of model\nparallelism that would facilitate the training of infinitely large language\nmodels. In Optimus, activations are partitioned and distributed among devices,\nfurther reducing redundancy. In terms of isoefficiency, Optimus significantly\noutperforms Megatron. On 64 GPUs of TACC Frontera, Optimus achieves 1.48X\nspeedup for training, 1.78X speedup for inference, and 8X increase in maximum\nbatch size over Megatron. Optimus surpasses Megatron in scaling efficiency by a\ngreat margin. The code is available at https://github.com/xuqifan897/Optimus.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:47:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Qifan", ""], ["Li", "Shenggui", ""], ["Gong", "Chaoyu", ""], ["You", "Yang", ""]]}, {"id": "2104.05438", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "Actors -- A Process Algebra Based Approach", "comments": "143 pages, 12 figures, 29 tables. arXiv admin note: text overlap with\n  arXiv:2101.05140", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model actors based on truly concurrent process algebra, and capture the\nactor model in the following characteristics: (1) Concurrency: all actors\nexecute concurrently; (2) Asynchrony: an actor receives and sends messages\nasynchronously; (3) Uniqueness: an actor has a unique name and the associate\nunique mail box name; (4) Concentration: an actor focuses on the processing\nmessages, including some local computations, creations of some new actors, and\nsending some messages to other actors; (5) Communication Dependency: the only\nway of affecting an actor is sending a message to it; (6) Abstraction: except\nfor the receiving and sending message, and creating new actors, the local\ncomputations are abstracted; (7) Persistence: an actor does not disappear after\nprocessing a message. Truly concurrent process algebra has rich expressive\nability to model the above characteristics of actors, and more importantly,\nthey are models for true concurrency, Comparing with other models of actors,\nthe truly concurrent process algebra based model has the following advantages:\n(1) The truly concurrent process algebra has rich expressive abilities to\ndescribe almost all characteristics of actors, especially for asynchronous\ncommunication, actor creation, recursion, abstraction, etc; (2) The truly\nconcurrent process algebra and actors are all models for true concurrency, and\nhave inborn intimacy; (3) The truly concurrent process algebra has a firm\nsemantics foundation and a powerful proof theory, the correctness of an actor\nsystem can be proven easily.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:25:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "2104.05491", "submitter": "Ali Raza", "authors": "Ali Raza, Zongshun Zhang, Nabeel Akhtar, Vatche Isahagian, Ibrahim\n  Matta", "title": "LIBRA: An Economical Hybrid Approach for Cloud Applications with Strict\n  SLAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Function-as-a-Service (FaaS) has recently emerged to reduce the deployment\ncost of running cloud applications compared to Infrastructure-as-a-Service\n(IaaS). FaaS follows a serverless 'pay-as-you-go' computing model; it comes at\na higher cost per unit of execution time but typically application functions\nexperience lower provisioning time (startup delay). IaaS requires the\nprovisioning of Virtual Machines, which typically suffer from longer cold-start\ndelays that cause higher queuing delays and higher request drop rates. We\npresent LIBRA, a balanced (hybrid) approach that leverages both VM-based and\nserverless resources to efficiently manage cloud resources for the\napplications. LIBRA closely monitors the application demand and provisions\nappropriate VM and serverless resources such that the running cost is minimized\nand Service-Level Agreements are met. Unlike state of the art, LIBRA not only\nhides VM cold-start delays, and hence reduces response time, by leveraging\nserverless, but also directs a low-rate bursty portion of the demand to\nserverless where it would be less costly than spinning up new VMs. We evaluate\nLIBRA on real traces in a simulated environment as well as on the AWS\ncommercial cloud. Our results show that LIBRA outperforms other\nresource-provisioning policies, including a recent hybrid approach - LIBRA\nachieves more than 85% reduction in SLA violations and up to 53% cost savings.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:19:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Raza", "Ali", ""], ["Zhang", "Zongshun", ""], ["Akhtar", "Nabeel", ""], ["Isahagian", "Vatche", ""], ["Matta", "Ibrahim", ""]]}, {"id": "2104.05509", "submitter": "Abdullatif Albaseer Mr", "authors": "Abdullatif Albaseer, Mohamed Abdallah, Ala Al-Fuqaha, and Aiman Erbad", "title": "Threshold-Based Data Exclusion Approach for Energy-Efficient Federated\n  Edge Learning", "comments": "accepted to IEEE ICC 2021 WS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated edge learning (FEEL) is a promising distributed learning technique\nfor next-generation wireless networks. FEEL preserves the user's privacy,\nreduces the communication costs, and exploits the unprecedented capabilities of\nedge devices to train a shared global model by leveraging a massive amount of\ndata generated at the network edge. However, FEEL might significantly shorten\nenergy-constrained participating devices' lifetime due to the power consumed\nduring the model training round. This paper proposes a novel approach that\nendeavors to minimize computation and communication energy consumption during\nFEEL rounds to address this issue. First, we introduce a modified local\ntraining algorithm that intelligently selects only the samples that enhance the\nmodel's quality based on a predetermined threshold probability. Then, the\nproblem is formulated as joint energy minimization and resource allocation\noptimization problem to obtain the optimal local computation time and the\noptimal transmission time that minimize the total energy consumption\nconsidering the worker's energy budget, available bandwidth, channel states,\nbeamforming, and local CPU speed. After that, we introduce a tractable solution\nto the formulated problem that ensures the robustness of FEEL. Our simulation\nresults show that our solution substantially outperforms the baseline FEEL\nalgorithm as it reduces the local consumed energy by up to 79%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:34:40 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Albaseer", "Abdullatif", ""], ["Abdallah", "Mohamed", ""], ["Al-Fuqaha", "Ala", ""], ["Erbad", "Aiman", ""]]}, {"id": "2104.05541", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang, Xiangru Chen, Sandip Ray", "title": "Optimizing the Whole-life Cost in End-to-end CNN Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The acceleration of CNNs has gained increasing atten-tion since their success\nin computer vision. With the heterogeneous functional layers that cannot be\npro-cessed by the accelerators proposed for convolution layers only, modern\nend-to-end CNN acceleration so-lutions either transform the diverse computation\ninto matrix/vector arithmetic, which loses data reuse op-portunities in\nconvolution, or introduce dedicated functional unit to each kind of layer,\nwhich results in underutilization and high update expense. To enhance the\nwhole-life cost efficiency, we need an acceleration solution that is efficient\nin processing CNN layers and has the generality to apply to all kinds of\nexisting and emerging layers. To this end, we pro-pose GCONV Chain, a method to\nconvert the entire CNN computation into a chain of standard general\nconvolutions (GCONV) that can be efficiently pro-cessed by the existing CNN\naccelerators. This paper comprehensively analyzes the GCONV Chain model and\nproposes a full-stack implementation to support GCONV Chain. On one hand, the\nresults on seven var-ious CNNs demonstrate that GCONV Chain improves the\nperformance and energy efficiency of existing CNN accelerators by an average of\n3.4x and 3.2x re-spectively. On the other hand, we show that GCONV Chain\nprovides low whole-life costs for CNN accelera-tion, including both developer\nefforts and total cost of ownership for the users.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:12:42 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 00:46:39 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhang", "Jiaqi", ""], ["Chen", "Xiangru", ""], ["Ray", "Sandip", ""]]}, {"id": "2104.05583", "submitter": "Yu Chen", "authors": "Ronghua Xu, Yu Chen", "title": "Fed-DDM: A Federated Ledgers based Framework for Hierarchical\n  Decentralized Data Marketplaces", "comments": "Manuscript submitted to the 4th International Workshop on Blockchain\n  Enabled Sustainable Smart Cities (BLESS), in conjunction with 30th\n  International Conference on Computer Communications and Networks (ICCCN\n  2021), Athens, Greece, July 19-22, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Data marketplaces (DMs) promote the benefits of the Internet of Things (IoT)\nin smart cities. To facilitate the easy exchanges of real-time IoT data streams\nbetween device owners and third-party applications, it is required to provide\nscalable, interoperable, and secured services for large numbers of distributed\nIoT devices operated by different application vendors. Thanks to\ndecentralization, immutability, and auditability, Blockchain is promising to\nenable a tamper-proof and trust-free framework to enhance performance and\nsecurity issues in centralized DMs. However, directly integrating blockchains\ninto large-scale IoT-based DMs still faces many limitations, such as high\nresource and energy demands, low transaction throughput, poor scalability, and\nchallenges in privacy preservation. This paper introduces a novel Federated\nLedgers-based Framework for Hierarchical Decentralized Data Marketplaces\n(Fed-DDM). In Fed-DDM, participants are divided into multiple permissioned\ndomains given their registrations. Each domain leverages an efficient Byzantine\nFault Tolerance (BFT) consensus protocol to commit transactions of a domain on\na private intra-ledger. A public inter-ledger network adopts a scalable\nProof-of-Work (PoW) consensus protocol to federate multiple private\nintra-ledger networks. We design a smart contract-enabled inter-ledger protocol\nto guarantee the security of the cross-domain operations on a public federated\nledger without exposing sensitive privacy information from private ledgers. A\nproof-of-concept prototype is implemented, and the experimental results verify\nthe feasibility of the proposed Fed-DDM solution with performance and security\nguarantees.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:56:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Ronghua", ""], ["Chen", "Yu", ""]]}, {"id": "2104.05588", "submitter": "Daniel Coquelin", "authors": "Daniel Coquelin, Charlotte Debus, Markus G\\\"otz, Fabrice von der Lehr,\n  James Kahn, Martin Siggel, and Achim Streit", "title": "Accelerating Neural Network Training with Distributed Asynchronous and\n  Selective Optimization (DASO)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With increasing data and model complexities, the time required to train\nneural networks has become prohibitively large. To address the exponential rise\nin training time, users are turning to data parallel neural networks (DPNN) to\nutilize large-scale distributed resources on computer clusters. Current DPNN\napproaches implement the network parameter updates by synchronizing and\naveraging gradients across all processes with blocking communication\noperations. This synchronization is the central algorithmic bottleneck. To\ncombat this, we introduce the Distributed Asynchronous and Selective\nOptimization (DASO) method which leverages multi-GPU compute node architectures\nto accelerate network training. DASO uses a hierarchical and asynchronous\ncommunication scheme comprised of node-local and global networks while\nadjusting the global synchronization rate during the learning process. We show\nthat DASO yields a reduction in training time of up to 34% on classical and\nstate-of-the-art networks, as compared to other existing data parallel training\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:02:20 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 09:37:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Coquelin", "Daniel", ""], ["Debus", "Charlotte", ""], ["G\u00f6tz", "Markus", ""], ["von der Lehr", "Fabrice", ""], ["Kahn", "James", ""], ["Siggel", "Martin", ""], ["Streit", "Achim", ""]]}, {"id": "2104.05617", "submitter": "Yu Chen", "authors": "Alem Fitwi, Yu Chen", "title": "Secure and Privacy-Preserving Stored Surveillance Video Sharing atop\n  Permissioned Blockchain", "comments": "Submitted to the 4th International Workshop on Blockchain Enabled\n  Sustainable Smart Cities (BLESS), in conjunction with 30th International\n  Conference on Computer Communications and Networks (ICCCN 2021), Athens,\n  Greece, July 19-22, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  At present, more than a billion closed-circuit television (CCTV) cameras are\nwatching the world. These cameras garner a lot of visual information that is\noften processed and stored in remote and centralized cloud servers. Multiple\noccasions have revealed that this traditional approach is plagued with security\nand privacy breaches. The breaches could be the interception of raw videos\nwhile in transit to distant surveillance analytics centers (SAC), infiltration\nto cameras and network video records (NVR), or abuse of cameras and stored\nvideos. Hence, the traditional video surveillance system (VSS) cannot guarantee\nthe protection of the privacy of individuals caught on CCTV cameras. Wherefore,\nthis paper proposes a Secure and Privacy-preserving Stored surveillance video\nsharing (SePriS) mechanism for authorized users/nodes based on smart contracts,\nblockchain (BC), and the enciphering of video frames using DAB, a mechanism\ndeveloped based on discrete cosine transform (DCT), advanced encryption\nstandard (AES), and block shuffling (BS) algorithm. The BC-based solution\ncreates an environment auspicious for creating decentralized, reliable SACs and\nstorage sites with secure and privacy-aware sharing of stored surveillance\nvideos across SAC nodes and by law enforcers, police departments, and courts\nsecurely connected to the SAC nodes. The experiments and analyses validate that\nthe proposed BC-based SePriS solution achieves the design purpose.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:41:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Fitwi", "Alem", ""], ["Chen", "Yu", ""]]}, {"id": "2104.05743", "submitter": "Pavlos Papadopoulos", "authors": "Tom Titcombe, Adam J. Hall, Pavlos Papadopoulos, Daniele Romanini", "title": "Practical Defences Against Model Inversion Attacks for Split Neural\n  Networks", "comments": "ICLR 2021 Workshop on Distributed and Private Machine Learning (DPML\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a threat model under which a split network-based federated\nlearning system is susceptible to a model inversion attack by a malicious\ncomputational server. We demonstrate that the attack can be successfully\nperformed with limited knowledge of the data distribution by the attacker. We\npropose a simple additive noise method to defend against model inversion,\nfinding that the method can significantly reduce attack efficacy at an\nacceptable accuracy trade-off on MNIST. Furthermore, we show that NoPeekNN, an\nexisting defensive method, protects different information from exposure,\nsuggesting that a combined defence is necessary to fully protect private user\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:12:17 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 11:01:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Titcombe", "Tom", ""], ["Hall", "Adam J.", ""], ["Papadopoulos", "Pavlos", ""], ["Romanini", "Daniele", ""]]}, {"id": "2104.05765", "submitter": "Hrishav Bakul Barua", "authors": "Hrishav Bakul Barua, Kartick Chandra Mondal", "title": "Cloud Big Data Mining and Analytics: Bringing Greenness and Acceleration\n  in the Cloud", "comments": "In book: Accepted in Handbook of Machine Learning for Data\n  SciencePublisher: Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data is gaining overwhelming attention since the last decade. Almost all\nthe fields of science and technology have experienced a considerable impact\nfrom it. The cloud computing paradigm has been targeted for big data processing\nand mining in a more efficient manner using the plethora of resources available\nfrom computing nodes to efficient storage. Cloud data mining introduces the\nconcept of performing data mining and analytics of huge data in the cloud\navailing the cloud resources. But can we do better? Yes, of course! The main\ncontribution of this chapter is the identification of four game-changing\ntechnologies for the acceleration of computing and analysis of data mining\ntasks in the cloud. Graphics Processing Units can be used to further accelerate\nthe mining or analytic process, which is called GPU accelerated analytics.\nFurther, Approximate Computing can also be introduced in big data analytics for\nbringing efficacy in the process by reducing time and energy and hence\nfacilitating greenness in the entire computing process. Quantum Computing is a\nparadigm that is gaining pace in recent times which can also facilitate\nefficient and fast big data analytics in very little time. We have surveyed\nthese three technologies and established their importance in big data mining\nwith a holistic architecture by combining these three game-changers with the\nperspective of big data. We have also talked about another future technology,\ni.e., Neural Processing Units or Neural accelerators for researchers to explore\nthe possibilities. A brief explanation of big data and cloud data mining\nconcepts are also presented here.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:52:49 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Barua", "Hrishav Bakul", ""], ["Mondal", "Kartick Chandra", ""]]}, {"id": "2104.05829", "submitter": "Misun Min Dr", "authors": "Paul Fischer, Stefan Kerkemeier, Misun Min, Yu-Hsiang Lan, Malachi\n  Phillips, Thilina Rathnayake, Elia Merzari, Ananias Tomboulides, Ali Karakus,\n  Noel Chalmers, Tim Warburton", "title": "NekRS, a GPU-Accelerated Spectral Element Navier-Stokes Solver", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of NekRS, a GPU-oriented thermal-fluids simulation code based\non the spectral element method (SEM) is described. For performance portability,\nthe code is based on the open concurrent compute abstraction and leverages\nscalable developments in the SEM code Nek5000 and in libParanumal, which is a\nlibrary of high-performance kernels for high-order discretizations and\nPDE-based miniapps. Critical performance sections of the Navier-Stokes time\nadvancement are addressed. Performance results on several platforms are\npresented, including scaling to 27,648 V100s on OLCF Summit, for calculations\nof up to 60B gridpoints.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:32:01 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Fischer", "Paul", ""], ["Kerkemeier", "Stefan", ""], ["Min", "Misun", ""], ["Lan", "Yu-Hsiang", ""], ["Phillips", "Malachi", ""], ["Rathnayake", "Thilina", ""], ["Merzari", "Elia", ""], ["Tomboulides", "Ananias", ""], ["Karakus", "Ali", ""], ["Chalmers", "Noel", ""], ["Warburton", "Tim", ""]]}, {"id": "2104.06023", "submitter": "Yuzhu Mao", "authors": "Yuzhu Mao, Zihao Zhao, Guangfeng Yan, Yang Liu, Tian Lan, Linqi Song\n  and Wenbo Ding", "title": "Communication Efficient Federated Learning with Adaptive Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has attracted tremendous attentions in recent years\ndue to its privacy preserving measures and great potentials in some distributed\nbut privacy-sensitive applications like finance and health. However, high\ncommunication overloads for transmitting high-dimensional networks and extra\nsecurity masks remains a bottleneck of FL. This paper proposes a\ncommunication-efficient FL framework with Adaptive Quantized Gradient (AQG)\nwhich adaptively adjusts the quantization level based on local gradient's\nupdate to fully utilize the heterogeneousness of local data distribution for\nreducing unnecessary transmissions. Besides, the client dropout issues are\ntaken into account and the Augmented AQG is developed, which could limit the\ndropout noise with an appropriate amplification mechanism for transmitted\ngradients. Theoretical analysis and experiment results show that the proposed\nAQG leads to 25%-50% of additional transmission reduction as compared to\nexisting popular methods including Quantized Gradient Descent (QGD) and Lazily\nAggregated Quantized (LAQ) gradient-based method without deteriorating\nconvergence properties. Particularly, experiments with heterogenous data\ndistributions corroborate a more significant transmission reduction compared\nwith independent identical data distributions. Meanwhile, the proposed AQG is\nrobust to a client dropping rate up to 90% empirically, and the Augmented AQG\nmanages to further improve the FL system's communication efficiency with the\npresence of moderate-scale client dropouts commonly seen in practical FL\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 08:45:47 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mao", "Yuzhu", ""], ["Zhao", "Zihao", ""], ["Yan", "Guangfeng", ""], ["Liu", "Yang", ""], ["Lan", "Tian", ""], ["Song", "Linqi", ""], ["Ding", "Wenbo", ""]]}, {"id": "2104.06069", "submitter": "Conglong Li", "authors": "Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari,\n  Yuxiong He", "title": "1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training\n  with LAMB's Convergence Speed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To train large models (like BERT and GPT-3) with hundreds or even thousands\nof GPUs, the communication has become a major bottleneck, especially on\ncommodity systems with limited-bandwidth TCP interconnects network. On one side\nlarge-batch optimization such as LAMB algorithm was proposed to reduce the\nnumber of communications. On the other side, communication compression\nalgorithms such as 1-bit SGD and 1-bit Adam help to reduce the volume of each\ncommunication. However, we find that simply using one of the techniques is not\nsufficient to solve the communication challenge, especially on low-bandwidth\nEthernet networks. Motivated by this we aim to combine the power of large-batch\noptimization and communication compression, but we find that existing\ncompression strategies cannot be directly applied to LAMB due to its unique\nadaptive layerwise learning rates. To this end, we design a new\ncommunication-efficient algorithm, 1-bit LAMB, which introduces a novel way to\nsupport adaptive layerwise learning rates even when communication is\ncompressed. In addition, we introduce a new system implementation for\ncompressed communication using the NCCL backend of PyTorch distributed, which\nimproves both usability and performance compared to existing MPI-based\nimplementation. For BERT-Large pre-training task with batch sizes from 8K to\n64K, our evaluations on up to 256 GPUs demonstrate that 1-bit LAMB with\nNCCL-based backend is able to achieve up to 4.6x communication volume\nreduction, up to 2.8x end-to-end speedup (in terms of number of training\nsamples per second), and the same convergence speed (in terms of number of\npre-training samples to reach the same accuracy on fine-tuning tasks) compared\nto uncompressed LAMB.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 10:07:49 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Li", "Conglong", ""], ["Awan", "Ammar Ahmad", ""], ["Tang", "Hanlin", ""], ["Rajbhandari", "Samyam", ""], ["He", "Yuxiong", ""]]}, {"id": "2104.06274", "submitter": "Xin Du", "authors": "Xin Du, Songtao Tang, Zhihui Lu, Keke Gai, Jie Wu, and Patrick C.K.\n  Hung", "title": "Optimal Data Placement for Data-Sharing Scientific Workflows in\n  Heterogeneous Edge-Cloud Computing Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heterogeneous edge-cloud computing paradigm can provide a more optimal\ndirection to deploy scientific workflows than traditional distributed computing\nor cloud computing environments. Due to the different sizes of scientific\ndatasets and some of these datasets must keep private, it is still a difficult\nproblem to finding an data placement strategy that can minimize data\ntransmission as well as placement cost. To address this issue, this paper\ncombines advantages of both edge and cloud computing to construct a data\nplacement model, which can balance data transfer time and data placement cost\nusing intelligent computation. The most difficult research challenge the model\nsolved is to consider many constrain in this hybrid computing environments,\nwhich including shared datasets within individual and among multiple workflows\nacross various geographical regions. According to the constructed model, the\nstudy propose a new data placement strategy named DE-DPSO-DPS, which using a\ndiscrete particle swarm optimization algorithm with differential evolution\n(DE-DPSO-DPA) to distribute these scientific datasets. The strategy also not\nonly consider the characteristics such as the number and storage capacity of\nedge micro-datacenters, the bandwidth between different datacenters and the\nproportion of private datasets, but also analysis the performance of algorithm\nduring the workflows execution. Comprehensive experiments are designed in\nsimulated heterogeneous edge-cloud computing environments demonstrate that the\ndata placement strategy can effectively reduce the data transmission time and\nplacement cost as compared to traditional strategies for data-sharing\nscientific workflows.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 15:05:45 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Du", "Xin", ""], ["Tang", "Songtao", ""], ["Lu", "Zhihui", ""], ["Gai", "Keke", ""], ["Wu", "Jie", ""], ["Hung", "Patrick C. K.", ""]]}, {"id": "2104.06357", "submitter": "Corey Nolet", "authors": "Corey J. Nolet, Divye Gala, Edward Raff, Joe Eaton, Brad Rees, John\n  Zedlewski, Tim Oates", "title": "Semiring Primitives for Sparse Neighborhood Methods on the GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.RA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-performance primitives for mathematical operations on sparse vectors\nmust deal with the challenges of skewed degree distributions and limits on\nmemory consumption that are typically not issues in dense operations. We\ndemonstrate that a sparse semiring primitive can be flexible enough to support\na wide range of critical distance measures while maintaining performance and\nmemory efficiency on the GPU. We further show that this primitive is a\nfoundational component for enabling many neighborhood-based information\nretrieval and machine learning algorithms to accept sparse input. To our\nknowledge, this is the first work aiming to unify the computation of several\ncritical distance measures on the GPU under a single flexible design paradigm\nand we hope that it provides a good baseline for future research in this area.\nOur implementation is fully open source and publicly available at\nhttps://github.com/rapidsai/cuml.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:05:03 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Nolet", "Corey J.", ""], ["Gala", "Divye", ""], ["Raff", "Edward", ""], ["Eaton", "Joe", ""], ["Rees", "Brad", ""], ["Zedlewski", "John", ""], ["Oates", "Tim", ""]]}, {"id": "2104.06494", "submitter": "Ioannis Sakiotis", "authors": "Ioannis Sakiotis, Kamesh Arumugam, Marc Paterno, Desh Ranjan,\n  Bal\\v{s}a Terzi\\'c, Mohammad Zubair", "title": "PAGANI: A Parallel Adaptive GPU Algorithm for Numerical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new adaptive parallel algorithm for the challenging problem of\nmulti-dimensional numerical integration on massively parallel architectures.\nAdaptive algorithms have demonstrated the best performance, but efficient\nmany-core utilization is difficult to achieve because the adaptive work-load\ncan vary greatly across the integration space and is impossible to predict a\npriori. Existing parallel algorithms utilize sequential computations on\nindependent processors, which results in bottlenecks due to the need for data\nredistribution and processor synchronization. Our algorithm employs a\nhigh-throughput approach in which all existing sub-regions are processed and\nsub-divided in parallel. Repeated sub-region classification and filtering\nimproves upon a brute-force approach and allows the algorithm to make efficient\nuse of computation and memory resources. A CUDA implementation shows orders of\nmagnitude speedup over the fastest open-source CPU method and extends the\nachievable accuracy for difficult integrands. Our algorithm typically\noutperforms other existing deterministic parallel methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 20:19:57 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 12:41:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sakiotis", "Ioannis", ""], ["Arumugam", "Kamesh", ""], ["Paterno", "Marc", ""], ["Ranjan", "Desh", ""], ["Terzi\u0107", "Bal\u0161a", ""], ["Zubair", "Mohammad", ""]]}, {"id": "2104.06513", "submitter": "Deepak Narayanan", "authors": "Deepak Narayanan, Fiodar Kazhamiaka, Firas Abuzaid, Peter Kraft, Matei\n  Zaharia", "title": "Don't Give Up on Large Optimization Problems; POP Them!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource allocation problems in many computer systems can be formulated as\nmathematical optimization problems. However, finding exact solutions to these\nproblems using off-the-shelf solvers in an online setting is often intractable\nfor \"hyper-scale\" system sizes with tight SLAs, leading system designers to\nrely on cheap, heuristic algorithms. In this work, we explore an alternative\napproach that reuses the original optimization problem formulation. By\nsplitting the original problem into smaller, more tractable problems for\nsubsets of the system and then coalescing resulting sub-allocations into a\nglobal solution, we achieve empirically quasi-optimal (within 1.5%) performance\nfor multiple domains with several orders-of-magnitude improvement in runtime.\nDeciding how to split a large problem into smaller sub-problems, and how to\ncoalesce split allocations into a unified allocation, needs to be performed\ncarefully in a domain-aware way. We show common principles for splitting\nproblems effectively across a variety of tasks, including cluster scheduling,\ntraffic engineering, and load balancing.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 20:55:52 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Narayanan", "Deepak", ""], ["Kazhamiaka", "Fiodar", ""], ["Abuzaid", "Firas", ""], ["Kraft", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "2104.06685", "submitter": "Heng Zhu", "authors": "Heng Zhu, Qing Ling", "title": "BROADCAST: Reducing Both Stochastic and Compression Noise to Robustify\n  Communication-Efficient Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication between workers and the master node to collect local stochastic\ngradients is a key bottleneck in a large-scale federated learning system.\nVarious recent works have proposed to compress the local stochastic gradients\nto mitigate the communication overhead. However, robustness to malicious\nattacks is rarely considered in such a setting. In this work, we investigate\nthe problem of Byzantine-robust federated learning with compression, where the\nattacks from Byzantine workers can be arbitrarily malicious. We point out that\na vanilla combination of compressed stochastic gradient descent (SGD) and\ngeometric median-based robust aggregation suffers from both stochastic and\ncompression noise in the presence of Byzantine attacks. In light of this\nobservation, we propose to jointly reduce the stochastic and compression noise\nso as to improve the Byzantine-robustness. For the stochastic noise, we adopt\nthe stochastic average gradient algorithm (SAGA) to gradually eliminate the\ninner variations of regular workers. For the compression noise, we apply the\ngradient difference compression and achieve compression for free. We\ntheoretically prove that the proposed algorithm reaches a neighborhood of the\noptimal solution at a linear convergence rate, and the asymptotic learning\nerror is in the same order as that of the state-of-the-art uncompressed method.\nFinally, numerical experiments demonstrate effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:16:03 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhu", "Heng", ""], ["Ling", "Qing", ""]]}, {"id": "2104.06700", "submitter": "Md Vasimuddin", "authors": "Vasimuddin Md, Sanchit Misra, Guixiang Ma, Ramanarayan Mohanty,\n  Evangelos Georganas, Alexander Heinecke, Dhiraj Kalamkar, Nesreen K. Ahmed,\n  Sasikanth Avancha", "title": "DistGNN: Scalable Distributed Training for Large-Scale Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Full-batch training on Graph Neural Networks (GNN) to learn the structure of\nlarge graphs is a critical problem that needs to scale to hundreds of compute\nnodes to be feasible. It is challenging due to large memory capacity and\nbandwidth requirements on a single compute node and high communication volumes\nacross multiple nodes. In this paper, we present DistGNN that optimizes the\nwell-known Deep Graph Library (DGL) for full-batch training on CPU clusters via\nan efficient shared memory implementation, communication reduction using a\nminimum vertex-cut graph partitioning algorithm and communication avoidance\nusing a family of delayed-update algorithms. Our results on four common GNN\nbenchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to\n3.7x speed-up using a single CPU socket and up to 97x speed-up using 128 CPU\nsockets, respectively, over baseline DGL implementations running on a single\nCPU socket\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:46:35 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 05:37:53 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 15:04:55 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Md", "Vasimuddin", ""], ["Misra", "Sanchit", ""], ["Ma", "Guixiang", ""], ["Mohanty", "Ramanarayan", ""], ["Georganas", "Evangelos", ""], ["Heinecke", "Alexander", ""], ["Kalamkar", "Dhiraj", ""], ["Ahmed", "Nesreen K.", ""], ["Avancha", "Sasikanth", ""]]}, {"id": "2104.06913", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi, Michael Hare, David Schultz, Frank W\\\"urthwein,\n  Benedikt Riedel, Tom Hutton, Steve Barnet and Vladimir Brik", "title": "Managing Cloud networking costs for data-intensive applications by\n  provisioning dedicated network links", "comments": "8 pages, 7 figures, 4 tables, to be published in proceedings of\n  PEARC21", "journal-ref": null, "doi": "10.1145/3437359.3465563", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many scientific high-throughput applications can benefit from the elastic\nnature of Cloud resources, especially when there is a need to reduce time to\ncompletion. Cost considerations are usually a major issue in such endeavors,\nwith networking often a major component; for data-intensive applications,\negress networking costs can exceed the compute costs. Dedicated network links\nprovide a way to lower the networking costs, but they do add complexity. In\nthis paper we provide a description of a 100 fp32 PFLOPS Cloud burst in support\nof IceCube production compute, that used Internet2 Cloud Connect service to\nprovision several logically-dedicated network links from the three major Cloud\nproviders, namely Amazon Web Services, Microsoft Azure and Google Cloud\nPlatform, that in aggregate enabled approximately 100 Gbps egress capability to\non-prem storage. It provides technical details about the provisioning process,\nthe benefits and limitations of such a setup and an analysis of the costs\nincurred.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:03:45 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Sfiligoi", "Igor", ""], ["Hare", "Michael", ""], ["Schultz", "David", ""], ["W\u00fcrthwein", "Frank", ""], ["Riedel", "Benedikt", ""], ["Hutton", "Tom", ""], ["Barnet", "Steve", ""], ["Brik", "Vladimir", ""]]}, {"id": "2104.06968", "submitter": "Haris Javaid", "authors": "Haris Javaid, Ji Yang, Nathania Santoso, Mohit Upadhyay,\n  Sundararajarao Mohan, Chengchen Hu, Gordon Brebner", "title": "Blockchain Machine: A Network-Attached Hardware Accelerator for\n  Hyperledger Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how Hyperledger Fabric, one of the most popular\npermissioned blockchains, can benefit from network-attached acceleration. The\nscalability and peak performance of Fabric is primarily limited by the\nbottlenecks present in its block validation/commit phase. We propose Blockchain\nMachine, a hardware accelerator coupled with a hardware-friendly communication\nprotocol, to act as the validator peer. It can be adapted to applications and\ntheir smart contracts, and is targeted for a server with network-attached FPGA\nacceleration card. The Blockchain Machine retrieves blocks and their\ntransactions in hardware directly from the network interface, which are then\nvalidated through a configurable and efficient block-level and\ntransaction-level pipeline. The validation results are then transferred to the\nhost CPU where non-bottleneck operations are executed. From our implementation\nintegrated with Fabric v1.4 LTS, we observed up to 17x speedup in block\nvalidation when compared to the software-only validator peer, with commit\nthroughput of up to 95,600 tps (~4.5x improvement over the best reported in\nliterature).\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:51:12 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Javaid", "Haris", ""], ["Yang", "Ji", ""], ["Santoso", "Nathania", ""], ["Upadhyay", "Mohit", ""], ["Mohan", "Sundararajarao", ""], ["Hu", "Chengchen", ""], ["Brebner", "Gordon", ""]]}, {"id": "2104.07122", "submitter": "Kyle Hale", "authors": "Kyle C. Hale", "title": "Coalescent Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computational infrastructure extends to the edge, it will increasingly\noffer the same fine-grained resource provisioning mechanisms used in\nlarge-scale cloud datacenters, and advances in low-latency, wireless networking\ntechnology will allow service providers to blur the distinction between local\nand remote resources for commodity computing. From the users' perspectives,\ntheir devices will no longer have fixed computational power, but rather will\nappear to have flexible computational capabilities that vary subject to the\nshared, disaggregated edge resources available in their physical proximity.\nSystem software will transparently leverage these ephemeral resources to\nprovide a better end-user experience. We discuss key systems challenges to\nenabling such tightly-coupled, disaggregated, and ephemeral infrastructure\nprovisioning, advocate for more research in the area, and outline possible\npaths forward.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:54:05 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hale", "Kyle C.", ""]]}, {"id": "2104.07145", "submitter": "Chaoyang He", "authors": "Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Yu Rong, Peilin\n  Zhao, Junzhou Huang, Murali Annavaram, Salman Avestimehr", "title": "FedGraphNN: A Federated Learning System and Benchmark for Graph Neural\n  Networks", "comments": "The first three authors contribute equally. Our shorter versions are\n  accepted to ICLR 2021 Workshop on Distributed and Private Machine\n  Learning(DPML) and MLSys 2021 GNNSys Workshop on Graph Neural Networks and\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Network (GNN) research is rapidly growing thanks to the capacity\nof GNNs to learn representations from graph-structured data. However,\ncentralizing a massive amount of real-world graph data for GNN training is\nprohibitive due to user-side privacy concerns, regulation restrictions, and\ncommercial competition. Federated learning (FL), a trending distributed\nlearning paradigm, aims to solve this challenge while preserving privacy.\nDespite recent advances in vision and language domains, there is no suitable\nplatform for the federated training of GNNs. To this end, we introduce\nFedGraphNN, an open research federated learning system and a benchmark to\nfacilitate GNN-based FL research. FedGraphNN is built on a unified formulation\nof federated GNNs and supports commonly used datasets, GNN models, FL\nalgorithms, and flexible APIs. We also contribute a new molecular dataset,\nhERG, to promote research exploration. Our experimental results present\nsignificant challenges in federated GNN training: federated GNNs perform worse\nin most datasets with a non-I.I.D split than centralized GNNs; the GNN model\nthat attains the best result in the centralized setting may not hold its\nadvantage in the federated setting. These results imply that more research\nefforts are needed to unravel the mystery behind federated GNN training.\nMoreover, our system performance analysis demonstrates that the FedGraphNN\nsystem is computationally affordable to most research labs with limited GPUs.\nWe maintain the source code at https://github.com/FedML-AI/FedGraphNN.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:11:35 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["He", "Chaoyang", ""], ["Balasubramanian", "Keshav", ""], ["Ceyani", "Emir", ""], ["Rong", "Yu", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2104.07293", "submitter": "Alexis Ghyselen", "authors": "Patrick Baillot (LIP), Alexis Ghyselen (LIP), Naoki Kobayashi", "title": "Sized Types with Usages for Parallel Complexity of Pi-Calculus Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of analysing the complexity of concurrent programs\nwritten in Pi-calculus. We are interested in parallel complexity, or span,\nunderstood as the execution time in a model with maximal parallelism. A type\nsystem for parallel complexity has been recently proposed by Baillot and\nGhyselen but it is too imprecise for non-linear channels and cannot analyse\nsome concurrent processes. Aiming for a more precise analysis, we design a type\nsystem which builds on the concepts of sized types and usages. The new variant\nof usages we define accounts for the various ways a channel is employed and\nrelies on time annotations to track under which conditions processes can\nsynchronize. We prove that a type derivation for a process provides an upper\nbound on its parallel complexity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:07:55 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Baillot", "Patrick", "", "LIP"], ["Ghyselen", "Alexis", "", "LIP"], ["Kobayashi", "Naoki", ""]]}, {"id": "2104.07298", "submitter": "Thomas Nowak", "authors": "Fabricio Cravo and Thomas Nowak", "title": "Towards a Fast and Accurate Model of Intercontact Times for Epidemic\n  Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an accurate user-encounter trace generator based on analytical\nmodels. Our method generates traces of intercontact times faster than models\nthat explicitly generate mobility traces. We use this trace generator to study\nthe characteristics of pair-wise intercontact-time distributions and visualize,\nusing simulations, how they combine to form the aggregate intercontact-time\ndistribution. Finally, we apply our trace-generation model to the epidemic\nrouting protocol.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:18:17 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Cravo", "Fabricio", ""], ["Nowak", "Thomas", ""]]}, {"id": "2104.07365", "submitter": "Erick Lavoie", "authors": "Aur\\'elien Bellet, Anne-Marie Kermarrec, Erick Lavoie", "title": "D-Cliques: Compensating NonIIDness in Decentralized Federated Learning\n  with Topology", "comments": "22 pages, 11 figures. Revisions in v2 and v3: Corrected typos, minor\n  mistakes in scaling analysis that did not affect the overall argument, and\n  clarified convergence plots to read better in black&white", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convergence speed of machine learning models trained with Federated\nLearning is significantly affected by non-independent and identically\ndistributed (non-IID) data partitions, even more so in a fully decentralized\nsetting without a central server. In this paper, we show that the impact of\nlocal class bias, an important type of data non-IIDness, can be significantly\nreduced by carefully designing the underlying communication topology. We\npresent D-Cliques, a novel topology that reduces gradient bias by grouping\nnodes in interconnected cliques such that the local joint distribution in a\nclique is representative of the global class distribution. We also show how to\nadapt the updates of decentralized SGD to obtain unbiased gradients and\nimplement an effective momentum with D-Cliques. Our empirical evaluation on\nMNIST and CIFAR10 demonstrates that our approach provides similar convergence\nspeed as a fully-connected topology with a significant reduction in the number\nof edges and messages. In a 1000-node topology, D-Cliques requires 98% less\nedges and 96% less total messages, with further possible gains using a\nsmall-world topology across cliques.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:47:27 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 10:05:56 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 13:58:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Kermarrec", "Anne-Marie", ""], ["Lavoie", "Erick", ""]]}, {"id": "2104.07508", "submitter": "Reid Priedhorsky", "authors": "Reid Priedhorsky (1), R. Shane Canon (2 and 3), Timothy Randles (1),\n  Andrew J. Younge (4) ((1) High Performance Computing Division, Los Alamos\n  National Laboratory, (2) National Energy Research Scientific Computing\n  Center, (3) Lawrence Berkeley National Laboratory, (4) Center for Computing\n  Research, Sandia National Laboratories)", "title": "Minimizing privilege for building HPC containers", "comments": "13 pages, 11 figures. Revision 2: clarifications, corrections of some\n  minor errors", "journal-ref": null, "doi": null, "report-no": "LA-UR 21-23314; SAND2021-4332 O", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HPC centers face increasing demand for software flexibility, and there is\ngrowing consensus that Linux containers are a promising solution. However,\nexisting container build solutions require root privileges and cannot be built\ndirectly on HPC resources. This limitation is compounded as supercomputer\ndiversity expands and HPC architectures become more dissimilar from commodity\ncomputing resources. Our evaluation of available options suggests this problem\ncan best be solved with low-privilege containers. We detail Linux kernel\nfeatures for varying container privilege and compare two open-source\nimplementations, mostly-unprivileged rootless Podman and fully-unprivileged\nCharliecloud. Our analysis demonstrates that low-privilege container build on\nHPC resources works now and will continue to improve, giving normal users a\nbetter workflow to securely and correctly build containers. Minimizing\nprivilege in this way can improve HPC user and developer productivity as well\nas reduce support workload for exascale applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:05:50 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 21:46:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Priedhorsky", "Reid", "", "2 and 3"], ["Canon", "R. Shane", "", "2 and 3"], ["Randles", "Timothy", ""], ["Younge", "Andrew J.", ""]]}, {"id": "2104.07515", "submitter": "Moming Duan", "authors": "Li Li, Moming Duan, Duo Liu, Yu Zhang, Ao Ren, Xianzhang Chen, Yujuan\n  Tan, Chengliang Wang", "title": "FedSAE: A Novel Self-Adaptive Federated Learning Framework in\n  Heterogeneous Systems", "comments": "This paper will be presented at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) is a novel distributed machine learning which allows\nthousands of edge devices to train model locally without uploading data\nconcentrically to the server. But since real federated settings are\nresource-constrained, FL is encountered with systems heterogeneity which causes\na lot of stragglers directly and then leads to significantly accuracy reduction\nindirectly. To solve the problems caused by systems heterogeneity, we introduce\na novel self-adaptive federated framework FedSAE which adjusts the training\ntask of devices automatically and selects participants actively to alleviate\nthe performance degradation. In this work, we 1) propose FedSAE which leverages\nthe complete information of devices' historical training tasks to predict the\naffordable training workloads for each device. In this way, FedSAE can estimate\nthe reliability of each device and self-adaptively adjust the amount of\ntraining load per client in each round. 2) combine our framework with Active\nLearning to self-adaptively select participants. Then the framework accelerates\nthe convergence of the global model. In our framework, the server evaluates\ndevices' value of training based on their training loss. Then the server\nselects those clients with bigger value for the global model to reduce\ncommunication overhead. The experimental result indicates that in a highly\nheterogeneous system, FedSAE converges faster than FedAvg, the vanilla FL\nframework. Furthermore, FedSAE outperforms than FedAvg on several federated\ndatasets - FedSAE improves test accuracy by 26.7% and reduces stragglers by\n90.3% on average.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:14:11 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Li", "Li", ""], ["Duan", "Moming", ""], ["Liu", "Duo", ""], ["Zhang", "Yu", ""], ["Ren", "Ao", ""], ["Chen", "Xianzhang", ""], ["Tan", "Yujuan", ""], ["Wang", "Chengliang", ""]]}, {"id": "2104.07574", "submitter": "Yanni Georghiades", "authors": "Yanni Georghiades, Robert Streit, Vijay Garg", "title": "Who Needs Consensus? A Distributed Monetary System Between Rational\n  Agents via Hearsay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel distributed monetary system called Hearsay that tolerates\nboth Byzantine and rational behavior without the need for agents to reach\nconsensus on executed transactions. Recent work [5, 10, 15] has shown that\ndistributed monetary systems do not require consensus and can operate using a\nbroadcast primitive with weaker guarantees, such as reliable broadcast.\nHowever, these protocols assume that some number of agents may be Byzantine and\nthe remaining agents are perfectly correct. For the application of a monetary\nsystem in which the agents are real people with economic interests, the\nassumption that agents are perfectly correct may be too strong. We expand upon\nthis line of thought by weakening the assumption of correctness and instead\nadopting a fault tolerance model which allows up to $t < \\frac{N}{3}$ agents to\nbe Byzantine and the remaining agents to be rational. A rational agent is one\nwhich will deviate from the protocol if it is in their own best interest. Under\nthis fault tolerance model, Hearsay implements a monetary system in which all\nrational agents achieve agreement on executed transactions. Moreover, Hearsay\nrequires only a single broadcast per transaction. In order to incentivize\nrational agents to behave correctly in Hearsay, agents are rewarded with\ntransaction fees for participation in the protocol and punished for noticeable\ndeviations from the protocol. Additionally, Hearsay uses a novel broadcast\nprimitive called Rational Reliable Broadcast to ensure that agents can\nbroadcast messages under Hearsay's fault tolerance model. Rational Reliable\nBroadcast achieves equivalent guarantees to Byzantine Reliable Broadcast [7]\nbut can tolerate the presence of rational agents. To show this, we prove that\nfollowing the Rational Reliable Broadcast protocol constitutes a Nash\nequilibrium between rational agents and may therefore be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:28:47 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Georghiades", "Yanni", ""], ["Streit", "Robert", ""], ["Garg", "Vijay", ""]]}, {"id": "2104.07582", "submitter": "Maciej Besta", "authors": "Maciej Besta, Raghavendra Kanakagiri, Grzegorz Kwasniewski, Rachata\n  Ausavarungnirun, Jakub Ber\\'anek, Konstantinos Kanellopoulos, Kacper Janda,\n  Zur Vonarburg-Shmaria, Lukas Gianinazzi, Ioana Stefan, Juan G\\'omez Luna,\n  Marcin Copik, Lukas Kapp-Schwoerer, Salvatore Di Girolamo, Marek Konieczny,\n  Onur Mutlu, Torsten Hoefler", "title": "SISA: Set-Centric Instruction Set Architecture for Graph Mining on\n  Processing-in-Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple graph algorithms such as PageRank have recently been the target of\nnumerous hardware accelerators. Yet, there also exist much more complex graph\nmining algorithms for problems such as clustering or maximal clique listing.\nThese algorithms are memory-bound and thus could be accelerated by hardware\ntechniques such as Processing-in-Memory (PIM). However, they also come with\nnon-straightforward parallelism and complicated memory access patterns. In this\nwork, we address this with a simple yet surprisingly powerful observation:\noperations on sets of vertices, such as intersection or union, form a large\npart of many complex graph mining algorithms, and can offer rich and simple\nparallelism at multiple levels. This observation drives our cross-layer design,\nin which we (1) expose set operations using a novel programming paradigm, (2)\nexpress and execute these operations efficiently with carefully designed\nset-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA\ninstructions. The key design idea is to alleviate the bandwidth needs of SISA\ninstructions by mapping set operations to two types of PIM: in-DRAM bulk\nbitwise computing for bitvectors representing high-degree vertices, and\nnear-memory logic layers for integer arrays representing low-degree vertices.\nSet-centric SISA-enhanced algorithms are efficient and outperform hand-tuned\nbaselines, offering more than 10x speedup over the established Bron-Kerbosch\nalgorithm for listing maximal cliques. We deliver more than 10 SISA set-centric\nalgorithm formulations, illustrating SISA's wide applicability.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:37:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Besta", "Maciej", ""], ["Kanakagiri", "Raghavendra", ""], ["Kwasniewski", "Grzegorz", ""], ["Ausavarungnirun", "Rachata", ""], ["Ber\u00e1nek", "Jakub", ""], ["Kanellopoulos", "Konstantinos", ""], ["Janda", "Kacper", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Gianinazzi", "Lukas", ""], ["Stefan", "Ioana", ""], ["Luna", "Juan G\u00f3mez", ""], ["Copik", "Marcin", ""], ["Kapp-Schwoerer", "Lukas", ""], ["Di Girolamo", "Salvatore", ""], ["Konieczny", "Marek", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2104.07699", "submitter": "Jo\\~ao Dinis Ferreira", "authors": "Jo\\~ao Dinis Ferreira, Gabriel Falcao, Juan G\\'omez-Luna, Mohammed\n  Alser, Lois Orosa, Mohammad Sadrosadati, Jeremie S. Kim, Geraldo F. Oliveira,\n  Taha Shahroodi, Anant Nori, Onur Mutlu", "title": "pLUTo: In-DRAM Lookup Tables to Enable Massively Parallel\n  General-Purpose Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data movement between main memory and the processor is a significant\ncontributor to the execution time and energy consumption of memory-intensive\napplications. This data movement bottleneck can be alleviated using\nProcessing-in-Memory (PiM), which enables computation inside the memory chip.\nHowever, existing PiM architectures often lack support for complex operations,\nsince supporting these operations increases design complexity, chip area, and\npower consumption.\n  We introduce pLUTo (processing-in-memory with lookup table [LUT] operations),\na new DRAM substrate that leverages the high area density of DRAM to enable the\nmassively parallel storing and querying of lookup tables (LUTs). The use of\nLUTs enables the efficient execution of complex operations in-memory, which has\nbeen a long-standing challenge in the domain of PiM. When running a\nstate-of-the-art binary neural network in a single DRAM subarray, pLUTo\noutperforms the baseline CPU and GPU implementations by $33\\times$ and\n$8\\times$, respectively, while simultaneously achieving energy savings of\n$110\\times$ and $80\\times$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:10:22 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ferreira", "Jo\u00e3o Dinis", ""], ["Falcao", "Gabriel", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Alser", "Mohammed", ""], ["Orosa", "Lois", ""], ["Sadrosadati", "Mohammad", ""], ["Kim", "Jeremie S.", ""], ["Oliveira", "Geraldo F.", ""], ["Shahroodi", "Taha", ""], ["Nori", "Anant", ""], ["Mutlu", "Onur", ""]]}, {"id": "2104.07857", "submitter": "Samyam Rajbhandari", "authors": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith,\n  Yuxiong He", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last three years, the largest dense deep learning models have grown\nover 1000x to reach hundreds of billions of parameters, while the GPU memory\nhas only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has\nbeen supported primarily though system innovations that allow large models to\nfit in the aggregate GPU memory of multiple GPUs. However, we are getting close\nto the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion\nparameter model for training, and such clusters are simply out of reach for\nmost data scientists. In addition, training models at that scale requires\ncomplex combinations of parallelism techniques that puts a big burden on the\ndata scientists to refactor their model.\n  In this paper we present ZeRO-Infinity, a novel heterogeneous system\ntechnology that leverages GPU, CPU, and NVMe memory to allow for unprecedented\nmodel scale on limited resources without requiring model code refactoring. At\nthe same time it achieves excellent training throughput and scalability,\nunencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models\nwith tens and even hundreds of trillions of parameters for training on current\ngeneration GPU clusters. It can be used to fine-tune trillion parameter models\non a single NVIDIA DGX-2 node, making large models more accessible. In terms of\ntraining throughput and scalability, it sustains over 25 petaflops on 512\nNVIDIA V100 GPUs(40% of peak), while also demonstrating super linear\nscalability. An open source implementation of ZeRO-Infinity is available\nthrough DeepSpeed, a deep learning optimization library that makes distributed\ntraining easy, efficient, and effective.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 02:22:12 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Rajbhandari", "Samyam", ""], ["Ruwase", "Olatunji", ""], ["Rasley", "Jeff", ""], ["Smith", "Shaden", ""], ["He", "Yuxiong", ""]]}, {"id": "2104.07904", "submitter": "Ionut Anghel", "authors": "Tudor Cioara, Ionut Anghel, Marcel Antal, Ioan Salomie, Claudia Antal,\n  Arcas Gabriel Ioan", "title": "An Overview of Digital Twins Application Domains in Smart Energy Grid", "comments": "To be submitted to an IEEE conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Digital Twins offer promising solutions for smart grid challenges related\nto the optimal operation, management, and control of energy assets, for safe\nand reliable distribution of energy. These challenges are more pressing\nnowadays than ever due to the large-scale adoption of distributed renewable\nresources at the edge of the grid. The digital twins are leveraging\ntechnologies such as the Internet of Things, big data analytics, machine\nlearning, and cloud computing, to analyze data from different energy sensors,\nview and verify the status of physical energy assets and extract useful\ninformation to predict and optimize the assets performance. In this paper, we\nwill provide an overview of the Digital Twins application domains in the smart\ngrid while analyzing existing the state of the art literature. We have focused\non the following application domains: energy asset modeling, fault and security\ndiagnosis, operational optimization, and business models. Most of the relevant\nliterature approaches found are published in the last three years showing that\nthe domain of Digital Twins application in smart grid is hot and gradually\ndeveloping. Anyway, there is no unified view on the Digital Twins\nimplementation and integration with energy management processes, thus, much\nwork still needs to be done to understand and automatize the smart grid\nmanagement.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:03:05 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Cioara", "Tudor", ""], ["Anghel", "Ionut", ""], ["Antal", "Marcel", ""], ["Salomie", "Ioan", ""], ["Antal", "Claudia", ""], ["Ioan", "Arcas Gabriel", ""]]}, {"id": "2104.08002", "submitter": "Narendra Chaudhary", "authors": "Narendra Chaudhary, Sanchit Misra, Dhiraj Kalamkar, Alexander\n  Heinecke, Evangelos Georganas, Barukh Ziv, Menachem Adelman, Bharat Kaul", "title": "Efficient and Generic 1D Dilated Convolution Layer for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have found many applications in tasks\ninvolving two-dimensional (2D) data, such as image classification and image\nprocessing. Therefore, 2D convolution layers have been heavily optimized on\nCPUs and GPUs. However, in many applications - for example genomics and speech\nrecognition, the data can be one-dimensional (1D). Such applications can\nbenefit from optimized 1D convolution layers. In this work, we introduce our\nefficient implementation of a generic 1D convolution layer covering a wide\nrange of parameters. It is optimized for x86 CPU architectures, in particular,\nfor architectures containing Intel AVX-512 and AVX-512 BFloat16 instructions.\nWe use the LIBXSMM library's batch-reduce General Matrix Multiplication\n(BRGEMM) kernel for FP32 and BFloat16 precision. We demonstrate that our\nimplementation can achieve up to 80% efficiency on Intel Xeon Cascade Lake and\nCooper Lake CPUs. Additionally, we show the generalization capability of our\nBRGEMM based approach by achieving high efficiency across a range of\nparameters. We consistently achieve higher efficiency than the 1D convolution\nlayer with Intel oneDNN library backend for varying input tensor widths, filter\nwidths, number of channels, filters, and dilation parameters. Finally, we\ndemonstrate the performance of our optimized 1D convolution layer by utilizing\nit in the end-to-end neural network training with real genomics datasets and\nachieve up to 6.86x speedup over the oneDNN library-based implementation on\nCascade Lake CPUs. We also demonstrate the scaling with 16 sockets of\nCascade/Cooper Lake CPUs and achieve significant speedup over eight V100 GPUs\nusing a similar power envelop. In the end-to-end training, we get a speedup of\n1.41x on Cascade Lake with FP32, 1.57x on Cooper Lake with FP32, and 2.27x on\nCooper Lake with BFloat16 over eight V100 GPUs with FP32.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:54:30 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chaudhary", "Narendra", ""], ["Misra", "Sanchit", ""], ["Kalamkar", "Dhiraj", ""], ["Heinecke", "Alexander", ""], ["Georganas", "Evangelos", ""], ["Ziv", "Barukh", ""], ["Adelman", "Menachem", ""], ["Kaul", "Bharat", ""]]}, {"id": "2104.08009", "submitter": "Andreas Kurth", "authors": "Andreas Kurth, Fabian Schuiki, Luca Benini", "title": "Implementing CNN Layers on the Manticore Cluster-Based Many-Core\n  Architecture", "comments": "Technical report. 18 pages, 4 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document presents implementations of fundamental convolutional neural\nnetwork (CNN) layers on the Manticore cluster-based many-core architecture and\ndiscusses their characteristics and trade-offs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:07:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kurth", "Andreas", ""], ["Schuiki", "Fabian", ""], ["Benini", "Luca", ""]]}, {"id": "2104.08094", "submitter": "Riccardo Presotto Mr", "authors": "Claudio Bettini, Gabriele Civitarese, Riccardo Presotto", "title": "Personalized Semi-Supervised Federated Learning for Human Activity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The most effective data-driven methods for human activities recognition (HAR)\nare based on supervised learning applied to the continuous stream of sensors\ndata. However, these methods perform well on restricted sets of activities in\ndomains for which there is a fully labeled dataset. It is still a challenge to\ncope with the intra- and inter-variability of activity execution among\ndifferent subjects in large scale real world deployment. Semi-supervised\nlearning approaches for HAR have been proposed to address the challenge of\nacquiring the large amount of labeled data that is necessary in realistic\nsettings. However, their centralised architecture incurs in the scalability and\nprivacy problems when the process involves a large number of users. Federated\nLearning (FL) is a promising paradigm to address these problems. However, the\nFL methods that have been proposed for HAR assume that the participating users\ncan always obtain labels to train their local models. In this work, we propose\nFedHAR: a novel hybrid method for HAR that combines semi-supervised and\nfederated learning. Indeed, FedHAR combines active learning and label\npropagation to semi-automatically annotate the local streams of unlabeled\nsensor data, and it relies on FL to build a global activity model in a scalable\nand privacy-aware fashion. FedHAR also includes a transfer learning strategy to\npersonalize the global model on each user. We evaluated our method on two\npublic datasets, showing that FedHAR reaches recognition rates and\npersonalization capabilities similar to state-of-the-art FL supervised\napproaches. As a major advantage, FedHAR only requires a very limited number of\nannotated data to populate a pre-trained model and a small number of active\nlearning questions that quickly decrease while using the system, leading to an\neffective and scalable solution for the data scarcity problem of HAR.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:24:18 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 12:56:03 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bettini", "Claudio", ""], ["Civitarese", "Gabriele", ""], ["Presotto", "Riccardo", ""]]}, {"id": "2104.08107", "submitter": "Tobias Heuer", "authors": "Lars Gottesb\\\"uren and Tobias Heuer and Peter Sanders and Sebastian\n  Schlag", "title": "Shared-Memory n-level Hypergraph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a shared-memory algorithm to compute high-quality solutions to the\nbalanced $k$-way hypergraph partitioning problem. This problem asks for a\npartition of the vertex set into $k$ disjoint blocks of bounded size that\nminimizes the connectivity metric (i.e., the sum of the number of different\nblocks connected by each hyperedge). High solution quality is achieved by\nparallelizing the core technique of the currently best sequential partitioner\nKaHyPar: the most extreme $n$-level version of the widely used multilevel\nparadigm, where only a single vertex is contracted on each level. This approach\nis made fast and scalable through intrusive algorithms and data structures that\nallow precise control of parallelism through atomic operations and fine-grained\nlocking. We perform extensive experiments on more than 500 real-world\nhypergraphs with up to $140$ million vertices and two billion pins (sum of\nhyperedge sizes). We find that our algorithm computes solutions that are on par\nwith a comparable configuration of KaHyPar while being an order of magnitude\nfaster on average. Moreover, we show that recent non-multilevel algorithms\nspecifically designed to partition large instances have considerable quality\npenalties and no clear advantage in running time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 13:23:48 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gottesb\u00fcren", "Lars", ""], ["Heuer", "Tobias", ""], ["Sanders", "Peter", ""], ["Schlag", "Sebastian", ""]]}, {"id": "2104.08184", "submitter": "Moming Duan", "authors": "Yu Zhang, Moming Duan, Duo Liu, Li Li, Ao Ren, Xianzhang Chen, Yujuan\n  Tan, Chengliang Wang", "title": "CSAFL: A Clustered Semi-Asynchronous Federated Learning Framework", "comments": "This paper will be presented at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) is an emerging distributed machine learning paradigm\nthat protects privacy and tackles the problem of isolated data islands. At\npresent, there are two main communication strategies of FL: synchronous FL and\nasynchronous FL. The advantages of synchronous FL are that the model has high\nprecision and fast convergence speed. However, this synchronous communication\nstrategy has the risk that the central server waits too long for the devices,\nnamely, the straggler effect which has a negative impact on some time-critical\napplications. Asynchronous FL has a natural advantage in mitigating the\nstraggler effect, but there are threats of model quality degradation and server\ncrash. Therefore, we combine the advantages of these two strategies to propose\na clustered semi-asynchronous federated learning (CSAFL) framework. We evaluate\nCSAFL based on four imbalanced federated datasets in a non-IID setting and\ncompare CSAFL to the baseline methods. The experimental results show that CSAFL\nsignificantly improves test accuracy by more than +5% on the four datasets\ncompared to TA-FedAvg. In particular, CSAFL improves absolute test accuracy by\n+34.4% on non-IID FEMNIST compared to TA-FedAvg.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:51:02 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhang", "Yu", ""], ["Duan", "Moming", ""], ["Liu", "Duo", ""], ["Li", "Li", ""], ["Ren", "Ao", ""], ["Chen", "Xianzhang", ""], ["Tan", "Yujuan", ""], ["Wang", "Chengliang", ""]]}, {"id": "2104.08245", "submitter": "Robert Schade", "authors": "Robert Schade, Tobias Kenter, Hossam Elgabarty, Michael Lass, Ole\n  Sch\\\"utt, Alfio Lazzaro, Hans Pabst, Stephan Mohr, J\\\"urg Hutter, Thomas D.\n  K\\\"uhne, Christian Plessl", "title": "Enabling Electronic Structure-Based Ab-Initio Molecular Dynamics\n  Simulations with Hundreds of Millions of Atoms", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We push the boundaries of electronic structure-based \\textit{ab-initio}\nmolecular dynamics (AIMD) beyond 100 million atoms. This scale is otherwise\nbarely reachable with classical force-field methods or novel neural network and\nmachine learning potentials. We achieve this breakthrough by combining\ninnovations in linear-scaling AIMD, efficient and approximate sparse linear\nalgebra, low and mixed-precision floating-point computation on GPUs, and a\ncompensation scheme for the errors introduced by numerical approximations.\n  The core of our work is the non-orthogonalized local submatrix (NOLSM)\nmethod, which scales very favorably to massively parallel computing systems and\ntranslates large sparse matrix operations into highly parallel, dense matrix\noperations that are ideally suited to hardware accelerators. We demonstrate\nthat the NOLSM method, which is at the center point of each AIMD step, is able\nto achieve a sustained performance of 324 PFLOP/s in mixed FP16/FP32 precision\ncorresponding to an efficiency of 67.7\\% when running on 1536 NVIDIA A100 GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:27:06 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 16:27:46 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Schade", "Robert", ""], ["Kenter", "Tobias", ""], ["Elgabarty", "Hossam", ""], ["Lass", "Michael", ""], ["Sch\u00fctt", "Ole", ""], ["Lazzaro", "Alfio", ""], ["Pabst", "Hans", ""], ["Mohr", "Stephan", ""], ["Hutter", "J\u00fcrg", ""], ["K\u00fchne", "Thomas D.", ""], ["Plessl", "Christian", ""]]}, {"id": "2104.08265", "submitter": "Haiwang Yu", "authors": "Haiwang Yu, Zhihua Dong, Kyle Knoepfel, Meifeng Lin, Brett Viren,\n  Kwangmin Yu", "title": "Evaluation of Portable Acceleration Solutions for LArTPC Simulation\n  Using Wire-Cell Toolkit", "comments": "10 pages, vCHEP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Liquid Argon Time Projection Chamber (LArTPC) technology plays an\nessential role in many current and future neutrino experiments. Accurate and\nfast simulation is critical to developing efficient analysis algorithms and\nprecise physics model projections. The speed of simulation becomes more\nimportant as Deep Learning algorithms are getting more widely used in LArTPC\nanalysis and their training requires a large simulated dataset. Heterogeneous\ncomputing is an efficient way to delegate computing-heavy tasks to specialized\nhardware. However, as the landscape of the compute accelerators is evolving\nfast, it becomes more and more difficult to manually adapt the code constantly\nto the latest hardware or software environments. A solution which is portable\nto multiple hardware architectures while not substantially compromising\nperformance would be very beneficial, especially for long-term projects such as\nthe LArTPC simulations. In search of a portable, scalable and maintainable\nsoftware solution for LArTPC simulations, we have started to explore high-level\nportable programming frameworks that support several hardware backends. In this\npaper, we will present our experience porting the LArTPC simulation code in the\nWire-Cell toolkit to NVIDIA GPUs, first with the CUDA programming model and\nthen with a portable library called Kokkos. Preliminary performance results on\nNVIDIA V100 GPUs and multi-core CPUs will be presented, followed by a\ndiscussion of the factors affecting the performance and plans for future\nimprovements.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:51:46 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Yu", "Haiwang", ""], ["Dong", "Zhihua", ""], ["Knoepfel", "Kyle", ""], ["Lin", "Meifeng", ""], ["Viren", "Brett", ""], ["Yu", "Kwangmin", ""]]}, {"id": "2104.08322", "submitter": "Jeffrey Larson", "authors": "Stephen Hudson, Jeffrey Larson, John-Luke Navarro, and Stefan M. Wild", "title": "libEnsemble: A Library to Coordinate the Concurrent Evaluation of\n  Dynamic Ensembles of Calculations", "comments": null, "journal-ref": null, "doi": "10.1109/TPDS.2021.3082815", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all applications stop scaling at some point; those that don't are\nseldom performant when considering time to solution on anything but\naspirational/unicorn resources. Recognizing these tradeoffs as well as greater\nuser functionality in a near-term exascale computing era, we present\nlibEnsemble, a library aimed at particular scalability- and\ncapability-stretching uses. libEnsemble enables running concurrent instances of\nan application in dynamically allocated ensembles through an extensible Python\nlibrary. We highlight the structure, execution, and capabilities of the library\non leading pre-exascale environments as well as advanced capabilities for\nexascale environments and beyond.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:09:59 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Hudson", "Stephen", ""], ["Larson", "Jeffrey", ""], ["Navarro", "John-Luke", ""], ["Wild", "Stefan M.", ""]]}, {"id": "2104.08335", "submitter": "Suchita Pati", "authors": "Suchita Pati, Shaizeen Aga, Nuwan Jayasena, Matthew D. Sinclair", "title": "Demystifying BERT: Implications for Accelerator Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning in natural language processing (NLP), as realized using\nmodels like BERT (Bi-directional Encoder Representation from Transformer), has\nsignificantly improved language representation with models that can tackle\nchallenging language problems. Consequently, these applications are driving the\nrequirements of future systems. Thus, we focus on BERT, one of the most popular\nNLP transfer learning algorithms, to identify how its algorithmic behavior can\nguide future accelerator design. To this end, we carefully profile BERT\ntraining and identify key algorithmic behaviors which are worthy of attention\nin accelerator design.\n  We observe that while computations which manifest as matrix multiplication\ndominate BERT's overall runtime, as in many convolutional neural networks,\nmemory-intensive computations also feature prominently. We characterize these\ncomputations, which have received little attention so far. Further, we also\nidentify heterogeneity in compute-intensive BERT computations and discuss\nsoftware and possible hardware mechanisms to further optimize these\ncomputations. Finally, we discuss implications of these behaviors as networks\nget larger and use distributed training environments, and how techniques such\nas micro-batching and mixed-precision training scale. Overall, our analysis\nidentifies holistic solutions to optimize systems for BERT-like models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 01:06:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pati", "Suchita", ""], ["Aga", "Shaizeen", ""], ["Jayasena", "Nuwan", ""], ["Sinclair", "Matthew D.", ""]]}, {"id": "2104.08364", "submitter": "Shijian Li", "authors": "Shijian Li, Oren Mangoubi, Lijie Xu, Tian Guo", "title": "Sync-Switch: Hybrid Parameter Synchronization for Distributed Deep\n  Learning", "comments": "15 pages, 16 figures, 6 tables, ICDCS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) has become the de facto way to train deep\nneural networks in distributed clusters. A critical factor in determining the\ntraining throughput and model accuracy is the choice of the parameter\nsynchronization protocol. For example, while Bulk Synchronous Parallel (BSP)\noften achieves better converged accuracy, the corresponding training throughput\ncan be negatively impacted by stragglers. In contrast, Asynchronous Parallel\n(ASP) can have higher throughput, but its convergence and accuracy can be\nimpacted by stale gradients. To improve the performance of synchronization\nprotocol, recent work often focuses on designing new protocols with a heavy\nreliance on hard-to-tune hyper-parameters. In this paper, we design a hybrid\nsynchronization approach that exploits the benefits of both BSP and ASP, i.e.,\nreducing training time while simultaneously maintaining the converged accuracy.\nBased on extensive empirical profiling, we devise a collection of adaptive\npolicies that determine how and when to switch between synchronization\nprotocols. Our policies include both offline ones that target recurring jobs\nand online ones for handling transient stragglers. We implement the proposed\npolicies in a prototype system, called Sync-Switch, on top of TensorFlow, and\nevaluate the training performance with popular deep learning models and\ndatasets. Our experiments show that Sync-Switch achieves up to 5.13X throughput\nspeedup and similar converged accuracy when comparing to BSP. Further, we\nobserve that Sync-Switch achieves 3.8% higher converged accuracy with just\n1.23X the training time compared to training with ASP. Moreover, Sync-Switch\ncan be used in settings when training with ASP leads to divergence errors.\nSync-Switch achieves all of these benefits with very low overhead, e.g., the\nframework overhead can be as low as 1.7% of the total training time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 20:49:28 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 00:25:37 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Shijian", ""], ["Mangoubi", "Oren", ""], ["Xu", "Lijie", ""], ["Guo", "Tian", ""]]}, {"id": "2104.08396", "submitter": "Jongwook Woo", "authors": "Mohsen Alam, Benjamin Cevallos, Oscar Flores, Randall Lunetto, Kotaro\n  Yayoshi, Jongwook Woo", "title": "Yelp Dataset Analysis using Scalable Big Data", "comments": "4 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Yelp has served and will continue to serve as a data-driven application. Yelp\nhas published a dataset containing business information, reviews, user\ninformation, and check-in information. This paper will examine this dataset to\nprovide descriptive analytics to understand business performance, geo-spatial\ndistribution of businesses, reviewers' rating and other characteristics, and\ntemporal distribution of check-ins in business premises. With these analysis we\nare able to establish that yelp reviews, tips, elite users and check ins have\nstarted to plummet over the years. Coincidentally, the paper also establishes\nthat Canadians have a more stable star ratings as well as sentiment ratings\nwhen compared to Americans.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 22:53:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Alam", "Mohsen", ""], ["Cevallos", "Benjamin", ""], ["Flores", "Oscar", ""], ["Lunetto", "Randall", ""], ["Yayoshi", "Kotaro", ""], ["Woo", "Jongwook", ""]]}, {"id": "2104.08416", "submitter": "Roger Ara\\'ujo", "authors": "Roger R. F. Ara\\'ujo, Lutz Gross, Samuel Xavier-de-Souza", "title": "Boosting Memory Access Locality of the Spectral Element Method with\n  Hilbert Space-Filling Curves", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm based on Hilbert space-filling curves to reorder mesh\nelements in memory for use with the Spectral Element Method, aiming to attain\nfewer cache misses, better locality of data reference and faster execution. We\npresent a technique to numerically simulate acoustic wave propagation in 2D\ndomains using the Spectral Element Method, and discuss computational\nperformance aspects of this procedure. We reorder mesh-related data via Hilbert\ncurves to achieve sizable reductions in execution time under several mesh\nconfigurations in shared-memory systems. Our experiments show that the Hilbert\ncurve approach works well with meshes of several granularities and also with\nsmall and large variations in element sizes, achieving reductions between 9%\nand 25% in execution time when compared to three other ordering schemes.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 01:27:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ara\u00fajo", "Roger R. F.", ""], ["Gross", "Lutz", ""], ["Xavier-de-Souza", "Samuel", ""]]}, {"id": "2104.08461", "submitter": "Yi-Jheng Lin", "authors": "Yi-Jheng Lin, Cheng-Shang Chang", "title": "PPoL: A Periodic Channel Hopping Sequence with Nearly Full Rendezvous\n  Diversity", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a periodic channel hopping (CH) sequence, called PPoL (Packing the\nPencil of Lines in a finite projective plane), for the multichannel rendezvous\nproblem. When $N-1$ is a prime power, its period is $N^2-N+1$, and the number\nof distinct rendezvous channels of PPoL is at least $N-2$ for any nonzero clock\ndrift. By channel remapping, we construct CH sequences with the maximum\ntime-to-rendezvous (MTTR) bounded by $N^2+3N+3$ if the number of commonly\navailable channels is at least two. This achieves a roughly 50% reduction of\nthe state-of-the-art MTTR bound in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:57:23 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lin", "Yi-Jheng", ""], ["Chang", "Cheng-Shang", ""]]}, {"id": "2104.08542", "submitter": "Huifeng Guo", "authors": "Huifeng Guo, Wei Guo, Yong Gao, Ruiming Tang, Xiuqiang He, Wenzhi Liu", "title": "ScaleFreeCTR: MixCache-based Distributed Training System for CTR Models\n  with Huge Embedding Table", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the superior feature representation ability of deep learning,\nvarious deep Click-Through Rate (CTR) models are deployed in the commercial\nsystems by industrial companies. To achieve better performance, it is necessary\nto train the deep CTR models on huge volume of training data efficiently, which\nmakes speeding up the training process an essential problem. Different from the\nmodels with dense training data, the training data for CTR models is usually\nhigh-dimensional and sparse. To transform the high-dimensional sparse input\ninto low-dimensional dense real-value vectors, almost all deep CTR models adopt\nthe embedding layer, which easily reaches hundreds of GB or even TB. Since a\nsingle GPU cannot afford to accommodate all the embedding parameters, when\nperforming distributed training, it is not reasonable to conduct the\ndata-parallelism only. Therefore, existing distributed training platforms for\nrecommendation adopt model-parallelism. Specifically, they use CPU (Host)\nmemory of servers to maintain and update the embedding parameters and utilize\nGPU worker to conduct forward and backward computations. Unfortunately, these\nplatforms suffer from two bottlenecks: (1) the latency of pull \\& push\noperations between Host and GPU; (2) parameters update and synchronization in\nthe CPU servers. To address such bottlenecks, in this paper, we propose the\nScaleFreeCTR: a MixCache-based distributed training system for CTR models.\nSpecifically, in SFCTR, we also store huge embedding table in CPU but utilize\nGPU instead of CPU to conduct embedding synchronization efficiently. To reduce\nthe latency of data transfer between both GPU-Host and GPU-GPU, the MixCache\nmechanism and Virtual Sparse Id operation are proposed. Comprehensive\nexperiments and ablation studies are conducted to demonstrate the effectiveness\nand efficiency of SFCTR.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:36:19 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:11:46 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Guo", "Huifeng", ""], ["Guo", "Wei", ""], ["Gao", "Yong", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""], ["Liu", "Wenzhi", ""]]}, {"id": "2104.08571", "submitter": "Robert Clucas", "authors": "Robert Clucas, Philip Blakely, Nikolaos Nikiforakis", "title": "Ripple : Simplified Large-Scale Computation on Heterogeneous\n  Architectures with Polymorphic Data Layout", "comments": "Preprint submitted to the Journal of Parallel and Distributed\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are now used for a wide range of problems within HPC. However, making\nefficient use of the computational power available with multiple GPUs is\nchallenging. The main challenges in achieving good performance are memory\nlayout, affecting memory bandwidth, effective use of the memory spaces with a\nGPU, inter-GPU communication, and synchronization. We address these problems\nwith the Ripple library, which provides a unified view of the computational\nspace across multiple dimensions and multiple GPUs, allows polymorphic data\nlayout, and provides a simple graph interface to describe an algorithm from\nwhich inter-GPU data transfers can be optimally scheduled. We describe the\nabstractions provided by Ripple to allow complex computations to be described\nsimply, and to execute efficiently across many GPUs with minimal overhead. We\nshow performance results for a number of examples, from particle motion to\nfinite-volume methods and the eikonal equation, as well as showing good strong\nand weak scaling results across multiple GPUs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:25:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Clucas", "Robert", ""], ["Blakely", "Philip", ""], ["Nikiforakis", "Nikolaos", ""]]}, {"id": "2104.08644", "submitter": "Avery Miller", "authors": "Colin Krisko, Avery Miller", "title": "Labeling Schemes for Deterministic Radio Multi-Broadcast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-broadcast problem in arbitrary connected radio networks\nconsisting of $n$ nodes. There are $k$ designated source nodes for some fixed\n$k \\in \\{1,\\ldots,n\\}$, and each source node has a distinct piece of\ninformation that it wants to share with all nodes in the network. We set out to\ndetermine the shortest possible labels so that multi-broadcast can be solved\ndeterministically in the labeled radio network by some universal deterministic\ndistributed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 21:19:12 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Krisko", "Colin", ""], ["Miller", "Avery", ""]]}, {"id": "2104.09014", "submitter": "Pulasthi Wickramasinghe", "authors": "Pulasthi Wickramasinghe, Geoffrey Fox", "title": "Multidimensional Scaling for Gene Sequence Data with Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multidimensional scaling of gene sequence data has long played a vital role\nin analysing gene sequence data to identify clusters and patterns. However the\ncomputation complexities and memory requirements of state-of-the-art\ndimensional scaling algorithms make it infeasible to scale to large datasets.\nIn this paper we present an autoencoder-based dimensional reduction model which\ncan easily scale to datasets containing millions of gene sequences, while\nattaining results comparable to state-of-the-art MDS algorithms with minimal\nresource requirements. The model also supports out-of-sample data points with a\n99.5%+ accuracy based on our experiments. The proposed model is evaluated\nagainst DAMDS with a real world fungi gene sequence dataset. The presented\nresults showcase the effectiveness of the autoencoder-based dimension reduction\nmodel and its advantages.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 02:14:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wickramasinghe", "Pulasthi", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2104.09075", "submitter": "Mohamed Wahib", "authors": "Albert Njoroge Kahira, Truong Thao Nguyen, Leonardo Bautista Gomez,\n  Ryousei Takano, Rosa M Badia, Mohamed Wahib", "title": "An Oracle for Guiding Large-Scale Model/Hybrid Parallel Training of\n  Convolutional Neural Networks", "comments": "The International ACM Symposium on High-Performance Parallel and\n  Distributed Computing 2021 (HPDC'21)", "journal-ref": null, "doi": "10.1145/3431379.3460644", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Network (DNN) frameworks use distributed training to enable\nfaster time to convergence and alleviate memory capacity limitations when\ntraining large models and/or using high dimension inputs. With the steady\nincrease in datasets and model sizes, model/hybrid parallelism is deemed to\nhave an important role in the future of distributed training of DNNs. We\nanalyze the compute, communication, and memory requirements of Convolutional\nNeural Networks (CNNs) to understand the trade-offs between different\nparallelism approaches on performance and scalability. We leverage our\nmodel-driven analysis to be the basis for an oracle utility which can help in\ndetecting the limitations and bottlenecks of different parallelism approaches\nat scale. We evaluate the oracle on six parallelization strategies, with four\nCNN models and multiple datasets (2D and 3D), on up to 1024 GPUs. The results\ndemonstrate that the oracle has an average accuracy of about 86.74% when\ncompared to empirical results, and as high as 97.57% for data parallelism.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:45:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kahira", "Albert Njoroge", ""], ["Nguyen", "Truong Thao", ""], ["Gomez", "Leonardo Bautista", ""], ["Takano", "Ryousei", ""], ["Badia", "Rosa M", ""], ["Wahib", "Mohamed", ""]]}, {"id": "2104.09091", "submitter": "Jim Samuel", "authors": "Jim Samuel, Margaret Brennan-Tonetta, Yana Samuel, Pradeep Subedi and\n  Jack Smith", "title": "Strategies for Democratization of Supercomputing: Availability,\n  Accessibility and Usability of High Performance Computing for Education and\n  Practice of Big Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been an increasing interest in and growing need for high\nperformance computing (HPC), popularly known as supercomputing, in domains such\nas textual analytics, business domains analytics, forecasting and natural\nlanguage processing (NLP), in addition to the relatively mature supercomputing\ndomains of quantum physics and biology. HPC has been widely used in computer\nscience (CS) and other traditionally computation intensive disciplines, but has\nremained largely siloed away from the vast array of social, behavioral,\nbusiness and economics disciplines. However, with ubiquitous big data, there is\na compelling need to make HPC technologically and economically accessible, easy\nto use, and operationally democratized. Therefore, this research focuses on\nmaking two key contributions, the first is the articulation of strategies based\non availability, accessibility and usability for the demystification and\ndemocratization of HPC, based on an analytical review of Caliburn, a notable\nsupercomputer at its inception. The second contribution is a set of principles\nfor HPC adoption based on an experiential narrative of HPC usage for textual\nanalytics and NLP of social media data from a first time user perspective.\nBoth, the HPC usage process and the output of the early stage analytics are\nsummarized. This research study synthesizes expert input on HPC democratization\nstrategies, and chronicles the challenges and opportunities from a\nmultidisciplinary perspective, of a case of rapid adoption of supercomputing\nfor textual analytics and NLP. Deductive logic is used to identify strategies\nwhich can lead to efficacious engagement, adoption, production and sustained\nusage for research, teaching, application and innovation by researchers,\nfaculty, professionals and students across a broad range of disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 07:32:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Samuel", "Jim", ""], ["Brennan-Tonetta", "Margaret", ""], ["Samuel", "Yana", ""], ["Subedi", "Pradeep", ""], ["Smith", "Jack", ""]]}, {"id": "2104.09096", "submitter": "Thomas Hayes", "authors": "Varsha Dani, Aayush Gupta, Thomas P. Hayes, Seth Pettie", "title": "Wake Up and Join Me! An Energy-Efficient Algorithm for Maximal Matching\n  in Radio Networks", "comments": "14 pages, 2 figures, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider networks of small, autonomous devices that communicate with each\nother wirelessly. Minimizing energy usage is an important consideration in\ndesigning algorithms for such networks, as battery life is a crucial and\nlimited resource. Working in a model where both sending and listening for\nmessages deplete energy, we consider the problem of finding a maximal matching\nof the nodes in a radio network of arbitrary and unknown topology.\n  We present a distributed randomized algorithm that produces, with high\nprobability, a maximal matching. The maximum energy cost per node is $O(\\log^2\nn)$, where $n$ is the size of the network. The total latency of our algorithm\nis $O(n \\log n)$ time steps. We observe that there exist families of network\ntopologies for which both of these bounds are simultaneously optimal up to\npolylog factors, so any significant improvement will require additional\nassumptions about the network topology.\n  We also consider the related problem of assigning, for each node in the\nnetwork, a neighbor to back up its data in case of node failure. Here, a key\ngoal is to minimize the maximum load, defined as the number of nodes assigned\nto a single node. We present a decentralized low-energy algorithm that finds a\nneighbor assignment whose maximum load is at most a polylog($n$) factor bigger\nthat the optimum.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 07:37:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dani", "Varsha", ""], ["Gupta", "Aayush", ""], ["Hayes", "Thomas P.", ""], ["Pettie", "Seth", ""]]}, {"id": "2104.09355", "submitter": "Sam Partee", "authors": "Sam Partee, Matthew Ellis, Alessandro Rigazzi, Scott Bachman, Gustavo\n  Marques, Andrew Shao, Benjamin Robbins", "title": "Using Machine Learning at Scale in HPC Simulations with SmartSim: An\n  Application to Ocean Climate Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.LG physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the first climate-scale, numerical ocean simulations improved\nthrough distributed, online inference of Deep Neural Networks (DNN) using\nSmartSim. SmartSim is a library dedicated to enabling online analysis and\nMachine Learning (ML) for traditional HPC simulations. In this paper, we detail\nthe SmartSim architecture and provide benchmarks including online inference\nwith a shared ML model on heterogeneous HPC systems. We demonstrate the\ncapability of SmartSim by using it to run a 12-member ensemble of global-scale,\nhigh-resolution ocean simulations, each spanning 19 compute nodes, all\ncommunicating with the same ML architecture at each simulation timestep. In\ntotal, 970 billion inferences are collectively served by running the ensemble\nfor a total of 120 simulated years. Finally, we show our solution is stable\nover the full duration of the model integrations, and that the inclusion of\nmachine learning has minimal impact on the simulation runtimes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:27:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Partee", "Sam", ""], ["Ellis", "Matthew", ""], ["Rigazzi", "Alessandro", ""], ["Bachman", "Scott", ""], ["Marques", "Gustavo", ""], ["Shao", "Andrew", ""], ["Robbins", "Benjamin", ""]]}, {"id": "2104.09455", "submitter": "Jack Kosaian", "authors": "Jack Kosaian, K. V. Rashmi", "title": "Arithmetic-Intensity-Guided Fault Tolerance for Neural Network Inference\n  on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NNs) are increasingly employed in domains that require high\nreliability, such as scientific computing and safety-critical systems, as well\nas in environments more prone to unreliability (e.g., soft errors), such as on\nspacecraft. As recent work has shown that faults in NN inference can lead to\nmispredictions and safety hazards, it is critical to impart fault tolerance to\nNN inference. Algorithm-based fault tolerance (ABFT) is emerging as an\nappealing approach for efficient fault tolerance in NNs.\n  In this work, we identify new, unexploited opportunities for low-overhead\nABFT for NN inference: current inference-optimized GPUs have high\ncompute-to-memory-bandwidth ratios, while many layers of current and emerging\nNNs have low arithmetic intensity. This leaves many convolutional and\nfully-connected layers in NNs memory-bandwidth-bound. These layers thus exhibit\nstalls in computation that could be filled by redundant execution, but that\ncurrent approaches to ABFT for NN inference cannot exploit.\n  To reduce execution-time overhead for such memory-bandwidth-bound layers, we\nfirst investigate thread-level ABFT schemes for inference-optimized GPUs that\nexploit this fine-grained compute underutilization. We then propose\nintensity-guided ABFT, an adaptive, arithmetic-intensity-guided approach to\nABFT that selects the best ABFT scheme for each individual layer between\ntraditional approaches to ABFT, which are suitable for compute-bound layers,\nand thread-level ABFT, which is suitable for memory-bandwidth-bound layers.\nThrough this adaptive approach, intensity-guided ABFT reduces execution-time\noverhead by 1.09--5.3$\\times$ across a variety of NNs, lowering the cost of\nfault tolerance for current and future NN inference workloads.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:13:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kosaian", "Jack", ""], ["Rashmi", "K. V.", ""]]}, {"id": "2104.09502", "submitter": "A. Yavuz Oruc", "authors": "A. Yavuz Oruc, A. Atmaca, Y. Nevzat Sengun, A. Semi Yeniyol", "title": "CodeAPeel: An Integrated and Layered Learning Technology For Computer\n  Architecture Courses", "comments": "Minor revision and some typos are fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a versatile, multi-layered technology to help support\nteaching and learning core computer architecture concepts. This technology,\ncalled CodeAPeel is already implemented in one particular form to describe\ninstruction processing in compiler, assembly, and machine layers of a generic\ninstruction set architecture by a comprehensive simulation of its\nfetch-decode-execute cycle as well as animation of the behavior of its CPU\nregisters, RAM, VRAM, STACK memories, various control registers, and graphics\nscreen. Unlike most educational CPU simulators that simulate a real processor\nsuch as MIPS or RISC-V, CodeAPeel is designed and implemented as a generic RISC\ninstruction set architecture simulator with both scalar and vector instructions\nto provide a dual-mode processor simulator as described by Flynn's\nclassification of SISD and SIMD processors. Vectorization of operations is\nbuilt into the instruction repertoire of CodeAPeel, making it straightforward\nto simulate such processors with powerful vector instructions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 00:04:49 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:15:43 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 19:56:22 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Oruc", "A. Yavuz", ""], ["Atmaca", "A.", ""], ["Sengun", "Y. Nevzat", ""], ["Yeniyol", "A. Semi", ""]]}, {"id": "2104.09565", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi, Daniel McDonald and Rob Knight", "title": "Accelerating key bioinformatics tasks 100-fold by improving memory\n  access", "comments": "6 pages, 3 tables, 7 algorithms, To be published in Proceedings of\n  PEARC21", "journal-ref": null, "doi": "10.1145/3437359.3465562", "report-no": null, "categories": "cs.DC q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most experimental sciences now rely on computing, and biological sciences are\nno exception. As datasets get bigger, so do the computing costs, making proper\noptimization of the codes used by scientists increasingly important. Many of\nthe codes developed in recent years are based on the Python-based NumPy, due to\nits ease of use and good performance characteristics. The composable nature of\nNumPy, however, does not generally play well with the multi-tier nature of\nmodern CPUs, making any non-trivial multi-step algorithm limited by the\nexternal memory access speeds, which are hundreds of times slower than the\nCPU's compute capabilities. In order to fully utilize the CPU compute\ncapabilities, one must keep the working memory footprint small enough to fit in\nthe CPU caches, which requires splitting the problem into smaller portions and\nfusing together as many steps as possible. In this paper, we present changes\nbased on these principles to two important functions in the scikit-bio library,\nprincipal coordinates analysis and the Mantel test, that resulted in over 100x\nspeed improvement in these widely used, general-purpose tools.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:52:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Sfiligoi", "Igor", ""], ["McDonald", "Daniel", ""], ["Knight", "Rob", ""]]}, {"id": "2104.09616", "submitter": "Callie Hao", "authors": "Lixiang Li, Yao Chen, Zacharie Zirnheld, Pan Li, and Cong Hao", "title": "MELOPPR: Software/Hardware Co-design for Memory-efficient Low-latency\n  Personalized PageRank", "comments": "Accepted by IEEE Design Automation Conference, 2021 (DAC'21). Six\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized PageRank (PPR) is a graph algorithm that evaluates the\nimportance of the surrounding nodes from a source node. Widely used in social\nnetwork related applications such as recommender systems, PPR requires\nreal-time responses (latency) for a better user experience. Existing works\neither focus on algorithmic optimization for improving precision while\nneglecting hardware implementations or focus on distributed global graph\nprocessing on large-scale systems for improving throughput rather than response\ntime. Optimizing low-latency local PPR algorithm with a tight memory budget on\nedge devices remains unexplored. In this work, we propose a memory-efficient,\nlow-latency PPR solution, namely MeLoPPR, with largely reduced memory\nrequirement and a flexible trade-off between latency and precision. MeLoPPR is\ncomposed of stage decomposition and linear decomposition and exploits the node\nscore sparsity: Through stage and linear decomposition, MeLoPPR breaks the\ncomputation on a large graph into a set of smaller sub-graphs, that\nsignificantly saves the computation memory; Through sparsity exploitation,\nMeLoPPR selectively chooses the sub-graphs that contribute the most to the\nprecision to reduce the required computation. In addition, through\nsoftware/hardware co-design, we propose a hardware implementation on a hybrid\nCPU and FPGA accelerating platform, that further speeds up the sub-graph\ncomputation. We evaluate the proposed MeLoPPR on memory-constrained devices\nincluding a personal laptop and Xilinx Kintex-7 KC705 FPGA using six real-world\ngraphs. First, MeLoPPR demonstrates significant memory saving by 1.5x to 13.4x\non CPU and 73x to 8699x on FPGA. Second, MeLoPPR allows flexible trade-offs\nbetween precision and execution time: when the precision is 80%, the speedup on\nCPU is up to 15x and up to 707x on FPGA; when the precision is around 90%, the\nspeedup is up to 70x on FPGA.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 04:37:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Lixiang", ""], ["Chen", "Yao", ""], ["Zirnheld", "Zacharie", ""], ["Li", "Pan", ""], ["Hao", "Cong", ""]]}, {"id": "2104.09801", "submitter": "Bishakh Chandra Ghosh", "authors": "Bishakh Chandra Ghosh, Tanay Bhartia, Sourav Kanti Addya, Sandip\n  Chakraborty", "title": "Leveraging Public-Private Blockchain Interoperability for Closed\n  Consortium Interfacing", "comments": "10 pages, 12 figures, accepted for publication in IEEE INFOCOM 2021", "journal-ref": "IEEE INFOCOM 2021", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing adoption of private blockchain platforms, consortia\noperating in various sectors such as trade, finance, logistics, etc., are\nbecoming common. Despite having the benefits of a completely decentralized\narchitecture which supports transparency and distributed control, existing\nprivate blockchains limit the data, assets, and processes within its closed\nboundary, which restricts secure and verifiable service provisioning to the\nend-consumers. Thus, platforms such as e-commerce with multiple sellers or\ncloud federation with a collection of cloud service providers cannot be\ndecentralized with the existing blockchain platforms. This paper proposes a\ndecentralized gateway architecture interfacing private blockchain with\nend-users by leveraging the unique combination of public and private blockchain\nplatforms through interoperation. Through the use case of decentralized cloud\nfederations, we have demonstrated the viability of the solution. Our testbed\nimplementation with Ethereum and Hyperledger Fabric, with three service\nproviders, shows that such consortium can operate within an acceptable response\nlatency while scaling up to 64 parallel requests per second for cloud\ninfrastructure provisioning. Further analysis over the Mininet emulation\nplatform indicates that the platform can scale well with minimal impact over\nthe latency as the number of participating service providers increases.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:25:33 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ghosh", "Bishakh Chandra", ""], ["Bhartia", "Tanay", ""], ["Addya", "Sourav Kanti", ""], ["Chakraborty", "Sandip", ""]]}, {"id": "2104.09902", "submitter": "Adnane Khattabi", "authors": "Danny Hendler, Adnane Khattabi, Alessia Milani, Corentin Travers", "title": "Upper and Lower Bounds for Deterministic Approximate Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relaxing the sequential specification of shared objects has been proposed as\na promising approach to obtain implementations with better complexity. In this\npaper, we study the step complexity of relaxed variants of two common shared\nobjects: max registers and counters. In particular, we consider the\n$k$-multiplicative-accurate max register and the $k$-multiplicative-accurate\ncounter, where read operations are allowed to err by a multiplicative factor of\n$k$ (for some $k \\in \\mathbb{N}$). More accurately, reads are allowed to return\nan approximate value $x$ of the maximum value $v$ previously written to the max\nregister, or of the number $v$ of increments previously applied to the counter,\nrespectively, such that $v/k \\leq x \\leq v \\cdot k$. We provide upper and lower\nbounds on the complexity of implementing these objects in a wait-free manner in\nthe shared memory model.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 11:24:29 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Hendler", "Danny", ""], ["Khattabi", "Adnane", ""], ["Milani", "Alessia", ""], ["Travers", "Corentin", ""]]}, {"id": "2104.09949", "submitter": "Stylianos Venieris", "authors": "Mario Almeida, Stefanos Laskaridis, Stylianos I. Venieris, Ilias\n  Leontiadis, Nicholas D. Lane", "title": "DynO: Dynamic Onloading of Deep Neural Networks from Cloud to Device", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an explosive growth of mobile and embedded\napplications using convolutional neural networks(CNNs). To alleviate their\nexcessive computational demands, developers have traditionally resorted to\ncloud offloading, inducing high infrastructure costs and a strong dependence on\nnetworking conditions. On the other end, the emergence of powerful SoCs is\ngradually enabling on-device execution. Nonetheless, low- and mid-tier\nplatforms still struggle to run state-of-the-art CNNs sufficiently. In this\npaper, we present DynO, a distributed inference framework that combines the\nbest of both worlds to address several challenges, such as device\nheterogeneity, varying bandwidth and multi-objective requirements. Key\ncomponents that enable this are its novel CNN-specific data packing method,\nwhich exploits the variability of precision needs in different parts of the CNN\nwhen onloading computation, and its novel scheduler that jointly tunes the\npartition point and transferred data precision at run time to adapt inference\nto its execution environment. Quantitative evaluation shows that DynO\noutperforms the current state-of-the-art, improving throughput by over an order\nof magnitude over device-only execution and up to 7.9x over competing CNN\noffloading systems, with up to 60x less data transferred.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:20:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Almeida", "Mario", ""], ["Laskaridis", "Stefanos", ""], ["Venieris", "Stylianos I.", ""], ["Leontiadis", "Ilias", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2104.09971", "submitter": "Valentin Zieglmeier", "authors": "Valentin Zieglmeier and Gabriel Loyola Daiqui", "title": "GDPR-Compliant Use of Blockchain for Secure Usage Logs", "comments": "Peer-reviewed version accepted for publication in the proceedings of\n  the 2021 International Conference on Evaluation and Assessment in Software\n  Engineering (EASE'21)", "journal-ref": null, "doi": "10.1145/3463274.3463349", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unique properties of blockchain enable central requirements of\ndistributed secure logging: Immutability, integrity, and availability.\nEspecially when providing transparency about data usages, a blockchain-based\nsecure log can be beneficial, as no trusted third party is required. Yet, with\ndata governed by privacy legislation such as the GDPR or CCPA, the core\nadvantage of immutability becomes a liability. After a rightful request, an\nindividual's personal data need to be rectified or deleted, which is impossible\nin an immutable blockchain. To solve this issue, we exploit a legal property of\npseudonymized data: They are only regarded personal data if they can be\nassociated with an individual's identity. We make use of this fact by\npresenting P3, a pseudonym provisioning system for secure usage logs including\na protocol for recording new usages. For each new block, a one-time transaction\npseudonym is generated. The pseudonym generation algorithm guarantees\nunlinkability and enables proof of ownership. These properties enable\nGDPR-compliant use of blockchain, as data subjects can exercise their legal\nrights with regards to their personal data. The new-usage protocol ensures\nnon-repudiation, and therefore accountability and liability. Most importantly,\nour approach does not require a trusted third party and is independent of the\nutilized blockchain software.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:03:01 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 09:21:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zieglmeier", "Valentin", ""], ["Daiqui", "Gabriel Loyola", ""]]}, {"id": "2104.10013", "submitter": "Ameya Jagtap Dr", "authors": "Khemraj Shukla, Ameya D. Jagtap, George Em Karniadakis", "title": "Parallel Physics-Informed Neural Networks via Domain Decomposition", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a distributed framework for the physics-informed neural networks\n(PINNs) based on two recent extensions, namely conservative PINNs (cPINNs) and\nextended PINNs (XPINNs), which employ domain decomposition in space and in\ntime-space, respectively. This domain decomposition endows cPINNs and XPINNs\nwith several advantages over the vanilla PINNs, such as parallelization\ncapacity, large representation capacity, efficient hyperparameter tuning, and\nis particularly effective for multi-scale and multi-physics problems. Here, we\npresent a parallel algorithm for cPINNs and XPINNs constructed with a hybrid\nprogramming model described by MPI $+$ X, where X $\\in\n\\{\\text{CPUs},~\\text{GPUs}\\}$. The main advantage of cPINN and XPINN over the\nmore classical data and model parallel approaches is the flexibility of\noptimizing all hyperparameters of each neural network separately in each\nsubdomain. We compare the performance of distributed cPINNs and XPINNs for\nvarious forward problems, using both weak and strong scalings. Our results\nindicate that for space domain decomposition, cPINNs are more efficient in\nterms of communication cost but XPINNs provide greater flexibility as they can\nalso handle time-domain decomposition for any differential equations, and can\ndeal with any arbitrarily shaped complex subdomains. To this end, we also\npresent an application of the parallel XPINN method for solving an inverse\ndiffusion problem with variable conductivity on the United States map, using\nten regions as subdomains.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:41:54 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 00:57:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Shukla", "Khemraj", ""], ["Jagtap", "Ameya D.", ""], ["Karniadakis", "George Em", ""]]}, {"id": "2104.10022", "submitter": "Bilal Farooq", "authors": "Sayed Mehdi Meshkani and Bilal Farooq", "title": "A Decentralized Shared CAV System Design and Application", "comments": "Presented at TRISTAN X, 17-21 June 2019, Hamilton Island, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel heuristic two-step algorithm for shared\nridehailing in which users can share their rides with only one more user. The\nalgorithm, which is centrally formulated, starts with matching users and\ncreating a set of passenger pairs in step 1 and is followed by solving an\nassignment problem to assign passenger pairs to the vehicles. To solve the\nproblem of high computational time in dynamic ride-matching problems, we\npropose a distributed system that is based on vehicle to infrastructure (V2I)\nand infrastructure to infrastructure (I2I) communication. To evaluate the\ndistributed system's performance, we compare it with the proposed centralized\nridehailing algorithm. Both centralized and distributed systems are implemented\nin a micro-traffic simulator to assess their performance and their impact on\ntraffic congestion. Downtown Toronto road network was chosen as the study area.\nBased on our obtained results, the service rate of the distributed system was\n91.59% which is close to 95.80% in the centralized system. However, the\ndistributed system yielded much lower computational time compared to\ncentralized. Furthermore, the scalability of the distributed system was shown\nby testing it on a small network and comparing with the entire network.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:40:45 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Meshkani", "Sayed Mehdi", ""], ["Farooq", "Bilal", ""]]}, {"id": "2104.10025", "submitter": "Ted Ralphs", "authors": "Stephen J. Maher and Ted K. Ralphs and Yuji Shinano", "title": "Assessing the Effectiveness of (Parallel) Branch-and-bound Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": "Laboratory for Computational Optimization Research @ Lehigh (COR@L)\n  Technical Report 19T-017", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical studies are fundamental in assessing the effectiveness of\nimplementations of branch-and-bound algorithms. The complexity of such\nimplementations makes empirical study difficult for a wide variety of reasons.\nVarious attempts have been made to develop and codify a set of standard\ntechniques for the assessment of optimization algorithms and their software\nimplementations; however, most previous work has been focused on classical\nsequential algorithms. Since parallel computation has become increasingly\nmainstream, it is necessary to re-examine and modernize these practices. In\nthis paper, we propose a framework for assessment based on the notion that\n\\emph{resource consumption} is at the heart of what we generally refer to as\nthe \"effectiveness\" of an implementation. The proposed framework carefully\ndistinguishes between an implementation's baseline efficiency, the efficacy\nwith which it utilizes a fixed allocation of resources, and its scalability, a\nmeasure of how the efficiency changes as resources (typically additional\ncomputing cores) are added or removed. Efficiency is typically applied to\nsequential implementations, whereas scalability is applied to parallel\nimplementations. Efficiency and scalability are both important contributors in\ndetermining the overall effectiveness of a given parallel implementation, but\nthe goal of improved efficiency is often at odds with the goal of improved\nscalability. Within the proposed framework, we review the challenges to\neffective evaluation and discuss the strengths and weaknesses of existing\nmethods of assessment.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 01:53:17 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Maher", "Stephen J.", ""], ["Ralphs", "Ted K.", ""], ["Shinano", "Yuji", ""]]}, {"id": "2104.10027", "submitter": "Ilche Georgievski", "authors": "Ilche Georgievski", "title": "HTN Planning Domain for Deployment of Cloud Applications", "comments": "Published in the proceedings of the 10th International Planning\n  Competition: Planner and Domain Abstracts - Hierarchical Task Network (HTN)\n  Planning Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud providers are facing a complex problem in configuring software\napplications ready for deployment on their infrastructures. Hierarchical Task\nNetwork (HTN) planning can provide effective means to solve such deployment\nproblems. We present an HTN planning domain that models deployment problems as\nfound in realistic Cloud environments.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:36:42 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 09:16:17 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Georgievski", "Ilche", ""]]}, {"id": "2104.10039", "submitter": "Morteza Ramezani", "authors": "Morteza Ramezani, Anand Sivasubramaniam, Mahmut T. Kandemir", "title": "GraphGuess: Approximate Graph Processing System with Adaptive Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based data structures have drawn great attention in recent years. The\nlarge and rapidly growing trend on developing graph processing systems focus\nmostly on improving the performance by preprocessing the input graph and\nmodifying its layout. These systems usually take several hours to days to\ncomplete processing a single graph on high-end machines, let alone the overhead\nof pre-processing which most of the time can be dominant. Yet for most of graph\napplications the exact answer is not always crucial, and providing a rough\nestimate of the final result is adequate. Approximate computing is introduced\nto trade off accuracy of results for computation or energy savings that could\nnot be achieved by conventional techniques alone. Although various computing\nplatforms and application domains benefit from approximate computing, it has\nnot been thoroughly explored yet in the context of graph processing systems.\n  In this work, we design, implement and evaluate GraphGuess, inspired from the\ndomain of approximate graph theory and extend it to a general, practical graph\nprocessing system. GraphGuess is essentially an approximate graph processing\ntechnique with adaptive correction, which can be implemented on top of any\ngraph processing system. We build a vertex-centric processing system based on\nGraphGuess, where it allows the user to trade off accuracy for better\nperformance. Our experimental studies show that using GraphGuess can\nsignificantly reduce the processing time for large scale graphs while\nmaintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:50:50 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ramezani", "Morteza", ""], ["Sivasubramaniam", "Anand", ""], ["Kandemir", "Mahmut T.", ""]]}, {"id": "2104.10042", "submitter": "Vasiliy Gurianov", "authors": "Vasyliy I. Gurianov", "title": "A Brief Overview of the UML Scientific Profile", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article provides a brief overview of the UML SP (UML Scientific\nProfile). It is an Object-Oriented Simulation Language and may find usage in\nOOS and ABS. UML SP allows for the application of Unified Process methodology\nfor the development of simulation software. It supports the first three stages\nof simulation: (1) definition of simulation objectives, definition of\nrequirements for simulation software, (2) conceptual modelling, (3) formal\ndescription and partially, (4) programming. UML SPs positioning is that of a\nlanguage for scientific simulation. It can easily be implemented in UML editors\nthat support UML profiles. The project is available as open source on the\nGitHub platform.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:10:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Gurianov", "Vasyliy I.", ""]]}, {"id": "2104.10073", "submitter": "Jun-Jie Zhang", "authors": "Xiao-Yan Cao, Jun-Jie Zhang", "title": "ZMCintegral-v5.1: Support for Multi-function Integrations on GPUs", "comments": "5pages, 1figure", "journal-ref": null, "doi": "10.1016/j.cpc.2021.107994", "report-no": null, "categories": "cs.DC hep-ph", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this new version of ZMCintegral, we have added the functionality of\nmulti-function integrations, i.e. the ability to integrate more than $10^{3}$\ndifferent functions on GPUs. The Python API remains the similar as the previous\nversions. For integrands less than 5 dimensions, it usually takes less than 10\nminutes to finish the evaluation of $10^{3}$ integrations on one Tesla v100\ncard. The performance scales linearly with the increasing of the GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:43:28 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Cao", "Xiao-Yan", ""], ["Zhang", "Jun-Jie", ""]]}, {"id": "2104.10095", "submitter": "Zezhong Zhang", "authors": "Zezhong Zhang, Guangxu Zhu, Rui Wang, Vincent K. N. Lau, and Kaibin\n  Huang", "title": "Turning Channel Noise into an Accelerator for Over-the-Air Principal\n  Component Analysis", "comments": "30 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently years, the attempts on distilling mobile data into useful knowledge\nhas been led to the deployment of machine learning algorithms at the network\nedge. Principal component analysis (PCA) is a classic technique for extracting\nthe linear structure of a dataset, which is useful for feature extraction and\ndata compression. In this work, we propose the deployment of distributed PCA\nover a multi-access channel based on the algorithm of stochastic gradient\ndescent to learn the dominant feature space of a distributed dataset at\nmultiple devices. Over-the-air aggregation is adopted to reduce the\nmulti-access latency, giving the name over-the-air PCA. The novelty of this\ndesign lies in exploiting channel noise to accelerate the descent in the region\naround each saddle point encountered by gradient descent, thereby increasing\nthe convergence speed of over-the-air PCA. The idea is materialized by\nproposing a power-control scheme which detects the type of descent region and\ncontrolling the level of channel noise accordingly. The scheme is proved to\nachieve a faster convergence rate than in the case without power control.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:28:33 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 02:21:15 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Zezhong", ""], ["Zhu", "Guangxu", ""], ["Wang", "Rui", ""], ["Lau", "Vincent K. N.", ""], ["Huang", "Kaibin", ""]]}, {"id": "2104.10106", "submitter": "Javier \\'Alvarez Cid-Fuentes", "authors": "Javier \\'Alvarez Cid-Fuentes, Pol \\'Alvarez, Salvi Sol\\`a, Kuninori\n  Ishii, Rafael K. Morizawa, Rosa M. Badia", "title": "ds-array: A Distributed Data Structure for Large Scale Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has proved to be a useful tool for extracting knowledge from\nscientific data in numerous research fields, including astrophysics, genomics,\nand molecular dynamics. Often, data sets from these research areas need to be\nprocessed in distributed platforms due to their magnitude. This can be done\nusing one of the various distributed machine learning libraries available. One\nof these libraries is dislib, a distributed machine learning library for Python\nespecially designed to process large scale data sets on HPC clusters, which\nmakes dislib an ideal candidate for analyzing scientific data. However,\ndislib's main distributed data structure, called Dataset, has some limitations,\nincluding poor performance in certain operations and low flexibility and\nusability. In this paper, we propose a novel distributed data structure for\ndislib, called ds-array, that addresses dislib's main limitations in data\nmanagement. Ds-arrays simplify distributed data management in dislib by\nexposing a NumPy-like API, provide more flexibility, and reduce the\ncomputational complexity of some operations. This results in performance\nimprovements of up to two orders of magnitude over Datasets, while also greatly\nimproving scalability and usability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:36:21 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Cid-Fuentes", "Javier \u00c1lvarez", ""], ["\u00c1lvarez", "Pol", ""], ["Sol\u00e0", "Salvi", ""], ["Ishii", "Kuninori", ""], ["Morizawa", "Rafael K.", ""], ["Badia", "Rosa M.", ""]]}, {"id": "2104.10407", "submitter": "Sateeshkrishna Dhuli", "authors": "Sateeshkrishna Dhuli and Fouzul Atik", "title": "Analysis of Distributed Average Consensus Algorithms for Robust IoT\n  networks", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Internet of Things(IoT) is a heterogeneous network consists of various\nphysical objects such as large number of sensors, actuators, RFID tags, smart\ndevices, and servers connected to the internet. IoT networks have potential\napplications in healthcare, transportation, smart home, and automotive\nindustries. To realize the IoT applications, all these devices need to be\ndynamically cooperated and utilize their resources effectively in a distributed\nfashion. Consensus algorithms have attracted much research attention in recent\nyears due to their simple execution, robustness to topology changes, and\ndistributed philosophy. These algorithms are extensively utilized for\nsynchronization, resource allocation, and security in IoT networks. Performance\nof the distributed consensus algorithms can be effectively quantified by the\nConvergence Time, Network Coherence, Maximum Communication Time-Delay. In this\nwork, we model the IoT network as a q-triangular r-regular ring network as\nq-triangular topologies exhibit both small-world and scale-free features.\nScale-free and small-world topologies widely applied for modelling IoT as these\ntopologies are effectively resilient to random attacks. In this paper, we\nderive explicit expressions for all eigenvalues of Laplacian matrix for\nq-triangular r-regular networks. We then apply the obtained eigenvalues to\ndetermine the convergence time, network coherence, and maximum communication\ntimedelay. Our analytical results indicate that the effects of noise and\ncommunication delay on the consensus process are negligible for q-triangular\nr-regular networks. We argue that q-triangulation operation is responsible for\nthe strong robustness with respect to noise and communication time-delay in the\nproposed network topologies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:39:30 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Dhuli", "Sateeshkrishna", ""], ["Atik", "Fouzul", ""]]}, {"id": "2104.10501", "submitter": "Shouhua Zhang", "authors": "Jiehan Zhou, Shouhua Zhang, Qinghua Lu, Wenbin Dai, Min Chen, Xin Liu,\n  Susanna Pirttikangas, Yang Shi, Weishan Zhang, Enrique Herrera-Viedma", "title": "A Survey on Federated Learning and its Applications for Accelerating\n  Industrial Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) brings collaborative intelligence into industries\nwithout centralized training data to accelerate the process of Industry 4.0 on\nthe edge computing level. FL solves the dilemma in which enterprises wish to\nmake the use of data intelligence with security concerns. To accelerate\nindustrial Internet of things with the further leverage of FL, existing\nachievements on FL are developed from three aspects: 1) define terminologies\nand elaborate a general framework of FL for accommodating various scenarios; 2)\ndiscuss the state-of-the-art of FL on fundamental researches including data\npartitioning, privacy preservation, model optimization, local model\ntransportation, personalization, motivation mechanism, platform & tools, and\nbenchmark; 3) discuss the impacts of FL from the economic perspective. To\nattract more attention from industrial academia and practice, a FL-transformed\nmanufacturing paradigm is presented, and future research directions of FL are\ngiven and possible immediate applications in Industry 4.0 domain are also\nproposed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:40:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhou", "Jiehan", ""], ["Zhang", "Shouhua", ""], ["Lu", "Qinghua", ""], ["Dai", "Wenbin", ""], ["Chen", "Min", ""], ["Liu", "Xin", ""], ["Pirttikangas", "Susanna", ""], ["Shi", "Yang", ""], ["Zhang", "Weishan", ""], ["Herrera-Viedma", "Enrique", ""]]}, {"id": "2104.10569", "submitter": "Houyi Li", "authors": "Houyi Li, Yongchao Liu, Yongyong Li, Bin Huang, Peng Zhang, Guowei\n  Zhang, Xintan Zeng, Kefeng Deng, Wenguang Chen, and Changhua He", "title": "GraphTheta: A Distributed Graph Neural Network Learning System With\n  Flexible Training Strategy", "comments": "15 pages, 9 figures, submitted to VLDB 2022", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have been demonstrated as a powerful tool for\nanalysing non-Euclidean graph data. However, the lack of efficient distributed\ngraph learning systems severely hinders applications of GNNs, especially when\ngraphs are big, of high density or with highly skewed node degree\ndistributions. In this paper, we present a new distributed graph learning\nsystem GraphTheta, which supports multiple training strategies and enables\nefficient and scalable learning on big graphs. GraphTheta implements both\nlocalized and globalized graph convolutions on graphs, where a new graph\nlearning abstraction NN-TGAR is designed to bridge the gap between graph\nprocessing and graph learning frameworks. A distributed graph engine is\nproposed to conduct the stochastic gradient descent optimization with\nhybrid-parallel execution. Moreover, we add support for a new cluster-batched\ntraining strategy in addition to the conventional global-batched and\nmini-batched ones. We evaluate GraphTheta using a number of network data with\nnetwork size ranging from small-, modest- to large-scale. Experimental results\nshow that GraphTheta scales almost linearly to 1,024 workers and trains an\nin-house developed GNN model within 26 hours on Alipay dataset of 1.4 billion\nnodes and 4.1 billion attributed edges. Moreover, GraphTheta also obtains\nbetter prediction results than the state-of-the-art GNN methods. To the best of\nour knowledge, this work represents the largest edge-attributed GNN learning\ntask conducted on a billion-scale network in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:51:33 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Houyi", ""], ["Liu", "Yongchao", ""], ["Li", "Yongyong", ""], ["Huang", "Bin", ""], ["Zhang", "Peng", ""], ["Zhang", "Guowei", ""], ["Zeng", "Xintan", ""], ["Deng", "Kefeng", ""], ["Chen", "Wenguang", ""], ["He", "Changhua", ""]]}, {"id": "2104.10716", "submitter": "Chien-Yu Lin", "authors": "Chien-Yu Lin, Liang Luo, Luis Ceze", "title": "Accelerating SpMM Kernel with Cache-First Edge Sampling for Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs), an emerging deep learning model class, can\nextract meaningful representations from highly expressive graph-structured data\nand are therefore gaining popularity for wider ranges of applications. However,\ncurrent GNNs suffer from the poor performance of their sparse-dense matrix\nmultiplication (SpMM) operator, even when using powerful GPUs. Our analysis\nshows that 95% of the inference time could be spent on SpMM when running\npopular GNN models on NVIDIA's advanced V100 GPU. Such SpMM performance\nbottleneck hinders GNNs' applicability to large-scale problems or the\ndevelopment of more sophisticated GNN models. To address this inference time\nbottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and\ncodesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit\ninto GPU's shared memory. It thus reduces the computation cost and improves\nSpMM's cache locality. To evaluate ES-SpMM's performance, we integrated it with\na popular GNN framework, DGL, and tested it using representative GNN models and\ndatasets. Our results show that ES-SpMM outperforms the highly optimized\ncuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with\nless than a 1% accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:33:30 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 20:50:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Lin", "Chien-Yu", ""], ["Luo", "Liang", ""], ["Ceze", "Luis", ""]]}, {"id": "2104.10819", "submitter": "Kun Li", "authors": "Kun Li, Liang Yuan, Yunquan Zhang, Gongwei Chen", "title": "An Accurate and Efficient Large-scale Regression Method through Best\n  Friend Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the data size in Machine Learning fields grows exponentially, it is\ninevitable to accelerate the computation by utilizing the ever-growing large\nnumber of available cores provided by high-performance computing hardware.\nHowever, existing parallel methods for clustering or regression often suffer\nfrom problems of low accuracy, slow convergence, and complex\nhyperparameter-tuning. Furthermore, the parallel efficiency is usually\ndifficult to improve while striking a balance between preserving model\nproperties and partitioning computing workloads on distributed systems. In this\npaper, we propose a novel and simple data structure capturing the most\nimportant information among data samples. It has several advantageous\nproperties supporting a hierarchical clustering strategy that is irrelevant to\nthe hardware parallelism, well-defined metrics for determining optimal\nclustering, balanced partition for maintaining the compactness property, and\nefficient parallelization for accelerating computation phases. Then we combine\nthe clustering with regression techniques as a parallel library and utilize a\nhybrid structure of data and model parallelism to make predictions. Experiments\nillustrate that our library obtains remarkable performance on convergence,\naccuracy, and scalability.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:34:29 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Li", "Kun", ""], ["Yuan", "Liang", ""], ["Zhang", "Yunquan", ""], ["Chen", "Gongwei", ""]]}, {"id": "2104.11272", "submitter": "Xiaoyi Lu", "authors": "Xiaoyi Lu and Arjun Kashyap", "title": "Towards Offloadable and Migratable Microservices on Disaggregated\n  Architectures: Vision, Challenges, and Research Roadmap", "comments": "7 pages, 2 figures, WORDS'21, co-located with ASPLOS'21", "journal-ref": "WORDS 2021: The Second Workshop On Resource Disaggregation and\n  Serverless", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Microservice and serverless computing systems open up massive versatility and\nopportunity to distributed and datacenter-scale computing. In the meantime, the\ndeployments of modern datacenter resources are moving to disaggregated\narchitectures. With the flourishing growths from both sides, we believe this is\nhigh time to write this vision paper to propose a potential research agenda to\nachieve efficient deployments, management, and executions of next-generation\nmicroservices on top of the emerging disaggregated datacenter architectures. In\nparticular, we envision a critical systems research direction of designing and\ndeveloping offloadable and migratable microservices on disaggregated\narchitectures. With this vision, we have surveyed the recent related work to\ndemonstrate the importance and necessity of researching it. We also outline the\nfundamental challenges that distributed systems and datacenter-scale computing\nresearch may encounter. We further propose a research roadmap to achieve our\nenvisioned objectives in a promising way. Within the roadmap, we identify\npotential techniques and methods that can be leveraged.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:28:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lu", "Xiaoyi", ""], ["Kashyap", "Arjun", ""]]}, {"id": "2104.11349", "submitter": "Jongwook Woo", "authors": "Shradha Shinde, Jay Joshi, Sowmya Mareedu, Yeon Pyo Kim, Jongwook Woo", "title": "Scalable Predictive Time-Series Analysis of COVID-19: Cases and\n  Fatalities", "comments": "8 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID 19 is an acute disease that started spreading throughout the world,\nbeginning in December 2019. It has spread worldwide and has affected more than\n7 million people, and 200 thousand people have died due to this infection as of\nOct 2020. In this paper, we have forecasted the number of deaths and the\nconfirmed cases in Los Angeles and New York of the United States using the\ntraditional and Big Data platforms based on the Times Series: ARIMA and ETS. We\nalso implemented a more sophisticated time-series forecast model using Facebook\nProphet API. Furthermore, we developed the classification models: Logistic\nRegression and Random Forest regression to show that the Weather does not\naffect the number of the confirmed cases. The models are built and run in\nlegacy systems (Azure ML Studio) and Big Data systems (Oracle Cloud and\nDatabricks). Besides, we present the accuracy of the models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 23:08:13 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Shinde", "Shradha", ""], ["Joshi", "Jay", ""], ["Mareedu", "Sowmya", ""], ["Kim", "Yeon Pyo", ""], ["Woo", "Jongwook", ""]]}, {"id": "2104.11375", "submitter": "Tao Sun", "authors": "Tao Sun, Dongsheng Li, Bao Wang", "title": "Decentralized Federated Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated averaging (FedAvg) is a communication efficient algorithm for the\ndistributed training with an enormous number of clients. In FedAvg, clients\nkeep their data locally for privacy protection; a central parameter server is\nused to communicate between clients. This central server distributes the\nparameters to each client and collects the updated parameters from clients.\nFedAvg is mostly studied in centralized fashions, which requires massive\ncommunication between server and clients in each communication. Moreover,\nattacking the central server can break the whole system's privacy. In this\npaper, we study the decentralized FedAvg with momentum (DFedAvgM), which is\nimplemented on clients that are connected by an undirected graph. In DFedAvgM,\nall clients perform stochastic gradient descent with momentum and communicate\nwith their neighbors only. To further reduce the communication cost, we also\nconsider the quantized DFedAvgM. We prove convergence of the (quantized)\nDFedAvgM under trivial assumptions; the convergence rate can be improved when\nthe loss function satisfies the P{\\L} property. Finally, we numerically verify\nthe efficacy of DFedAvgM.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 02:01:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Sun", "Tao", ""], ["Li", "Dongsheng", ""], ["Wang", "Bao", ""]]}, {"id": "2104.11385", "submitter": "Michael Rowan", "authors": "Michael E. Rowan, Axel Huebl, Kevin N. Gott, Jack Deslippe, Maxence\n  Th\\'evenet, Remi Lehe, Jean-Luc Vay", "title": "In-Situ Assessment of Device-Side Compute Work for Dynamic Load\n  Balancing in a GPU-Accelerated PIC Code", "comments": "11 pages, 8 figures. Paper accepted in the Platform for Advanced\n  Scientific Computing Conference (PASC '21), July 5 to 9, 2021, Geneva,\n  Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.acc-ph physics.comp-ph physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining computational load balance is important to the performant\nbehavior of codes which operate under a distributed computing model. This is\nespecially true for GPU architectures, which can suffer from memory\noversubscription if improperly load balanced. We present enhancements to\ntraditional load balancing approaches and explicitly target GPU architectures,\nexploring the resulting performance. A key component of our enhancements is the\nintroduction of several GPU-amenable strategies for assessing compute work.\nThese strategies are implemented and benchmarked to find the most optimal data\ncollection methodology for in-situ assessment of GPU compute work. For the\nfully kinetic particle-in-cell code WarpX, which supports MPI+CUDA parallelism,\nwe investigate the performance of the improved dynamic load balancing via a\nstrong scaling-based performance model and show that, for a laser-ion\nacceleration test problem run with up to 6144 GPUs on Summit, the enhanced\ndynamic load balancing achieves from 62%--74% (88% when running on 6 GPUs) of\nthe theoretically predicted maximum speedup; for the 96-GPU case, we find that\ndynamic load balancing improves performance relative to baselines without load\nbalancing (3.8x speedup) and with static load balancing (1.2x speedup). Our\nresults provide important insights into dynamic load balancing and performance\nassessment, and are particularly relevant in the context of distributed memory\napplications ran on GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 02:43:45 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Rowan", "Michael E.", ""], ["Huebl", "Axel", ""], ["Gott", "Kevin N.", ""], ["Deslippe", "Jack", ""], ["Th\u00e9venet", "Maxence", ""], ["Lehe", "Remi", ""], ["Vay", "Jean-Luc", ""]]}, {"id": "2104.11471", "submitter": "Binrui Li", "authors": "Binrui Li, Shenggan Cheng, James Lin", "title": "tcFFT: Accelerating Half-Precision FFT through Tensor Cores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Fourier Transform (FFT) is an essential tool in scientific and\nengineering computation. The increasing demand for mixed-precision FFT has made\nit possible to utilize half-precision floating-point (FP16) arithmetic for\nfaster speed and energy saving. Specializing in lower precision, NVIDIA Tensor\nCores can deliver extremely high computation performance. However, the fixed\ncomputation pattern makes it hard to utilize the computing power of Tensor\nCores in FFT. Therefore, we developed tcFFT to accelerate FFT with Tensor\nCores. Our tcFFT supports batched 1D and 2D FFT of various sizes and it\nexploits a set of optimizations to achieve high performance: 1) single-element\nmanipulation on Tensor Core fragments to support special operations needed by\nFFT; 2) fine-grained data arrangement design to coordinate with the GPU memory\naccess pattern. We evaluated our tcFFT and the NVIDIA cuFFT in various sizes\nand dimensions on NVIDIA V100 and A100 GPUs. The results show that our tcFFT\ncan outperform cuFFT 1.29x-3.24x and 1.10x-3.03x on the two GPUs, respectively.\nOur tcFFT has a great potential for mixed-precision scientific applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:42:01 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Li", "Binrui", ""], ["Cheng", "Shenggan", ""], ["Lin", "James", ""]]}, {"id": "2104.11700", "submitter": "Saeedeh Parsaeefard", "authors": "Saeedeh Parsaeefard, Sayed Ehsan Etesami and Alberto Leon Garcia", "title": "Robust Federated Learning by Mixture of Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel weighted average model based on the mixture of experts\n(MoE) concept to provide robustness in Federated learning (FL) against the\npoisoned/corrupted/outdated local models. These threats along with the non-IID\nnature of data sets can considerably diminish the accuracy of the FL model. Our\nproposed MoE-FL setup relies on the trust between users and the server where\nthe users share a portion of their public data sets with the server. The server\napplies a robust aggregation method by solving the optimization problem or the\nSoftmax method to highlight the outlier cases and to reduce their adverse\neffect on the FL process. Our experiments illustrate that MoE-FL outperforms\nthe performance of the traditional aggregation approach for high rate of\npoisoned data from attackers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:41:04 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Parsaeefard", "Saeedeh", ""], ["Etesami", "Sayed Ehsan", ""], ["Garcia", "Alberto Leon", ""]]}, {"id": "2104.11725", "submitter": "Sinan Aksoy", "authors": "Sinan Aksoy, Stephen Young, Jesun Firoz, Roberto Gioiosa, Mark Raugas,\n  Juan Escobedo", "title": "SpectralFly: Ramanujan Graphs as Flexible and Efficient Interconnection\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, graph theoretic considerations have become increasingly\nimportant in the design of HPC interconnection topologies. One approach is to\nseek optimal or near-optimal families of graphs with respect to a particular\ngraph theoretic property, such as diameter. In this work, we consider\ntopologies which optimize the spectral gap. In particular, we study a novel HPC\ntopology, SpectralFly, designed around the Ramanujan graph construction of\nLubotzky, Phillips, and Sarnak (LPS). We show combinatorial properties, such as\ndiameter, bisection bandwidth, average path length, and resilience to link\nfailure, of SpectralFly topologies are better than, or comparable to, similarly\nconstrained DragonFly, SlimFly, and BundleFly topologies. Additionally, we\nsimulate the performance of SpectralFly topologies on a representative sample\nof physics-inspired HPC workloads using the Structure Simulation Toolkit\nMacroscale Element Library simulator and demonstrate considerable benefit to\nusing the LPS construction as the basis of the SpectralFly topology.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:21:55 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Aksoy", "Sinan", ""], ["Young", "Stephen", ""], ["Firoz", "Jesun", ""], ["Gioiosa", "Roberto", ""], ["Raugas", "Mark", ""], ["Escobedo", "Juan", ""]]}, {"id": "2104.11805", "submitter": "Gunduz Vehbi Demirci", "authors": "Gunduz Vehbi Demirci, Hakan Ferhatosmanoglu", "title": "Partitioning sparse deep neural networks for scalable training and\n  inference", "comments": "Gunduz Vehbi Demirci and Hakan Ferhatosmanoglu. 2021. Partitioning\n  Sparse Deep Neural Networks for Scalable Training and Inference. In 2021\n  International Conference on Supercomputing (ICS '21), June 14-17, 2021,\n  Virtual Event, USA. ACM, New York, NY, USA, 12 pages", "journal-ref": null, "doi": "10.1145/3447818.3460372", "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art deep neural networks (DNNs) have significant\ncomputational and data management requirements. The size of both training data\nand models continue to increase. Sparsification and pruning methods are shown\nto be effective in removing a large fraction of connections in DNNs. The\nresulting sparse networks present unique challenges to further improve the\ncomputational efficiency of training and inference in deep learning. Both the\nfeedforward (inference) and backpropagation steps in stochastic gradient\ndescent (SGD) algorithm for training sparse DNNs involve consecutive sparse\nmatrix-vector multiplications (SpMVs). We first introduce a distributed-memory\nparallel SpMV-based solution for the SGD algorithm to improve its scalability.\nThe parallelization approach is based on row-wise partitioning of weight\nmatrices that represent neuron connections between consecutive layers. We then\npropose a novel hypergraph model for partitioning weight matrices to reduce the\ntotal communication volume and ensure computational load-balance among\nprocessors. Experiments performed on sparse DNNs demonstrate that the proposed\nsolution is highly efficient and scalable. By utilizing the proposed matrix\npartitioning scheme, the performance of our solution is further improved\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 20:05:52 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Demirci", "Gunduz Vehbi", ""], ["Ferhatosmanoglu", "Hakan", ""]]}, {"id": "2104.11866", "submitter": "Andreas Grammenos", "authors": "Wei Jiang, Andreas Grammenos, Evangelia Kalyvianaki, Themistoklis\n  Charalambous", "title": "An Asynchronous Approximate Distributed Alternating Direction Method of\n  Multipliers in Digraphs", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the asynchronous distributed optimization problem\nin which each node has its own convex cost function and can communicate\ndirectly only with its neighbors, as determined by a directed communication\ntopology (directed graph or digraph). First, we reformulate the optimization\nproblem so that Alternating Direction Method of Multipliers (ADMM) can be\nutilized. Then, we propose an algorithm, herein called Asynchronous Approximate\nDistributed Alternating Direction Method of Multipliers (AsyAD-ADMM), using\nfinite-time asynchronous approximate ratio consensus, to solve the multi-node\nconvex optimization problem, in which every node performs iterative\ncomputations and exchanges information with its neighbors asynchronously. More\nspecifically, at every iteration of AsyAD-ADMM, each node solves a local convex\noptimization problem for one of the primal variables and utilizes a finite-time\nasynchronous approximate consensus protocol to obtain the value of the other\nvariable which is close to the optimal value, since the cost function for the\nsecond primal variable is not decomposable. If the individual cost functions\nare convex but not necessarily differentiable, the proposed algorithm converges\nat a rate of $\\mathcal{O}(1/k)$, where $k$ is the iteration counter. The\nefficacy of AsyAD-ADMM is exemplified via a proof-of-concept distributed\nleast-square optimization problem with different performance-influencing\nfactors investigated.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 02:56:16 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jiang", "Wei", ""], ["Grammenos", "Andreas", ""], ["Kalyvianaki", "Evangelia", ""], ["Charalambous", "Themistoklis", ""]]}, {"id": "2104.11958", "submitter": "Mark Burgess", "authors": "Mark Burgess and Alva Couch", "title": "On system rollback and totalised fields", "comments": null, "journal-ref": "Journal of Logic and Algebraic Programming 80 (2011), pp. 427-443", "doi": "10.1016/j.jlap.2011.07.001", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In system operations it is commonly assumed that arbitrary changes to a\nsystem can be reversed or `rolled back', when errors of judgement and procedure\noccur. We point out that this view is flawed and provide an alternative\napproach to determining the outcome of changes.\n  Convergent operators are fixed-point generators that stem from the basic\nproperties of multiplication by zero. They are capable of yielding a repeated\nand predictable outcome even in an incompletely specified or `open' system. We\nformulate such `convergent operators' for configuration change in the language\nof groups and rings and show that, in this form, the problem of convergent\nreversibility becomes equivalent to the `division by zero' problem. Hence, we\ndiscuss how recent work by Bergstra and Tucker on zero-totalised fields helps\nto clear up long-standing confusion about the options for `rollback' in change\nmanagement.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 14:01:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Burgess", "Mark", ""], ["Couch", "Alva", ""]]}, {"id": "2104.11981", "submitter": "Kun Yuan", "authors": "Kun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui\n  Xu, Wotao Yin", "title": "DecentLaM: Decentralized Momentum SGD for Large-batch Deep Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scale of deep learning nowadays calls for efficient distributed training\nalgorithms. Decentralized momentum SGD (DmSGD), in which each node averages\nonly with its neighbors, is more communication efficient than vanilla Parallel\nmomentum SGD that incurs global average across all computing nodes. On the\nother hand, the large-batch training has been demonstrated critical to achieve\nruntime speedup. This motivates us to investigate how DmSGD performs in the\nlarge-batch scenario.\n  In this work, we find the momentum term can amplify the inconsistency bias in\nDmSGD. Such bias becomes more evident as batch-size grows large and hence\nresults in severe performance degradation. We next propose DecentLaM, a novel\ndecentralized large-batch momentum SGD to remove the momentum-incurred bias.\nThe convergence rate for both non-convex and strongly-convex scenarios is\nestablished. Our theoretical results justify the superiority of DecentLaM to\nDmSGD especially in the large-batch scenario. Experimental results on a variety\nof computer vision tasks and models demonstrate that DecentLaM promises both\nefficient and high-quality training.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 16:21:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yuan", "Kun", ""], ["Chen", "Yiming", ""], ["Huang", "Xinmeng", ""], ["Zhang", "Yingya", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""], ["Yin", "Wotao", ""]]}, {"id": "2104.12057", "submitter": "Naoki Kitamura", "authors": "Naoki Kitamura, Taisuke Izumi", "title": "A Subquadratic-Time Distributed Algorithm for Exact Maximum Matching", "comments": "19 pages, 4figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph G=(V,E), finding a set of disjoint edges that do not share any\nvertices is called a matching problem, and finding the maximum matching is a\nfundamental problem in the theory of distributed graph algorithms. Although\nlocal algorithms for the approximate maximum matching problem have been widely\nstudied, exact algorithms has not been much studied. In fact, no exact maximum\nmatching algorithm that is faster than the trivial upper bound of O(n^2) rounds\nis known for the general instance. In this paper, we propose a randomized\nO(s_{max}^{3/2}+log n)-round algorithm in the CONGEST model, where s_{\\max} is\nthe size of maximum matching. This is the first exact maximum matching\nalgorithm in o(n^2) rounds for general instances in the CONGEST model. The key\ntechnical ingredient of our result is a distributed algorithms of finding an\naugmenting path in O(s_{\\max}) rounds, which is based on a novel technique of\nconstructing a sparse certificate of augmenting paths, which is a subgraph of\nthe input graph preserving at least one augmenting path. To establish a highly\nparallel construction of sparse certificates, we also propose a new\ncharacterization of sparse certificates, which might also be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 04:28:30 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kitamura", "Naoki", ""], ["Izumi", "Taisuke", ""]]}, {"id": "2104.12116", "submitter": "Tai Le Quy", "authors": "Tai Le Quy, Arjun Roy, Gunnar Friege and Eirini Ntoutsi", "title": "Fair-Capacitated Clustering", "comments": "10 pages, 5 figures, 14th International Conference on Educational\n  Data Mining - EDM 2021 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, clustering algorithms focus on partitioning the data into\ngroups of similar instances. The similarity objective, however, is not\nsufficient in applications where a fair-representation of the groups in terms\nof protected attributes like gender or race, is required for each cluster.\nMoreover, in many applications, to make the clusters useful for the end-user, a\nbalanced cardinality among the clusters is required. Our motivation comes from\nthe education domain where studies indicate that students might learn better in\ndiverse student groups and of course groups of similar cardinality are more\npractical e.g., for group assignments. To this end, we introduce the\nfair-capacitated clustering problem that partitions the data into clusters of\nsimilar instances while ensuring cluster fairness and balancing cluster\ncardinalities. We propose a two-step solution to the problem: i) we rely on\nfairlets to generate minimal sets that satisfy the fair constraint and ii) we\npropose two approaches, namely hierarchical clustering and partitioning-based\nclustering, to obtain the fair-capacitated clustering. The hierarchical\napproach embeds the additional cardinality requirements during the merging step\nwhile the partitioning-based one alters the assignment step using a knapsack\nproblem formulation to satisfy the additional requirements. Our experiments on\nfour educational datasets show that our approaches deliver well-balanced\nclusters in terms of both fairness and cardinality while maintaining a good\nclustering quality.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 09:39:39 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 07:24:28 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Quy", "Tai Le", ""], ["Roy", "Arjun", ""], ["Friege", "Gunnar", ""], ["Ntoutsi", "Eirini", ""]]}, {"id": "2104.12197", "submitter": "Juhyun Bae", "authors": "Juhyun Bae, Gong Su, Arun Iyengar, Yanzhao Wu, Ling Liu", "title": "RDMAbox : Optimizing RDMA for Memory Intensive Workloads", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RDMAbox, a set of low level RDMA opti-mizations that provide\nbetter performance than previous ap-proaches. The optimizations are packaged in\neasy-to-use ker-nel and userspace libraries and presented through simple\nnodelevel abstractions. We demonstrate the flexibility and effec-tiveness of\nRDMAbox by implementing a kernel remote pag-ing system and a userspace file\nsystem using RDMAbox.RDMAbox employs two optimization techniques. First,\nwesuggest Load-aware Batching to further reduce the total num-ber of I/O\noperations to the RDMA NIC beyond existing door-bell batching. The I/O merge\nqueue at the same time functionsas a traffic regulator to enforce admission\ncontrol and avoidoverloading the NIC. Second, we propose Adaptive Pollingto\nachieve higher efficiency of polling Work Completion thanexisting busy polling\nwhile maintaining the low CPU over-head of event trigger. Our implementation of\na remote paging system with RDMAbox outperforms existing representative\nsolutions with up to 6.48x throughput improvement and up to 83% decrease in\naverage tail latency in bigdata workloads, and up to 83% reduction in\ncompletion time in machine learn-ing workloads. Our implementation of a user\nspace file system based on RDMAbox achieves up to 6x higher throughput over\nexisting representative solutions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 16:35:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bae", "Juhyun", ""], ["Su", "Gong", ""], ["Iyengar", "Arun", ""], ["Wu", "Yanzhao", ""], ["Liu", "Ling", ""]]}, {"id": "2104.12202", "submitter": "Archak Das", "authors": "Archak Das, Avisek Sharma and Buddhadeb Sau", "title": "Power of Finite Memory and Finite Communication Robots under\n  Asynchronous Scheduler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In swarm robotics, a set of robots has to perform a given task with specified\ninternal capabilities (model) and under a given adversarial scheduler. Relation\nbetween a model $M_1$ under scheduler $S_1$, and that of a model $M_2$ under\nscheduler $S_2$ can be of four different types: not less powerful, more\npowerful, equivalent and orthogonal. In literature there are four main models\nof robots with lights: $\\mathcal{LUMI}$, where robots have the power of\nobserving the lights of all the robots, $\\mathcal{FSTA}$ , where each robot can\nsee only its own light, $\\mathcal{FCOM}$, where each robot can observe the\nlight of all other robots except its own and $\\mathcal{OBLOT}$, where the\nrobots do not have any light. In this paper, we investigate the computational\npower of $\\mathcal{FSTA}$ and $\\mathcal{FCOM}$ model under asynchronous\nscheduler by comparing it with other model and scheduler combinations. Our main\nfocus is to understand and compare the power of persistent memory and explicit\ncommunication in robots under asynchronous scheduler.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 16:55:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Das", "Archak", ""], ["Sharma", "Avisek", ""], ["Sau", "Buddhadeb", ""]]}, {"id": "2104.12350", "submitter": "Varun Sharma", "authors": "Varun Sharma and Paul Chow", "title": "A PGAS Communication Library for Heterogeneous Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a heterogeneous communication library for clusters of\nprocessors and FPGAs. This library, Shoal, supports the Partitioned Global\nAddress Space (PGAS) memory model for applications. PGAS is a shared memory\nmodel for clusters that creates a distinction between local and remote memory\naccess. Through Shoal and its common application programming interface for\nhardware and software, applications can be more freely migrated to the optimal\nplatform and deployed onto dynamic cluster topologies.\n  The library is tested using a thorough suite of microbenchmarks to establish\nlatency and throughput performance. We also show an implementation of the\nJacobi iterative method that demonstrates the ease with which applications can\nbe moved between platforms to yield faster run times. Through this work, we\nhave demonstrated the feasibility of using a PGAS programming model for\nmulti-node heterogeneous platforms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 05:05:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sharma", "Varun", ""], ["Chow", "Paul", ""]]}, {"id": "2104.12416", "submitter": "Zhefeng Qiao", "authors": "Zhefeng Qiao, Xianghao Yu, Jun Zhang, Khaled B. Letaief", "title": "Communication-Efficient Federated Learning with Dual-Side Low-Rank\n  Compression", "comments": "6 pages, 5 figures, submitted for potential publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a promising and powerful approach for training\ndeep learning models without sharing the raw data of clients. During the\ntraining process of FL, the central server and distributed clients need to\nexchange a vast amount of model information periodically. To address the\nchallenge of communication-intensive training, we propose a new training\nmethod, referred to as federated learning with dual-side low-rank compression\n(FedDLR), where the deep learning model is compressed via low-rank\napproximations at both the server and client sides. The proposed FedDLR not\nonly reduces the communication overhead during the training stage but also\ndirectly generates a compact model to speed up the inference process. We shall\nprovide convergence analysis, investigate the influence of the key parameters,\nand empirically show that FedDLR outperforms the state-of-the-art solutions in\nterms of both the communication and computation efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:13:31 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Qiao", "Zhefeng", ""], ["Yu", "Xianghao", ""], ["Zhang", "Jun", ""], ["Letaief", "Khaled B.", ""]]}, {"id": "2104.12425", "submitter": "Qiuping Wang", "authors": "Qiuping Wang, Jinhong Li, Patrick P. C. Lee, Guangliang Zhao, Chao\n  Shi, Lilong Huang", "title": "In Search of Optimal Data Placement for Eliminating Write Amplification\n  in Log-Structured Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-structured storage has been widely deployed in various domains of storage\nsystems for high performance. However, its garbage collection (GC) incurs\nsevere write amplification (WA) due to the frequent rewrites of live data. This\nmotivates many research studies, particularly on data placement strategies,\nthat mitigate WA in log-structured storage. We show how to design an optimal\ndata placement scheme that leads to the minimum WA with the future knowledge of\nblock invalidation time (BIT) of each written block. Guided by this\nobservation, we propose InferBIT, a novel data placement algorithm that aims to\nminimize WA in log-structured storage. Its core idea is to infer the BITs of\nwritten blocks from the underlying storage workloads, so as to place the blocks\nwith similar estimated BITs into the same group in a fine-grained manner. We\nshow via both mathematical and trace analyses that InferBIT can infer the BITs\nby leveraging the write skewness property in real-world storage workloads.\nEvaluation on block-level I/O traces from real-world cloud block storage\nworkloads shows that InferBIT achieves the lowest WA compared to eight\nstate-of-the-art data placement schemes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:36:24 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 02:29:39 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Qiuping", ""], ["Li", "Jinhong", ""], ["Lee", "Patrick P. C.", ""], ["Zhao", "Guangliang", ""], ["Shi", "Chao", ""], ["Huang", "Lilong", ""]]}, {"id": "2104.12466", "submitter": "Lorenzo Bacchiani", "authors": "Lorenzo Bacchiani, Mario Bravetti, Saverio Giallorenzo, Jacopo Mauro,\n  Iacopo Talevi, Gianluigi Zavattaro", "title": "Microservice Dynamic Architecture-Level Deployment Orchestration\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the BI-REX (Big Data Innovation and Research Excellence)\ncompetence center SEAWALL (SEAmless loW lAtency cLoud pLatforms) project\n(scientific coordinator Prof. Maurizio Gabbrielli) we develop a novel approach\nfor run-time global adaptation of microservice applications, based on synthesis\nof architecture-level reconfiguration orchestrations. More precisely, we devise\nan algorithm for automatic reconfiguration that reaches a target system Maximum\nComputational Load by performing optimal deployment orchestrations. To conceive\nand simulate our approach, we introduce a novel integrated timed architectural\nmodeling/execution language based on an extension of the actor-based\nobject-oriented Abstract Behavioral Specification (ABS) language. In\nparticular, we realize a timed extension of SmartDeployer, whose ABS code\nannotations make it possible to express architectural properties. Our Timed\nSmartDeployer tool fully integrates time features of ABS and architectural\nannotations by generating timed deployment orchestrations. We evaluate the\napplicability of our approach on a realistic microservice application taken\nfrom the literature: an Email Pipeline Processing System. We prove its\neffectiveness by simulating such an application and by comparing\narchitecture-level reconfiguration with traditional local scaling techniques\n(which detect scaling needs and enact replications at the level of single\nmicroservices). Our comparison results show that our approach avoids cascading\nslowdowns and consequent increased message loss and latency, which affect\ntraditional local scaling.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:50:48 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 18:32:00 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 10:06:00 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bacchiani", "Lorenzo", ""], ["Bravetti", "Mario", ""], ["Giallorenzo", "Saverio", ""], ["Mauro", "Jacopo", ""], ["Talevi", "Iacopo", ""], ["Zavattaro", "Gianluigi", ""]]}, {"id": "2104.12608", "submitter": "Saeedeh Parsaeefard", "authors": "Saeedeh Parsaeefard and Alberto Leon Garcia", "title": "Generalized ADMM in Distributed Learning via Variational Inequality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the explosion in size and complexity of modern data sets and privacy\nconcerns of data holders, it is increasingly important to be able to solve\nmachine learning problems in distributed manners. The Alternating Direction\nMethod of Multipliers (ADMM) through the concept of consensus variables is a\npractical algorithm in this context where its diverse variations and its\nperformance have been studied in different application areas. In this paper, we\nstudy the effect of the local data sets of users in the distributed learning of\nADMM. Our aim is to deploy variational inequality (VI) to attain an unified\nview of ADMM variations. Through the simulation results, we demonstrate how\nmore general definitions of consensus parameters and introducing the uncertain\nparameters in distribute approach can help to get the better results in\nlearning processes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:16:14 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Parsaeefard", "Saeedeh", ""], ["Garcia", "Alberto Leon", ""]]}, {"id": "2104.12762", "submitter": "Omar Abdel Wahab", "authors": "Ahmed Saleh Bataineh, Jamal Bentahar, Rabeb Mizouni, Omar Abdel Wahab,\n  Gaith Rjoub, May El Barachi", "title": "Cloud computing as a platform for monetizing data services: A two-sided\n  game business model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the unprecedented reliance on cloud computing as the backbone for\nstoring today's big data, we argue in this paper that the role of the cloud\nshould be reshaped from being a passive virtual market to become an active\nplatform for monetizing the big data through Artificial Intelligence (AI)\nservices. The objective is to enable the cloud to be an active platform that\ncan help big data service providers reach a wider set of customers and cloud\nusers (i.e., data consumers) to be exposed to a larger and richer variety of\ndata to run their data analytic tasks. To achieve this vision, we propose a\nnovel game theoretical model, which consists of a mix of cooperative and\ncompetitive strategies. The players of the game are the big data service\nproviders, cloud computing platform, and cloud users. The strategies of the\nplayers are modeled using the two-sided market theory that takes into\nconsideration the network effects among involved parties, while integrating the\nexternalities between the cloud resources and consumer demands into the design\nof the game. Simulations conducted using Amazon and google clustered data show\nthat the proposed model improves the total surplus of all the involved parties\nin terms of cloud resources provision and monetary profits compared to the\ncurrent merchant model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:54:31 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bataineh", "Ahmed Saleh", ""], ["Bentahar", "Jamal", ""], ["Mizouni", "Rabeb", ""], ["Wahab", "Omar Abdel", ""], ["Rjoub", "Gaith", ""], ["Barachi", "May El", ""]]}, {"id": "2104.12876", "submitter": "Aman Priyanshu", "authors": "Aman Priyanshu and Mudit Sinha and Shreyans Mehta", "title": "Continual Distributed Learning for Crisis Management", "comments": "6 pages, 3 figures, Accepted at 3rd Workshop on Continual and\n  Multimodal Learning for Internet of Things and Presented at IEEESBM Manipal\n  Paper Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media platforms such as Twitter, Facebook etc can be utilised as an\nimportant source of information during disaster events. This information can be\nused for disaster response and crisis management if processed accurately and\nquickly. However, the data present in such situations is ever-changing, and\nusing considerable resources during such a crisis is not feasible. Therefore,\nwe have to develop a low resource and continually learning system that\nincorporates text classification models which are robust against noisy and\nunordered data. We utilised Distributed learning which enabled us to learn on\nresource-constrained devices, then to alleviate catastrophic forgetting in our\ntarget neural networks we utilized regularization. We then applied federated\naveraging for distributed learning and to aggregate the central model for\ncontinual learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 21:01:29 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 05:19:29 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Priyanshu", "Aman", ""], ["Sinha", "Mudit", ""], ["Mehta", "Shreyans", ""]]}, {"id": "2104.12911", "submitter": "Cy Chan", "authors": "Cy Chan, Anu Kuncheria, Bingyu Zhao, Theophile Cabannes, Alexander\n  Keimer, Bin Wang, Alexandre Bayen, Jane Macfarlane", "title": "Quasi-Dynamic Traffic Assignment using High Performance Computing", "comments": "28 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic assignment methods are some of the key approaches used to model flow\npatterns that arise in transportation networks. Since static traffic assignment\ndoes not have a notion of time, it is not designed to represent temporal\ndynamics that arise as vehicles flow through the network and demand varies\nthrough the day. Dynamic traffic assignment methods attempt to resolve these\nissues, but require significant computational resources if modeling urban-scale\nregions (on the order of millions of links and vehicles) and often take days of\ncompute time to complete. The focus of this work is two-fold: 1) to introduce a\nnew traffic assignment approach - a quasi-dynamic traffic assignment (QDTA)\nmodel and 2) to describe how we parallelized the QDTA algorithms to leverage\nHigh-Performance Computing (HPC) and scale to large metropolitan areas while\ndramatically reducing compute time. We examine and compare different scenarios,\nincluding a baseline static traffic assignment (STA) and a quasi-dynamic\nscenario inspired by the user-equilibrium (UET). Results are presented for the\nSan Francisco Bay Area which accounts for 19M trips/day and an urban road\nnetwork of 1M links. We utilize an iterative gradient descent method, where the\nstep size is selected using a Quasi-Newton method with parallelized cost\nfunction evaluations and compare it to using pre-defined step sizes (MSA).\nUsing the parallelized line search provides a 16 percent reduction in total\nexecution time due to a reduction in the number of gradient descent iterations\nrequired for convergence. The full day QDTA comprising 96 optimization steps\nover 15 minute intervals runs in about 4 minutes on 1,024 cores of the NERSC\nCori computer, which represents a speedup of over 36x versus serial execution.\nTo our knowledge, this compute time is significantly lower than other traffic\nassignment solutions for a problem of this scale.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 23:36:49 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 23:01:49 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chan", "Cy", ""], ["Kuncheria", "Anu", ""], ["Zhao", "Bingyu", ""], ["Cabannes", "Theophile", ""], ["Keimer", "Alexander", ""], ["Wang", "Bin", ""], ["Bayen", "Alexandre", ""], ["Macfarlane", "Jane", ""]]}, {"id": "2104.12913", "submitter": "Pawel Kryszkiewicz", "authors": "Pawel Kryszkiewicz, Filip Idzikowski, Bartosz Bossy, Bartosz Kopras,\n  Hanna Bogucka", "title": "Energy Savings by Task Offloading to a Fog Considering Radio Front-End\n  Characteristics", "comments": "IEEE International Symposium on Personal, Indoor and Mobile Radio\n  Communications 2019", "journal-ref": null, "doi": "10.1109/PIMRC.2019.8904231", "report-no": null, "categories": "cs.NI cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing can be used to offload computationally intensive tasks from\nbattery powered Internet of Things (IoT) devices. Although it reduces energy\nrequired for computations in an IoT device, it uses energy for communications\nwith the fog. This paper analyzes when usage of fog computing is more energy\nefficient than local computing. Detailed energy consumption models are built in\nboth scenarios with the focus set on the relation between energy consumption\nand distortion introduced by a Power Amplifier (PA). Numerical results show\nthat task offloading to a fog is the most energy efficient for short, wideband\nlinks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 23:49:56 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kryszkiewicz", "Pawel", ""], ["Idzikowski", "Filip", ""], ["Bossy", "Bartosz", ""], ["Kopras", "Bartosz", ""], ["Bogucka", "Hanna", ""]]}, {"id": "2104.13092", "submitter": "Bin Cao", "authors": "Mingrui Cao, Long Zhang, Bin Cao", "title": "Towards On-Device Federated Learning: A Direct Acyclic Graph-based\n  Blockchain Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the distributed characteristics of Federated Learning (FL), the\nvulnerability of global model and coordination of devices are the main\nobstacle. As a promising solution of decentralization, scalability and\nsecurity, leveraging blockchain in FL has attracted much attention in recent\nyears. However, the traditional consensus mechanisms designed for blockchain\nlike Proof of Work (PoW) would cause extreme resource consumption, which\nreduces the efficiency of FL greatly, especially when the participating devices\nare wireless and resource-limited. In order to address device asynchrony and\nanomaly detection in FL while avoiding the extra resource consumption caused by\nblockchain, this paper introduces a framework for empowering FL using Direct\nAcyclic Graph (DAG)-based blockchain systematically (DAG-FL). Accordingly,\nDAG-FL is first introduced from a three-layer architecture in details, and then\ntwo algorithms DAG-FL Controlling and DAG-FL Updating are designed running on\ndifferent nodes to elaborate the operation of DAG-FL consensus mechanism. After\nthat, a Poisson process model is formulated to discuss that how to set\ndeployment parameters to maintain DAG-FL stably in different federated learning\ntasks. The extensive simulations and experiments show that DAG-FL can achieve\nbetter performance in terms of training efficiency and model accuracy compared\nwith the typical existing on-device federated learning systems as the\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:29:38 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Cao", "Mingrui", ""], ["Zhang", "Long", ""], ["Cao", "Bin", ""]]}, {"id": "2104.13144", "submitter": "Bin Cao", "authors": "Yixin Li, Bin Cao, Liang Liang, Deming Mao, Lei Zhang", "title": "Block Access Control in Wireless Blockchain Network: Design, Modeling\n  and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless blockchain network is proposed to enable a decentralized and safe\nwireless networks for various blockchain applications. To achieve blockchain\nconsensus in wireless network, one of the important steps is to broadcast new\nblock using wireless channel. Under wireless network protocols, the block\ntransmitting will be affected significantly. In this work, we focus on the\nconsensus process in blockchain-based wireless local area network (B-WLAN) by\ninvestigating the impact of the media access control (MAC) protocol, CSMA/CA.\nWith the randomness of the backoff counter in CSMA/CA, it is possible for\nlatter blocks to catch up or outpace the earlier one, which complicates\nblockchain forking problem. In view of this, we propose mining strategies to\npause mining for reducing the forking probability, and a discard strategy to\nremove the forking blocks that already exist in CSMA/CA backoff procedure.\nBased on the proposed strategies, we design Block Access Control (BAC)\napproaches to effectively schedule block mining and transmitting for improving\nthe performance of B-WLAN. Then, Markov chain models are presented to conduct\nperformance analysis in B-WLAN. The results show that BAC approaches can help\nthe network to achieve a high transaction throughput while improving block\nutilization and saving computational power. Meanwhile, the trade-off between\ntransaction throughput and block utilization is demonstrated, which can act as\na guidance for practical deployment of blockchain.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 12:39:42 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Li", "Yixin", ""], ["Cao", "Bin", ""], ["Liang", "Liang", ""], ["Mao", "Deming", ""], ["Zhang", "Lei", ""]]}, {"id": "2104.13187", "submitter": "Tegg Sung", "authors": "Tegg Taekyong Sung, Bo Ryu", "title": "A Scalable and Reproducible System-on-Chip Simulation for Reinforcement\n  Learning", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) underlies in a simulated environment and\noptimizes objective goals. By extending the conventional interaction scheme,\nthis paper proffers gym-ds3, a scalable and reproducible open environment\ntailored for a high-fidelity Domain-Specific System-on-Chip (DSSoC)\napplication. The simulation corroborates to schedule hierarchical jobs onto\nheterogeneous System-on-Chip (SoC) processors and bridges the system to\nreinforcement learning research. We systematically analyze the representative\nSoC simulator and discuss the primary challenging aspects that the system (1)\ncontinuously generates indefinite jobs at a rapid injection rate, (2) optimizes\ncomplex objectives, and (3) operates in steady-state scheduling. We provide\nexemplary snippets and experimentally demonstrate the run-time performances on\ndifferent schedulers that successfully mimic results achieved from the standard\nDS3 framework and real-world embedded systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:46:57 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Sung", "Tegg Taekyong", ""], ["Ryu", "Bo", ""]]}, {"id": "2104.13209", "submitter": "Mohammad Almasri", "authors": "Mohammad Almasri, Izzat El Hajj, Rakesh Nagi, Jinjun Xiong, Wen-mei\n  Hwu", "title": "K-Clique Counting on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting k-cliques in a graph is an important problem in graph analysis with\nmany applications. Counting k-cliques is typically done by traversing search\ntrees starting at each vertex in the graph. An important optimization is to\neliminate search tree branches that discover the same clique redundantly.\nEliminating redundant clique discovery is typically done via graph orientation\nor pivoting. Parallel implementations for both of these approaches have\ndemonstrated promising performance on CPUs. In this paper, we present our GPU\nimplementations of k-clique counting for both the graph orientation and\npivoting approaches. Our implementations explore both vertex-centric and\nedge-centric parallelization schemes, and replace recursive search tree\ntraversal with iterative traversal based on an explicitly-managed shared stack.\nWe also apply various optimizations to reduce memory consumption and improve\nthe utilization of parallel execution resources. Our evaluation shows that our\nbest GPU implementation outperforms the best state-of-the-art parallel CPU\nimplementation by a geometric mean speedup of 12.39x, 6.21x, and 18.99x for k =\n4, 7, and 10, respectively. We also evaluate the impact of the choice of\nparallelization scheme and the incremental speedup of each optimization. Our\ncode will be open-sourced to enable further research on parallelizing k-clique\ncounting on GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:18:03 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Almasri", "Mohammad", ""], ["Hajj", "Izzat El", ""], ["Nagi", "Rakesh", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2104.13248", "submitter": "Mohamed Wahib", "authors": "Peng Chen, Mohamed Wahib, Xiao Wang, Shinichiro Takizawa, Takahiro\n  Hirofuchi, Hirotaka Ogawa, Satoshi Matsuoka", "title": "Performance Portable Back-projection Algorithms on CPUs: Agnostic Data\n  Locality and Vectorization Optimizations", "comments": "ACM International Conference on Supercomputing 2021 (ICS'21)", "journal-ref": null, "doi": "10.1145/3447818.3460353", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computed Tomography (CT) is a key 3D imaging technology that fundamentally\nrelies on the compute-intense back-projection operation to generate 3D volumes.\nGPUs are typically used for back-projection in production CT devices. However,\nwith the rise of power-constrained micro-CT devices, and also the emergence of\nCPUs comparable in performance to GPUs, back-projection for CPUs could become\nfavorable. Unlike GPUs, extracting parallelism for back-projection algorithms\non CPUs is complex given that parallelism and locality are not explicitly\ndefined and controlled by the programmer, as is the case when using CUDA for\ninstance. We propose a collection of novel back-projection algorithms that\nreduce the arithmetic computation, robustly enable vectorization, enforce a\nregular memory access pattern, and maximize the data locality. We also\nimplement the novel algorithms as efficient back-projection kernels that are\nperformance portable over a wide range of CPUs. Performance evaluation using a\nvariety of CPUs from different vendors and generations demonstrates that our\nback-projection implementation achieves on average 5.2x speedup over the\nmulti-threaded implementation of the most widely used, and optimized, open\nlibrary. With a state-of-the-art CPU, we reach performance that rivals\ntop-performing GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 15:01:12 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chen", "Peng", ""], ["Wahib", "Mohamed", ""], ["Wang", "Xiao", ""], ["Takizawa", "Shinichiro", ""], ["Hirofuchi", "Takahiro", ""], ["Ogawa", "Hirotaka", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "2104.13263", "submitter": "J. Lowell Wofford", "authors": "J. Lowell Wofford", "title": "Designing a scalable framework for declarative automation on distributed\n  systems", "comments": "12 pages, 6 figures, Submitted to SC21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As distributed systems grow in scale and complexity, the need for flexible\nautomation of systems management functions also grows. We outline a framework\nfor building tools that provide distributed, scalable, declarative, modular,\nand continuous automation for distributed systems. We focus on four points of\ndesign: 1) a state-management approach that prescribes source-of-truth for\nconfigured and discovered system states; 2) a technique to solve the\ndeclarative unification problem for a class of automation problems, providing\nstate convergence and modularity; 3) an eventual-consistency approach to state\nsynchronization which provides automation at scale; 4) an event-driven\narchitecture that provides always-on state enforcement.\n  We describe the methodology, software architecture for the framework, and\nconstraints for these techniques to apply to an automation problem. We overview\na reference application built on this framework that provides state-aware\nsystem provisioning and node lifecycle management, highlighting key advantages.\nWe conclude with a discussion of current and future applications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 15:33:11 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wofford", "J. Lowell", ""]]}, {"id": "2104.13429", "submitter": "Andreas Grammenos", "authors": "Andreas Grammenos, Evangelia Kalyvianaki, Peter Pietzuch", "title": "Pronto: Federated Task Scheduling", "comments": "19 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a federated, asynchronous, memory-limited algorithm for online\ntask scheduling across large-scale networks of hundreds of workers. This is\nachieved through recent advancements in federated edge computing that unlocks\nthe ability to incrementally compute local model updates within each node\nseparately. This local model is then used along with incoming data to generate\na rejection signal which reflects the overall node responsiveness and if it is\nable to accept an incoming task without resulting in degraded performance.\nThrough this innovation, we allow each node to execute scheduling decisions on\nwhether to accept an incoming job independently based on the workload seen thus\nfar. Further, using the aggregate of the iterates a global view of the system\ncan be constructed, as needed, and could be used to produce a holistic\nperspective of the system. We complement our findings, by an empirical\nevaluation on a large-scale real-world dataset of traces from a virtualized\nproduction data center that shows, while using limited memory, that our\nalgorithm exhibits state-of-the-art performance. Concretely, it is able to\npredict changes in the system responsiveness ahead of time based on the\nindustry-standard CPU-Ready metric and, in turn, can lead to better scheduling\ndecisions and overall utilization of the available resources. Finally, in the\nabsence of communication latency, it exhibits attractive horizontal\nscalability.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:58:38 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Grammenos", "Andreas", ""], ["Kalyvianaki", "Evangelia", ""], ["Pietzuch", "Peter", ""]]}, {"id": "2104.13499", "submitter": "Sima Hajiaghaei Shanjani", "authors": "Sima Hajiaghaei Shanjani and Valerie King", "title": "Communication Costs in a Geometric Communication Network", "comments": "A version of this work was submitted to ICDCN 2021", "journal-ref": null, "doi": "10.1145/3427796.3427800", "report-no": null, "categories": "cs.DC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A communication network is a graph in which each node has only local\ninformation about the graph and nodes communicate by passing messages along its\nedges. Here, we consider the {\\it geometric communication network} where the\nnodes also occupy points in space and the distance between points is the\nEuclidean distance. Our goal is to understand the communication cost needed to\nsolve several fundamental geometry problems, including Convex Hull, Diameter,\nClosest Pair, and approximations of these problems, in the asynchronous CONGEST\nKT1 model. This extends the 2011 result of Rajsbaum and Urrutia for finding a\nconvex hull of a planar geometric communication network to networks of\narbitrary topology.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 22:53:03 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Shanjani", "Sima Hajiaghaei", ""], ["King", "Valerie", ""]]}, {"id": "2104.13629", "submitter": "Sohei Itahara", "authors": "Sohei Itahara, Takayuki Nishio, and Koji Yamamoto", "title": "Packet-Loss-Tolerant Split Inference for Delay-Sensitive Deep Learning\n  in Lossy Wireless Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed inference framework is an emerging technology for real-time\napplications empowered by cutting-edge deep machine learning (ML) on\nresource-constrained Internet of things (IoT) devices. In distributed\ninference, computational tasks are offloaded from the IoT device to other\ndevices or the edge server via lossy IoT networks. However, narrow-band and\nlossy IoT networks cause non-negligible packet losses and retransmissions,\nresulting in non-negligible communication latency. This study solves the\nproblem of the incremental retransmission latency caused by packet loss in a\nlossy IoT network. We propose a split inference with no retransmissions (SI-NR)\nmethod that achieves high accuracy without any retransmissions, even when\npacket loss occurs. In SI-NR, the key idea is to train the ML model by\nemulating the packet loss by a dropout method, which randomly drops the output\nof hidden units in a DNN layer. This enables the SI-NR system to obtain\nrobustness against packet losses. Our ML experimental evaluation reveals that\nSI-NR obtains accurate predictions without packet retransmission at a packet\nloss rate of 60%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:28:22 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Itahara", "Sohei", ""], ["Nishio", "Takayuki", ""], ["Yamamoto", "Koji", ""]]}, {"id": "2104.13732", "submitter": "Alexander Brauckmann", "authors": "Alexander Brauckmann, Andr\\'es Goens, Jeronimo Castrillon", "title": "A Reinforcement Learning Environment for Polyhedral Optimizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polyhedral model allows a structured way of defining semantics-preserving\ntransformations to improve the performance of a large class of loops. Finding\nprofitable points in this space is a hard problem which is usually approached\nby heuristics that generalize from domain-expert knowledge. Existing problem\nformulations in state-of-the-art heuristics depend on the shape of particular\nloops, making it hard to leverage generic and more powerful optimization\ntechniques from the machine learning domain. In this paper, we propose PolyGym,\na shape-agnostic formulation for the space of legal transformations in the\npolyhedral model as a Markov Decision Process (MDP). Instead of using\ntransformations, the formulation is based on an abstract space of possible\nschedules. In this formulation, states model partial schedules, which are\nconstructed by actions that are reusable across different loops. With a simple\nheuristic to traverse the space, we demonstrate that our formulation is\npowerful enough to match and outperform state-of-the-art heuristics. On the\nPolybench benchmark suite, we found transformations that led to a speedup of\n3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our\ngeneric MDP formulation enables using reinforcement learning to learn\noptimization policies over a wide range of loops. This also contributes to the\nemerging field of machine learning in compilers, as it exposes a novel problem\nformulation that can push the limits of existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 12:41:52 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 08:04:04 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Brauckmann", "Alexander", ""], ["Goens", "Andr\u00e9s", ""], ["Castrillon", "Jeronimo", ""]]}, {"id": "2104.13813", "submitter": "Mirko Zichichi", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo", "title": "MOVO: a dApp for DLT-based Smart Mobility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenty of research on smart mobility is currently devoted to the inclusion of\nnovel decentralized software architectures to these systems, due to the\ninherent advantages in terms of transparency, traceability, trustworthiness.\nMOVO is a decentralized application (dApp) for smart mobility. It includes: (i)\na module for collecting data from vehicles and smartphones sensors; (ii) a\ncomponent for interacting with Distributed Ledger Technologies (DLT) and\nDecentralized File Storages (DFS), for storing and validating sensor data;\n(iii) a module for \"offline\" interaction between devices. The dApp consists of\nan Android application intended for use inside a vehicle, which helps the\nuser/driver collect contextually generated data (e.g. a driver's stress level,\nan electric vehicle's battery level), which can then be shared through the use\nof DLT (i.e., IOTA DLT and Ethereum smart contracts) and DFS (i.e., IPFS). The\nthird module consists of an implementation of a communication channel that, via\nWi-Fi Direct, allows two devices to exchange data and payment information with\nrespect to DLT (i.e. cryptocurrency and token) assets. In this paper, we\ndescribe the main software components and provide an experimental evaluation\nthat confirms the viability of the MOVO dApp in real mobility scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:01:28 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "2104.13819", "submitter": "Mirko Zichichi", "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D'Angelo", "title": "Towards Decentralized Complex Queries over Distributed Ledgers: a Data\n  Marketplace Use-case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Ledger Technologies (DLT) and Decentralized File Storages (DFS)\nare becoming increasingly used to create common, decentralized and trustless\ninfrastructures where participants interact and collaborate in Peer-to-Peer\ninteractions. A prominent use case is represented by decentralized data\nmarketplaces, where users are consumers and providers at the same time, and\ntrustless interactions are required. However, data in DLTs and DFS are usually\nunstructured and there are no efficient mechanisms to query a certain type of\ndata for the search in the market. In this paper, we propose the use of a\nDistributed Hash Table (DHT) as a layer on top of DLTs where, once the data are\nacquired and stored in the ledger, these can be searched through multiple\nkeyword based queries, thanks to the lookup functionalities offered by the DHT.\nThe DHT network is a hypercube overlay structure, organized for an efficient\nprocessing of multiple keyword-based queries. We provide the architecture of\nsuch solution for a decentralized data marketplace and an analysis based on a\nsimulation that proves the viability of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:11:50 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 08:53:23 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zichichi", "Mirko", ""], ["Serena", "Luca", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "2104.13869", "submitter": "Francisco Romero", "authors": "Francisco Romero, Gohar Irfan Chaudhry, \\'I\\~nigo Goiri, Pragna Gopa,\n  Paul Batum, Neeraja J. Yadwadkar, Rodrigo Fonseca, Christos Kozyrakis,\n  Ricardo Bianchini", "title": "Faa$T: A Transparent Auto-Scaling Cache for Serverless Applications", "comments": "18 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Function-as-a-Service (FaaS) has become an increasingly popular way for users\nto deploy their applications without the burden of managing the underlying\ninfrastructure. However, existing FaaS platforms rely on remote storage to\nmaintain state, limiting the set of applications that can be run efficiently.\nRecent caching work for FaaS platforms has tried to address this problem, but\nhas fallen short: it disregards the widely different characteristics of FaaS\napplications, does not scale the cache based on data access patterns, or\nrequires changes to applications. To address these limitations, we present\nFaa\\$T, a transparent auto-scaling distributed cache for serverless\napplications. Each application gets its own Faa\\$T cache. After a function\nexecutes and the application becomes inactive, the cache is unloaded from\nmemory with the application. Upon reloading for the next invocation, Faa\\$T\npre-warms the cache with objects likely to be accessed. In addition to\ntraditional compute-based scaling, Faa\\$T scales based on working set and\nobject sizes to manage cache space and I/O bandwidth. We motivate our design\nwith a comprehensive study of data access patterns in a large-scale commercial\nFaaS provider. We implement Faa\\$T for the provider's production FaaS platform.\nOur experiments show that Faa\\$T can improve performance by up to 92% (57% on\naverage) for challenging applications, and reduce cost for most users compared\nto state-of-the-art caching systems, i.e. the cost of having to stand up\nadditional serverful resources.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:31:19 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Romero", "Francisco", ""], ["Chaudhry", "Gohar Irfan", ""], ["Goiri", "\u00cd\u00f1igo", ""], ["Gopa", "Pragna", ""], ["Batum", "Paul", ""], ["Yadwadkar", "Neeraja J.", ""], ["Fonseca", "Rodrigo", ""], ["Kozyrakis", "Christos", ""], ["Bianchini", "Ricardo", ""]]}, {"id": "2104.13974", "submitter": "Mohammad Shojafar", "authors": "Farooq Hoseiny, Sadoon Azizi, Mohammad Shojafar, Rahim Tafazolli", "title": "Joint QoS-aware and Cost-efficient Task Scheduling for Fog-Cloud\n  Resources in a Volunteer Computing System", "comments": "21 pages, 6 figures, ACM Transactions on Internet Technology (TOIT)", "journal-ref": null, "doi": "10.1145/3418501", "report-no": null, "categories": "cs.DC math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Volunteer computing is an Internet-based distributed computing system in\nwhich volunteers share their extra available resources to manage large-scale\ntasks. However, computing devices in a Volunteer Computing System (VCS) are\nhighly dynamic and heterogeneous in terms of their processing power, monetary\ncost, and data transferring latency. To ensure both the high Quality of Service\n(QoS) and low cost for different requests, all of the available computing\nresources must be used efficiently. Task scheduling is an NP-hard problem that\nis considered one of the main critical challenges in a heterogeneous VCS. Due\nto this, in this paper, we design two task scheduling algorithms for VCSs,\nnamed Min-CCV and Min-V. The main goal of the proposed algorithms is jointly\nminimizing the computation, communication and delay violation cost for the\nInternet of Things (IoT) requests. Our extensive simulation results show that\nproposed algorithms are able to allocate tasks to volunteer fog/cloud resources\nmore efficiently than the state-of-the-art. Specifically, our algorithms\nimprove the deadline satisfaction task rates by around 99.5% and decrease the\ntotal cost between 15 to 53% in comparison with the genetic-based algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:58:51 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Hoseiny", "Farooq", ""], ["Azizi", "Sadoon", ""], ["Shojafar", "Mohammad", ""], ["Tafazolli", "Rahim", ""]]}, {"id": "2104.14050", "submitter": "Ahmed Ali-Eldin", "authors": "Ahmed Ali-Eldin, Bin Wang and Prashant Shenoy", "title": "The Hidden cost of the Edge: A Performance Comparison of Edge and Cloud\n  Latencies", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing has emerged as a popular paradigm for running\nlatency-sensitive applications due to its ability to offer lower network\nlatencies to end-users. In this paper, we argue that despite its lower network\nlatency, the resource-constrained nature of the edge can result in higher\nend-to-end latency, especially at higher utilizations, when compared to cloud\ndata centers. We study this edge performance inversion problem through an\nanalytic comparison of edge and cloud latencies and analyze conditions under\nwhich the edge can yield worse performance than the cloud. To verify our\nanalytic results, we conduct a detailed experimental comparison of the edge and\nthe cloud latencies using a realistic application and real cloud workloads.\nBoth our analytical and experimental results show that even at moderate\nutilizations, the edge queuing delays can offset the benefits of lower network\nlatencies, and even result in performance inversion where running in the cloud\nwould provide superior latencies. We finally discuss practical implications of\nour results and provide insights into how application designers and service\nproviders should design edge applications and systems to avoid these pitfalls.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:15:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ali-Eldin", "Ahmed", ""], ["Wang", "Bin", ""], ["Shenoy", "Prashant", ""]]}, {"id": "2104.14087", "submitter": "Bin Wang", "authors": "Bin Wang, Ahmed Ali-Eldin, Prashant Shenoy", "title": "LaSS: Running Latency Sensitive Serverless Computations at the Edge", "comments": "Accepted to ACM HPDC 2021", "journal-ref": null, "doi": "10.1145/3431379.3460646", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has emerged as a new paradigm for running short-lived\ncomputations in the cloud. Due to its ability to handle IoT workloads, there\nhas been considerable interest in running serverless functions at the edge.\nHowever, the constrained nature of the edge and the latency sensitive nature of\nworkloads result in many challenges for serverless platforms. In this paper, we\npresent LaSS, a platform that uses model-driven approaches for running\nlatency-sensitive serverless computations on edge resources. LaSS uses\nprincipled queuing-based methods to determine an appropriate allocation for\neach hosted function and auto-scales the allocated resources in response to\nworkload dynamics. LaSS uses a fair-share allocation approach to guarantee a\nminimum of allocated resources to each function in the presence of overload. In\naddition, it utilizes resource reclamation methods based on container deflation\nand termination to reassign resources from over-provisioned functions to\nunder-provisioned ones. We implement a prototype of our approach on an\nOpenWhisk serverless edge cluster and conduct a detailed experimental\nevaluation. Our results show that LaSS can accurately predict the resources\nneeded for serverless functions in the presence of highly dynamic workloads,\nand reprovision container capacity within hundreds of milliseconds while\nmaintaining fair share allocation guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:17:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wang", "Bin", ""], ["Ali-Eldin", "Ahmed", ""], ["Shenoy", "Prashant", ""]]}, {"id": "2104.14088", "submitter": "Zixian An", "authors": "Yunkai Wei, Zixian An, Supeng Leng and Kun Yang", "title": "Connecting AI Learning and Blockchain Mining in 6G Systems", "comments": "7 pages, 6 figures, submitted to IEEE Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sixth generation (6G) systems are generally recognized to be established\non ubiquitous Artificial Intelligence (AI) and distributed ledger such as\nblockchain. However, the AI training demands tremendous computing resource,\nwhich is limited in most 6G devices. Meanwhile, miners in Proof-of-Work (PoW)\nbased blockchains devote massive computing power to block mining, and are\nwidely criticized for the waste of computation. To address this dilemma, we\npropose an Evolved-Proof-of-Work (E-PoW) consensus that can integrate the\nmatrix computations, which are widely existed in AI training, into the process\nof brute-force searches in the block mining. Consequently, E-PoW can connect AI\nlearning and block mining via the multiply used common computing resource.\nExperimental results show that E-PoW can salvage by up to 80 percent computing\npower from pure block mining for parallel AI training in 6G systems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:19:52 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wei", "Yunkai", ""], ["An", "Zixian", ""], ["Leng", "Supeng", ""], ["Yang", "Kun", ""]]}, {"id": "2104.14186", "submitter": "Hatem Ltaief", "authors": "D. Keyes, H. Ltaief, Y. Nakatsukasa, and D. Sukkari", "title": "High-Performance Partial Spectrum Computation for Symmetric eigenvalue\n  problems and the SVD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current dense symmetric eigenvalue (EIG) and singular value decomposition\n(SVD) implementations may suffer from the lack of concurrency during the\ntridiagonal and bidiagonal reductions, respectively. This performance\nbottleneck is typical for the two-sided transformations due to the Level-2 BLAS\nmemory-bound calls. Therefore, the current state-of-the-art EIG and SVD\nimplementations may achieve only a small fraction of the system's sustained\npeak performance. The QR-based Dynamically Weighted Halley (QDWH) algorithm may\nbe used as a pre-processing step toward the EIG and SVD solvers, while\nmitigating the aforementioned bottleneck. QDWH-EIG and QDWH-SVD expose more\nparallelism, while relying on compute-bound matrix operations. Both run closer\nto the sustained peak performance of the system, but at the expense of\nperforming more FLOPS than the standard EIG and SVD algorithms. In this paper,\nwe introduce a new QDWH-based solver for computing the partial spectrum for EIG\n(QDWHpartial-EIG) and SVD (QDWHpartial-SVD) problems. By optimizing the\nrational function underlying the algorithms only in the desired part of the\nspectrum, QDWHpartial-EIG and QDWHpartial-SVD algorithms efficiently compute a\nfraction (say 1-20%) of the corresponding spectrum. We develop high-performance\nimplementations of QDWHpartial-EIG and QDWHpartial-SVD on distributed-memory\nanymore systems and demonstrate their numerical robustness. Experimental\nresults using up to 36K MPI processes show performance speedups for\nQDWHpartial-SVD up to 6X and 2X against PDGESVD from ScaLAPACK and KSVD,\nrespectively. QDWHpartial-EIG outperforms PDSYEVD from ScaLAPACK up to 3.5X but\nremains slower compared to ELPA. QDWHpartial-EIG achieves, however, a better\noccupancy of the underlying hardware by extracting higher sustained peak\nperformance than ELPA, which is critical moving forward with accelerator-based\nsupercomputers.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:04:23 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Keyes", "D.", ""], ["Ltaief", "H.", ""], ["Nakatsukasa", "Y.", ""], ["Sukkari", "D.", ""]]}, {"id": "2104.14246", "submitter": "Roberto Rocco", "authors": "Roberto Rocco, Davide Gadioli, Gianluca Palermo", "title": "Legio: Fault Resiliency for Embarrassingly Parallel MPI Applications", "comments": null, "journal-ref": null, "doi": "10.1007/s11227-021-03951-w", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the increasing size of HPC machines, the fault presence is becoming an\neventuality that applications must face. Natively, MPI provides no support for\nthe execution past the detection of a fault, and this is becoming more and more\nconstraining. With the introduction of ULFM (User Level Fault Mitigation\nlibrary), it has been provided with a possible way to overtake a fault during\nthe application execution at the cost of code modifications. ULFM is intrusive\nin the application and requires also a deep understanding of its recovery\nprocedures.\n  In this paper we propose Legio, a framework that lowers the complexity of\nintroducing resiliency in an embarrassingly parallel MPI application. By hiding\nULFM behind the MPI calls, the library is capable to expose resiliency features\nto the application in a transparent manner thus removing any integration\neffort. Upon fault, the failed nodes are discarded and the execution continues\nonly with the non-failed ones. A hierarchical implementation of the solution\nhas been also proposed to reduce the overhead of the repair process when\nscaling towards a large number of nodes.\n  We evaluated our solutions on the Marconi100 cluster at CINECA, showing that\nthe overhead introduced by the library is negligible and it does not limit the\nscalability properties of MPI. Moreover, we also integrated the solution in\nreal-world applications to further prove its robustness by injecting faults.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 10:34:21 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rocco", "Roberto", ""], ["Gadioli", "Davide", ""], ["Palermo", "Gianluca", ""]]}, {"id": "2104.14270", "submitter": "Mahbuba Afrin", "authors": "Mahbuba Afrin, Jiong Jin, Akhlaqur Rahman, Ashfaqur Rahman, Jiafu Wan\n  and Ekram Hossain", "title": "Resource Allocation and Service Provisioning in Multi-Agent Cloud\n  Robotics: A Comprehensive Survey", "comments": null, "journal-ref": null, "doi": "10.1109/COMST.2021.3061435", "report-no": null, "categories": "cs.RO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic applications nowadays are widely adopted to enhance operational\nautomation and performance of real-world Cyber-Physical Systems (CPSs)\nincluding Industry 4.0, agriculture, healthcare, and disaster management. These\napplications are composed of latency-sensitive, data-heavy, and\ncompute-intensive tasks. The robots, however, are constrained in the\ncomputational power and storage capacity. The concept of multi-agent cloud\nrobotics enables robot-to-robot cooperation and creates a complementary\nenvironment for the robots in executing large-scale applications with the\ncapability to utilize the edge and cloud resources. However, in such a\ncollaborative environment, the optimal resource allocation for robotic tasks is\nchallenging to achieve. Heterogeneous energy consumption rates and application\nof execution costs associated with the robots and computing instances make it\neven more complex. In addition, the data transmission delay between local\nrobots, edge nodes, and cloud data centres adversely affects the real-time\ninteractions and impedes service performance guarantee. Taking all these issues\ninto account, this paper comprehensively surveys the state-of-the-art on\nresource allocation and service provisioning in multi-agent cloud robotics. The\npaper presents the application domains of multi-agent cloud robotics through\nexplicit comparison with the contemporary computing paradigms and identifies\nthe specific research challenges. A complete taxonomy on resource allocation is\npresented for the first time, together with the discussion of resource pooling,\ncomputation offloading, and task scheduling for efficient service provisioning.\nFurthermore, we highlight the research gaps from the learned lessons, and\npresent future directions deemed beneficial to further advance this emerging\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 11:41:49 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Afrin", "Mahbuba", ""], ["Jin", "Jiong", ""], ["Rahman", "Akhlaqur", ""], ["Rahman", "Ashfaqur", ""], ["Wan", "Jiafu", ""], ["Hossain", "Ekram", ""]]}, {"id": "2104.14354", "submitter": "Tegg Sung", "authors": "Tegg Taekyong Sung, Bo Ryu", "title": "SoCRATES: System-on-Chip Resource Adaptive Scheduling using Deep\n  Reinforcement Learning", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) is being increasingly applied to the\nproblem of resource allocation for emerging System-on-Chip (SoC) applications,\nand has shown remarkable promises. In this paper, we introduce SoCRATES (SoC\nResource AdapTivE Scheduler), an extremely efficient DRL-based SoC scheduler\nwhich maps a wide range of hierarchical jobs to heterogeneous resources within\nSoC using the Eclectic Interaction Matching (EIM) technique. It is noted that\nthe majority of SoC resource management approaches have been targeting makespan\nminimization with fixed number of jobs in the system. In contrast, SoCRATES\naims at minimizing average latency in a steady-state condition while assigning\ntasks in the ready queue to heterogeneous resources (processing elements). We\nfirst show that existing DRL-based schedulers developed with the makespan\nminimization objective are ineffective for the latency-minimization-driven SoC\napplications due to their characteristics such as high-frequency job workload\nand distributed/parallel job execution. We then demonstrate that through its\nEIM technique, SoCRATES successfully addresses the challenge of concurrent\nobservations caused by the task dependency inherent in the latency minimization\nobjective. Extensive tests show that SoCRATES outperforms other existing neural\nand non-neural schedulers with as high as 38% gain in latency reduction under a\nvariety of job types, queue length, and incoming rates. The resulting model is\nalso compact in size and has very favorable energy consumption behaviors,\nmaking it highly practical for deployment in future SoC systems with built-in\nneural accelerator.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:46:02 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:49:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sung", "Tegg Taekyong", ""], ["Ryu", "Bo", ""]]}, {"id": "2104.14362", "submitter": "Ji Liu", "authors": "Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi Xiong,\n  Dejing Dou", "title": "From Distributed Machine Learning to Federated Learning: A Survey", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data and computing resources are typically distributed in\nthe devices of end users, various regions or organizations. Because of laws or\nregulations, the distributed data and computing resources cannot be directly\nshared among different regions or organizations for machine learning tasks.\nFederated learning emerges as an efficient approach to exploit distributed data\nand computing resources, so as to collaboratively train machine learning\nmodels, while obeying the laws and regulations and ensuring data security and\ndata privacy. In this paper, we provide a comprehensive survey of existing\nworks for federated learning. We propose a functional architecture of federated\nlearning systems and a taxonomy of related techniques. Furthermore, we present\nthe distributed training, data communication, and security of FL systems.\nFinally, we analyze their limitations and propose future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:15:11 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 10:55:17 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Ji", ""], ["Huang", "Jizhou", ""], ["Zhou", "Yang", ""], ["Li", "Xuhong", ""], ["Ji", "Shilei", ""], ["Xiong", "Haoyi", ""], ["Dou", "Dejing", ""]]}, {"id": "2104.14380", "submitter": "Fan Mo", "authors": "Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard Marin, Diego Perino,\n  Nicolas Kourtellis", "title": "PPFL: Privacy-preserving Federated Learning with Trusted Execution\n  Environments", "comments": "15 pages, 8 figures, accepted to MobiSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose and implement a Privacy-preserving Federated Learning ($PPFL$)\nframework for mobile systems to limit privacy leakages in federated learning.\nLeveraging the widespread presence of Trusted Execution Environments (TEEs) in\nhigh-end and mobile devices, we utilize TEEs on clients for local training, and\non servers for secure aggregation, so that model/gradient updates are hidden\nfrom adversaries. Challenged by the limited memory size of current TEEs, we\nleverage greedy layer-wise training to train each model's layer inside the\ntrusted area until its convergence. The performance evaluation of our\nimplementation shows that $PPFL$ can significantly improve privacy while\nincurring small system overheads at the client-side. In particular, $PPFL$ can\nsuccessfully defend the trained model against data reconstruction, property\ninference, and membership inference attacks. Furthermore, it can achieve\ncomparable model utility with fewer communication rounds (0.54$\\times$) and a\nsimilar amount of network traffic (1.002$\\times$) compared to the standard\nfederated learning of a complete model. This is achieved while only introducing\nup to ~15% CPU time, ~18% memory usage, and ~21% energy consumption overhead in\n$PPFL$'s client-side.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:46:16 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 20:51:12 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Mo", "Fan", ""], ["Haddadi", "Hamed", ""], ["Katevas", "Kleomenis", ""], ["Marin", "Eduard", ""], ["Perino", "Diego", ""], ["Kourtellis", "Nicolas", ""]]}, {"id": "2104.14392", "submitter": "Shreshth Tuli", "authors": "Shreshth Tuli, Shivananda Poojara, Satish N. Srirama, Giuliano Casale,\n  Nicholas R. Jennings", "title": "COSCO: Container Orchestration using Co-Simulation and Gradient Based\n  Optimization for Fog Computing Environments", "comments": "Accepted in IEEE Transactions on Parallel and Distributed Systems,\n  2021", "journal-ref": null, "doi": "10.1109/TPDS.2021.3087349", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent task placement and management of tasks in large-scale fog\nplatforms is challenging due to the highly volatile nature of modern workload\napplications and sensitive user requirements of low energy consumption and\nresponse time. Container orchestration platforms have emerged to alleviate this\nproblem with prior art either using heuristics to quickly reach scheduling\ndecisions or AI driven methods like reinforcement learning and evolutionary\napproaches to adapt to dynamic scenarios. The former often fail to quickly\nadapt in highly dynamic environments, whereas the latter have run-times that\nare slow enough to negatively impact response time. Therefore, there is a need\nfor scheduling policies that are both reactive to work efficiently in volatile\nenvironments and have low scheduling overheads. To achieve this, we propose a\nGradient Based Optimization Strategy using Back-propagation of gradients with\nrespect to Input (GOBI). Further, we leverage the accuracy of predictive\ndigital-twin models and simulation capabilities by developing a Coupled\nSimulation and Container Orchestration Framework (COSCO). Using this, we create\na hybrid simulation driven decision approach, GOBI*, to optimize Quality of\nService (QoS) parameters. Co-simulation and the back-propagation approaches\nallow these methods to adapt quickly in volatile environments. Experiments\nconducted using real-world data on fog applications using the GOBI and GOBI*\nmethods, show a significant improvement in terms of energy consumption,\nresponse time, Service Level Objective and scheduling time by up to 15, 40, 4,\nand 82 percent respectively when compared to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:09:44 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 17:32:14 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 13:08:48 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Tuli", "Shreshth", ""], ["Poojara", "Shivananda", ""], ["Srirama", "Satish N.", ""], ["Casale", "Giuliano", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "2104.14447", "submitter": "Quang-Thinh Ha", "authors": "Quang-Thinh Ha, Paul A. Kuberry, Nathaniel A. Trask, Emily M. Ryan", "title": "Parallel implementation of a compatible high-order meshless method for\n  the Stokes' equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA cs.PF math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel implementation of a compatible discretization scheme for\nsteady-state Stokes problems is presented in this work. The scheme uses\ngeneralized moving least squares to generate differential operators and apply\nboundary conditions. This meshless scheme allows a high-order convergence for\nboth the velocity and pressure, while also incorporates finite-difference-like\nsparse discretization. Additionally, the method is inherently scalable: the\nstencil generation process requires local inversion of matrices amenable to GPU\nacceleration, and the divergence-free treatment of velocity replaces the\ntraditional saddle point structure of the global system with elliptic diagonal\nblocks amenable to algebraic multigrid. The implementation in this work uses a\nvariety of Trilinos packages to exploit this local and global parallelism, and\nbenchmarks demonstrating high-order convergence and weak scalability are\nprovided.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:06:13 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ha", "Quang-Thinh", ""], ["Kuberry", "Paul A.", ""], ["Trask", "Nathaniel A.", ""], ["Ryan", "Emily M.", ""]]}, {"id": "2104.14549", "submitter": "Hrishikesh Dutta", "authors": "Hrishikesh Dutta and Subir Biswas", "title": "Medium Access using Distributed Reinforcement Learning for IoTs with\n  Low-Complexity Wireless Transceivers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a distributed Reinforcement Learning (RL) based framework\nthat can be used for synthesizing MAC layer wireless protocols in IoT networks\nwith low-complexity wireless transceivers. The proposed framework does not rely\non complex hardware capabilities such as carrier sensing and its associated\nalgorithmic complexities that are often not supported in wireless transceivers\nof low-cost and low-energy IoT devices. In this framework, the access protocols\nare first formulated as Markov Decision Processes (MDP) and then solved using\nRL. A distributed and multi-Agent RL framework is used as the basis for\nprotocol synthesis. Distributed behavior makes the nodes independently learn\noptimal transmission strategies without having to rely on full network level\ninformation and direct knowledge of behavior of other nodes. The nodes learn to\nminimize packet collisions such that optimal throughput can be attained and\nmaintained for loading conditions that are higher than what the known benchmark\nprotocols (such as ALOHA) for IoT devices without complex transceivers. In\naddition, the nodes are observed to be able to learn to act optimally in the\npresence of heterogeneous loading and network topological conditions. Finally,\nthe proposed learning approach allows the wireless bandwidth to be fairly\ndistributed among network nodes in a way that is not dependent on such\nheterogeneities. Via simulation experiments, the paper demonstrates the\nperformance of the learning paradigm and its abilities to make nodes adapt\ntheir optimal transmission strategies on the fly in response to various network\ndynamics.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:57:43 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Dutta", "Hrishikesh", ""], ["Biswas", "Subir", ""]]}, {"id": "2104.14641", "submitter": "Yao Wang", "authors": "Yao Wang, Xingyu Zhou, Yanming Wang, Rui Li, Yong Wu, Vin Sharma", "title": "Tuna: A Static Analysis Approach to Optimizing Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Tuna, a static analysis approach to optimizing deep neural\nnetwork programs. The optimization of tensor operations such as convolutions\nand matrix multiplications is the key to improving the performance of deep\nneural networks. Many deep learning model optimization mechanisms today use\ndynamic analysis, which relies on experimental execution on a target device to\nbuild a data-driven cost model of the program. The reliance on dynamic\nprofiling not only requires access to target hardware at compilation time but\nalso incurs significant cost in machine resources. We introduce an approach\nthat profiles the program by constructing features based on the target hardware\ncharacteristics in order. We use static analysis of the relative performance of\ntensor operations to optimize the deep learning program. Experiments show that\nour approach can achieve up to 11x performance compared to dynamic profiling\nbased methods with the same compilation time.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 20:22:02 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 19:04:53 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 02:44:50 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Yao", ""], ["Zhou", "Xingyu", ""], ["Wang", "Yanming", ""], ["Li", "Rui", ""], ["Wu", "Yong", ""], ["Sharma", "Vin", ""]]}, {"id": "2104.15003", "submitter": "Vitaly Aksenov", "authors": "Vitaly Aksenov, Nikita Koval, Petr Kuznetsov", "title": "Memory-Optimality for Non-Blocking Containers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A bounded container maintains a collection of elements that can be inserted\nand extracted as long as the number of stored elements does not exceed the\npredefined capacity. We consider the concurrent implementations of a bounded\ncontainer more or less memory-friendly depending on how much memory they use in\naddition to storing the elements. This way, memory-optimal implementation\nemploys the minimal possible memory overhead and ensures that data and metadata\nare stored most compactly, reducing cache misses and unnecessary memory\nreclamation.\n  In this paper, we show that non-blocking implementations of a large class of\nelement-independent bounded containers, including queues, stacks, and pools,\nmust incur linear in the number of concurrent processes memory overhead. In the\nspecial cases when the ABA problem is excluded, e.g., by assuming that the\nhardware supports LL/SC instructions or by storing distinct elements, we show\nthat the lower bound can be circumvented, by presenting lock-free bounded\nqueues with constant memory overhead. For the general case, we present a\nCAS-based bounded queue implementation that asymptotically matches our lower\nbound. We believe that these results open a new research avenue devoted to the\nmemory-optimality phenomenon.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:51:48 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Aksenov", "Vitaly", ""], ["Koval", "Nikita", ""], ["Kuznetsov", "Petr", ""]]}, {"id": "2104.15063", "submitter": "Kadir Korkmaz", "authors": "Kadir Korkmaz, Joachim Bruneau-Queyreix, Sonia Ben Mokthar, Laurent\n  R\\'eveill\\`ere", "title": "Dandelion: multiplexing Byzantine agreements to unlock blockchain\n  performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissionless blockchain protocols are known to consume an outrageous amount\nof computing power and suffer from a trade-off between latency and confidence\nin transaction confirmation. The recently proposed Algorand blockchain protocol\nemploys Byzantine agreements and has shown transaction confirmation latency on\nthe order of seconds. Its strong resilience to Denial-of-Service and Sybil\nattacks and its low computing power footprint make it a strong candidate for\nthe venue of decentralized economies and business ecosystems across industries.\nNevertheless, Algorand's throughput is still far from the requirements of such\napplications. In this paper, we empower Algorand's protocol by multiplexing its\nbyzantine agreements in order to improve performance. Experiments on wide area\nnetworks with up to ten thousand nodes show a 4-fold throughput increase\ncompared to the original Algorand protocol.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:32:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Korkmaz", "Kadir", ""], ["Bruneau-Queyreix", "Joachim", ""], ["Mokthar", "Sonia Ben", ""], ["R\u00e9veill\u00e8re", "Laurent", ""]]}, {"id": "2104.15099", "submitter": "Duong Nguyen", "authors": "Sandeep S Kulkarni, Gabe Appleton, Duong Nguyen", "title": "Achieving Causality with Physical Clocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physical clocks provide more precision than applications can use. For\nexample, a 64 bit NTP clock allows a precision of 233 picoseconds. In this\npaper, we focus on whether the least significant bits that are not useful to\nthe application could be used to track (one way) causality among events. We\npresent PWC (Physical clock With Causality) that uses the extraneous bits in\nthe physical clock. We show that PWC is very robust to errors in clock skew and\ntransient errors. We show that PWC can be used as both a physical and logical\nclock for a typical distributed application even if just 6-9 extraneous bits\n(corresponding to precision of 15-120 nanoseconds) are available. Another\nimportant characteristic of PWC is that the standard integer $<$ operation can\nbe used to compare timestamps to deduce (one-way) causality among events. Thus,\nPWC is significantly more versatile than previous approaches for using the\nphysical clock to provide causality information.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:23:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kulkarni", "Sandeep S", ""], ["Appleton", "Gabe", ""], ["Nguyen", "Duong", ""]]}]