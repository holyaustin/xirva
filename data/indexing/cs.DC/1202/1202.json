[{"id": "1202.0242", "submitter": "Daniel Zinn", "authors": "Daniel Zinn", "title": "Weak Forms of Monotonicity and Coordination-Freeness", "comments": "Early Research Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our earlier work titled: \"Win-move is Coordination-Free (Sometimes)\" has\nshown that the classes of queries that can be distributedly computed in a\ncoordination-free manner form a strict hierarchy depending on the assumptions\nof the model for distributed computations. In this paper, we further\ncharacterize these classes by revealing a tight relationship between them and\nnovel weakened forms of monotonicity.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 18:46:41 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Zinn", "Daniel", ""]]}, {"id": "1202.0457", "submitter": "Nicolas Le Scouarnec", "authors": "Nicolas Le Scouarnec", "title": "Exact Scalar Minimum Storage Coordinated Regenerating Codes", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the exact and optimal repair of multiple failures in codes for\ndistributed storage. More particularly, we examine the use of interference\nalignment to build exact scalar minimum storage coordinated regenerating codes\n(MSCR). We show that it is possible to build codes for the case of k = 2 and d\n> k by aligning interferences independently but that this technique cannot be\napplied as soon as k > 2 and d > k. Our results also apply to adaptive\nregenerating codes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 15:17:28 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Scouarnec", "Nicolas Le", ""]]}, {"id": "1202.0612", "submitter": "Dr. Nitin", "authors": "Ravi Rastogi, Nitin, Durg Singh Chauhan and Mahesh Chandra Govil", "title": "Disjoint Paths Multi-stage Interconnection Networks Stability Problem", "comments": "12 pages, 16 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 4, No 1, pages 260-271, July 2011", "doi": null, "report-no": "IJCSI-8-4-1-260-271", "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research paper emphasizes that the Stable Matching problems are the same\nas the problems of stable configurations of Multi-stage Interconnection\nNetworks (MIN). The authors have solved the Stability Problem of Existing\nRegular Gamma Multi-stage Interconnection Network (GMIN), 3-Disjoint Gamma\nMulti-stage Interconnection Network (3DGMIN) and 3-Disjoint Path Cyclic Gamma\nMulti-stage Interconnection Network (3DCGMIN) using the approaches and\nsolutions provided by the Stable Matching Problem. Specifically Stable Marriage\nProblem is used as an example of Stable Matching. For MINs to prove Stable two\nexisting algorithms are used:-the first algorithm generates the MINs\nPreferences List in time and second algorithm produces a set of most Optimal\nPairs of the Switching Elements (SEs) (derived from the MINs Preferences List)\nin time. Moreover, the paper also solves the problem of Ties that occurs\nbetween the Optimal Pairs. The results are promising as the comparison of the\nMINs based on their stability shows that the ASEN, ABN, CLN, GMIN, 3DCGMIN are\nhighly stable in comparison to HZTN, QTN, DGMIN. However, on comparing the\nirregular and regular MINs in totality upon their stability the regular MINs\ncomes out to be more stable than the irregular MINs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 06:42:44 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Rastogi", "Ravi", ""], ["Nitin", "", ""], ["Chauhan", "Durg Singh", ""], ["Govil", "Mahesh Chandra", ""]]}, {"id": "1202.0616", "submitter": "Dr. Nitin", "authors": "Ravi Rastogi, Amit Singh, Nikhil Singhal, Nitin and Durg Singh Chauhan", "title": "Case Tool: Fast Interconnections with New 3-Disjoint Paths MIN\n  Simulation Module", "comments": "6 pages, 6 figures", "journal-ref": "International Journal of Computer Applications (0975 - 8887),\n  Volume 19 - No.6, April 2011", "doi": null, "report-no": "pxc3873110", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-stage interconnection networks (MIN) can be designed to achieve fault\ntolerance and collision solving by providing a set of disjoint paths. In this\npaper, we are discussing the new simulator added to the tool designed for\ndeveloping fault tolerant MINs. The designed tool is one of its own kind and\nwill help the user in developing 2 and 3-disjoint path networks. The java\ntechnology has been used to design the tool and have been tested on different\nsoftware platform.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 07:03:49 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Rastogi", "Ravi", ""], ["Singh", "Amit", ""], ["Singhal", "Nikhil", ""], ["Nitin", "", ""], ["Chauhan", "Durg Singh", ""]]}, {"id": "1202.0970", "submitter": "Josef Spillner", "authors": "Josef Spillner and Alexander Schill", "title": "{\\pi}-Control: A Personal Cloud Control Centre", "comments": "Position paper: 4 pages, 5 figures; Russian translation available in\n  the source archive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumption of online services and cloud computing offerings is on the rise,\nlargely due to compelling advantages over traditional local applications. From\na user perspective, these include zero-maintenance of software, the always-on\nnature of such services, mashups of different applications and the networking\neffect with other users. Associated disadvantages are known, but effective\nmeans and tools to limit their effect are not yet well-established and not yet\ngenerally available to service users. We propose (1) a user-centric model of\ncloud elements beyond the conventional <SPI>aaS layers, including activities\nacross trust zones, and (2) a personal control console for all individual and\ncollaborative user activities in the cloud.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2012 15:25:02 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Spillner", "Josef", ""], ["Schill", "Alexander", ""]]}, {"id": "1202.1050", "submitter": "Nihar Shah", "authors": "K. V. Rashmi, Nihar B. Shah, Kannan Ramchandran and P. Vijay Kumar", "title": "Regenerating Codes for Errors and Erasures in Distributed Storage", "comments": "ISIT 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regenerating codes are a class of codes proposed for providing reliability of\ndata and efficient repair of failed nodes in distributed storage systems. In\nthis paper, we address the fundamental problem of handling errors and erasures\nduring the data-reconstruction and node-repair operations. We provide explicit\nregenerating codes that are resilient to errors and erasures, and show that\nthese codes are optimal with respect to storage and bandwidth requirements. As\na special case, we also establish the capacity of a class of distributed\nstorage systems in the presence of malicious adversaries. While our code\nconstructions are based on previously constructed Product-Matrix codes, we also\nprovide necessary and sufficient conditions for introducing resilience in any\nregenerating code.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 05:31:07 GMT"}, {"version": "v2", "created": "Wed, 23 May 2012 06:46:57 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Rashmi", "K. V.", ""], ["Shah", "Nihar B.", ""], ["Ramchandran", "Kannan", ""], ["Kumar", "P. Vijay", ""]]}, {"id": "1202.1056", "submitter": "Michael McKerns", "authors": "Michael M. McKerns, Leif Strand, Tim Sullivan, Alta Fang, and Michael\n  A. G. Aivazis", "title": "Building a Framework for Predictive Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key questions that scientists and engineers typically want to address can be\nformulated in terms of predictive science. Questions such as: \"How well does my\ncomputational model represent reality?\", \"What are the most important\nparameters in the problem?\", and \"What is the best next experiment to perform?\"\nare fundamental in solving scientific problems. Mystic is a framework for\nmassively-parallel optimization and rigorous sensitivity analysis that enables\nthese motivating questions to be addressed quantitatively as global\noptimization problems. Often realistic physics, engineering, and materials\nmodels may have hundreds of input parameters, hundreds of constraints, and may\nrequire execution times of seconds or longer. In more extreme cases, realistic\nmodels may be multi-scale, and require the use of high-performance computing\nclusters for their evaluation. Predictive calculations, formulated as a global\noptimization over a potential surface in design parameter space, may require an\nalready prohibitively large simulation to be performed hundreds, if not\nthousands, of times. The need to prepare, schedule, and monitor thousands of\nmodel evaluations, and dynamically explore and analyze results, is a\nchallenging problem that requires a software infrastructure capable of\ndistributing and managing computations on large-scale heterogeneous resources.\nIn this paper, we present the design behind an optimization framework, and also\na framework for heterogeneous computing, that when utilized together, can make\ncomputationally intractable sensitivity and optimization problems much more\ntractable.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 06:53:25 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["McKerns", "Michael M.", ""], ["Strand", "Leif", ""], ["Sullivan", "Tim", ""], ["Fang", "Alta", ""], ["Aivazis", "Michael A. G.", ""]]}, {"id": "1202.1062", "submitter": "Dr. Nitin", "authors": "Ravi Rastogi, Nitin, Durg Singh Chauhan and Mahesh Chandra Govil", "title": "On Stability Problems of Omega and 3-Disjoint Paths Omega Multi-stage\n  Interconnection Networks", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 4, No 2, July 2011, 66-76", "doi": null, "report-no": "IJCSI-8-4-2-66-76", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research paper emphasizes that the Stable Matching problems are the same\nas the problems of stable configurations of Multi-stage Interconnection\nNetworks (MIN). We have discusses the Stability Problems of Existing Regular\nOmega Multi-stage Interconnection Network (OMIN) and Proposed 3-Disjoint Paths\nOmega Multi-stage Interconnection Network (3DON) using the approaches and\nsolutions provided by the Stable Matching Problem. Specifically, Stable\nMarriage Problem is used as an example of Stable Matching. On application of\nthe concept of the Stable Marriage over the MINs states that OMIN is highly\nstable in comparison to 3DON.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 07:39:06 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Rastogi", "Ravi", ""], ["Nitin", "", ""], ["Chauhan", "Durg Singh", ""], ["Govil", "Mahesh Chandra", ""]]}, {"id": "1202.1186", "submitter": "Yuval Emek", "authors": "Yuval Emek and Jasmin Smula and Roger Wattenhofer", "title": "Stone Age Distributed Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional models of distributed computing focus mainly on networks of\ncomputer-like devices that can exchange large messages with their neighbors and\nperform arbitrary local computations. Recently, there is a trend to apply\ndistributed computing methods to networks of sub-microprocessor devices, e.g.,\nbiological cellular networks or networks of nano-devices. However, the\nsuitability of the traditional distributed computing models to these types of\nnetworks is questionable: do tiny bio/nano nodes \"compute\" and/or \"communicate\"\nessentially the same as a computer? In this paper, we introduce a new model\nthat depicts a network of randomized finite state machines operating in an\nasynchronous environment. Although the computation and communication\ncapabilities of each individual device in the new model are, by design, much\nweaker than those of a computer, we show that some of the most important and\nextensively studied distributed computing problems can still be solved\nefficiently.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 16:20:06 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 08:03:51 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Emek", "Yuval", ""], ["Smula", "Jasmin", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1202.1350", "submitter": "Justin Thaler", "authors": "Justin Thaler, Mike Roberts, Michael Mitzenmacher, Hanspeter Pfister", "title": "Verifiable Computation with Massively Parallel Interactive Proofs", "comments": "17 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the cloud computing paradigm has gained prominence, the need for\nverifiable computation has grown increasingly urgent. The concept of verifiable\ncomputation enables a weak client to outsource difficult computations to a\npowerful, but untrusted, server. Protocols for verifiable computation aim to\nprovide the client with a guarantee that the server performed the requested\ncomputations correctly, without requiring the client to perform the\ncomputations herself. By design, these protocols impose a minimal computational\nburden on the client. However, existing protocols require the server to perform\na large amount of extra bookkeeping in order to enable a client to easily\nverify the results. Verifiable computation has thus remained a theoretical\ncuriosity, and protocols for it have not been implemented in real cloud\ncomputing systems.\n  Our goal is to leverage GPUs to reduce the server-side slowdown for\nverifiable computation. To this end, we identify abundant data parallelism in a\nstate-of-the-art general-purpose protocol for verifiable computation,\noriginally due to Goldwasser, Kalai, and Rothblum, and recently extended by\nCormode, Mitzenmacher, and Thaler. We implement this protocol on the GPU,\nobtaining 40-120x server-side speedups relative to a state-of-the-art\nsequential implementation. For benchmark problems, our implementation reduces\nthe slowdown of the server to factors of 100-500x relative to the original\ncomputations requested by the client. Furthermore, we reduce the already small\nruntime of the client by 100x. Similarly, we obtain 20-50x server-side and\nclient-side speedups for related protocols targeted at specific streaming\nproblems. We believe our results demonstrate the immediate practicality of\nusing GPUs for verifiable computation, and more generally that protocols for\nverifiable computation have become sufficiently mature to deploy in real cloud\ncomputing systems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 04:42:49 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2012 05:42:28 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2012 04:15:37 GMT"}], "update_date": "2012-02-23", "authors_parsed": [["Thaler", "Justin", ""], ["Roberts", "Mike", ""], ["Mitzenmacher", "Michael", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1202.1565", "submitter": "Endre Cs\\'oka", "authors": "Endre Cs\\'oka", "title": "Random local algorithms", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem when we want to construct some structure on a bounded\ndegree graph, e.g. an almost maximum matching, and we want to decide about each\nedge depending only on its constant radius neighbourhood. We show that the\ninformation about the local statistics of the graph does not help here. Namely,\nif there exists a random local algorithm which can use any local statistics\nabout the graph, and produces an almost optimal structure, then the same can be\nachieved by a random local algorithm using no statistics.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 23:56:49 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 23:31:00 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Cs\u00f3ka", "Endre", ""]]}, {"id": "1202.1567", "submitter": "Robert Nix", "authors": "Robert Nix and Murat Kantarcioglu", "title": "Efficient Query Verification on Outsourced Data: A Game-Theoretic\n  Approach", "comments": "13 pages, 8 figures, pre-publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To save time and money, businesses and individuals have begun outsourcing\ntheir data and computations to cloud computing services. These entities would,\nhowever, like to ensure that the queries they request from the cloud services\nare being computed correctly. In this paper, we use the principles of economics\nand competition to vastly reduce the complexity of query verification on\noutsourced data. We consider two cases: First, we consider the scenario where\nmultiple non-colluding data outsourcing services exist, and then we consider\nthe case where only a single outsourcing service exists. Using a game theoretic\nmodel, we show that given the proper incentive structure, we can effectively\ndeter dishonest behavior on the part of the data outsourcing services with very\nfew computational and monetary resources. We prove that the incentive for an\noutsourcing service to cheat can be reduced to zero. Finally, we show that a\nsimple verification method can achieve this reduction through extensive\nexperimental evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 00:35:35 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Nix", "Robert", ""], ["Kantarcioglu", "Murat", ""]]}, {"id": "1202.1801", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler, Asaf Cohen, Chen Avin, Muriel M\\'edard", "title": "Network Coded Gossip with Correlated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and analyze gossip algorithms for networks with correlated data. In\nthese networks, either the data to be distributed, the data already available\nat the nodes, or both, are correlated. This model is applicable for a variety\nof modern networks, such as sensor, peer-to-peer and content distribution\nnetworks.\n  Although coding schemes for correlated data have been studied extensively,\nthe focus has been on characterizing the rate region in static memory-free\nnetworks. In a gossip-based scheme, however, nodes communicate among each other\nby continuously exchanging packets according to some underlying communication\nmodel. The main figure of merit in this setting is the stopping time -- the\ntime required until nodes can successfully decode. While Gossip schemes are\npractical, distributed and scalable, they have only been studied for\nuncorrelated data.\n  We wish to close this gap by providing techniques to analyze network coded\ngossip in (dynamic) networks with correlated data. We give a clean framework\nfor oblivious network models that applies to a multitude of network and\ncommunication scenarios, specify a general setting for distributed correlated\ndata, and give tight bounds on the stopping times of network coded protocols in\nthis wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 19:42:29 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Cohen", "Asaf", ""], ["Avin", "Chen", ""], ["M\u00e9dard", "Muriel", ""]]}, {"id": "1202.1925", "submitter": "Matthias F\\\"ugger", "authors": "Danny Dolev, Matthias F\\\"ugger, Christoph Lenzen, Markus Posch, Ulrich\n  Schmid, Andreas Steininger", "title": "FATAL+: A Self-Stabilizing Byzantine Fault-tolerant Clocking Scheme for\n  SoCs", "comments": "arXiv admin note: significant text overlap with arXiv:1105.4780", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present concept and implementation of a self-stabilizing Byzantine\nfault-tolerant distributed clock generation scheme for multi-synchronous GALS\narchitectures in critical applications. It combines a variant of a recently\nintroduced self-stabilizing algorithm for generating low-frequency,\nlow-accuracy synchronized pulses with a simple non-stabilizing high-frequency,\nhigh-accuracy clock synchronization algorithm. We provide thorough correctness\nproofs and a performance analysis, which use methods from fault-tolerant\ndistributed computing research but also addresses hardware-related issues like\nmetastability. The algorithm, which consists of several concurrent\ncommunicating asynchronous state machines, has been implemented in VHDL using\nPetrify in conjunction with some extensions, and synthetisized for an Altera\nCyclone FPGA. An experimental validation of this prototype has been carried out\nto confirm the skew and clock frequency bounds predicted by the theoretical\nanalysis, as well as the very short stabilization times (required for\nrecovering after excessively many transient failures) achievable in practice.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 09:30:32 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Dolev", "Danny", ""], ["F\u00fcgger", "Matthias", ""], ["Lenzen", "Christoph", ""], ["Posch", "Markus", ""], ["Schmid", "Ulrich", ""], ["Steininger", "Andreas", ""]]}, {"id": "1202.1941", "submitter": "Atul Mishra", "authors": "A.K. Sharma, Atul Mishra, Vijay Singh", "title": "An Intelligent Mobile-Agent Based Scalable Network Management\n  Architecture for Large-Scale Enterprise System", "comments": "16 pages, 10 figures, published in IJCNC", "journal-ref": "International Journal of Computer Networks and Communications\n  (IJCNC), January 2012, Volume 4, Number 1, ISSN [online : 0974-9322]", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several Mobile Agent based distributed network management models have been\nproposed in recent times to address the scalability and flexibility problems of\ncentralized (SNMP or CMIP management models) models. Though the use of Mobile\nAgents to distribute and delegate management tasks comes handy in dealing with\nthe previously stated issues, many of the agent-based management frameworks\nlike initial flat bed models and static mid-level managers employing mobile\nagents models cannot efficiently meet the demands of current networks which are\ngrowing in size and complexity. Moreover, varied technologies, such as SONET,\nATM, Ethernet, DWDM etc., present at different layers of the Access, Metro and\nCore (long haul) sections of the network, have contributed to the complexity in\nterms of their own framing and protocol structures. Thus, controlling and\nmanaging the traffic in these networks is a challenging task. This paper\npresents an intelligent scalable hierarchical agent based model for the\nmanagement of large-scale complex networks to address aforesaid issues. The\ncost estimation, carried out with a view to compute the overall management cost\nin terms of management data overhead, is being presented. The results obtained\nthereafter establish the usefulness of the presented architecture as compare to\ncentralized and flat bed agent based models.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 10:48:37 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Sharma", "A. K.", ""], ["Mishra", "Atul", ""], ["Singh", "Vijay", ""]]}, {"id": "1202.1983", "submitter": "Seth Pettie", "authors": "Leonid Barenboim, Michael Elkin, Seth Pettie, and Johannes Schneider", "title": "The Locality of Distributed Symmetry Breaking", "comments": "In submission to J. ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry breaking problems are among the most well studied in the field of\ndistributed computing and yet the most fundamental questions about their\ncomplexity remain open. In this paper we work in the LOCAL model (where the\ninput graph and underlying distributed network are identical) and study the\nrandomized complexity of four fundamental symmetry breaking problems on graphs:\ncomputing MISs (maximal independent sets), maximal matchings, vertex colorings,\nand ruling sets. A small sample of our results includes\n  - An MIS algorithm running in $O(\\log^2\\Delta + 2^{O(\\sqrt{\\log\\log n})})$\ntime, where $\\Delta$ is the maximum degree. This is the first MIS algorithm to\nimprove on the 1986 algorithms of Luby and Alon, Babai, and Itai, when $\\log n\n\\ll \\Delta \\ll 2^{\\sqrt{\\log n}}$, and comes close to the $\\Omega(\\log \\Delta)$\nlower bound of Kuhn, Moscibroda, and Wattenhofer.\n  - A maximal matching algorithm running in $O(\\log\\Delta + \\log^4\\log n)$\ntime. This is the first significant improvement to the 1986 algorithm of\nIsraeli and Itai. Moreover, its dependence on $\\Delta$ is provably optimal.\n  - A method for reducing symmetry breaking problems in low\narboricity/degeneracy graphs to low degree graphs. (Roughly speaking, the\narboricity or degeneracy of a graph bounds the density of any subgraph.)\nCorollaries of this reduction include an $O(\\sqrt{\\log n})$-time maximal\nmatching algorithm for graphs with arboricity up to $2^{\\sqrt{\\log n}}$ and an\n$O(\\log^{2/3} n)$-time MIS algorithm for graphs with arboricity up to $2^{(\\log\nn)^{1/3}}$.\n  Each of our algorithms is based on a simple, but powerful technique for\nreducing a randomized symmetry breaking task to a corresponding deterministic\none on a poly$(\\log n)$-size graph.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 13:46:11 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 15:07:27 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2015 21:24:15 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Barenboim", "Leonid", ""], ["Elkin", "Michael", ""], ["Pettie", "Seth", ""], ["Schneider", "Johannes", ""]]}, {"id": "1202.2092", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler, Gopal Pandurangan, David Peleg, Rajmohan Rajaraman,\n  Zhifeng Sun", "title": "Discovery through Gossip", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study randomized gossip-based processes in dynamic networks that are\nmotivated by discovery processes in large-scale distributed networks like\npeer-to-peer or social networks.\n  A well-studied problem in peer-to-peer networks is the resource discovery\nproblem. There, the goal for nodes (hosts with IP addresses) is to discover the\nIP addresses of all other hosts. In social networks, nodes (people) discover\nnew nodes through exchanging contacts with their neighbors (friends). In both\ncases the discovery of new nodes changes the underlying network - new edges are\nadded to the network - and the process continues in the changed network.\nRigorously analyzing such dynamic (stochastic) processes with a continuously\nself-changing topology remains a challenging problem with obvious applications.\n  This paper studies and analyzes two natural gossip-based discovery processes.\nIn the push process, each node repeatedly chooses two random neighbors and puts\nthem in contact (i.e., \"pushes\" their mutual information to each other). In the\npull discovery process, each node repeatedly requests or \"pulls\" a random\ncontact from a random neighbor. Both processes are lightweight, local, and\nnaturally robust due to their randomization.\n  Our main result is an almost-tight analysis of the time taken for these two\nrandomized processes to converge. We show that in any undirected n-node graph\nboth processes take O(n log^2 n) rounds to connect every node to all other\nnodes with high probability, whereas Omega(n log n) is a lower bound. In the\ndirected case we give an O(n^2 log n) upper bound and an Omega(n^2) lower bound\nfor strongly connected directed graphs. A key technical challenge that we\novercome is the analysis of a randomized process that itself results in a\nconstantly changing network which leads to complicated dependencies in every\nround.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 19:48:46 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Pandurangan", "Gopal", ""], ["Peleg", "David", ""], ["Rajaraman", "Rajmohan", ""], ["Sun", "Zhifeng", ""]]}, {"id": "1202.2153", "submitter": "Everthon Valadao", "authors": "Everthon Valadao, Dorgival Guedes, Ricardo Duarte", "title": "Caracteriza\\c{c}\\~ao de tempos de ida-e-volta na Internet", "comments": null, "journal-ref": "Revista Brasileira de Redes de Computadores e Sistemas\n  Distribu\\'idos, v. 3, p. 21-34, 2010", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Round-trip times (RTTs) are an important metric for the operation of many\napplications in the Internet. For instance, they are taken into account when\nchoosing servers or peers in streaming systems, and they impact the operation\nof fault detectors and congestion control algorithms. Therefore, detailed\nknowledge about RTTs is important for application and protocol developers. In\nthis work we present results on measuring RTTs between 81 PlanetLab nodes every\nten seconds, for ten days. The resulting dataset has over 550 million\nmeasurements. Our analysis gives us a profile of delays in the network and\nidentifies a Gamma distribution as the model that best fits our data. The\naverage times observed are below 500 ms in more than 99% of the pairs, but\nthere is significant variation, not only when we compare different pairs of\nhosts during the experiment, but also considering any given pair of hosts over\ntime. By using a clustering technique, we observe that links can be divided in\nfive distinct groups based on the distribution of RTTs over time and the losses\nobserved, ranging from groups of near, well-connected pairs, to groups of\ndistant hosts, with lower quality links between them.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 23:50:42 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Valadao", "Everthon", ""], ["Guedes", "Dorgival", ""], ["Duarte", "Ricardo", ""]]}, {"id": "1202.2293", "submitter": "Adrian  Neumann", "authors": "Karl Bringmann, Kurt Mehlhorn and Adrian Neumann (MPI for Informatics)", "title": "Remarks on Category-Based Routing in Social Networks", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that individuals can route messages on short paths through\nsocial networks, given only simple information about the target and using only\nlocal knowledge about the topology. Sociologists conjecture that people find\nroutes greedily by passing the message to an acquaintance that has more in\ncommon with the target than themselves, e.g. if a dentist in Saarbr\\\"ucken\nwants to send a message to a specific lawyer in Munich, he may forward it to\nsomeone who is a lawyer and/or lives in Munich. Modelling this setting,\nEppstein et al. introduced the notion of category-based routing. The goal is to\nassign a set of categories to each node of a graph such that greedy routing is\npossible. By proving bounds on the number of categories a node has to be in we\ncan argue about the plausibility of the underlying sociological model. In this\npaper we substantially improve the upper bounds introduced by Eppstein et al.\nand prove new lower bounds.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 16:06:58 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2012 09:31:44 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2012 16:56:17 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Bringmann", "Karl", "", "MPI for Informatics"], ["Mehlhorn", "Kurt", "", "MPI for Informatics"], ["Neumann", "Adrian", "", "MPI for Informatics"]]}, {"id": "1202.2466", "submitter": "Amitabh Trehan", "authors": "Amitabh Trehan", "title": "Self-healing systems and virtual structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern networks are large, highly complex and dynamic. Add to that the\nmobility of the agents comprising many of these networks. It is difficult or\neven impossible for such systems to be managed centrally in an efficient\nmanner. It is imperative for such systems to attain a degree of\nself-management. Self-healing i.e. the capability of a system in a good state\nto recover to another good state in face of an attack, is desirable for such\nsystems. In this paper, we discuss the self-healing model for dynamic\nreconfigurable systems. In this model, an omniscient adversary inserts or\ndeletes nodes from a network and the algorithm responds by adding a limited\nnumber of edges in order to maintain invariants of the network. We look at some\nof the results in this model and argue for their applicability and further\nextensions of the results and the model. We also look at some of the techniques\nwe have used in our earlier work, in particular, we look at the idea of\nmaintaining virtual graphs mapped over the existing network and assert that\nthis may be a useful technique to use in many problem domains.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2012 20:11:39 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2012 14:55:51 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Trehan", "Amitabh", ""]]}, {"id": "1202.2509", "submitter": "Bogdan Alexandru Caprarescu", "authors": "Nicolo M. Calcavecchia, Bogdan Alexandru Caprarescu, Elisabetta Di\n  Nitto, Daniel J. Dubois, Dana Petcu", "title": "DEPAS: A Decentralized Probabilistic Algorithm for Auto-Scaling", "comments": "Submitted to Springer Computing", "journal-ref": null, "doi": null, "report-no": "Technical Report 2012.5, Politecnico di Milano", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic provisioning of virtualized resources offered by cloud computing\ninfrastructures allows applications deployed in a cloud environment to\nautomatically increase and decrease the amount of used resources. This\ncapability is called auto-scaling and its main purpose is to automatically\nadjust the scale of the system that is running the application to satisfy the\nvarying workload with minimum resource utilization. The need for auto-scaling\nis particularly important during workload peaks, in which applications may need\nto scale up to extremely large-scale systems.\n  Both the research community and the main cloud providers have already\ndeveloped auto-scaling solutions. However, most research solutions are\ncentralized and not suitable for managing large-scale systems, moreover cloud\nproviders' solutions are bound to the limitations of a specific provider in\nterms of resource prices, availability, reliability, and connectivity.\n  In this paper we propose DEPAS, a decentralized probabilistic auto-scaling\nalgorithm integrated into a P2P architecture that is cloud provider\nindependent, thus allowing the auto-scaling of services over multiple cloud\ninfrastructures at the same time. Our simulations, which are based on real\nservice traces, show that our approach is capable of: (i) keeping the overall\nutilization of all the instantiated cloud resources in a target range, (ii)\nmaintaining service response times close to the ones obtained using optimal\ncentralized auto-scaling approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2012 09:26:40 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Calcavecchia", "Nicolo M.", ""], ["Caprarescu", "Bogdan Alexandru", ""], ["Di Nitto", "Elisabetta", ""], ["Dubois", "Daniel J.", ""], ["Petcu", "Dana", ""]]}, {"id": "1202.2551", "submitter": "Ciprian Dobre", "authors": "Ciprian Dobre, Florin Pop, Valentin Cristea", "title": "A Simulation Model for Evaluating Distributed Systems Dependability", "comments": "Please cite this as \"Ciprian Dobre, Florin Pop, Valentin Cristea, A\n  Simulation Model for Evaluating Distributed Systems Dependability, 23rd\n  annual European Simulation and Modelling Conference (ESM'2009), 2009, pp.\n  62-69, ISBN: 978-90-77381-52-6, EUROSIS-ETI\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new simulation model designed to evaluate the\ndependability in distributed systems. This model extends the MONARC simulation\nmodel with new capabilities for capturing reliability, safety, availability,\nsecurity, and maintainability requirements. The model has been implemented as\nan extension of the multithreaded, process oriented simulator MONARC, which\nallows the realistic simulation of a wide-range of distributed system\ntechnologies, with respect to their specific components and characteristics.\nThe extended simulation model includes the necessary components to inject\nvarious failure events, and provides the mechanisms to evaluate different\nstrategies for replication, redundancy procedures, and security enforcement\nmechanisms, as well. The results obtained in simulation experiments presented\nin this paper probe that the use of discrete-event simulators, such as MONARC,\nin the design and development of distributed systems is appealing due to their\nefficiency and scalability.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2012 18:17:10 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Dobre", "Ciprian", ""], ["Pop", "Florin", ""], ["Cristea", "Valentin", ""]]}, {"id": "1202.2981", "submitter": "Bogdan Alexandru Caprarescu", "authors": "Bogdan Alexandru Caprarescu, Eva Kaslik, Dana Petcu", "title": "Theoretical Analysis and Tuning of Decentralized Probabilistic\n  Auto-Scaling", "comments": "Submitted to Journal of Computer and System Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major impediment towards the industrial adoption of decentralized\ndistributed systems comes from the difficulty to theoretically prove that these\nsystems exhibit the required behavior. In this paper, we use probability theory\nto analyze a decentralized auto-scaling algorithm in which each node\nprobabilistically decides to scale in or out. We prove that, in the context of\ndynamic workloads, the average load of the system is maintained within a\nvariation interval with a given probability, provided that the number of nodes\nand the variation interval length are higher than certain bounds. The paper\nalso proposes numerical algorithms for approximating these minimum bounds.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 10:21:51 GMT"}], "update_date": "2012-02-15", "authors_parsed": [["Caprarescu", "Bogdan Alexandru", ""], ["Kaslik", "Eva", ""], ["Petcu", "Dana", ""]]}, {"id": "1202.3084", "submitter": "Florian Huc", "authors": "Rachid Guerraoui and Florian Huc and Anne-Marie Kermarrec", "title": "On Dynamic Distributed Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows for the first time that distributed computing can be both\nreliable and efficient in an environment that is both highly dynamic and\nhostile. More specifically, we show how to maintain clusters of size $O(\\log\nN)$, each containing more than two thirds of honest nodes with high\nprobability, within a system whose size can vary \\textit{polynomially} with\nrespect to its initial size. Furthermore, the communication cost induced by\neach node arrival or departure is polylogarithmic with respect to $N$, the\nmaximal size of the system. Our clustering can be achieved despite the presence\nof a Byzantine adversary controlling a fraction $\\bad \\leq \\{1}{3}-\\epsilon$ of\nthe nodes, for some fixed constant $\\epsilon > 0$, independent of $N$. So far,\nsuch a clustering could only be performed for systems who size can vary\nconstantly and it was not clear whether that was at all possible for polynomial\nvariances.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:50:12 GMT"}, {"version": "v2", "created": "Wed, 15 May 2013 21:12:11 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Huc", "Florian", ""], ["Kermarrec", "Anne-Marie", ""]]}, {"id": "1202.3108", "submitter": "Dohy Hong", "authors": "Dohy Hong", "title": "D-iteration based asynchronous distributed computation", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to explain how the D-iteration can be used for an\nefficient asynchronous distributed computation. We present the main ideas of\nthe method and illustrate them through very simple examples.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 18:07:32 GMT"}], "update_date": "2012-02-15", "authors_parsed": [["Hong", "Dohy", ""]]}, {"id": "1202.3173", "submitter": "Olga Holtz", "authors": "Grey Ballard, James Demmel, Olga Holtz, Benjamin Lipshitz, Oded\n  Schwartz", "title": "Communication-Optimal Parallel Algorithm for Strassen's Matrix\n  Multiplication", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.NA math.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel matrix multiplication is one of the most studied fundamental\nproblems in distributed and high performance computing. We obtain a new\nparallel algorithm that is based on Strassen's fast matrix multiplication and\nminimizes communication. The algorithm outperforms all known parallel matrix\nmultiplication algorithms, classical and Strassen-based, both asymptotically\nand in practice.\n  A critical bottleneck in parallelizing Strassen's algorithm is the\ncommunication between the processors. Ballard, Demmel, Holtz, and Schwartz\n(SPAA'11) prove lower bounds on these communication costs, using expansion\nproperties of the underlying computation graph. Our algorithm matches these\nlower bounds, and so is communication-optimal. It exhibits perfect strong\nscaling within the maximum possible range.\n  Benchmarking our implementation on a Cray XT4, we obtain speedups over\nclassical and Strassen-based algorithms ranging from 24% to 184% for a fixed\nmatrix dimension n=94080, where the number of nodes ranges from 49 to 7203.\n  Our parallelization approach generalizes to other fast matrix multiplication\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 23:12:23 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Ballard", "Grey", ""], ["Demmel", "James", ""], ["Holtz", "Olga", ""], ["Lipshitz", "Benjamin", ""], ["Schwartz", "Oded", ""]]}, {"id": "1202.3177", "submitter": "Olga Holtz", "authors": "Grey Ballard, James Demmel, Olga Holtz, Benjamin Lipshitz, Oded\n  Schwartz", "title": "Strong Scaling of Matrix Multiplication Algorithms and\n  Memory-Independent Communication Lower Bounds", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.NA math.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel algorithm has perfect strong scaling if its running time on P\nprocessors is linear in 1/P, including all communication costs.\nDistributed-memory parallel algorithms for matrix multiplication with perfect\nstrong scaling have only recently been found. One is based on classical matrix\nmultiplication (Solomonik and Demmel, 2011), and one is based on Strassen's\nfast matrix multiplication (Ballard, Demmel, Holtz, Lipshitz, and Schwartz,\n2012). Both algorithms scale perfectly, but only up to some number of\nprocessors where the inter-processor communication no longer scales.\n  We obtain a memory-independent communication cost lower bound on classical\nand Strassen-based distributed-memory matrix multiplication algorithms. These\nbounds imply that no classical or Strassen-based parallel matrix multiplication\nalgorithm can strongly scale perfectly beyond the ranges already attained by\nthe two parallel algorithms mentioned above. The memory-independent bounds and\nthe strong scaling bounds generalize to other algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 23:42:19 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Ballard", "Grey", ""], ["Demmel", "James", ""], ["Holtz", "Olga", ""], ["Lipshitz", "Benjamin", ""], ["Schwartz", "Oded", ""]]}, {"id": "1202.3205", "submitter": "Julian Shun", "authors": "Guy Blelloch, Jeremy Fineman, Julian Shun", "title": "Greedy Sequential Maximal Independent Set and Matching are Parallel on\n  Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greedy sequential algorithm for maximal independent set (MIS) loops over\nthe vertices in arbitrary order adding a vertex to the resulting set if and\nonly if no previous neighboring vertex has been added. In this loop, as in many\nsequential loops, each iterate will only depend directly on a subset of the\nprevious iterates (i.e. knowing that any one of a vertices neighbors is in the\nMIS or knowing that it has no previous neighbors is sufficient to decide its\nfate). This leads to a dependence structure among the iterates. If this\nstructure is shallow then running the iterates in parallel while respecting the\ndependencies can lead to an efficient parallel implementation mimicking the\nsequential algorithm.\n  In this paper, we show that for any graph, and for a random ordering of the\nvertices, the dependence depth of the sequential greedy MIS algorithm is\npolylogarithmic (O(log^2 n) with high probability). Our results extend previous\nresults that show polylogarithmic bounds only for random graphs. We show\nsimilar results for a greedy maximal matching (MM). For both problems we\ndescribe simple linear work parallel algorithms based on the approach. The\nalgorithms allow for a smooth tradeoff between more parallelism and reduced\nwork, but always return the same result as the sequential greedy algorithms. We\npresent experimental results that demonstrate efficiency and the tradeoff\nbetween work and parallelism.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 05:19:40 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Blelloch", "Guy", ""], ["Fineman", "Jeremy", ""], ["Shun", "Julian", ""]]}, {"id": "1202.3669", "submitter": "Samer Al-Kiswany", "authors": "Samer Al-Kiswany, Abdullah Gharaibeh, Matei Ripeanu", "title": "GPUs as Storage System Accelerators", "comments": "IEEE Transactions on Parallel and Distributed Systems, 2012", "journal-ref": null, "doi": "10.1109/TPDS.2012.239", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively multicore processors, such as Graphics Processing Units (GPUs),\nprovide, at a comparable price, a one order of magnitude higher peak\nperformance than traditional CPUs. This drop in the cost of computation, as any\norder-of-magnitude drop in the cost per unit of performance for a class of\nsystem components, triggers the opportunity to redesign systems and to explore\nnew ways to engineer them to recalibrate the cost-to-performance relation. This\nproject explores the feasibility of harnessing GPUs' computational power to\nimprove the performance, reliability, or security of distributed storage\nsystems. In this context, we present the design of a storage system prototype\nthat uses GPU offloading to accelerate a number of computationally intensive\nprimitives based on hashing, and introduce techniques to efficiently leverage\nthe processing power of GPUs. We evaluate the performance of this prototype\nunder two configurations: as a content addressable storage system that\nfacilitates online similarity detection between successive versions of the same\nfile and as a traditional system that uses hashing to preserve data integrity.\nFurther, we evaluate the impact of offloading to the GPU on competing\napplications' performance. Our results show that this technique can bring\ntangible performance gains without negatively impacting the performance of\nconcurrently running applications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 19:08:29 GMT"}, {"version": "v2", "created": "Wed, 16 May 2012 18:21:46 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Al-Kiswany", "Samer", ""], ["Gharaibeh", "Abdullah", ""], ["Ripeanu", "Matei", ""]]}, {"id": "1202.3777", "submitter": "Lu Zheng", "authors": "Lu Zheng, Ole Mengshoel, Jike Chong", "title": "Belief Propagation by Message Passing in Junction Trees: Computing Each\n  Message Faster Using GPU Parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-822-830", "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compiling Bayesian networks (BNs) to junction trees and performing belief\npropagation over them is among the most prominent approaches to computing\nposteriors in BNs. However, belief propagation over junction tree is known to\nbe computationally intensive in the general case. Its complexity may increase\ndramatically with the connectivity and state space cardinality of Bayesian\nnetwork nodes. In this paper, we address this computational challenge using GPU\nparallelization. We develop data structures and algorithms that extend existing\njunction tree techniques, and specifically develop a novel approach to\ncomputing each belief propagation message in parallel. We implement our\napproach on an NVIDIA GPU and test it using BNs from several applications.\nExperimentally, we study how junction tree parameters affect parallelization\nopportunities and hence the performance of our algorithm. We achieve speedups\nranging from 0.68 to 9.18 for the BNs studied.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Zheng", "Lu", ""], ["Mengshoel", "Ole", ""], ["Chong", "Jike", ""]]}, {"id": "1202.3943", "submitter": "Daniel S. Katz", "authors": "Daniel S. Katz, Timothy G. Armstrong, Zhao Zhang, Michael Wilde,\n  Justin M. Wozniak", "title": "Many-Task Computing and Blue Waters", "comments": null, "journal-ref": "Technical Report CI-TR-13-0911. Computation Institute, University\n  of Chicago & Argonne National Laboratory. 2012. http://www.ci.uchicago.\n  edu/research/papers/CI-TR-13-0911", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report discusses many-task computing (MTC) generically and in the\ncontext of the proposed Blue Waters systems, which is planned to be the largest\nNSF-funded supercomputer when it begins production use in 2012. The aim of this\nreport is to inform the BW project about MTC, including understanding aspects\nof MTC applications that can be used to characterize the domain and\nunderstanding the implications of these aspects to middleware and policies.\nMany MTC applications do not neatly fit the stereotypes of high-performance\ncomputing (HPC) or high-throughput computing (HTC) applications. Like HTC\napplications, by definition MTC applications are structured as graphs of\ndiscrete tasks, with explicit input and output dependencies forming the graph\nedges. However, MTC applications have significant features that distinguish\nthem from typical HTC applications. In particular, different engineering\nconstraints for hardware and software must be met in order to support these\napplications. HTC applications have traditionally run on platforms such as\ngrids and clusters, through either workflow systems or parallel programming\nsystems. MTC applications, in contrast, will often demand a short time to\nsolution, may be communication intensive or data intensive, and may comprise\nvery short tasks. Therefore, hardware and software for MTC must be engineered\nto support the additional communication and I/O and must minimize task dispatch\noverheads. The hardware of large-scale HPC systems, with its high degree of\nparallelism and support for intensive communication, is well suited for MTC\napplications. However, HPC systems often lack a dynamic resource-provisioning\nfeature, are not ideal for task communication via the file system, and have an\nI/O system that is not optimized for MTC-style applications. Hence, additional\nsoftware support is likely to be required to gain full benefit from the HPC\nhardware.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2012 16:01:53 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Katz", "Daniel S.", ""], ["Armstrong", "Timothy G.", ""], ["Zhang", "Zhao", ""], ["Wilde", "Michael", ""], ["Wozniak", "Justin M.", ""]]}, {"id": "1202.4185", "submitter": "Charles Cartledge", "authors": "Charles L. Cartledge and Michael L. Nelson", "title": "When Should I Make Preservation Copies of Myself?", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how different preservation policies ranging from least\naggressive to Most aggressive affect the level of preservation achieved by\nautonomic processes used by smart digital objects (DOs). The mechanisms used to\nsupport preservation across different hosts can be used for automatic link\ngeneration and support preservation activities by moving data preservation from\nan archive centric perspective to a data centric preservation. Based on\nsimulations of small-world graphs of DOs created using the Unsupervised\nSmall-World algorithm, we report quantitative and qualitative results for\ngraphs ranging in size from 10 to 5000 DOs. Our results show that a Most\naggressive preservation policy makes the best use of distributed host resources\nwhile using one half of the number of messages of a Moderately aggressive\npreservation policy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2012 21:03:33 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Cartledge", "Charles L.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1202.4229", "submitter": "EPTCS", "authors": "Johannes Reich (SAP), Bernd Finkbeiner (Universit\\\"at des Saarlandes)", "title": "Proceedings Second International Workshop on Interactions, Games and\n  Protocols", "comments": "EPTCS 78, 2012", "journal-ref": null, "doi": "10.4204/EPTCS.78", "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the second International Workshop on\nInteractions, Games and Protocols (IWIGP 2012). The workshop was held in\nTallinn on March 25, 2012, as a satellite event of ETAPS 2012. The previous\nworkshop took place in Saarbr\\\"ucken as part of ETAPS 2011.\n  The goal of this workshop was to bring researchers from industry and academia\ntogether and to explore how a better understanding of the interrelation between\ninteractions, games and protocols leads to better-designed and more reliable\ninteracting systems. We invited scientific contributions both from a\ntheoretical and a practical perspective.\n  The program consisted of two invited talks and four refereed papers, selected\nby a strong program committee of international reputation. The refereed papers\nare contained in this volume.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 05:17:10 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Reich", "Johannes", "", "SAP"], ["Finkbeiner", "Bernd", "", "Universit\u00e4t des Saarlandes"]]}, {"id": "1202.4347", "submitter": "Amit Bawaskar", "authors": "Jayshree Ghorpade, Jitendra Parande, Madhura Kulkarni, Amit Bawaskar", "title": "GPGPU Processing in CUDA Architecture", "comments": "16 pages, 5 figures, Advanced Computing: an International Journal\n  (ACIJ) 2012", "journal-ref": null, "doi": "10.5121/acij.2012.3109", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future of computation is the Graphical Processing Unit, i.e. the GPU. The\npromise that the graphics cards have shown in the field of image processing and\naccelerated rendering of 3D scenes, and the computational capability that these\nGPUs possess, they are developing into great parallel computing units. It is\nquite simple to program a graphics processor to perform general parallel tasks.\nBut after understanding the various architectural aspects of the graphics\nprocessor, it can be used to perform other taxing tasks as well. In this paper,\nwe will show how CUDA can fully utilize the tremendous power of these GPUs.\nCUDA is NVIDIA's parallel computing architecture. It enables dramatic increases\nin computing performance, by harnessing the power of the GPU. This paper talks\nabout CUDA and its architecture. It takes us through a comparison of CUDA C/C++\nwith other parallel programming languages like OpenCL and DirectCompute. The\npaper also lists out the common myths about CUDA and how the future seems to be\npromising for CUDA.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 15:16:40 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Ghorpade", "Jayshree", ""], ["Parande", "Jitendra", ""], ["Kulkarni", "Madhura", ""], ["Bawaskar", "Amit", ""]]}, {"id": "1202.4486", "submitter": "Yoann Dieudonn\\'e", "authors": "Yoann dieudonn\\'e and Florence Lev\\'e and Franck Petit and Vincent\n  Villain", "title": "Deterministic Leader Election Among Disoriented Anonymous Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the Leader Election (LE) problem in networks of anonymous sensors\nsharing no kind of common coordinate system. Leader Election is a fundamental\nsymmetry breaking problem in distributed computing. Its goal is to assign value\n1 (leader) to one of the entities and value 0 (non-leader) to all others. In\nthis paper, assuming n > 1 disoriented anonymous sensors, we provide a complete\ncharac- terization on the sensors positions to deterministically elect a\nleader, provided that all the sensors' positions are known by every sensor.\nMore precisely, our contribution is twofold: First, assuming n anonymous\nsensors agreeing on a common handedness (chirality) of their own coordinate\nsystem, we provide a complete characterization on the sensors positions to\ndeterministically elect a leader. Second, we also provide such a complete\nchararacterization for sensors devoided of a common handedness. Both\ncharacterizations rely on a particular object from combinatorics on words,\nnamely the Lyndon Words.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 22:42:06 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["dieudonn\u00e9", "Yoann", ""], ["Lev\u00e9", "Florence", ""], ["Petit", "Franck", ""], ["Villain", "Vincent", ""]]}, {"id": "1202.4508", "submitter": "EPTCS", "authors": "Johannes Reich", "title": "Processes, Roles and Their Interactions", "comments": "In Proceedings IWIGP 2012, arXiv:1202.4229", "journal-ref": "EPTCS 78, 2012, pp. 24-38", "doi": "10.4204/EPTCS.78.3", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking an interaction network oriented perspective in informatics raises the\nchallenge to describe deterministic finite systems which take part in networks\nof nondeterministic interactions. The traditional approach to describe\nprocesses as stepwise executable activities which are not based on the\nordinarily nondeterministic interaction shows strong centralization tendencies.\nAs suggested in this article, viewing processes and their interactions as\ncomplementary can circumvent these centralization tendencies.\n  The description of both, processes and their interactions is based on the\nsame building blocks, namely finite input output automata (or transducers).\nProcesses are viewed as finite systems that take part in multiple, ordinarily\nnondeterministic interactions. The interactions between processes are described\nas protocols.\n  The effects of communication between processes as well as the necessary\ncoordination of different interactions within a processes are both based on the\nrestriction of the transition relation of product automata. The channel based\nouter coupling represents the causal relation between the output and the input\nof different systems. The coordination condition based inner coupling\nrepresents the causal relation between the input and output of a single system.\n  All steps are illustrated with the example of a network of resource\nadministration processes which is supposed to provide requesting user processes\nexclusive access to a single resource.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2012 01:41:49 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Reich", "Johannes", ""]]}, {"id": "1202.4576", "submitter": "Maxwell Young", "authors": "Seth Gilbert, Maxwell Young", "title": "Making Evildoers Pay: Resource-Competitive Broadcast in Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a time-slotted, single-hop, wireless sensor network (WSN) consisting\nof n correct devices and and t=f*n Byzantine devices where f>=0 is any\nconstant; that is, the Byzantine devices may outnumber the correct ones. There\nexists a trusted sender Alice who wishes to deliver a message m over a single\nchannel to the correct devices. There also exists a malicious user Carol who\ncontrols the t Byzantine devices and uses them to disrupt the communication\nchannel. For a constant k>=2, the correct and Byzantine devices each possess a\nmeager energy budget of O(n^{1/k}), Alice and Carol each possess a limited\nbudget of \\tilde{O}(n^{1/k}), and sending or listening in a slot incurs unit\ncost. This general setup captures the inherent challenges of guaranteeing\ncommunication despite scarce resources and attacks on the network. Given this\nAlice versus Carol scenario, we ask: Is communication of m feasible and, if so,\nat what cost?\n  We develop a protocol which, for an arbitrarily small constant \\epsilon>0,\nensures that at least (1-\\epsilon)n correct devices receive m with high\nprobability. Furthermore, if Carol's devices expend T energy jamming the\nchannel, then Alice and the correct devices each spend only\n\\tilde{O}(T^{1/(k+1)}). In other words, delaying the transmission of m forces a\njammer to rapidly deplete its energy supply and, consequently, cease attacks on\nthe network.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2012 09:42:51 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2012 05:36:14 GMT"}, {"version": "v3", "created": "Sun, 22 Apr 2012 17:48:38 GMT"}, {"version": "v4", "created": "Tue, 15 May 2012 00:45:22 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Gilbert", "Seth", ""], ["Young", "Maxwell", ""]]}, {"id": "1202.5261", "submitter": "Vladimir Savic Dr", "authors": "Vladimir Savic, Henk Wymeersch, Santiago Zazo", "title": "Belief Consensus Algorithms for Fast Distributed Target Tracking in\n  Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed target tracking for wireless sensor networks, agreement on the\ntarget state can be achieved by the construction and maintenance of a\ncommunication path, in order to exchange information regarding local likelihood\nfunctions. Such an approach lacks robustness to failures and is not easily\napplicable to ad-hoc networks. To address this, several methods have been\nproposed that allow agreement on the global likelihood through fully\ndistributed belief consensus (BC) algorithms, operating on local likelihoods in\ndistributed particle filtering (DPF). However, a unified comparison of the\nconvergence speed and communication cost has not been performed. In this paper,\nwe provide such a comparison and propose a novel BC algorithm based on belief\npropagation (BP). According to our study, DPF based on metropolis belief\nconsensus (MBC) is the fastest in loopy graphs, while DPF based on BP consensus\nis the fastest in tree graphs. Moreover, we found that BC-based DPF methods\nhave lower communication overhead than data flooding when the network is\nsufficiently sparse.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 18:32:59 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2012 07:58:46 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2013 12:20:22 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2013 12:08:33 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Savic", "Vladimir", ""], ["Wymeersch", "Henk", ""], ["Zazo", "Santiago", ""]]}, {"id": "1202.5480", "submitter": "Richard McClatchey", "authors": "Khawar Hasham, Antonio Delgado Peris, Ashiq Anjum, Dave Evans, Dirk\n  Hufnagel, Eduardo Huedo, Jos\\'e M. Hern\\'andez, Richard McClatchey, Stephen\n  Gowdy, Simon Metson", "title": "CMS Workflow Execution using Intelligent Job Scheduling and Data Access\n  Strategies", "comments": "12 pages, 12 figures", "journal-ref": "IEEE Transactions in Nuclear Science 58 (3) pp. 1221-1232. ISSN\n  0018-9499 2011", "doi": "10.1109/TNS.2011.2146276", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex scientific workflows can process large amounts of data using\nthousands of tasks. The turnaround times of these workflows are often affected\nby various latencies such as the resource discovery, scheduling and data access\nlatencies for the individual workflow processes or actors. Minimizing these\nlatencies will improve the overall execution time of a workflow and thus lead\nto a more efficient and robust processing environment. In this paper, we\npropose a pilot job based infrastructure that has intelligent data reuse and\njob execution strategies to minimize the scheduling, queuing, execution and\ndata access latencies. The results have shown that significant improvements in\nthe overall turnaround time of a workflow can be achieved with this approach.\nThe proposed approach has been evaluated, first using the CMS Tier0 data\nprocessing workflow, and then simulating the workflows to evaluate its\neffectiveness in a controlled environment.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 15:36:38 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Hasham", "Khawar", ""], ["Peris", "Antonio Delgado", ""], ["Anjum", "Ashiq", ""], ["Evans", "Dave", ""], ["Hufnagel", "Dirk", ""], ["Huedo", "Eduardo", ""], ["Hern\u00e1ndez", "Jos\u00e9 M.", ""], ["McClatchey", "Richard", ""], ["Gowdy", "Stephen", ""], ["Metson", "Simon", ""]]}, {"id": "1202.5482", "submitter": "Richard McClatchey", "authors": "Hanene Boussi Rahmouni, Kamran Munir, Mohammed Odeh and Richard\n  McClatchey", "title": "Risk-Driven Compliant Access Controls for Clouds", "comments": "9 pages, 3 figures. International Arab Conference on Information\n  Technology (ACIT 2011) / Riyadh, Saudi Arabia. December 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is widespread agreement that cloud computing have proven cost cutting\nand agility benefits. However, security and regulatory compliance issues are\ncontinuing to challenge the wide acceptance of such technology both from social\nand commercial stakeholders. An important facture behind this is the fact that\nclouds and in particular public clouds are usually deployed and used within\nbroad geographical or even international domains. This implies that the\nexchange of private and other protected data within the cloud environment would\nbe governed by multiple jurisdictions. These jurisdictions have a great degree\nof harmonisation; however, they present possible conflicts that are hard to\nnegotiate at run time. So far, important efforts were played in order to deal\nwith regulatory compliance management for large distributed systems. However,\nmeasurable solutions are required for the context of cloud. In this position\npaper, we are suggesting an approach that starts with a conceptual model of\nexplicit regulatory requirements for exchanging private data on a\nmultijurisdictional environment and build on it in order to define metrics for\nnon-compliance or, in other terms, risks to compliance. These metrics will be\nintegrated within usual data access-control policies and will be checked at\npolicy analysis time before a decision to allow/deny the data access is made.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 15:49:39 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 08:55:44 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Rahmouni", "Hanene Boussi", ""], ["Munir", "Kamran", ""], ["Odeh", "Mohammed", ""], ["McClatchey", "Richard", ""]]}, {"id": "1202.5483", "submitter": "Richard McClatchey", "authors": "Zaheer Khan, David Ludlow, Richard McClatchey, Ashiq Anjum", "title": "An Architecture for Integrated Intelligence in Urban Management using\n  Cloud Computing", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of new methodologies and technologies it has now become\npossible to manage large amounts of environmental sensing data and apply new\nintegrated computing models to acquire information intelligence. This paper\nadvocates the application of cloud capacity to support the information,\ncommunication and decision making needs of a wide variety of stakeholders in\nthe complex business of the management of urban and regional development. The\ncomplexity lies in the interactions and impacts embodied in the concept of the\nurban-ecosystem at various governance levels. This highlights the need for more\neffective integrated environmental management systems. This paper offers a\nuser-orientated approach based on requirements for an effective management of\nthe urban-ecosystem and the potential contributions that can be supported by\nthe cloud computing community. Furthermore, the commonality of the influence of\nthe drivers of change at the urban level offers the opportunity for the cloud\ncomputing community to develop generic solutions that can serve the needs of\nhundreds of cities from Europe and indeed globally.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 15:52:56 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Khan", "Zaheer", ""], ["Ludlow", "David", ""], ["McClatchey", "Richard", ""], ["Anjum", "Ashiq", ""]]}, {"id": "1202.5509", "submitter": "Jacob Beal", "authors": "Jacob Beal, Stefan Dulman, Kyle Usbeck, Mirko Viroli, and Nikolaus\n  Correll", "title": "Organizing the Aggregate: Languages for Spatial Computing", "comments": "60 pages; Review chapter to appear as a chapter in book \"Formal and\n  Practical Aspects of Domain-Specific Languages: Recent Developments\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of computing devices embedded into engineered systems continues\nto rise, there is a widening gap between the needs of the user to control\naggregates of devices and the complex technology of individual devices. Spatial\ncomputing attempts to bridge this gap for systems with local communication by\nexploiting the connection between physical locality and device connectivity. A\nlarge number of spatial computing domain specific languages (DSLs) have emerged\nacross diverse domains, from biology and reconfigurable computing, to sensor\nnetworks and agent-based systems. In this chapter, we develop a framework for\nanalyzing and comparing spatial computing DSLs, survey the current state of the\nart, and provide a roadmap for future spatial computing DSL investigation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 17:35:03 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2012 03:13:16 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Beal", "Jacob", ""], ["Dulman", "Stefan", ""], ["Usbeck", "Kyle", ""], ["Viroli", "Mirko", ""], ["Correll", "Nikolaus", ""]]}, {"id": "1202.5512", "submitter": "Richard McClatchey", "authors": "Asif Osman, Ashiq Anjum, Naheed Batool, Richard McClatchey", "title": "A Fault Tolerant, Dynamic and Low Latency BDII Architecture for Grids", "comments": "18 pages; 10 figures; 4 tables", "journal-ref": "International Journal of Grid and Distributed Computing Vol. 3,\n  No. 4, December, 2010", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current BDII model relies on information gathering from agents that run\non each core node of a Grid. This information is then published into a Grid\nwide information resource known as Top BDII. The Top level BDIIs are updated\ntypically in cycles of a few minutes each. A new BDDI architecture is proposed\nand described in this paper based on the hypothesis that only a few attribute\nvalues change in each BDDI information cycle and consequently it may not be\nnecessary to update each parameter in a cycle. It has been demonstrated that\nsignificant performance gains can be achieved by exchanging only the\ninformation about records that changed during a cycle. Our investigations have\nled us to implement a low latency and fault tolerant BDII system that involves\nonly minimal data transfer and facilitates secure transactions in a Grid\nenvironment.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 17:51:35 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Osman", "Asif", ""], ["Anjum", "Ashiq", ""], ["Batool", "Naheed", ""], ["McClatchey", "Richard", ""]]}, {"id": "1202.5519", "submitter": "Richard McClatchey", "authors": "Saad Liaquat Kiani, Ashiq Anjum, Nick Antonopoulos, Michael\n  Knappmeyer, Nigel Baker, Richard McClatchey", "title": "Context-Aware Service Utilisation in the Clouds and Energy Conservation", "comments": "27 pages; 17 figures; 2 tables. Under review at the Journal of\n  Ambient Intelligence and Humanized Computing. 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous computing environments are characterised by smart, interconnected\nartefacts embedded in our physical world that are projected to provide useful\nservices to human inhabitants unobtrusively. Mobile devices are becoming the\nprimary tools of human interaction with these embedded artefacts and\nutilisation of services available in smart computing environments such as\nclouds. Advancements in capabilities of mobile devices allow a number of user\nand environment related context consumers to be hosted on these devices.\nWithout a coordinating component, these context consumers and providers are a\npotential burden on device resources; specifically the effect of uncoordinated\ncomputation and communication with cloud-enabled services can negatively impact\nthe battery life. Therefore energy conservation is a major concern in realising\nthe collaboration and utilisation of mobile device based context-aware\napplications and cloud based services. This paper presents the concept of a\ncontext-brokering component to aid in coordination and communication of context\ninformation between mobile devices and services deployed in a cloud\ninfrastructure. A prototype context broker is experimentally analysed for\neffects on energy conservation when accessing and coordinating with cloud\nservices on a smart device, with results signifying reduction in energy\nconsumption.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 18:21:23 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Kiani", "Saad Liaquat", ""], ["Anjum", "Ashiq", ""], ["Antonopoulos", "Nick", ""], ["Knappmeyer", "Michael", ""], ["Baker", "Nigel", ""], ["McClatchey", "Richard", ""]]}, {"id": "1202.6094", "submitter": "Lewis Tseng", "authors": "Nitin Vaidya, Lewis Tseng, Guanfeng Liang", "title": "Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs -\n  Part II: Synchronous and Asynchronous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report contains two related sets of results with different assumptions\non synchrony. The first part is about iterative algorithms in synchronous\nsystems. Following our previous work on synchronous iterative approximate\nByzantine consensus (IABC) algorithms, we provide a more intuitive tight\nnecessary and sufficient condition for the existence of such algorithms in\nsynchronous networks1. We believe this condition and the previous results also\nhold in partially asynchronous algorithmic model.\n  In the second part of the report, we explore the problem in asynchronous\nnetworks. While the traditional Byzantine consensus is not solvable in\nasynchronous systems, approximate Byzantine consensus can be solved using\niterative algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 00:01:48 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2012 15:55:23 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Vaidya", "Nitin", ""], ["Tseng", "Lewis", ""], ["Liang", "Guanfeng", ""]]}, {"id": "1202.6134", "submitter": "Jianfeng Zhan", "authors": "Jianfeng Zhan, Lixin Zhang, Ninghui Sun, Lei Wang, Zhen Jia, and\n  Chunjie Luo", "title": "High Volume Computing: Identifying and Characterizing Throughput\n  Oriented Workloads in Data Centers", "comments": "10 pages", "journal-ref": "Workshop on Large-Scale Parallel Processing in conjunction with\n  26th IEEE International Parallel and Distributed Processing Symposium, 2012,\n  Shanghai, China", "doi": "10.1109/IPDPSW.2012.213", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the first time, this paper systematically identifies three categories of\nthroughput oriented workloads in data centers: services, data processing\napplications, and interactive real-time applications, whose targets are to\nincrease the volume of throughput in terms of processed requests or data, or\nsupported maximum number of simultaneous subscribers, respectively, and we coin\na new term high volume computing (in short HVC) to describe those workloads and\ndata center computer systems designed for them. We characterize and compare HVC\nwith other computing paradigms, e.g., high throughput computing,\nwarehouse-scale computing, and cloud computing, in terms of levels, workloads,\nmetrics, coupling degree, data scales, and number of jobs or service instances.\nWe also preliminarily report our ongoing work on the metrics and benchmarks for\nHVC systems, which is the foundation of designing innovative data center\ncomputer systems for HVC workloads.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 06:37:31 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2013 07:54:43 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhan", "Jianfeng", ""], ["Zhang", "Lixin", ""], ["Sun", "Ninghui", ""], ["Wang", "Lei", ""], ["Jia", "Zhen", ""], ["Luo", "Chunjie", ""]]}, {"id": "1202.6163", "submitter": "Lawrence Murray", "authors": "Lawrence Murray", "title": "GPU acceleration of the particle filter: the Metropolis resampler", "comments": "Originally presented at Distributed Machine Learning and Sparse\n  Representation with Massive Data Sets (DMMD 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider deployment of the particle filter on modern massively parallel\nhardware architectures, such as Graphics Processing Units (GPUs), with a focus\non the resampling stage. While standard multinomial and stratified resamplers\nrequire a sum of importance weights computed collectively between threads, a\nMetropolis resampler favourably requires only pair-wise ratios between weights,\ncomputed independently by threads, and can be further tuned for performance by\nadjusting its number of iterations. While achieving respectable results for the\nstratified and multinomial resamplers, we demonstrate that a Metropolis\nresampler can be faster where the variance in importance weights is modest, and\nso is worth considering in a performance-critical context, such as particle\nMarkov chain Monte Carlo and real-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 10:13:50 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Murray", "Lawrence", ""]]}, {"id": "1202.6168", "submitter": "Dohy Hong", "authors": "Dohy Hong", "title": "D-iteration: Evaluation of the Asynchronous Distributed Computation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present a first evaluation of the potential of an\nasynchronous distributed computation associated to the recently proposed\napproach, D-iteration: the D-iteration is a fluid diffusion based iterative\nmethod, which has the advantage of being natively distributive. It exploits a\nsimple intuitive decomposition of the matrix-vector product as elementary\noperations of fluid diffusion associated to a new algebraic representation. We\nshow through experiments on real datasets how much this approach can improve\nthe computation efficiency when the parallelism is applied: with the proposed\nsolution, when the computation is distributed over $K$ virtual machines (PIDs),\nthe memory size to be handled by each virtual machine decreases linearly with\n$K$ and the computation speed increases almost linearly with $K$ with a slope\nbecoming closer to one when the number $N$ of linear equations to be solved\nincreases.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 10:27:46 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Hong", "Dohy", ""]]}, {"id": "1202.6291", "submitter": "Antonio Fern\\'andez Anta", "authors": "Jordi Arjona Aroca and Antonio Fern\\'andez Anta", "title": "Bisection (Band)Width of Product Networks with Application to Data\n  Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bisection width of interconnection networks has always been important in\nparallel computing, since it bounds the amount of information that can be moved\nfrom one side of a network to another, i.e., the bisection bandwidth. Finding\nits exact value has proven to be challenging for some network families. For\ninstance, the problem of finding the exact bisection width of the\nmultidimensional torus was posed by Leighton and has remained open for almost\n20 years. In this paper we provide the exact value of the bisection width of\nthe torus, as well as of several d-dimensional classical parallel topologies\nthat can be obtained by the application of the Cartesian product of graphs. To\ndo so, we first provide two general results that allow to obtain upper and\nlower bounds on the bisection width of a product graph as a function of some\nproperties of its factor graphs. We also apply these results to obtain bounds\nfor the bisection bandwidth of a d-dimensional BCube network, a recently\nproposed topology for data centers.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 17:19:56 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Aroca", "Jordi Arjona", ""], ["Anta", "Antonio Fern\u00e1ndez", ""]]}, {"id": "1202.6456", "submitter": "Maxwell Young", "authors": "Valerie King, Seth Pettie, Jared Saia, Maxwell Young", "title": "A Resource-Competitive Jamming Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a scenario where Alice wishes to send a message $m$ to Bob in a\ntime-slotted wireless network. However, there exists an adversary, Carol, who\naims to prevent the transmission of $m$ by jamming the communication channel.\nThere is a per-slot cost of $1$ to send, receive or jam $m$ on the channel, and\nwe are interested in how much Alice and Bob need to spend relative to Carol in\norder to guarantee communication.\n  Our approach is to design an algorithm in the framework of\nresource-competitive analysis where the cost to correct network devices (i.e.,\nAlice and Bob) is parameterized by the cost to faulty devices (i.e., Carol). We\npresent an algorithm that guarantees the successful transmission of $m$ and has\nthe following property: if Carol incurs a cost of $T$ to jam, then both Alice\nand Bob have a cost of $O(T^{\\varphi - 1} + 1)=O(T^{.62}+1)$ in expectation,\nwhere $\\varphi = (1+ \\sqrt{5})/2$ is the golden ratio. In other words, it\npossible for Alice and Bob to communicate while incurring asymptotically less\ncost than Carol. We generalize to the case where Alice wishes to send $m$ to\n$n$ receivers, and we achieve a similar result.\n  Our findings hold even if (1) $T$ is unknown to either party; (2) Carol knows\nthe algorithms of both parties, but not their random bits; (3) Carol can jam\nusing knowledge of past actions of both parties; and (4) Carol can jam\nreactively, so long as there is sufficient network traffic in addition to $m$.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 06:28:53 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 01:55:18 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["King", "Valerie", ""], ["Pettie", "Seth", ""], ["Saia", "Jared", ""], ["Young", "Maxwell", ""]]}, {"id": "1202.6513", "submitter": "Mariusz Slonina", "authors": "Mariusz Slonina, Krzysztof Gozdziewski, Cezary Migaszewski", "title": "Mechanic: a new numerical MPI framework for the dynamical astronomy", "comments": "4 pages, 7 figures, in GREAT-ESF Workshop 'Orbital Couples: \"Pas de\n  Deux\" in the Solar System and the Milky Way', Paris, IMCCE proceedings, in\n  press", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.EP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the Mechanic package, which is a new numerical framework for\ndynamical astronomy. The aim of our software is to help in massive numerical\nsimulations by efficient task management and unified data storage. The code is\nbuilt on top of the Message Passing Interface (MPI) and Hierarchical Data\nFormat (HDF5) standards and uses the Task Farm approach to manage numerical\ntasks. It relies on the core-module approach. The numerical problem implemented\nin the user-supplied module is separated from the host code (core). The core is\ndesigned to handle basic setup, data storage and communication between nodes in\na computing pool. It has been tested on large CPU-clusters, as well as desktop\ncomputers. The Mechanic may be used in computing dynamical maps, data\noptimization or numerical integration. The code and sample modules are freely\navailable at http://git.astri.umk.pl/projects/mechanic.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 11:11:45 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Slonina", "Mariusz", ""], ["Gozdziewski", "Krzysztof", ""], ["Migaszewski", "Cezary", ""]]}, {"id": "1202.6575", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff", "title": "Simplified, stable parallel merging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note makes an observation that significantly simplifies a number of\nprevious parallel, two-way merge algorithms based on binary search and\nsequential merge in parallel. First, it is shown that the additional merge step\nof distinguished elements as found in previous algorithms is not necessary,\nthus simplifying the implementation and reducing constant factors. Second, by\nfixating the requirements to the binary search, the merge algorithm becomes\nstable, provided that the sequential merge subroutine is stable. The stable,\nparallel merge algorithm can easily be used to implement a stable, parallel\nmerge sort.\n  For ordered sequences with $n$ and $m$ elements, $m\\leq n$, the simplified\nmerge algorithm runs in $O(n/p+\\log n)$ operations using $p$ processing\nelements. It can be implemented on an EREW PRAM, but since it requires only a\nsingle synchronization step, it is also a candidate for implementation on other\nparallel, shared-memory computers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 15:32:34 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2012 09:42:07 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2012 12:17:38 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""]]}]