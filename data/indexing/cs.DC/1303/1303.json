[{"id": "1303.0031", "submitter": "Anatoly Manita", "authors": "Anatoly Manita", "title": "Time Scales in Probabilistic Models of Wireless Sensor Networks", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC cs.MA math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic model of clock synchronization in a wireless network\nconsisting of N sensors interacting with one dedicated accurate time server.\nFor large N we find an estimate of the final time sychronization error for\nglobal and relative synchronization. Main results concern a behavior of the\nnetwork on different time scales $t=t_N \\to \\infty$, $N \\to \\infty$. We discuss\nexistence of phase transitions and find exact time scales on which an effective\nclock synchronization of the system takes place.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 21:53:55 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Manita", "Anatoly", ""]]}, {"id": "1303.0276", "submitter": "Wei-Lun Hung Wei-Lun Hung", "authors": "Wei-Lun Hung and Vijay K. Garg", "title": "AutoSynch: An Automatic-Signal Monitor Based on Predicate Tagging", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most programming languages use monitors with explicit signals for\nsynchronization in shared-memory programs. Requiring program- mers to signal\nthreads explicitly results in many concurrency bugs due to missed\nnotifications, or notifications on wrong condition variables. In this paper, we\ndescribe an implementation of an au- tomatic signaling monitor in Java called\nAutoSynch that eliminates such concurrency bugs by removing the burden of\nsignaling from the programmer. We show that the belief that automatic signaling\nmonitors are prohibitively expensive is wrong. For most problems, programs\nbased on AutoSynch are almost as fast as those based on explicit signaling. For\nsome, AutoSynch is even faster than explicit signaling because it never uses\nsignalAll, whereas the programmers end up using signalAll with the explicit\nsignal mechanism. AutoSynch achieves efficiency in synchronization based on\nthree novel ideas. We introduce an operation called globalization that enables\nthe predicate evaluation in every thread, thereby reducing context switches\nduring the execution of the program. Secondly, AutoSynch avoids signalAll by\nusing a property called relay invari- ance that guarantees that whenever\npossible there is always at least one thread whose condition is true which has\nbeen signaled. Finally, AutoSynch uses a technique called predicate tagging to\nefficiently determine a thread that should be signaled. To evaluate the effi-\nciency of AutoSynch, we have implemented many different well- known\nsynchronization problems such as the producers/consumers problem, the\nreaders/writers problems, and the dining philosophers problem. The results show\nthat AutoSynch is almost as efficient as the explicit-signal monitor and even\nmore efficient for some cases.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 20:49:44 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Hung", "Wei-Lun", ""], ["Garg", "Vijay K.", ""]]}, {"id": "1303.0351", "submitter": "George Abraham", "authors": "Ramesh Babu, George Abraham, Kiransinh Borasia", "title": "A Review On Securing Distributed Systems Using Symmetric Key\n  Cryptography", "comments": "7 pages, 7 figures, 1 table, Journal", "journal-ref": "International Journal of Advances in Science and Technology, Vol.\n  4, No.4, 2012", "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review is aimed to evaluate the importance of Symmetric Key Cryptography\nfor Security in Distributed Systems. Businesses around the world as well as\nresearch and other such areas rely heavily on distributed systems these days.\nHence, security is also a major concern due to the openness of the system. Out\nof the various available security measures, we, in this paper, concentrate in\ngeneral on the symmetric key cryptographic technique. We review two widely used\nand popular symmetric key cryptographic algorithms, viz. DES and AES. These two\nalgorithms are evaluated on the parameters such as key size, block size, number\nof iterations, etc.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 04:40:00 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Babu", "Ramesh", ""], ["Abraham", "George", ""], ["Borasia", "Kiransinh", ""]]}, {"id": "1303.0598", "submitter": "M.M.A. Hashem", "authors": "Kawser Wazed Nafi, Tonny Shekha Kar, Sayed Anisul Hoque and M. M. A.\n  Hashem", "title": "A Newer User Authentication, File encryption and Distributed Server\n  Based Cloud Computing Security Architecture", "comments": null, "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), Vol. 3, No. 10, pp. 181-186, [ISSN: 2156-5570] (2012)", "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cloud computing platform gives people the opportunity for sharing\nresources, services and information among the people of the whole world. In\nprivate cloud system, information is shared among the persons who are in that\ncloud. For this, security or personal information hiding process hampers. In\nthis paper we have proposed new security architecture for cloud computing\nplatform. This ensures secure communication system and hiding information from\nothers. AES based file encryption system and asynchronous key system for\nexchanging information or data is included in this model. This structure can be\neasily applied with main cloud computing features, e.g. PaaS, SaaS and IaaS.\nThis model also includes onetime password system for user authentication\nprocess. Our work mainly deals with the security system of the whole cloud\ncomputing platform.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 04:19:15 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Nafi", "Kawser Wazed", ""], ["Kar", "Tonny Shekha", ""], ["Hoque", "Sayed Anisul", ""], ["Hashem", "M. M. A.", ""]]}, {"id": "1303.1379", "submitter": "Mehmet Deveci", "authors": "Mehmet Deveci, Kamer Kaya, Bora Ucar, Umit V. Catalyurek", "title": "GPU accelerated maximum cardinality matching algorithms for bipartite\n  graphs", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design, implement, and evaluate GPU-based algorithms for the maximum\ncardinality matching problem in bipartite graphs. Such algorithms have a\nvariety of applications in computer science, scientific computing,\nbioinformatics, and other areas. To the best of our knowledge, ours is the\nfirst study which focuses on GPU implementation of the maximum cardinality\nmatching algorithms. We compare the proposed algorithms with serial and\nmulticore implementations from the literature on a large set of real-life\nproblems where in majority of the cases one of our GPU-accelerated algorithms\nis demonstrated to be faster than both the sequential and multicore\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 16:38:37 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Deveci", "Mehmet", ""], ["Kaya", "Kamer", ""], ["Ucar", "Bora", ""], ["Catalyurek", "Umit V.", ""]]}, {"id": "1303.1384", "submitter": "Silvia Crafa", "authors": "Silvia Crafa and Federica Russo", "title": "Causality in concurrent systems", "comments": "This is an interdisciplinary paper. It addresses a class of causal\n  models developed in computer science from an epistemic perspective, namely in\n  terms of philosophy of causality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent systems identify systems, either software, hardware or even\nbiological systems, that are characterized by sets of independent actions that\ncan be executed in any order or simultaneously. Computer scientists resort to a\ncausal terminology to describe and analyse the relations between the actions in\nthese systems. However, a thorough discussion about the meaning of causality in\nsuch a context has not been developed yet. This paper aims to fill the gap.\nFirst, the paper analyses the notion of causation in concurrent systems and\nattempts to build bridges with the existing philosophical literature,\nhighlighting similarities and divergences between them. Second, the paper\nanalyses the use of counterfactual reasoning in ex-post analysis in concurrent\nsystems (i.e. execution trace analysis).\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 16:50:06 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Crafa", "Silvia", ""], ["Russo", "Federica", ""]]}, {"id": "1303.1950", "submitter": "Alexandre Vaniachine", "authors": "A.V. Vaniachine (on behalf of the ATLAS and CMS Collaborations)", "title": "Advancements in Big Data Processing in the ATLAS and CMS Experiments", "comments": "7 pages, 7 figures", "journal-ref": "In: Proc. of the Fifth International Conference \"Distributed\n  computing and Grid-technologies in Science and Education\" (Dubna, July 16-21,\n  2012), Dubna, JINR, 2012, p. 224", "doi": null, "report-no": "ANL-HEP-CP-12-77; ATL-SOFT-PROC-2012-068", "categories": "cs.DC cs.DB hep-ex", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The ever-increasing volumes of scientific data present new challenges for\ndistributed computing and Grid technologies. The emerging Big Data revolution\ndrives exploration in scientific fields including nanotechnology, astrophysics,\nhigh-energy physics, biology and medicine. New initiatives are transforming\ndata-driven scientific fields enabling massive data analysis in new ways. In\npetascale data processing scientists deal with datasets, not individual files.\nAs a result, a task (comprised of many jobs) became a unit of petascale data\nprocessing on the Grid. Splitting of a large data processing task into jobs\nenabled fine-granularity checkpointing analogous to the splitting of a large\nfile into smaller TCP/IP packets during data transfers. Transferring large data\nin small packets achieves reliability through automatic re-sending of the\ndropped TCP/IP packets. Similarly, transient job failures on the Grid can be\nrecovered by automatic re-tries to achieve reliable six sigma production\nquality in petascale data processing on the Grid. The computing experience of\nthe ATLAS and CMS experiments provides foundation for reliability engineering\nscaling up Grid technologies for data processing beyond the petascale.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 11:06:44 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Vaniachine", "A. V.", "", "on behalf of the ATLAS and CMS Collaborations"]]}, {"id": "1303.2043", "submitter": "Bernadette Charron-Bost", "authors": "Bernadette Charron-Bost", "title": "Orientation and Connectivity Based Criteria for Asymptotic Consensus", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we establish orientation and connectivity based criteria for\nthe agreement algorithm to achieve asymptotic consensus in the context of\ntime-varying topology and communication delays. These criteria unify and extend\nmany earlier convergence results on the agreement algorithm for deterministic\nand discrete-time multiagent systems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 16:22:46 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Charron-Bost", "Bernadette", ""]]}, {"id": "1303.2171", "submitter": "Kishore Kothapalli", "authors": "Kishore Kothapalli, Dip Sankar Banerjee, P. J. Narayanan, Surinder\n  Sood, Aman Kumar Bahl, Shashank Sharma, Shrenik Lad, Krishna Kumar Singh,\n  Kiran Matam, Sivaramakrishna Bharadwaj, Rohit Nigam, Parikshit Sakurikar,\n  Aditya Deshpande, Ishan Misra, Siddharth Choudhary, Shubham Gupta", "title": "CPU and/or GPU: Revisiting the GPU Vs. CPU Myth", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel computing using accelerators has gained widespread research\nattention in the past few years. In particular, using GPUs for general purpose\ncomputing has brought forth several success stories with respect to time taken,\ncost, power, and other metrics. However, accelerator based computing has\nsignifi- cantly relegated the role of CPUs in computation. As CPUs evolve and\nalso offer matching computational resources, it is important to also include\nCPUs in the computation. We call this the hybrid computing model. Indeed, most\ncomputer systems of the present age offer a degree of heterogeneity and\ntherefore such a model is quite natural.\n  We reevaluate the claim of a recent paper by Lee et al.(ISCA 2010). We argue\nthat the right question arising out of Lee et al. (ISCA 2010) should be how to\nuse a CPU+GPU platform efficiently, instead of whether one should use a CPU or\na GPU exclusively. To this end, we experiment with a set of 13 diverse\nworkloads ranging from databases, image processing, sparse matrix kernels, and\ngraphs. We experiment with two different hybrid platforms: one consisting of a\n6-core Intel i7-980X CPU and an NVidia Tesla T10 GPU, and another consisting of\nan Intel E7400 dual core CPU with an NVidia GT520 GPU. On both these platforms,\nwe show that hybrid solutions offer good advantage over CPU or GPU alone\nsolutions. On both these platforms, we also show that our solutions are 90%\nresource efficient on average.\n  Our work therefore suggests that hybrid computing can offer tremendous\nadvantages at not only research-scale platforms but also the more realistic\nscale systems with significant performance gains and resource efficiency to the\nlarge scale user community.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 05:35:31 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Kothapalli", "Kishore", ""], ["Banerjee", "Dip Sankar", ""], ["Narayanan", "P. J.", ""], ["Sood", "Surinder", ""], ["Bahl", "Aman Kumar", ""], ["Sharma", "Shashank", ""], ["Lad", "Shrenik", ""], ["Singh", "Krishna Kumar", ""], ["Matam", "Kiran", ""], ["Bharadwaj", "Sivaramakrishna", ""], ["Nigam", "Rohit", ""], ["Sakurikar", "Parikshit", ""], ["Deshpande", "Aditya", ""], ["Misra", "Ishan", ""], ["Choudhary", "Siddharth", ""], ["Gupta", "Shubham", ""]]}, {"id": "1303.2289", "submitter": "Alexander Olshevsky", "authors": "Angelia Nedic, Alex Olshevsky", "title": "Distributed optimization over time-varying directed graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed optimization by a collection of nodes, each having\naccess to its own convex function, whose collective goal is to minimize the sum\nof the functions. The communications between nodes are described by a\ntime-varying sequence of directed graphs, which is uniformly strongly\nconnected. For such communications, assuming that every node knows its\nout-degree, we develop a broadcast-based algorithm, termed the\nsubgradient-push, which steers every node to an optimal value under a standard\nassumption of subgradient boundedness. The subgradient-push requires no\nknowledge of either the number of agents or the graph sequence to implement.\nOur analysis shows that the subgradient-push algorithm converges at a rate of\n$O(\\ln(t)/\\sqrt{t})$, where the constant depends on the initial values at the\nnodes, the subgradient norms, and, more interestingly, on both the consensus\nspeed and the imbalances of influence among the nodes.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 05:29:49 GMT"}, {"version": "v2", "created": "Sun, 16 Mar 2014 01:53:49 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Nedic", "Angelia", ""], ["Olshevsky", "Alex", ""]]}, {"id": "1303.2619", "submitter": "Russell Power", "authors": "Russell Power", "title": "Making Systems More Robust with Flexible RPC Lookup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern distributed systems use names everywhere. Lockservices such as Chubby\nand ZooKeeper provide an effective mechanism for mapping from application names\nto server instances, but proper usage of them requires a large amount of\nerror-prone boiler-plate code.\n  Application programmers often try to write wrappers to abstract away this\nlogic, but it turns out there is a more general and easier way of handling the\nissue. We show that by extending the existing name resolution capabilities of\nRPC libraries, we can remove the need for such annoying boiler-plate code while\nat the same time making our services more robust.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 19:05:37 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Power", "Russell", ""]]}, {"id": "1303.2837", "submitter": "Franck Iutzeler", "authors": "Franck Iutzeler, Pascal Bianchi, Philippe Ciblat and Walid Hachem", "title": "Asynchronous Distributed Optimization using a Randomized Alternating\n  Direction Method of Multipliers", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set of networked agents endowed with private cost functions and\nseeking to find a consensus on the minimizer of the aggregate cost. A new class\nof random asynchronous distributed optimization methods is introduced. The\nmethods generalize the standard Alternating Direction Method of Multipliers\n(ADMM) to an asynchronous setting where isolated components of the network are\nactivated in an uncoordinated fashion. The algorithms rely on the introduction\nof randomized Gauss-Seidel iterations of a Douglas-Rachford operator for\nfinding zeros of a sum of two monotone operators. Convergence to the sought\nminimizers is provided under mild connectivity conditions. Numerical results\nsustain our claims.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 11:04:39 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Iutzeler", "Franck", ""], ["Bianchi", "Pascal", ""], ["Ciblat", "Philippe", ""], ["Hachem", "Walid", ""]]}, {"id": "1303.2857", "submitter": "Jebbar Mostafa", "authors": "Mostafa Jebbar and Abderrahim Sekkaki and Othman Benammar", "title": "Integration of SOA and Cloud Computing in RM-ODP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of ODP is according to ITU-T Recommendation X.901 stated as\nfollows: The objective of ODP standardization is the development of standards\nthat allow the benefits of distributing information processing services to be\nrealized in an environment of heterogeneous IT resources and multiple\norganizational domains. These standards address constraints on system\nspecification and the provision of a system infrastructure that accommodate\ndifficulties inherent in the design and programming of distributed systems.\nThis objective seems to cover cloud computing systems. Therefore, we in this\npaper discuss the concepts of cloud, and discuss the use of RM-ODP for\nspecifying the solution. We indicate that the current RM-ODP may be too\nabstract for the purpose, and indicate how to adapt RM-ODP to fit the purpose\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 12:22:27 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Jebbar", "Mostafa", ""], ["Sekkaki", "Abderrahim", ""], ["Benammar", "Othman", ""]]}, {"id": "1303.3517", "submitter": "Yingyi Bu Yingyi Bu", "authors": "Joshua Rosen, Neoklis Polyzotis, Vinayak Borkar, Yingyi Bu, Michael J.\n  Carey, Markus Weimer, Tyson Condie, Raghu Ramakrishnan", "title": "Iterative MapReduce for Large Scale Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets (\"Big Data\") are becoming ubiquitous because the potential\nvalue in deriving insights from data, across a wide range of business and\nscientific applications, is increasingly recognized. In particular, machine\nlearning - one of the foundational disciplines for data analysis, summarization\nand inference - on Big Data has become routine at most organizations that\noperate large clouds, usually based on systems such as Hadoop that support the\nMapReduce programming paradigm. It is now widely recognized that while\nMapReduce is highly scalable, it suffers from a critical weakness for machine\nlearning: it does not support iteration. Consequently, one has to program\naround this limitation, leading to fragile, inefficient code. Further, reliance\non the programmer is inherently flawed in a multi-tenanted cloud environment,\nsince the programmer does not have visibility into the state of the system when\nhis or her program executes. Prior work has sought to address this problem by\neither developing specialized systems aimed at stylized applications, or by\naugmenting MapReduce with ad hoc support for saving state across iterations\n(driven by an external loop). In this paper, we advocate support for looping as\na first-class construct, and propose an extension of the MapReduce programming\nparadigm called {\\em Iterative MapReduce}. We then develop an optimizer for a\nclass of Iterative MapReduce programs that cover most machine learning\ntechniques, provide theoretical justifications for the key optimization steps,\nand empirically demonstrate that system-optimized programs for significant\nmachine learning tasks are competitive with state-of-the-art specialized\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 04:24:12 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Rosen", "Joshua", ""], ["Polyzotis", "Neoklis", ""], ["Borkar", "Vinayak", ""], ["Bu", "Yingyi", ""], ["Carey", "Michael J.", ""], ["Weimer", "Markus", ""], ["Condie", "Tyson", ""], ["Ramakrishnan", "Raghu", ""]]}, {"id": "1303.3626", "submitter": "Niloufar Shafiei", "authors": "Niloufar Shafiei", "title": "Non-blocking Patricia Tries with Replace Operations", "comments": "To appear in the 33rd IEEE International Conference on Distributed\n  Computing Systems (ICDCS 2013)", "journal-ref": null, "doi": "10.1109/ICDCS.2013.43", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a non-blocking Patricia trie implementation for an\nasynchronous shared-memory system using Compare&Swap. The trie implements a\nlinearizable set and supports three update operations: insert adds an element,\ndelete removes an element and replace replaces one element by another. The\nreplace operation is interesting because it changes two different locations of\ntree atomically. If all update operations modify different parts of the trie,\nthey run completely concurrently. The implementation also supports a wait-free\nfind operation, which only reads shared memory and never changes the data\nstructure. Empirically, we compare our algorithms to some existing set\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 22:14:36 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Shafiei", "Niloufar", ""]]}, {"id": "1303.3632", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Javid Taheri, Reza Moraveji, Albert Y. Zomaya", "title": "Statistical Regression to Predict Total Cumulative CPU Usage of\n  MapReduce Jobs", "comments": "16 pages- previously published as \"On Modelling and Prediction of\n  Total CPU Usage for Applications in MapReduce Enviornments\" in IEEE 12th\n  International Conference on Algorithms and Architectures for Parallel\n  Processing (ICA3PP-12), Fukuoka, Japan, 4-7 September, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, businesses have started using MapReduce as a popular computation\nframework for processing large amount of data, such as spam detection, and\ndifferent data mining tasks, in both public and private clouds. Two of the\nchallenging questions in such environments are (1) choosing suitable values for\nMapReduce configuration parameters e.g., number of mappers, number of reducers,\nand DFS block size, and (2) predicting the amount of resources that a user\nshould lease from the service provider. Currently, the tasks of both choosing\nconfiguration parameters and estimating required resources are solely the users\nresponsibilities. In this paper, we present an approach to provision the total\nCPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce\njob, a profile of total CPU usage in clock cycles is built from the job past\nexecutions with different values of two configuration parameters e.g., number\nof mappers, and number of reducers. Then, a polynomial regression is used to\nmodel the relation between these configuration parameters and total CPU usage\nin clock cycles of the job. We also briefly study the influence of input data\nscaling on measured total CPU usage in clock cycles. This derived model along\nwith the scaling result can then be used to provision the total CPU usage in\nclock cycles of the same jobs with different input data size. We validate the\naccuracy of our models using three realistic applications (WordCount, Exim\nMainLog parsing, and TeraSort). Results show that the predicted total CPU usage\nin clock cycles of generated resource provisioning options are less than 8% of\nthe measured total CPU usage in clock cycles in our 20-node virtual Hadoop\ncluster.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 22:40:32 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Taheri", "Javid", ""], ["Moraveji", "Reza", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1303.3692", "submitter": "Gang Liao", "authors": "Gang Liao, Qi Sun, Longfei Ma, Sha Ding and Wen Xie", "title": "Ultra-fast Multiple Genome Sequence Matching Using GPU", "comments": "The 2013 International Conference on High Performance Computing &\n  Simulation (ACM/IEEE HPCS 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a contrastive evaluation of massively parallel implementations\nof suffix tree and suffix array to accelerate genome sequence matching are\nproposed based on Intel Core i7 3770K quad-core and NVIDIA GeForce GTX680 GPU.\nBesides suffix array only held approximately 20%~30% of the space relative to\nsuffix tree, the coalesced binary search and tile optimization make suffix\narray clearly outperform suffix tree using GPU. Consequently, the experimental\nresults show that multiple genome sequence matching based on suffix array is\nmore than 99 times speedup than that of CPU serial implementation. There is no\ndoubt that massively parallel matching algorithm based on suffix array is an\nefficient approach to high-performance bioinformatics applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 07:00:14 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 03:38:17 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 19:52:18 GMT"}, {"version": "v4", "created": "Sun, 3 May 2015 20:03:29 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Liao", "Gang", ""], ["Sun", "Qi", ""], ["Ma", "Longfei", ""], ["Ding", "Sha", ""], ["Xie", "Wen", ""]]}, {"id": "1303.4153", "submitter": "Xiao Li", "authors": "Xiao Li and Anna Scaglione", "title": "Robust Decentralized State Estimation and Tracking for Power Systems via\n  Network Gossiping", "comments": "to appear in IEEE JSAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fully decentralized adaptive re-weighted state\nestimation (DARSE) scheme for power systems via network gossiping. The enabling\ntechnique is the proposed Gossip-based Gauss-Newton (GGN) algorithm, which\nallows to harness the computation capability of each area (i.e. a database\nserver that accrues data from local sensors) to collaboratively solve for an\naccurate global state. The DARSE scheme mitigates the influence of bad data by\nupdating their error variances online and re-weighting their contributions\nadaptively for state estimation. Thus, the global state can be estimated and\ntracked robustly using near-neighbor communications in each area. Compared to\nother distributed state estimation techniques, our communication model is\nflexible with respect to reconfigurations and resilient to random failures as\nlong as the communication network is connected. Furthermore, we prove that the\nJacobian of the power flow equations satisfies the Lipschitz condition that is\nessential for the GGN algorithm to converge to the desired solution.\nSimulations of the IEEE-118 system show that the DARSE scheme can estimate and\ntrack online the global power system state accurately, and degrades gracefully\nwhen there are random failures and bad data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 04:18:09 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 19:03:24 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Li", "Xiao", ""], ["Scaglione", "Anna", ""]]}, {"id": "1303.4191", "submitter": "Sadagopan Narasimhan", "authors": "Surabhi Jain and N.Sadagopan", "title": "Parallel Search with Extended Fibonacci Primitive", "comments": "7 pages, 5 figures This paper has been withdrawn by the author due to\n  an error in the simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search pattern experienced by the processor to search an element in secondary\nstorage devices follows a random sequence. Formally, it is a random walk and\nits modeling is crucial in studying performance metrics like memory access\ntime. In this paper, we first model the random walk using extended Fibonacci\nseries. Our simulation is done on a parallel computing model (PRAM) with EREW\nstrategy. Three search primitives are proposed under parallel computing model\nand each primitive is thoroughly tested on an array of size $10^7$ with the\nsize of random walk being $10^4$. Our findings reveal that search primitive\nwith pointer jumping is better than the other two primitives. Our key\ncontribution lies in modeling random walk as an extended Fibonacci series\ngenerator and simulating the same with various search primitives.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 09:03:56 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 07:52:19 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2013 07:09:14 GMT"}, {"version": "v4", "created": "Sat, 6 Jul 2013 12:19:44 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Jain", "Surabhi", ""], ["Sadagopan", "N.", ""]]}, {"id": "1303.4312", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Christian Siebert and Jesper Larsson Tr\\\"aff", "title": "Perfectly load-balanced, optimal, stable, parallel merge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, work-optimal and synchronization-free solution to the\nproblem of stably merging in parallel two given, ordered arrays of m and n\nelements into an ordered array of m+n elements. The main contribution is a new,\nsimple, fast and direct algorithm that determines, for any prefix of the stably\nmerged output sequence, the exact prefixes of each of the two input sequences\nneeded to produce this output prefix. More precisely, for any given index\n(rank) in the resulting, but not yet constructed output array representing an\noutput prefix, the algorithm computes the indices (co-ranks) in each of the two\ninput arrays representing the required input prefixes without having to merge\nthe input arrays. The co-ranking algorithm takes O(log min(m,n)) time steps.\nThe algorithm is used to devise a perfectly load-balanced, stable, parallel\nmerge algorithm where each of p processing elements has exactly the same number\nof input elements to merge. Compared to other approaches to the parallel merge\nproblem, our algorithm is considerably simpler and can be faster up to a factor\nof two. Compared to previous algorithms for solving the co-ranking problem, the\nalgorithm given here is direct and maintains stability in the presence of\nrepeated elements at no extra space or time cost. When the number of processing\nelements p does not exceed (m+n)/log min(m,n), the parallel merge algorithm has\noptimal speedup. It is easy to implement on both shared and distributed memory\nparallel systems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 16:40:58 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 16:58:16 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Siebert", "Christian", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1303.4538", "submitter": "Georg Hager", "authors": "Christoph Scheit, Georg Hager, Jan Treibig, Stefan Becker, Gerhard\n  Wellein", "title": "Optimization of FASTEST-3D for Modern Multicore Systems", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FASTEST-3D is an MPI-parallel finite-volume flow solver based on\nblock-structured meshes that has been developed at the University of\nErlangen-Nuremberg since the early 1990s. It can be used to solve the laminar\nor turbulent incompressible Navier-Stokes equations. Up to now its scalability\nwas strongly limited by a rather rigid communication infrastructure, which led\nto a dominance of MPI time already at small process counts.\n  This paper describes several optimizations to increase the performance,\nscalability, and flexibility of FASTEST-3D. First, a node-level performance\nanalysis is carried out in order to pinpoint the main bottlenecks and identify\nsweet spots for energy-efficient execution. In addition, a single-precision\nversion of the solver for the linear equation system arising from the\ndiscretization of the governing equations is devised, which significantly\nincreases the single-core performance. Then the communication mechanisms in\nFASTEST-3D are analyzed and a new communication strategy based on non-blocking\ncalls is implemented. Performance results with the revised version show\nsignificantly increased single-node performance and considerably improved\ncommunication patterns along with much better parallel scalability. In this\ncontext we discuss the concept of \"acceptable parallel efficiency\" and how it\ninfluences the real gain of the optimizations. Scaling measurements are carried\nout on a modern petascale system. The obtained improvements are of major\nimportance for the use of FASTEST-3D on current high-performance computer\nclusters and will help to perform simulations with much higher spatial and\ntemporal resolution to tackle turbulent flow in technical applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 10:18:40 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Scheit", "Christoph", ""], ["Hager", "Georg", ""], ["Treibig", "Jan", ""], ["Becker", "Stefan", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1303.5164", "submitter": "Jianlong Zhong", "authors": "Jianlong Zhong, Bingsheng He", "title": "Kernelet: High-Throughput GPU Kernel Executions with Dynamic Slicing and\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics processors, or GPUs, have recently been widely used as accelerators\nin the shared environments such as clusters and clouds. In such shared\nenvironments, many kernels are submitted to GPUs from different users, and\nthroughput is an important metric for performance and total ownership cost.\nDespite the recently improved runtime support for concurrent GPU kernel\nexecutions, the GPU can be severely underutilized, resulting in suboptimal\nthroughput. In this paper, we propose Kernelet, a runtime system with dynamic\nslicing and scheduling techniques to improve the throughput of concurrent\nkernel executions on the GPU. With slicing, Kernelet divides a GPU kernel into\nmultiple sub-kernels (namely slices). Each slice has tunable occupancy to allow\nco-scheduling with other slices and to fully utilize the GPU resources. We\ndevelop a novel and effective Markov chain based performance model to guide the\nscheduling decision. Our experimental results demonstrate up to 31.1% and 23.4%\nperformance improvement on NVIDIA Tesla C2050 and GTX680 GPUs, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 04:50:48 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Zhong", "Jianlong", ""], ["He", "Bingsheng", ""]]}, {"id": "1303.5234", "submitter": "Piotr Dendek", "authors": "Piotr Jan Dendek and Artur Czeczko and Mateusz Fedoryszak and Adam\n  Kawa and Piotr Wendykier and Lukasz Bolikowski", "title": "How to perform research in Hadoop environment not losing mental\n  equilibrium - case study", "comments": "This paper (with changed content) appeared under the title \"Chrum:\n  The Tool for Convenient Generation of Apache Oozie Workflows\" in \"Intelligent\n  Tools for Building a Scientific Information Platform: From Research to\n  Implementation\", \"Studies in Computational Intelligence\", Volume 541, 2014,\n  http://link.springer.com/book/10.1007/978-3-319-04714-0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting a research in an efficient, repetitive, evaluable, but also\nconvenient (in terms of development) way has always been a challenge. To\nsatisfy those requirements in a long term and simultaneously minimize costs of\nthe software engineering process, one has to follow a certain set of\nguidelines. This article describes such guidelines based on the research\nenvironment called Content Analysis System (CoAnSys) created in the Center for\nOpen Science (CeON). Best practices and tools for working in the Apache Hadoop\nenvironment, as well as the process of establishing these rules are portrayed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 11:53:37 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 10:17:14 GMT"}, {"version": "v3", "created": "Sun, 16 Mar 2014 22:30:10 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Dendek", "Piotr Jan", ""], ["Czeczko", "Artur", ""], ["Fedoryszak", "Mateusz", ""], ["Kawa", "Adam", ""], ["Wendykier", "Piotr", ""], ["Bolikowski", "Lukasz", ""]]}, {"id": "1303.5275", "submitter": "Gerard Gorman", "authors": "Michael Lange, Gerard Gorman, Michele Weiland, Lawrence Mitchell and\n  James Southern", "title": "Achieving Efficient Strong Scaling with PETSc using Hybrid MPI/OpenMP\n  Optimisation", "comments": null, "journal-ref": "Lecture Notes in Computer Science 7905:97-108 (2013)", "doi": "10.1007/978-3-642-38750-0_8", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing number of processing elements and decreas- ing memory to core\nratio in modern high-performance platforms makes efficient strong scaling a key\nrequirement for numerical algorithms. In order to achieve efficient scalability\non massively parallel systems scientific software must evolve across the entire\nstack to exploit the multiple levels of parallelism exposed in modern\narchitectures. In this paper we demonstrate the use of hybrid MPI/OpenMP\nparallelisation to optimise parallel sparse matrix-vector multiplication in\nPETSc, a widely used scientific library for the scalable solution of partial\ndifferential equations. Using large matrices generated by Fluidity, an open\nsource CFD application code which uses PETSc as its linear solver engine, we\nevaluate the effect of explicit communication overlap using task-based\nparallelism and show how to further improve performance by explicitly load\nbalancing threads within MPI processes. We demonstrate a significant speedup\nover the pure-MPI mode and efficient strong scaling of sparse matrix-vector\nmultiplication on Fujitsu PRIMEHPC FX10 and Cray XE6 systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 14:56:02 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Lange", "Michael", ""], ["Gorman", "Gerard", ""], ["Weiland", "Michele", ""], ["Mitchell", "Lawrence", ""], ["Southern", "James", ""]]}, {"id": "1303.5800", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Eliana-Dina Tirsa", "title": "Line-Constrained Geometric Server Placement", "comments": null, "journal-ref": "Metalurgia International, vol. 16, no. 11, pp. 106-110, 2011.\n  (ISSN: 1582-2214)", "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present new algorithmic solutions for several constrained\ngeometric server placement problems. We consider the problems of computing the\n1-center and obnoxious 1-center of a set of line segments, constrained to lie\non a line segment, and the problem of computing the K-median of a set of\npoints, constrained to lie on a line. The presented algorithms have\napplications in many types of distributed systems, as well as in various fields\nwhich make use of distributed systems for running some of their applications\n(like chemistry, metallurgy, physics, etc.).\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 00:29:24 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tirsa", "Eliana-Dina", ""]]}, {"id": "1303.5837", "submitter": "Mathias Jacquelin", "authors": "Laura Grigori (INRIA Paris-Rocquencourt), Mathias Jacquelin (INRIA\n  Paris-Rocquencourt), Amal Khabou (INRIA Paris-Rocquencourt)", "title": "Multilevel communication optimal LU and QR factorizations for\n  hierarchical platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-8270", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study focuses on the performance of two classical dense linear algebra\nalgorithms, the LU and the QR factorizations, on multilevel hierarchical\nplatforms. We first introduce a new model called Hierarchical Cluster Platform\n(HCP), encapsulating the characteristics of such platforms. The focus is set on\nreducing the communication requirements of studied algorithms at each level of\nthe hierarchy. Lower bounds on communications are therefore extended with\nrespect to the HCP model. We then introduce multilevel LU and QR algorithms\ntailored for those platforms, and provide a detailed performance analysis. We\nalso provide a set of numerical experiments and performance predictions\ndemonstrating the need for such algorithms on large platforms.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 11:40:50 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Grigori", "Laura", "", "INRIA Paris-Rocquencourt"], ["Jacquelin", "Mathias", "", "INRIA\n  Paris-Rocquencourt"], ["Khabou", "Amal", "", "INRIA Paris-Rocquencourt"]]}, {"id": "1303.5891", "submitter": "Bharath Balasubramanian", "authors": "Bharath Balasubramanian and Vijay K. Garg", "title": "Fault Tolerance in Distributed Systems using Fused State Machines", "comments": "This is under review with the Distributed Computing journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication is a standard technique for fault tolerance in distributed\nsystems modeled as deterministic finite state machines (DFSMs or machines). To\ncorrect f crash or f/2 Byzantine faults among n different machines, replication\nrequires nf additional backup machines. We present a solution called fusion\nthat requires just f additional backup machines. First, we build a framework\nfor fault tolerance in DFSMs based on the notion of Hamming distances. We\nintroduce the concept of an (f,m)-fusion, which is a set of m backup machines\nthat can correct f crash faults or f/2 Byzantine faults among a given set of\nmachines. Second, we present an algorithm to generate an (f,f)-fusion for a\ngiven set of machines. We ensure that our backups are efficient in terms of the\nsize of their state and event sets. Our evaluation of fusion on the widely used\nMCNC'91 benchmarks for DFSMs show that the average state space savings in\nfusion (over replication) is 38% (range 0-99%). To demonstrate the practical\nuse of fusion, we describe its potential application to the MapReduce\nframework. Using a simple case study, we compare replication and fusion as\napplied to this framework. While a pure replication-based solution requires 1.8\nmillion map tasks, our fusion-based solution requires only 1.4 million map\ntasks with minimal overhead during normal operation or recovery. Hence, fusion\nresults in considerable savings in state space and other resources such as the\npower needed to run the backup tasks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 23:03:34 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Balasubramanian", "Bharath", ""], ["Garg", "Vijay K.", ""]]}, {"id": "1303.5907", "submitter": "Dmitry Zinoviev", "authors": "Dmitry Zinoviev and Hamid Benbrahim and Greta Meszoely and Dan\n  Stefanescu", "title": "Simulating Resilience in Transaction-Oriented Networks", "comments": "5 pages, 5 figures. Accepted to HPC-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power of networks manifests itself in a highly non-linear amplification\nof a number of effects, and their weakness - in propagation of cascading\nfailures. The potential systemic risk effects can be either exacerbated or\nmitigated, depending on the resilience characteristics of the network. The\ngoals of this paper are to study some characteristics of network amplification\nand resilience. We simulate random Erdos-Renyi networks and measure\namplification by varying node capacity, transaction volume, and expected\nfailure rates. We discover that network throughput scales almost quadratically\nwith respect to the node capacity and that the effects of excessive network\nload and random and irreparable node faults are equivalent and almost perfectly\nanticorrelated. This knowledge can be used by capacity planners to determine\noptimal reliability requirements that maximize the optimal operational regions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 02:37:42 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Zinoviev", "Dmitry", ""], ["Benbrahim", "Hamid", ""], ["Meszoely", "Greta", ""], ["Stefanescu", "Dan", ""]]}, {"id": "1303.6092", "submitter": "Mathias Buerger", "authors": "Mathias B\\\"urger and Giuseppe Notarstefano and Frank Allg\\\"ower", "title": "A Polyhedral Approximation Framework for Convex and Robust Distributed\n  Optimization", "comments": "submitted to IEEE Transactions on Automatic Control", "journal-ref": null, "doi": "10.1109/TAC.2013.2281883", "report-no": null, "categories": "cs.SY cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a general problem set-up for a wide class of convex\nand robust distributed optimization problems in peer-to-peer networks. In this\nset-up convex constraint sets are distributed to the network processors who\nhave to compute the optimizer of a linear cost function subject to the\nconstraints. We propose a novel fully distributed algorithm, named\ncutting-plane consensus, to solve the problem, based on an outer polyhedral\napproximation of the constraint sets. Processors running the algorithm compute\nand exchange linear approximations of their locally feasible sets.\nIndependently of the number of processors in the network, each processor stores\nonly a small number of linear constraints, making the algorithm scalable to\nlarge networks. The cutting-plane consensus algorithm is presented and analyzed\nfor the general framework. Specifically, we prove that all processors running\nthe algorithm agree on an optimizer of the global problem, and that the\nalgorithm is tolerant to node and link failures as long as network connectivity\nis preserved. Then, the cutting plane consensus algorithm is specified to three\ndifferent classes of distributed optimization problems, namely (i) inequality\nconstrained problems, (ii) robust optimization problems, and (iii) almost\nseparable optimization problems with separable objective functions and coupling\nconstraints. For each one of these problem classes we solve a concrete problem\nthat can be expressed in that framework and present computational results. That\nis, we show how to solve: position estimation in wireless sensor networks, a\ndistributed robust linear program and, a distributed microgrid control problem.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 11:25:57 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["B\u00fcrger", "Mathias", ""], ["Notarstefano", "Giuseppe", ""], ["Allg\u00f6wer", "Frank", ""]]}, {"id": "1303.6609", "submitter": "Jagan Sankaranarayanan", "authors": "Jeff LeFevre, Jagan Sankaranarayanan, Hakan Hacigumus, Junichi\n  Tatemura, Neoklis Polyzotis, Michael J. Carey", "title": "Exploiting Opportunistic Physical Design in Large-scale Data Analytics", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Large-scale systems, such as MapReduce and Hadoop, perform aggressive\nmaterialization of intermediate job results in order to support fault\ntolerance. When jobs correspond to exploratory queries submitted by data\nanalysts, these materializations yield a large set of materialized views that\ntypically capture common computation among successive queries from the same\nanalyst, or even across queries of different analysts who test similar\nhypotheses. We propose to treat these views as an opportunistic physical design\nand use them for the purpose of query optimization. We develop a novel\nquery-rewrite algorithm that addresses the two main challenges in this context:\nhow to search the large space of rewrites, and how to reason about views that\ncontain UDFs (a common feature in large-scale data analytics). The algorithm,\nwhich provably finds the minimum-cost rewrite, is inspired by nearest-neighbor\nsearches in non-metric spaces. We present an extensive experimental study on\nreal-world datasets with a prototype data-analytics system based on Hive. The\nresults demonstrate that our approach can result in dramatic performance\nimprovements on complex data-analysis queries, reducing total execution time by\nan average of 61% and up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 19:08:55 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 17:35:09 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["LeFevre", "Jeff", ""], ["Sankaranarayanan", "Jagan", ""], ["Hacigumus", "Hakan", ""], ["Tatemura", "Junichi", ""], ["Polyzotis", "Neoklis", ""], ["Carey", "Michael J.", ""]]}, {"id": "1303.6753", "submitter": "Stefan Schmid", "authors": "Ernesto Abarca, Johannes Grassler, Gregor Schaffrath, Stefan Schmid", "title": "A Federated CloudNet Architecture: The PIP and the VNP Role", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic and flexible architecture to realize CloudNets: virtual\nnetworks connecting cloud resources with resource guarantees. Our architecture\nis federated and supports different (and maybe even competing) economical\nroles, by providing explicit negotiation and provisioning interfaces.\nContract-based interactions and a resource description language that allows for\naggregation and abstraction, preserve the different roles' autonomy without\nsacrificing flexibility. Moreover, since our CloudNet architecture is plugin\nbased, essentially all cloud operating systems (e.g., OpenStack) or link\ntechnologies (e.g., VLANs, OpenFlow, VPLS) can be used within the framework.\n  This paper describes two roles in more detail: The Physical Infrastructure\nProviders (PIP) which own the substrate network and resources, and the Virtual\nNetwork Providers (VNP) which can act as resource and CloudNet brokers and\nresellers. Both roles are fully implemented in our wide-area prototype that\nspans remote sites and resources.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 07:07:53 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Abarca", "Ernesto", ""], ["Grassler", "Johannes", ""], ["Schaffrath", "Gregor", ""], ["Schmid", "Stefan", ""]]}, {"id": "1303.7032", "submitter": "Zhe Yao", "authors": "Zhe Yao, Vincent Gripon and Michael G. Rabbat", "title": "A Massively Parallel Associative Memory Based on Sparse Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associative memories store content in such a way that the content can be\nlater retrieved by presenting the memory with a small portion of the content,\nrather than presenting the memory with an address as in more traditional\nmemories. Associative memories are used as building blocks for algorithms\nwithin database engines, anomaly detection systems, compression algorithms, and\nface recognition systems. A classical example of an associative memory is the\nHopfield neural network. Recently, Gripon and Berrou have introduced an\nalternative construction which builds on ideas from the theory of error\ncorrecting codes and which greatly outperforms the Hopfield network in\ncapacity, diversity, and efficiency. In this paper we implement a variation of\nthe Gripon-Berrou associative memory on a general purpose graphical processing\nunit (GPU). The work of Gripon and Berrou proposes two retrieval rules,\nsum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector\nmultiplication and is easily implemented on the GPU. The sum-of-max rule is\nmuch less straightforward to implement because it involves non-linear\noperations. However, the sum-of-max rule gives significantly better retrieval\nerror rates. We propose a hybrid rule tailored for implementation on a GPU\nwhich achieves a 880-fold speedup without sacrificing any accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 03:49:57 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2013 14:29:21 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Yao", "Zhe", ""], ["Gripon", "Vincent", ""], ["Rabbat", "Michael G.", ""]]}, {"id": "1303.7103", "submitter": "Federico Penna", "authors": "Federico Penna, Slawomir Stanczak", "title": "Decentralized Eigenvalue Algorithms for Distributed Signal Detection in\n  Cognitive Networks", "comments": "Submitted to IEEE JSAC Cognitive Radio Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive and analyze two algorithms -- referred to as\ndecentralized power method (DPM) and decentralized Lanczos algorithm (DLA) --\nfor distributed computation of one (the largest) or multiple eigenvalues of a\nsample covariance matrix over a wireless network. The proposed algorithms,\nbased on sequential average consensus steps for computations of matrix-vector\nproducts and inner vector products, are first shown to be equivalent to their\ncentralized counterparts in the case of exact distributed consensus. Then,\nclosed-form expressions of the error introduced by non-ideal consensus are\nderived for both algorithms. The error of the DPM is shown to vanish\nasymptotically under given conditions on the sequence of consensus errors.\nFinally, we consider applications to spectrum sensing in cognitive radio\nnetworks, and we show that virtually all eigenvalue-based tests proposed in the\nliterature can be implemented in a distributed setting using either the DPM or\nthe DLA. Simulation results are presented that validate the effectiveness of\nthe proposed algorithms in conditions of practical interest (large-scale\nnetworks, small number of samples, and limited number of iterations).\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 11:42:28 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Penna", "Federico", ""], ["Stanczak", "Slawomir", ""]]}, {"id": "1303.7195", "submitter": "Marc Bux", "authors": "Marc Bux and Ulf Leser", "title": "Parallelization in Scientific Workflow Management Systems", "comments": "24 pages, 17 figures (13 PDF, 4 PNG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, scientific workflow management systems (SWfMS)\nhave emerged as a means to facilitate the design, execution, and monitoring of\nreusable scientific data processing pipelines. At the same time, the amounts of\ndata generated in various areas of science outpaced enhancements in\ncomputational power and storage capabilities. This is especially true for the\nlife sciences, where new technologies increased the sequencing throughput from\nkilobytes to terabytes per day. This trend requires current SWfMS to adapt:\nNative support for parallel workflow execution must be provided to increase\nperformance; dynamically scalable \"pay-per-use\" compute infrastructures have to\nbe integrated to diminish hardware costs; adaptive scheduling of workflows in\ndistributed compute environments is required to optimize resource utilization.\nIn this survey we give an overview of parallelization techniques for SWfMS,\nboth in theory and in their realization in concrete systems. We find that\ncurrent systems leave considerable room for improvement and we propose key\nadvancements to the landscape of SWfMS.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 18:20:17 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Bux", "Marc", ""], ["Leser", "Ulf", ""]]}, {"id": "1303.7270", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Reza Moraveji, Javid Taheri, MohammadReza HosseinyFarahabady, Nikzad\n  Babaii Rizvandi, Albert Y. Zomaya", "title": "Data-Intensive Workload Consolidation on Hadoop Distributed File System", "comments": "Published at IEEE Grid 2012", "journal-ref": null, "doi": "10.1109/Grid.2012.25", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workload consolidation, sharing physical resources among multiple workloads,\nis a promising technique to save cost and energy in cluster computing systems.\nThis paper highlights a few challenges of workload consolidation for Hadoop as\none of the current state-of-the-art data-intensive cluster computing system.\nThrough a systematic step-by-step procedure, we investigate challenges for\nefficient server consolidation in Hadoop environments. To this end, we first\ninvestigate the inter-relationship between last level cache (LLC) contention\nand throughput degradation for consolidated workloads on a single physical\nserver employing Hadoop distributed file system (HDFS). We then investigate the\ngeneral case of consolidation on multiple physical servers so that their\nthroughput never falls below a desired/predefined utilization level. We use our\nempirical results to model consolidation as a classic two-dimensional bin\npacking problem and then design a computationally efficient greedy algorithm to\nachieve minimum throughput degradation on multiple servers. Results are very\npromising and show that our greedy approach is able to achieve near optimal\nsolution in all experimented cases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 23:15:36 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Moraveji", "Reza", ""], ["Taheri", "Javid", ""], ["HosseinyFarahabady", "MohammadReza", ""], ["Rizvandi", "Nikzad Babaii", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1303.7300", "submitter": "Madakasira Kaushik", "authors": "Giddaluru Madhavi, M.K. Kaushik", "title": "Queuing Methodology Based Power Efficient Routing Protocol for Reliable\n  Data Communications in Manets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile ad hoc network (MANET) is a wireless network that uses multi-hop\npeer-to- peer routing instead of static network infrastructure to provide\nnetwork connectivity. MANETs have applications in rapidly deployed and dynamic\nmilitary and civilian systems. The network topology in a MANET usually changes\nwith time. Therefore, there are new challenges for routing protocols in MANETs\nsince traditional routing protocols may not be suitable for MANETs. In recent\nyears, a variety of new routing protocols targeted specifically at this\nenvironment have been developed, but little performance information on each\nprotocol and no realistic performance comparison between them is available.\nThis paper presents the results of a detailed packet-level simulation comparing\nthree multi-hop wireless ad hoc network routing protocols that cover a range of\ndesign choices: DSR, NFPQR, and clustered NFPQR. By applying queuing\nmethodology to the introduced routing protocol the reliability and throughput\nof the network is increased.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 06:01:08 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Madhavi", "Giddaluru", ""], ["Kaushik", "M. K.", ""]]}, {"id": "1303.7425", "submitter": "Jacques Laskar", "authors": "Micka\u007fel Gastineau and Jacques Laskar", "title": "Highly Scalable Multiplication for Distributed Sparse Multivariate\n  Polynomials on Many-core Systems", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC astro-ph.IM cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly scalable algorithm for multiplying sparse multivariate\npolynomials represented in a distributed format. This algo- rithm targets not\nonly the shared memory multicore computers, but also computers clusters or\nspecialized hardware attached to a host computer, such as graphics processing\nunits or many-core coprocessors. The scal- ability on the large number of cores\nis ensured by the lacks of synchro- nizations, locks and false-sharing during\nthe main parallel step.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 15:47:45 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Gastineau", "Micka\u007fel", ""], ["Laskar", "Jacques", ""]]}]