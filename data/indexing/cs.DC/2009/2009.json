[{"id": "2009.00041", "submitter": "Hamed Rahimi", "authors": "Hamed Rahimi, Yvan Picaud, Salvatore Costanzo, Giyyarpuram Madhusudan,\n  Olivier Boissier, kamal Deep Singh", "title": "Design and Simulation of a Hybrid Architecture for Edge Computing in 5G\n  and Beyond", "comments": "Submitted to Special Issue on Smart Edge Computing and IoT of IEEE\n  Transaction on Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge Computing in 5G and Beyond is a promising solution for ultra-low latency\napplications (e.g. Autonomous Vehicle, Augmented Reality, and Remote Surgery),\nwhich have an extraordinarily low tolerance for the delay and require fast data\nprocessing for a very high volume of data. The requirements of delay-sensitive\napplications (e.g. Low latency, proximity, and Location/Context-awareness)\ncannot be satisfied by Cloud Computing due to the high latency between User\nEquipment and Cloud. Nevertheless, Edge Computing in 5G and beyond can promise\nan ultra-high-speed caused by placing computation capabilities closer to\nendpoint devices, whereas 5G encourages the speed rate that is 200 times faster\nthan 4G LTE-Advanced. This paper deeply investigates Edge Computing in 5G and\ncharacterizes it based on the requirements of ultra-low latency applications.\nAs a contribution, we propose a hybrid architecture that takes advantage of\nnovel and sustainable technologies (e.g. D2D communication, Massive MIMO, SDN,\nand NFV) and has major features such as scalability, reliability and ultra-low\nlatency support. The proposed architecture is evaluated based on an agent-based\nsimulation that demonstrates it can satisfy requirements and has the ability to\nrespond to high volume demands with low latency.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 18:19:14 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Rahimi", "Hamed", ""], ["Picaud", "Yvan", ""], ["Costanzo", "Salvatore", ""], ["Madhusudan", "Giyyarpuram", ""], ["Boissier", "Olivier", ""], ["Singh", "kamal Deep", ""]]}, {"id": "2009.00072", "submitter": "Subhadeep Sahoo", "authors": "Subhadeep Sahoo, Xiao Han Dong, Zi Qian Liu, Joydeep Sahoo", "title": "Under Water Waste Cleaning by Mobile Edge Computing and Intelligent\n  Image Processing Based Robotic Fish", "comments": "This is an innovative project report awarded by Ericsson Innovation\n  Award 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As water pollution is a serious threat to underwater resources, i.e.,\nunderwater plants and species, we focus on protecting the resources by cleaning\nthe non-biodegradable waste from the water. The waste can be recycled for\nfurther usage. Here we design a robotic fish which mainly comprises optical\nbiosensor, camera module, piston module, and wireless transceiver. By\nexploiting the LTE and 5G network architecture, the fish stores the information\nabout the underwater waste in the nearest mobile edge computing server as well\nas in the centralized cloud server. Finally, when the fish clears the\nunderwater waste, it offloads the captured image of the located object to the\nmobile edge computing server or sometimes to the cloud server for making a\ndecision. The servers employ intelligent image processing technology and an\nadaptive learning process to make a decision. However, if the servers fail to\nmake a decision, then the fish utilizes its optical biosensor. By this scheme,\nthe time delay for clearing any water body is minimized and the waste\ncollection capacity of the fish is maximized. This technique can effectively\nhelp the government or municipal personnel for making clean water without\nmanual efforts.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:22:54 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Sahoo", "Subhadeep", ""], ["Dong", "Xiao Han", ""], ["Liu", "Zi Qian", ""], ["Sahoo", "Joydeep", ""]]}, {"id": "2009.00081", "submitter": "Afaf Ta\\\"ik", "authors": "Afaf Ta\\\"ik and Soumaya Cherkaoui", "title": "Federated Edge Learning : Design Issues and Challenges", "comments": "Submitted to IEEE Network Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a distributed machine learning technique, where\neach device contributes to the learning model by independently computing the\ngradient based on its local training data. It has recently become a hot\nresearch topic, as it promises several benefits related to data privacy and\nscalability. However, implementing FL at the network edge is challenging due to\nsystem and data heterogeneity and resources constraints. In this article, we\nexamine the existing challenges and trade-offs in Federated Edge Learning\n(FEEL). The design of FEEL algorithms for resources-efficient learning raises\nseveral challenges. These challenges are essentially related to the\nmultidisciplinary nature of the problem. As the data is the key component of\nthe learning, this article advocates a new set of considerations for data\ncharacteristics in wireless scheduling algorithms in FEEL. Hence, we propose a\ngeneral framework for the data-aware scheduling as a guideline for future\nresearch directions. We also discuss the main axes and requirements for data\nevaluation and some exploitable techniques and metrics.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:56:36 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ta\u00efk", "Afaf", ""], ["Cherkaoui", "Soumaya", ""]]}, {"id": "2009.00083", "submitter": "Jonas Lukasczyk", "authors": "Jonas Lukasczyk, Christoph Garth, Ross Maciejewski, and Julien Tierny", "title": "Localized Topological Simplification of Scalar Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a localized algorithm for the topological simplification\nof scalar data, an essential pre-processing step of topological data analysis\n(TDA). Given a scalar field f and a selection of extrema to preserve, the\nproposed localized topological simplification (LTS) derives a function g that\nis close to f and only exhibits the selected set of extrema. Specifically, sub-\nand superlevel set components associated with undesired extrema are first\nlocally flattened and then correctly embedded into the global scalar field,\nsuch that these regions are guaranteed -- from a combinatorial perspective --\nto no longer contain any undesired extrema. In contrast to previous global\napproaches, LTS only and independently processes regions of the domain that\nactually need to be simplified, which already results in a noticeable speedup.\nMoreover, due to the localized nature of the algorithm, LTS can utilize\nshared-memory parallelism to simplify regions simultaneously with a high\nparallel efficiency (70%). Hence, LTS significantly improves interactivity for\nthe exploration of simplification parameters and their effect on subsequent\ntopological analysis. For such exploration tasks, LTS brings the overall\nexecution time of a plethora of TDA pipelines from minutes down to seconds,\nwith an average observed speedup over state-of-the-art techniques of up to x36.\nFurthermore, in the special case where preserved extrema are selected based on\ntopological persistence, an adapted version of LTS partially computes the\npersistence diagram and simultaneously simplifies features below a predefined\npersistence threshold. The effectiveness of LTS, its parallel efficiency, and\nits resulting benefits for TDA are demonstrated on several simulated and\nacquired datasets from different application domains, including physics,\nchemistry, and biomedical imaging.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:57:40 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Lukasczyk", "Jonas", ""], ["Garth", "Christoph", ""], ["Maciejewski", "Ross", ""], ["Tierny", "Julien", ""]]}, {"id": "2009.00156", "submitter": "Abhinav Aggarwal", "authors": "John Erickson, Abhinav Aggarwal, G. Matthew Fricke, Melanie E. Moses", "title": "LoCUS: A multi-robot loss-tolerant algorithm for surveying volcanic\n  plumes", "comments": "Accepted to IRC 2020 (8 pages, 7 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement of volcanic CO2 flux by a drone swarm poses special challenges.\nDrones must be able to follow gas concentration gradients while tolerating\nfrequent drone loss. We present the LoCUS algorithm as a solution to this\nproblem and prove its robustness. LoCUS relies on swarm coordination and\nself-healing to solve the task. As a point of contrast we also implement the\nMoBS algorithm, derived from previously published work, which allows drones to\nsolve the task independently. We compare the effectiveness of these algorithms\nusing drone simulations, and find that LoCUS provides a reliable and efficient\nsolution to the volcano survey problem. Further, the novel data-structures and\nalgorithms underpinning LoCUS have application in other areas of fault-tolerant\nalgorithm research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 00:39:51 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Erickson", "John", ""], ["Aggarwal", "Abhinav", ""], ["Fricke", "G. Matthew", ""], ["Moses", "Melanie E.", ""]]}, {"id": "2009.00250", "submitter": "Rafael Ferreira da Silva", "authors": "Rafael Ferreira da Silva (1), Lo\\\"ic Pottier (1), Tain\\~a Coleman (1),\n  Ewa Deelman (1), Henri Casanova (2) ((1) University of Southern California,\n  (2) University of Hawaii at Manoa)", "title": "WorkflowHub: Community Framework for Enabling Scientific Workflow\n  Research and Development -- Technical Report", "comments": "Technical Report", "journal-ref": null, "doi": "10.1109/WORKS51914.2020.00012", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific workflows are a cornerstone of modern scientific computing. They\nare used to describe complex computational applications that require efficient\nand robust management of large volumes of data, which are typically\nstored/processed at heterogeneous, distributed resources. The workflow research\nand development community has employed a number of methods for the quantitative\nevaluation of existing and novel workflow algorithms and systems. In\nparticular, a common approach is to simulate workflow executions. In previous\nwork, we have presented a collection of tools that have been used for aiding\nresearch and development activities in the Pegasus project, and that have been\nadopted by others for conducting workflow research. Despite their popularity,\nthere are several shortcomings that prevent easy adoption, maintenance, and\nconsistency with the evolving structures and computational requirements of\nproduction workflows. In this work, we present WorkflowHub, a community\nframework that provides a collection of tools for analyzing workflow execution\ntraces, producing realistic synthetic workflow traces, and simulating workflow\nexecutions. We demonstrate the realism of the generated synthetic traces by\ncomparing simulated executions of these traces with actual workflow executions.\nWe also contrast these results with those obtained when using the previously\navailable collection of tools. We find that our framework not only can be used\nto generate representative synthetic workflow traces (i.e., with workflow\nstructures and task characteristics distributions that resembles those in\ntraces obtained from real-world workflow executions), but can also generate\nrepresentative workflow traces at larger scales than that of available workflow\ntraces.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 06:02:39 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["da Silva", "Rafael Ferreira", ""], ["Pottier", "Lo\u00efc", ""], ["Coleman", "Tain\u00e3", ""], ["Deelman", "Ewa", ""], ["Casanova", "Henri", ""]]}, {"id": "2009.00304", "submitter": "S\\\"oren Henning", "authors": "S\\\"oren Henning, Wilhelm Hasselbring", "title": "Theodolite: Scalability Benchmarking of Distributed Stream Processing\n  Engines in Microservice Architectures", "comments": "28 pages", "journal-ref": "Big Data Research 25 (2021)", "doi": "10.1016/j.bdr.2021.100209", "report-no": null, "categories": "cs.SE cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed stream processing engines are designed with a focus on\nscalability to process big data volumes in a continuous manner. We present the\nTheodolite method for benchmarking the scalability of distributed stream\nprocessing engines. Core of this method is the definition of use cases that\nmicroservices implementing stream processing have to fulfill. For each use\ncase, our method identifies relevant workload dimensions that might affect the\nscalability of a use case. We propose to design one benchmark per use case and\nrelevant workload dimension. We present a general benchmarking framework, which\ncan be applied to execute the individual benchmarks for a given use case and\nworkload dimension. Our framework executes an implementation of the use case's\ndataflow architecture for different workloads of the given dimension and\nvarious numbers of processing instances. This way, it identifies how resources\ndemand evolves with increasing workloads. Within the scope of this paper, we\npresent 4 identified use cases, derived from processing Industrial Internet of\nThings data, and 7 corresponding workload dimensions. We provide\nimplementations of 4 benchmarks with Kafka Streams and Apache Flink as well as\nan implementation of our benchmarking framework to execute scalability\nbenchmarks in cloud environments. We use both for evaluating the Theodolite\nmethod and for benchmarking Kafka Streams' and Flink's scalability for\ndifferent deployment options.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 09:13:16 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 14:45:35 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 14:30:56 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Henning", "S\u00f6ren", ""], ["Hasselbring", "Wilhelm", ""]]}, {"id": "2009.00319", "submitter": "James Riehl", "authors": "James R. Riehl, Jonathan Ward", "title": "Transaction Pricing for Maximizing Throughput in a Sharded Blockchain\n  Ledger", "comments": null, "journal-ref": "2020 Crypto Valley Conference on Blockchain Technology (CVCBT),\n  Rotkreuz, Switzerland, 2020, pp. 36-42", "doi": "10.1109/CVCBT50464.2020.00008.", "report-no": null, "categories": "eess.SY cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a pricing mechanism that aligns incentives of\nagents who exchange resources on a decentralized ledger with the goal of\nmaximizing transaction throughput. Subdividing a blockchain ledger into shards\npromises to greatly increase transaction throughput with minimal loss of\nsecurity. However, the organization and type of the transactions also affects\nthe ledger's efficiency, which is increased by wallet agents transacting in a\nsingle shard whenever possible while collectively distributing their\ntransactions uniformly across the available shards. Since there is no central\nauthority to enforce these properties, the only means of achieving them is to\ndesign the system such that it is in agents' interest to act in a way that\nbenefits overall throughput. We show that our proposed pricing policy does\nexactly this by inducing a potential game for the agents, where the potential\nfunction relates directly to ledger throughput. Simulations demonstrate that\nthis policy leads to near-optimal throughput under a variety of conditions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 09:52:47 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Riehl", "James R.", ""], ["Ward", "Jonathan", ""]]}, {"id": "2009.00361", "submitter": "Ana Sofia Gomes", "authors": "Jo\\~ao Oliveirinha, Ana Sofia Gomes, Pedro Cardoso, Pedro Bizarro", "title": "Railgun: streaming windows for mission critical systems", "comments": "Previously submitted to CIDR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some mission critical systems, such as fraud detection, require accurate,\nreal-time metrics over long time windows on applications that demand high\nthroughputs and low latencies. As these applications need to run \"forever\",\ncope with large and spiky data loads, they further require to be run in a\ndistributed setting. Unsurprisingly, we are unaware of any distributed\nstreaming system that provides all those properties. Instead, existing systems\ntake large simplifications, such as implementing sliding windows as a fixed set\nof partially overlapping windows, jeopardizing metric accuracy (violating\nfinancial regulator rules) or latency (breaching service agreements).\n  In this paper, we propose Railgun, a fault-tolerant, elastic, and distributed\nstreaming system supporting real-time sliding windows for scenarios requiring\nhigh loads and millisecond-level latencies. We benchmarked an initial prototype\nof Railgun using real data, showing significant lower latency than Flink, and\nlow memory usage, independent of window size.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 11:33:19 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 11:00:01 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 10:08:03 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Oliveirinha", "Jo\u00e3o", ""], ["Gomes", "Ana Sofia", ""], ["Cardoso", "Pedro", ""], ["Bizarro", "Pedro", ""]]}, {"id": "2009.00439", "submitter": "Haris Mansoor", "authors": "Haris Mansoor, Naveed Arshad", "title": "Market Model for Demand Response under Block Rate Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewable sources are taking center stage in electricity generation. However,\nmatching supply with demand in a renewable-rich system is a difficult task due\nto the intermittent nature of renewable resources (wind, solar, etc.). As a\nresult, Demand Response (DR) programs are an essential part of the modern grid.\nAn efficient DR technique is to devise different pricing schemes that encourage\ncustomers to reduce or shift the electric load. In this paper, we consider a\nmarket model for DR using Block Rate Pricing (BRP) of two blocks. We use a\nutility maximization approach in a competitive market. We show that when\ncustomers are price taking and the utility cost function is quadratic the\nresulting system achieves an equilibrium. Moreover, the equilibrium is unique\nand efficient, which maximizes social welfare. A distributed algorithm is\nproposed to find the optimal pricing of both blocks and the load. Both the\ncustomers and the utility runs the market. The proposed scheme encourages\ncustomers to curtail or shift their load. Numerical results are presented to\nvalidate our technique.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 17:52:17 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Mansoor", "Haris", ""], ["Arshad", "Naveed", ""]]}, {"id": "2009.00858", "submitter": "Gadekallu Thippa Reddy", "authors": "Natarajan Deepa, Quoc-Viet Pham, Dinh C. Nguyen, Sweta Bhattacharya, B\n  Prabadevi, Thippa Reddy Gadekallu, Praveen Kumar Reddy Maddikunta, Fang Fang,\n  Pubudu N. Pathirana", "title": "A Survey on Blockchain for Big Data: Approaches, Opportunities, and\n  Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data has generated strong interest in various scientific and engineering\ndomains over the last few years. Despite many advantages and applications,\nthere are many challenges in big data to be tackled for better quality of\nservice, e.g., big data analytics, big data management, and big data privacy\nand security. Blockchain with its decentralization and security nature has the\ngreat potential to improve big data services and applications. In this article,\nwe provide a comprehensive survey on blockchain for big data, focusing on\nup-to-date approaches, opportunities, and future directions. First, we present\na brief overview of blockchain and big data as well as the motivation behind\ntheir integration. Next, we survey various blockchain services for big data,\nincluding blockchain for secure big data acquisition, data storage, data\nanalytics, and data privacy preservation. Then, we review the state-of-the-art\nstudies on the use of blockchain for big data applications in different\nvertical domains such as smart city, smart healthcare, smart transportation,\nand smart grid. For a better understanding, some representative blockchain-big\ndata projects are also presented and analyzed. Finally, challenges and future\ndirections are discussed to further drive research in this promising area.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:14:45 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 06:14:13 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Deepa", "Natarajan", ""], ["Pham", "Quoc-Viet", ""], ["Nguyen", "Dinh C.", ""], ["Bhattacharya", "Sweta", ""], ["Prabadevi", "B", ""], ["Gadekallu", "Thippa Reddy", ""], ["Maddikunta", "Praveen Kumar Reddy", ""], ["Fang", "Fang", ""], ["Pathirana", "Pubudu N.", ""]]}, {"id": "2009.00915", "submitter": "Mustafa Abduljabbar", "authors": "Jing Chen, Pirah Noor Soomro, Mustafa Abduljabbar, Madhavan\n  Manivannan, Miquel Pericas", "title": "Scheduling Task-parallel Applications in Dynamically Asymmetric\n  Environments", "comments": "Published in ICPP Workshops '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared resource interference is observed by applications as dynamic\nperformance asymmetry. Prior art has developed approaches to reduce the impact\nof performance asymmetry mainly at the operating system and architectural\nlevels. In this work, we study how application-level scheduling techniques can\nleverage moldability (i.e. flexibility to work as either single-threaded or\nmultithreaded task) and explicit knowledge on task criticality to handle\nscenarios in which system performance is not only unknown but also changing\nover time. Our proposed task scheduler dynamically learns the performance\ncharacteristics of the underlying platform and uses this knowledge to devise\nbetter schedules aware of dynamic performance asymmetry, hence reducing the\nimpact of interference. Our evaluation shows that both criticality-aware\nscheduling and parallelism tuning are effective schemes to address interference\nin both shared and distributed memory applications\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 09:35:12 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 18:34:56 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Chen", "Jing", ""], ["Soomro", "Pirah Noor", ""], ["Abduljabbar", "Mustafa", ""], ["Manivannan", "Madhavan", ""], ["Pericas", "Miquel", ""]]}, {"id": "2009.00993", "submitter": "Albert Reuther PhD", "authors": "Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally,\n  Siddharth Samsi and Jeremy Kepner", "title": "Survey of Machine Learning Accelerators", "comments": "12 pages, 2 figures, IEEE-HPEC conference, Waltham, MA, September\n  21-25, 2020. arXiv admin note: text overlap with arXiv:1908.11348", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286149", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New machine learning accelerators are being announced and released each month\nfor a variety of applications from speech recognition, video object detection,\nassisted driving, and many data center applications. This paper updates the\nsurvey of of AI accelerators and processors from last year's IEEE-HPEC paper.\nThis paper collects and summarizes the current accelerators that have been\npublicly announced with performance and power consumption numbers. The\nperformance and power values are plotted on a scatter graph and a number of\ndimensions and observations from the trends on this plot are discussed and\nanalyzed. For instance, there are interesting trends in the plot regarding\npower consumption, numerical precision, and inference versus training. This\nyear, there are many more announced accelerators that are implemented with many\nmore architectures and technologies from vector engines, dataflow engines,\nneuromorphic designs, flash-based analog memory processing, and photonic-based\nprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 01:28:59 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Reuther", "Albert", ""], ["Michaleas", "Peter", ""], ["Jones", "Michael", ""], ["Gadepally", "Vijay", ""], ["Samsi", "Siddharth", ""], ["Kepner", "Jeremy", ""]]}, {"id": "2009.01177", "submitter": "Yuxuan Li", "authors": "Yuxuan Li, Mingcheng Chen, Yaojian Chen, Haitian Lu, Lin Gan, Chaoyang\n  Lu, Jianwei Pan, Haohuan Fu, and Guangwen Yang", "title": "Benchmarking 50-Photon Gaussian Boson Sampling on the Sunway TaihuLight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boson sampling is expected to be one of an important milestones that will\ndemonstrate quantum supremacy. The present work establishes the benchmarking of\nGaussian boson sampling (GBS) with threshold detection based on the Sunway\nTaihuLight supercomputer. To achieve the best performance and provide a\ncompetitive scenario for future quantum computing studies, the selected\nsimulation algorithm is fully optimized based on a set of innovative\napproaches, including a parallel scheme and instruction-level optimizing\nmethod. Furthermore, data precision and instruction scheduling are handled in a\nsophisticated manner by an adaptive precision optimization scheme and a\nDAG-based heuristic search algorithm, respectively. Based on these methods, a\nhighly efficient and parallel quantum sampling algorithm is designed. The\nlargest run enables us to obtain one Torontonian function of a 100 x 100\nsubmatrix from 50-photon GBS within 20 hours in 128-bit precision and 2 days in\n256-bit precision.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:37:33 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Li", "Yuxuan", ""], ["Chen", "Mingcheng", ""], ["Chen", "Yaojian", ""], ["Lu", "Haitian", ""], ["Gan", "Lin", ""], ["Lu", "Chaoyang", ""], ["Pan", "Jianwei", ""], ["Fu", "Haohuan", ""], ["Yang", "Guangwen", ""]]}, {"id": "2009.01431", "submitter": "Zhe Lin", "authors": "Zhe Lin, Sharad Sinha, Wei Zhang", "title": "Towards Efficient and Scalable Acceleration of Online Decision Tree\n  Learning on FPGA", "comments": "appear as a conference paper in FCCM 2019", "journal-ref": null, "doi": "10.1109/FCCM.2019.00032", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees are machine learning models commonly used in various\napplication scenarios. In the era of big data, traditional decision tree\ninduction algorithms are not suitable for learning large-scale datasets due to\ntheir stringent data storage requirement. Online decision tree learning\nalgorithms have been devised to tackle this problem by concurrently training\nwith incoming samples and providing inference results. However, even the most\nup-to-date online tree learning algorithms still suffer from either high memory\nusage or high computational intensity with dependency and long latency, making\nthem challenging to implement in hardware. To overcome these difficulties, we\nintroduce a new quantile-based algorithm to improve the induction of the\nHoeffding tree, one of the state-of-the-art online learning models. The\nproposed algorithm is light-weight in terms of both memory and computational\ndemand, while still maintaining high generalization ability. A series of\noptimization techniques dedicated to the proposed algorithm have been\ninvestigated from the hardware perspective, including coarse-grained and\nfine-grained parallelism, dynamic and memory-based resource sharing, pipelining\nwith data forwarding. We further present a high-performance, hardware-efficient\nand scalable online decision tree learning system on a field-programmable gate\narray (FPGA) with system-level optimization techniques. Experimental results\nshow that our proposed algorithm outperforms the state-of-the-art Hoeffding\ntree learning method, leading to 0.05% to 12.3% improvement in inference\naccuracy. Real implementation of the complete learning system on the FPGA\ndemonstrates a 384x to 1581x speedup in execution time over the\nstate-of-the-art design.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 03:23:43 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lin", "Zhe", ""], ["Sinha", "Sharad", ""], ["Zhang", "Wei", ""]]}, {"id": "2009.01497", "submitter": "Gregor Stefan Bankhamer", "authors": "Gregor Bankhamer, Robert Els\\\"asser, Stefan Schmid", "title": "Local Fast Rerouting with Low Congestion: A Randomized Approach", "comments": null, "journal-ref": null, "doi": "10.1109/ICNP.2019.8888087", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern communication networks include fast rerouting mechanisms,\nimplemented entirely in the data plane, to quickly recover connectivity after\nlink failures. By relying on local failure information only, these data plane\nmechanisms provide very fast reaction times, but at the same time introduce an\nalgorithmic challenge in case of multiple link failures: failover routes need\nto be robust to additional but locally unknown failures downstream.\n  This paper presents local fast rerouting algorithms which not only provide a\nhigh degree of resilience against multiple link failures, but also ensure a low\ncongestion on the resulting failover paths. We consider a randomized approach\nand focus on networks which are highly connected before the failures occur. Our\nmain contribution are three simple algorithms which come with provable\nguarantees and provide interesting resilience-load tradeoffs, significantly\noutperforming any deterministic fast rerouting algorithm with high probability.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 07:45:34 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Bankhamer", "Gregor", ""], ["Els\u00e4sser", "Robert", ""], ["Schmid", "Stefan", ""]]}, {"id": "2009.01502", "submitter": "Pengyuan Zhou", "authors": "Pengyuan Zhou, Xianfu Chen, Zhi Liu, Tristan Braud, Pan Hui, Jussi\n  Kangasharju", "title": "DRLE: Decentralized Reinforcement Learning at the Edge for Traffic Light\n  Control in the IoV", "comments": "Accepted by IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2020.3035841", "report-no": null, "categories": "cs.MA cs.DC cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Internet of Vehicles (IoV) enables real-time data exchange among vehicles\nand roadside units and thus provides a promising solution to alleviate traffic\njams in the urban area. Meanwhile, better traffic management via efficient\ntraffic light control can benefit the IoV as well by enabling a better\ncommunication environment and decreasing the network load. As such, IoV and\nefficient traffic light control can formulate a virtuous cycle. Edge computing,\nan emerging technology to provide low-latency computation capabilities at the\nedge of the network, can further improve the performance of this cycle.\nHowever, while the collected information is valuable, an efficient solution for\nbetter utilization and faster feedback has yet to be developed for\nedge-empowered IoV. To this end, we propose a Decentralized Reinforcement\nLearning at the Edge for traffic light control in the IoV (DRLE). DRLE exploits\nthe ubiquity of the IoV to accelerate the collection of traffic data and its\ninterpretation towards alleviating congestion and providing better traffic\nlight control. DRLE operates within the coverage of the edge servers and uses\naggregated data from neighboring edge servers to provide city-scale traffic\nlight control. DRLE decomposes the highly complex problem of large area\ncontrol. into a decentralized multi-agent problem. We prove its global optima\nwith concrete mathematical reasoning. The proposed decentralized reinforcement\nlearning algorithm running at each edge node adapts the traffic lights in real\ntime. We conduct extensive evaluations and demonstrate the superiority of this\napproach over several state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 08:09:04 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 10:03:08 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhou", "Pengyuan", ""], ["Chen", "Xianfu", ""], ["Liu", "Zhi", ""], ["Braud", "Tristan", ""], ["Hui", "Pan", ""], ["Kangasharju", "Jussi", ""]]}, {"id": "2009.01507", "submitter": "Loic Cudennec", "authors": "Lo\\\"ic Cudennec (DACLE-LIST, DGA.MI)", "title": "Software-Distributed Shared Memory for Heterogeneous Machines: Design\n  and Use Considerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed shared memory (DSM) allows to implement and deploy applications\nonto distributed architectures using the convenient shared memory programming\nmodel in which a set of tasks are able to allocate and access data despite\ntheir remote localization. With the development of distributed heterogeneous\narchitectures in both HPC and embedded contexts, there is a renewal of interest\nfor systems such as DSM that ease the programmability of complex hardware. In\nthis report, some design considerations are given to build a complete\nsoftware-DSM (S-DSM). This S-DSM called SAT (Share Among Things) is developed\nat CEA (the French Alternative Energies and Atomic Energy Commission) within\nthe framework of European project M2DC (Modular Microserver DataCentre) to\ntackle the problem of managing shared data over microserver architec-tures. The\nS-DSM features the automatic decomposition of large data into atomic pieces\ncalled chunks, the possibility to deploy multiple coherence protocols to manage\ndifferent chunks, an hybrid programming model based on event programming and a\nmicro-sleep mechanism to decrease the energy consumption on message reception.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 08:23:43 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Cudennec", "Lo\u00efc", "", "DACLE-LIST, DGA.MI"]]}, {"id": "2009.01544", "submitter": "Avery Miller", "authors": "Avery Miller, Ullash Saha", "title": "Fast Byzantine Gathering with Visibility in Graphs", "comments": "Conference version appeared at ALGOSENSORS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the gathering task by a team of $m$ synchronous mobile robots in\na graph of $n$ nodes. Each robot has an identifier (ID) and runs its own\ndeterministic algorithm, i.e., there is no centralized coordinator. We consider\na particularly challenging scenario: there are $f$ Byzantine robots in the team\nthat can behave arbitrarily, and even have the ability to change their IDs to\nany value at any time. There is no way to distinguish these robots from\nnon-faulty robots, other than perhaps observing strange or unexpected\nbehaviour. The goal of the gathering task is to eventually have all non-faulty\nrobots located at the same node in the same round. It is known that no\nalgorithm can solve this task unless there at least $f+1$ non-faulty robots in\nthe team. In this paper, we design an algorithm that runs in polynomial time\nwith respect to $n$ and $m$ that matches this bound, i.e., it works in a team\nthat has exactly $f+1$ non-faulty robots. In our model, we have equipped the\nrobots with sensors that enable each robot to see the subgraph (including\nrobots) within some distance $H$ of its current node. We prove that the\ngathering task is solvable if this visibility range $H$ is at least the radius\nof the graph, and not solvable if $H$ is any fixed constant.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:39:07 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Miller", "Avery", ""], ["Saha", "Ullash", ""]]}, {"id": "2009.01588", "submitter": "Duy Thanh Nguyen", "authors": "Duy Thanh Nguyen, Hyun Kim, and Hyuk-Jae Lee", "title": "Layer-specific Optimization for Mixed Data Flow with Mixed Precision in\n  FPGA Design for CNN-based Object Detectors", "comments": "Accepted for publication in IEEE Transaction on Circuit and System\n  for Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3020569", "report-no": null, "categories": "cs.CV cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) require both intensive computation and\nfrequent memory access, which lead to a low processing speed and large power\ndissipation. Although the characteristics of the different layers in a CNN are\nfrequently quite different, previous hardware designs have employed common\noptimization schemes for them. This paper proposes a layer-specific design that\nemploys different organizations that are optimized for the different layers.\nThe proposed design employs two layer-specific optimizations: layer-specific\nmixed data flow and layer-specific mixed precision. The mixed data flow aims to\nminimize the off-chip access while demanding a minimal on-chip memory (BRAM)\nresource of an FPGA device. The mixed precision quantization is to achieve both\na lossless accuracy and an aggressive model compression, thereby further\nreducing the off-chip access. A Bayesian optimization approach is used to\nselect the best sparsity for each layer, achieving the best trade-off between\nthe accuracy and compression. This mixing scheme allows the entire network\nmodel to be stored in BRAMs of the FPGA to aggressively reduce the off-chip\naccess, and thereby achieves a significant performance enhancement. The model\nsize is reduced by 22.66-28.93 times compared to that in a full-precision\nnetwork with a negligible degradation of accuracy on VOC, COCO, and ImageNet\ndatasets. Furthermore, the combination of mixed dataflow and mixed precision\nsignificantly outperforms the previous works in terms of both throughput,\noff-chip access, and on-chip memory requirement.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 11:27:40 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Nguyen", "Duy Thanh", ""], ["Kim", "Hyun", ""], ["Lee", "Hyuk-Jae", ""]]}, {"id": "2009.01745", "submitter": "Guido Carnevale", "authors": "Guido Carnevale, Francesco Farina, Ivano Notarnicola, Giuseppe\n  Notarstefano", "title": "Distributed Online Optimization via Gradient Tracking with Adaptive\n  Momentum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a network of computing agents aiming to solve an online\noptimization problem in a distributed fashion, i.e., by means of local\ncomputation and communication, without any central coordinator. We propose the\ngradient tracking with adaptive momentum estimation (GTAdam) distributed\nalgorithm, which combines a gradient tracking mechanism with first and second\norder momentum estimates of the gradient. The algorithm is analyzed in the\nonline setting for strongly convex and smooth cost functions. We prove that the\naverage dynamic regret is bounded and that the convergence rate is linear. The\nalgorithm is tested on a time-varying classification problem, on a (moving)\ntarget localization problem and in a stochastic optimization setup from image\nclassification. In these numerical experiments from multi-agent learning,\nGTAdam outperforms state-of-the-art distributed optimization methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 15:20:21 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Carnevale", "Guido", ""], ["Farina", "Francesco", ""], ["Notarnicola", "Ivano", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "2009.01845", "submitter": "Stefano Carrazza", "authors": "Stavros Efthymiou, Sergi Ramos-Calderer, Carlos Bravo-Prieto, Adri\\'an\n  P\\'erez-Salinas, Diego Garc\\'ia-Mart\\'in, Artur Garcia-Saez, Jos\\'e Ignacio\n  Latorre, Stefano Carrazza", "title": "Qibo: a framework for quantum simulation with hardware acceleration", "comments": "15 pages, 12 figures, 5 tables,code available at\n  https://github.com/Quantum-TII/qibo", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Qibo, a new open-source software for fast evaluation of quantum\ncircuits and adiabatic evolution which takes full advantage of hardware\naccelerators. The growing interest in quantum computing and the recent\ndevelopments of quantum hardware devices motivates the development of new\nadvanced computational tools focused on performance and usage simplicity. In\nthis work we introduce a new quantum simulation framework that enables\ndevelopers to delegate all complicated aspects of hardware or platform\nimplementation to the library so they can focus on the problem and quantum\nalgorithms at hand. This software is designed from scratch with simulation\nperformance, code simplicity and user friendly interface as target goals. It\ntakes advantage of hardware acceleration such as multi-threading CPU, single\nGPU and multi-GPU devices.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:00:01 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Efthymiou", "Stavros", ""], ["Ramos-Calderer", "Sergi", ""], ["Bravo-Prieto", "Carlos", ""], ["P\u00e9rez-Salinas", "Adri\u00e1n", ""], ["Garc\u00eda-Mart\u00edn", "Diego", ""], ["Garcia-Saez", "Artur", ""], ["Latorre", "Jos\u00e9 Ignacio", ""], ["Carrazza", "Stefano", ""]]}, {"id": "2009.02186", "submitter": "Shreshth Tuli", "authors": "Shreshth Tuli, Shashikant Ilager, Kotagiri Ramamohanarao and Rajkumar\n  Buyya", "title": "Dynamic Scheduling for Stochastic Edge-Cloud Computing Environments\n  using A3C learning and Residual Recurrent Neural Networks", "comments": "Accepted in IEEE Transaction on Mobile Computing", "journal-ref": null, "doi": "10.1109/TMC.2020.3017079", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ubiquitous adoption of Internet-of-Things (IoT) based applications has\nresulted in the emergence of the Fog computing paradigm, which allows\nseamlessly harnessing both mobile-edge and cloud resources. Efficient\nscheduling of application tasks in such environments is challenging due to\nconstrained resource capabilities, mobility factors in IoT, resource\nheterogeneity, network hierarchy, and stochastic behaviors. xisting heuristics\nand Reinforcement Learning based approaches lack generalizability and quick\nadaptability, thus failing to tackle this problem optimally. They are also\nunable to utilize the temporal workload patterns and are suitable only for\ncentralized setups. However, Asynchronous-Advantage-Actor-Critic (A3C) learning\nis known to quickly adapt to dynamic scenarios with less data and Residual\nRecurrent Neural Network (R2N2) to quickly update model parameters. Thus, we\npropose an A3C based real-time scheduler for stochastic Edge-Cloud environments\nallowing decentralized learning, concurrently across multiple agents. We use\nthe R2N2 architecture to capture a large number of host and task parameters\ntogether with temporal patterns to provide efficient scheduling decisions. The\nproposed model is adaptive and able to tune different hyper-parameters based on\nthe application requirements. We explicate our choice of hyper-parameters\nthrough sensitivity analysis. The experiments conducted on real-world data set\nshow a significant improvement in terms of energy consumption, response time,\nService-Level-Agreement and running cost by 14.4%, 7.74%, 31.9%, and 4.64%,\nrespectively when compared to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 13:36:34 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Tuli", "Shreshth", ""], ["Ilager", "Shashikant", ""], ["Ramamohanarao", "Kotagiri", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2009.02235", "submitter": "Roland Schmid", "authors": "Zeta Avarikioti, Lioba Heimbach, Roland Schmid, Laurent Vanbever,\n  Roger Wattenhofer, Patrick Wintermeyer", "title": "FnF-BFT: Exploring Performance Limits of BFT Protocols", "comments": "31 pages, 25 figures (including algorithms)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce FnF-BFT, a parallel-leader byzantine fault-tolerant\nstate-machine replication protocol for the partially synchronous model with\ntheoretical performance bounds during synchrony. By allowing all replicas to\nact as leaders and propose requests independently, FnF-BFT parallelizes the\nexecution of requests. Leader parallelization distributes the load over the\nentire network -- increasing throughput by overcoming the single-leader\nbottleneck. We further use historical data to ensure that well-performing\nreplicas are in command. FnF-BFT's communication complexity is linear in the\nnumber of replicas during synchrony and thus competitive with state-of-the-art\nprotocols. Finally, with FnF-BFT, we introduce a BFT protocol with performance\nguarantees in stable network conditions under truly byzantine attacks.\n  A prototype implementation of \\prot outperforms (state-of-the-art) HotStuff's\nthroughput, especially as replicas increase, showcasing \\prot's significantly\nimproved scaling capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:08:34 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 12:45:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Avarikioti", "Zeta", ""], ["Heimbach", "Lioba", ""], ["Schmid", "Roland", ""], ["Vanbever", "Laurent", ""], ["Wattenhofer", "Roger", ""], ["Wintermeyer", "Patrick", ""]]}, {"id": "2009.02240", "submitter": "Cornelis Jan Van Leeuwen", "authors": "Cornelis Jan van Leeuwen and Przemyz{\\l}aw Pawe{\\l}czak", "title": "Hybrid DCOP Solvers: Boosting Performance of Local Search Algorithms", "comments": "16 pages, 6 figures, 2 tables, 2 algorithms with pseudocode.\n  Presented at the International Workshop on Optimization in Multiagent Systems\n  (OptMAS-18), during the AAMAS conference 2018 in Stockholm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for expediting both symmetric and asymmetric\nDistributed Constraint Optimization Problem (DCOP) solvers. The core idea is\nbased on initializing DCOP solvers with greedy fast non-iterative DCOP solvers.\nThis is contrary to existing methods where initialization is always achieved\nusing a random value assignment. We empirically show that changing the starting\nconditions of existing DCOP solvers not only reduces the algorithm convergence\ntime by up to 50\\%, but also reduces the communication overhead and leads to a\nbetter solution quality. We show that this effect is due to structural\nimprovements in the variable assignment, which is caused by the spreading\npattern of DCOP algorithm activation.) /Subject (Hybrid DCOPs)\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:17:24 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["van Leeuwen", "Cornelis Jan", ""], ["Pawe\u0142czak", "Przemyz\u0142aw", ""]]}, {"id": "2009.02353", "submitter": "Gianni Antichi", "authors": "Giuseppe Siracusano, Salvator Galea, Davide Sanvito, Mohammad\n  Malekzadeh, Hamed Haddadi, Gianni Antichi, Roberto Bifulco", "title": "Running Neural Networks on the NIC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the data plane of commodity programmable (Network\nInterface Cards) NICs can run neural network inference tasks required by packet\nmonitoring applications, with low overhead. This is particularly important as\nthe data transfer costs to the host system and dedicated machine learning\naccelerators, e.g., GPUs, can be more expensive than the processing task\nitself. We design and implement our system -- N3IC -- on two different NICs and\nwe show that it can greatly benefit three different network monitoring use\ncases that require machine learning inference as first-class-primitive. N3IC\ncan perform inference for millions of network flows per second, while\nforwarding traffic at 40Gb/s. Compared to an equivalent solution implemented on\na general purpose CPU, N3IC can provide 100x lower processing latency, with\n1.5x increase in throughput.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 18:35:58 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Siracusano", "Giuseppe", ""], ["Galea", "Salvator", ""], ["Sanvito", "Davide", ""], ["Malekzadeh", "Mohammad", ""], ["Haddadi", "Hamed", ""], ["Antichi", "Gianni", ""], ["Bifulco", "Roberto", ""]]}, {"id": "2009.02355", "submitter": "Hesam Nejati Sharif Aldin", "authors": "Hesam Nejati Sharif Aldin, Mostafa Razavi Ghods, Hossein Deldari", "title": "\"Reduction of Monetary Cost in Cloud Storage System by Using Extended\n  Strict Timed Causal Consistency\"", "comments": "PDF-Latex, 15 pages, 15 pdf figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud storage systems have been introduced to provide a scalable, secure,\nreliable, and highly available data storage environment for the organizations\nand end-users. Therefore, the service provider should grow in a geographical\nextent. Consequently, extensive storage service provision requires a\nreplication mechanism. Replication imposes many costs on the cloud storage,\nincluding the synchronization, communications, storage, etc., costs among the\nreplicas. Moreover, the synchronization process among replicas is a major\nchallenge in cloud storage. Therefore, consistency can be defined as the\ncoordination among the replicas. In this paper, we propose an extension to the\nstrict timed causal consistency by adding the considerations for the monetary\ncosts and the number of violations in the cloud storage systems and call it the\nextended strict timed causal consistency. Our proposed supports monotonic read,\nread your write, monotonic write, and write follow read, models by taking into\naccount the causal relations between users' operations, at the client-side.\nBesides, it supports timed causal at the server-side. We employed the Cassandra\ncloud database that supports various consistencies such as all, one, quorum,\netc. Our method performs better in reducing staleness rate, the severity of\nviolations, and monetary cost in comparison with all, one, quorum, and causal.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 18:38:21 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Aldin", "Hesam Nejati Sharif", ""], ["Ghods", "Mostafa Razavi", ""], ["Deldari", "Hossein", ""]]}, {"id": "2009.02436", "submitter": "Vasileios Charisopoulos", "authors": "Vasileios Charisopoulos, Austin R. Benson, Anil Damle", "title": "Communication-efficient distributed eigenspace estimation", "comments": "v2: Removes unnecessary assumption, fixes typo in Theorem 3. 39\n  pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distributed computing is a standard way to scale up machine learning and data\nscience algorithms to process large amounts of data. In such settings, avoiding\ncommunication amongst machines is paramount for achieving high performance.\nRather than distribute the computation of existing algorithms, a common\npractice for avoiding communication is to compute local solutions or parameter\nestimates on each machine and then combine the results; in many convex\noptimization problems, even simple averaging of local solutions can work well.\nHowever, these schemes do not work when the local solutions are not unique.\nSpectral methods are a collection of such problems, where solutions are\northonormal bases of the leading invariant subspace of an associated data\nmatrix, which are only unique up to rotation and reflections. Here, we develop\na communication-efficient distributed algorithm for computing the leading\ninvariant subspace of a data matrix. Our algorithm uses a novel alignment\nscheme that minimizes the Procrustean distance between local solutions and a\nreference solution, and only requires a single round of communication. For the\nimportant case of principal component analysis (PCA), we show that our\nalgorithm achieves a similar error rate to that of a centralized estimator. We\npresent numerical experiments demonstrating the efficacy of our proposed\nalgorithm for distributed PCA, as well as other problems where solutions\nexhibit rotational symmetry, such as node embeddings for graph data and\nspectral initialization for quadratic sensing.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 02:11:22 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 22:43:50 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Charisopoulos", "Vasileios", ""], ["Benson", "Austin R.", ""], ["Damle", "Anil", ""]]}, {"id": "2009.02449", "submitter": "Charlene Yang", "authors": "Charlene Yang", "title": "Hierarchical Roofline Analysis: How to Collect Data using Performance\n  Tools on Intel CPUs and NVIDIA GPUs", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys a range of methods to collect necessary performance data\non Intel CPUs and NVIDIA GPUs for hierarchical Roofline analysis. As of\nmid-2020, two vendor performance tools, Intel Advisor and NVIDIA Nsight\nCompute, have integrated Roofline analysis into their supported feature set.\nThis paper fills the gap for when these tools are not available, or when users\nwould like a more customized workflow for certain analysis. Specifically, we\nwill discuss how to use Intel Advisor, RRZE LIKWID, Intel SDE and Intel\nAmplifier on Intel architectures, and nvprof, Nsight Compute metrics, and\nNsight Compute section files on NVIDIA architectures. These tools will be used\nto collect information for as many memory/cache levels in the memory hierarchy\nas possible in order to provide insights into application's data reuse and\ncache locality characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 03:14:42 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 05:27:51 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 20:23:56 GMT"}, {"version": "v4", "created": "Sun, 4 Oct 2020 17:04:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yang", "Charlene", ""]]}, {"id": "2009.02457", "submitter": "Daehyeok Kim", "authors": "Daehyeok Kim, Ankush Jain, Zaoxing Liu, George Amvrosiadis, Damian\n  Hazen, Bradley Settlemyer, Vyas Sekar", "title": "Unleashing In-network Computing on Scientific Workloads", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many recent efforts have shown that in-network computing can benefit various\ndatacenter applications. In this paper, we explore a relatively less-explored\ndomain which we argue can benefit from in-network computing: scientific\nworkloads in high-performance computing. By analyzing canonical examples of HPC\napplications, we observe unique opportunities and challenges for exploiting\nin-network computing to accelerate scientific workloads. In particular, we find\nthat the dynamic and demanding nature of scientific workloads is the major\nobstacle to the adoption of in-network approaches which are mostly open-loop\nand lack runtime feedback. In this paper, we present NSinC (Network-accelerated\nScIeNtific Computing), an architecture for fully unleashing the potential\nbenefits of in-network computing for scientific workloads by providing\nclosed-loop runtime feedback to in-network acceleration services. We outline\nkey challenges in realizing this vision and a preliminary design to enable\nacceleration for scientific applications.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 04:35:21 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kim", "Daehyeok", ""], ["Jain", "Ankush", ""], ["Liu", "Zaoxing", ""], ["Amvrosiadis", "George", ""], ["Hazen", "Damian", ""], ["Settlemyer", "Bradley", ""], ["Sekar", "Vyas", ""]]}, {"id": "2009.02511", "submitter": "Frederic Le Mouel", "authors": "David Fern\\'andez Blanco and Fr\\'ed\\'eric Le Mou\\\"el", "title": "Infrastructure de Services Cloud FaaS sur noeuds IoT", "comments": "in French. Conf\\'erence d'informatique en Parall\\'elisme,\n  Architecture et Syst\\`emes (ComPAS'2020), Lyon, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe the PyCloudIoT cloud infrastructure. PyCloudIoT\nuses a FaaS cloud computing model for offloading numerical computations on a\ncluster with resource-constrained nodes rather than powerful datacenter. This\ninfrastructure aims at exploiting unused resources of IoT nodes - already\ndeployed at the edge of the network - to reduce latency of user requests. This\nextra computation must be done without significant energy consumption - IoT\nnodes being battery-powered.\n  --\n  Dans cet article, nous d\\'ecrivons l'infrastructure cloud PyCloudIoT.\nPyCloudIoT s'appuie sur un mod\\`ele de cloud computing FaaS pour du calcul\nnum\\'erique d\\'eport\\'e vers une ferme de calcul compos\\'ee de noeuds \\`a\ncapacit\\'es restreintes au lieu d'un datacentre puissant. Cette infrastructure\nvise \\`a tirer profit des ressources inutilis\\'ees d\\'eploy\\'ees sur n{\\oe}uds\nIoT sans augmenter significativement leur consommation d'\\'energie et, en\nm\\^eme temps, \\`a rapprocher ces ressources des utilisateurs pour r\\'eduire la\nlatence, en d\\'eveloppant un mod\\`ele cloud en bord de r\\'eseau.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 10:29:59 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Blanco", "David Fern\u00e1ndez", ""], ["Mou\u00ebl", "Fr\u00e9d\u00e9ric Le", ""]]}, {"id": "2009.02643", "submitter": "Qinghua Lu", "authors": "Weishan Zhang, Qinghua Lu, Qiuyu Yu, Zhaotong Li, Yue Liu, Sin Kit Lo,\n  Shiping Chen, Xiwei Xu, Liming Zhu", "title": "Blockchain-based Federated Learning for Device Failure Detection in\n  Industrial IoT", "comments": "accepted by IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device failure detection is one of most essential problems in industrial\ninternet of things (IIoT). However, in conventional IIoT device failure\ndetection, client devices need to upload raw data to the central server for\nmodel training, which might lead to disclosure of sensitive business data.\nTherefore, in this paper, to ensure client data privacy, we propose a\nblockchain-based federated learning approach for device failure detection in\nIIoT. First, we present a platform architecture of blockchain-based federated\nlearning systems for failure detection in IIoT, which enables verifiable\nintegrity of client data. In the architecture, each client periodically creates\na Merkle tree in which each leaf node represents a client data record, and\nstores the tree root on a blockchain. Further, to address the data\nheterogeneity issue in IIoT failure detection, we propose a novel centroid\ndistance weighted federated averaging (CDW\\_FedAvg) algorithm taking into\naccount the distance between positive class and negative class of each client\ndataset. In addition, to motivate clients to participate in federated learning,\na smart contact based incentive mechanism is designed depending on the size and\nthe centroid distance of client data used in local model training. A prototype\nof the proposed architecture is implemented with our industry partner, and\nevaluated in terms of feasibility, accuracy and performance. The results show\nthat the approach is feasible, and has satisfactory accuracy and performance.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 04:03:13 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 11:26:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Weishan", ""], ["Lu", "Qinghua", ""], ["Yu", "Qiuyu", ""], ["Li", "Zhaotong", ""], ["Liu", "Yue", ""], ["Lo", "Sin Kit", ""], ["Chen", "Shiping", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2009.02742", "submitter": "Quan-Lin Li", "authors": "Heng-Li Liu, Quan-Lin Li, Chi Zhang", "title": "Matched Queues with Matching Batch Pair (m, n)", "comments": "52 Pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2001.00946", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.AI cs.DC cs.NI math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss an interesting but challenging bilateral\nstochastically matching problem: A more general matched queue with matching\nbatch pair (m, n) and two types (i.e., types A and B) of impatient customers,\nwhere the arrivals of A- and B-customers are both Poisson processes, m\nA-customers and n B-customers are matched as a group which leaves the system\nimmediately, and the customers' impatient behavior is to guarantee the\nstability of the system. We show that this matched queue can be expressed as a\nnovel bidirectional level-dependent quasi-birth-and-death (QBD) process. Based\non this, we provide a detailed analysis for this matched queue, including the\nsystem stability, the average stationary queue lengthes, and the average\nsojourn time of any A-customer or B-customer. We believe that the methodology\nand results developed in this paper can be applicable to dealing with more\ngeneral matched queueing systems, which are widely encountered in various\npractical areas, for example, sharing economy, ridesharing platform, bilateral\nmarket, organ transplantation, taxi services, assembly systems, and so on.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 14:14:47 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 16:51:22 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 09:03:29 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 07:23:08 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Heng-Li", ""], ["Li", "Quan-Lin", ""], ["Zhang", "Chi", ""]]}, {"id": "2009.02813", "submitter": "Farnaz Niknia", "authors": "Farnaz Niknia, Kiamehr Rezaee, Vesal Hakami", "title": "An SMDP-Based Approach to Thermal-Aware Task Scheduling in NoC-based\n  MPSoC platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One efficient approach to control chip-wide thermal distribution in\nmulti-core systems is the optimization of online assignments of tasks to\nprocessing cores. Online task assignment, however, faces several uncertainties\nin real-world Systems and does not show a deterministic nature. In this paper,\nwe consider the operation of a thermal-aware task scheduler, dispatching tasks\nfrom an arrival queue as well as setting the voltage and frequency of the\nprocessing cores to optimize the mean temperature margin of the entire chip\n(i.e., cores as well as the NoC routers). We model the decision process of the\ntask scheduler as a semi-Markov decision problem (SMDP). Then, to solve the\nformulated SMDP, we propose two reinforcement learning algorithms that are\ncapable of computing the optimal task assignment policy without requiring the\nstatistical knowledge of the stochastic dynamics underlying the system states.\nThe proposed algorithms also rely on function approximation techniques to\nhandle the infinite length of the task queue as well as the continuous nature\nof temperature readings. Compared to related research, the simulation results\nshow a nearly 6 Kelvin reduction in system average peak temperature and 66\nmilliseconds decrease in mean task service time.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 20:43:53 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Niknia", "Farnaz", ""], ["Rezaee", "Kiamehr", ""], ["Hakami", "Vesal", ""]]}, {"id": "2009.02845", "submitter": "Hui Li", "authors": "Yuqiu Qian, Conghui Tan, Danhao Ding, Hui Li, Nikos Mamoulis", "title": "Fast and Secure Distributed Nonnegative Matrix Factorization", "comments": "Published in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE). This arXiv version includes the appendices with additional proofs", "journal-ref": null, "doi": "10.1109/TKDE.2020.2985964", "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has been successfully applied in\nseveral data mining tasks. Recently, there is an increasing interest in the\nacceleration of NMF, due to its high cost on large matrices. On the other hand,\nthe privacy issue of NMF over federated data is worthy of attention, since NMF\nis prevalently applied in image and text analysis which may involve leveraging\nprivacy data (e.g, medical image and record) across several parties (e.g.,\nhospitals). In this paper, we study the acceleration and security problems of\ndistributed NMF. Firstly, we propose a distributed sketched alternating\nnonnegative least squares (DSANLS) framework for NMF, which utilizes a matrix\nsketching technique to reduce the size of nonnegative least squares subproblems\nwith a convergence guarantee. For the second problem, we show that DSANLS with\nmodification can be adapted to the security setting, but only for one or\nlimited iterations. Consequently, we propose four efficient distributed NMF\nmethods in both synchronous and asynchronous settings with a security\nguarantee. We conduct extensive experiments on several real datasets to show\nthe superiority of our proposed methods. The implementation of our methods is\navailable at https://github.com/qianyuqiu79/DSANLS.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 01:12:20 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Qian", "Yuqiu", ""], ["Tan", "Conghui", ""], ["Ding", "Danhao", ""], ["Li", "Hui", ""], ["Mamoulis", "Nikos", ""]]}, {"id": "2009.02858", "submitter": "Hessam Moeini", "authors": "Hessam Moeini, I-Ling Yen, Farokh Bastani", "title": "Summarization in Semantic Based Service Discovery in Dynamic IoT-Edge\n  Networks", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, many semantic-based routing protocols had been designed\nfor peer-to-peer systems. However, they are not suitable for IoT systems,\nmainly due to their high demands in memory and computing power which are not\navailable in many IoT devices. In this paper, we develop a semantic-based\nrouting protocol for dynamic IoT systems to facilitate dynamic IoT capability\ndiscovery and composition. Our protocol is a fully decentralized routing\nprotocol. To reduce the space requirement for routing, each node maintains a\nsummarized routing table. We design an ontology-based summarization algorithm\nto smartly group similar capabilities in the routing tables and support\nadaptive routing table compression. We also design an ontology coding scheme to\ncode keywords used in the routing tables and query messages. To complete the\nsummarization scheme, we consider the metrics for choosing the summarization\ncandidates in an overflowing routing table. Some of these metrics are novel and\nare difficult to measure, such as coverage and stability. Our solutions\nsignificantly reduce the routing table size, ensuring that the routing table\nsize can be bounded by the available memory of the IoT devices, while\nsupporting efficient IoT capability lookup. Experimental results show that our\napproach can yield significantly lower network traffic and memory requirement\nfor IoT capability lookup when compared with existing semantic-based routing\nalgorithms including a centralized solution, a DHT-based approach, a controlled\nflooding scheme, and a cache-based solution.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 02:16:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Moeini", "Hessam", ""], ["Yen", "I-Ling", ""], ["Bastani", "Farokh", ""]]}, {"id": "2009.03066", "submitter": "Jaume Bosch", "authors": "Jaume Bosch, Carlos \\'Alvarez, Daniel Jim\\'enez-Gonz\\'alez, Xavier\n  Martorell, Eduard Ayguad\\'e", "title": "Asynchronous Runtime with Distributed Manager for Task-based Programming\n  Models", "comments": "2020 Parallel Computing", "journal-ref": "Parallel Computing, Volume 97, 2020", "doi": "10.1016/j.parco.2020.102664", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel task-based programming models, like OpenMP, allow application\ndevelopers to easily create a parallel version of their sequential codes. The\nstandard OpenMP 4.0 introduced the possibility of describing a set of data\ndependences per task that the runtime uses to order the tasks execution. This\norder is calculated using shared graphs, which are updated by all threads in\nexclusive access using synchronization mechanisms (locks) to ensure the\ndependence management correctness. The contention in the access to these\nstructures becomes critical in many-core systems because several threads may be\nwasting computation resources waiting their turn.\n  This paper proposes an asynchronous management of the runtime structures,\nlike task dependence graphs, suitable for task-based programming model\nruntimes. In such organization, the threads request actions to the runtime\ninstead of doing them directly. The requests are then handled by a distributed\nruntime manager (DDAST) which does not require dedicated resources. Instead,\nthe manager uses the idle threads to modify the runtime structures. The paper\nalso presents an implementation, analysis and performance evaluation of such\nruntime organization. The performance results show that the proposed\nasynchronous organization outperforms the speedup obtained by the original\nruntime for different benchmarks and different many-core architectures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 12:46:25 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 06:21:12 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bosch", "Jaume", ""], ["\u00c1lvarez", "Carlos", ""], ["Jim\u00e9nez-Gonz\u00e1lez", "Daniel", ""], ["Martorell", "Xavier", ""], ["Ayguad\u00e9", "Eduard", ""]]}, {"id": "2009.03190", "submitter": "Zhengchun Liu", "authors": "Zhengchun Liu, Rajkumar Kettimuthu, Joaquin Chung, Rachana\n  Ananthakrishnan, Michael Link, Ian Foster", "title": "Design and Evaluation of a Simple Data Interface for Efficient Data\n  Transfer Across Diverse Storage", "comments": null, "journal-ref": "ACM Transactions on Modeling and Performance Evaluation of\n  Computing Systems 2021", "doi": "10.1145/3452007", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern science and engineering computing environments often feature storage\nsystems of different types, from parallel file systems in high-performance\ncomputing centers to object stores operated by cloud providers. To enable easy,\nreliable, secure, and performant data exchange among these different systems,\nwe propose Connector, a pluggable data access architecture for diverse,\ndistributed storage. By abstracting low-level storage system details, this\nabstraction permits a managed data transfer service (Globus in our case) to\ninteract with a large and easily extended set of storage systems. Equally\nimportant, it supports third-party transfers: that is, direct data transfers\nfrom source to destination that are initiated by a third-party client but do\nnot engage that third party in the data path. The abstraction also enables\nmanagement of transfers for performance optimization, error handling, and\nend-to-end integrity. We present the Connector design, describe implementations\nfor different storage services, evaluate tradeoffs inherent in managed vs.\\\ndirect transfers, motivate recommended deployment options, and propose a\nperformance model-based method that allows for easy characterization of\nperformance in different contexts without exhaustive benchmarking.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:52:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Zhengchun", ""], ["Kettimuthu", "Rajkumar", ""], ["Chung", "Joaquin", ""], ["Ananthakrishnan", "Rachana", ""], ["Link", "Michael", ""], ["Foster", "Ian", ""]]}, {"id": "2009.03254", "submitter": "Will Usher", "authors": "Will Usher and Valerio Pascucci", "title": "Interactive Visualization of Terascale Data in the Browser: Fact or\n  Fiction?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information visualization applications have become ubiquitous, in no small\npart thanks to the ease of wide distribution and deployment to users enabled by\nthe web browser. Scientific visualization applications, relying on native code\nlibraries and parallel processing, have been less suited to such widespread\ndistribution, as browsers do not provide the required libraries or compute\ncapabilities. In this paper, we revisit this gap in visualization technologies\nand explore how new web technologies, WebAssembly and WebGPU, can be used to\ndeploy powerful visualization solutions for large-scale scientific data in the\nbrowser. In particular, we evaluate the programming effort required to bring\nscientific visualization applications to the browser through these technologies\nand assess their competitiveness against classic native solutions. As a main\nexample, we present a new GPU-driven isosurface extraction method for\nblock-compressed data sets, that is suitable for interactive isosurface\ncomputation on large volumes in resource-constrained environments, such as the\nbrowser. We conclude that web browsers are on the verge of becoming a\ncompetitive platform for even the most demanding scientific visualization\ntasks, such as interactive visualization of isosurfaces from a 1TB DNS\nsimulation. We call on researchers and developers to consider investing in a\ncommunity software stack to ease use of these upcoming browser features to\nbring accessible scientific visualization to the browser.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:24:45 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Usher", "Will", ""], ["Pascucci", "Valerio", ""]]}, {"id": "2009.03368", "submitter": "Mengjiao Han", "authors": "Mengjiao Han, Ingo Wald, Will Usher, Nate Morrical, Aaron Knoll,\n  Valerio Pascucci, Chris R. Johnson", "title": "A Virtual Frame Buffer Abstraction for Parallel Rendering of Large Tiled\n  Display Walls", "comments": "5 pages, IEEE Vis 2020 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present dw2, a flexible and easy-to-use software infrastructure for\ninteractive rendering of large tiled display walls. Our library represents the\ntiled display wall as a single virtual screen through a display \"service\",\nwhich renderers connect to and send image tiles to be displayed, either from an\non-site or remote cluster. The display service can be easily configured to\nsupport a range of typical network and display hardware configurations; the\nclient library provides a straightforward interface for easy integration into\nexisting renderers. We evaluate the performance of our display wall service in\ndifferent configurations using a CPU and GPU ray tracer, in both on-site and\nremote rendering scenarios using multiple display walls\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 18:45:46 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Han", "Mengjiao", ""], ["Wald", "Ingo", ""], ["Usher", "Will", ""], ["Morrical", "Nate", ""], ["Knoll", "Aaron", ""], ["Pascucci", "Valerio", ""], ["Johnson", "Chris R.", ""]]}, {"id": "2009.03518", "submitter": "AKM Mubashwir Alam", "authors": "A K M Mubashwir Alam, Sagar Sharma, Keke Chen", "title": "SGX-MR: Regulating Dataflows for Protecting Access Patterns of\n  Data-Intensive SGX Applications", "comments": "To appear in Privacy Enhancing Technologies Symposium, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intel SGX has been a popular trusted execution environment (TEE) for\nprotecting the integrity and confidentiality of applications running on\nuntrusted platforms such as cloud. However, the access patterns of SGX-based\nprograms can still be observed by adversaries, which may leak important\ninformation for successful attacks. Researchers have been experimenting with\nOblivious RAM (ORAM) to address the privacy of access patterns. ORAM is a\npowerful low-level primitive that provides application-agnostic protection for\nany I/O operations, however, at a high cost. We find that some\napplication-specific access patterns, such as sequential block I/O, do not\nprovide additional information to adversaries. Others, such as sorting, can be\nreplaced with specific oblivious algorithms that are more efficient than ORAM.\nThe challenge is that developers may need to look into all the details of\napplication-specific access patterns to design suitable solutions, which is\ntime-consuming and error-prone. In this paper, we present the lightweight SGX\nbased MapReduce (SGX-MR) approach that regulates the dataflow of data-intensive\nSGX applications for easier application-level access-pattern analysis and\nprotection. It uses the MapReduce framework to cover a large class of\ndata-intensive applications, and the entire framework can be implemented with a\nsmall memory footprint. With this framework, we have examined the stages of\ndata processing, identified the access patterns that need protection, and\ndesigned corresponding efficient protection methods. Our experiments show that\nSGX-MR based applications are much more efficient than ORAM-based\nimplementations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 04:53:10 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Alam", "A K M Mubashwir", ""], ["Sharma", "Sagar", ""], ["Chen", "Keke", ""]]}, {"id": "2009.03585", "submitter": "Junya Nakamura", "authors": "Junya Nakamura and Masahiro Shibata and Yuichi Sudo and Yonghwan Kim", "title": "Self-Stabilizing Construction of a Minimal Weakly\n  $\\mathcal{ST}$-Reachable Directed Acyclic Graph", "comments": null, "journal-ref": "Proceedings of the 39th International Symposium on Reliable\n  Distributed Systems (SRDS), 2020, 1-10", "doi": "10.1109/SRDS51746.2020.00008", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-stabilizing algorithm to construct a minimal weakly\n$\\mathcal{ST}$-reachable directed acyclic graph (DAG), which is suited for\nrouting messages on wireless networks. Given an arbitrary, simple, connected,\nand undirected graph $G=(V, E)$ and two sets of nodes, senders $\\mathcal{S}\n(\\subset V)$ and targets $\\mathcal{T} (\\subset V)$, a directed subgraph\n$\\vec{G}$ of $G$ is a weakly $\\mathcal{ST}$-reachable DAG on $G$, if $\\vec{G}$\nis a DAG and every sender can reach at least one target, and every target is\nreachable from at least one sender in $\\vec{G}$. We say that a weakly\n$\\mathcal{ST}$-reachable DAG $\\vec{G}$ on $G$ is minimal if any proper subgraph\nof $\\vec{G}$ is no longer a weakly $\\mathcal{ST}$-reachable DAG. This DAG is a\nrelaxed version of the original (or strongly) $\\mathcal{ST}$-reachable DAG,\nwhere every target is reachable from every sender. This is because a strongly\n$\\mathcal{ST}$-reachable DAG $G$ does not always exist; some graph has no\nstrongly $\\mathcal{ST}$-reachable DAG even in the case\n$|\\mathcal{S}|=|\\mathcal{T}|=2$. On the other hand, the proposed algorithm\nalways constructs a weakly $\\mathcal{ST}$-reachable DAG for any $|\\mathcal{S}|$\nand $|\\mathcal{T}|$. Furthermore, the proposed algorithm is self-stabilizing;\neven if the constructed DAG deviates from the reachability requirement by a\nbreakdown or exhausting the battery of a node having an arc in the DAG, this\nalgorithm automatically reconstructs the DAG to satisfy the requirement again.\nThe convergence time of the algorithm is $O(D)$ asynchronous rounds, where $D$\nis the diameter of a given graph. We conduct small simulations to evaluate the\nperformance of the proposed algorithm. The simulation result indicates that its\nexecution time decreases when the number of sender nodes or target nodes is\nlarge.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 08:52:04 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 04:25:15 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Nakamura", "Junya", ""], ["Shibata", "Masahiro", ""], ["Sudo", "Yuichi", ""], ["Kim", "Yonghwan", ""]]}, {"id": "2009.03598", "submitter": "Minxian Xu", "authors": "Minxian Xu, Chengxi Gao, Shashikant Ilager, Huaming Wu, Chengzhong Xu,\n  Rajkumar Buyya", "title": "Green-aware Mobile Edge Computing for IoT: Challenges, Solutions and\n  Future Directions", "comments": "19 pages, 2 figures, 2 tables, to be appeared in Mobile Edge\n  Computing book published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of Internet of Things (IoT) technology enables the rapid\ngrowth of connected smart devices and mobile applications. However, due to the\nconstrained resources and limited battery capacity, there are bottlenecks when\nutilizing the smart devices. Mobile edge computing (MEC) offers an attractive\nparadigm to handle this challenge. In this work, we concentrate on the MEC\napplication for IoT and deal with the energy saving objective via offloading\nworkloads between cloud and edge. In this regard, we firstly identify the\nenergy-related challenges in MEC. Then we present a green-aware framework for\nMEC to address the energy-related challenges, and provide a generic model\nformulation for the green MEC. We also discuss some state-of-the-art workloads\noffloading approaches to achieve green IoT and compare them in comprehensive\nperspectives. Finally, some future research directions related to energy\nefficiency in MEC are given.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 09:19:20 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Xu", "Minxian", ""], ["Gao", "Chengxi", ""], ["Ilager", "Shashikant", ""], ["Wu", "Huaming", ""], ["Xu", "Chengzhong", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2009.03816", "submitter": "Qing Ye", "authors": "Qing Ye, Yuxuan Han, Yanan sun and JIancheng Lv", "title": "PSO-PS: Parameter Synchronization with Particle Swarm Optimization for\n  Distributed Training of Deep Neural Networks", "comments": "7pages", "journal-ref": "IJCNN2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter updating is an important stage in parallelism-based distributed\ndeep learning. Synchronous methods are widely used in distributed training the\nDeep Neural Networks (DNNs). To reduce the communication and synchronization\noverhead of synchronous methods, decreasing the synchronization frequency\n(e.g., every $n$ mini-batches) is a straightforward approach. However, it often\nsuffers from poor convergence. In this paper, we propose a new algorithm of\nintegrating Particle Swarm Optimization (PSO) into the distributed training\nprocess of DNNs to automatically compute new parameters. In the proposed\nalgorithm, a computing work is encoded by a particle, the weights of DNNs and\nthe training loss are modeled by the particle attributes. At each\nsynchronization stage, the weights are updated by PSO from the sub weights\ngathered from all workers, instead of averaging the weights or the gradients.\nTo verify the performance of the proposed algorithm, the experiments are\nperformed on two commonly used image classification benchmarks: MNIST and\nCIFAR10, and compared with the peer competitors at multiple different\nsynchronization configurations. The experimental results demonstrate the\ncompetitiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 05:18:32 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Ye", "Qing", ""], ["Han", "Yuxuan", ""], ["sun", "Yanan", ""], ["Lv", "JIancheng", ""]]}, {"id": "2009.03987", "submitter": "Thorsten G\\\"otte", "authors": "Thorsten G\\\"otte, Kristian Hinnenthal, Christian Scheideler, Julian\n  Werthmann", "title": "Time-Optimal Construction of Overlay Networks", "comments": "39 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to construct an overlay network of constant degree and diameter\n$O(\\log n)$ in time $O(\\log n)$ starting from an arbitrary weakly connected\ngraph. We assume a synchronous communication network in which nodes can send\nmessages to nodes they know the identifier of, and new connections can be\nestablished by sending node identifiers. If the initial network's graph is\nweakly connected and has constant degree, then our algorithm constructs the\ndesired topology with each node sending and receiving only $O(\\log n)$ messages\nin each round in time $O(\\log n)$, w.h.p., which beats the currently best\n$O(\\log^{3/2} n)$ time algorithm of [G\\\"otte et al., SIROCCO'19]. Since the\nproblem cannot be solved faster than by using pointer jumping for $O(\\log n)$\nrounds (which would even require each node to communicate $\\Omega(n)$ bits),\nour algorithm is asymptotically optimal. We achieve this speedup by using short\nrandom walks to repeatedly establish random connections between the nodes that\nquickly reduce the conductance of the graph using an observation of [Kwok and\nLau, APPROX'14]. Additionally, we show how our algorithm can be used to\nefficiently solve graph problems in \\emph{hybrid networks} [Augustine et al.,\nSODA'20]. Motivated by the idea that nodes possess two different modes of\ncommunication, we assume that communication of the \\emph{initial} edges is\nunrestricted, whereas only polylogarithmically many messages can be\ncommunicated over edges that have been established throughout an algorithm's\nexecution. For an (undirected) graph $G$ with arbitrary degree, we show how to\ncompute connected components, a spanning tree, and biconnected components in\ntime $O(\\log n)$, w.h.p. Furthermore, we show how to compute an MIS in time\n$O(\\log d + \\log \\log n)$, w.h.p., where $d$ is the initial degree of $G$.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 20:36:05 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 17:10:47 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["G\u00f6tte", "Thorsten", ""], ["Hinnenthal", "Kristian", ""], ["Scheideler", "Christian", ""], ["Werthmann", "Julian", ""]]}, {"id": "2009.04061", "submitter": "Keren Zhou", "authors": "Keren Zhou, Xiaozhu Meng, Ryuichi Sai, John Mellor-Crummey", "title": "GPA: A GPU Performance Advisor Based on Instruction Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient GPU kernels can be difficult because of the complexity\nof GPU architectures and programming models. Existing performance tools only\nprovide coarse-grained suggestions at the kernel level, if any. In this paper,\nwe describe GPA, a performance advisor for NVIDIA GPUs that suggests potential\ncode optimization opportunities at a hierarchy of levels, including individual\nlines, loops, and functions. To relieve users of the burden of interpreting\nperformance counters and analyzing bottlenecks, GPA uses data flow analysis to\napproximately attribute measured instruction stalls to their root causes and\nuses information about a program's structure and the GPU to match inefficiency\npatterns with suggestions for optimization. To quantify each suggestion's\npotential benefits, we developed PC sampling-based performance models to\nestimate its speedup. Our experiments with benchmarks and applications show\nthat GPA provides an insightful report to guide performance optimization. Using\nGPA, we obtained speedups on a Volta V100 GPU ranging from 1.01$\\times$ to\n3.53$\\times$, with a geometric mean of 1.22$\\times$.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 01:35:08 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 17:05:59 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zhou", "Keren", ""], ["Meng", "Xiaozhu", ""], ["Sai", "Ryuichi", ""], ["Mellor-Crummey", "John", ""]]}, {"id": "2009.04199", "submitter": "Philipp H. Kindt", "authors": "Philipp H. Kindt, Swaminathan Narayanaswamy, Marco Saur, Samarjit\n  Chakraborty", "title": "Optimizing BLE-Like Neighbor Discovery", "comments": "To appear in the IEEE Transactions on Mobile Computing (TMC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighbor discovery (ND) protocols are used for establishing a first contact\nbetween multiple wireless devices. The energy consumption and discovery latency\nof this procedure are determined by the parametrization of the protocol. In\nmost existing protocols, reception and transmission are temporally coupled.\nSuch schemes are referred to as \\textit{slotted}, for which the problem of\nfinding optimized parametrizations has been studied thoroughly in the\nliterature. However, slotted approaches are not efficient in applications in\nwhich new devices join the network gradually and only the joining devices and a\nmaster node need to run the ND protocol simultaneously. For example, this is\ntypically the case in IoT scenarios or Bluetooth Low Energy (BLE) piconets.\nHere, protocols in which packets are transmitted with periodic intervals (PI)\ncan achieve significantly lower worst-case latencies than slotted ones. For\nthis class of protocols, optimal parameter values remain unknown. To address\nthis, we propose an optimization framework for PI-based BLE-like protocols,\nwhich translates any specified duty-cycle (and therefore energy budget) into a\nset of optimized parameter values. We show that the parametrizations resulting\nfrom one variant of our proposed scheme are optimal when one receiver discovers\none transmitter, and no other parametrization or ND protocol - neither slotted\nnor slotless - can guarantee lower discovery latencies for a given duty-cycle\nin this scenario. Since the resulting protocol utilizes the channel more\naggressively than other ND protocols, beacons will collide more frequently.\nHence, due to collisions, the rate of successful discoveries gracefully\ndecreases for larger numbers of devices discovering each other simultaneously.\nWe also propose a scheme for configuring the BLE protocol (and not just\nBLE-\\textit{like} protocols).\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 10:33:29 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Kindt", "Philipp H.", ""], ["Narayanaswamy", "Swaminathan", ""], ["Saur", "Marco", ""], ["Chakraborty", "Samarjit", ""]]}, {"id": "2009.04373", "submitter": "Huan Li", "authors": "Huan Li, Zhouchen Lin, Yongchun Fang", "title": "Variance Reduced EXTRA and DIGing and Their Optimal Acceleration for\n  Strongly Convex Decentralized Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study stochastic decentralized optimization for the problem of training\nmachine learning models with large-scale distributed data. We extend the widely\nused EXTRA and DIGing methods with variance reduction (VR), and propose two\nmethods: VR-EXTRA and VR-DIGing. The proposed VR-EXTRA requires the time of\n$O((\\kappa_s+n)\\log\\frac{1}{\\epsilon})$ stochastic gradient evaluations and\n$O((\\kappa_b+\\kappa_c)\\log\\frac{1}{\\epsilon})$ communication rounds to reach\nprecision $\\epsilon$, where $\\kappa_s$ and $\\kappa_b$ are the stochastic\ncondition number and batch condition number for strongly convex and smooth\nproblems, respectively, $\\kappa_c$ is the condition number of the communication\nnetwork, and $n$ is the sample size on each distributed node. The proposed\nVR-DIGing has a little higher communication cost of\n$O((\\kappa_b+\\kappa_c^2)\\log\\frac{1}{\\epsilon})$. Our stochastic gradient\ncomputation complexities are the same as the ones of single-machine VR methods,\nsuch as SAG, SAGA, and SVRG, and our communication complexities keep the same\nas those of EXTRA and DIGing, respectively. To further speed up the\nconvergence, we also propose the accelerated VR-EXTRA and VR-DIGing with both\nthe optimal $O((\\sqrt{n\\kappa_s}+n)\\log\\frac{1}{\\epsilon})$ stochastic gradient\ncomputation complexity and $O(\\sqrt{\\kappa_b\\kappa_c}\\log\\frac{1}{\\epsilon})$\ncommunication complexity. Our stochastic gradient computation complexity is\nalso the same as the ones of single-machine accelerated VR methods, such as\nKatyusha, and our communication complexity keeps the same as those of\naccelerated full batch decentralized methods, such as MSDA.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 15:48:44 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 13:09:53 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Huan", ""], ["Lin", "Zhouchen", ""], ["Fang", "Yongchun", ""]]}, {"id": "2009.04403", "submitter": "Devang Jhabakh Jai", "authors": "Devang Jhabakh Jai, Sudeep Das", "title": "Learning Slab Classes to Alleviate Memory Holes in Memcached", "comments": "5 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of memory holes in slab allocators, where an item\nentered into memory occupies more memory than it actually requires due to a\ndifference between the nearest larger slab class size and the size of the\nentered item. We solve this problem by using a greedy algorithm that analyses\nthe pattern of the sizes of items previously entered into the memory and\naccordingly re-configuring the default slab classes to better suit the learned\ntraffic pattern to minimize memory holes. Using this approach for a consistent\ndata pattern, in our findings, has yielded significant reductions in memory\nwastage. We consider Memcached as it is one of the most widely used\nimplementations of slab allocators today, and has native support to reconfigure\nits default slab classes.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 16:26:11 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Jai", "Devang Jhabakh", ""], ["Das", "Sudeep", ""]]}, {"id": "2009.04509", "submitter": "Nhan Tran", "authors": "Michael Wang, Tingjun Yang, Maria Acosta Flechas, Philip Harris,\n  Benjamin Hawks, Burt Holzman, Kyle Knoepfel, Jeffrey Krupa, Kevin Pedro, Nhan\n  Tran", "title": "GPU-accelerated machine learning inference as a service for computing in\n  neutrino experiments", "comments": "15 pages, 7 figures, 2 tables", "journal-ref": null, "doi": "10.3389/fdata.2020.604083", "report-no": "FERMILAB-PUB-20-428-ND-SCD", "categories": "physics.comp-ph cs.DC hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are becoming increasingly prevalent and\nperformant in the reconstruction of events in accelerator-based neutrino\nexperiments. These sophisticated algorithms can be computationally expensive.\nAt the same time, the data volumes of such experiments are rapidly increasing.\nThe demand to process billions of neutrino events with many machine learning\nalgorithm inferences creates a computing challenge. We explore a computing\nmodel in which heterogeneous computing with GPU coprocessors is made available\nas a web service. The coprocessors can be efficiently and elastically deployed\nto provide the right amount of computing for a given processing task. With our\napproach, Services for Optimized Network Inference on Coprocessors (SONIC), we\nintegrate GPU acceleration specifically for the ProtoDUNE-SP reconstruction\nchain without disrupting the native computing workflow. With our integrated\nframework, we accelerate the most time-consuming task, track and particle\nshower hit identification, by a factor of 17. This results in a factor of 2.7\nreduction in the total processing time when compared with CPU-only production.\nFor this particular task, only 1 GPU is required for every 68 CPU threads,\nproviding a cost-effective solution.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 18:37:47 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 15:45:16 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wang", "Michael", ""], ["Yang", "Tingjun", ""], ["Flechas", "Maria Acosta", ""], ["Harris", "Philip", ""], ["Hawks", "Benjamin", ""], ["Holzman", "Burt", ""], ["Knoepfel", "Kyle", ""], ["Krupa", "Jeffrey", ""], ["Pedro", "Kevin", ""], ["Tran", "Nhan", ""]]}, {"id": "2009.04552", "submitter": "Youguang Chen", "authors": "Youguang Chen, William Ruys, George Biros", "title": "KNN-DBSCAN: a DBSCAN in high dimensions", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental task in machine learning. One of the most\nsuccessful and broadly used algorithms is DBSCAN, a density-based clustering\nalgorithm. DBSCAN requires $\\epsilon$-nearest neighbor graphs of the input\ndataset, which are computed with range-search algorithms and spatial data\nstructures like KD-trees. Despite many efforts to design scalable\nimplementations for DBSCAN, existing work is limited to low-dimensional\ndatasets, as constructing $\\epsilon$-nearest neighbor graphs is expensive in\nhigh-dimensions. In this paper, we modify DBSCAN to enable use of\n$\\kappa$-nearest neighbor graphs of the input dataset. The $\\kappa$-nearest\nneighbor graphs are constructed using approximate algorithms based on\nrandomized projections. Although these algorithms can become inaccurate or\nexpensive in high-dimensions, they possess a much lower memory overhead than\nconstructing $\\epsilon$-nearest neighbor graphs ($\\mathcal{O}(nk)$ vs.\n$\\mathcal{O}(n^2)$). We delineate the conditions under which $k$NN-DBSCAN\nproduces the same clustering as DBSCAN. We also present an efficient parallel\nimplementation of the overall algorithm using OpenMP for shared memory and MPI\nfor distributed memory parallelism. We present results on up to 16 billion\npoints in 20 dimensions, and perform weak and strong scaling studies using\nsynthetic data. Our code is efficient in both low and high dimensions. We can\ncluster one billion points in 3D in less than one second on 28K cores on the\nFrontera system at the Texas Advanced Computing Center (TACC). In our largest\nrun, we cluster 65 billion points in 20 dimensions in less than 40 seconds\nusing 114,688 x86 cores on TACC's Frontera system. Also, we compare with a\nstate of the art parallel DBSCAN code; on 20d/4M point dataset, our code is up\nto 37$\\times$ faster.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 20:22:54 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chen", "Youguang", ""], ["Ruys", "William", ""], ["Biros", "George", ""]]}, {"id": "2009.04561", "submitter": "Aakash Sharma", "authors": "Aakash Sharma, Saravanan Dhakshinamurthy, George Kesidis, Chita R. Das", "title": "CASH: A Credit Aware Scheduling for Public Cloud Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The public cloud offers a myriad of services which allows its tenants to\nprocess large scale big data in a flexible, easy and cost effective manner.\nTenants generally use large scale data processing frameworks such as MapReduce,\nTez, Spark etc. to process their data. Tenants can configure their frameworks\nto run individual tasks by the framework itself or have a middleware cluster\nmanager like YARN or Mesos to arbitrate resource scheduling in their\npublic-cloud cluster. Cluster managers need to be cognizant about the workload\nrequirement along with the state of the individual resource such as CPU and\ndisk in the cluster. Cloud providers use a token bucket mechanism for their\nindividual hardware resources as an indicator of the quality-of-service that\nindividual hardware resource can provide. In this paper, through our changes in\nYARN, Hadoop and Tez, we show how middleware cluster managers can be made\ncognizant about the expected quality-of-service of individual hardware\nresources in the cluster. Our optimized cluster manager with a coarse grained\nknowledge of task requirement and fine grained knowledge of expected\nquality-of-service of hardware resources in the cluster performs highly optimal\ntask placements. Our experiments with our optimizations show CPU credit based\ninstances like the Amazon T3 instances as a viable cost effective option for\nrunning bigdata workloads. We also show that streaming SQL queries on a Hive\nwarehouse can be accelerated by up to 31% leading to public cloud cost savings\nof up to 22%.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 20:40:01 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 15:48:19 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Sharma", "Aakash", ""], ["Dhakshinamurthy", "Saravanan", ""], ["Kesidis", "George", ""], ["Das", "Chita R.", ""]]}, {"id": "2009.04598", "submitter": "Charlene Yang", "authors": "Yunsong Wang, Charlene Yang, Steven Farrell, Yan Zhang, Thorsten\n  Kurth, Samuel Williams", "title": "Time-Based Roofline for Deep Learning Performance Analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning applications are usually very compute-intensive and require a\nlong run time for training and inference. This has been tackled by researchers\nfrom both hardware and software sides, and in this paper, we propose a\nRoofline-based approach to performance analysis to facilitate the optimization\nof these applications. This approach is an extension of the Roofline model\nwidely used in traditional high-performance computing applications, and it\nincorporates both compute/bandwidth complexity and run time in its formulae to\nprovide insights into deep learning-specific characteristics. We take two sets\nof representative kernels, 2D convolution and long short-term memory, to\nvalidate and demonstrate the use of this new approach, and investigate how\narithmetic intensity, cache locality, auto-tuning, kernel launch overhead, and\nTensor Core usage can affect performance. Compared to the common ad-hoc\napproach, this study helps form a more systematic way to analyze code\nperformance and identify optimization opportunities for deep learning\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 23:29:04 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 07:11:36 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 21:51:45 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Wang", "Yunsong", ""], ["Yang", "Charlene", ""], ["Farrell", "Steven", ""], ["Zhang", "Yan", ""], ["Kurth", "Thorsten", ""], ["Williams", "Samuel", ""]]}, {"id": "2009.04611", "submitter": "Xikui Wang", "authors": "Xikui Wang, Michael J. Carey, Vassilis J. Tsotras", "title": "Subscribing to Big Data at Scale", "comments": "36 pages, 47 figures, submitted to TOCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, data is being actively generated by a variety of devices, services,\nand applications. Such data is important not only for the information that it\ncontains, but also for its relationships to other data and to interested users.\nMost existing Big Data systems focus on passively answering queries from users,\nrather than actively collecting data, processing it, and serving it to users.\nTo satisfy both passive and active requests at scale, users need either to\nheavily customize an existing passive Big Data system or to glue multiple\nsystems together. Either choice would require significant effort from users and\nincur additional overhead. In this paper, we present the BAD (Big Active Data)\nsystem, which is designed to preserve the merits of passive Big Data systems\nand introduce new features for actively serving Big Data to users at scale. We\nshow the design and implementation of the BAD system, demonstrate how BAD\nfacilitates providing both passive and active data services, investigate the\nBAD system's performance at scale, and illustrate the complexities that would\nresult from instead providing BAD-like services with a \"glued\" system.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 00:04:53 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Wang", "Xikui", ""], ["Carey", "Michael J.", ""], ["Tsotras", "Vassilis J.", ""]]}, {"id": "2009.04619", "submitter": "Ryuichi Sai", "authors": "Ryuichi Sai, John Mellor-Crummey, Xiaozhu Meng, Mauricio Araya-Polo,\n  Jie Meng", "title": "Accelerating High-Order Stencils on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil computations are widely used in HPC applications. Today, many HPC\nplatforms use GPUs as accelerators. As a result, understanding how to perform\nstencil computations fast on GPUs is important. While implementation strategies\nfor low-order stencils on GPUs have been well-studied in the literature, not\nall of proposed enhancements work well for high-order stencils, such as those\nused for seismic modeling. Furthermore, coping with boundary conditions often\nrequires different computational logic, which complicates efficient\nexploitation of the thread-level parallelism on GPUs. In this paper, we study\nhigh-order stencils and their unique characteristics on GPUs. We manually\ncrafted a collection of implementations of a 25-point seismic modeling stencil\nin CUDA and related boundary conditions. We evaluate their code shapes, memory\nhierarchy usage, data-fetching patterns, and other performance attributes. We\nconducted an empirical evaluation of these stencils using several mature and\nemerging tools and discuss our quantitative findings. Among our\nimplementations, we achieve twice the performance of a proprietary code\ndeveloped in C and mapped to GPUs using OpenACC. Additionally, several of our\nimplementations have excellent performance portability.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 00:59:39 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 11:37:31 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Sai", "Ryuichi", ""], ["Mellor-Crummey", "John", ""], ["Meng", "Xiaozhu", ""], ["Araya-Polo", "Mauricio", ""], ["Meng", "Jie", ""]]}, {"id": "2009.04739", "submitter": "Kostas Kolomvatsos", "authors": "Kostas Kolomvatsos", "title": "A Probabilistic Approach for Data Management in Pervasive Computing\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current advances in Pervasive Computing (PC) involve the adoption of the huge\ninfrastructures of the Internet of Things (IoT) and the Edge Computing (EC).\nBoth, IoT and EC, can support innovative applications around end users to\nfacilitate their activities. Such applications are built upon the collected\ndata and the appropriate processing demanded in the form of requests. To limit\nthe latency, instead of relying on Cloud for data storage and processing, the\nresearch community provides a number of models for data management at the EC.\nRequests, usually defined in the form of tasks or queries, demand the\nprocessing of specific data. A model for pre-processing the data preparing them\nand detecting their statistics before requests arrive is necessary. In this\npaper, we propose a promising and easy to implement scheme for selecting the\nappropriate host of the incoming data based on a probabilistic approach. Our\naim is to store similar data in the same distributed datasets to have,\nbeforehand, knowledge on their statistics while keeping their solidity at high\nlevels. As solidity, we consider the limited statistical deviation of data,\nthus, we can support the storage of highly correlated data in the same dataset.\nAdditionally, we propose an aggregation mechanism for outliers detection\napplied just after the arrival of data. Outliers are transferred to Cloud for\nfurther processing. When data are accepted to be locally stored, we propose a\nmodel for selecting the appropriate datasets where they will be replicated for\nbuilding a fault tolerant system. We analytically describe our model and\nevaluate it through extensive simulations presenting its pros and cons.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:09:22 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Kolomvatsos", "Kostas", ""]]}, {"id": "2009.04755", "submitter": "Stijn Heldens", "authors": "Stijn Heldens, Pieter Hijma, Ben van Werkhoven, Jason Maassen, Henri\n  Bal, Rob van Nieuwpoort", "title": "Rocket: Efficient and Scalable All-Pairs Computations on Heterogeneous\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  All-pairs compute problems apply a user-defined function to each combination\nof two items of a given data set. Although these problems present an abundance\nof parallelism, data reuse must be exploited to achieve good performance.\nSeveral researchers considered this problem, either resorting to partial\nreplication with static work distribution or dynamic scheduling with full\nreplication. In contrast, we present a solution that relies on hierarchical\nmulti-level software-based caches to maximize data reuse at each level in the\ndistributed memory hierarchy, combined with a divide-and-conquer approach to\nexploit data locality, hierarchical work-stealing to dynamically balance the\nworkload, and asynchronous processing to maximize resource utilization. We\nevaluate our solution using three real-world applications (from digital\nforensics, localization microscopy, and bioinformatics) on different platforms\n(from a desktop machine to a supercomputer). Results shows excellent efficiency\nand scalability when scaling to 96 GPUs, even obtaining super-linear speedups\ndue to a distributed cache.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:49:34 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Heldens", "Stijn", ""], ["Hijma", "Pieter", ""], ["van Werkhoven", "Ben", ""], ["Maassen", "Jason", ""], ["Bal", "Henri", ""], ["van Nieuwpoort", "Rob", ""]]}, {"id": "2009.04981", "submitter": "Mattia Bianchi", "authors": "Mattia Bianchi and Sergio Grammatico", "title": "Nash equilibrium seeking under partial-decision information over\n  directed communication networks", "comments": "To appear in the 59th Conference on Decision and Control (CDC 2020)", "journal-ref": null, "doi": "10.1109/CDC42340.2020.9304267", "report-no": null, "categories": "math.OC cs.DC cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Nash equilibrium problem in a partial-decision information\nscenario. Specifically, each agent can only receive information from some\nneighbors via a communication network, while its cost function depends on the\nstrategies of possibly all agents. In particular, while the existing methods\nassume undirected or balanced communication, in this paper we allow for\nnon-balanced, directed graphs. We propose a fully-distributed pseudo-gradient\nscheme, which is guaranteed to converge with linear rate to a Nash equilibrium,\nunder strong monotonicity and Lipschitz continuity of the game mapping. Our\nalgorithm requires global knowledge of the communication structure, namely of\nthe Perron-Frobenius eigenvector of the adjacency matrix and of a certain\nconstant related to the graph connectivity. Therefore, we adapt the procedure\nto setups where the network is not known in advance, by computing the\neigenvector online and by means of vanishing step sizes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:44:04 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bianchi", "Mattia", ""], ["Grammatico", "Sergio", ""]]}, {"id": "2009.04987", "submitter": "Joachim Neu", "authors": "Joachim Neu, Ertem Nusret Tas, David Tse", "title": "Ebb-and-Flow Protocols: A Resolution of the Availability-Finality\n  Dilemma", "comments": "Forthcoming in IEEE Symposium on Security and Privacy 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CAP theorem says that no blockchain can be live under dynamic\nparticipation and safe under temporary network partitions. To resolve this\navailability-finality dilemma, we formulate a new class of flexible consensus\nprotocols, ebb-and-flow protocols, which support a full dynamically available\nledger in conjunction with a finalized prefix ledger. The finalized ledger\nfalls behind the full ledger when the network partitions but catches up when\nthe network heals. Gasper, the current candidate protocol for Ethereum 2.0's\nbeacon chain, combines the finality gadget Casper FFG with the LMD GHOST fork\nchoice rule and aims to achieve this property. However, we discovered an attack\nin the standard synchronous network model, highlighting a general difficulty\nwith existing finality-gadget-based designs. We present a construction of\nprovably secure ebb-and-flow protocols with optimal resilience. Nodes run an\noff-the-shelf dynamically available protocol, take snapshots of the growing\navailable ledger, and input them into a separate off-the-shelf BFT protocol to\nfinalize a prefix. We explore connections with flexible BFT and improve upon\nthe state-of-the-art for that problem.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:55:36 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 23:48:14 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 06:07:34 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Neu", "Joachim", ""], ["Tas", "Ertem Nusret", ""], ["Tse", "David", ""]]}, {"id": "2009.05018", "submitter": "David Grimsman", "authors": "David Grimsman, Joshua H. Seaton, Jason R. Marden, Philip N. Brown", "title": "The Cost of Denied Observation in Multiagent Submodular Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A popular formalism for multiagent control applies tools from game theory,\ncasting a multiagent decision problem as a cooperation-style game in which\nindividual agents make local choices to optimize their own local utility\nfunctions in response to the observable choices made by other agents. When the\nsystem-level objective is submodular maximization, it is known that if every\nagent can observe the action choice of all other agents, then all Nash\nequilibria of a large class of resulting games are within a factor of $2$ of\noptimal; that is, the price of anarchy is $1/2$. However, little is known if\nagents cannot observe the action choices of other relevant agents. To study\nthis, we extend the standard game-theoretic model to one in which a subset of\nagents either become \\emph{blind} (unable to observe others' choices) or\n\\emph{isolated} (blind, and also invisible to other agents), and we prove exact\nexpressions for the price of anarchy as a function of the number of compromised\nagents. When $k$ agents are compromised (in any combination of blind or\nisolated), we show that the price of anarchy for a large class of utility\nfunctions is exactly $1/(2+k)$. We then show that if agents use marginal-cost\nutility functions and at least $1$ of the compromised agents is blind (rather\nthan isolated), the price of anarchy improves to $1/(1+k)$. We also provide\nsimulation results demonstrating the effects of these observation denials in a\ndynamic setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:33:11 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 18:11:49 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Grimsman", "David", ""], ["Seaton", "Joshua H.", ""], ["Marden", "Jason R.", ""], ["Brown", "Philip N.", ""]]}, {"id": "2009.05230", "submitter": "Suresh Krishna", "authors": "Suresh Krishna, Ravi Krishna", "title": "Accelerating Recommender Systems via Hardware \"scale-in\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's era of \"scale-out\", this paper makes the case that a specialized\nhardware architecture based on \"scale-in\"--placing as many specialized\nprocessors as possible along with their memory systems and interconnect links\nwithin one or two boards in a rack--would offer the potential to boost large\nrecommender system throughput by 12-62x for inference and 12-45x for training\ncompared to the DGX-2 state-of-the-art AI platform, while minimizing the\nperformance impact of distributing large models across multiple processors. By\nanalyzing Facebook's representative model--Deep Learning Recommendation Model\n(DLRM)--from a hardware architecture perspective, we quantify the impact on\nthroughput of hardware parameters such as memory system design, collective\ncommunications latency and bandwidth, and interconnect topology. By focusing on\nconditions that stress hardware, our analysis reveals limitations of existing\nAI accelerators and hardware platforms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 04:51:14 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Krishna", "Suresh", ""], ["Krishna", "Ravi", ""]]}, {"id": "2009.05257", "submitter": "Charlene Yang", "authors": "Charlene Yang, Yunsong Wang, Steven Farrell, Thorsten Kurth, Samuel\n  Williams", "title": "Hierarchical Roofline Performance Analysis for Deep Learning\n  Applications", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a practical methodology for collecting performance data\nnecessary to conduct hierarchical Roofline analysis on NVIDIA GPUs. It\ndiscusses the extension of the Empirical Roofline Toolkit for broader support\nof a range of data precisions and Tensor Core support and introduces a Nsight\nCompute based method to accurately collect application performance information.\nThis methodology allows for automated machine characterization and application\ncharacterization for Roofline analysis across the entire memory hierarchy on\nNVIDIA GPUs, and it is validated by a complex deep learning application used\nfor climate image segmentation. We use two versions of the code, in TensorFlow\nand PyTorch respectively, to demonstrate the use and effectiveness of this\nmethodology. We highlight how the application utilizes the compute and memory\ncapabilities on the GPU and how the implementation and performance differ in\ntwo deep learning frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 07:16:55 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 07:14:16 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 21:57:58 GMT"}, {"version": "v4", "created": "Wed, 25 Nov 2020 02:52:41 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Yang", "Charlene", ""], ["Wang", "Yunsong", ""], ["Farrell", "Steven", ""], ["Kurth", "Thorsten", ""], ["Williams", "Samuel", ""]]}, {"id": "2009.05334", "submitter": "Andreas Kurth", "authors": "Andreas Kurth, Wolfgang R\\\"onninger, Thomas Benz, Matheus Cavalcante,\n  Fabian Schuiki, Florian Zaruba, Luca Benini", "title": "An Open-Source Platform for High-Performance Non-Coherent On-Chip\n  Communication", "comments": "14 pages, 24 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-chip communication infrastructure is a central component of modern\nsystems-on-chip (SoCs), and it continues to gain importance as the number of\ncores, the heterogeneity of components, and the on-chip and off-chip bandwidth\ncontinue to grow. Decades of research on on-chip networks enabled\ncache-coherent shared-memory multiprocessors. However, communication fabrics\nthat meet the needs of heterogeneous many-cores and accelerator-rich SoCs,\nwhich are not, or only partially, coherent, are a much less mature research\narea.\n  In this work, we present a modular, topology-agnostic, high-performance\non-chip communication platform. The platform includes components to build and\nlink subnetworks with customizable bandwidth and concurrency properties and\nadheres to a state-of-the-art, industry-standard protocol. We discuss\nmicroarchitectural trade-offs and timing/area characteristics of our modules\nand show that they can be composed to build high-bandwidth (e.g., 2.5 GHz and\n1024 bit data width) end-to-end on-chip communication fabrics (not only network\nswitches but also DMA engines and memory controllers) with high degrees of\nconcurrency. We design and implement a state-of-the-art ML training\naccelerator, where our communication fabric scales to 1024 cores on a die,\nproviding 32 TB/s cross-sectional bandwidth at only 24 ns round-trip latency\nbetween any two cores.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:53:37 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Kurth", "Andreas", ""], ["R\u00f6nninger", "Wolfgang", ""], ["Benz", "Thomas", ""], ["Cavalcante", "Matheus", ""], ["Schuiki", "Fabian", ""], ["Zaruba", "Florian", ""], ["Benini", "Luca", ""]]}, {"id": "2009.05504", "submitter": "Saurabh Raje Mr.", "authors": "Saurabh Raje, Fr\\'ed\\'eric Wagner", "title": "Kvik: A task based middleware with composable scheduling policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Kvik: an implementation of a task-based \"middleware\"\nfor shared memory parallel programming in the Rust language built on top of the\nRayon library. We devise a system allowing several task-splitting schedulers to\nbe finely tuned by the end users. Among these, we propose an implementation of\nan adaptive scheduler reducing tasks creations (splits) to bare minimum by\nlinking tasks splitting to steal requests. Another important scheduler that\nallows turning computations into sequences of parallel operations is described.\nThis operator proves itself particularly useful for interruptible computations.\nWe exhibit different code examples well suited for different types of\nschedulers. We conclude our work with a set of benchmarks making heavy use of\ncomposability. In particular we present a parallel stable sort implementation\nwith up to 1.5x more speedup when compared to the state-of-the-art parallel\nsorting implementation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:58:47 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Raje", "Saurabh", ""], ["Wagner", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2009.05615", "submitter": "Benjamin Bolling", "authors": "Benjamin Bolling", "title": "Computer-Aided Generation of N-shift RWS", "comments": "9 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generating schedules for shift workers is essential for many employers,\nwhether the employer is a small or a large industrial complex, research\nlaboratory, or other businesses involving shift works.\n  Previous methods for creating rotational workforce schedules included\ninteractions between the schedule maker and the algorithm, including defining\nthe length of sequences of consecutive days of working shifts.\n  In this method, an algorithm takes into account inputs (or constraints) from\nthe schedule maker and then presents the possible solutions (incl. that all\nshifts must be filled, working hours per week, minimal resting time, etc.) in a\nfirst phase. The schedule maker can then select which solutions are most\nfeasible to proceed with in the second phase, where the final schedules are\nthen constructed and exported.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:56:20 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 09:00:41 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Bolling", "Benjamin", ""]]}, {"id": "2009.05710", "submitter": "Joshua Daymude", "authors": "Shengkai Li, Bahnisikha Dutta, Sarah Cannon, Joshua J. Daymude, Ram\n  Avinery, Enes Aydin, Andr\\'ea W. Richa, Daniel I. Goldman, Dana Randall", "title": "Programming Active Cohesive Granular Matter with Mechanically Induced\n  Phase Changes", "comments": null, "journal-ref": null, "doi": "10.1126/sciadv.abe8494", "report-no": null, "categories": "cond-mat.soft cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active matter physics and swarm robotics have provided powerful tools for the\nstudy and control of ensembles driven by internal sources. At the macroscale,\ncontrolling swarms typically utilizes significant memory, processing power, and\ncoordination unavailable at the microscale, e.g., for colloidal robots, which\ncould be useful for fighting disease, fabricating intelligent textiles, and\ndesigning nanocomputers. To develop principles that that can leverage physics\nof interactions and thus can be utilized across scales, we take a two-pronged\napproach: a theoretical abstraction of self-organizing particle systems and an\nexperimental robot system of active cohesive granular matter that intentionally\nlacks digital electronic computation and communication, using minimal (or no)\nsensing and control, to test theoretical predictions. We consider the problems\nof aggregation, dispersion, and collective transport. As predicted by the\ntheory, as a parameter representing interparticle attraction increases, the\nrobots transition from a dispersed phase to an aggregated one, forming a dense,\ncompact collective. When aggregated, the collective can transport non-robot\n\"impurities\" in their environment, thus performing an emergent task driven by\nthe physics underlying the transition. These results point to a fruitful\ninterplay between algorithm design and active matter robophysics that can\nresult in new nonequilibrium physics and principles for programming collectives\nwithout the need for complex algorithms or capabilities.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 03:08:06 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 22:28:30 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Shengkai", ""], ["Dutta", "Bahnisikha", ""], ["Cannon", "Sarah", ""], ["Daymude", "Joshua J.", ""], ["Avinery", "Ram", ""], ["Aydin", "Enes", ""], ["Richa", "Andr\u00e9a W.", ""], ["Goldman", "Daniel I.", ""], ["Randall", "Dana", ""]]}, {"id": "2009.05766", "submitter": "Pan Zhou", "authors": "Pan Zhou, Qian Lin, Dumitrel Loghin, Beng Chin Ooi, Yuncheng Wu,\n  Hongfang Yu", "title": "Communication-efficient Decentralized Machine Learning over\n  Heterogeneous Networks", "comments": "17 pages, 19 figures, accepted by conference ICDE'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, distributed machine learning has been usually executed\nover heterogeneous networks such as a local area network within a multi-tenant\ncluster or a wide area network connecting data centers and edge clusters. In\nthese heterogeneous networks, the link speeds among worker nodes vary\nsignificantly, making it challenging for state-of-the-art machine learning\napproaches to perform efficient training. Both centralized and decentralized\ntraining approaches suffer from low-speed links. In this paper, we propose a\ndecentralized approach, namely NetMax, that enables worker nodes to communicate\nvia high-speed links and, thus, significantly speed up the training process.\nNetMax possesses the following novel features. First, it consists of a novel\nconsensus algorithm that allows worker nodes to train model copies on their\nlocal dataset asynchronously and exchange information via peer-to-peer\ncommunication to synchronize their local copies, instead of a central master\nnode (i.e., parameter server). Second, each worker node selects one peer\nrandomly with a fine-tuned probability to exchange information per iteration.\nIn particular, peers with high-speed links are selected with high probability.\nThird, the probabilities of selecting peers are designed to minimize the total\nconvergence time. Moreover, we mathematically prove the convergence of NetMax.\nWe evaluate NetMax on heterogeneous cluster networks and show that it achieves\nspeedups of 3.7X, 3.4X, and 1.9X in comparison with the state-of-the-art\ndecentralized training approaches Prague, Allreduce-SGD, and AD-PSGD,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 11:17:55 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 13:02:06 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zhou", "Pan", ""], ["Lin", "Qian", ""], ["Loghin", "Dumitrel", ""], ["Ooi", "Beng Chin", ""], ["Wu", "Yuncheng", ""], ["Yu", "Hongfang", ""]]}, {"id": "2009.05776", "submitter": "Amitabh Trehan", "authors": "Walter Hussak and Amitabh Trehan", "title": "Terminating cases of flooding", "comments": "Submitted for journal publication. 26 pages. Related to\n  arXiv:1907.07078, https://doi.org/10.4230/LIPIcs.STACS.2020.17, and\n  https://doi.org/10.1145/3293611.3331586", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic synchronous flooding proceeds in rounds. Given a finite undirected\n(network) graph $G$, a set of sources $I \\subseteq G$ initiate flooding in the\nfirst round by every node in $I$ sending the same message to all of its\nneighbours. In each subsequent round, nodes send the message to all of their\nneighbours from which they did not receive the message in the previous round.\nFlooding terminates when no node in $G$ sends a message in a round. The\nquestion of termination has not been settled - rather, non-termination is\nimplicitly assumed to be possible.\n  We show that flooding terminates on every finite graph. In the case of a\nsingle source $g_0$, flooding terminates in $e$ rounds if $G$ is bipartite and\n$j$ rounds with $e < j \\leq e+d+1$ otherwise, where $e$ and $d$ are the\neccentricity of $g_0$ and diameter of $G$ respectively. For\ncommunication/broadcast to all nodes, this is asymptotically time optimal and\nobviates the need for construction and maintenance of spanning structures. We\nextend to dynamic flooding initiated in multiple rounds with possibly multiple\nmessages. The cases where a node only sends a message to neighbours from which\nit did not receive {\\it any} message in the previous round, and where a node\nsends some highest ranked message to all neighbours from which it did not\nreceive {\\it that} message in the previous round, both terminate. All these\ncases also hold if the network graph loses edges over time. Non-terminating\ncases include asynchronous flooding, flooding where messages have fixed delays\nat edges, cases of multiple-message flooding and cases where the network graph\nacquires edges over time.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 12:05:48 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hussak", "Walter", ""], ["Trehan", "Amitabh", ""]]}, {"id": "2009.05868", "submitter": "Hangyu Zhu", "authors": "Hangyu Zhu, Haoyu Zhang and Yaochu Jin", "title": "From Federated Learning to Federated Neural Architecture Search: A\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a recently proposed distributed machine learning\nparadigm for privacy preservation, which has found a wide range of applications\nwhere data privacy is of primary concern. Meanwhile, neural architecture search\nhas become very popular in deep learning for automatically tuning the\narchitecture and hyperparameters of deep neural networks. While both federated\nlearning and neural architecture search are faced with many open challenges,\nsearching for optimized neural architectures in the federated learning\nframework is particularly demanding. This survey paper starts with a brief\nintroduction to federated learning, including both horizontal, vertical, and\nhybrid federated learning. Then, neural architecture search approaches based on\nreinforcement learning, evolutionary algorithms and gradient-based are\npresented. This is followed by a description of federated neural architecture\nsearch that has recently been proposed, which is categorized into online and\noffline implementations, and single- and multi-objective search approaches.\nFinally, remaining open research questions are outlined and promising research\ntopics are suggested.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 21:29:27 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhu", "Hangyu", ""], ["Zhang", "Haoyu", ""], ["Jin", "Yaochu", ""]]}, {"id": "2009.06005", "submitter": "Sudipta Paul Ms.", "authors": "Sudipta Paul, Poushali Sengupta and Subhankar Mishra", "title": "FLaPS: Federated Learning and Privately Scaling", "comments": "5 figures, 8 tables, Accepted to the SLICE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) is a distributed learning process where the model\n(weights and checkpoints) is transferred to the devices that posses data rather\nthan the classical way of transferring and aggregating the data centrally. In\nthis way, sensitive data does not leave the user devices. FL uses the FedAvg\nalgorithm, which is trained in the iterative model averaging way, on the\nnon-iid and unbalanced distributed data, without depending on the data\nquantity. Some issues with the FL are, 1) no scalability, as the model is\niteratively trained over all the devices, which amplifies with device drops; 2)\nsecurity and privacy trade-off of the learning process still not robust enough\nand 3) overall communication efficiency and the cost are higher. To mitigate\nthese challenges we present Federated Learning and Privately Scaling (FLaPS)\narchitecture, which improves scalability as well as the security and privacy of\nthe system. The devices are grouped into clusters which further gives better\nprivacy scaled turn around time to finish a round of training. Therefore, even\nif a device gets dropped in the middle of training, the whole process can be\nstarted again after a definite amount of time. The data and model both are\ncommunicated using differentially private reports with iterative shuffling\nwhich provides a better privacy-utility trade-off. We evaluated FLaPS on MNIST,\nCIFAR10, and TINY-IMAGENET-200 dataset using various CNN models. Experimental\nresults prove FLaPS to be an improved, time and privacy scaled environment\nhaving better and comparable after-learning-parameters with respect to the\ncentral and FL models.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 14:20:17 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Paul", "Sudipta", ""], ["Sengupta", "Poushali", ""], ["Mishra", "Subhankar", ""]]}, {"id": "2009.06043", "submitter": "Peter Davies", "authors": "Artur Czumaj, Peter Davies, Merav Parter", "title": "Simple, Deterministic, Constant-Round Coloring in the Congested Clique", "comments": "20 pages, appeared at PODC 2020", "journal-ref": null, "doi": "10.1145/3382734.3405751", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We settle the complexity of the $(\\Delta+1)$-coloring and $(\\Delta+1)$-list\ncoloring problems in the CONGESTED CLIQUE model by presenting a simple\ndeterministic algorithm for both problems running in a constant number of\nrounds. This matches the complexity of the recent breakthrough randomized\nconstant-round $(\\Delta+1)$-list coloring algorithm due to Chang et al.\n(PODC'19), and significantly improves upon the state-of-the-art $O(\\log\n\\Delta)$-round deterministic $(\\Delta+1)$-coloring bound of Parter (ICALP'18).\n  A remarkable property of our algorithm is its simplicity. Whereas the\nstate-of-the-art randomized algorithms for this problem are based on the quite\ninvolved local coloring algorithm of Chang et al. (STOC'18), our algorithm can\nbe described in just a few lines. At a high level, it applies a careful\nderandomization of a recursive procedure which partitions the nodes and their\nrespective palettes into separate bins. We show that after $O(1)$ recursion\nsteps, the remaining uncolored subgraph within each bin has linear size, and\nthus can be solved locally by collecting it to a single node. This algorithm\ncan also be implemented in the Massively Parallel Computation (MPC) model\nprovided that each machine has linear (in $n$, the number of nodes in the input\ngraph) space.\n  We also show an extension of our algorithm to the MPC regime in which\nmachines have sublinear space: we present the first deterministic\n$(\\Delta+1)$-list coloring algorithm designed for sublinear-space MPC, which\nruns in $O(\\log \\Delta + \\log\\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 16:59:21 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""], ["Parter", "Merav", ""]]}, {"id": "2009.06061", "submitter": "David Brayford", "authors": "David Brayford, Christoph Bernau, Wolfram Hesse, Carla Guillen", "title": "Analyzing Performance Properties Collected by the PerSyst Scalable HPC\n  Monitoring Tool", "comments": "9 pages, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to understand how a scientific application is executed on a large\nHPC system is of great importance in allocating resources within the HPC data\ncenter. In this paper, we describe how we used system performance data to\nidentify: execution patterns, possible code optimizations and improvements to\nthe system monitoring. We also identify candidates for employing machine\nlearning techniques to predict the performance of similar scientific codes.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 18:38:17 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Brayford", "David", ""], ["Bernau", "Christoph", ""], ["Hesse", "Wolfram", ""], ["Guillen", "Carla", ""]]}, {"id": "2009.06149", "submitter": "Avery Miller", "authors": "Barun Gorain, Avery Miller, Andrzej Pelc", "title": "Four Shades of Deterministic Leader Election in Anonymous Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leader election is one of the fundamental problems in distributed computing:\na single node, called the leader, must be specified. This task can be\nformulated either in a weak way, where one node outputs 'leader' and all other\nnodes output 'non-leader', or in a strong way, where all nodes must also learn\nwhich node is the leader. If the nodes of the network have distinct\nidentifiers, then such an agreement means that all nodes have to output the\nidentifier of the elected leader. For anonymous networks, the strong version of\nleader election requires that all nodes must be able to find a path to the\nleader, as this is the only way to identify it. For any network in which leader\nelection (weak or strong) is possible knowing the map of the network, there is\na minimum time in which this can be done. We consider four formulations of\nleader election discussed in the literature in the context of anonymous\nnetworks : one is the weak formulation, and the three others specify three\ndifferent ways of finding the path to the leader in the strong formulation. Our\naim is to compare the amount of initial information needed to accomplish each\nof these \"four shades\" of leader election in minimum time. We show that the\namount of information required to accomplish leader election in the weak\nformulation in minimum time is exponentially smaller than that needed for any\nof the strong formulations. Thus, if the required amount of advice is used as a\nmeasure of the difficulty of the task, the weakest version of leader election\nin minimum time is drastically easier than any version of the strong\nformulation in minimum time.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 02:05:57 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Gorain", "Barun", ""], ["Miller", "Avery", ""], ["Pelc", "Andrzej", ""]]}, {"id": "2009.06303", "submitter": "Pengqian Yu", "authors": "Pengqian Yu, Achintya Kundu, Laura Wynter, Shiau Hong Lim", "title": "Fed+: A Unified Approach to Robust Personalized Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of methods for robust, personalized federated learning,\ncalled Fed+, that unifies many federated learning algorithms. The principal\nadvantage of this class of methods is to better accommodate the real-world\ncharacteristics found in federated training, such as the lack of IID data\nacross parties, the need for robustness to outliers or stragglers, and the\nrequirement to perform well on party-specific datasets. We achieve this through\na problem formulation that allows the central server to employ robust ways of\naggregating the local models while keeping the structure of local computation\nintact. Without making any statistical assumption on the degree of\nheterogeneity of local data across parties, we provide convergence guarantees\nfor Fed+ for convex and non-convex loss functions and robust aggregation. The\nFed+ theory is also equipped to handle heterogeneous computing environments\nincluding stragglers without additional assumptions; specifically, the\nconvergence results cover the general setting where the number of local update\nsteps across parties can vary. We demonstrate the benefits of Fed+ through\nextensive experiments across standard benchmark datasets as well as on a\nchallenging real-world problem in financial portfolio management where the\nheterogeneity of party-level data can lead to training failure in standard\nfederated learning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 10:04:30 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 03:24:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yu", "Pengqian", ""], ["Kundu", "Achintya", ""], ["Wynter", "Laura", ""], ["Lim", "Shiau Hong", ""]]}, {"id": "2009.06374", "submitter": "Venktesh Viswanathan", "authors": "Venktesh V, Pooja B Bindal, Devesh Singhal, A V Subramanyam, Vivek\n  Kumar", "title": "OneStopTuner: An End to End Architecture for JVM Tuning of Spark\n  Applications", "comments": "Submitted to IEEE BigData2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Java is the backbone of widely used big data frameworks, such as Apache\nSpark, due to its productivity, portability from JVM-based execution, and\nsupport for a rich set of libraries. However, the performance of these\napplications can widely vary depending on the runtime flags chosen out of all\nexisting JVM flags. Manually tuning these flags is both cumbersome and\nerror-prone. Automated tuning approaches can ease the task, but current\nsolutions either require considerable processing time or target a subset of\nflags to avoid time and space requirements. In this paper, we present\nOneStopTuner, a Machine Learning based novel framework for autotuning JVM\nflags. OneStopTuner controls the amount of data generation by leveraging batch\nmode active learning to characterize the user application. Based on the\nuser-selected optimization metric, OneStopTuner then discards the irrelevant\nJVM flags by applying feature selection algorithms on the generated data.\nFinally, it employs sample efficient methods such as Bayesian optimization and\nregression guided Bayesian optimization on the shortlisted JVM flags to find\nthe optimal values for the chosen set of flags. We evaluated OneStopTuner on\nwidely used Spark benchmarks and compare its performance with the traditional\nsimulated annealing based autotuning approach. We demonstrate that for\noptimizing execution time, the flags chosen by OneStopTuner provides a speedup\nof up to 1.35x over default Spark execution, as compared to 1.15x speedup by\nusing the flag configurations proposed by simulated annealing. OneStopTuner was\nable to reduce the number of executions for data-generation by 70% and was able\nto suggest the optimal flag configuration 2.4x faster than the standard\nsimulated annealing based approach, excluding the time for data-generation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 06:15:02 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["V", "Venktesh", ""], ["Bindal", "Pooja B", ""], ["Singhal", "Devesh", ""], ["Subramanyam", "A V", ""], ["Kumar", "Vivek", ""]]}, {"id": "2009.06487", "submitter": "Chengyu Wang", "authors": "Chengyu Wang, Mengli Cheng, Xu Hu, Jun Huang", "title": "EasyASR: A Distributed Machine Learning Platform for End-to-end\n  Automatic Speech Recognition", "comments": "aaai 2021 demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present EasyASR, a distributed machine learning platform for training and\nserving large-scale Automatic Speech Recognition (ASR) models, as well as\ncollecting and processing audio data at scale. Our platform is built upon the\nMachine Learning Platform for AI of Alibaba Cloud. Its main functionality is to\nsupport efficient learning and inference for end-to-end ASR models on\ndistributed GPU clusters. It allows users to learn ASR models with either\npre-defined or user-customized network architectures via simple user interface.\nOn EasyASR, we have produced state-of-the-art results over several public\ndatasets for Mandarin speech recognition.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:47:02 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 09:44:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Chengyu", ""], ["Cheng", "Mengli", ""], ["Hu", "Xu", ""], ["Huang", "Jun", ""]]}, {"id": "2009.06497", "submitter": "Fauzi Adi Rafrastara", "authors": "Cinantya Paramita, Fauzi Adi Rafrastara, Usman Sudibyo, R.I.W. Agung\n  Wibowo", "title": "Performance Evaluation of Linear Regression Algorithm in Cluster\n  Environment", "comments": "4 pages, 2 figures, International Journal of Computer Science and\n  Information Security (IJCSIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster computing was introduced to replace the superiority of super\ncomputers. Cluster computing is able to overcome the problems that cannot be\neffectively dealt with supercomputers. In this paper, we are going to evaluate\nthe performance of cluster computing by executing one of data mining techniques\nin the cluster environment. The experiment will attempt to predict the flight\ndelay by using linear regression algorithm with apache spark as a framework for\ncluster computing. The result shows that, by involving 5 PCs in cluster\nenvironment with equal specifications can increase the performance of\ncomputation up to 39.76% compared to the standalone one. Attaching more nodes\nto the cluster can make the process become faster significantly.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:58:51 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Paramita", "Cinantya", ""], ["Rafrastara", "Fauzi Adi", ""], ["Sudibyo", "Usman", ""], ["Wibowo", "R. I. W. Agung", ""]]}, {"id": "2009.06541", "submitter": "Fangyi Zhou", "authors": "Fangyi Zhou (1), Francisco Ferreira (1), Raymond Hu (2), Rumyana\n  Neykova (3) and Nobuko Yoshida (1) ((1) Imperial College London, (2)\n  University of Hertfordshire, (3) Brunel University London)", "title": "Statically Verified Refinements for Multiparty Protocols", "comments": "Conditionally Accepted by OOPSLA' 20. Full version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With distributed computing becoming ubiquitous in the modern era, safe\ndistributed programming is an open challenge. To address this, multiparty\nsession types (MPST) provide a typing discipline for message-passing\nconcurrency, guaranteeing communication safety properties such as deadlock\nfreedom.\n  While originally MPST focus on the communication aspects, and employ a simple\ntyping system for communication payloads, communication protocols in the real\nworld usually contain constraints on the payload. We introduce refined\nmultiparty session types (RMPST), an extension of MPST, that express data\ndependent protocols via refinement types on the data types.\n  We provide an implementation of RMPST, in a toolchain called Session*, using\nScribble, a multiparty protocol description toolchain, and targeting F*, a\nverification-oriented functional programming language. Users can describe a\nprotocol in Scribble and implement the endpoints in F* using refinement-typed\nAPIs generated from the protocol. The F* compiler can then statically verify\nthe refinements. Moreover, we use a novel approach of callback-styled API\ngeneration, providing static linearity guarantees with the inversion of\ncontrol. We evaluate our approach with real world examples and show that it has\nlittle overhead compared to a na\\\"ive implementation, while guaranteeing safety\nproperties from the underlying theory.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 16:09:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhou", "Fangyi", ""], ["Ferreira", "Francisco", ""], ["Hu", "Raymond", ""], ["Neykova", "Rumyana", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "2009.06693", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda, Sandeep Polisetty, Arjun Guha, Marco Serafini", "title": "Accelerating Graph Sampling for Graph Machine Learning using GPUs", "comments": "Published in EuroSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning algorithms automatically learn the features of data.\nSeveral representation learning algorithms for graph data, such as DeepWalk,\nnode2vec, and GraphSAGE, sample the graph to produce mini-batches that are\nsuitable for training a DNN. However, sampling time can be a significant\nfraction of training time, and existing systems do not efficiently parallelize\nsampling.\n  Sampling is an embarrassingly parallel problem and may appear to lend itself\nto GPU acceleration, but the irregularity of graphs makes it hard to use GPU\nresources effectively. This paper presents NextDoor, a system designed to\neffectively perform graph sampling on GPUs. NextDoor employs a new approach to\ngraph sampling that we call transit-parallelism, which allows load balancing\nand caching of edges. NextDoor provides end-users with a high-level abstraction\nfor writing a variety of graph sampling algorithms. We implement several graph\nsampling applications, and show that NextDoor runs them orders of magnitude\nfaster than existing systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 19:03:33 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 18:28:53 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 18:16:56 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 00:57:02 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Jangda", "Abhinav", ""], ["Polisetty", "Sandeep", ""], ["Guha", "Arjun", ""], ["Serafini", "Marco", ""]]}, {"id": "2009.06988", "submitter": "Maksym Planeta", "authors": "Maksym Planeta, Jan Bierbaum, Leo Sahaya Daphne Antony, Torsten\n  Hoefler, Hermann H\\\"artig", "title": "MigrOS: Transparent Operating Systems Live Migration Support for\n  Containerised RDMA-applications", "comments": "16 pages, 13 figures, 4 tables, 1 listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major data centre providers are introducing RDMA-based networks for their\ntenants, as well as for operating the underlying infrastructure. In comparison\nto traditional socket-based network stacks, RDMA-based networks offer higher\nthroughput, lower latency and reduced CPU overhead. However, transparent\ncheckpoint and migration operations become much more difficult. The key reason\nis that the OS is removed from the critical path of communication. As a result,\nsome of the communication state itself resides in the NIC hardware and is no\nmore under the direct control of the OS. This control includes especially the\nsupport for virtualisation of communication which is needed for live migration\nof communication partners. In this paper, we propose the basic principles\nrequired to implement a migration-capable RDMA-based network. We recommend some\nchanges at the software level and small changes at the hardware level. As a\nproof of concept, we integrate the proposed changes into SoftRoCE, an\nopen-source kernel-level implementation of the RoCE protocol. We claim that\nthese changes introduce no runtime overhead when migration does not happen.\nFinally, we develop a proof-of-concept implementation for migrating\ncontainerised applications that use RDMA-based networks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 11:15:25 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 12:21:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Planeta", "Maksym", ""], ["Bierbaum", "Jan", ""], ["Antony", "Leo Sahaya Daphne", ""], ["Hoefler", "Torsten", ""], ["H\u00e4rtig", "Hermann", ""]]}, {"id": "2009.07174", "submitter": "Jan Martens", "authors": "Johri van Eerd, Jan Friso Groote, Pieter Hijma, Jan Martens and Anton\n  Wijs", "title": "Term Rewriting on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a way to implement term rewriting on a GPU. We do this by letting\nthe GPU repeatedly perform a massively parallel evaluation of all subterms. We\nfind that if the term rewrite systems exhibit sufficient internal parallelism,\nGPU rewriting substantially outperforms the CPU. Since we expect that our\nimplementation can be further optimized, and because in any case GPUs will\nbecome much more powerful in the future, this suggests that GPUs are an\ninteresting platform for term rewriting. As term rewriting can be viewed as a\nuniversal programming language, this also opens a route towards programming\nGPUs by term rewriting, especially for irregular computations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 15:23:25 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["van Eerd", "Johri", ""], ["Groote", "Jan Friso", ""], ["Hijma", "Pieter", ""], ["Martens", "Jan", ""], ["Wijs", "Anton", ""]]}, {"id": "2009.07226", "submitter": "Mert Hidayetoglu", "authors": "Mert Hidayetoglu, Tekin Bicer, Simon Garcia de Gonzalo, Bin Ren,\n  Vincent De Andrade, Doga Gursoy, Raj Kettimuthu, Ian T. Foster, Wen-mei W.\n  Hwu", "title": "Petascale XCT: 3D Image Reconstruction with Hierarchical Communications\n  on Multi-GPU Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray computed tomography is a commonly used technique for noninvasive\nimaging at synchrotron facilities. Iterative tomographic reconstruction\nalgorithms are often preferred for recovering high quality 3D volumetric images\nfrom 2D X-ray images, however, their use has been limited to small/medium\ndatasets due to their computational requirements. In this paper, we propose a\nhigh-performance iterative reconstruction system for terabyte(s)-scale 3D\nvolumes. Our design involves three novel optimizations: (1) optimization of\n(back)projection operators by extending the 2D memory-centric approach to 3D;\n(2) performing hierarchical communications by exploiting \"fat-node\"\narchitecture with many GPUs; (3) utilization of mixed-precision types while\npreserving convergence rate and quality. We extensively evaluate the proposed\noptimizations and scaling on the Summit supercomputer. Our largest\nreconstruction is a mouse brain volume with 9Kx11Kx11K voxels, where the total\nreconstruction time is under three minutes using 24,576 GPUs, reaching 65\nPFLOPS: 34% of Summit's peak performance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:05:07 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Hidayetoglu", "Mert", ""], ["Bicer", "Tekin", ""], ["de Gonzalo", "Simon Garcia", ""], ["Ren", "Bin", ""], ["De Andrade", "Vincent", ""], ["Gursoy", "Doga", ""], ["Kettimuthu", "Raj", ""], ["Foster", "Ian T.", ""], ["Hwu", "Wen-mei W.", ""]]}, {"id": "2009.07325", "submitter": "Matin Hashemi", "authors": "Soheil Shahrouz, Saber Salehkaleybar, Matin Hashemi", "title": "gIM: GPU Accelerated RIS-based Influence Maximization Algorithm", "comments": "Accepted for publication in IEEE Transactions on Parallel and\n  Distributed Systems (TPDS)", "journal-ref": null, "doi": "10.1109/TPDS.2021.3066215", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a social network modeled as a weighted graph $G$, the influence\nmaximization problem seeks $k$ vertices to become initially influenced, to\nmaximize the expected number of influenced nodes under a particular diffusion\nmodel. The influence maximization problem has been proven to be NP-hard, and\nmost proposed solutions to the problem are approximate greedy algorithms, which\ncan guarantee a tunable approximation ratio for their results with respect to\nthe optimal solution. The state-of-the-art algorithms are based on Reverse\nInfluence Sampling (RIS) technique, which can offer both computational\nefficiency and non-trivial $(1-\\frac{1}{e}-\\epsilon)$-approximation ratio\nguarantee for any $\\epsilon >0$. RIS-based algorithms, despite their lower\ncomputational cost compared to other methods, still require long running times\nto solve the problem in large-scale graphs with low values of $\\epsilon$. In\nthis paper, we present a novel and efficient parallel implementation of a\nRIS-based algorithm, namely IMM, on GPU. The proposed GPU-accelerated influence\nmaximization algorithm, named gIM, can significantly reduce the running time on\nlarge-scale graphs with low values of $\\epsilon$. Furthermore, we show that gIM\nalgorithm can solve other variations of the IM problem, only by applying minor\nmodifications. Experimental results show that the proposed solution reduces the\nruntime by a factor up to $220 \\times$. The source code of gIM is publicly\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 19:06:10 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 13:50:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Shahrouz", "Soheil", ""], ["Salehkaleybar", "Saber", ""], ["Hashemi", "Matin", ""]]}, {"id": "2009.07399", "submitter": "Jie Zhao", "authors": "Jie Zhao, Maria A. Rodriguez and Rajkumar Buyya", "title": "High-Performance Mining of COVID-19 Open Research Datasets for Text\n  Classification and Insights in Cloud Computing Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 global pandemic is an unprecedented health crisis. Since the\noutbreak, many researchers around the world have produced an extensive\ncollection of literatures. For the research community and the general public to\ndigest, it is crucial to analyse the text and provide insights in a timely\nmanner, which requires a considerable amount of computational power. Clouding\ncomputing has been widely adopted in academia and industry in recent years. In\nparticular, hybrid cloud is gaining popularity since its two-fold benefits:\nutilising existing resource to save cost and using additional cloud service\nproviders to gain assess to extra computing resources on demand. In this paper,\nwe developed a system utilising the Aneka PaaS middleware with parallel\nprocessing and multi-cloud capability to accelerate the ETL and article\ncategorising process using machine learning technology on a hybrid cloud. The\nresult is then persisted for further referencing, searching and visualising.\nOur performance evaluation shows that the system can help with reducing\nprocessing time and achieving linear scalability. Beyond COVID-19, the\napplication might be used directly in broader scholarly article indexing and\nanalysing.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 00:25:51 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Zhao", "Jie", ""], ["Rodriguez", "Maria A.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2009.07400", "submitter": "Rafael Ravedutti Lucio Machado", "authors": "Rafael Ravedutti L. Machado (1), Jonas Schmitt (1), Sebastian Eibl\n  (1), Jan Eitzinger (2), Roland Lei{\\ss}a (3), Sebastian Hack (3), Ars\\`ene\n  P\\'erard-Gayot (3), Richard Membarth (3 and 4) and Harald K\\\"ostler (1) ((1)\n  Chair for System Simulation at University of Erlangen-N\\\"urnberg, (2)\n  Regional Computer Center Erlangen at University of Erlangen-N\\\"urnberg, (3)\n  Saarland Informatics Campus at Saarland University, (4) German Research\n  Center for Artificial Intelligence at Saarland Informatics Campus)", "title": "tinyMD: A Portable and Scalable Implementation for Pairwise Interactions\n  Simulations", "comments": "35 pages, 8 figures, submitted to Journal of Computational Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.PL physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the suitability of the AnyDSL partial evaluation\nframework to implement tinyMD: an efficient, scalable, and portable simulation\nof pairwise interactions among particles. We compare tinyMD with the miniMD\nproxy application that scales very well on parallel supercomputers. We discuss\nthe differences between both implementations and contrast miniMD's performance\nfor single-node CPU and GPU targets, as well as its scalability on SuperMUC-NG\nand Piz Daint supercomputers. Additionaly, we demonstrate tinyMD's flexibility\nby coupling it with the waLBerla multi-physics framework. This allow us to\nexecute tinyMD simulations using the load-balancing mechanism implemented in\nwaLBerla.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 00:29:13 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Machado", "Rafael Ravedutti L.", "", "3 and 4"], ["Schmitt", "Jonas", "", "3 and 4"], ["Eibl", "Sebastian", "", "3 and 4"], ["Eitzinger", "Jan", "", "3 and 4"], ["Lei\u00dfa", "Roland", "", "3 and 4"], ["Hack", "Sebastian", "", "3 and 4"], ["P\u00e9rard-Gayot", "Ars\u00e8ne", "", "3 and 4"], ["Membarth", "Richard", "", "3 and 4"], ["K\u00f6stler", "Harald", ""]]}, {"id": "2009.07463", "submitter": "Chenglong Zhang", "authors": "Chenglong Zhang", "title": "A New Perspective of Graph Data and A Generic and Efficient Method for\n  Large Scale Graph Data Traversal", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BFS algorithm is a basic graph data processing algorithm and many other\ngraph data processing algorithms have similar architectural features with BFS\nalgorithm and can be built on the basis of BFS algorithm model. We analyze the\ndifferences between graph algorithms and traditional high-performance\nalgorithms in detail, propose a new way of classifying algorithms into data\nindependent algorithm and data correlation algorithm based on their run-time\ncorrelation with data, and use this new classification to explain the validity\nof the methods proposed in this paper. Through a deeper analysis of graph data,\nwe propose a new fundamental perspective on understanding graph data,\nestablishing a link between two basic data structures, graph and tree, and\nviewing graph data as consisting of smaller subgraphs and edge trees. Small\ndegree vertices are found to be one of important cause of random memory access.\nBased on this, we propose a general, easy to implement, and efficient method\nfor graph data processing, with the basic idea of treating low-degree vertices\nand core subgraphs separately, thus significantly reducing the size of random\nmemory access and improving the efficiency of memory access. Finally, we\nevaluated the performance of the method on three major data center computing\nplatforms (Intel, AMD, and ARM), and the experiments showed that it brought\n19.7%, 31.8% and 17.9% performance improvement, respectively, with a\nperformance-power ratio of 282.70 MTEPS/s on the ARM platform, ranking it among\nthe Green graph500 in November 2019. World No. 1 on the big dataset list.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 04:30:21 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 00:46:52 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Zhang", "Chenglong", ""]]}, {"id": "2009.07482", "submitter": "Anirban Ghose", "authors": "Anirban Ghose, Siddharth Singh, Vivek Kulaharia, Lokesh Dokara,\n  Srijeeta Maity and Soumyajit Dey", "title": "PySchedCL: Leveraging Concurrency in Heterogeneous Data-Parallel Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, high performance compute capabilities exhibited by\nheterogeneous GPGPU platforms have led to the popularity of data parallel\nprogramming languages such as CUDA and OpenCL. Such languages, however, involve\na steep learning curve as well as developing an extensive understanding of the\nunderlying architecture of the compute devices in heterogeneous platforms. This\nhas led to the emergence of several High Performance Computing frameworks which\nprovide high-level abstractions for easing the development of data-parallel\napplications on heterogeneous platforms. However, the scheduling decisions\nundertaken by such frameworks only exploit coarse-grained concurrency in data\nparallel applications. In this paper, we propose PySchedCL, a framework which\nexplores fine-grained concurrency aware scheduling decisions that harness the\npower of heterogeneous CPU/GPU architectures efficiently. %, a feature which is\nnot provided by existing HPC frameworks. We showcase the efficacy of such\nscheduling mechanisms over existing coarse-grained dynamic scheduling schemes\nby conducting extensive experimental evaluations for a Machine Learning based\ninferencing application.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 05:59:42 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ghose", "Anirban", ""], ["Singh", "Siddharth", ""], ["Kulaharia", "Vivek", ""], ["Dokara", "Lokesh", ""], ["Maity", "Srijeeta", ""], ["Dey", "Soumyajit", ""]]}, {"id": "2009.07785", "submitter": "Boro Sofranac", "authors": "Boro Sofranac, Ambros Gleixner, Sebastian Pokutta", "title": "Accelerating Domain Propagation: an Efficient GPU-Parallel Algorithm\n  over Sparse Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.MS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast domain propagation of linear constraints has become a crucial component\nof today's best algorithms and solvers for mixed integer programming and\npseudo-boolean optimization to achieve peak solving performance. Irregularities\nin the form of dynamic algorithmic behaviour, dependency structures, and\nsparsity patterns in the input data make efficient implementations of domain\npropagation on GPUs and, more generally, on parallel architectures challenging.\nThis is one of the main reasons why domain propagation in state-of-the-art\nsolvers is single thread only. In this paper, we present a new algorithm for\ndomain propagation which (a) avoids these problems and allows for an efficient\nimplementation on GPUs, and is (b) capable of running propagation rounds\nentirely on the GPU, without any need for synchronization or communication with\nthe CPU. We present extensive computational results which demonstrate the\neffectiveness of our approach and show that ample speedups are possible on\npractically relevant problems: on state-of-the-art GPUs, our geometric mean\nspeed-up for reasonably-large instances is around 10x to 20x and can be as high\nas 180x on favorably-large instances.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:25:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 13:17:25 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 16:27:00 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 14:28:53 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Sofranac", "Boro", ""], ["Gleixner", "Ambros", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "2009.07834", "submitter": "Andriy Miranskyy", "authors": "William Pourmajidi and Lei Zhang and John Steinbacher and Tony Erwin\n  and Andriy Miranskyy", "title": "Immutable Log Storage as a Service on Private and Public Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the normal operation of a Cloud solution, no one pays attention to the\nlogs except the system reliability engineers, who may periodically check them\nto ensure that the Cloud platform's performance conforms to the Service Level\nAgreements (SLA). However, the moment a component fails, or a customer\ncomplains about a breach of SLA, the importance of logs increases\nsignificantly. All departments, including management, customer support, and\neven the actual customer, may turn to logs to determine the cause and timeline\nof the issue and to find the party responsible for the issue. The party at\nfault may be motivated to tamper with the logs to hide their role. Given the\nnumber and volume of logs generated by the Cloud platforms, many tampering\nopportunities exist. We argue that the critical nature of logs calls for\nimmutability and verification mechanism without the presence of a single\ntrusted party.\n  This paper proposes such a mechanism by describing a blockchain-based log\nsystem, called Logchain, which can be integrated with existing private and\npublic blockchain solutions. Logchain uses the immutability feature of\nblockchain to provide a tamper-resistance storage platform for log storage.\nAdditionally, we propose a hierarchical structure to combine the hash-binding\nof two blockchains to address blockchains' scalability issues. To validate the\nmechanism, we integrate Logchain into two different types of blockchains. We\nchoose Ethereum as a public, permission-less blockchain and IBM Blockchain as a\nprivate, permission-based one. We show that the solution is scalable on both\nthe private and public blockchains. Additionally, we perform the analysis of\nthe cost of ownership for private and public blockchains implementations to\nhelp a reader selecting an implementation that would be applicable to their\nneeds.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:51:48 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Pourmajidi", "William", ""], ["Zhang", "Lei", ""], ["Steinbacher", "John", ""], ["Erwin", "Tony", ""], ["Miranskyy", "Andriy", ""]]}, {"id": "2009.07914", "submitter": "Daniel J\\\"unger", "authors": "Daniel J\\\"unger (1), Robin Kobus (1), Andr\\'e M\\\"uller (1), Christian\n  Hundt (2), Kai Xu (3), Weiguo Liu (3), Bertil Schmidt (1) ((1) Johannes\n  Gutenberg University, (2) NVIDIA AI Technology Center, (3) Shandong\n  University)", "title": "WarpCore: A Library for fast Hash Tables on GPUs", "comments": "To be published in HiPC 2020 (best paper award winner)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are ubiquitous. Properties such as an amortized constant time\ncomplexity for insertion and querying as well as a compact memory layout make\nthem versatile associative data structures with manifold applications.\n  The rapidly growing amount of data emerging in many fields motivated the need\nfor accelerated hash tables designed for modern parallel architectures. In this\nwork, we exploit the fast memory interface of modern GPUs together with a\nparallel hashing scheme tailored to improve global memory access patterns, to\ndesign WarpCore -- a versatile library of hash table data structures. Unique\ndevice-sided operations allow for building high performance data processing\npipelines entirely on the GPU. Our implementation achieves up to 1.6 billion\ninserts and up to 4.3 billion retrievals per second on a single GV100 GPU\nthereby outperforming the state-of-the-art solutions cuDPP, SlabHash, and\nNVIDIA RAPIDS cuDF. This performance advantage becomes even more pronounced for\nhigh load factors of over $90\\%$. To overcome the memory limitation of a single\nGPU, we scale our approach over a dense NVLink topology which gives us\nclose-to-optimal weak scaling on DGX servers. We further show how WarpCore can\nbe used for accelerating a real world bioinformatics application (metagenomic\nclassification) with speedups of over two orders-of-magnitude against\nstate-of-the-art CPU-based solutions. WC is written in C++/CUDA-C and is openly\navailable at https://github.com/sleeepyjack/warpcore.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 19:56:57 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 21:38:32 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["J\u00fcnger", "Daniel", ""], ["Kobus", "Robin", ""], ["M\u00fcller", "Andr\u00e9", ""], ["Hundt", "Christian", ""], ["Xu", "Kai", ""], ["Liu", "Weiguo", ""], ["Schmidt", "Bertil", ""]]}, {"id": "2009.07929", "submitter": "Mark Blanco", "authors": "Mark Blanco, Tze Meng Low, Kyungjoo Kim", "title": "Exploration of Fine-Grained Parallelism for Load Balancing Eager K-truss\n  on GPU and CPU", "comments": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916473", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a performance exploration on Eager K-truss, a\nlinear-algebraic formulation of the K-truss graph algorithm. We address\nperformance issues related to load imbalance of parallel tasks in symmetric,\ntriangular graphs by presenting a fine-grained parallel approach to executing\nthe support computation. This approach also increases available parallelism,\nmaking it amenable to GPU execution. We demonstrate our fine-grained parallel\napproach using implementations in Kokkos and evaluate them on an Intel Skylake\nCPU and an Nvidia Tesla V100 GPU. Overall, we observe between a 1.261. 48x\nimprovement on the CPU and a 9.97-16.92x improvement on the GPU due to our\nfine-grained parallel formulation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 20:45:04 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Blanco", "Mark", ""], ["Low", "Tze Meng", ""], ["Kim", "Kyungjoo", ""]]}, {"id": "2009.08044", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Nick Gonsalves, Christina Lee, Anand Raman, Brendan\n  Walsh, Siddhartha Prasad, Dalitso Banda, Lucy Zhang, Lei Zhang, William T.\n  Freeman", "title": "Large-Scale Intelligent Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.DC cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deploying Machine Learning (ML) algorithms within databases is a challenge\ndue to the varied computational footprints of modern ML algorithms and the\nmyriad of database technologies each with its own restrictive syntax. We\nintroduce an Apache Spark-based micro-service orchestration framework that\nextends database operations to include web service primitives. Our system can\norchestrate web services across hundreds of machines and takes full advantage\nof cluster, thread, and asynchronous parallelism. Using this framework, we\nprovide large scale clients for intelligent services such as speech, vision,\nsearch, anomaly detection, and text analysis. This allows users to integrate\nready-to-use intelligence into any datastore with an Apache Spark connector. To\neliminate the majority of overhead from network communication, we also\nintroduce a low-latency containerized version of our architecture. Finally, we\ndemonstrate that the services we investigate are competitive on a variety of\nbenchmarks, and present two applications of this framework to create\nintelligent search engines, and real-time auto race analytics systems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 03:38:28 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 20:51:47 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Hamilton", "Mark", ""], ["Gonsalves", "Nick", ""], ["Lee", "Christina", ""], ["Raman", "Anand", ""], ["Walsh", "Brendan", ""], ["Prasad", "Siddhartha", ""], ["Banda", "Dalitso", ""], ["Zhang", "Lucy", ""], ["Zhang", "Lei", ""], ["Freeman", "William T.", ""]]}, {"id": "2009.08173", "submitter": "Simon Eismann", "authors": "Simon Eismann, Joel Scheuner, Erwin van Eyk, Maximilian Schwinger,\n  Johannes Grohmann, Cristina L. Abad, Alexandru Iosup", "title": "Serverless Applications: Why, When, and How?", "comments": "8 pages, 3 figures, IEEE Software", "journal-ref": null, "doi": "10.1109/MS.2020.3023302", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing shows good promise for efficiency and ease-of-use. Yet,\nthere are only a few, scattered and sometimes conflicting reports on questions\nsuch as 'Why do so many companies adopt serverless?', 'When are serverless\napplications well suited?', and 'How are serverless applications currently\nimplemented?' To address these questions, we analyze 89 serverless applications\nfrom open-source projects, industrial sources, academic literature, and\nscientific computing - the most extensive study to date.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 09:32:20 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 09:02:17 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Eismann", "Simon", ""], ["Scheuner", "Joel", ""], ["van Eyk", "Erwin", ""], ["Schwinger", "Maximilian", ""], ["Grohmann", "Johannes", ""], ["Abad", "Cristina L.", ""], ["Iosup", "Alexandru", ""]]}, {"id": "2009.08208", "submitter": "Victor Isaac Kolobov", "authors": "Keren Censor-Hillel, Victor I. Kolobov, Gregory Schwartzman", "title": "Finding Subgraphs in Highly Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the fundamental problem of finding subgraphs in\nhighly dynamic distributed networks - networks which allow an arbitrary number\nof links to be inserted / deleted per round. We show that the problems of\n$k$-clique membership listing (for any $k\\geq 3$), 4-cycle listing and 5-cycle\nlisting can be deterministically solved in $O(1)$-amortized round complexity,\neven with limited logarithmic-sized messages.\n  To achieve $k$-clique membership listing we introduce a very useful\ncombinatorial structure which we name the robust $2$-hop neighborhood. This is\na subset of the 2-hop neighborhood of a node, and we prove that it can be\nmaintained in highly dynamic networks in $O(1)$-amortized rounds. We also show\nthat maintaining the actual 2-hop neighborhood of a node requires near linear\namortized time, showing the necessity of our definition. For $4$-cycle and\n$5$-cycle listing, we need edges within hop distance 3, for which we similarly\ndefine the robust $3$-hop neighborhood and prove it can be maintained in highly\ndynamic networks in $O(1)$-amortized rounds.\n  We complement the above with several impossibility results. We show that\nmembership listing of any other graph on $k\\geq 3$ nodes except $k$-clique\nrequires an almost linear number of amortized communication rounds. We also\nshow that $k$-cycle listing for $k\\geq 6$ requires $\\Omega(\\sqrt{n} / \\log n)$\namortized rounds. This, combined with our upper bounds, paints a detailed\npicture of the complexity landscape for ultra fast graph finding algorithms in\nthis highly dynamic environment.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 11:02:09 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Kolobov", "Victor I.", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "2009.08289", "submitter": "Mohak Chadha", "authors": "Mohak Chadha, Jophin John, Michael Gerndt", "title": "Extending SLURM for Dynamic Resource-Aware Adaptive Batch Scheduling", "comments": "IEEE International Conference on High Performance Computing, Data,\n  and Analytics (HiPC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing constraints on power budget and increasing hardware failure\nrates, the operation of future exascale systems faces several challenges.\nTowards this, resource awareness and adaptivity by enabling malleable jobs has\nbeen actively researched in the HPC community. Malleable jobs can change their\ncomputing resources at runtime and can significantly improve HPC system\nperformance. However, due to the rigid nature of popular parallel programming\nparadigms such as MPI and lack of support for dynamic resource management in\nbatch systems, malleable jobs have been largely unrealized. In this paper, we\nextend the SLURM batch system to support the execution and batch scheduling of\nmalleable jobs. The malleable applications are written using a new adaptive\nparallel paradigm called Invasive MPI which extends the MPI standard to support\nresource-adaptivity at runtime. We propose two malleable job scheduling\nstrategies to support performance-aware and power-aware dynamic reconfiguration\ndecisions at runtime. We implement the strategies in SLURM and evaluate them on\na production HPC system. Results for our performance-aware scheduling strategy\nshow improvements in makespan, average system utilization, average response,\nand waiting times as compared to other scheduling strategies. Moreover, we\ndemonstrate dynamic power corridor management using our power-aware strategy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:43:08 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 10:04:02 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chadha", "Mohak", ""], ["John", "Jophin", ""], ["Gerndt", "Michael", ""]]}, {"id": "2009.08327", "submitter": "Tayyebeh Jahani-Nezhad", "authors": "Tayyebeh Jahani-Nezhad, Mohammad Ali Maddah-Ali", "title": "Berrut Approximated Coded Computing: Straggler Resistance Beyond\n  Polynomial Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in using distributed learning to train\ncomplicated models with large data sets is to deal with stragglers effect. As a\nsolution, coded computation has been recently proposed to efficiently add\nredundancy to the computation tasks. In this technique, coding is used across\ndata sets, and computation is done over coded data, such that the results of an\narbitrary subset of worker nodes with a certain size are enough to recover the\nfinal results. The major challenges with those approaches are (1) they are\nlimited to polynomial function computations, (2) the size of the subset of\nservers that we need to wait for grows with the multiplication of the size of\nthe data set and the model complexity (the degree of the polynomial), which can\nbe prohibitively large, (3) they are not numerically stable for computation\nover real numbers. In this paper, we propose Berrut Approximated Coded\nComputing (BACC), as an alternative approach, which is not limited to\npolynomial function computation. In addition, the master node can approximately\ncalculate the final results, using the outcomes of any arbitrary subset of\navailable worker nodes. The approximation approach is proven to be numerically\nstable with low computational complexity. In addition, the accuracy of the\napproximation is established theoretically and verified by simulation results\nin different settings such as distributed learning problems. In particular,\nBACC is used to train a deep neural network on a cluster of servers, which\noutperforms repetitive computation (repetition coding) in terms of the rate of\nconvergence.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 14:23:38 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 16:11:42 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Jahani-Nezhad", "Tayyebeh", ""], ["Maddah-Ali", "Mohammad Ali", ""]]}, {"id": "2009.08495", "submitter": "Paula Olaya", "authors": "Paula Olaya, Jay Lofstead, and Michela Taufer", "title": "Building Containerized Environments for Reproducibility and Traceability\n  of Scientific Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists rely on simulations to study natural phenomena. Trusting the\nsimulation results is vital to develop sciences in any field. One approach to\nbuild trust is to ensure the reproducibility and traceability of the\nsimulations through the annotation of executions at the system-level; by the\ngeneration of record trails of data moving through the simulation workflow. In\nthis work, we present a system-level solution that leverages the intrinsic\ncharacteristics of containers (i.e., portability, isolation, encapsulation, and\nunique identifiers). Our solution consists of a containerized environment\ncapable to annotate workflows, capture provenance metadata, and build record\ntrails. We assess our environment on four different workflows and measure\ncontainerization costs in terms of time and space. Our solution, built with a\ntolerable time and space overhead, enables transparent and automatic provenance\nmetadata collection and access, an easy-to-read record trail, and tight\nconnections between data and metadata.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 18:43:44 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Olaya", "Paula", ""], ["Lofstead", "Jay", ""], ["Taufer", "Michela", ""]]}, {"id": "2009.08575", "submitter": "Scott Kominers", "authors": "Daniel M. Kane, Scott Duke Kominers", "title": "Prisoners, Rooms, and Lightswitches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.GT math.CO math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a new variant of the classic prisoners and lightswitches puzzle: A\nwarden leads his $n$ prisoners in and out of $r$ rooms, one at a time, in some\norder, with each prisoner eventually visiting every room an arbitrarily large\nnumber of times. The rooms are indistinguishable, except that each one has $s$\nlightswitches; the prisoners win their freedom if at some point a prisoner can\ncorrectly declare that each prisoner has been in every room at least once. What\nis the minimum number of switches per room, $s$, such that the prisoners can\nmanage this? We show that if the prisoners do not know the switches' starting\nconfiguration, then they have no chance of escape -- but if the prisoners do\nknow the starting configuration, then the minimum sufficient $s$ is\nsurprisingly small. The analysis gives rise to a number of puzzling open\nquestions, as well.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 01:05:22 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Kane", "Daniel M.", ""], ["Kominers", "Scott Duke", ""]]}, {"id": "2009.08716", "submitter": "Zhengjie Yang", "authors": "Zhengjie Yang, Wei Bao, Dong Yuan, Nguyen H. Tran, and Albert Y.\n  Zomaya", "title": "Federated Learning with Nesterov Accelerated Gradient Momentum Method", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a fast-developing technique that allows multiple\nworkers to train a global model based on a distributed dataset. Conventional FL\nemploys gradient descent algorithm, which may not be efficient enough. It is\nwell known that Nesterov Accelerated Gradient (NAG) is more advantageous in\ncentralized training environment, but it is not clear how to quantify the\nbenefits of NAG in FL so far. In this work, we focus on a version of FL based\non NAG (FedNAG) and provide a detailed convergence analysis. The result is\ncompared with conventional FL based on gradient descent. One interesting\nconclusion is that as long as the learning step size is sufficiently small,\nFedNAG outperforms FedAvg. Extensive experiments based on real-world datasets\nare conducted, verifying our conclusions and confirming the better convergence\nperformance of FedNAG.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:38:11 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Yang", "Zhengjie", ""], ["Bao", "Wei", ""], ["Yuan", "Dong", ""], ["Tran", "Nguyen H.", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "2009.08847", "submitter": "Talley Amir", "authors": "Talley Amir, James Aspnes, John Lazarsfeld", "title": "Approximate Majority With Catalytic Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a class of algorithms for modeling distributed\ncomputation in networks of finite-state agents communicating through pairwise\ninteractions. Their suitability for analyzing numerous chemical processes has\nmotivated the adaptation of the original population protocol framework to\nbetter model these chemical systems. In this paper, we further the study of two\nsuch adaptations in the context of solving approximate majority:\npersistent-state agents (or catalysts) and spontaneous state changes (or\nleaks).\n  Based on models considered in recent protocols for populations with\npersistent-state agents, we assume a population with $n$ catalytic input agents\nand $m$ worker agents, and the goal of the worker agents is to compute some\npredicate over the states of the catalytic inputs. We call this model the\nCatalytic Input (CI) model. For $m = \\Theta(n)$, we show that computing the\nparity of the input population with high probability requires at least\n$\\Omega(n^2)$ total interactions, demonstrating a strong separation between the\nCI model and the standard population protocol model. On the other hand, we show\nthat the simple third-state dynamics of Angluin et al. for approximate majority\nin the standard model can be naturally adapted to the CI model: we present such\na constant-state protocol for the CI model that solves approximate majority in\n$O(n \\log n)$ total steps with high probability when the input margin is\n$\\Omega(\\sqrt{n \\log n})$.\n  We then show the robustness of third-state dynamics protocols to the\ntransient leaks events introduced by Alistarh et al. In both the original and\nCI models, these protocols successfully compute approximate majority with high\nprobability in the presence of leaks occurring at each step with probability\n$\\beta \\leq O\\left(\\sqrt{n \\log n}/n\\right)$, exhibiting a resilience to leaks\nsimilar to that of Byzantine agents in previous works.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 14:05:36 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 23:13:44 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 15:02:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Amir", "Talley", ""], ["Aspnes", "James", ""], ["Lazarsfeld", "John", ""]]}, {"id": "2009.08912", "submitter": "Akshay Dhumal", "authors": "Akshay Dhumal (IIT Madras), Dharanipragada Janakiram (IIT Madras)", "title": "C-Balancer: A System for Container Profiling and Scheduling", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linux containers have gained high popularity in recent times. This popularity\nis significantly due to various advantages of containers over Virtual Machines\n(VM). The containers are lightweight, occupy lesser storage, have fast boot-up\ntime, easy to deploy and have faster auto-scaling. The key reason behind the\npopularity of containers is that they leverage the mechanism of micro-service\nstyle software development, where applications are designed as independently\ndeployable services. There are various container orchestration tools for\ndeploying and managing the containers in the cluster. The prominent among them\nare Docker Swarm and Kubernetes. However, they do not address the effects of\nresource contention when multiple containers are deployed on a node. Moreover,\nthey do not provide support for container migration in the event of an attack\nor increased resource contention. To address such issues, we propose\nC-Balancer, a scheduling framework for efficient placement of containers in the\ncluster environment. C-Balancer works by periodically profiling the containers\nand deciding the optimal container to node placement. Our proposed approach\nimproves the performance of containers in terms of resource utilization and\nthroughput. Experiments using a workload mix of Stress-NG and iPerf benchmark\nshows that our proposed approach achieves a maximum performance improvement of\n58% for the workload mix. Our approach also reduces the variance in resource\nutilization across the cluster by 60% on average.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 16:17:10 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Dhumal", "Akshay", "", "IIT Madras"], ["Janakiram", "Dharanipragada", "", "IIT Madras"]]}, {"id": "2009.09103", "submitter": "Santosh Pandey", "authors": "Santosh Pandey, Lingda Li, Adolfy Hoisie, Xiaoye S. Li, Hang Liu", "title": "C-SAW: A Framework for Graph Sampling and Random Walk on GPUs", "comments": "12 pages,IEEE Proceedings of the International Conference for High\n  Performance Computing, Networking, Storage and Analysis (SC20)", "journal-ref": null, "doi": "10.1109/SC41405.2020.00060", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require to learn, mine, analyze and visualize large-scale\ngraphs. These graphs are often too large to be addressed efficiently using\nconventional graph processing technologies. Many applications have requirements\nto analyze, transform, visualize and learn large scale graphs. These graphs are\noften too large to be addressed efficiently using conventional graph processing\ntechnologies. Recent literatures convey that graph sampling/random walk could\nbe an efficient solution. In this paper, we propose, to the best of our\nknowledge, the first GPU-based framework for graph sampling/random walk. First,\nour framework provides a generic API which allows users to implement a wide\nrange of sampling and random walk algorithms with ease. Second, offloading this\nframework on GPU, we introduce warp-centric parallel selection, and two\noptimizations for collision migration. Third, towards supporting graphs that\nexceed GPU memory capacity, we introduce efficient data transfer optimizations\nfor out-of-memory sampling, such as workload-aware scheduling and batched\nmulti-instance sampling. In its entirety, our framework constantly outperforms\nthe state-of-the-art projects. First, our framework provides a generic API\nwhich allows users to implement a wide range of sampling and random walk\nalgorithms with ease. Second, offloading this framework on GPU, we introduce\nwarp-centric parallel selection, and two novel optimizations for collision\nmigration. Third, towards supporting graphs that exceed the GPU memory\ncapacity, we introduce efficient data transfer optimizations for out-of-memory\nand multi-GPU sampling, such as workload-aware scheduling and batched\nmulti-instance sampling. Taken together, our framework constantly outperforms\nthe state of the art projects in addition to the capability of supporting a\nwide range of sampling and random walk algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 22:05:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Pandey", "Santosh", ""], ["Li", "Lingda", ""], ["Hoisie", "Adolfy", ""], ["Li", "Xiaoye S.", ""], ["Liu", "Hang", ""]]}, {"id": "2009.09271", "submitter": "Negar Foroutan", "authors": "Negar Foroutan Eghlidi and Martin Jaggi", "title": "Sparse Communication for Training Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous stochastic gradient descent (SGD) is the most common method used\nfor distributed training of deep learning models. In this algorithm, each\nworker shares its local gradients with others and updates the parameters using\nthe average gradients of all workers. Although distributed training reduces the\ncomputation time, the communication overhead associated with the gradient\nexchange forms a scalability bottleneck for the algorithm. There are many\ncompression techniques proposed to reduce the number of gradients that needs to\nbe communicated. However, compressing the gradients introduces yet another\noverhead to the problem. In this work, we study several compression schemes and\nidentify how three key parameters affect the performance. We also provide a set\nof insights on how to increase performance and introduce a simple\nsparsification scheme, random-block sparsification, that reduces communication\nwhile keeping the performance close to standard SGD.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 17:28:11 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Eghlidi", "Negar Foroutan", ""], ["Jaggi", "Martin", ""]]}, {"id": "2009.09480", "submitter": "Andrew Lewis-Pye", "authors": "Andrew Lewis-Pye and Tim Roughgarden", "title": "A General Framework for the Security Analysis of Blockchain Protocols", "comments": "This paper has been merged with arXiv:2101.07095", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain protocols differ in fundamental ways, including the mechanics of\nselecting users to produce blocks (e.g., proof-of-work vs. proof-of-stake) and\nthe method to establish consensus (e.g., longest chain rules vs. Byzantine\nfault-tolerant (BFT) inspired protocols). These fundamental differences have\nhindered \"apples-to-apples\" comparisons between different categories of\nblockchain protocols and, in turn, the development of theory to formally\ndiscuss their relative merits.\n  This paper presents a parsimonious abstraction sufficient for capturing and\ncomparing properties of many well-known permissionless blockchain protocols,\nsimultaneously capturing essential properties of both proof-of-work (PoW) and\nproof-of-stake (PoS) protocols, and of both longest-chain-type and BFT-type\nprotocols. Our framework blackboxes the precise mechanics of the user selection\nprocess, allowing us to isolate the properties of the selection process that\nare significant for protocol design.\n  We demonstrate the utility of our general framework with several concrete\nresults:\n  1. We prove a CAP-type impossibility theorem asserting that liveness with an\nunknown level of participation rules out security in a partially synchronous\nsetting.\n  2. Delving deeper into the partially synchronous setting, we prove that a\nnecessary and sufficient condition for security is the production of\n\"certificates,\" meaning stand-alone proofs of block confirmation.\n  3. Restricting to synchronous settings, we prove that typical protocols with\na known level of participation (including longest chain-type PoS protocols) can\nbe adapted to provide certificates, but those with an unknown level of\nparticipation cannot.\n  4. Finally, we use our framework to articulate a modular two-step approach to\nblockchain security analysis that effectively reduces the permissionless case\nto the permissioned case.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 17:30:22 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:22:16 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 12:53:18 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Lewis-Pye", "Andrew", ""], ["Roughgarden", "Tim", ""]]}, {"id": "2009.09523", "submitter": "Andrew Or", "authors": "Andrew Or, Haoyu Zhang, Michael J. Freedman", "title": "VirtualFlow: Decoupling Deep Learning Models from the Underlying\n  Hardware", "comments": "12 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning systems such as TensorFlow and PyTorch tightly\ncouple the model with the underlying hardware. This coupling requires the user\nto modify application logic in order to run the same job across a different set\nof resources, thereby limiting the choice of hardware for a given workload and\npotentially forcing the user to forgo more efficient hardware configurations.\n  We propose VirtualFlow, a system leveraging a novel abstraction called\nvirtual node processing to decouple the model from the hardware. In each step\nof training or inference, the batch of input data is split across virtual nodes\ninstead of hardware accelerators (e.g. GPUs and TPUs). Mapping multiple virtual\nnodes to each accelerator and processing them sequentially effectively time\nslices the batch, thereby allowing users to reduce the memory requirement of\ntheir workloads and mimic large batch sizes on small clusters.\n  Using this technique, VirtualFlow enables many new use cases, such as\nreproducing training results across different hardware, resource elasticity,\nand heterogeneous training. In our evaluation, our implementation of\nVirtualFlow for TensorFlow achieved strong convergence guarantees across\ndifferent hardware with out-of-the-box hyperparameters, up to 48% lower job\ncompletion times with resource elasticity, and up to 42% higher throughput with\nheterogeneous training.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 20:49:48 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 20:35:46 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Or", "Andrew", ""], ["Zhang", "Haoyu", ""], ["Freedman", "Michael J.", ""]]}, {"id": "2009.09549", "submitter": "Babar Shahzaad", "authors": "Babar Shahzaad (1), Athman Bouguettaya (1), Sajib Mistry (2), Azadeh\n  Ghari Neiat (3) ((1) The University of Sydney, Sydney NSW 2000, Australia,\n  (2) Curtin University, Perth WA 6102, Australia, (3) Deakin University,\n  Geelong VIC 3220, Australia)", "title": "Resilient Composition of Drone Services for Delivery", "comments": "48 pages, 16 figures. This is an accepted paper and it is going to\n  appear in Future Generation Computer Systems journal Elsevier. Content may\n  change prior to final publication", "journal-ref": "Future Generation Computer Systems, 2020", "doi": "10.1016/j.future.2020.09.023", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel resilient drone service composition framework for delivery\nin dynamic weather conditions. We use a skyline approach to select an optimal\nset of candidate drone services at the source node in a skyway network. Drone\nservices are initially composed using a novel constraint-aware deterministic\nlookahead algorithm using the multi-armed bandit tree exploration. We propose a\nheuristic-based resilient service composition approach that adapts to runtime\nchanges and periodically updates the composition to meet delivery expectations.\nExperimental results prove the efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 00:19:47 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Shahzaad", "Babar", ""], ["Bouguettaya", "Athman", ""], ["Mistry", "Sajib", ""], ["Neiat", "Azadeh Ghari", ""]]}, {"id": "2009.09645", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang", "title": "The Complexity Landscape of Distributed Locally Checkable Problems on\n  Trees", "comments": "To appear in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research revealed the existence of gaps in the complexity landscape of\nlocally checkable labeling (LCL) problems in the LOCAL model of distributed\ncomputing. For example, the deterministic round complexity of any LCL problem\non bounded-degree graphs is either $O(\\log^\\ast n)$ or $\\Omega(\\log n)$ [Chang,\nKopelowitz, and Pettie, FOCS 2016]. The complexity landscape of LCL problems is\nnow quite well-understood, but a few questions remain open.\n  For bounded-degree trees, there is an LCL problem with round complexity\n$\\Theta(n^{1/k})$ for each positive integer $k$ [Chang and Pettie, FOCS 2017].\nIt is conjectured that no LCL problem has round complexity $o(n^{1/(k-1)})$ and\n$\\omega(n^{1/k})$ on bounded-degree trees. As of now, only the case of $k = 2$\nhas been proved [Balliu et al., DISC 2018].\n  In this paper, we show that for LCL problems on bounded-degree trees, there\nis indeed a gap between $\\Theta(n^{1/(k-1)})$ and $\\Theta(n^{1/k})$ for each $k\n\\geq 2$. Our proof is constructive in the sense that it offers a sequential\nalgorithm that decides which side of the gap a given LCL problem belongs to. We\nalso show that it is EXPTIME-hard to distinguish between $\\Theta(1)$-round and\n$\\Theta(n)$-round LCL problems on bounded-degree trees. This improves upon a\nprevious PSPACE-hardness result [Balliu et al., PODC 2019].\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:07:55 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chang", "Yi-Jun", ""]]}, {"id": "2009.09647", "submitter": "Soohyun Park", "authors": "Soohyun Park, Jeman Park, David Mohaisen, Joongheon Kim", "title": "Reinforced Edge Selection using Deep Learning for Robust Surveillance in\n  Unmanned Aerial Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep Q-network (DQN)-based edge selection\nalgorithm designed specifically for real-time surveillance in unmanned aerial\nvehicle (UAV) networks. The proposed algorithm is designed under the\nconsideration of delay, energy, and overflow as optimizations to ensure\nreal-time properties while striking a balance for other environment-related\nparameters. The merit of the proposed algorithm is verified via\nsimulation-based performance evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:12:07 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Park", "Soohyun", ""], ["Park", "Jeman", ""], ["Mohaisen", "David", ""], ["Kim", "Joongheon", ""]]}, {"id": "2009.09671", "submitter": "Dimitrios Vasilas", "authors": "Dimitrios Vasilas (DELYS, SU), Marc Shapiro (DELYS, SU), Bradley King,\n  Sara Hamouda (DELYS, SU)", "title": "Towards application-specific query processing systems", "comments": null, "journal-ref": "36{\\`e}me Conf{\\'e}rence sur la Gestion de Donn{\\'e}es --\n  Principes, Technologies et Applications (BDA 2020), Oct 2020, Paris, France", "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database systems use query processing subsystems for enabling efficient\nquery-based data retrieval. An essential aspect of designing any\nquery-intensive application is tuning the query system to fit the application's\nrequirements and workload characteristics. However, the configuration\nparameters provided by traditional database systems do not cover the design\ndecisions and trade-offs that arise from the geo-distribution of users and\ndata. In this paper, we present a vision towards a new type of query system\narchitecture that addresses this challenge by enabling query systems to be\ndesigned and deployed in a per use case basis. We propose a distributed\nabstraction called Query Processing Unit that encapsulates primitive query\nprocessing tasks, and show how it can be used as a building block for\nassembling query systems. Using this approach, application architects can\nconstruct query systems specialized to their use cases, by controlling the\nquery system's architecture and the placement of its state. We demonstrate the\nexpressiveness of this approach by applying it to the design of a query system\nthat can flexibly place its state in the data center or at the edge, and show\nthat state placement decisions affect the trade-off between query response time\nand query result freshness.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:13:19 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Vasilas", "Dimitrios", "", "DELYS, SU"], ["Shapiro", "Marc", "", "DELYS, SU"], ["King", "Bradley", "", "DELYS, SU"], ["Hamouda", "Sara", "", "DELYS, SU"]]}, {"id": "2009.09800", "submitter": "Ji Liu", "authors": "Ji Liu (1), Hang Zhao (1), Jiyuan Yang (1), Yu Shi (1), Ruichang Liu\n  (1), Dong Yuan (1) and Shiping Chen (1 and 2) ((1) School of Electrical &\n  Information Engineering, University of Sydney, Australia, (2) Commonwealth\n  Scientific & Industrial Research Organisation (CSIRO), DATA61, Australia)", "title": "ServiceNet: A P2P Service Network", "comments": "15 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large number of online services on the Internet, from time to time,\npeople are still struggling to find out the services that they need. On the\nother hand, when there are considerable research and development on service\ndiscovery and service recommendation, most of the related work are centralized\nand thus suffers inherent shortages of the centralized systems, e.g.,\nadv-driven, lack at trust, transparence and fairness. In this paper, we propose\na ServiceNet - a peer-to-peer (P2P) service network for service discovery and\nservice recommendation. ServiceNet is inspired by blockchain technology and\naims at providing an open, transparent and self-growth, and self-management\nservice ecosystem. The paper will present the basic idea, an architecture\ndesign of the prototype, and an initial implementation and performance\nevaluation the prototype design.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 08:58:41 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Liu", "Ji", "", "1 and 2"], ["Zhao", "Hang", "", "1 and 2"], ["Yang", "Jiyuan", "", "1 and 2"], ["Shi", "Yu", "", "1 and 2"], ["Liu", "Ruichang", "", "1 and 2"], ["Yuan", "Dong", "", "1 and 2"], ["Chen", "Shiping", "", "1 and 2"]]}, {"id": "2009.09845", "submitter": "Johann Schleier-Smith", "authors": "Johann Schleier-Smith and Leonhard Holz and Nathan Pemberton and\n  Joseph M. Hellerstein", "title": "A FaaS File System for Serverless Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing with cloud functions is quickly gaining adoption, but\nconstrains programmers with its limited support for state management. We\nintroduce a shared file system for cloud functions. It offers familiar POSIX\nsemantics while taking advantage of distinctive aspects of cloud functions to\nachieve scalability and performance beyond what traditional shared file systems\ncan offer. We take advantage of the function-grained fault tolerance model of\ncloud functions to proceed optimistically using local state, safe in the\nknowledge that we can restart if cache reads or lock activity cannot be\nreconciled upon commit. The boundaries of cloud functions provide implicit\ncommit and rollback points, giving us the flexibility to use transaction\nprocessing techniques without changing the programming model or API. This\nallows a variety of stateful sever-based applications to benefit from the\nsimplicity and scalability of serverless computing, often with little or no\nmodification.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 08:16:27 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Schleier-Smith", "Johann", ""], ["Holz", "Leonhard", ""], ["Pemberton", "Nathan", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "2009.10035", "submitter": "Md. Khaledur Rahman", "authors": "Md. Khaledur Rahman, Majedul Haque Sujon, Ariful Azad", "title": "Force2Vec: Parallel force-directed graph embedding", "comments": "Accepted for publication in ICDM 2020 as a regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A graph embedding algorithm embeds a graph into a low-dimensional space such\nthat the embedding preserves the inherent properties of the graph. While graph\nembedding is fundamentally related to graph visualization, prior work did not\nexploit this connection explicitly. We develop Force2Vec that uses\nforce-directed graph layout models in a graph embedding setting with an aim to\nexcel in both machine learning (ML) and visualization tasks. We make Force2Vec\nhighly parallel by mapping its core computations to linear algebra and\nutilizing multiple levels of parallelism available in modern processors. The\nresultant algorithm is an order of magnitude faster than existing methods (43x\nfaster than DeepWalk, on average) and can generate embeddings from graphs with\nbillions of edges in a few hours. In comparison to existing methods, Force2Vec\nis better in graph visualization and performs comparably or better in ML tasks\nsuch as link prediction, node classification, and clustering. Source code is\navailable at https://github.com/HipGraph/Force2Vec.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 03:53:25 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Rahman", "Md. Khaledur", ""], ["Sujon", "Majedul Haque", ""], ["Azad", "Ariful", ""]]}, {"id": "2009.10043", "submitter": "Michael Eischer", "authors": "Michael Eischer and Tobias Distler", "title": "Resilient Cloud-based Replication with Low Latency", "comments": "25 pages, extended version of Middleware 2020 paper", "journal-ref": null, "doi": "10.1145/3423211.3425689", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to tolerate Byzantine faults in geo-replicated\nenvironments require systems to execute complex agreement protocols over\nwide-area links and consequently are often associated with high response times.\nIn this paper we address this problem with Spider, a resilient replication\narchitecture for geo-distributed systems that leverages the availability\ncharacteristics of today's public-cloud infrastructures to minimize complexity\nand reduce latency. Spider models a system as a collection of loosely coupled\nreplica groups whose members are hosted in different cloud-provided fault\ndomains (i.e., availability zones) of the same geographic region. This\nstructural organization makes it possible to achieve low response times by\nplacing replica groups in close proximity to clients while still enabling the\nreplicas of a group to interact over short-distance links. To handle the\ninter-group communication necessary for strong consistency Spider uses a\nreliable group-to-group message channel with first-in-first-out semantics and\nbuilt-in flow control that significantly simplifies system design.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:31:42 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Eischer", "Michael", ""], ["Distler", "Tobias", ""]]}, {"id": "2009.10182", "submitter": "Javad Mohammadi", "authors": "Javad Mohammadi and Jesse Thornburg", "title": "Connecting Distributed Pockets of EnergyFlexibility through Federated\n  Computations:Limitations and Possibilities", "comments": "Accepted for presentation at ICMLA 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric grids are traditionally operated as multi-entity systems with each\nentity managing a geographical region. Interest and demand for decarbonization\nand energy democratization is resulting in growing penetration of controllable\nenergy resources. In turn, this process is increasing the number of grid\nentities. The paradigm shift is also fueled by increased adoption of\nintelligent sensors and actuators equipped with advanced processing and\ncomputing capabilities. While collaboration among power grid entities (agents)\nreduces energy cost and increases overall reliability, achieving effective\ncollaboration is challenging. The main challenges stem from the heterogeneity\nof system agents and their collected information. Furthermore, the scale of\ndata collection is constantly increasing and many grid entities have strict\nprivacy requirements. Another challenge is the energy industry's common\npractice of keeping data in silos. Federated computation is an approach well\nsuited to addressing these issues that are increasingly important for\nmulti-agent energy systems. Through federated computation, agents\ncollaboratively solve learning and optimization problems while respecting each\nagent's privacy and overcoming barriers of cross-device and cross-organization\ndata isolation. In this paper, we first establish the need for federated\ncomputations to achieve energy optimization goals of the future power grid. We\ndiscuss practical challenges of performing multi-agent data processing in\ngeneral. Then we address challenges that arise specifically for orchestrating\noperation of connected distributed energy resources in the Internet of Things.\nWe conclude this paper by presenting a novel federated computation framework\nthat addresses some of these issues, and we share examples of two initial field\ntest setups in research demonstrations and commercial building applications\nwith Grid Fruit LLC.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 21:21:27 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mohammadi", "Javad", ""], ["Thornburg", "Jesse", ""]]}, {"id": "2009.10245", "submitter": "EPTCS", "authors": "Stefano Forti (Department of Computer Science, University of Pisa,\n  Italy), Antonio Brogi (Department of Computer Science, University of Pisa,\n  Italy)", "title": "Continuous Reasoning for Managing Next-Gen Distributed Applications", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158", "journal-ref": "EPTCS 325, 2020, pp. 164-177", "doi": "10.4204/EPTCS.325.22", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous reasoning has proven effective in incrementally analysing changes\nin application codebases within Continuous Integration/Continuous Deployment\n(CI/CD) software release pipelines. In this article, we present a novel\ndeclarative continuous reasoning approach to support the management of\nmulti-service applications over the Cloud-IoT continuum, in particular when\ninfrastructure variations impede meeting application's hardware, software, IoT\nor network QoS requirements. We show how such an approach brings considerable\nspeed-ups compared to non-incremental reasoning.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:49:38 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Forti", "Stefano", "", "Department of Computer Science, University of Pisa,\n  Italy"], ["Brogi", "Antonio", "", "Department of Computer Science, University of Pisa,\n  Italy"]]}, {"id": "2009.10348", "submitter": "Cristian Galleguillos", "authors": "Cristian Galleguillos, Zeynep Kiziltan, Ricardo Soto", "title": "A Constraint Programming-based Job Dispatcher for Modern HPC Systems and\n  Applications", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constraint Programming (CP) is a well-established area in AI as a programming\nparadigm for modelling and solving discrete optimization problems, and it has\nbeen been successfully applied to tackle the on-line job dispatching problem in\nHPC systems including those running modern applications. The limitations of the\navailable CP-based job dispatchers may hinder their practical use in today's\nsystems that are becoming larger in size and more demanding in resource\nallocation. In an attempt to bring basic AI research closer to a deployed\napplication, we present a new CP-based on-line job dispatcher for modern HPC\nsystems and applications. Unlike its predecessors, our new dispatcher tackles\nthe entire problem in CP and its model size is independent of the system size.\nExperimental results based on a simulation study show that with our approach\ndispatching performance increases significantly in a large system and in a\nsystem where allocation is nontrivial.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 07:02:26 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 20:28:03 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Galleguillos", "Cristian", ""], ["Kiziltan", "Zeynep", ""], ["Soto", "Ricardo", ""]]}, {"id": "2009.10401", "submitter": "Qinghua Lu", "authors": "Weishan Zhang, Tao Zhou, Qinghua Lu, Xiao Wang, Chunsheng Zhu, Haoyun\n  Sun, Zhipeng Wang, Sin Kit Lo, Fei-Yue Wang", "title": "Dynamic Fusion based Federated Learning for COVID-19 Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical diagnostic image analysis (e.g., CT scan or X-Ray) using machine\nlearning is an efficient and accurate way to detect COVID-19 infections.\nHowever, sharing diagnostic images across medical institutions is usually not\nallowed due to the concern of patients' privacy. This causes the issue of\ninsufficient datasets for training the image classification model. Federated\nlearning is an emerging privacy-preserving machine learning paradigm that\nproduces an unbiased global model based on the received updates of local models\ntrained by clients without exchanging clients' local data. Nevertheless, the\ndefault setting of federated learning introduces huge communication cost of\ntransferring model updates and can hardly ensure model performance when data\nheterogeneity of clients heavily exists. To improve communication efficiency\nand model performance, in this paper, we propose a novel dynamic fusion-based\nfederated learning approach for medical diagnostic image analysis to detect\nCOVID-19 infections. First, we design an architecture for dynamic fusion-based\nfederated learning systems to analyse medical diagnostic images. Further, we\npresent a dynamic fusion method to dynamically decide the participating clients\naccording to their local model performance and schedule the model fusion-based\non participating clients' training time. In addition, we summarise a category\nof medical diagnostic image datasets for COVID-19 detection, which can be used\nby the machine learning community for image analysis. The evaluation results\nshow that the proposed approach is feasible and performs better than the\ndefault setting of federated learning in terms of model performance,\ncommunication efficiency and fault tolerance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 09:09:10 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 01:38:33 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 09:04:14 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 01:37:53 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Weishan", ""], ["Zhou", "Tao", ""], ["Lu", "Qinghua", ""], ["Wang", "Xiao", ""], ["Zhu", "Chunsheng", ""], ["Sun", "Haoyun", ""], ["Wang", "Zhipeng", ""], ["Lo", "Sin Kit", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "2009.10443", "submitter": "Alberto Parravicini", "authors": "Alberto Parravicini, Francesco Sgherzi, Marco D. Santambrogio", "title": "A reduced-precision streaming SpMV architecture for Personalized\n  PageRank on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-vector multiplication is often employed in many data-analytic\nworkloads in which low latency and high throughput are more valuable than exact\nnumerical convergence. FPGAs provide quick execution times while offering\nprecise control over the accuracy of the results thanks to reduced-precision\nfixed-point arithmetic. In this work, we propose a novel streaming\nimplementation of Coordinate Format (COO) sparse matrix-vector multiplication,\nand study its effectiveness when applied to the Personalized PageRank\nalgorithm, a common building block of recommender systems in e-commerce\nwebsites and social networks. Our implementation achieves speedups up to 6x\nover a reference floating-point FPGA architecture and a state-of-the-art\nmulti-threaded CPU implementation on 8 different data-sets, while preserving\nthe numerical fidelity of the results and reaching up to 42x higher energy\nefficiency compared to the CPU implementation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 10:44:46 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Parravicini", "Alberto", ""], ["Sgherzi", "Francesco", ""], ["Santambrogio", "Marco D.", ""]]}, {"id": "2009.10465", "submitter": "Hao Zhang", "authors": "Hao Zhang, Joey Tianyi Zhou, Tianying Wang, Ivor W. Tsang, Rick Siow\n  Mong Goh", "title": "Deep N-ary Error Correcting Output Codes", "comments": "EAI MOBIMEDIA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning consistently improves the performance of multi-class\nclassification through aggregating a series of base classifiers. To this end,\ndata-independent ensemble methods like Error Correcting Output Codes (ECOC)\nattract increasing attention due to its easiness of implementation and\nparallelization. Specifically, traditional ECOCs and its general extension\nN-ary ECOC decompose the original multi-class classification problem into a\nseries of independent simpler classification subproblems. Unfortunately,\nintegrating ECOCs, especially N-ary ECOC with deep neural networks, termed as\ndeep N-ary ECOC, is not straightforward and yet fully exploited in the\nliterature, due to the high expense of training base learners. To facilitate\nthe training of N-ary ECOC with deep learning base learners, we further propose\nthree different variants of parameter sharing architectures for deep N-ary\nECOC. To verify the generalization ability of deep N-ary ECOC, we conduct\nexperiments by varying the backbone with different deep neural network\narchitectures for both image and text classification tasks. Furthermore,\nextensive ablation studies on deep N-ary ECOC show its superior performance\nover other deep data-independent ensemble methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:35:03 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 03:20:23 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 09:24:31 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 02:33:15 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zhang", "Hao", ""], ["Zhou", "Joey Tianyi", ""], ["Wang", "Tianying", ""], ["Tsang", "Ivor W.", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2009.10515", "submitter": "Kostas Kolomvatsos", "authors": "Panagiotis Oikonomou, Kostas Kolomvatsos, Nikos Tziritas, Georgios\n  Theodoropoulos, Thanasis Loukopoulos, Georgios Stamoulis", "title": "A Fuzzy Logic Controller for Tasks Scheduling Using Unreliable Cloud\n  Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cloud infrastructure offers to end users a broad set of heterogenous\ncomputational resources using the pay-as-you-go model. These virtualized\nresources can be provisioned using different pricing models like the unreliable\nmodel where resources are provided at a fraction of the cost but with no\nguarantee for an uninterrupted processing. However, the enormous gamut of\nopportunities comes with a great caveat as resource management and scheduling\ndecisions are increasingly complicated. Moreover, the presented uncertainty in\noptimally selecting resources has also a negatively impact on the quality of\nsolutions delivered by scheduling algorithms. In this paper, we present a\ndynamic scheduling algorithm (i.e., the Uncertainty-Driven Scheduling - UDS\nalgorithm) for the management of scientific workflows in Cloud. Our model\nminimizes both the makespan and the monetary cost by dynamically selecting\nreliable or unreliable virtualized resources. For covering the uncertainty in\ndecision making, we adopt a Fuzzy Logic Controller (FLC) to derive the pricing\nmodel of the resources that will host every task. We evaluate the performance\nof the proposed algorithm using real workflow applications being tested under\nthe assumption of different probabilities regarding the revocation of\nunreliable resources. Numerical results depict the performance of the proposed\napproach and a comparative assessment reveals the position of the paper in the\nrelevant literature.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 13:15:19 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Oikonomou", "Panagiotis", ""], ["Kolomvatsos", "Kostas", ""], ["Tziritas", "Nikos", ""], ["Theodoropoulos", "Georgios", ""], ["Loukopoulos", "Thanasis", ""], ["Stamoulis", "Georgios", ""]]}, {"id": "2009.10579", "submitter": "Jonathan Hasenburg", "authors": "Jonathan Hasenburg, Martin Grambow, David Bermbach", "title": "MockFog 2.0: Automated Execution of Fog Application Experiments in the\n  Cloud", "comments": "Accepted for publication in IEEE Transactions on Cloud Computing", "journal-ref": null, "doi": "10.1109/TCC.2021.3074988", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fog computing is an emerging computing paradigm that uses processing and\nstorage capabilities located at the edge, in the cloud, and possibly in\nbetween. Testing and benchmarking fog applications, however, is hard since\nruntime infrastructure will typically be in use or may not exist, yet. While\napproaches for the emulation of infrastructure testbeds do exist, their focus\nis typically the emulation of edge devices. Other approaches also emulate\ninfrastructure within the core network or the cloud, but they miss support for\nautomated experiment orchestration.\n  In this paper, we propose to evaluate fog applications on an emulated\ninfrastructure testbed created in the cloud which can be manipulated based on a\npre-defined orchestration schedule. Developers can freely design the\ninfrastructure, configure performance characteristics, manage application\ncomponents, and orchestrate their experiments. We also present our\nproof-of-concept implementation MockFog 2.0. We use MockFog 2.0 to evaluate a\nfog-based smart factory application and showcase how its features can be used\nto study the impact of infrastructure changes and workload variations. With\nthese experiments, we also show that MockFog can achieve good experiment\nreproducibility, even in a public cloud environment.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 14:42:02 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 11:10:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Hasenburg", "Jonathan", ""], ["Grambow", "Martin", ""], ["Bermbach", "David", ""]]}, {"id": "2009.10593", "submitter": "Veljko Milutinovic Prof", "authors": "Veljko Milutinovic, Erfan Sadeqi Azer, Kristy Yoshimoto, Gerhard\n  Klimeck, Miljan Djordjevic, Milos Kotlar, Miroslav Bojovic, Bozidar\n  Miladinovic, Nenad Korolija, Stevan Stankovic, Nenad Filipovi\\'c, Zoran\n  Babovic, Miroslav Kosanic, Akira Tsuda, Mateo Valero, Massimo De Santo, Erich\n  Neuhold, Jelena Skoru\\v{c}ak, Laura Dipietro and Ivan Ratkovic", "title": "The Ultimate DataFlow for Ultimate SuperComputers-on-a-Chip, for\n  Scientific Computing, Geo Physics, Complex Mathematics, and Information\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article starts from the assumption that near future 100BTransistor\nSuperComputers-on-a-Chip will include N big multi-core processors, 1000N small\nmany-core processors, a TPU-like fixed-structure systolic array accelerator for\nthe most frequently used Machine Learning algorithms needed in bandwidth-bound\napplications and a flexible-structure reprogrammable accelerator for less\nfrequently used Machine Learning algorithms needed in latency-critical\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 10:34:32 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 12:26:04 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 14:23:04 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2021 16:41:02 GMT"}, {"version": "v5", "created": "Mon, 8 Feb 2021 11:32:12 GMT"}, {"version": "v6", "created": "Mon, 5 Jul 2021 15:23:54 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Milutinovic", "Veljko", ""], ["Azer", "Erfan Sadeqi", ""], ["Yoshimoto", "Kristy", ""], ["Klimeck", "Gerhard", ""], ["Djordjevic", "Miljan", ""], ["Kotlar", "Milos", ""], ["Bojovic", "Miroslav", ""], ["Miladinovic", "Bozidar", ""], ["Korolija", "Nenad", ""], ["Stankovic", "Stevan", ""], ["Filipovi\u0107", "Nenad", ""], ["Babovic", "Zoran", ""], ["Kosanic", "Miroslav", ""], ["Tsuda", "Akira", ""], ["Valero", "Mateo", ""], ["De Santo", "Massimo", ""], ["Neuhold", "Erich", ""], ["Skoru\u010dak", "Jelena", ""], ["Dipietro", "Laura", ""], ["Ratkovic", "Ivan", ""]]}, {"id": "2009.10601", "submitter": "Xu Chen", "authors": "Shuai Yu and Xu Chen and Zhi Zhou and Xiaowen Gong and Di Wu", "title": "When Deep Reinforcement Learning Meets Federated Learning: Intelligent\n  Multi-Timescale Resource Management for Multi-access Edge Computing in 5G\n  Ultra Dense Network", "comments": "Accepted by IEEE IoTJ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra-dense edge computing (UDEC) has great potential, especially in the 5G\nera, but it still faces challenges in its current solutions, such as the lack\nof: i) efficient utilization of multiple 5G resources (e.g., computation,\ncommunication, storage and service resources); ii) low overhead offloading\ndecision making and resource allocation strategies; and iii) privacy and\nsecurity protection schemes. Thus, we first propose an intelligent ultra-dense\nedge computing (I-UDEC) framework, which integrates blockchain and Artificial\nIntelligence (AI) into 5G ultra-dense edge computing networks. First, we show\nthe architecture of the framework. Then, in order to achieve real-time and low\noverhead computation offloading decisions and resource allocation strategies,\nwe design a novel two-timescale deep reinforcement learning (\\textit{2Ts-DRL})\napproach, consisting of a fast-timescale and a slow-timescale learning process,\nrespectively. The primary objective is to minimize the total offloading delay\nand network resource usage by jointly optimizing computation offloading,\nresource allocation and service caching placement. We also leverage federated\nlearning (FL) to train the \\textit{2Ts-DRL} model in a distributed manner,\naiming to protect the edge devices' data privacy. Simulation results\ncorroborate the effectiveness of both the \\textit{2Ts-DRL} and FL in the I-UDEC\nframework and prove that our proposed algorithm can reduce task execution time\nup to 31.87%.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:08:00 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Yu", "Shuai", ""], ["Chen", "Xu", ""], ["Zhou", "Zhi", ""], ["Gong", "Xiaowen", ""], ["Wu", "Di", ""]]}, {"id": "2009.10618", "submitter": "Fauzi Adi Rafrastara", "authors": "Fauzi Adi Rafrastara, Qi Deyu", "title": "A Survey and Taxonomy of Distributed Data Mining Research Studies: A\n  Systematic Literature Review", "comments": "19 pages, 14 figures, International Journal of Computer Science and\n  Information Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Data Mining (DM) method has been evolving year by year and as of\ntoday there is also the enhancement of DM technique that can be run several\ntimes faster than the traditional one, called Distributed Data Mining (DDM). It\nis not a new field in data processing actually, but in the recent years many\nresearchers have been paying more attention on this area. Problems: The number\nof publication regarding DDM in high reputation journals and conferences has\nincreased significantly. It makes difficult for researchers to gain a\ncomprehensive view of DDM that require further research. Solution: We conducted\na systematic literature review to map the previous research in DDM field. Our\nobjective is to provide the motivation for new research by identifying the gap\nin DDM field as well as the hot area itself. Result: Our analysis came up with\nsome conclusions by answering 7 research questions proposed in this literature\nreview. In addition, the taxonomy of DDM research area is presented in this\npaper. Finally, this systematic literature review provides the statistic of\ndevelopment of DDM since 2000 to 2015, in which this will help the future\nresearchers to have a comprehensive overview of current situation of DDM.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:58:42 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Rafrastara", "Fauzi Adi", ""], ["Deyu", "Qi", ""]]}, {"id": "2009.10656", "submitter": "Franyell Silfa", "authors": "Franyell Silfa, Jose Maria Arnau, and Antonio Gonzalez", "title": "E-BATCH: Energy-Efficient and High-Throughput RNN Batching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) inference exhibits low hardware utilization\ndue to the strict data dependencies across time-steps. Batching multiple\nrequests can increase throughput. However, RNN batching requires a large amount\nof padding since the batched input sequences may largely differ in length.\nSchemes that dynamically update the batch every few time-steps avoid padding.\nHowever, they require executing different RNN layers in a short timespan,\ndecreasing energy efficiency. Hence, we propose E-BATCH, a low-latency and\nenergy-efficient batching scheme tailored to RNN accelerators. It consists of a\nruntime system and effective hardware support. The runtime concatenates\nmultiple sequences to create large batches, resulting in substantial energy\nsavings. Furthermore, the accelerator notifies it when the evaluation of a\nsequence is done, so that a new sequence can be immediately added to a batch,\nthus largely reducing the amount of padding. E-BATCH dynamically controls the\nnumber of time-steps evaluated per batch to achieve the best trade-off between\nlatency and energy efficiency for the given hardware platform. We evaluate\nE-BATCH on top of E-PUR and TPU. In E-PUR, E-BATCH improves throughput by 1.8x\nand energy-efficiency by 3.6x, whereas in TPU, it improves throughput by 2.1x\nand energy-efficiency by 1.6x, over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:22:23 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Silfa", "Franyell", ""], ["Arnau", "Jose Maria", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "2009.10664", "submitter": "Laurent Chuat", "authors": "Joel Wanner and Laurent Chuat and Adrian Perrig", "title": "A Formally Verified Protocol for Log Replication with Byzantine Fault\n  Tolerance", "comments": "International Symposium on Reliable Distributed Systems (SRDS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine fault tolerant protocols enable state replication in the presence\nof crashed, malfunctioning, or actively malicious processes. Designing such\nprotocols without the assistance of verification tools, however, is remarkably\nerror-prone. In an adversarial environment, performance and flexibility come at\nthe cost of complexity, making the verification of existing protocols extremely\ndifficult. We take a different approach and propose a formally verified\nconsensus protocol designed for a specific use case: secure logging. Our\nprotocol allows each node to propose entries in a parallel subroutine, and\nguarantees that correct nodes agree on the set of all proposed entries, without\nleader election. It is simple yet practical, as it can accommodate the workload\nof a logging system such as Certificate Transparency. We show that it is\noptimal in terms of both required rounds and tolerable faults. Using\nIsabelle/HOL, we provide a fully machine-checked security proof based upon the\nHeard-Of model, which we extend to support signatures. We also present and\nevaluate a prototype implementation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:35:31 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Wanner", "Joel", ""], ["Chuat", "Laurent", ""], ["Perrig", "Adrian", ""]]}, {"id": "2009.10697", "submitter": "Leopold Cambier", "authors": "L\\'eopold Cambier, Yizhou Qian, Eric Darve", "title": "TaskTorrent: a Lightweight Distributed Task-Based Runtime System in C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TaskTorrent, a lightweight distributed task-based runtime in C++.\nTaskTorrent uses a parametrized task graph to express the task DAG, and\none-sided active messages to trigger remote tasks asynchronously. As a result\nthe task DAG is completely distributed and discovered in parallel. It is a\nC++14 library and only depends on MPI. We explain the API and the\nimplementation. We perform a series of benchmarks against StarPU and ScaLAPACK.\nMicro benchmarks show it has a minimal overhead compared to other solutions. We\nthen apply it to two large linear algebra problems. TaskTorrent scales very\nwell to thousands of cores, exhibiting good weak and strong scalings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 17:16:11 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Cambier", "L\u00e9opold", ""], ["Qian", "Yizhou", ""], ["Darve", "Eric", ""]]}, {"id": "2009.10717", "submitter": "Margalit Glasgow", "authors": "Margalit Glasgow, Mary Wootters", "title": "Asynchronous Distributed Optimization with Stochastic Delays", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.09638", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study asynchronous finite sum minimization in a distributed-data setting\nwith a central parameter server. While asynchrony is well understood in\nparallel settings where the data is accessible by all machines -- e.g.,\nmodifications of variance-reduced gradient algorithms like SAGA work well --\nlittle is known for the distributed-data setting. We develop an algorithm\nADSAGA based on SAGA for the distributed-data setting, in which the data is\npartitioned between many machines. We show that with $m$ machines, under a\nnatural stochastic delay model with an mean delay of $m$, ADSAGA converges in\n$\\tilde{O}\\left(\\left(n + \\sqrt{m}\\kappa\\right)\\log(1/\\epsilon)\\right)$\niterations, where $n$ is the number of component functions, and $\\kappa$ is a\ncondition number. This complexity sits squarely between the complexity\n$\\tilde{O}\\left(\\left(n + \\kappa\\right)\\log(1/\\epsilon)\\right)$ of SAGA\n\\textit{without delays} and the complexity $\\tilde{O}\\left(\\left(n +\nm\\kappa\\right)\\log(1/\\epsilon)\\right)$ of parallel asynchronous algorithms\nwhere the delays are \\textit{arbitrary} (but bounded by $O(m)$), and the data\nis accessible by all. Existing asynchronous algorithms with distributed-data\nsetting and arbitrary delays have only been shown to converge in\n$\\tilde{O}(n^2\\kappa\\log(1/\\epsilon))$ iterations. We empirically compare on\nleast-squares problems the iteration complexity and wallclock performance of\nADSAGA to existing parallel and distributed algorithms, including synchronous\nminibatch algorithms. Our results demonstrate the wallclock advantage of\nvariance-reduced asynchronous approaches over SGD or synchronous approaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 17:59:06 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 05:13:23 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 05:35:10 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Glasgow", "Margalit", ""], ["Wootters", "Mary", ""]]}, {"id": "2009.10761", "submitter": "David Harris", "authors": "David G. Harris, Hsin-Hao Su, Hoa T. Vu", "title": "On the Locality of Nash-Williams Forest Decomposition and Star-Forest\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$ with arboricity $\\alpha$, we study the problem of\ndecomposing the edges of $G$ into $(1+\\epsilon)\\alpha$ disjoint forests in the\ndistributed LOCAL model. Barenboim and Elkin [PODC `08] gave a LOCAL algorithm\nthat computes a $(2+\\epsilon)\\alpha$-forest decomposition using $O(\\frac{\\log\nn}{\\epsilon})$ rounds. Ghaffari and Su [SODA `17] made further progress by\ncomputing a $(1+\\epsilon) \\alpha$-forest decomposition in $O(\\frac{\\log^3\nn}{\\epsilon^4})$ rounds when $\\epsilon \\alpha = \\Omega(\\sqrt{\\alpha \\log n})$,\ni.e. the limit of their algorithm is an $(\\alpha+ \\Omega(\\sqrt{\\alpha \\log\nn}))$-forest decomposition. This algorithm, based on a combinatorial\nconstruction of Alon, McDiarmid \\& Reed [Combinatorica `92], in fact provides a\ndecomposition of the graph into \\emph{star-forests}, i.e. each forest is a\ncollection of stars.\n  Our main result in this paper is to reduce the threshold of $\\epsilon \\alpha$\nin $(1+\\epsilon)\\alpha$-forest decomposition and star-forest decomposition.\nThis further answers the $10^{\\text{th}}$ open question from Barenboim and\nElkin's \"Distributed Graph Algorithms\" book. Moreover, it gives the first\n$(1+\\epsilon)\\alpha$-orientation algorithms with {\\it linear dependencies} on\n$\\epsilon^{-1}$.\n  At a high level, our results for forest-decomposition are based on a\ncombination of network decomposition, load balancing, and a new structural\nresult on local augmenting sequences. Our result for star-forest decomposition\nuses a more careful probabilistic analysis for the construction of Alon,\nMcDiarmid, \\& Reed; the bounds on star-arboricity here were not previously\nknown, even non-constructively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 18:44:06 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 19:59:25 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 12:08:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Harris", "David G.", ""], ["Su", "Hsin-Hao", ""], ["Vu", "Hoa T.", ""]]}, {"id": "2009.10861", "submitter": "Samira Pouyanfar", "authors": "Ankit Srivastava, Samira Pouyanfar, Joshua Allen, Ken Johnston, Qida\n  Ma", "title": "Distributed Differentially Private Mutual Information Ranking and Its\n  Applications", "comments": null, "journal-ref": "2020 IEEE 21st International Conference on Information Reuse and\n  Integration for Data Science (IRI) (pp. 90-96)", "doi": "10.1109/IRI49571.2020.00021", "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of Mutual Information (MI) helps understand the amount of\ninformation shared between a pair of random variables. Automated feature\nselection techniques based on MI ranking are regularly used to extract\ninformation from sensitive datasets exceeding petabytes in size, over millions\nof features and classes. Series of one-vs-all MI computations can be cascaded\nto produce n-fold MI results, rapidly pinpointing informative relationships.\nThis ability to quickly pinpoint the most informative relationships from\ndatasets of billions of users creates privacy concerns. In this paper, we\npresent Distributed Differentially Private Mutual Information (DDP-MI), a\nprivacy-safe fast batch MI, across various scenarios such as feature selection,\nsegmentation, ranking, and query expansion. This distributed implementation is\nprotected with global model differential privacy to provide strong assurances\nagainst a wide range of privacy attacks. We also show that our DDP-MI can\nsubstantially improve the efficiency of MI calculations compared to standard\nimplementations on a large-scale public dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 23:55:08 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Srivastava", "Ankit", ""], ["Pouyanfar", "Samira", ""], ["Allen", "Joshua", ""], ["Johnston", "Ken", ""], ["Ma", "Qida", ""]]}, {"id": "2009.10917", "submitter": "Noel Chalmers", "authors": "Noel Chalmers and Tim Warburton", "title": "Portable high-order finite element kernels I: Streaming Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the development of highly efficient kernels\nperforming vector operations relevant in linear system solvers. In particular,\nwe focus on the low arithmetic intensity operations (i.e., streaming\noperations) performed within the conjugate gradient iterative method, using the\nparameters specified in the CEED benchmark problems for high-order hexahedral\nfinite elements. We propose a suite of new Benchmark Streaming tests to focus\non the distinct streaming operations which must be performed. We implemented\nthese new tests using the OCCA abstraction framework to demonstrate portability\nof these streaming operations on different GPU architectures, and propose a\nsimple performance model for such kernels which can accurately capture data\nmovement rates as well as kernel launch costs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 03:11:10 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Chalmers", "Noel", ""], ["Warburton", "Tim", ""]]}, {"id": "2009.10924", "submitter": "Zhen Zheng", "authors": "Zhen Zheng, Pengzhan Zhao, Guoping Long, Feiwen Zhu, Kai Zhu, Wenyi\n  Zhao, Lansong Diao, Jun Yang, Wei Lin", "title": "FusionStitching: Boosting Memory Intensive Computations for Deep\n  Learning Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show in this work that memory intensive computations can result in severe\nperformance problems due to off-chip memory access and CPU-GPU context switch\noverheads in a wide range of deep learning models. For this problem, current\njust-in-time kernel fusion and code generation techniques have limitations,\nsuch as kernel schedule incompatibilities and rough fusion plan exploration\nstrategies. We propose FusionStitching, a Deep Learning compiler capable of\nfusing memory intensive operators, with varied data dependencies and\nnon-homogeneous parallelism, into large GPU kernels to reduce global memory\naccess and operation scheduling overhead automatically. FusionStitching\nexplores large fusion spaces to decide optimal fusion plans with considerations\nof memory access costs, kernel calls and resource usage constraints. We\nthoroughly study the schemes to stitch operators together for complex\nscenarios. FusionStitching tunes the optimal stitching scheme just-in-time with\na domain-specific cost model efficiently. Experimental results show that\nFusionStitching can reach up to 2.78x speedup compared to TensorFlow and\ncurrent state-of-the-art. Besides these experimental results, we integrated our\napproach into a compiler product and deployed it onto a production cluster for\nAI workloads with thousands of GPUs. The system has been in operation for more\nthan 4 months and saves 7,000 GPU hours on average for approximately 30,000\ntasks per month.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 04:00:53 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zheng", "Zhen", ""], ["Zhao", "Pengzhan", ""], ["Long", "Guoping", ""], ["Zhu", "Feiwen", ""], ["Zhu", "Kai", ""], ["Zhao", "Wenyi", ""], ["Diao", "Lansong", ""], ["Yang", "Jun", ""], ["Lin", "Wei", ""]]}, {"id": "2009.10926", "submitter": "Yuichi Sudo", "authors": "Daisuke Yokota, Yuichi Sudo, Toshimitsu Masuzawa", "title": "Time-Optimal Self-Stabilizing Leader Election on Rings in Population\n  Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-stabilizing leader election protocol on directed rings in\nthe model of population protocols. Given an upper bound $N$ on the population\nsize $n$, the proposed protocol elects a unique leader within $O(nN)$ expected\nsteps starting from any configuration and uses $O(N)$ states. This convergence\ntime is optimal if a given upper bound $N$ is asymptotically tight, i.e.,\n$N=O(n)$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 04:21:16 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yokota", "Daisuke", ""], ["Sudo", "Yuichi", ""], ["Masuzawa", "Toshimitsu", ""]]}, {"id": "2009.10950", "submitter": "Antoni Navarro Mu\\~noz", "authors": "Antoni Navarro (1), Arthur F. Lorenzon (2), Eduard Ayguad\\'e (1),\n  Vicen\\c{c} Beltran (1) ((1) Barcelona Supercomputing Center, (2) Federal\n  University of Pampa)", "title": "Enhancing Resource Management through Prediction-based Policies", "comments": "Postprint submitted and published at Euro-Par2020: International\n  European Conference on Parallel and Distributed Computing (Springer)\n  (https://link.springer.com/chapter/10.1007%2F978-3-030-57675-2_31)", "journal-ref": "International European Conference on Parallel and Distributed\n  Computing, 12247, 493-509 (2020)", "doi": "10.1007/978-3-030-57675-2_31", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-based programming models are emerging as a promising alternative to make\nthe most of multi-/many-core systems. These programming models rely on runtime\nsystems, and their goal is to improve application performance by properly\nscheduling application tasks to cores. Additionally, these runtime systems\noffer policies to cope with application phases that lack in parallelism to fill\nall cores. However, these policies are usually static and favor either\nperformance or energy efficiency. In this paper, we have extended a task-based\nruntime system with a lightweight monitoring and prediction infrastructure that\ndynamically predicts the optimal number of cores required for each application\nphase, thus improving both performance and energy efficiency. Through the\nexecution of several benchmarks in multi-/many-core systems, we show that our\nprediction-based policies have competitive performance while improving energy\nefficiency when compared to state of the art policies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:41:56 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Navarro", "Antoni", ""], ["Lorenzon", "Arthur F.", ""], ["Ayguad\u00e9", "Eduard", ""], ["Beltran", "Vicen\u00e7", ""]]}, {"id": "2009.10955", "submitter": "Tianhui Shi", "authors": "Tianhui Shi, Mingshu Zhai, Yi Xu, Jidong Zhai", "title": "GraphPi: High Performance Graph Pattern Matching through Effective\n  Redundancy Elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern matching, which aims to discover structural patterns in graphs,\nis considered one of the most fundamental graph mining problems in many real\napplications. Despite previous efforts, existing systems face two main\nchallenges. First, inherent symmetry existing in patterns can introduce a large\namount of redundant computation. Second, different matching orders for a\npattern have significant performance differences and are quite hard to predict.\nWhen these factors are mixed, this problem becomes extremely complicated. High\nefficient pattern matching remains an open problem currently. To address these\nchallenges, we propose GraphPi, a high performance distributed pattern matching\nsystem. GraphPi utilizes a new algorithm based on 2-cycles in group theory to\ngenerate multiple sets of asymmetric restrictions, where each set can eliminate\nredundant computation completely. We further design an accurate performance\nmodel to determine the optimal matching order and asymmetric restriction set\nfor efficient pattern matching. We evaluate GraphPi on Tianhe-2A supercomputer.\nResults show that GraphPi outperforms the state-ofthe-art system, by up to 105X\nfor 6 real-world graph datasets on a single node. We also scale GraphPi to\n1,024 computing nodes (24,576 cores).\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:58:03 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Shi", "Tianhui", ""], ["Zhai", "Mingshu", ""], ["Xu", "Yi", ""], ["Zhai", "Jidong", ""]]}, {"id": "2009.10965", "submitter": "Jinyuan Chen", "authors": "Jinyuan Chen", "title": "Fundamental Limits of Byzantine Agreement", "comments": "The proposed protocol is simplified; the communication complexity is\n  slightly improved; and the optimal communication complexity exponent is\n  characterized", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine agreement (BA) is a distributed consensus problem where $n$\nprocessors want to reach agreement on an $\\ell$-bit message or value, but up to\n$t$ of the processors are dishonest or faulty. The challenge of this BA problem\nlies in achieving agreement despite the presence of dishonest processors who\nmay arbitrarily deviate from the designed protocol. The quality of a BA\nprotocol is measured primarily by using the following three parameters: the\nnumber of processors $n$ as a function of $t$ allowed (resilience); the number\nof rounds (round complexity, denoted by $r$); and the total number of\ncommunication bits (communication complexity, denoted by $b$). For any\nerror-free BA protocol, the known lower bounds on those three parameters are\n$n\\geq 3t+1$, $r\\geq t+1$ and $b\\geq\\Omega(\\max\\{n\\ell, nt\\})$, respectively,\nwhere a protocol that is guaranteed to be correct in all executions is said to\nbe error free. In this work by using coding theory, together with graph theory\nand linear algebra, we design a coded BA protocol (termed as COOL) that\nachieves consensus on an $\\ell$-bit message with optimal resilience,\nasymptotically optimal round complexity, and asymptotically optimal\ncommunication complexity when $\\ell \\geq t\\log t$, simultaneously. The proposed\nCOOL is an error-free and deterministic BA protocol that does not rely on\ncryptographic technique. It is secure against computationally unbounded\nadversary. With the achievable performance by the proposed COOL and the known\nlower bounds, we characterize the optimal communication complexity exponent as\n\\[\\beta^*(\\alpha,\\delta)=\\max\\{1+\\alpha,1+\\delta\\}\\] for $\\beta=\n\\lim_{n\\to\\infty}\\log b/\\log n$, $\\alpha=\\lim_{n \\to \\infty} \\log \\ell/\\log n$\nand $\\delta=\\lim_{n\\to\\infty} \\log t/\\log n$. This work reveals that coding is\nan effective approach for achieving the fundamental limits of Byzantine\nagreement and its variants.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:11:42 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 17:37:25 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Jinyuan", ""]]}, {"id": "2009.11033", "submitter": "Chander Govindarajan Mr", "authors": "Prabal Banerjee, Chander Govindarajan, Praveen Jayachandran, Sushmita\n  Ruj", "title": "Reliable, Fair and Decentralized Marketplace for Content Sharing Using\n  Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content sharing platforms such as Youtube and Vimeo have promoted pay per\nview models for artists to monetize their content. Yet, artists remain at the\nmercy of centralized platforms that control content listing and advertisement,\nwith little transparency and fairness in terms of number of views or revenue.\nOn the other hand, consumers are distanced from the publishers and cannot\nauthenticate originality of the content. In this paper, we develop a reliable\nand fair platform for content sharing without a central facilitator. The\nplatform is built as a decentralized data storage layer to store and share\ncontent in a fault-tolerant manner, where the peers also participate in a\nblockchain network. The blockchain is used to manage content listings and as an\nauditable and fair marketplace transaction processor that automatically pays\nout the content creators and the storage facilitators using smart contracts. We\ndemonstrate the system with the blockchain layer built on Hyperledger Fabric\nand the data layer built on Tahoe-LAFS,and show that our design is practical\nand scalable with low overheads.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 10:04:15 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Banerjee", "Prabal", ""], ["Govindarajan", "Chander", ""], ["Jayachandran", "Praveen", ""], ["Ruj", "Sushmita", ""]]}, {"id": "2009.11208", "submitter": "Mohamed Handaoui", "authors": "Mohamed Handaoui and Jean-Emile Dartois and Jalil Boukhobza and\n  Olivier Barais and Laurent d'Orazio", "title": "ReLeaSER: A Reinforcement Learning Strategy for Optimizing Utilization\n  Of Ephemeral Cloud Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud data center capacities are over-provisioned to handle demand peaks and\nhardware failures which leads to low resources' utilization. One way to improve\nresource utilization and thus reduce the total cost of ownership is to offer\nunused resources (referred to as ephemeral resources) at a lower price.\nHowever, reselling resources needs to meet the expectations of its customers in\nterms of Quality of Service. The goal is so to maximize the amount of reclaimed\nresources while avoiding SLA penalties. To achieve that, cloud providers have\nto estimate their future utilization to provide availability guarantees. The\nprediction should consider a safety margin for resources to react to\nunpredictable workloads. The challenge is to find the safety margin that\nprovides the best trade-off between the amount of resources to reclaim and the\nrisk of SLA violations. Most state-of-the-art solutions consider a fixed safety\nmargin for all types of metrics (e.g., CPU, RAM). However, a unique fixed\nmargin does not consider various workloads variations over time which may lead\nto SLA violations or/and poor utilization. In order to tackle these challenges,\nwe propose ReLeaSER, a Reinforcement Learning strategy for optimizing the\nephemeral resources' utilization in the cloud. ReLeaSER dynamically tunes the\nsafety margin at the host-level for each resource metric. The strategy learns\nfrom past prediction errors (that caused SLA violations). Our solution reduces\nsignificantly the SLA violation penalties on average by 2.7x and up to 3.4x. It\nalso improves considerably the CPs' potential savings by 27.6% on average and\nup to 43.6%.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:19:28 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:23:21 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 09:59:20 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2020 10:48:38 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Handaoui", "Mohamed", ""], ["Dartois", "Jean-Emile", ""], ["Boukhobza", "Jalil", ""], ["Barais", "Olivier", ""], ["d'Orazio", "Laurent", ""]]}, {"id": "2009.11221", "submitter": "Florian Wirthm\\\"uller", "authors": "Florian Wirthm\\\"uller, Marvin Klimke, Julian Schlechtriemen, Jochen\n  Hipp and Manfred Reichert", "title": "A Fleet Learning Architecture for Enhanced Behavior Predictions during\n  Challenging External Conditions", "comments": "the article has been accepted for publication during the 2020 IEEE\n  Symposium Series on Computational Intelligence (SSCI) within the IEEE\n  Symposium on Computational Intelligence in Vehicles and Transportation\n  Systems (CIVTS), 7 pages, 6 figures", "journal-ref": null, "doi": "10.1109/SSCI47803.2020.9308542", "report-no": null, "categories": "cs.RO cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Already today, driver assistance systems help to make daily traffic more\ncomfortable and safer. However, there are still situations that are quite rare\nbut are hard to handle at the same time. In order to cope with these situations\nand to bridge the gap towards fully automated driving, it becomes necessary to\nnot only collect enormous amounts of data but rather the right ones. This data\ncan be used to develop and validate the systems through machine learning and\nsimulation pipelines. Along this line this paper presents a fleet\nlearning-based architecture that enables continuous improvements of systems\npredicting the movement of surrounding traffic participants. Moreover, the\npresented architecture is applied to a testing vehicle in order to prove the\nfundamental feasibility of the system. Finally, it is shown that the system\ncollects meaningful data which are helpful to improve the underlying prediction\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:33:42 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 06:31:56 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wirthm\u00fcller", "Florian", ""], ["Klimke", "Marvin", ""], ["Schlechtriemen", "Julian", ""], ["Hipp", "Jochen", ""], ["Reichert", "Manfred", ""]]}, {"id": "2009.11224", "submitter": "Jacek Czaja", "authors": "Jacek Czaja, Michal Gallus, Joanna Wozna, Adam Grygielski, Luo Tao", "title": "Applying the Roofline model for Deep Learning performance optimizations", "comments": "oneDNN library analysis with roofline model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper We present a methodology for creating Roofline models\nautomatically for Non-Unified Memory Access (NUMA) using Intel Xeon as an\nexample. Finally, we present an evaluation of highly efficient deep learning\nprimitives as implemented in the Intel oneDNN Library.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:39:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Czaja", "Jacek", ""], ["Gallus", "Michal", ""], ["Wozna", "Joanna", ""], ["Grygielski", "Adam", ""], ["Tao", "Luo", ""]]}, {"id": "2009.11552", "submitter": "Laxman Dhulipala", "authors": "Soheil Behnezhad, Laxman Dhulipala, Hossein Esfandiari, Jakub\n  {\\L}\\k{a}cki, Vahab Mirrokni, Warren Schudy", "title": "Parallel Graph Algorithms in Constant Adaptive Rounds: Theory meets\n  Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fundamental graph problems such as graph connectivity, minimum\nspanning forest (MSF), and approximate maximum (weight) matching in a\ndistributed setting. In particular, we focus on the Adaptive Massively Parallel\nComputation (AMPC) model, which is a theoretical model that captures\nMapReduce-like computation augmented with a distributed hash table.\n  We show the first AMPC algorithms for all of the studied problems that run in\na constant number of rounds and use only $O(n^\\epsilon)$ space per machine,\nwhere $0 < \\epsilon < 1$. Our results improve both upon the previous results in\nthe AMPC model, as well as the best-known results in the MPC model, which is\nthe theoretical model underpinning many popular distributed computation\nframeworks, such as MapReduce, Hadoop, Beam, Pregel and Giraph.\n  Finally, we provide an empirical comparison of the algorithms in the MPC and\nAMPC models in a fault-tolerant distriubted computation environment. We\nempirically evaluate our algorithms on a set of large real-world graphs and\nshow that our AMPC algorithms can achieve improvements in both running time and\nround-complexity over optimized MPC baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:47:33 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Dhulipala", "Laxman", ""], ["Esfandiari", "Hossein", ""], ["\u0141\u0105cki", "Jakub", ""], ["Mirrokni", "Vahab", ""], ["Schudy", "Warren", ""]]}, {"id": "2009.11665", "submitter": "Jiayu Li", "authors": "Langshi Chen, Jiayu Li, Ariful Azad, Cenk Sahinalp, Madhav Marathe,\n  Anil Vullikanti, Andrey Nikolaev, Egor Smirnov, Ruslan Israfilov, Judy Qiu", "title": "SubGraph2Vec: Highly-Vectorized Tree-likeSubgraph Counting", "comments": "arXiv admin note: text overlap with arXiv:1903.04395", "journal-ref": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9006037", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counting aims to count occurrences of a template T in a given\nnetwork G(V, E). It is a powerful graph analysis tool and has found real-world\napplications in diverse domains. Scaling subgraph counting problems is known to\nbe memory bounded and computationally challenging with exponential complexity.\nAlthough scalable parallel algorithms are known for several graph problems such\nas Triangle Counting and PageRank, this is not common for counting complex\nsubgraphs. Here we address this challenge and study connected acyclic graphs or\ntrees. We propose a novel vectorized subgraph counting algorithm, named\nSubgraph2Vec, as well as both shared memory and distributed implementations: 1)\nreducing algorithmic complexity by minimizing neighbor traversal; 2) achieving\na highly-vectorized implementation upon linear algebra kernels to significantly\nimprove performance and hardware utilization. 3) Subgraph2Vec improves the\noverall performance over the state-of-the-art work by orders of magnitude and\nup to 660x on a single node. 4) Subgraph2Vec in distributed mode can scale up\nthe template size to 20 and maintain good strong scalability. 5) enabling\nportability to both CPU and GPU.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:16:40 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 22:24:02 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chen", "Langshi", ""], ["Li", "Jiayu", ""], ["Azad", "Ariful", ""], ["Sahinalp", "Cenk", ""], ["Marathe", "Madhav", ""], ["Vullikanti", "Anil", ""], ["Nikolaev", "Andrey", ""], ["Smirnov", "Egor", ""], ["Israfilov", "Ruslan", ""], ["Qiu", "Judy", ""]]}, {"id": "2009.11722", "submitter": "Bogdan Trasnea", "authors": "Sorin Grigorescu, Tiberiu Cocias, Bogdan Trasnea, Andrea Margheri,\n  Federico Lombardi, Leonardo Aniello", "title": "Cloud2Edge Elastic AI Framework for Prototyping and Deployment of AI\n  Inference Engines in Autonomous Vehicles", "comments": "21 pages Published in Sensors:\n  https://www.mdpi.com/1424-8220/20/19/5450", "journal-ref": null, "doi": "10.3390/s20195450", "report-no": null, "categories": "cs.SE cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving cars and autonomous vehicles are revolutionizing the automotive\nsector, shaping the future of mobility altogether. Although the integration of\nnovel technologies such as Artificial Intelligence (AI) and Cloud/Edge\ncomputing provides golden opportunities to improve autonomous driving\napplications, there is the need to modernize accordingly the whole prototyping\nand deployment cycle of AI components. This paper proposes a novel framework\nfor developing so-called AI Inference Engines for autonomous driving\napplications based on deep learning modules, where training tasks are deployed\nelastically over both Cloud and Edge resources, with the purpose of reducing\nthe required network bandwidth, as well as mitigating privacy issues. Based on\nour proposed data driven V-Model, we introduce a simple yet elegant solution\nfor the AI components development cycle, where prototyping takes place in the\ncloud according to the Software-in-the-Loop (SiL) paradigm, while deployment\nand evaluation on the target ECUs (Electronic Control Units) is performed as\nHardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework\nis demonstrated using two real-world use-cases of AI inference engines for\nautonomous vehicles, that is environment perception and most probable path\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 09:23:29 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Grigorescu", "Sorin", ""], ["Cocias", "Tiberiu", ""], ["Trasnea", "Bogdan", ""], ["Margheri", "Andrea", ""], ["Lombardi", "Federico", ""], ["Aniello", "Leonardo", ""]]}, {"id": "2009.11742", "submitter": "Francesco Rizzi", "authors": "Francesco Rizzi, Eric J. Parish, Patrick J. Blonigan, John Tencer", "title": "A compute-bound formulation of Galerkin model reduction for linear\n  time-invariant dynamical systems", "comments": "Revised version, 28 pages, 9 figures", "journal-ref": null, "doi": "10.1016/j.cma.2021.113973", "report-no": "SAND2020-10291", "categories": "physics.comp-ph cs.CE cs.DC cs.MS math.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work aims to advance computational methods for projection-based reduced\norder models (ROMs) of linear time-invariant (LTI) dynamical systems. For such\nsystems, current practice relies on ROM formulations expressing the state as a\nrank-1 tensor (i.e., a vector), leading to computational kernels that are\nmemory bandwidth bound and, therefore, ill-suited for scalable performance on\nmodern many-core and hybrid computing nodes. This weakness can be particularly\nlimiting when tackling many-query studies, where one needs to run a large\nnumber of simulations. This work introduces a reformulation, called rank-2\nGalerkin, of the Galerkin ROM for LTI dynamical systems which converts the\nnature of the ROM problem from memory bandwidth to compute bound. We present\nthe details of the formulation and its implementation, and demonstrate its\nutility through numerical experiments using, as a test case, the simulation of\nelastic seismic shear waves in an axisymmetric domain. We quantify and analyze\nperformance and scaling results for varying numbers of threads and problem\nsizes. Finally, we present an end-to-end demonstration of using the rank-2\nGalerkin ROM for a Monte Carlo sampling study. We show that the rank-2 Galerkin\nROM is one order of magnitude more efficient than the rank-1 Galerkin ROM (the\ncurrent practice) and about 970X more efficient than the full order model,\nwhile maintaining accuracy in both the mean and statistics of the field.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:06:00 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 16:02:32 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 11:55:18 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Rizzi", "Francesco", ""], ["Parish", "Eric J.", ""], ["Blonigan", "Patrick J.", ""], ["Tencer", "John", ""]]}, {"id": "2009.12009", "submitter": "Andrew  Myers", "authors": "Weiqun Zhang, Andrew Myers, Kevin Gott, Ann Almgren, John Bell", "title": "AMReX: Block-Structured Adaptive Mesh Refinement for Multiphysics\n  Applications", "comments": "16 pages, 9 figures, submitted to IJHPCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block-structured adaptive mesh refinement (AMR) provides the basis for the\ntemporal and spatial discretization strategy for a number of ECP applications\nin the areas of accelerator design, additive manufacturing, astrophysics,\ncombustion, cosmology, multiphase flow, and wind plant modelling. AMReX is a\nsoftware framework that provides a unified infrastructure with the\nfunctionality needed for these and other AMR applications to be able to\neffectively and efficiently utilize machines from laptops to exascale\narchitectures. AMR reduces the computational cost and memory footprint compared\nto a uniform mesh while preserving accurate descriptions of different physical\nprocesses in complex multi-physics algorithms. AMReX supports algorithms that\nsolve systems of partial differential equations (PDEs) in simple or complex\ngeometries, and those that use particles and/or particle-mesh operations to\nrepresent component physical processes. In this paper, we will discuss the core\nelements of the AMReX framework such as data containers and iterators as well\nas several specialized operations to meet the needs of the application\nprojects. In addition we will highlight the strategy that the AMReX team is\npursuing to achieve highly performant code across a range of accelerator-based\narchitectures for a variety of different applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 02:59:30 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Zhang", "Weiqun", ""], ["Myers", "Andrew", ""], ["Gott", "Kevin", ""], ["Almgren", "Ann", ""], ["Bell", "John", ""]]}, {"id": "2009.12186", "submitter": "Franck Iutzeler", "authors": "Gilles Bareilles (DAO), Yassine Laguel (DAO), Dmitry Grishchenko (DAO,\n  AMA), Franck Iutzeler (DAO), J\\'er\\^ome Malick (DAO)", "title": "Randomized Progressive Hedging methods for Multi-stage Stochastic\n  Programming", "comments": null, "journal-ref": "Annals of Operations Research, Springer Verlag, In press", "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progressive Hedging is a popular decomposition algorithm for solving\nmulti-stage stochastic optimization problems. A computational bottleneck of\nthis algorithm is that all scenario subproblems have to be solved at each\niteration. In this paper, we introduce randomized versions of the Progressive\nHedging algorithm able to produce new iterates as soon as a single scenario\nsubproblem is solved. Building on the relation between Progressive Hedging and\nmonotone operators, we leverage recent results on randomized fixed point\nmethods to derive and analyze the proposed methods. Finally, we release the\ncorresponding code as an easy-to-use Julia toolbox and report computational\nexperiments showing the practical interest of randomized algorithms, notably in\na parallel context. Throughout the paper, we pay a special attention to\npresentation, stressing main ideas, avoiding extra-technicalities, in order to\nmake the randomized methods accessible to a broad audience in the Operations\nResearch community.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:44:13 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Bareilles", "Gilles", "", "DAO"], ["Laguel", "Yassine", "", "DAO"], ["Grishchenko", "Dmitry", "", "DAO,\n  AMA"], ["Iutzeler", "Franck", "", "DAO"], ["Malick", "J\u00e9r\u00f4me", "", "DAO"]]}, {"id": "2009.12263", "submitter": "Tim Besard", "authors": "Thomas Faingnaert, Tim Besard, Bjorn De Sutter", "title": "Flexible Performant GEMM Kernels on GPUs", "comments": "This paper was submitted to IEEE TPDS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General Matrix Multiplication or GEMM kernels take center place in high\nperformance computing and machine learning. Recent NVIDIA GPUs include GEMM\naccelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered by\nthe two-language problem: it requires either low-level programming which\nimplies low programmer productivity or using libraries that only offer a\nlimited set of components. Because rephrasing algorithms in terms of\nestablished components often introduces overhead, the libraries' lack of\nflexibility limits the freedom to explore new algorithms. Researchers using\nGEMMs can hence not enjoy programming productivity, high performance, and\nresearch flexibility at once.\n  In this paper we solve this problem. We present three sets of abstractions\nand interfaces to program GEMMs within the scientific Julia programming\nlanguage. The interfaces and abstractions are co-designed for researchers'\nneeds and Julia's features to achieve sufficient separation of concerns and\nflexibility to easily extend basic GEMMs in many different ways without paying\na performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS\nand CUTLASS, we demonstrate that our performance is mostly on par with, and in\nsome cases even exceeds, the libraries, without having to write a single line\nof code in CUDA C++ or assembly, and without facing flexibility limitations.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:29:08 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 12:41:57 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 06:43:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Faingnaert", "Thomas", ""], ["Besard", "Tim", ""], ["De Sutter", "Bjorn", ""]]}, {"id": "2009.12415", "submitter": "Haruna Isah", "authors": "Ruoran Liu, Haruna Isah, Farhana Zulkernine", "title": "A Big Data Lake for Multilevel Streaming Analytics", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large organizations are seeking to create new architectures and scalable\nplatforms to effectively handle data management challenges due to the explosive\nnature of data rarely seen in the past. These data management challenges are\nlargely posed by the availability of streaming data at high velocity from\nvarious sources in multiple formats. The changes in data paradigm have led to\nthe emergence of new data analytics and management architecture. This paper\nfocuses on storing high volume, velocity and variety data in the raw formats in\na data storage architecture called a data lake. First, we present our study on\nthe limitations of traditional data warehouses in handling recent changes in\ndata paradigms. We discuss and compare different open source and commercial\nplatforms that can be used to develop a data lake. We then describe our\nend-to-end data lake design and implementation approach using the Hadoop\nDistributed File System (HDFS) on the Hadoop Data Platform (HDP). Finally, we\npresent a real-world data lake development use case for data stream ingestion,\nstaging, and multilevel streaming analytics which combines structured and\nunstructured data. This study can serve as a guide for individuals or\norganizations planning to implement a data lake solution for their use cases.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 19:57:21 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liu", "Ruoran", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""]]}, {"id": "2009.12477", "submitter": "Shreyas Pai", "authors": "Kishore Kothapalli and Shreyas Pai and Sriram V. Pemmaraju", "title": "Sample-and-Gather: Fast Ruling Set Algorithms in the Low-Memory MPC\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by recent progress on symmetry breaking problems such as maximal\nindependent set (MIS) and maximal matching in the low-memory Massively Parallel\nComputation (MPC) model (e.g., Behnezhad et al.~PODC 2019; Ghaffari-Uitto SODA\n2019), we investigate the complexity of ruling set problems in this model. The\nMPC model has become very popular as a model for large-scale distributed\ncomputing and it comes with the constraint that the memory-per-machine is\nstrongly sublinear in the input size. For graph problems, extremely fast MPC\nalgorithms have been designed assuming $\\tilde{\\Omega}(n)$ memory-per-machine,\nwhere $n$ is the number of nodes in the graph (e.g., the $O(\\log\\log n)$ MIS\nalgorithm of Ghaffari et al., PODC 2018). However, it has proven much more\ndifficult to design fast MPC algorithms for graph problems in the low-memory\nMPC model, where the memory-per-machine is restricted to being strongly\nsublinear in the number of nodes, i.e., $O(n^\\eps)$ for $0 < \\eps < 1$.\n  In this paper, we present an algorithm for the 2-ruling set problem, running\nin $\\tilde{O}(\\log^{1/6} \\Delta)$ rounds whp, in the low-memory MPC model. We\nthen extend this result to $\\beta$-ruling sets for any integer $\\beta > 1$.\nSpecifically, we show that a $\\beta$-ruling set can be computed in the\nlow-memory MPC model with $O(n^\\eps)$ memory-per-machine in $\\tilde{O}(\\beta\n\\cdot \\log^{1/(2^{\\beta+1}-2)} \\Delta)$ rounds, whp. From this it immediately\nfollows that a $\\beta$-ruling set for $\\beta = \\Omega(\\log\\log\\log\n\\Delta)$-ruling set can be computed in in just $O(\\beta \\log\\log n)$ rounds\nwhp. The above results assume a total memory of $\\tilde{O}(m + n^{1+\\eps})$. We\nalso present algorithms for $\\beta$-ruling sets in the low-memory MPC model\nassuming that the total memory over all machines is restricted to\n$\\tilde{O}(m)$.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 00:04:01 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kothapalli", "Kishore", ""], ["Pai", "Shreyas", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "2009.12498", "submitter": "Mark Greaves", "authors": "Gianluca Longoni (1), Ryan LaMothe (1), Jeremy Teuton (1), Mark\n  Greaves (1), Nicole Nichols (1), William Smith (1) ((1) Pacific Northwest\n  National Laboratory)", "title": "Machine Learning Algorithms for Active Monitoring of High Performance\n  Computing as a Service (HPCaaS) Cloud Environments", "comments": "9 pages, 13 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing provides ubiquitous and on-demand access to vast\nreconfigurable resources that can meet any computational need. Many service\nmodels are available, but the Infrastructure as a Service (IaaS) model is\nparticularly suited to operate as a high performance computing (HPC) platform,\nby networking large numbers of cloud computing nodes. We used the Pacific\nNorthwest National Laboratory (PNNL) cloud computing environment to perform our\nexperiments. A number of cloud computing providers such as Amazon Web Services,\nMicrosoft Azure, or IBM Cloud, offer flexible and scalable computing resources.\nThis paper explores the viability identifying types of engineering applications\nrunning on a cloud infrastructure configured as an HPC platform using privacy\npreserving features as input to statistical models. The engineering\napplications considered in this work include MCNP6, a radiation transport code\ndeveloped by Los Alamos National Laboratory, OpenFOAM, an open source\ncomputational fluid dynamics code, and CADO-NFS, a numerical implementation of\nthe general number field sieve algorithm used for prime number factorization.\nOur experiments use the OpenStack cloud management tool to create a cloud HPC\nenvironment and the privacy preserving Ceilometer billing meters as\nclassification features to demonstrate identification of these applications.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 01:29:19 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Longoni", "Gianluca", ""], ["LaMothe", "Ryan", ""], ["Teuton", "Jeremy", ""], ["Greaves", "Mark", ""], ["Nichols", "Nicole", ""], ["Smith", "William", ""]]}, {"id": "2009.12617", "submitter": "Lawrence Stewart", "authors": "Lawrence C. Stewart and Carlo Pascoe and Brian W. Sherman and Martin\n  Herbordt and Vipin Sachdeva", "title": "Particle Mesh Ewald for Molecular Dynamics in OpenCL on an FPGA Cluster", "comments": "Accepted as a poster at FCCM21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular Dynamics (MD) simulations play a central role in physics-driven\ndrug discovery. MD applications often use the Particle Mesh Ewald (PME)\nalgorithm to accelerate electrostatic force computations, but efficient\nparallelization has proven difficult due to the high communication requirements\nof distributed 3D FFTs. In this paper, we present the design and implementation\nof a scalable PME algorithm that runs on a cluster of Intel Stratix 10 FPGAs\nand can handle FFT sizes appropriate to address real-world drug discovery\nprojects (grids up to $128^3$). To our knowledge, this is the first work to\nfully integrate all aspects of the PME algorithm (charge spreading, 3D\nFFT/IFFT, and force interpolation) within a distributed FPGA framework. The\ndesign is fully implemented with OpenCL for flexibility and ease of development\nand uses 100 Gbps links for direct FPGA-to-FPGA communications without the need\nfor host interaction. We present experimental data up to 4 FPGAs (e.g., 206\nmicroseconds per timestep for a 65536 atom simulation and $64^3$ 3D FFT),\noutperforming GPUs. Additionally, we discuss design scalability on clusters\nwith differing topologies up to 64 FPGAs (with expected performance greater\nthan all known GPU implementations) and integration with other hardware\ncomponents to form a complete molecular dynamics application. We predict\nbest-case performance of 6.6 microseconds per timestep on 64 FPGAs.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 15:12:20 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 12:44:38 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 02:53:34 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 15:13:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Stewart", "Lawrence C.", ""], ["Pascoe", "Carlo", ""], ["Sherman", "Brian W.", ""], ["Herbordt", "Martin", ""], ["Sachdeva", "Vipin", ""]]}, {"id": "2009.12672", "submitter": "Anup Das", "authors": "Twisha Titirsha and Anup Das", "title": "Reliability-Performance Trade-offs in Neuromorphic Computing", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic architectures built with Non-Volatile Memory (NVM) can\nsignificantly improve the energy efficiency of machine learning tasks designed\nwith Spiking Neural Networks (SNNs). A major source of voltage drop in a\ncrossbar of these architectures are the parasitic components on the crossbar's\nbitlines and wordlines, which are deliberately made longer to achieve lower\ncost-per-bit. We observe that the parasitic voltage drops create a significant\nasymmetry in programming speed and reliability of NVM cells in a crossbar.\nSpecifically, NVM cells that are on shorter current paths are faster to program\nbut have lower endurance than those on longer current paths, and vice versa.\nThis asymmetry in neuromorphic architectures create reliability-performance\ntrade-offs, which can be exploited efficiently using SNN mapping techniques. In\nthis work, we demonstrate such trade-offs using a previously-proposed SNN\nmapping technique with 10 workloads from contemporary machine learning tasks\nfor a state-of-the art neuromoorphic hardware.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 19:38:18 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Titirsha", "Twisha", ""], ["Das", "Anup", ""]]}, {"id": "2009.12828", "submitter": "Gilad Stern", "authors": "Ittai Abraham and Gilad Stern", "title": "Information Theoretic HotStuff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Information Theoretic HotStuff (IT-HS), a new optimally\nresilient protocol for solving Byzantine Agreement in partial synchrony with\ninformation theoretic security guarantees. In particular, IT-HS does not depend\non any PKI or common setup assumptions and is resilient to computationally\nunbounded adversaries. IT-HS is based on the Primary-Backup view-based\nparadigm. In IT-HS, in each view, and in each view change, each party sends\nonly a constant number of words to every other party. This yields an $O(n^2)$\nword and message complexity in each view. In addition, IT-HS requires just\n$O(1)$ persistent local storage and $O(n)$ transient local storage. Finally,\nlike all Primary-Backup view-based protocols in partial synchrony, after the\nsystem becomes synchronous, all nonfaulty parties decide on a value in the\nfirst view a nonfaulty leader is chosen. Moreover, like PBFT and HotStuff,\nIT-HS is optimistically responsive: with a nonfaulty leader, parties decide as\nquickly as the network allows them to do so, without regard for the known upper\nbound on network delay. Our work improves in multiple dimensions upon the\ninformation theoretic version of PBFT presented by Miguel Castro, and can be\nseen as an information theoretic variant of the HotStuff paradigm.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 12:25:34 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 20:37:02 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Abraham", "Ittai", ""], ["Stern", "Gilad", ""]]}, {"id": "2009.12850", "submitter": "Nick Brown", "authors": "Nick Brown, Angus Lepper, Mich\\`ele Weiland, Adrian Hill, Ben Shipway,\n  Chris Maynard", "title": "A directive based hybrid Met Office NERC Cloud model", "comments": null, "journal-ref": null, "doi": "10.1145/2832105.2832115", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Eddy Simulation is a critical modelling tool for the investigation of\natmospheric flows, turbulence and cloud microphysics. The models used by the UK\natmospheric research community are homogeneous and the latest model, MONC, is\ndesigned to run on substantial HPC systems with very high CPU core counts. In\norder to future proof these codes it is worth investigating other technologies\nand architectures which might support the communities running their codes at\nthe exa-scale.\n  In this paper we present a hybrid version of MONC, where the most\ncomputationally intensive aspect is offloaded to the GPU while the rest of the\nfunctionality runs concurrently on the CPU. Developed using the directive\ndriven OpenACC, we consider the suitability and maturity of this technology to\nmodern Fortran scientific codes as well general software engineering techniques\nwhich aid this type of porting work. The performance of our hybrid model at\nscale is compared against the CPU version before considering other tuning\noptions and making a comparison between the energy usage of the homo- and\nhetero-geneous versions. The result of this work is a promising hybrid model\nthat shows performance benefits of our approach when the GPU has a significant\ncomputational workload which can not only be applied to the MONC model but also\nother weather and climate simulations in use by the community.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 14:10:55 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Brown", "Nick", ""], ["Lepper", "Angus", ""], ["Weiland", "Mich\u00e8le", ""], ["Hill", "Adrian", ""], ["Shipway", "Ben", ""], ["Maynard", "Chris", ""]]}, {"id": "2009.12922", "submitter": "Olga Poppe", "authors": "Olga Poppe, Tayo Amuneke, Dalitso Banda, Aritra De, Ari Green, Manon\n  Knoertzer, Ehi Nosakhare, Karthik Rajendran, Deepak Shankargouda, Meina Wang,\n  Alan Au, Carlo Curino, Qun Guo, Alekh Jindal, Ajay Kalhan, Morgan Oslake,\n  Sonia Parchani, Vijay Ramani, Raj Sellappan, Saikat Sen, Sheetal Shrotri,\n  Soundararajan Srinivasan, Ping Xia, Shize Xu, Alicia Yang, Yiwen Zhu", "title": "Seagull: An Infrastructure for Load Prediction and Optimized Resource\n  Allocation", "comments": "Technical report for the paper in VLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsoft Azure is dedicated to guarantee high quality of service to its\ncustomers, in particular, during periods of high customer activity, while\ncontrolling cost. We employ a Data Science (DS) driven solution to predict user\nload and leverage these predictions to optimize resource allocation. To this\nend, we built the Seagull infrastructure that processes per-server telemetry,\nvalidates the data, trains and deploys ML models. The models are used to\npredict customer load per server (24h into the future), and optimize service\noperations. Seagull continually re-evaluates accuracy of predictions, fallback\nto previously known good models and triggers alerts as appropriate. We deployed\nthis infrastructure in production for PostgreSQL and MySQL servers across all\nAzure regions, and applied it to the problem of scheduling server backups\nduring low-load time. This minimizes interference with user-induced load and\nimproves customer experience.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 18:41:32 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 19:22:57 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Poppe", "Olga", ""], ["Amuneke", "Tayo", ""], ["Banda", "Dalitso", ""], ["De", "Aritra", ""], ["Green", "Ari", ""], ["Knoertzer", "Manon", ""], ["Nosakhare", "Ehi", ""], ["Rajendran", "Karthik", ""], ["Shankargouda", "Deepak", ""], ["Wang", "Meina", ""], ["Au", "Alan", ""], ["Curino", "Carlo", ""], ["Guo", "Qun", ""], ["Jindal", "Alekh", ""], ["Kalhan", "Ajay", ""], ["Oslake", "Morgan", ""], ["Parchani", "Sonia", ""], ["Ramani", "Vijay", ""], ["Sellappan", "Raj", ""], ["Sen", "Saikat", ""], ["Shrotri", "Sheetal", ""], ["Srinivasan", "Soundararajan", ""], ["Xia", "Ping", ""], ["Xu", "Shize", ""], ["Yang", "Alicia", ""], ["Zhu", "Yiwen", ""]]}, {"id": "2009.12999", "submitter": "Shaoming Song", "authors": "Shaoming Song, Yunfeng Shao, Jian Li", "title": "Loosely Coupled Federated Learning Over Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) was proposed to achieve collaborative machine\nlearning among various clients without uploading private data. However, due to\nmodel aggregation strategies, existing frameworks require strict model\nhomogeneity, limiting the application in more complicated scenarios. Besides,\nthe communication cost of FL's model and gradient transmission is extremely\nhigh. This paper proposes Loosely Coupled Federated Learning (LC-FL), a\nframework using generative models as transmission media to achieve low\ncommunication cost and heterogeneous federated learning. LC-FL can be applied\non scenarios where clients possess different kinds of machine learning models.\nExperiments on real-world datasets covering different multiparty scenarios\ndemonstrate the effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 01:09:23 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Song", "Shaoming", ""], ["Shao", "Yunfeng", ""], ["Li", "Jian", ""]]}, {"id": "2009.13006", "submitter": "Uri Meir", "authors": "Uri Meir, Ami Paz, Gregory Schwartzman", "title": "Models of Smoothing in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed analysis is a framework suggested for mediating gaps between\nworst-case and average-case complexities. In a recent work, Dinitz et\nal.~[Distributed Computing, 2018] suggested to use smoothed analysis in order\nto study dynamic networks. Their aim was to explain the gaps between real-world\nnetworks that function well despite being dynamic, and the strong theoretical\nlower bounds for arbitrary networks. To this end, they introduced a basic model\nof smoothing in dynamic networks, where an adversary picks a sequence of\ngraphs, representing the topology of the network over time, and then each of\nthese graphs is slightly perturbed in a random manner. The model suggested\nabove is based on a per-round noise, and our aim in this work is to extend it\nto models of noise more suited for multiple rounds. This is motivated by\nlong-lived networks, where the amount and location of noise may vary over time.\nTo this end, we present several different models of noise. First, we extend the\nprevious model to cases where the amount of noise is very small. Then, we move\nto more refined models, where the amount of noise can change between different\nrounds, e.g., as a function of the number of changes the network undergoes. We\nalso study a model where the noise is not arbitrarily spread among the network,\nbut focuses in each round in the areas where changes have occurred. Finally, we\nstudy the power of an adaptive adversary, who can choose its actions in\naccordance with the changes that have occurred so far. We use the flooding\nproblem as a running case-study, presenting very different behaviors under the\ndifferent models of noise, and analyze the flooding time in different models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 01:29:12 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Meir", "Uri", ""], ["Paz", "Ami", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "2009.13178", "submitter": "Filip De Turck", "authors": "Filip De Turck", "title": "Efficient Resource Allocation through Integer Linear Programming: a\n  detailed example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how a resource allocation problem can be solved\nthrough Integer Linear Programming (ILP). A detailed illustrative example is\npresented, together with an exhaustive overview of the mathematical model. The\nsize of the required vectors and matrix are determined as well. The presented\nexample can be used to learn students the fundamental basics of ILP-based\nresource allocation. Next, the specific benefits of the ILP approach compared\nto other resource allocation algorithms are outlined in this paper. Finally, a\nrelated work section is provided with relevant references for further reading.\nThe provided references contain examples of ILP-based resource allocation in\nmodern networks and computing infrastructures.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 09:47:43 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["De Turck", "Filip", ""]]}, {"id": "2009.13416", "submitter": "Robert Kl\\\"ofkorn", "authors": "Andreas Dedner, Robert Kl\\\"ofkorn", "title": "Extendible and Efficient Python Framework for Solving Evolution\n  Equations with Stabilized Discontinuous Galerkin Method", "comments": "36 pages, 15 figures, various Python code examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a Python interface for the recently published\nDUNE-FEM-DG module which provides highly efficient implementations of the\nDiscontinuous Galerkin (DG) method for solving a wide range of non linear\npartial differential equations (PDE). Although the C++ interfaces of\nDUNE-FEM-DG are highly flexible and customizable, a solid knowledge of C++ is\nnecessary to make use of this powerful tool. With this work easier user\ninterfaces based on Python and the Unified Form Language are provided to open\nDUNE-FEM-DG for a broader audience. The Python interfaces are demonstrated for\nboth parabolic and first order hyperbolic PDEs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:23:57 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 09:28:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dedner", "Andreas", ""], ["Kl\u00f6fkorn", "Robert", ""]]}, {"id": "2009.13569", "submitter": "Michael Axtmann", "authors": "Michael Axtmann, Sascha Witt, Daniel Ferizovic, Peter Sanders", "title": "Engineering In-place (Shared-memory) Sorting Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sorting algorithms that represent the fastest known techniques for\na wide range of input sizes, input distributions, data types, and machines. A\npart of the speed advantage is due to the feature to work in-place. Previously,\nthe in-place feature often implied performance penalties. Our main algorithmic\ncontribution is a blockwise approach to in-place data distribution that is\nprovably cache-efficient. We also parallelize this approach taking dynamic load\nbalancing and memory locality into account. Our comparison-based algorithm,\nIn-place Superscalar Samplesort (IPS$^4$o), combines this technique with\nbranchless decision trees. By taking cases with many equal elements into\naccount and by adapting the distribution degree dynamically, we obtain a highly\nrobust algorithm that outperforms the best in-place parallel comparison-based\ncompetitor by almost a factor of three. IPS$^4$o also outperforms the best\ncomparison-based competitors in the in-place or not in-place, parallel or\nsequential settings. IPS$^4$o even outperforms the best integer sorting\nalgorithms in a wide range of situations. In many of the remaining cases (often\ninvolving near-uniform input distributions, small keys, or a sequential\nsetting), our new in-place radix sorter turns out to be the best algorithm.\nClaims to have the, in some sense, \"best\" sorting algorithm can be found in\nmany papers which cannot all be true. Therefore, we base our conclusions on\nextensive experiments involving a large part of the cross product of 21\nstate-of-the-art sorting codes, 6 data types, 10 input distributions, 4\nmachines, 4 memory allocation strategies, and input sizes varying over 7 orders\nof magnitude. This confirms the robust performance of our algorithms while\nrevealing major performance problems in many competitors outside the concrete\nset of measurements reported in the associated publications.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 18:35:30 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:09:25 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Axtmann", "Michael", ""], ["Witt", "Sascha", ""], ["Ferizovic", "Daniel", ""], ["Sanders", "Peter", ""]]}, {"id": "2009.13644", "submitter": "Sergio Rajsbaum", "authors": "Sergio Rajsbaum", "title": "A Distributed Computing Perspective of Unconditionally Secure\n  Information Transmission in Russian Cards Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of $A$ privately transmitting information to $B$ by a public\nannouncement overheard by an eavesdropper $C$ is considered. To do so by a\ndeterministic protocol, their inputs must be correlated. Dependent inputs are\nrepresented using a deck of cards. There is a publicly known signature\n$(a,b,c)$, where $n = a + b + c + r$, and $A$ gets $a$ cards, $B$ gets $b$\ncards, and $C$ gets $c$ cards, out of the deck of $n$ cards.\n  Using a deterministic protocol, $A$ decides its announcement based on her\nhand. Using techniques from coding theory, Johnson graphs, and additive number\ntheory, a novel perspective inspired by distributed computing theory is\nprovided, to analyze the amount of information that $A$ needs to send, while\npreventing $C$ from learning a single card of her hand. In one extreme, the\ngeneralized Russian cards problem, $B$ wants to learn all of $A$'s cards, and\nin the other, $B$ wishes to learn something about $A$'s hand.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:44:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Rajsbaum", "Sergio", ""]]}, {"id": "2009.13701", "submitter": "Haosen Wen", "authors": "Haosen Wen, Wentao Cai, Mingzhe Du, Louis Jenkins, Benjamin Valpey,\n  Michael L. Scott", "title": "Montage: A General System for Buffered Durably Linearizable Data\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent emergence of fast, dense, nonvolatile main memory suggests that\ncertain long-lived data might remain in its natural pointer-rich format across\nprogram runs and hardware reboots. Operations on such data must be instrumented\nwith explicit write-back and fence instructions to ensure consistency in the\nwake of a crash. Techniques to minimize the cost of this instrumentation are an\nactive topic of research.\n  We present what we believe to be the first general-purpose approach to\nbuilding buffered durably linearizable persistent data structures, and a\nsystem, Montage, to support that approach. Montage is built on top of the\nRalloc nonblocking persistent allocator. It employs a slow-ticking epoch clock,\nand ensures that no operation appears to span an epoch boundary. It also\narranges to persist only that data minimally required to reconstruct the\nstructure after a crash. If a crash occurs in epoch $e$, all work performed in\nepochs $e$ and $e-1$ is lost, but work from prior epochs is preserved.\n  We describe the implementation of Montage, argue its correctness, and report\nunprecedented throughput for persistent queues, sets/mappings, and general\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 00:30:33 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wen", "Haosen", ""], ["Cai", "Wentao", ""], ["Du", "Mingzhe", ""], ["Jenkins", "Louis", ""], ["Valpey", "Benjamin", ""], ["Scott", "Michael L.", ""]]}, {"id": "2009.13863", "submitter": "Zhaoyang Zhang", "authors": "Zhuojun Tian, Zhaoyang Zhang, Jue Wang, Xiaoming Chen, Wei Wang, and\n  Huaiyu Dai", "title": "Distributed ADMM with Synergetic Communication and Computation", "comments": "Accepted for publication in IEEE TCOM", "journal-ref": null, "doi": "10.1109/TCOMM.2020.3027032", "report-no": null, "categories": "eess.SP cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel distributed alternating direction method of\nmultipliers (ADMM) algorithm with synergetic communication and computation,\ncalled SCCD-ADMM, to reduce the total communication and computation cost of the\nsystem. Explicitly, in the proposed algorithm, each node interacts with only\npart of its neighboring nodes, the number of which is progressively determined\naccording to a heuristic searching procedure, which takes into account both the\npredicted convergence rate and the communication and computation costs at each\niteration, resulting in a trade-off between communication and computation. Then\nthe node chooses its neighboring nodes according to an importance sampling\ndistribution derived theoretically to minimize the variance with the latest\ninformation it locally stores. Finally, the node updates its local information\nwith a new update rule which adapts to the number of communication nodes. We\nprove the convergence of the proposed algorithm and provide an upper bound of\nthe convergence variance brought by randomness. Extensive simulations validate\nthe excellent performances of the proposed algorithm in terms of convergence\nrate and variance, the overall communication and computation cost, the impact\nof network topology as well as the time for evaluation, in comparison with the\ntraditional counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:36:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Tian", "Zhuojun", ""], ["Zhang", "Zhaoyang", ""], ["Wang", "Jue", ""], ["Chen", "Xiaoming", ""], ["Wang", "Wei", ""], ["Dai", "Huaiyu", ""]]}, {"id": "2009.13903", "submitter": "Georg Hager", "authors": "Christie L. Alappat, Jan Laukemann, Thomas Gruber, Georg Hager,\n  Gerhard Wellein, Nils Meyer, Tilo Wettig", "title": "Performance Modeling of Streaming Kernels and Sparse Matrix-Vector\n  Multiplication on A64FX", "comments": "6 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The A64FX CPU powers the current number one supercomputer on the Top500 list.\nAlthough it is a traditional cache-based multicore processor, its peak\nperformance and memory bandwidth rival accelerator devices. Generating\nefficient code for such a new architecture requires a good understanding of its\nperformance features. Using these features, we construct the\nExecution-Cache-Memory (ECM) performance model for the A64FX processor in the\nFX700 supercomputer and validate it using streaming loops. We also identify\narchitectural peculiarities and derive optimization hints. Applying the ECM\nmodel to sparse matrix-vector multiplication (SpMV), we motivate why the CRS\nmatrix storage format is inappropriate and how the SELL-C-sigma format with\nsuitable code optimizations can achieve bandwidth saturation for SpMV.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 09:52:59 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Alappat", "Christie L.", ""], ["Laukemann", "Jan", ""], ["Gruber", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Meyer", "Nils", ""], ["Wettig", "Tilo", ""]]}, {"id": "2009.13982", "submitter": "Xinyue Liang", "authors": "Xinyue Liang, Alireza M. Javid, Mikael Skoglund, Saikat Chatterjee", "title": "A Low Complexity Decentralized Neural Net with Centralized Equivalence\n  using Layer-wise Learning", "comments": "Accepted to The International Joint Conference on Neural Networks\n  (IJCNN) 2020, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a low complexity decentralized learning algorithm to train a\nrecently proposed large neural network in distributed processing nodes\n(workers). We assume the communication network between the workers is\nsynchronized and can be modeled as a doubly-stochastic mixing matrix without\nhaving any master node. In our setup, the training data is distributed among\nthe workers but is not shared in the training process due to privacy and\nsecurity concerns. Using alternating-direction-method-of-multipliers (ADMM)\nalong with a layerwise convex optimization approach, we propose a decentralized\nlearning algorithm which enjoys low computational complexity and communication\ncost among the workers. We show that it is possible to achieve equivalent\nlearning performance as if the data is available in a single place. Finally, we\nexperimentally illustrate the time complexity and convergence behavior of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:08:12 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Liang", "Xinyue", ""], ["Javid", "Alireza M.", ""], ["Skoglund", "Mikael", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "2009.14123", "submitter": "Fahad Saeed", "authors": "Fahad Saeed", "title": "Communication Lower-Bounds for Distributed-Memory Computations for Mass\n  Spectrometry based Omics Data", "comments": "15 pages, 4 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.GN q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass spectrometry based omics data analysis require significant time and\nresources. To date, few parallel algorithms have been proposed for deducing\npeptides from mass spectrometry based data. However, these parallel algorithms\nwere designed, and developed when the amount of data that needed to be\nprocessed was smaller in scale. In this paper, we prove that the communication\nbound that is reached by the \\emph{existing} parallel algorithms is\n$\\Omega(mn+2r\\frac{q}{p})$, where $m$ and $n$ are the dimensions of the\ntheoretical database matrix, $q$ and $r$ are dimensions of spectra, and $p$ is\nthe number of processors. We further prove that communication-optimal strategy\nwith fast-memory $\\sqrt{M} = mn + \\frac{2qr}{p}$ can achieve\n$\\Omega({\\frac{2mnq}{p}})$ but is not achieved by any existing parallel\nproteomics algorithms till date. To further validate our claim, we performed a\nmeta-analysis of published parallel algorithms, and their performance results.\nWe show that sub-optimal speedups with increasing number of processors is a\ndirect consequence of not achieving the communication lower-bounds proved in\nthis paper. Consequently, we assert that next-generation of provable, and\ndemonstrated superior parallel algorithms are urgently needed for MS based\nlarge systems-biology studies especially for meta-proteomics, protegenomics,\nmicrobiome, and proteomics for non-model organisms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:11:59 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Saeed", "Fahad", ""]]}, {"id": "2009.14125", "submitter": "Xuebin Ren Dr", "authors": "Xuebin Ren, Chia-Mu Yu, Wei Yu, Xinyu Yang, Jun Zhao, Shusen Yang", "title": "DPCrowd: Privacy-preserving and Communication-efficient Decentralized\n  Statistical Estimation for Real-time Crowd-sourced Data", "comments": null, "journal-ref": null, "doi": "10.1109/JIOT.2020.3020089", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Internet of Things (IoT) driven smart-world systems, real-time\ncrowd-sourced databases from multiple distributed servers can be aggregated to\nextract dynamic statistics from a larger population, thus providing more\nreliable knowledge for our society. Particularly, multiple distributed servers\nin a decentralized network can realize real-time collaborative statistical\nestimation by disseminating statistics from their separate databases. Despite\nno raw data sharing, the real-time statistics could still expose the data\nprivacy of crowd-sourcing participants. For mitigating the privacy concern,\nwhile traditional differential privacy (DP) mechanism can be simply implemented\nto perturb the statistics in each timestamp and independently for each\ndimension, this may suffer a great utility loss from the real-time and\nmulti-dimensional crowd-sourced data. Also, the real-time broadcasting would\nbring significant overheads in the whole network. To tackle the issues, we\npropose a novel privacy-preserving and communication-efficient decentralized\nstatistical estimation algorithm (DPCrowd), which only requires intermittently\nsharing the DP protected parameters with one-hop neighbors by exploiting the\ntemporal correlations in real-time crowd-sourced data. Then, with further\nconsideration of spatial correlations, we develop an enhanced algorithm,\nDPCrowd+, to deal with multi-dimensional infinite crowd-data streams. Extensive\nexperiments on several datasets demonstrate that our proposed schemes DPCrowd\nand DPCrowd+ can significantly outperform existing schemes in providing\naccurate and consensus estimation with rigorous privacy protection and great\ncommunication efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:18:44 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Ren", "Xuebin", ""], ["Yu", "Chia-Mu", ""], ["Yu", "Wei", ""], ["Yang", "Xinyu", ""], ["Zhao", "Jun", ""], ["Yang", "Shusen", ""]]}, {"id": "2009.14349", "submitter": "Liangkai Liu", "authors": "Liangkai Liu, Sidi Lu, Ren Zhong, Baofu Wu, Yongtao Yao, Qingyang\n  Zhang, and Weisong Shi", "title": "Computing Systems for Autonomous Driving: State-of-the-Art and\n  Challenges", "comments": "Accepted to IEEE Internet of Things Journal (19 pages, 2 figures, 4\n  tables)", "journal-ref": null, "doi": null, "report-no": "Technical Report: CAR-TR-2020-009", "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proliferation of computing technologies (e.g., sensors, computer\nvision, machine learning, and hardware acceleration), and the broad deployment\nof communication mechanisms (e.g., DSRC, C-V2X, 5G) have pushed the horizon of\nautonomous driving, which automates the decision and control of vehicles by\nleveraging the perception results based on multiple sensors. The key to the\nsuccess of these autonomous systems is making a reliable decision in real-time\nfashion. However, accidents and fatalities caused by early deployed autonomous\nvehicles arise from time to time. The real traffic environment is too\ncomplicated for current autonomous driving computing systems to understand and\nhandle. In this paper, we present state-of-the-art computing systems for\nautonomous driving, including seven performance metrics and nine key\ntechnologies, followed by twelve challenges to realize autonomous driving. We\nhope this paper will gain attention from both the computing and automotive\ncommunities and inspire more research in this direction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 00:01:54 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 01:26:54 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 20:08:44 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liu", "Liangkai", ""], ["Lu", "Sidi", ""], ["Zhong", "Ren", ""], ["Wu", "Baofu", ""], ["Yao", "Yongtao", ""], ["Zhang", "Qingyang", ""], ["Shi", "Weisong", ""]]}, {"id": "2009.14414", "submitter": "Haiwen Du Dr.", "authors": "Dongjie Zhu, Haiwen Du, Yundong Sun, Zhaoshuo Tian", "title": "CTDGM: A Data Grouping Model Based on Cache Transaction for Unstructured\n  Data Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cache prefetching technology has become the mainstream data access\noptimization strategy in the data centers. However, the rapidly increasing of\nunstructured data generates massive pairwise access relationships, which can\nresult in a heavy computational burden for the existing prefetching model and\nlead to severe degradation in the performance of data access. We propose\ncache-transaction-based data grouping model (CTDGM) to solve the problems\ndescribed above by optimizing the feature representation method and grouping\nefficiency. First, we provide the definition of the cache transaction and\npropose the method for extracting the cache transaction feature (CTF). Second,\nwe design a data chunking algorithm based on CTF and spatiotemporal locality to\noptimize the relationship calculation efficiency. Third, we propose CTDGM by\nconstructing a relation graph that groups data into independent groups\naccording to the strength of the data access relation. Based on the results of\nthe experiment, compared with the state-of-the-art methods, our algorithm\nachieves an average increase in the cache hit rate of 12% on the MSR dataset\nwith small cache size (0.001% of all the data), which in turn reduces the\nnumber of data I/O accesses by 50% when the cache size is less than 0.008% of\nall the data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 03:51:24 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Zhu", "Dongjie", ""], ["Du", "Haiwen", ""], ["Sun", "Yundong", ""], ["Tian", "Zhaoshuo", ""]]}, {"id": "2009.14426", "submitter": "Yonghwan Kim", "authors": "Yonghwan Kim, Yoshiaki Katayama, and Koichi Wada", "title": "Pairbot: A Novel Model for Autonomous Mobile Robot Systems Consisting of\n  Paired Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable matter consists of many self-organizing computational entities\nwhich are autonomous and cooperative with one another to achieve a goal and it\nhas been widely studied in various fields, e.g., robotics or mobile agents,\ntheoretically and practically. In the field of computer science, programmable\nmatter can be theoretically modeled as a distributed system consisting of\nsimple and small robots equipped with limited capabilities, e.g., no memory\nand/or no geometrical coordination. A lot of theoretical research is studied\nbased on such theoretical models, to clarify the relation between the\nsolvability of various problems and the considered models.\n  We newly propose a computational model named Pairbot model where two\nautonomous mobile robots operate as a pair on a grid plane. In Pairbot model,\nevery robot has the one robot as its unique partner, called buddy, each other.\nWe call the paired two robots pairbot. Two robots in one pairbot can recognize\neach other, and repeatedly change their geometrical relationships, long and\nshort, to achieve the goal.\n  In this paper, as a first step to show the feasibility and effectiveness of\nthe proposed Pairbot model, we introduce two simple problems, the marching\nproblem and the object coating problem, and propose two algorithms to solve\nthese two problems, respectively. In both algorithms, it is assumed that the\nvisibility range is one (every robot can observe only its neighboring robots)\nand the scheduler is asynchronous (ASYNC).\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 04:38:41 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kim", "Yonghwan", ""], ["Katayama", "Yoshiaki", ""], ["Wada", "Koichi", ""]]}, {"id": "2009.14435", "submitter": "David Monniaux", "authors": "Camille Coti, David Monniaux (VERIMAG - IMAG), Hang Yu", "title": "A task-based approach to parallel parametric linear programming solving,\n  and application to polyhedral computations", "comments": "arXiv admin note: text overlap with arXiv:1904.06079", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric linear programming is a central operation for polyhedral\ncomputations, as well as in certain control applications.Here we propose a\ntask-based scheme for parallelizing it, with quasi-linear speedup over large\nproblems.This type of parallel applications is challenging, because several\ntasks mightbe computing the same region. In this paper, we are presenting\nthealgorithm itself with a parallel redundancy elimination algorithm,\nandconducting a thorough performance analysis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 07:04:09 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Coti", "Camille", "", "VERIMAG - IMAG"], ["Monniaux", "David", "", "VERIMAG - IMAG"], ["Yu", "Hang", ""]]}, {"id": "2009.14467", "submitter": "Aydin Buluc", "authors": "Oguz Selvitopi, Saliya Ekanayake, Giulia Guidi, Georgios Pavlopoulos,\n  Ariful Azad, and Aydin Buluc", "title": "Distributed Many-to-Many Protein Sequence Alignment using Sparse\n  Matrices", "comments": "To appear in International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying similar protein sequences is a core step in many computational\nbiology pipelines such as detection of homologous protein sequences, generation\nof similarity protein graphs for downstream analysis, functional annotation and\ngene location. Performance and scalability of protein similarity searches have\nproven to be a bottleneck in many bioinformatics pipelines due to increases in\ncheap and abundant sequencing data. This work presents a new distributed-memory\nsoftware, PASTIS. PASTIS relies on sparse matrix computations for efficient\nidentification of possibly similar proteins. We use distributed sparse matrices\nfor scalability and show that the sparse matrix infrastructure is a great fit\nfor protein similarity searches when coupled with a fully-distributed\ndictionary of sequences that allows remote sequence requests to be fulfilled.\nOur algorithm incorporates the unique bias in amino acid sequence substitution\nin searches without altering the basic sparse matrix model, and in turn,\nachieves ideal scaling up to millions of protein sequences.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 06:31:41 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Selvitopi", "Oguz", ""], ["Ekanayake", "Saliya", ""], ["Guidi", "Giulia", ""], ["Pavlopoulos", "Georgios", ""], ["Azad", "Ariful", ""], ["Buluc", "Aydin", ""]]}, {"id": "2009.14549", "submitter": "Francesco Liberati", "authors": "Francesco Liberati, Andrea Tortorelli, Cesar Mazquiaran, Muhammad\n  Imran, Martina Panfili", "title": "Optimal Control of Industrial Assembly Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of assembly line control and introduces an\noptimal control formulation that can be used to improve the performance of the\nassembly line, in terms of cycle time minimization, resources' utilization,\netc. A deterministic formulation of the problem is introduced, based on\nmixed-integer linear programming. A simple numerical simulation provides a\nfirst proof of the proposed concept.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 10:32:55 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Liberati", "Francesco", ""], ["Tortorelli", "Andrea", ""], ["Mazquiaran", "Cesar", ""], ["Imran", "Muhammad", ""], ["Panfili", "Martina", ""]]}, {"id": "2009.14590", "submitter": "Lorenzo De Stefani", "authors": "Lorenzo De Stefani", "title": "Communication-Optimal Parallel Standard and Karatsuba Integer\n  Multiplication in the Distributed Memory Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COPSIM a parallel implementation of standard integer\nmultiplication for the distributed memory setting, and COPK a parallel\nimplementation of Karatsuba's fast integer multiplication algorithm for a\ndistributed memory setting. When using $\\mathcal{P}$ processors, each equipped\nwith a local non-shared memory, to compute the product of tho $n$-digits\ninteger numbers, under mild conditions, our algorithms achieve optimal speedup\nof the computational time. That is, $\\mathcal{O}\\left(n^2/\\mathcal{P}\\right)$\nfor COPSIM, and $\\mathcal{O}\\left(n^{\\log_2 3}/\\mathcal{P}\\right)$ for COPK.\nThe total amount of memory required across the processors is\n$\\mathcal{O}\\left(n\\right)$, that is, within a constant factor of the minimum\nspace required to store the input values. We rigorously analyze the\nInput/Output (I/O) cost of the proposed algorithms. We show that their\nbandwidth cost (i.e., the number of memory words sent or received by at least\none processors) matches asymptotically corresponding known I/O lower bounds,\nand their latency (i.e., the number of messages sent or received in the\nalgorithm's critical execution path) is asymptotically within a multiplicative\nfactor $\\mathcal{O}\\left(\\log^2_2 \\mathcal{P}\\right)$ of the corresponding\nknown I/O lower bounds. Hence, our algorithms are asymptotically optimal with\nrespect to the bandwidth cost and almost asymptotically optimal with respect to\nthe latency cost.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 12:10:54 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["De Stefani", "Lorenzo", ""]]}, {"id": "2009.14600", "submitter": "Orestis Zachariadis", "authors": "Orestis Zachariadis, Nitin Satpute, Juan G\\'omez-Luna, Joaqu\\'in\n  Olivares", "title": "Accelerating Sparse Matrix-Matrix Multiplication with GPU Tensor Cores", "comments": "Accepted in CAEE", "journal-ref": "Comput. Electr. Eng. 88 (2020) 106848", "doi": "10.1016/j.compeleceng.2020.106848", "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse general matrix-matrix multiplication (spGEMM) is an essential\ncomponent in many scientific and data analytics applications. However, the\nsparsity pattern of the input matrices and the interaction of their patterns\nmake spGEMM challenging. Modern GPUs include Tensor Core Units (TCUs), which\nspecialize in dense matrix multiplication. Our aim is to re-purpose TCUs for\nsparse matrices. The key idea of our spGEMM algorithm, tSparse, is to multiply\nsparse rectangular blocks using the mixed precision mode of TCUs. tSparse\npartitions the input matrices into tiles and operates only on tiles which\ncontain one or more elements. It creates a task list of the tiles, and performs\nmatrix multiplication of these tiles using TCUs. To the best of our knowledge,\nthis is the first time that TCUs are used in the context of spGEMM. We show\nthat spGEMM, with our tiling approach, benefits from TCUs. Our approach\nsignificantly improves the performance of spGEMM in comparison to cuSPARSE,\nCUSP, RMerge2, Nsparse, AC-SpGEMM and spECK.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 14:10:15 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Zachariadis", "Orestis", ""], ["Satpute", "Nitin", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Olivares", "Joaqu\u00edn", ""]]}, {"id": "2009.14729", "submitter": "Shaked Matar", "authors": "Elkin Michael, Matar Shaked", "title": "Deterministic PRAM Approximate Shortest Paths in Polylogarithmic Time\n  and Slightly Super-Linear Work", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study a $(1+\\epsilon)$-approximate single-source shortest paths\n(henceforth, $(1+\\epsilon)$-SSSP) in $n$-vertex undirected, weighted graphs in\nthe parallel (PRAM) model of computation. A randomized algorithm with\npolylogarithmic time and slightly super-linear work $\\tilde{O}(|E|\\cdot\nn^\\rho)$, for an arbitrarily small $\\rho>0$, was given by Cohen [Coh94] more\nthan $25$ years ago.\n  Exciting progress on this problem was achieved in recent years\n[ElkinN17,ElkinN19,Li19,AndoniSZ19], culminating in randomized polylogarithmic\ntime and $\\tilde{O}(|E|)$ work. However, the question of whether there exists a\ndeterministic counterpart of Cohen's algorithm remained wide open.\n  In the current paper we devise the first deterministic polylogarithmic-time\nalgorithm for this fundamental problem, with work $\\tilde{O}(|E|\\cdot n^\\rho)$,\nfor an arbitrarily small $\\rho>0$. This result is based on the first efficient\ndeterministic parallel algorithm for building hopsets, which we devise in this\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:09:54 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Michael", "Elkin", ""], ["Shaked", "Matar", ""]]}, {"id": "2009.14763", "submitter": "Nirupam Gupta", "authors": "Nirupam Gupta, Thinh T. Doan and Nitin H. Vaidya", "title": "Byzantine Fault-Tolerance in Decentralized Optimization under Minimal\n  Redundancy", "comments": "An extension of our prior work on fault-tolerant distributed\n  optimization, for the server-based system architecture\n  (https://dl.acm.org/doi/10.1145/3382734.3405748), to the more general\n  peer-to-peer system architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of Byzantine fault-tolerance in multi-agent\ndecentralized optimization. In this problem, each agent has a local cost\nfunction. The goal of a decentralized optimization algorithm is to allow the\nagents to cooperatively compute a common minimum point of their aggregate cost\nfunction. We consider the case when a certain number of agents may be Byzantine\nfaulty. Such faulty agents may not follow a prescribed algorithm, and they may\nshare arbitrary or incorrect information with other non-faulty agents. Presence\nof such Byzantine agents renders a typical decentralized optimization algorithm\nineffective. We propose a decentralized optimization algorithm with provable\nexact fault-tolerance against a bounded number of Byzantine agents, provided\nthe non-faulty agents have a minimal redundancy.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 16:02:15 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Gupta", "Nirupam", ""], ["Doan", "Thinh T.", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2009.14783", "submitter": "Yifan Ding", "authors": "Yifan Ding, Nicholas Botzer and Tim Weninger", "title": "HetSeq: Distributed GPU Training on Heterogeneous Infrastructure", "comments": "7 pages, 3 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning systems like PyTorch and Tensorflow are able to train\nenormous models with billions (or trillions) of parameters on a distributed\ninfrastructure. These systems require that the internal nodes have the same\nmemory capacity and compute performance. Unfortunately, most organizations,\nespecially universities, have a piecemeal approach to purchasing computer\nsystems resulting in a heterogeneous infrastructure, which cannot be used to\ncompute large models. The present work describes HetSeq, a software package\nadapted from the popular PyTorch package that provides the capability to train\nlarge neural network models on heterogeneous infrastructure. Experiments with\ntransformer translation and BERT language model shows that HetSeq scales over\nheterogeneous systems. HetSeq can be easily extended to other models like image\nclassification. Package with supported document is publicly available at\nhttps://github.com/yifding/hetseq.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 19:57:42 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ding", "Yifan", ""], ["Botzer", "Nicholas", ""], ["Weninger", "Tim", ""]]}]