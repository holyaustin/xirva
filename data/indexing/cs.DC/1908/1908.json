[{"id": "1908.00198", "submitter": "Cheng-Shang Chang", "authors": "Cheng-Shang Chang and Jang-Ping Sheu and Yi-Jheng Lin", "title": "On the Theoretical Gap of Channel Hopping Sequences with Maximum\n  Rendezvous Diversity in the Multichannel Rendezvous Problem", "comments": null, "journal-ref": "IEEE/ACM Transactions on Networking, March, 2021", "doi": "10.1109/TNET.2021.3067643", "report-no": null, "categories": "cs.DC cs.DM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature, there are several well-known periodic channel hopping (CH)\nsequences that can achieve maximum rendezvous diversity in a cognitive radio\nnetwork (CRN). For a CRN with $N$ channels, it is known that the period of such\na CH sequence is at least $N^2$. The asymptotic approximation ratio, defined as\nthe ratio of the period of a CH sequence to the lower bound $N^2$ when $N \\to\n\\infty$, is still 2.5 for the best known CH sequence in the literature. An open\nquestion in the multichannel rendezvous problem is whether it is possible to\nconstruct a periodic CH sequence that has the asymptotic approximation ratio 1.\nIn this paper, we tighten the theoretical gap by proposing CH sequences, called\nIDEAL-CH, that have the asymptotic approximation ratio 2.\n  For a weaker requirement that only needs the two users to rendezvous on one\ncommonly available channel in a period, we propose channel hopping sequences,\ncalled ORTHO-CH, with period $(2p +1)p$, where $p$ is the smallest prime not\nless than $N$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 03:47:10 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 10:44:07 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Chang", "Cheng-Shang", ""], ["Sheu", "Jang-Ping", ""], ["Lin", "Yi-Jheng", ""]]}, {"id": "1908.00204", "submitter": "Sheldon Tan", "authors": "Shaoyi Peng and Sheldon X.-D. Tan", "title": "GLU3.0: Fast GPU-based Parallel Sparse LU Factorization for Circuit\n  Simulation", "comments": "10 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LU factorization for sparse matrices is the most important computing step for\nmany engineering and scientific computing problems such as circuit simulation.\nBut parallelizing LU factorization with the Graphic Processing Units (GPU)\nstill remains a challenging problem due to high data dependency and irregular\nmemory accesses. Recently GPU-based hybrid right-looking sparse LU solver,\ncalled GLU (1.0 and 2.0), has been proposed to exploit the fine grain level\nparallelism of GPU. However, a new type of data dependency (called double-U\ndependency) introduced by GLU slows down the preprocessing step. Furthermore,\nGLU uses fixed GPU thread allocation strategy, which limits the parallelism. In\nthis article, we propose a new GPU-based sparse LU factorization method, called\n{\\it GLU3.0}, which solves the aforementioned problems. First, it introduces a\nmuch more efficient data dependency detection algorithm. Second, we observe\nthat the potential parallelism is different as the matrix factorization goes\non. We then develop three different modes of GPU kernel which adapt to\ndifferent stages to accommodate the computing task changes in the\nfactorization. Experimental results on circuit matrices from University of\nFlorida Sparse Matrix Collection (UFL) show that GLU3.0 delivers 2-3 orders of\nmagnitude speedup over GLU2.0 for the data dependency detection. Furthermore,\nGLU3.0 achieve 13.0 $\\times$ (arithmetic mean) or 6.7$\\times$ (geometric mean)\nspeedup over GLU2.0 and 7.1$\\times$ (arithmetic mean) or 4.8 $\\times$\n(geometric mean) over the recently proposed enhanced GLU2.0 sparse LU solver on\nthe same set of circuit matrices.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 04:28:49 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 00:46:42 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 21:48:20 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Peng", "Shaoyi", ""], ["Tan", "Sheldon X. -D.", ""]]}, {"id": "1908.00213", "submitter": "Shunta Saito", "authors": "Seiya Tokui, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa,\n  Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, Hiroyuki Yamazaki\n  Vincent", "title": "Chainer: A Deep Learning Framework for Accelerating the Research Cycle", "comments": "Accepted for Applied Data Science Track in KDD'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software frameworks for neural networks play a key role in the development\nand application of deep learning methods. In this paper, we introduce the\nChainer framework, which intends to provide a flexible, intuitive, and high\nperformance means of implementing the full range of deep learning models needed\nby researchers and practitioners. Chainer provides acceleration using Graphics\nProcessing Units with a familiar NumPy-like API through CuPy, supports general\nand dynamic models in Python through Define-by-Run, and also provides add-on\npackages for state-of-the-art computer vision models as well as distributed\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 05:07:00 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Tokui", "Seiya", ""], ["Okuta", "Ryosuke", ""], ["Akiba", "Takuya", ""], ["Niitani", "Yusuke", ""], ["Ogawa", "Toru", ""], ["Saito", "Shunta", ""], ["Suzuki", "Shuji", ""], ["Uenishi", "Kota", ""], ["Vogel", "Brian", ""], ["Vincent", "Hiroyuki Yamazaki", ""]]}, {"id": "1908.00236", "submitter": "Hoa Vu", "authors": "Hsin-Hao Su, Hoa T. Vu", "title": "Distributed Data Summarization in Well-Connected Networks", "comments": "Conference version to appear at DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed algorithms for some fundamental problems in data\nsummarization. Given a communication graph $G$ of $n$ nodes each of which may\nhold a value initially, we focus on computing $\\sum_{i=1}^N g(f_i)$, where\n$f_i$ is the number of occurrences of value $i$ and $g$ is some fixed function.\nThis includes important statistics such as the number of distinct elements,\nfrequency moments, and the empirical entropy of the data.\n  In the CONGEST model, a simple adaptation from streaming lower bounds shows\nthat it requires $\\tilde{\\Omega}(D+ n)$ rounds, where $D$ is the diameter of\nthe graph, to compute some of these statistics exactly. However, these lower\nbounds do not hold for graphs that are well-connected. We give an algorithm\nthat computes $\\sum_{i=1}^{N} g(f_i)$ exactly in $\\tau_G \\cdot 2^{O(\\sqrt{\\log\nn})}$ rounds where $\\tau_G$ is the mixing time of $G$. This also has\napplications in computing the top $k$ most frequent elements.\n  We demonstrate that there is a high similarity between the GOSSIP model and\nthe CONGEST model in well-connected graphs. In particular, we show that each\nround of the GOSSIP model can be simulated almost-perfectly in\n$\\tilde{O}(\\tau_G $ rounds of the CONGEST model. To this end, we develop a new\nalgorithm for the GOSSIP model that $1\\pm \\epsilon$ approximates the $p$-th\nfrequency moment $F_p = \\sum_{i=1}^N f_i^p$ in $\\tilde{O}(\\epsilon^{-2}\nn^{1-k/p})$ rounds, for $p \\geq2$, when the number of distinct elements $F_0$\nis at most $O\\left(n^{1/(k-1)}\\right)$. This result can be translated back to\nthe CONGEST model with a factor $\\tilde{O}(\\tau_G)$ blow-up in the number of\nrounds.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 07:05:02 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 17:30:23 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Su", "Hsin-Hao", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1908.00338", "submitter": "Ioannis Christou Ph.D.", "authors": "Ioannis T. Christou", "title": "Popt4jlib: A Parallel/Distributed Optimization Library for Java", "comments": "9 pages, 4 figures, 1 table, accepted in IEEE CEC 2018 (but is not in\n  IEEE Xplore as did not present paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the architectural design as well as key implementation\ndetails of the Open Source popt4jlib library\n(https://githhub.org/ioannischristou/popt4jlib) that contains a fairly large\nnumber of meta-heuristic and other exact optimization algorithms\nparallel/distributed Java implementations. Although we report on speedup and\nefficiency issues on some of the algorithms in the library, our main concern is\nto detail the design decisions for the key parallel/distributed infrastructure\nbuilt into the library, so as to make it easier for developers to develop their\nown parallel implementations of the algorithms of their choice, rather than\nsimply using it as an off-the-self application library.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 11:33:39 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Christou", "Ioannis T.", ""]]}, {"id": "1908.00432", "submitter": "Georgia Avarikioti", "authors": "Georgia Avarikioti, Kenan Besic, Yuyi Wang, Roger Wattenhofer", "title": "Online Payment Network Design", "comments": "To appear in 3rd International Workshop on Cryptocurrencies and\n  Blockchain Technology (CBT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment channels allow transactions between participants of the blockchain to\nbe executed securely off-chain, and thus provide a promising solution for the\nscalability problem of popular blockchains. We study the online network design\nproblem for payment channels, assuming a central coordinator. We focus on a\nsingle channel, where the coordinator desires to maximize the number of\naccepted transactions under given capital constraints. Despite the simplicity\nof the problem, we present a flurry of impossibility results, both for\ndeterministic and randomized algorithms against adaptive as well as oblivious\nadversaries.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:29:43 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Avarikioti", "Georgia", ""], ["Besic", "Kenan", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1908.00600", "submitter": "Shahin Khobahi", "authors": "Shahin Khobahi, Mojtaba Soltanalian, Feng Jiang and A. Lee\n  Swindlehurst", "title": "Optimized Transmission for Parameter Estimation in Wireless Sensor\n  Networks", "comments": "Accepted for publication in IEEE Transactions on Signal and\n  Information Processing over Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in analog wireless sensor networks is to design the gain or\nphase-shifts of the sensor nodes (i.e. the relaying configuration) in order to\nachieve an accurate estimation of some parameter of interest at a fusion\ncenter, or more generally, at each node by employing a distributed parameter\nestimation scheme. In this paper, by using an over-parametrization of the\noriginal design problem, we devise a cyclic optimization approach that can\nhandle tuning both gains and phase-shifts of the sensor nodes, even in\nintricate scenarios involving sensor selection or discrete phase-shifts. Each\niteration of the proposed design framework consists of a combination of the\nGram-Schmidt process and power method-like iterations, and as a result, enjoys\na low computational cost. Along with formulating the design problem for a\nfusion center, we further present a consensus-based framework for decentralized\nestimation of deterministic parameters in a distributed network, which results\nin a similar sensor gain design problem. The numerical results confirm the\ncomputational advantage of the suggested approach in comparison with the\nstate-of-the-art methods---an advantage that becomes more pronounced when the\nsensor network grows large.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 19:57:56 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Khobahi", "Shahin", ""], ["Soltanalian", "Mojtaba", ""], ["Jiang", "Feng", ""], ["Swindlehurst", "A. Lee", ""]]}, {"id": "1908.00741", "submitter": "Takeshi Iwashita", "authors": "Takeshi Iwashita, Senxi Li, Takeshi Fukaya", "title": "Hierarchical Block Multi-Color Ordering: A New Parallel Ordering Method\n  for Vectorization and Parallelization of the Sparse Triangular Solver in the\n  ICCG Method", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new parallel ordering method to vectorize and\nparallelize the sparse triangular solver, which is called hierarchical block\nmulti-color ordering. In this method, the parallel forward and backward\nsubstitutions can be vectorized while preserving the advantages of block\nmulti-color ordering, that is, fast convergence and fewer thread\nsynchronizations. To evaluate the proposed method in a parallel ICCG\n(Incomplete Cholesky Conjugate Gradient) solver, numerical tests were conducted\nusing five test matrices on three types of computational nodes. The numerical\nresults indicate that the proposed method outperforms the conventional block\nand nodal multi-color ordering methods in 13 out of 15 test cases, which\nconfirms the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 08:03:33 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Iwashita", "Takeshi", ""], ["Li", "Senxi", ""], ["Fukaya", "Takeshi", ""]]}, {"id": "1908.01009", "submitter": "Xiangju Qin", "authors": "Xiangju Qin, Paul Blomstedt and Samuel Kaski", "title": "Scalable Bayesian Non-linear Matrix Completion", "comments": "7 pages, 1 figures, 2 tables. The paper has been accepted for\n  publication in the proceedings of the 28th International Joint Conference on\n  Artificial Intelligence (IJCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion aims to predict missing elements in a partially observed\ndata matrix which in typical applications, such as collaborative filtering, is\nlarge and extremely sparsely observed. A standard solution is matrix\nfactorization, which predicts unobserved entries as linear combinations of\nlatent variables. We generalize to non-linear combinations in massive-scale\nmatrices. Bayesian approaches have been proven beneficial in linear matrix\ncompletion, but not applied in the more general non-linear case, due to limited\nscalability. We introduce a Bayesian non-linear matrix completion algorithm,\nwhich is based on a recent Bayesian formulation of Gaussian process latent\nvariable models. To solve the challenges regarding scalability and computation,\nwe propose a data-parallel distributed computational approach with a restricted\ncommunication scheme. We evaluate our method on challenging out-of-matrix\nprediction tasks using both simulated and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 16:48:31 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Qin", "Xiangju", ""], ["Blomstedt", "Paul", ""], ["Kaski", "Samuel", ""]]}, {"id": "1908.01059", "submitter": "Xin Wang", "authors": "Xin Wang, Hideaki Ishii, Linkang Du, Peng Cheng, Jiming Chen", "title": "Privacy-preserving Distributed Machine Learning via Local Randomization\n  and ADMM Perturbation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.3009007", "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.MA cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of training data, distributed machine learning (DML)\nis becoming more competent for large-scale learning tasks. However, privacy\nconcerns have to be given priority in DML, since training data may contain\nsensitive information of users. In this paper, we propose a privacy-preserving\nADMM-based DML framework with two novel features: First, we remove the\nassumption commonly made in the literature that the users trust the server\ncollecting their data. Second, the framework provides heterogeneous privacy for\nusers depending on data's sensitive levels and servers' trust degrees. The\nchallenging issue is to keep the accumulation of privacy losses over ADMM\niterations minimal. In the proposed framework, a local randomization approach,\nwhich is differentially private, is adopted to provide users with\nself-controlled privacy guarantee for the most sensitive information. Further,\nthe ADMM algorithm is perturbed through a combined noise-adding method, which\nsimultaneously preserves privacy for users' less sensitive information and\nstrengthens the privacy protection of the most sensitive information. We\nprovide detailed analyses on the performance of the trained model according to\nits generalization error. Finally, we conduct extensive experiments using\nreal-world datasets to validate the theoretical results and evaluate the\nclassification performance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 06:31:16 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 07:47:40 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Wang", "Xin", ""], ["Ishii", "Hideaki", ""], ["Du", "Linkang", ""], ["Cheng", "Peng", ""], ["Chen", "Jiming", ""]]}, {"id": "1908.01153", "submitter": "Narges Mehran", "authors": "Narges Mehran, Dragi Kimovski, Radu Prodan", "title": "MAPO: A Multi-Objective Model for IoT Application Placement in a Fog\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of the Fog computing paradigm that leverages in-network\nvirtualized resources raises important challenges in terms of resource and IoT\napplication management in a heterogeneous environment offering only limited\ncomputing resources. In this work, we propose a novel Pareto-based approach for\napplication placement close to the data sources called Multiobjective IoT\napplication Placement in fOg (MAPO). MAPO models applications based on a finite\nstate machine and uses three conflicting optimization objectives, namely\ncompletion time, energy consumption, and economic cost, considering both the\ncomputation and communication aspects. In contrast to existing solutions that\noptimize a single objective value, MAPO enables multi-objective energy and\ncost-aware application placement. To evaluate the quality of the MAPO\nplacements, we created both simulated and real-world testbeds tailored for a\nset of medical IoT application case studies. Compared to the state-of-the-art\napproaches, MAPO reduces the economic cost by up to 27%, while decreasing the\nenergy requirements by 23-68%, and optimizes the completion time by up to 7.3\ntimes.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 11:54:29 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Mehran", "Narges", ""], ["Kimovski", "Dragi", ""], ["Prodan", "Radu", ""]]}, {"id": "1908.01338", "submitter": "Sudhakar Singh", "authors": "Pankaj Singh, Sudhakar Singh, P. K. Mishra, Rakhi Garg", "title": "A Data Structure Perspective to the RDD-based Apriori Algorithm on Spark", "comments": "14 pages", "journal-ref": "International Journal of Information Technology, Springer, 2019", "doi": "10.1007/s41870-019-00337-3", "report-no": "BJIT-D-18-00717", "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the recent years, a number of efficient and scalable frequent itemset\nmining algorithms for big data analytics have been proposed by many\nresearchers. Initially, MapReduce-based frequent itemset mining algorithms on\nHadoop cluster were proposed. Although, Hadoop has been developed as a cluster\ncomputing system for handling and processing big data, but the performance of\nHadoop does not meet the expectation for the iterative algorithms of data\nmining, due to its high I/O, and writing and then reading intermediate results\nin the disk. Consequently, Spark has been developed as another cluster\ncomputing infrastructure which is much faster than Hadoop due to its in-memory\ncomputation. It is highly suitable for iterative algorithms and supports batch,\ninteractive, iterative, and stream processing of data. Many frequent itemset\nmining algorithms have been re-designed on the Spark, and most of them are\nApriori-based. All these Spark-based Apriori algorithms use Hash Tree as the\nunderlying data structure. This paper investigates the efficiency of various\ndata structures for the Spark-based Apriori. Although, the data structure\nperspective has been investigated previously, but for MapReduce-based Apriori,\nand it must be re-investigated in the distributed computing environment of\nSpark. The considered underlying data structures are Hash Tree, Trie, and Hash\nTable Trie. The experimental results on the benchmark datasets show that the\nperformance of Spark-based Apriori with Trie and Hash Table Trie are almost\nsimilar but both perform many times better than Hash Tree in the distributed\ncomputing environment of Spark.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 13:13:28 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Singh", "Pankaj", ""], ["Singh", "Sudhakar", ""], ["Mishra", "P. K.", ""], ["Garg", "Rakhi", ""]]}, {"id": "1908.01407", "submitter": "Carl Yang", "authors": "Carl Yang, Aydin Buluc, John D. Owens", "title": "GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on\n  the GPU", "comments": "50 pages, 14 figures, 14 tables, to appear in ACM Transactions on\n  Mathematical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance implementations of graph algorithms are challenging to\nimplement on new parallel hardware such as GPUs because of three challenges:\n(1) the difficulty of coming up with graph building blocks, (2) load imbalance\non parallel hardware, and (3) graph problems having low arithmetic intensity.\nTo address some of these challenges, GraphBLAS is an innovative, on-going\neffort by the graph analytics community to propose building blocks based on\nsparse linear algebra, which will allow graph algorithms to be expressed in a\nperformant, succinct, composable and portable manner. In this paper, we examine\nthe performance challenges of a linear-algebra-based approach to building graph\nframeworks and describe new design principles for overcoming these bottlenecks.\nAmong the new design principles is exploiting input sparsity, which allows\nusers to write graph algorithms without specifying push and pull direction.\nExploiting output sparsity allows users to tell the backend which values of the\noutput in a single vectorized computation they do not want computed.\nLoad-balancing is an important feature for balancing work amongst parallel\nworkers. We describe the important load-balancing features for handling graphs\nwith different characteristics. The design principles described in this paper\nhave been implemented in \"GraphBLAST\", the first high-performance linear\nalgebra-based graph framework on NVIDIA GPUs that is open-source. The results\nshow that on a single GPU, GraphBLAST has on average at least an order of\nmagnitude speedup over previous GraphBLAS implementations SuiteSparse and GBTL,\ncomparable performance to the fastest GPU hardwired primitives and\nshared-memory graph frameworks Ligra and Gunrock, and better performance than\nany other GPU graph framework, while offering a simpler and more concise\nprogramming model.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 21:54:05 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 17:09:39 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 09:00:37 GMT"}, {"version": "v4", "created": "Sat, 14 Nov 2020 07:32:47 GMT"}, {"version": "v5", "created": "Tue, 15 Jun 2021 03:29:49 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Yang", "Carl", ""], ["Buluc", "Aydin", ""], ["Owens", "John D.", ""]]}, {"id": "1908.01455", "submitter": "Jelle Hellings", "authors": "Jelle Hellings and Mohammad Sadoghi", "title": "The fault-tolerant cluster-sending problem", "comments": "A brief announcement of this work will be presented at the 33rd\n  International Symposium on Distributed Computing (DISC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of fault-tolerant distributed systems that can tolerate\nByzantine behavior has traditionally been focused on consensus protocols, which\nsupport fully-replicated designs. For the development of more sophisticated\nhigh-performance Byzantine distributed systems, more specialized fault-tolerant\ncommunication primitives are necessary, however.\n  In this paper, we identify an essential communication primitive and study it\nin depth. In specifics, we formalize the cluster-sending problem, the problem\nof sending a message from one Byzantine cluster to another Byzantine cluster in\na reliable manner. We not only formalize this fundamental problem, but also\nestablish lower bounds on the complexity of this problem under crash failures\nand Byzantine failures. Furthermore, we develop practical cluster-sending\nprotocols that meet these lower bounds and, hence, have optimal complexity. As\nsuch, our work provides a strong foundation for the further exploration of\nnovel designs that address challenges encountered in fault-tolerant distributed\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 03:36:22 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Hellings", "Jelle", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1908.01458", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Jelle Hellings, Mohammad Sadoghi", "title": "Revisiting consensus protocols through wait-free parallelization", "comments": "Brief Announcement at DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent surge of blockchain systems has renewed the interest in\ntraditional Byzantine fault-tolerant consensus protocols. Many such consensus\nprotocols have a primary-backup design in which an assigned replica, the\nprimary, is responsible for coordinating the consensus protocol. Although the\nprimary-backup design leads to relatively simple and high performance consensus\nprotocols, it places an unreasonable burden on a good primary and allows\nmalicious primaries to substantially affect the system performance. In this\npaper, we propose a protocol-agnostic approach to improve the design of primary\nbackup consensus protocols. At the core of our approach is a novel wait-free\napproach of running several instances of the underlying consensus protocol in\nparallel. To yield a high performance parallelized design, we present\ncoordination-free techniques to order operations across parallel instances,\ndeal with instance failures, and assign clients to specific instances.\nConsequently, the design we present is able to reduce the load on individual\ninstances and primaries, while also reducing the adverse effects of any\nmalicious replicas.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 04:07:21 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 02:14:40 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Gupta", "Suyash", ""], ["Hellings", "Jelle", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1908.01526", "submitter": "Alessandro Di Stefano", "authors": "Andrea Araldo, Alessandro Di Stefano and Antonella Di Stefano", "title": "EdgeMORE: Improving Resource Allocation with Multiple Options from\n  Tenants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Under the paradigm of Edge Computing (EC), a Network Operator (NO) deploys\ncomputational resources at the network edge and let third-party Service\nProviders (SPs) run on top of them, as tenants. Besides the clear advantages\nfor SPs and final users thanks to the vicinity of computation nodes, a NO aims\nto allocate edge resources in order to increase its own utility, including\nbandwidth saving, operational cost reduction, QoE for its users, etc. However,\nwhile the number of third-party services competing for edge resources is\nexpected to dramatically grow, the resources deployed cannot increase\naccordingly, due to physical limitations. Therefore, smart strategies are\nneeded to fully exploit the potential of EC, despite its constrains.\n  To this aim, we propose to leverage service adaptability, a dimension that\nhas mainly been neglected so far: each service can adapt to the amount of\nresources that the NO has allocated to it, balancing the fraction of service\ncomputation performed at the edge and relying on remote servers, e.g., in the\nCloud, for the rest. We propose EdgeMORE, a resource allocation strategy in\nwhich SPs express their capabilities to adapt to different resource\nconstraints, by declaring the different configurations under which they are\nable to run, specifying the resources needed and the utility provided to the\nNO. The NO then chooses the most convenient option per each SP, in order to\nmaximize the total utility.\n  We formalize EdgeMORE as a Integer Linear Program. We show via simulation\nthat EdgeMORE greatly improves EC utility with respect to the standard where no\nmultiple options for running services are allowed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 09:13:36 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 11:14:22 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 18:46:25 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Araldo", "Andrea", ""], ["Di Stefano", "Alessandro", ""], ["Di Stefano", "Antonella", ""]]}, {"id": "1908.01527", "submitter": "Patrick P. C. Lee", "authors": "Xiaolu Li, Zuoru Yang, Jinhong Li, Runhui Li, Patrick P. C. Lee, Qun\n  Huang, Yuchong Hu", "title": "Repair Pipelining for Erasure-Coded Storage: Algorithms and Evaluation", "comments": "28 pages. Accepted by ACM Transactions on Storage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose repair pipelining, a technique that speeds up the repair\nperformance in general erasure-coded storage. By carefully scheduling the\nrepair of failed data in small-size units across storage nodes in a pipelined\nmanner, repair pipelining reduces the single-block repair time to approximately\nthe same as the normal read time for a single block in homogeneous\nenvironments. We further design different extensions of repair pipelining\nalgorithms for heterogeneous environments and multi-block repair operations. We\nimplement a repair pipelining prototype, called ECPipe, and integrate it as a\nmiddleware system into two versions of Hadoop Distributed File System (HDFS)\n(namely HDFS-RAID and HDFS-3) as well as Quantcast File System (QFS).\nExperiments on a local testbed and Amazon EC2 show that repair pipelining\nsignificantly improves the performance of degraded reads and full-node recovery\nover existing repair techniques.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 09:13:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:28:50 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 08:22:31 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Li", "Xiaolu", ""], ["Yang", "Zuoru", ""], ["Li", "Jinhong", ""], ["Li", "Runhui", ""], ["Lee", "Patrick P. C.", ""], ["Huang", "Qun", ""], ["Hu", "Yuchong", ""]]}, {"id": "1908.01554", "submitter": "Minh Duc Nguyen", "authors": "Minh-Duc Nguyen, Alexander Kryukov, Julia Dubenskaya, Elena\n  Korosteleva, Igor Bychkov, Andrey Mikhailov, and Alexey Shigarov", "title": "Data Aggregation In The Astroparticle Physics Distributed Data Storage", "comments": "6 pages, 2 figures, Proceedings of the 3rd International Workshop on\n  Data Life Cycle in Physics (Irkutsk, Russia, April 2-7, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  German-Russian Astroparticle Data Life Cycle Initiative is an international\nproject whose aim is to develop a distributed data storage system that\naggregates data from the storage systems of different astroparticle\nexperiments. The prototype of such a system, which is called the Astroparticle\nPhysics Distributed Data Storage (APPDS), has been being developed. In this\npaper, the Data Aggregation Service, one of the core services of APDDS, is\npresented. The Data Aggregation Service connects all distributed services of\nAPPDS together to find the necessary data and deliver them to users on demand.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 10:41:27 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Nguyen", "Minh-Duc", ""], ["Kryukov", "Alexander", ""], ["Dubenskaya", "Julia", ""], ["Korosteleva", "Elena", ""], ["Bychkov", "Igor", ""], ["Mikhailov", "Andrey", ""], ["Shigarov", "Alexey", ""]]}, {"id": "1908.01656", "submitter": "Simone Disabato", "authors": "Simone Disabato, Manuel Roveri, Cesare Alippi", "title": "Distributed Deep Convolutional Neural Networks for the\n  Internet-of-Things", "comments": null, "journal-ref": "in IEEE Transactions on Computers, vol. 70, no. 8, pp. 1239-1252,\n  1 Aug. 2021", "doi": "10.1109/TC.2021.3062227", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Severe constraints on memory and computation characterizing the\nInternet-of-Things (IoT) units may prevent the execution of Deep Learning\n(DL)-based solutions, which typically demand large memory and high processing\nload. In order to support a real-time execution of the considered DL model at\nthe IoT unit level, DL solutions must be designed having in mind constraints on\nmemory and processing capability exposed by the chosen IoT technology. In this\npaper, we introduce a design methodology aiming at allocating the execution of\nConvolutional Neural Networks (CNNs) on a distributed IoT application. Such a\nmethodology is formalized as an optimization problem where the latency between\nthe data-gathering phase and the subsequent decision-making one is minimized,\nwithin the given constraints on memory and processing load at the units level.\nThe methodology supports multiple sources of data as well as multiple CNNs in\nexecution on the same IoT system allowing the design of CNN-based applications\ndemanding autonomy, low decision-latency, and high Quality-of-Service.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 12:19:52 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 18:41:23 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Disabato", "Simone", ""], ["Roveri", "Manuel", ""], ["Alippi", "Cesare", ""]]}, {"id": "1908.01738", "submitter": "Dragos-Adrian (Adi) Seredinschi PhD", "authors": "Rachid Guerraoui and Petr Kuznetsov and Matteo Monti and Matej\n  Pavlovic and Dragos-Adrian Seredinschi and Yann Vonlanthen", "title": "Scalable Byzantine Reliable Broadcast (Extended Version)", "comments": "This is an extended version of a conference article, appearing (best\n  paper award) in the proceedings of the 33rd International Symposium on\n  Distributed Computing (DISC 2019), October 14--18, 2019, Budapest, Hungary", "journal-ref": null, "doi": "10.4230/LIPIcs.DISC.2019.22", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine reliable broadcast is a powerful primitive that allows a set of\nprocesses to agree on a message from a designated sender, even if some\nprocesses (including the sender) are Byzantine. Existing broadcast protocols\nfor this setting scale poorly, as they typically build on quorum systems with\nstrong intersection guarantees, which results in linear per-process\ncommunication and computation complexity.\n  We generalize the Byzantine reliable broadcast abstraction to the\nprobabilistic setting, allowing each of its properties to be violated with a\nfixed, arbitrarily small probability. We leverage these relaxed guarantees in a\nprotocol where we replace quorums with stochastic samples. Compared to quorums,\nsamples are significantly smaller in size, leading to a more scalable design.\nWe obtain the first Byzantine reliable broadcast protocol with logarithmic\nper-process communication and computation complexity.\n  We conduct a complete and thorough analysis of our protocol, deriving bounds\non the probability of each of its properties being compromised. During our\nanalysis, we introduce a novel general technique we call adversary decorators.\nAdversary decorators allow us to make claims about the optimal strategy of the\nByzantine adversary without having to make any additional assumptions. We also\nintroduce Threshold Contagion, a model of message propagation through a system\nwith Byzantine processes. To the best of our knowledge, this is the first\nformal analysis of a probabilistic broadcast protocol in the Byzantine fault\nmodel. We show numerically that practically negligible failure probabilities\ncan be achieved with realistic security parameters.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 17:30:00 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 20:36:21 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 19:50:03 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Kuznetsov", "Petr", ""], ["Monti", "Matteo", ""], ["Pavlovic", "Matej", ""], ["Seredinschi", "Dragos-Adrian", ""], ["Vonlanthen", "Yann", ""]]}, {"id": "1908.01894", "submitter": "Alex Weaver", "authors": "Michael Dinitz, Magn\\'us M. Halld\\'orsson, Calvin Newport, and Alex\n  Weaver", "title": "The Capacity of Smartphone Peer-to-Peer Networks", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.DISC.2019.14", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three capacity problems in the mobile telephone model, a network\nabstraction that models the peer-to-peer communication capabilities implemented\nin most commodity smartphone operating systems. The capacity of a network\nexpresses how much sustained throughput can be maintained for a set of\ncommunication demands, and is therefore a fundamental bound on the usefulness\nof a network. Because of this importance, wireless network capacity has been\nactive area of research for the last two decades.\n  The three capacity problems that we study differ in the structure of the\ncommunication demands. The first problem is pairwise capacity, where the\ndemands are (source, destination) pairs. Pairwise capacity is one of the most\nclassical definitions, as it was analyzed in the seminal paper of Gupta and\nKumar on wireless network capacity. The second problem we study is broadcast\ncapacity, in which a single source must deliver packets to all other nodes in\nthe network. Finally, we turn our attention to all-to-all capacity, in which\nall nodes must deliver packets to all other nodes. In all three of these\nproblems we characterize the optimal achievable throughput for any given\nnetwork, and design algorithms which asymptotically match this performance. We\nalso study these problems in networks generated randomly by a process\nintroduced by Gupta and Kumar, and fully characterize their achievable\nthroughput.\n  Interestingly, the techniques that we develop for all-to-all capacity also\nallow us to design a one-shot gossip algorithm that runs within a\npolylogarithmic factor of optimal in every graph. This largely resolves an open\nquestion from previous work on the one-shot gossip problem in this model.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 22:52:48 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 18:27:11 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Dinitz", "Michael", ""], ["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Newport", "Calvin", ""], ["Weaver", "Alex", ""]]}, {"id": "1908.01915", "submitter": "Naoki Shibata", "authors": "Naoki Shibata", "title": "Proof-of-Search: Combining Blockchain Consensus Formation with Solving\n  Optimization Problems", "comments": "in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2956698", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the large amount of energy wasted by blockchains, we propose a\ndecentralized consensus protocol for blockchains in which the computation can\nbe used to search for good approximate solutions to any optimization problem.\nOur protocol allows the wasted energy to be used for finding approximate\nsolutions to problems submitted by any nodes~(called clients). Our protocol\nworks in a similar way to proof-of-work, and it makes nodes evaluate a large\nnumber of solution candidates to add a new block to the chain. A client\nprovides a search program that implements any search algorithm that finds a\ngood solution by evaluating a large number of solution candidates. The node\nthat finds the best approximate solution is rewarded by the client. Our\nanalysis shows that the probability of a fork and the variance in the block\ntime with our protocol are lower than those in proof-of-work.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 01:00:20 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 03:01:33 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Shibata", "Naoki", ""]]}, {"id": "1908.01924", "submitter": "Tianshu Hao", "authors": "Tianshu Hao, Yunyou Huang, Xu Wen, Wanling Gao, Fan Zhang, Chen Zheng,\n  Lei Wang, Hainan Ye, Kai Hwang, Zujie Ren, and Jianfeng Zhan", "title": "Edge AIBench: Towards Comprehensive End-to-end Edge Computing\n  Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In edge computing scenarios, the distribution of data and collaboration of\nworkloads on different layers are serious concerns for performance, privacy,\nand security issues. So for edge computing benchmarking, we must take an\nend-to-end view, considering all three layers: client-side devices, edge\ncomputing layer, and cloud servers. Unfortunately, the previous work ignores\nthis most important point. This paper presents the BenchCouncil's coordinated e\nort on edge AI benchmarks, named Edge AIBench. In total, Edge AIBench models\nfour typical application scenarios: ICU Patient Monitor, Surveillance Camera,\nSmart Home, and Autonomous Vehicle with the focus on data distribution and\nworkload collaboration on three layers. Edge AIBench is a part of the\nopen-source AIBench project, publicly available from\nhttp://www.benchcouncil.org/AIBench/index.html. We also build an edge computing\ntestbed with a federated learning framework to resolve performance, privacy,\nand security issues.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 01:34:51 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Hao", "Tianshu", ""], ["Huang", "Yunyou", ""], ["Wen", "Xu", ""], ["Gao", "Wanling", ""], ["Zhang", "Fan", ""], ["Zheng", "Chen", ""], ["Wang", "Lei", ""], ["Ye", "Hainan", ""], ["Hwang", "Kai", ""], ["Ren", "Zujie", ""], ["Zhan", "Jianfeng", ""]]}, {"id": "1908.02063", "submitter": "Matthieu Perrin", "authors": "Gr\\'egoire Bonin, Achour Most\\'efaoui and Matthieu Perrin", "title": "Wait-Free Universality of Consensus in the Infinite Arrival Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical asynchronous distributed systems composed of a fixed number n of\nprocesses where some proportion may fail by crashing, many objects do not have\na wait-free linearizable implementation (e.g. stacks, queues, etc.). It has\nbeen proved that consensus is universal in such systems, which means that this\nsystem augmented with consensus objects allows to implement any object that has\na sequential specification. To this end, many universal constructions have been\nproposed in systems augmented with consensus objects or with different\nequivalent objects or special hardware instructions (compare&swap, fetch&add,\netc.). In this paper, we consider a more general system model called infinite\narrival model where infinitely many processes may arrive and leave or crash\nduring a run. We prove that consensus is still universal in this more general\nmodel. For that, we propose a universal construction. As a first step we build\na weak log for which we propose two implementations using consensus objects for\nthe first and the compare&swap special instruction for the other.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 10:33:59 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Bonin", "Gr\u00e9goire", ""], ["Most\u00e9faoui", "Achour", ""], ["Perrin", "Matthieu", ""]]}, {"id": "1908.02135", "submitter": "Takahiro Hirofuchi", "authors": "Atsushi Koshiba and Takahiro Hirofuchi and Ryousei Takano and Mitaro\n  Namiki", "title": "A Software-based NVM Emulator Supporting Read/Write Asymmetric Latencies", "comments": "To appear in IEICE Transactions on Information and Systems, December,\n  2019", "journal-ref": null, "doi": "10.1587/transinf.2019PAP0018", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) is a promising technology for low-energy and\nhigh-capacity main memory of computers. The characteristics of NVM devices,\nhowever, tend to be fundamentally different from those of DRAM (i.e., the\nmemory device currently used for main memory), because of differences in\nprinciples of memory cells. Typically, the write latency of an NVM device such\nas PCM and ReRAM is much higher than its read latency. The asymmetry in\nread/write latencies likely affects the performance of applications\nsignificantly. For analyzing behavior of applications running on NVM-based main\nmemory, most researchers use software-based emulation tools due to the limited\nnumber of commercial NVM products. However, these existing emulation tools are\ntoo slow to emulate a large-scale, realistic workload or too simplistic to\ninvestigate the details of application behavior on NVM with asymmetric\nread/write latencies. This paper therefore proposes a new NVM emulation\nmechanism that is not only light-weight but also aware of a read/write latency\ngap in NVM-based main memory. We implemented the prototype of the proposed\nmechanism for the Intel CPU processors of the Haswell architecture. We also\nevaluated its accuracy and performed case studies for practical benchmarks. The\nresults showed that our prototype accurately emulated write-latencies of\nNVM-based main memory: it emulated the NVM write latencies in a range from 200\nns to 1000 ns with negligible errors from 0.2% to 1.1%. We confirmed that the\nuse of our emulator enabled us to successfully estimate performance of\npractical workloads for NVM-based main memory, while an existing light-weight\nemulation model misestimated.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 04:37:07 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Koshiba", "Atsushi", ""], ["Hirofuchi", "Takahiro", ""], ["Takano", "Ryousei", ""], ["Namiki", "Mitaro", ""]]}, {"id": "1908.02136", "submitter": "Maliheh Heydarpour", "authors": "Maliheh Heydarpour Shahrezaei and Reza Tavoli", "title": "Parallelization of Kmeans++ using CUDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means++ is an algorithm which is invented to improve the process of finding\ninitial seeds in K-means algorithm. In this algorithm, initial seeds are chosen\nconsecutively by a probability which is proportional to the distance to the\nnearest center. The most crucial problem of this algorithm is that when running\nin serial mode, it decreases the speed of clustering. In this paper, we aim to\nparallelize the most time consuming steps of the k-means++ algorithm. Our\npurpose is to reduce the running time while maintaining the quality of the\nserial algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 22:37:21 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Shahrezaei", "Maliheh Heydarpour", ""], ["Tavoli", "Reza", ""]]}, {"id": "1908.02149", "submitter": "Amir Nakib", "authors": "Leo Souquet, Amir Nakib", "title": "Multi-node environment strategy for Parallel Deterministic\n  Multi-Objective Fractal Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new implementation of deterministic multiobjective (MO)\noptimization called Multiobjective Fractal Decomposition Algorithm (Mo-FDA).\nThe original algorithm was designed for mono-objective large scale continuous\noptimization problems. It is based on a divide and conquer strategy and a\ngeometric fractal decomposition of the search space using hyperspheres. Then,\nto deal with MO problems a scalarization approach is used. In this work, a new\napproach has been developed on a multi-node environment using containers. The\nperformance of Mo-FDA was compared to state of the art algorithms from the\nliterature on classical benchmark of multi-objective optimization\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 00:58:12 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Souquet", "Leo", ""], ["Nakib", "Amir", ""]]}, {"id": "1908.02238", "submitter": "Tosiron Adegbija", "authors": "Keeley Criswell and Tosiron Adegbija", "title": "A Survey of Phase Classification Techniques for Characterizing Variable\n  Application Behavior", "comments": "To appear in IEEE Transactions on Parallel and Distributed Systems\n  (TPDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptable computing is an increasingly important paradigm that specializes\nsystem resources to variable application requirements, environmental\nconditions, or user requirements. Adapting computing resources to variable\napplication requirements (or application phases) is otherwise known as\nphase-based optimization. Phase-based optimization takes advantage of\napplication phases, or execution intervals of an application, that behave\nsimilarly, to enable effective and beneficial adaptability. In order for\nphase-based optimization to be effective, the phases must first be classified\nto determine when application phases begin and end, and ensure that system\nresources are accurately specialized. In this paper, we present a survey of\nphase classification techniques that have been proposed to exploit the\nadvantages of adaptable computing through phase-based optimization. We focus on\nrecent techniques and classify these techniques with respect to several factors\nin order to highlight their similarities and differences. We divide the\ntechniques by their major defining characteristics---online/offline and\nserial/parallel. In addition, we discuss other characteristics such as\nprediction and detection techniques, the characteristics used for prediction,\ninterval type, etc. We also identify gaps in the state-of-the-art and discuss\nfuture research directions to enable and fully exploit the benefits of\nadaptable computing.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 23:37:14 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Criswell", "Keeley", ""], ["Adegbija", "Tosiron", ""]]}, {"id": "1908.02239", "submitter": "Rawan Naous", "authors": "Rawan Naous, Lazar Supic, Yoonhwan Kang, Ranko Sredojevic, Anish\n  Singhani, and Vladimir Stojanovic", "title": "Tuning Algorithms and Generators for Efficient Edge Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A surge in artificial intelligence and autonomous technologies have increased\nthe demand toward enhanced edge-processing capabilities. Computational\ncomplexity and size of state-of-the-art Deep Neural Networks (DNNs) are rising\nexponentially with diverse network models and larger datasets. This growth\nlimits the performance scaling and energy-efficiency of both distributed and\nembedded inference platforms. Embedded designs at the edge are constrained by\nenergy and speed limitations of available processor substrates and processor to\nmemory communication required to fetch the model coefficients. While many\nhardware accelerator and network deployment frameworks have been in\ndevelopment, a framework is needed to allow the variety of existing\narchitectures, and those in development, to be expressed in critical parts of\nthe flow that perform various optimization steps. Moreover, premature\narchitecture-blind network selection and optimization diminish the\neffectiveness of schedule optimizations and hardware-specific mappings. In this\npaper, we address these issues by creating a cross-layer software-hardware\ndesign framework that encompasses network training and model compression that\nis aware of and tuned to the underlying hardware architecture. This approach\nleverages the available degrees of DNN structure and sparsity to create a\nconverged network that can be partitioned and efficiently scheduled on the\ntarget hardware platform, minimizing data movement, and improving the overall\nthroughput and energy. To further streamline the design, we leverage the\nhigh-level, flexible SoC generator platform based on RISC-V ROCC framework.\nThis integration allows seamless extensions of the RISC-V instruction set and\nChisel-based rapid generator design. Utilizing this approach, we implemented a\nsilicon prototype in a 16 nm TSMC process node achieving record processing\nefficiency of up to 18 TOPS/W.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 02:52:52 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 20:22:12 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Naous", "Rawan", ""], ["Supic", "Lazar", ""], ["Kang", "Yoonhwan", ""], ["Sredojevic", "Ranko", ""], ["Singhani", "Anish", ""], ["Stojanovic", "Vladimir", ""]]}, {"id": "1908.02244", "submitter": "Andri Pranolo", "authors": "Roman Voliansky, Andri Pranolo", "title": "Parallel mathematical models of dynamic objects", "comments": null, "journal-ref": "International Journal of Advances in Intelligent Informatics,\n  4(2), 120-131 (2019)", "doi": "10.26555/ijain.v4i2.229", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The paper deals with the developing of the methodological backgrounds for the\nmodeling and simulation of complex dynamical objects. Such backgrounds allow us\nto perform coordinate transformation and formulate the algorithm of its usage\nfor transforming the serial mathematical model into parallel ones. This\nalgorithm is based on partial fraction decomposition of the transfer function\nof a dynamic object. Usage of proposed algorithms is one of the ways to\ndecrease calculation time and improve PC usage while a simulation is being\nperformed. We prove our approach by considering the example of modeling and\nsimulating of fourth order dynamical object with various eigenvalues. This\nexample shows that developed parallel model is stable, well-convergent, and\nhigh-accuracy model. There is no defined any calculation errors between\nwell-known serial model and proposed parallel one. Nevertheless, the proposed\napproach's usage allows us to reduce calculation time by more than 20% by using\nseveral CPU's cores while calculations are being performed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 16:04:32 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Voliansky", "Roman", ""], ["Pranolo", "Andri", ""]]}, {"id": "1908.02280", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "The performance wall of parallelized sequential computing: the dark\n  performance and the roofline of performance gain", "comments": "15 page, 8 figures. arXiv admin note: text overlap with\n  arXiv:1808.05338", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computing performance today is developing mainly using parallelized\nsequential computing, in many forms. The paper scrutinizes whether the\nperformance of that type of computing has an upper limit. The simple\nconsiderations point out that the theoretically possible upper bound is\npractically achieved, and that the main obstacle to step further is the\npresently used computing paradigm and implementation technology. In addition to\nthe former \"walls\", also the \"performance wall\" must be considered. As the\npaper points out, similarly to the \"dark silicon\", also the \"dark performance\"\nis always present in the parallelized many-processor systems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 19:33:18 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1908.02287", "submitter": "Visara Urovi", "authors": "Andine Havelange, Michel Dumontier, Birgit Wouters, Jona Linde, David\n  Townend, Arno Riedl and Visara Urovi", "title": "LUCE: A Blockchain Solution for monitoring data License accoUntability\n  and CompliancE", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our preliminary work on monitoring data License\naccoUntability and CompliancE (LUCE). LUCE is a blockchain platform solution\ndesigned to stimulate data sharing and reuse, by facilitating compliance with\nlicensing terms. The platform enables data accountability by recording the use\nof data and their purpose on a blockchain-supported platform. LUCE allows for\nindividual data to be rectified and erased. In doing so LUCE can ensure\nsubjects' General Data Protection Regulation's (GDPR) rights to access,\nrectification and erasure. Our contribution is to provide a distributed\nsolution for the automatic management of data accountability and their license\nterms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 10:40:19 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Havelange", "Andine", ""], ["Dumontier", "Michel", ""], ["Wouters", "Birgit", ""], ["Linde", "Jona", ""], ["Townend", "David", ""], ["Riedl", "Arno", ""], ["Urovi", "Visara", ""]]}, {"id": "1908.02335", "submitter": "Martin Thomas Horsch", "authors": "M. T. Horsch and C. Niethammer and G. Boccardo and P. Carbone and S.\n  Chiacchiera and M. Chiricotto and J. D. Elliott and V. Lobaskin and P.\n  Neumann and P. Schiffels and M. A. Seaton and I. T. Todorov and J. Vrabec and\n  W. L. Cavalcanti", "title": "Semantic interoperability and characterization of data provenance in\n  computational molecular engineering", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jced.9b00739", "report-no": null, "categories": "cs.DC cs.CE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By introducing a common representational system for metadata that describe\nthe employed simulation workflows, diverse sources of data and platforms in\ncomputational molecular engineering, such as workflow management systems, can\nbecome interoperable at the semantic level. To achieve semantic\ninteroperability, the present work introduces two ontologies that provide a\nformal specification of the entities occurring in a simulation workflow and the\nrelations between them: The software ontology VISO is developed to represent\nsoftware packages and their features, and OSMO, an ontology for simulation,\nmodelling, and optimization, is introduced on the basis of MODA, a previously\ndeveloped semi-intuitive graph notation for workflows in materials modelling.\nAs a proof of concept, OSMO is employed to describe a use case of the TaLPas\nworkflow management system, a scheduler and workflow optimizer for\nparticle-based simulations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 16:55:29 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 20:32:12 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Horsch", "M. T.", ""], ["Niethammer", "C.", ""], ["Boccardo", "G.", ""], ["Carbone", "P.", ""], ["Chiacchiera", "S.", ""], ["Chiricotto", "M.", ""], ["Elliott", "J. D.", ""], ["Lobaskin", "V.", ""], ["Neumann", "P.", ""], ["Schiffels", "P.", ""], ["Seaton", "M. A.", ""], ["Todorov", "I. T.", ""], ["Vrabec", "J.", ""], ["Cavalcanti", "W. L.", ""]]}, {"id": "1908.02415", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far, Emina Soljanin", "title": "Redundancy Scheduling in Systems with Bi-Modal Job Service Time\n  Distribution", "comments": "Presented at Allerton 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Queuing systems with redundant requests have drawn great attention because of\ntheir promise to reduce the job completion time and variability. Despite a\nlarge body of work on the topic, we are still far from fully understanding the\nbenefits of redundancy in practice. We here take one step towards practical\nsystems by studying queuing systems with bi-modal job service time\ndistribution. Such distributions have been observed in practice, as can be seen\nin, e.g., Google cluster traces. We develop an analogy to a classical urns and\nballs problem, and use it to study the queuing time performance of two\nnon-adaptive classical scheduling policies: random and round-robin. We\nintroduce new performance indicators in the analogous model, and argue that\nthey are good predictors of the queuing time in non-adaptive scheduling\npolicies. We then propose a non-adaptive scheduling policy that is based on\ncombinatorial designs, and show that it has better performance indicators.\nSimulations confirm that the proposed scheduling policy, as the performance\nindicators suggest, reduces the queuing times compared to random and\nround-robin scheduling.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 01:42:14 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 19:11:51 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""]]}, {"id": "1908.02640", "submitter": "Ahsan Javed Awan Dr", "authors": "Gagandeep Singh, Lorenzo Chelini, Stefano Corda, Ahsan Javed Awan,\n  Sander Stuijk, Roel Jordans, Henk Corporaal and Albert-Jan Boonstra", "title": "Near-Memory Computing: Past, Present, and Future", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional approach of moving data to the CPU for computation has\nbecome a significant performance bottleneck for emerging scale-out\ndata-intensive applications due to their limited data reuse. At the same time,\nthe advancement in 3D integration technologies has made the decade-old concept\nof coupling compute units close to the memory --- called near-memory computing\n(NMC) --- more viable. Processing right at the \"home\" of data can significantly\ndiminish the data movement problem of data-intensive applications.\n  In this paper, we survey the prior art on NMC across various dimensions\n(architecture, applications, tools, etc.) and identify the key challenges and\nopen issues with future research directions. We also provide a glimpse of our\napproach to near-memory computing that includes i) NMC specific\nmicroarchitecture independent application characterization ii) a compiler\nframework to offload the NMC kernels on our target NMC platform and iii) an\nanalytical model to evaluate the potential of NMC.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 14:00:08 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Singh", "Gagandeep", ""], ["Chelini", "Lorenzo", ""], ["Corda", "Stefano", ""], ["Awan", "Ahsan Javed", ""], ["Stuijk", "Sander", ""], ["Jordans", "Roel", ""], ["Corporaal", "Henk", ""], ["Boonstra", "Albert-Jan", ""]]}, {"id": "1908.02675", "submitter": "Roy Friedman", "authors": "Yehonatan Buchnik and Roy Friedman", "title": "A Generic Efficient Biased Optimizer for Consensus Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus is one of the most fundamental distributed computing problems. In\nparticular, it serves as a building block in many replication based\nfault-tolerant systems and in particular in multiple recent blockchain\nsolutions. Depending on its exact variant and other environmental assumptions,\nsolving consensus requires multiple communication rounds. Yet, there are known\noptimistic protocols that guarantee termination in a single communication round\nunder favorable conditions.\n  In this paper we present a generic optimizer than can turn any consensus\nprotocol into an optimized protocol that terminates in a single communication\nround whenever all nodes start with the same predetermined value and no\nByzantine failures occur (although node crashes are allowed). This is\nregardless of the network timing assumptions and additional oracle capabilities\nassumed by the base consensus protocol being optimized.\n  In the case of benign failures, our optimizer works whenever the number of\nfaulty nodes $f<n/2$. For Byzantine behavior, our optimizer's resiliency\ndepends on the validity variant sought. In the case of classical validity, it\ncan accommodate $f<n/4$ Byzantine failures. With the more recent external\nvalidity function assumption, it works whenever $f<n/3$. Either way, our\noptimizer only relies on oral messages, thereby imposing very light-weight\ncrypto requirements.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 15:14:09 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Buchnik", "Yehonatan", ""], ["Friedman", "Roy", ""]]}, {"id": "1908.02741", "submitter": "Wei Quan Lim", "authors": "Seth Gilbert, Wei Quan Lim", "title": "Parallel Finger Search Structures", "comments": "Full version of a paper published in DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two versions of a parallel finger structure FS on p\nprocessors that supports searches, insertions and deletions, and has a finger\nat each end. This is to our knowledge the first implementation of a parallel\nsearch structure that is work-optimal with respect to the finger bound and yet\nhas very good parallelism (within a factor of O( (log p)^2 ) of optimal). We\nutilize an extended implicit batching framework that transparently facilitates\nthe use of FS by any parallel program P that is modelled by a dynamically\ngenerated DAG D where each node is either a unit-time instruction or a call to\nFS.\n  The total work done by either version of FS is bounded by the finger bound\nF[L] (for some linearization L of D ), i.e. each operation on an item with\nfinger distance r takes O( log r + 1 ) amortized work; it is cheaper for items\ncloser to a finger. Running P using the simpler version takes O( ( T[1] + F[L]\n) / p + T[inf] + d * ( (log p)^2 + log n ) ) time on a greedy scheduler, where\nT[1],T[inf] are the size and span of D respectively, and n is the maximum\nnumber of items in FS, and d is the maximum number of calls to FS along any\npath in D. Using the faster version, this is reduced to O( ( T[1] + F[L] ) / p\n+ T[inf] + d * (log p)^2 + s[L] ) time, where s[L] is the weighted span of D\nwhere each call to FS is weighted by its cost according to F[L]. We also sketch\nhow to extend FS to support a fixed number of movable fingers.\n  The data structures in our paper fit into the dynamic multithreading\nparadigm, and their performance bounds are directly composable with other data\nstructures given in the same paradigm. Also, the results can be translated to\npractical implementations using work-stealing schedulers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 17:35:50 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 02:59:54 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 08:49:53 GMT"}, {"version": "v4", "created": "Thu, 10 Oct 2019 17:58:10 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gilbert", "Seth", ""], ["Lim", "Wei Quan", ""]]}, {"id": "1908.02743", "submitter": "Joel Rybicki", "authors": "Thomas Nowak and Joel Rybicki", "title": "Byzantine Approximate Agreement on Graphs", "comments": "25 pages, 3 figures. Conference version appeared in DISC 2019. Minor\n  revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a distributed system with $n$ processors out of which $f$ can be\nByzantine faulty. In the approximate agreement task, each processor $i$\nreceives an input value $x_i$ and has to decide on an output value $y_i$ such\nthat\n  - the output values are in the convex hull of the non-faulty processors'\ninput values,\n  - the output values are within distance $d$ of each other.\n  Classically, the values are assumed to be from an $m$-dimensional Euclidean\nspace, where $m \\ge 1$.\n  In this work, we study the task in a discrete setting, where input values\nwith some structure expressible as a graph. Namely, the input values are\nvertices of a finite graph $G$ and the goal is to output vertices that are\nwithin distance $d$ of each other in $G$, but still remain in the graph-induced\nconvex hull of the input values. For $d=0$, the task reduces to consensus and\ncannot be solved with a deterministic algorithm in an asynchronous system even\nwith a single crash fault. For any $d \\ge 1$, we show that the task is solvable\nin asynchronous systems when $G$ is chordal and $n > (\\omega+1)f$, where\n$\\omega$ is the clique number of~$G$. In addition, we give the first\nByzantine-tolerant algorithm for a variant of lattice agreement. For\nsynchronous systems, we show tight resilience bounds for the exact variants of\nthese and related tasks over a large class of combinatorial structures.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 17:47:34 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 12:56:23 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Nowak", "Thomas", ""], ["Rybicki", "Joel", ""]]}, {"id": "1908.03022", "submitter": "Merav Parter", "authors": "Merav Parter", "title": "Small Cuts and Connectivity Certificates: A Fault Tolerant Approach", "comments": "DISC 2019 (new version fixes minor typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit classical connectivity problems in the CONGEST model of\ndistributed computing. By using techniques from fault tolerant network design,\nwe show improved constructions, some of which are even \"local\" (i.e., with\n$\\widetilde{O}(1)$ rounds) for problems that are closely related to hard global\nproblems (i.e., with a lower bound of $\\Omega(Diam+\\sqrt{n})$ rounds).\n  Our main results are:\n  (1) For $D$-diameter unweighted graphs with constant edge connectivity, we\nshow an exact distributed deterministic computation of the minimum cut in\n$poly(D)$ rounds. This resolves one the open problems recently raised in Daga,\nHenzinger, Nanongkai and Saranurak, STOC'19.\n  (2) For $D$-diameter unweighted graphs, we present a deterministic algorithm\nthat computes of all edge connectivities up to constant in $poly(D)\\cdot\n2^{O(\\sqrt{\\log n\\log\\log n})}$ rounds.\n  (3) Computation of sparse $\\lambda$ connectivity certificates in\n$\\widetilde{O}(\\lambda)$ rounds. Previous constructions where known only for\n$\\lambda \\leq 3$ and required $O(D)$ rounds. This resolves the problem raised\nby Dori PODC'18.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 11:35:44 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 13:53:51 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Parter", "Merav", ""]]}, {"id": "1908.03072", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Yunjae Lee, Minsoo Rhu", "title": "TensorDIMM: A Practical Near-Memory Processing Architecture for\n  Embeddings and Tensor Operations in Deep Learning", "comments": "Accepted for publication at the 52nd IEEE/ACM International Symposium\n  on Microarchitecture (MICRO-52), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies from several hyperscalars pinpoint to embedding layers as the\nmost memory-intensive deep learning (DL) algorithm being deployed in today's\ndatacenters. This paper addresses the memory capacity and bandwidth challenges\nof embedding layers and the associated tensor operations. We present our\nvertically integrated hardware/software co-design, which includes a custom DIMM\nmodule enhanced with near-data processing cores tailored for DL tensor\noperations. These custom DIMMs are populated inside a GPU-centric system\ninterconnect as a remote memory pool, allowing GPUs to utilize for scalable\nmemory bandwidth and capacity expansion. A prototype implementation of our\nproposal on real DL systems shows an average 6.2-17.6x performance improvement\non state-of-the-art recommender systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 13:45:33 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 11:15:05 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kwon", "Youngeun", ""], ["Lee", "Yunjae", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1908.03092", "submitter": "Yunus Sarikaya", "authors": "Yunus Sarikaya and Ozgur Ercetin", "title": "Motivating Workers in Federated Learning: A Stackelberg Game Perspective", "comments": "arXiv admin note: text overlap with arXiv:1811.12082 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the large size of the training data, distributed learning approaches\nsuch as federated learning have gained attention recently. However, the\nconvergence rate of distributed learning suffers from heterogeneous worker\nperformance. In this paper, we consider an incentive mechanism for workers to\nmitigate the delays in completion of each batch. We analytically obtained\nequilibrium solution of a Stackelberg game. Our numerical results indicate that\nwith a limited budget, the model owner should judiciously decide on the number\nof workers due to trade off between the diversity provided by the number of\nworkers and the latency of completing the training.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 14:00:35 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Sarikaya", "Yunus", ""], ["Ercetin", "Ozgur", ""]]}, {"id": "1908.03121", "submitter": "Patrick Diehl", "authors": "Gregor Dai{\\ss} and Parsa Amini and John Biddiscombe and Patrick Diehl\n  and Juhan Frank and Kevin Huck and Hartmut Kaiser and Dominic Marcello and\n  David Pfander and Dirk Pfl\\\"uger", "title": "From Piz Daint to the Stars: Simulation of Stellar Mergers using\n  High-Level Abstractions", "comments": "Accepted at SC19", "journal-ref": null, "doi": "10.1145/3295500.3356221", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the simulation of stellar mergers, which requires complex\nsimulations with high computational demands. We have developed Octo-Tiger, a\nfinite volume grid-based hydrodynamics simulation code with Adaptive Mesh\nRefinement which is unique in conserving both linear and angular momentum to\nmachine precision. To face the challenge of increasingly complex, diverse, and\nheterogeneous HPC systems, Octo-Tiger relies on high-level programming\nabstractions.\n  We use HPX with its futurization capabilities to ensure scalability both\nbetween nodes and within, and present first results replacing MPI with\nlibfabric achieving up to a 2.8x speedup. We extend Octo-Tiger to heterogeneous\nGPU-accelerated supercomputers, demonstrating node-level performance and\nportability. We show scalability up to full system runs on Piz Daint. For the\nscenario's maximum resolution, the compute-critical parts (hydrodynamics and\ngravity) achieve 68.1% parallel efficiency at 2048 nodes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 15:35:02 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 16:40:40 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Dai\u00df", "Gregor", ""], ["Amini", "Parsa", ""], ["Biddiscombe", "John", ""], ["Diehl", "Patrick", ""], ["Frank", "Juhan", ""], ["Huck", "Kevin", ""], ["Kaiser", "Hartmut", ""], ["Marcello", "Dominic", ""], ["Pfander", "David", ""], ["Pfl\u00fcger", "Dirk", ""]]}, {"id": "1908.03179", "submitter": "Alexey Gotsman", "authors": "Artem Khyzha, Hagit Attiya and Alexey Gotsman", "title": "Privatization-Safe Transactional Memories (Extended Version)", "comments": "Extended version of a paper from DISC'19 (International Symposium on\n  Distributed Computing). arXiv admin note: substantial text overlap with\n  arXiv:1801.04249", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional memory (TM) facilitates the development of concurrent\napplications by letting the programmer designate certain code blocks as atomic.\nProgrammers using a TM often would like to access the same data both inside and\noutside transactions, and would prefer their programs to have a strongly atomic\nsemantics, which allows transactions to be viewed as executing atomically with\nrespect to non-transactional accesses. Since guaranteeing such semantics for\narbitrary programs is prohibitively expensive, researchers have suggested\nguaranteeing it only for certain data-race free (DRF) programs, particularly\nthose that follow the privatization idiom: from some point on, threads agree\nthat a given object can be accessed non-transactionally.\n  In this paper we show that a variant of Transactional DRF (TDRF) by\nDalessandro et al. is appropriate for a class of privatization-safe TMs, which\nallow using privatization idioms. We prove that, if such a TM satisfies a\ncondition we call privatization-safe opacity and a program using the TM is TDRF\nunder strongly atomic semantics, then the program indeed has such semantics. We\nalso present a method for proving privatization-safe opacity that reduces\nproving this generalization to proving the usual opacity, and apply the method\nto a TM based on two-phase locking and a privatization-safe version of TL2.\nFinally, we establish the inherent cost of privatization-safety: we prove that\na TM cannot be progressive and have invisible reads if it guarantees strongly\natomic semantics for TDRF programs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 17:19:42 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Khyzha", "Artem", ""], ["Attiya", "Hagit", ""], ["Gotsman", "Alexey", ""]]}, {"id": "1908.03206", "submitter": "Sebastian Frischbier", "authors": "Sebastian Frischbier, Mario Paic, Alexander Echler, Christian Roth", "title": "Managing the Complexity of Processing Financial Data at Scale -- an\n  Experience Report", "comments": "12 pages, 2 figures, to be published in the proceedings of the 10th\n  Complex Systems Design & Management conference (CSD&M'19) by Springer", "journal-ref": null, "doi": "10.1007/978-3-030-34843-4_2", "report-no": null, "categories": "cs.DB cs.DC q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets are extremely data-driven and regulated. Participants rely\non notifications about significant events and background information that meet\ntheir requirements regarding timeliness, accuracy, and completeness. As one of\nEurope's leading providers of financial data and regulatory solutions vwd\nprocesses a daily average of 18 billion notifications from 500+ data sources\nfor 30 million symbols. Our large-scale geo-distributed systems handle daily\npeak rates of 1+ million notifications/sec. In this paper we give practical\ninsights about the different types of complexity we face regarding the data we\nprocess, the systems we operate, and the regulatory constraints we must comply\nwith. We describe the volume, variety, velocity, and veracity of the data we\nprocess, the infrastructure we operate, and the architecture we apply. We\nillustrate the load patterns created by trading and how the markets' attention\nto the Brexit vote and similar events stressed our systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 21:27:35 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Frischbier", "Sebastian", ""], ["Paic", "Mario", ""], ["Echler", "Alexander", ""], ["Roth", "Christian", ""]]}, {"id": "1908.03301", "submitter": "Hassan Dashtian", "authors": "Hassan Dashtian and Muhammad Sahimi", "title": "Efficient Simulation of Fluid Flow and Transport in Heterogeneous Media\n  Using Graphics Processing Units (GPUs)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks of interconnected resistors, springs and beams, or pores are\nstandard models of studying scalar and vector transport processes in\nheterogeneous materials and media, such as fluid flow in porous media, and\nconduction, deformations, and electric and dielectric breakdown in\nheterogeneous solids. The computation time and required memory are two limiting\nfactors that hinder the scalability of the computations to very large sizes. We\npresent a dual approach, based on the use of a combination of the central\nprocessing units (CPUs) and graphics processing units (GPUs), to simulation of\nflow, transport, and similar problems using the network models. A\nmixed-precision algorithm, together with the conjugate-gradient method is\nimplemented on a single GPU solver. The efficiency of the method is tested with\na variety of cases, including pore- and random-resistor network models in which\nthe conductances are long-range correlated, and also contain percolation\ndisorder. Both isotropic and anisotropic networks are considered. To put the\nmethod to a stringent test, the long-range correlations are generated by a\nfractional Brownian motion (FBM), which we generate by a message-passing\ninterface method. For all the cases studied an overall speed-up factor of about\none order of magnitude or better is obtained, which increases with the size of\nthe network. Even the critical slow-down in networks near the percolation\nthreshold does not decrease the speed-up significantly. We also obtain\napproximate but accurate bounds for the permeability anisotropy $K_x/K_y$ for\nstratified porous media.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 03:57:21 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Dashtian", "Hassan", ""], ["Sahimi", "Muhammad", ""]]}, {"id": "1908.03356", "submitter": "Niall Robinson PhD", "authors": "Niall H. Robinson and Joe Hamman and Ryan Abernathey", "title": "Seven Principles for Effective Scientific Big-DataSystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We should be in a golden age of scientific discovery, given that we have more\ndata and more compute power available than ever before, plus a new generation\nof algorithms that can learn effectively from data. But paradoxically, in many\ndata-driven fields, the eureka moments are becoming increasingly rare.\nScientists are struggling to keep pace with the explosion in the volume and\ncomplexity of scientific data. We describe here a few simple architectural\nprinciples that we believe are essential in order to create effective, robust,\nand flexible platforms that make the best use of emerging technology to deal\nwith the exponential growth of scientific data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 08:12:54 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 16:14:43 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Robinson", "Niall H.", ""], ["Hamman", "Joe", ""], ["Abernathey", "Ryan", ""]]}, {"id": "1908.03363", "submitter": "Ami Paz", "authors": "Pierluigi Crescenzi and Pierre Fraigniaud and Ami Paz", "title": "Trade-offs in Distributed Interactive Proofs", "comments": "To be presented in DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of interactive proofs in the context of distributed network\ncomputing is a novel topic, recently introduced by Kol, Oshman, and Saxena\n[PODC 2018]. In the spirit of sequential interactive proofs theory, we study\nthe power of distributed interactive proofs. This is achieved via a series of\nresults establishing trade-offs between various parameters impacting the power\nof interactive proofs, including the number of interactions, the certificate\nsize, the communication complexity, and the form of randomness used. Our\nresults also connect distributed interactive proofs with the established field\nof distributed verification. In general, our results contribute to providing\nstructure to the landscape of distributed interactive proofs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 08:30:36 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Crescenzi", "Pierluigi", ""], ["Fraigniaud", "Pierre", ""], ["Paz", "Ami", ""]]}, {"id": "1908.03461", "submitter": "Alexander Weinert", "authors": "Brigitte Boden, Jan Flink, Niklas F\\\"orst, Robert Mischke, Kathrin\n  Schaffert, Alexander Weinert, Annika Wohlan, Andreas Schreiber", "title": "RCE: An Integration Environment for Engineering and Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RCE (Remote Component Environment), an open-source framework\ndeveloped primarily at DLR (German Aerospace Center) that enables its users to\nconstruct and execute multidisciplinary engineering workflows comprising\nmultiple disciplinary tools. To this end, RCE supplies users with an\neasy-to-use graphical interface that allows for the intuitive integration of\ndisciplinary tools. Users can execute the individual tools on arbitrary nodes\npresent in the network and all data accrued during the execution of the\nworkflow are collected and stored centrally. Hence, RCE makes it easy for\ncollaborating engineers to contribute their individual disciplinary tools to a\nmultidisciplinary design or analysis, and simplifies the subsequent analysis of\nthe workflow's results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 14:05:24 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 06:55:03 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Boden", "Brigitte", ""], ["Flink", "Jan", ""], ["F\u00f6rst", "Niklas", ""], ["Mischke", "Robert", ""], ["Schaffert", "Kathrin", ""], ["Weinert", "Alexander", ""], ["Wohlan", "Annika", ""], ["Schreiber", "Andreas", ""]]}, {"id": "1908.03500", "submitter": "Julian Portmann", "authors": "Mohsen Ghaffari, Julian Portmann", "title": "Improved Network Decompositions using Small Messages with Applications\n  on MIS, Neighborhood Covers, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network decompositions, as introduced by Awerbuch, Luby, Goldberg, and\nPlotkin [FOCS'89], are one of the key algorithmic tools in distributed graph\nalgorithms. We present an improved deterministic distributed algorithm for\nconstructing network decompositions of power graphs using small messages, which\nimproves upon the algorithm of Ghaffari and Kuhn [DISC'18]. In addition, we\nprovide a randomized distributed network decomposition algorithm, based on our\ndeterministic algorithm, with failure probability exponentially small in the\ninput size that works with small messages as well. Compared to the previous\nalgorithm of Elkin and Neiman [PODC'16], our algorithm achieves a better\nsuccess probability at the expense of its round complexity, while giving a\nnetwork decomposition of the same quality. As a consequence of the randomized\nalgorithm for network decomposition, we get a faster randomized algorithm for\ncomputing a Maximal Independent Set, improving on a result of Ghaffari\n[SODA'19]. Other implications of our improved deterministic network\ndecomposition algorithm are: a faster deterministic distributed algorithms for\nconstructing spanners and approximations of distributed set cover, improving\nresults of Ghaffari, and Kuhn [DISC'18] and Deurer, Kuhn, and Maus [PODC'19];\nand faster a deterministic distributed algorithm for constructing neighborhood\ncovers, resolving an open question of Elkin [SODA'04].\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 15:42:16 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Portmann", "Julian", ""]]}, {"id": "1908.03583", "submitter": "Joseph Izraelevitz", "authors": "Jian Yang, Juno Kim, Morteza Hoseinzadeh, Joseph Izraelevitz, Steven\n  Swanson", "title": "An Empirical Guide to the Behavior and Use of Scalable Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  After nearly a decade of anticipation, scalable nonvolatile memory DIMMs are\nfinally commercially available with the release of Intel's 3D XPoint DIMM. This\nnew nonvolatile DIMM supports byte-granularity accesses with access times on\nthe order of DRAM, while also providing data storage that survives power\noutages. Researchers have not idly waited for real nonvolatile DIMMs (NVDIMMs)\nto arrive. Over the past decade, they have written a slew of papers proposing\nnew programming models, file systems, libraries, and applications built to\nexploit the performance and flexibility that NVDIMMs promised to deliver. Those\npapers drew conclusions and made design decisions without detailed knowledge of\nhow real NVDIMMs would behave or how industry would integrate them into\ncomputer architectures. Now that 3D XPoint NVDIMMs are actually here, we can\nprovide detailed performance numbers, concrete guidance for programmers on\nthese systems, reevaluate prior art for performance, and reoptimize persistent\nmemory software for the real 3D XPoint DIMM. In this paper, we explore the\nperformance properties and characteristics of Intel's new 3D XPoint DIMM at the\nmicro and macro level. First, we investigate the basic characteristics of the\ndevice, taking special note of the particular ways in which its performance is\npeculiar relative to traditional DRAM or other past methods used to emulate\nNVM. From these observations, we recommend a set of best practices to maximize\nthe performance of the device. With our improved understanding, we then explore\nthe performance of prior art in application-level software for persistent\nmemory, taking note of where their performance was influenced by our\nguidelines.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 18:01:32 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yang", "Jian", ""], ["Kim", "Juno", ""], ["Hoseinzadeh", "Morteza", ""], ["Izraelevitz", "Joseph", ""], ["Swanson", "Steven", ""]]}, {"id": "1908.03617", "submitter": "Youssef Bassil", "authors": "Youssef Bassil", "title": "Memory-Based Multi-Processing Method For Big Data Computation", "comments": "LACSC Lebanese Association for Computational Sciences,\n  http://www.lacsc.org", "journal-ref": "International Journal of Advanced Research and Publications, vol.\n  3, no. 3, pp. 141-146, 2019", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of the Internet and computer applications have generated\ncolossal amount of data. They are referred to as Big Data and they consist of\nhuge volume, high velocity, and variable datasets that need to be managed at\nthe right speed and within the right time frame to allow real-time data\nprocessing and analysis. Several Big Data solutions were developed, however\nthey are all based on distributed computing which can be sometimes expensive to\nbuild, manage, troubleshoot, and secure. This paper proposes a novel method for\nprocessing Big Data using memory-based, multi-processing, and one-server\narchitecture. It is memory-based because data are loaded into memory prior to\nstart processing. It is multi-processing because it leverages the power of\nparallel programming using shared memory and multiple threads running over\nseveral CPUs in a concurrent fashion. It is one-server because it only requires\na single server that operates in a non-distributed computing environment. The\nforemost advantages of the proposed method are high performance, low cost, and\nease of management. The experiments conducted showed outstanding results as the\nproposed method outperformed other conventional methods that currently exist on\nthe market. Further research can improve upon the proposed method so that it\nsupports message passing between its different processes using remote procedure\ncalls among other techniques.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:57:21 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Bassil", "Youssef", ""]]}, {"id": "1908.03668", "submitter": "Mohsen Amini Salehi", "authors": "Sahan Ahmad, SM Zobaed, Raju Gottumukkala, Mohsen Amini Salehi", "title": "Edge Computing for User-Centric Secure Search on Cloud-Based Encrypted\n  Big Data", "comments": "High Performance Computing and Communications (HPCC '19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud service providers offer a low-cost and convenient solution to host\nunstructured data. However, cloud services act as third-party solutions and do\nnot provide control of the data to users. This has raised security and privacy\nconcerns for many organizations (users) with sensitive data to utilize\ncloud-based solutions. User-side encryption can potentially address these\nconcerns by establishing user-centric cloud services and granting data control\nto the user. Nonetheless, user-side encryption limits the ability to process\n(e.g., search) encrypted data on the cloud. Accordingly, in this research, we\nprovide a framework that enables processing (in particular, searching) of\nencrypted multi-organizational (i.e., multi-source) big data without revealing\nthe data to cloud provider. Our framework leverages locality feature of edge\ncomputing to offer a user-centric search ability in a real-time manner. In\nparticular, the edge system intelligently predicts the user's search pattern\nand prunes the multi-source big data search space to reduce the search time.\nThe pruning system is based on efficient sampling from the clustered big\ndataset on the cloud. For each cluster, the pruning system dynamically samples\nappropriate number of terms based on the user's search tendency, so that the\ncluster is optimally represented. We developed a prototype of a user-centric\nsearch system and evaluated it against multiple datasets. Experimental results\ndemonstrate 27% improvement in the pruning quality and search accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 02:19:37 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Ahmad", "Sahan", ""], ["Zobaed", "SM", ""], ["Gottumukkala", "Raju", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "1908.03775", "submitter": "Mojtaba Sedigh Fazli", "authors": "Mojtaba S. Fazli, Rachel V. Stadler, BahaaEddin Alaila, Stephen A.\n  Vella, Silvia N. J. Moreno, Gary E. Ward, and Shannon Quinn", "title": "Lightweight and Scalable Particle Tracking and Motion Clustering of 3D\n  Cell Trajectories", "comments": "Accepted to 2019 IEEE International Conference on Data Science and\n  Advanced Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking cell particles in 3D microscopy videos is a challenging task but is\nof great significance for modeling the motion of cells. Proper characterization\nof the cell's shape, evolution, and their movement over time is crucial to\nunderstanding and modeling the mechanobiology of cell migration in many\ndiseases. One in particular, toxoplasmosis is the disease caused by the\nparasite Toxoplasma gondii. Roughly, one-third of the world's population tests\npositive for T. gondii. Its virulence is linked to its lytic cycle, predicated\non its motility and ability to enter and exit nucleated cells; therefore,\nstudies elucidating its motility patterns are critical to the eventual\ndevelopment of therapeutic strategies. Here, we present a computational\nframework for fast and scalable detection, tracking, and identification of T.\ngondii motion phenotypes in 3D videos, in a completely unsupervised fashion.\nOur pipeline consists of several different modules including preprocessing,\nsparsification, cell detection, cell tracking, trajectories extraction,\nparametrization of the trajectories; and finally, a clustering step.\nAdditionally, we identified the computational bottlenecks, and developed a\nlightweight and highly scalable pipeline through a combination of task\ndistribution and parallelism. Our results prove both the accuracy and\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 15:43:49 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 22:09:09 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 14:24:05 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Fazli", "Mojtaba S.", ""], ["Stadler", "Rachel V.", ""], ["Alaila", "BahaaEddin", ""], ["Vella", "Stephen A.", ""], ["Moreno", "Silvia N. J.", ""], ["Ward", "Gary E.", ""], ["Quinn", "Shannon", ""]]}, {"id": "1908.03935", "submitter": "Vanderson Martins do Rosario", "authors": "Vanderson M. do Rosario, Mauricio Breternitz Jr. and Edson Borin", "title": "Efficiency and Scalability of Multi-Lane Capsule Networks (MLCN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some Deep Neural Networks (DNN) have what we call lanes, or they can be\nreorganized as such. Lanes are paths in the network which are data-independent\nand typically learn different features or add resilience to the network. Given\ntheir data-independence, lanes are amenable for parallel processing. The\nMulti-lane CapsNet (MLCN) is a proposed reorganization of the Capsule Network\nwhich is shown to achieve better accuracy while bringing highly-parallel lanes.\nHowever, the efficiency and scalability of MLCN had not been systematically\nexamined. In this work, we study the MLCN network with multiple GPUs finding\nthat it is 2x more efficient than the original CapsNet when using\nmodel-parallelism. Further, we present the load balancing problem of\ndistributing heterogeneous lanes in homogeneous or heterogeneous accelerators\nand show that a simple greedy heuristic can be almost 50% faster than a naive\nrandom approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 17:04:14 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Rosario", "Vanderson M. do", ""], ["Breternitz", "Mauricio", "Jr."], ["Borin", "Edson", ""]]}, {"id": "1908.04081", "submitter": "Erin Carson", "authors": "Erin C. Carson", "title": "An Adaptive $s$-step Conjugate Gradient Algorithm with Dynamic Basis\n  Updating", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive $s$-step CG algorithm is a solver for sparse, symmetric positive\ndefinite linear systems designed to reduce the synchronization cost per\niteration while still achieving a user-specified accuracy requirement. In this\nwork, we improve the adaptive $s$-step conjugate gradient algorithm by use of\niteratively updated estimates of the largest and smallest Ritz values, which\ngive approximations of the largest and smallest eigenvalues of $A$, using a\ntechnique due to Meurant and Tich{\\' y} [G. Meurant and P. Tich{\\' y}, Numer.\nAlgs. (2018), pp.~1--32]. The Ritz value estimates are used to dynamically\nupdate parameters for constructing Newton or Chebyshev polynomials so that the\nconditioning of the $s$-step bases can be continuously improved throughout the\niterations. These estimates are also used to automatically set a variable\nrelated to the ratio of the sizes of the error and residual, which was\npreviously treated as an input parameter. We show through numerical experiments\nthat in many cases the new algorithm improves upon the previous adaptive\n$s$-step approach both in terms of numerical behavior and reduction in number\nof synchronizations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 10:30:33 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Carson", "Erin C.", ""]]}, {"id": "1908.04141", "submitter": "Alexander Noe", "authors": "Monika Henzinger, Alexander Noe and Christian Schulz", "title": "Shared-Memory Branch-and-Reduce for Multiterminal Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the fastest known exact algorithm~for~the multiterminal cut\nproblem with k terminals. In particular, we engineer existing as well as new\ndata reduction rules. We use the rules within a branch-and-reduce framework and\nto boost the performance of an ILP formulation. Our algorithms achieve\nimprovements in running time of up to multiple orders of magnitudes over the\nILP formulation without data reductions, which has been the de facto standard\nused by practitioners. This allows us to solve instances to optimality that are\nsignificantly larger than was previously possible.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 13:26:57 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 18:32:14 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Henzinger", "Monika", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""]]}, {"id": "1908.04207", "submitter": "Shigang Li", "authors": "Shigang Li, Tal Ben-Nun, Salvatore Di Girolamo, Dan Alistarh, Torsten\n  Hoefler", "title": "Taming Unbalanced Training Workloads in Deep Learning with Partial\n  Collective Operations", "comments": "Published in Proceedings of the 25th ACM SIGPLAN Symposium on\n  Principles and Practice of Parallel Programming (PPoPP'20), pp. 45-61. 2020", "journal-ref": null, "doi": "10.1145/3332466.3374528", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load imbalance pervasively exists in distributed deep learning training\nsystems, either caused by the inherent imbalance in learned tasks or by the\nsystem itself. Traditional synchronous Stochastic Gradient Descent (SGD)\nachieves good accuracy for a wide variety of tasks, but relies on global\nsynchronization to accumulate the gradients at every training step. In this\npaper, we propose eager-SGD, which relaxes the global synchronization for\ndecentralized accumulation. To implement eager-SGD, we propose to use two\npartial collectives: solo and majority. With solo allreduce, the faster\nprocesses contribute their gradients eagerly without waiting for the slower\nprocesses, whereas with majority allreduce, at least half of the participants\nmust contribute gradients before continuing, all without using a central\nparameter server. We theoretically prove the convergence of the algorithms and\ndescribe the partial collectives in detail. Experimental results on\nload-imbalanced environments (CIFAR-10, ImageNet, and UCF101 datasets) show\nthat eager-SGD achieves 1.27x speedup over the state-of-the-art synchronous\nSGD, without losing accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 15:37:51 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 07:35:30 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 23:13:50 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Li", "Shigang", ""], ["Ben-Nun", "Tal", ""], ["Di Girolamo", "Salvatore", ""], ["Alistarh", "Dan", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1908.04249", "submitter": "Suryanarayana Murthy Durbhakula", "authors": "Murthy Durbhakula", "title": "Cache Optimization for Memory Intensive Workloads on Multi-socket\n  Multi-core servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major chip manufacturers have all introduced multicore microprocessors.\nMulti-socket systems built from these processors are used for running various\nserver applications. Depending on the application that is run on the system,\nremote memory accesses can impact overall performance. This paper presents a\ncache optimization that can cut down remote DRAM accesses. By keeping track of\nremote cache lines loaded from remote DRAM and by biasing the cache replacement\npolicy towards such remote DRAM cache lines the number of cache misses are\nreduced. This in turn results in improvement of overall performance. I present\nthe design details in this paper. I do a qualitative comparison of various\nsolutions to the problem of performance impact of remote DRAM accesses. This\nwork can be extended by doing a quantitative evaluation and by further refining\ncache optimization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 16:58:02 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Durbhakula", "Murthy", ""]]}, {"id": "1908.04279", "submitter": "Tamar Shinar", "authors": "Steven Cook and Tamar Shinar", "title": "Enabling Simulation of High-Dimensional Micro-Macro Biophysical Models\n  through Hybrid CPU and Multi-GPU Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-macro models provide a powerful tool to study the relationship between\nmicroscale mechanisms and emergent macroscopic behavior. However, the detailed\nmicroscopic modeling may require tracking and evolving a high-dimensional\nconfiguration space at high computational cost. In this work, we present a\nparallel algorithm for simulation a high-dimensional micro-macro model of a\ngliding motility assay. We utilize a holistic approach aligning the data\nresidency and simulation scales with the hybrid CPU and multi-GPU hardware.\nWith a combination of algorithmic modifications, GPU optimizations, and scaling\nto multiple GPUs, we achieve speedup factors of up to 27 over our previous\nhybrid CPU-GPU implementation and up to 540 over our single-threaded\nimplementation. This approach enables micro-macro simulations of higher\ncomplexity and resolution than would otherwise be feasible.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 17:42:34 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Cook", "Steven", ""], ["Shinar", "Tamar", ""]]}, {"id": "1908.04465", "submitter": "Florian Hofer", "authors": "Florian Hofer, Martin A. Sehr, Antonio Iannopollo, Ines Ugalde,\n  Alberto Sangiovanni-Vincentelli, Barbara Russo", "title": "Industrial Control via Application Containers: Migrating from Bare-Metal\n  to IAAS", "comments": "8 pages with figures and tables submitted to CloudCom,\n  2019.cloudcom.org/", "journal-ref": null, "doi": "10.1109/CloudCom.2019.00021", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the challenges and opportunities of shifting industrial control\nsoftware from dedicated hardware to bare-metal servers or cloud computing\nplatforms using off the shelf technologies. In particular, we demonstrate that\nexecuting time-critical applications on cloud platforms is viable based on a\nseries of dedicated latency tests targeting relevant real-time configurations.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 02:31:59 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Hofer", "Florian", ""], ["Sehr", "Martin A.", ""], ["Iannopollo", "Antonio", ""], ["Ugalde", "Ines", ""], ["Sangiovanni-Vincentelli", "Alberto", ""], ["Russo", "Barbara", ""]]}, {"id": "1908.04511", "submitter": "Ruslan Nikolaev", "authors": "Ruslan Nikolaev", "title": "A Scalable, Portable, and Memory-Efficient Lock-Free FIFO Queue", "comments": null, "journal-ref": "33rd International Symposium on Distributed Computing (DISC 2019)", "doi": "10.4230/LIPIcs.DISC.2019.28", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new lock-free multiple-producer and multiple-consumer (MPMC)\nFIFO queue design which is scalable and, unlike existing high-performant\nqueues, very memory efficient. Moreover, the design is ABA safe and does not\nrequire any external memory allocators or safe memory reclamation techniques,\ntypically needed by other scalable designs. In fact, this queue itself can be\nleveraged for object allocation and reclamation, as in data pools. We use FAA\n(fetch-and-add), a specialized and more scalable than CAS (compare-and-set)\ninstruction, on the most contended hot spots of the algorithm. However, unlike\nprior attempts with FAA, our queue is both lock-free and linearizable.\n  We propose a general approach, SCQ, for bounded queues. This approach can\neasily be extended to support unbounded FIFO queues which can store an\narbitrary number of elements. SCQ is portable across virtually all existing\narchitectures and flexible enough for a wide variety of uses. We measure the\nperformance of our algorithm on the x86-64 and PowerPC architectures. Our\nevaluation validates that our queue has exceptional memory efficiency compared\nto other algorithms and its performance is often comparable to, or exceeding\nthat of state-of-the-art scalable algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 06:34:10 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Nikolaev", "Ruslan", ""]]}, {"id": "1908.04705", "submitter": "Yu Emma Wang", "authors": "Yu Emma Wang, Carole-Jean Wu, Xiaodong Wang, Kim Hazelwood, David\n  Brooks", "title": "Exploiting Parallelism Opportunities with Deep Learning Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  State-of-the-art machine learning frameworks support a wide variety of design\nfeatures to enable a flexible machine learning programming interface and to\nease the programmability burden on machine learning developers. Identifying and\nusing a performance-optimal setting in feature-rich frameworks, however,\ninvolves a non-trivial amount of performance profiling efforts and often relies\non domain-specific knowledge. This paper takes a deep dive into analyzing the\nperformance impact of key design features in a machine learning framework and\nquantifies the role of parallelism. The observations and insights distill into\na simple set of guidelines that one can use to achieve much higher training and\ninference speedup. Across a diverse set of real-world deep learning models, the\nevaluation results show that the proposed performance tuning guidelines\noutperform the Intel and TensorFlow recommended settings by 1.29x and 1.34x,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 15:41:14 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 23:37:48 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Wang", "Yu Emma", ""], ["Wu", "Carole-Jean", ""], ["Wang", "Xiaodong", ""], ["Hazelwood", "Kim", ""], ["Brooks", "David", ""]]}, {"id": "1908.04744", "submitter": "Kyle Kuan", "authors": "Kyle Kuan and Tosiron Adegbija", "title": "Energy and Performance Analysis of STTRAM Caches for Mobile Applications", "comments": "To appear in IEEE International Symposium on Embedded\n  Multicore/Many-core Systems-on-Chip (MCSoC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spin-Transfer Torque RAMs (STTRAMs) have been shown to offer much promise for\nimplementing emerging cache architectures. This paper studies the viability of\nSTTRAM caches for mobile workloads from the perspective of energy and latency.\nSpecifically, we explore the benefits of reduced retention STTRAM caches for\nmobile applications. We analyze the characteristics of mobile applications'\ncache blocks and how those characteristics dictate the appropriate retention\ntime for mobile device caches. We show that due to their inherently interactive\nnature, mobile applications' execution characteristics---and hence, STTRAM\ncache design requirements---differ from other kinds of applications. We also\nexplore various STTRAM cache designs in both single and multicore systems, and\nat different cache levels, that can efficiently satisfy mobile applications'\nexecution requirements, in order to maximize energy savings without introducing\nsubstantial latency overhead.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 20:19:24 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Kuan", "Kyle", ""], ["Adegbija", "Tosiron", ""]]}, {"id": "1908.04824", "submitter": "Matthew Turner", "authors": "Matthew Turner, Hana Khamfroush", "title": "Meeting QoS of Users in a Edge to Cloud Platform via Optimally Placing\n  Services and Scheduling Tasks", "comments": "6 Pages, 6 Figures, submitted to ICNC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of service placement and task scheduling on\na three-tiered edge-to-cloud platform when user requests must be met by a\ncertain deadline. Time-sensitive applications (e.g., augmented reality, gaming,\nreal-time video analysis) have tight constraints that must be met. With\nmultiple possible computation centers, the \"where\" and \"when\" of solving these\nrequests becomes paramount when meeting their deadlines. We formulate the\nproblem of meeting users' deadlines while minimizing the total cost of the\nedge-to-cloud service provider as an Integer Linear Programming (ILP) problem.\nWe show the NP-hardness of this problem, and propose two heuristics based on\nmaking decisions on a local vs global scale. We vary the number of users, the\nQoS constraint, and the cost difference between remote cloud and cloudlets(edge\nclouds), and run multiple Monte-Carlo runs for each case. Our simulation\nresults show that the proposed heuristics are performing close to optimal while\nreducing the complexity.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 18:55:29 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Turner", "Matthew", ""], ["Khamfroush", "Hana", ""]]}, {"id": "1908.04904", "submitter": "Feng Li", "authors": "Xuening Zhu, Feng Li, Hansheng Wang", "title": "Least Squares Approximation for a Distributed System", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 2021", "doi": "10.1080/10618600.2021.1923517", "report-no": null, "categories": "stat.ME cs.DC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a distributed least squares approximation (DLSA)\nmethod that is able to solve a large family of regression problems (e.g.,\nlinear regression, logistic regression, and Cox's model) on a distributed\nsystem. By approximating the local objective function using a local quadratic\nform, we are able to obtain a combined estimator by taking a weighted average\nof local estimators. The resulting estimator is proved to be statistically as\nefficient as the global estimator. Moreover, it requires only one round of\ncommunication. We further conduct a shrinkage estimation based on the DLSA\nestimation using an adaptive Lasso approach. The solution can be easily\nobtained by using the LARS algorithm on the master node. It is theoretically\nshown that the resulting estimator possesses the oracle property and is\nselection consistent by using a newly designed distributed Bayesian information\ncriterion (DBIC). The finite sample performance and computational efficiency\nare further illustrated by an extensive numerical study and an airline dataset.\nThe airline dataset is 52 GB in size. The entire methodology has been\nimplemented in Python for a {\\it de-facto} standard Spark system. The proposed\nDLSA algorithm on the Spark system takes 26 minutes to obtain a logistic\nregression estimator, which is more efficient and memory friendly than\nconventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:05:21 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 01:46:47 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 07:11:52 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 09:53:50 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhu", "Xuening", ""], ["Li", "Feng", ""], ["Wang", "Hansheng", ""]]}, {"id": "1908.04907", "submitter": "Ryan Chard", "authors": "Ryan Chard, Tyler J. Skluzacek, Zhuozhao Li, Yadu Babuji, Anna\n  Woodard, Ben Blaiszik, Steven Tuecke, Ian Foster, Kyle Chard", "title": "Serverless Supercomputing: High Performance Function as a Service for\n  Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing data volumes and velocities are driving exciting new methods across\nthe sciences in which data analytics and machine learning are increasingly\nintertwined with research. These new methods require new approaches for\nscientific computing in which computation is mobile, so that, for example, it\ncan occur near data, be triggered by events (e.g., arrival of new data), or be\noffloaded to specialized accelerators. They also require new design approaches\nin which monolithic applications can be decomposed into smaller components,\nthat may in turn be executed separately and on the most efficient resources. To\naddress these needs we propose funcX---a high-performance function-as-a-service\n(FaaS) platform that enables intuitive, flexible, efficient, scalable, and\nperformant remote function execution on existing infrastructure including\nclouds, clusters, and supercomputers. It allows users to register and then\nexecute Python functions without regard for the physical resource location,\nscheduler architecture, or virtualization technology on which the function is\nexecuted---an approach we refer to as \"serverless supercomputing.\" We motivate\nthe need for funcX in science, describe our prototype implementation, and\ndemonstrate, via experiments on two supercomputers, that funcX can process\nmillions of functions across more than 65000 concurrent workers. We also\noutline five scientific scenarios in which funcX has been deployed and\nhighlight the benefits of funcX in these scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:17:18 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Chard", "Ryan", ""], ["Skluzacek", "Tyler J.", ""], ["Li", "Zhuozhao", ""], ["Babuji", "Yadu", ""], ["Woodard", "Anna", ""], ["Blaiszik", "Ben", ""], ["Tuecke", "Steven", ""], ["Foster", "Ian", ""], ["Chard", "Kyle", ""]]}, {"id": "1908.04909", "submitter": "Yan Xu", "authors": "Steven Gardner, Oleg Golovidov, Joshua Griffin, Patrick Koch, Wayne\n  Thompson, Brett Wujek and Yan Xu", "title": "Constrained Multi-Objective Optimization for Automated Machine Learning", "comments": "10 pages, 8 figures, accepted at DSAA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated machine learning has gained a lot of attention recently. Building\nand selecting the right machine learning models is often a multi-objective\noptimization problem. General purpose machine learning software that\nsimultaneously supports multiple objectives and constraints is scant, though\nthe potential benefits are great. In this work, we present a framework called\nAutotune that effectively handles multiple objectives and constraints that\narise in machine learning problems. Autotune is built on a suite of\nderivative-free optimization methods, and utilizes multi-level parallelism in a\ndistributed computing environment for automatically training, scoring, and\nselecting good models. Incorporation of multiple objectives and constraints in\nthe model exploration and selection process provides the flexibility needed to\nsatisfy trade-offs necessary in practical machine learning applications.\nExperimental results from standard multi-objective optimization benchmark\nproblems show that Autotune is very efficient in capturing Pareto fronts. These\nbenchmark results also show how adding constraints can guide the search to more\npromising regions of the solution space, ultimately producing more desirable\nPareto fronts. Results from two real-world case studies demonstrate the\neffectiveness of the constrained multi-objective optimization capability\noffered by Autotune.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:31:45 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Gardner", "Steven", ""], ["Golovidov", "Oleg", ""], ["Griffin", "Joshua", ""], ["Koch", "Patrick", ""], ["Thompson", "Wayne", ""], ["Wujek", "Brett", ""], ["Xu", "Yan", ""]]}, {"id": "1908.04935", "submitter": "Siva Leela Krishna Chand Gudi", "authors": "Siva Leela Krishna Chand Gudi, Benjamin Johnston, and Mary-Anne\n  Williams", "title": "Fog Robotics: A Summary, Challenges and Future Scope", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot interaction plays a crucial role to make robots closer to humans.\nUsually, robots are limited by their own capabilities. Therefore, they utilise\nCloud Robotics to enhance their dexterity. Its ability includes the sharing of\ninformation such as maps, images and the processing power. This whole process\ninvolves distributing data which intend to rise enormously. New issues can\narise such as bandwidth, network congestion at backhaul and fronthaul systems\nresulting in high latency. Thus, it can make an impact on seamless connectivity\nbetween the robots, users and the cloud. Also, a robot may not accomplish its\ngoal successfully within a stipulated time. As a consequence, Cloud Robotics\ncannot be in a position to handle the traffic imposed by robots. On the\ncontrary, impending Fog Robotics can act as a solution by solving major\nproblems of Cloud Robotics. Therefore to check its feasibility, we discuss the\nneed and architectures of Fog Robotics in this paper. To evaluate the\narchitectures, we used a realistic scenario of Fog Robotics by comparing them\nwith Cloud Robotics. Next, latency is chosen as the primary factor for\nvalidating the effectiveness of the system. Besides, we utilised real-time\nlatency using Pepper robot, Fog robot server and the Cloud server. Experimental\nresults show that Fog Robotics reduces latency significantly compared to Cloud\nRobotics. Moreover, advantages, challenges and future scope of the Fog Robotics\nsystem is further discussed.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 03:07:12 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Gudi", "Siva Leela Krishna Chand", ""], ["Johnston", "Benjamin", ""], ["Williams", "Mary-Anne", ""]]}, {"id": "1908.04960", "submitter": "Mohsen Amini Salehi", "authors": "SM Zobaed, Sahan Ahmad, Raju Gottumukkala, Mohsen Amini Salehi", "title": "ClustCrypt: Privacy-Preserving Clustering of Unstructured Big Data in\n  the Cloud", "comments": "High Performance Computing and Communications (HPCC '19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security and confidentiality of big data stored in the cloud are important\nconcerns for many organizations to adopt cloud services. One common approach to\naddress the concerns is client-side encryption where data is encrypted on the\nclient machine before being stored in the cloud. Having encrypted data in the\ncloud, however, limits the ability of data clustering, which is a crucial part\nof many data analytics applications, such as search systems. To overcome the\nlimitation, in this paper, we present an approach named ClustCrypt for\nefficient topic-based clustering of encrypted unstructured big data in the\ncloud. ClustCrypt dynamically estimates the optimal number of clusters based on\nthe statistical characteristics of encrypted data. It also provides clustering\napproach for encrypted data. We deploy ClustCrypt within the context of a\nsecure cloud-based semantic search system (S3BD). Experimental results obtained\nfrom evaluating ClustCrypt on three datasets demonstrate on average 60%\nimprovement on clusters' coherency. ClustCrypt also decreases the search-time\noverhead by up to 78% and increases the accuracy of search results by up to 35%\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 05:23:12 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Zobaed", "SM", ""], ["Ahmad", "Sahan", ""], ["Gottumukkala", "Raju", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "1908.05156", "submitter": "Damian Straszak", "authors": "Adam G\\k{a}gol, Damian Le\\'sniak, Damian Straszak, Micha{\\l}\n  \\'Swi\\k{e}tek", "title": "Aleph: Efficient Atomic Broadcast in Asynchronous Networks with\n  Byzantine Nodes", "comments": "Accepted for presentation at AFT'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectacular success of Bitcoin and Blockchain Technology in recent years\nhas provided enough evidence that a widespread adoption of a common\ncryptocurrency system is not merely a distant vision, but a scenario that might\ncome true in the near future. However, the presence of Bitcoin's obvious\nshortcomings such as excessive electricity consumption, unsatisfying\ntransaction throughput, and large validation time (latency) makes it clear that\na new, more efficient system is needed.\n  We propose a protocol in which a set of nodes maintains and updates a linear\nordering of transactions that are being submitted by users. Virtually every\ncryptocurrency system has such a protocol at its core, and it is the efficiency\nof this protocol that determines the overall throughput and latency of the\nsystem. We develop our protocol on the grounds of the well-established field of\nAsynchronous Byzantine Fault Tolerant (ABFT) systems. This allows us to\nformally reason about correctness, efficiency, and security in the strictest\npossible model, and thus convincingly prove the overall robustness of our\nsolution.\n  Our protocol improves upon the state-of-the-art HoneyBadgerBFT by Miller et\nal. by reducing the asymptotic latency while matching the optimal communication\ncomplexity. Furthermore, in contrast to the above, our protocol does not\nrequire a trusted dealer thanks to a novel implementation of a trustless ABFT\nRandomness Beacon.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 14:54:04 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 09:01:49 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["G\u0105gol", "Adam", ""], ["Le\u015bniak", "Damian", ""], ["Straszak", "Damian", ""], ["\u015awi\u0119tek", "Micha\u0142", ""]]}, {"id": "1908.05385", "submitter": "Yasaman Keshtkarjahromi", "authors": "Yasaman Keshtkarjahromi, Rawad Bitar, Venkat Dasari, Salim El\n  Rouayheb, Hulya Seferoglu", "title": "Secure Coded Cooperative Computation at the Heterogeneous Edge against\n  Byzantine Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is emerging as a new paradigm to allow processing data at the\nedge of the network, where data is typically generated and collected, by\nexploiting multiple devices at the edge collectively. However, offloading tasks\nto other devices leaves the edge computing applications at the complete mercy\nof an attacker. One of the attacks, which is also the focus of this work, is\nByzantine attacks, where one or more devices can corrupt the offloaded tasks.\nFurthermore, exploiting the potential of edge computing is challenging mainly\ndue to the heterogeneous and time-varying nature of the devices at the edge. In\nthis paper, we develop a secure coded cooperative computation mechanism (SC3)\nthat provides both security and computation efficiency guarantees by gracefully\ncombining homomorphic hash functions and coded cooperative computation.\nHomomorphic hash functions are used against Byzantine attacks and coded\ncooperative computation is used to improve computation efficiency when edge\nresources are heterogeneous and time-varying. Simulations results show that SC3\nimproves task completion delay significantly.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 01:11:43 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Keshtkarjahromi", "Yasaman", ""], ["Bitar", "Rawad", ""], ["Dasari", "Venkat", ""], ["Rouayheb", "Salim El", ""], ["Seferoglu", "Hulya", ""]]}, {"id": "1908.05655", "submitter": "Kiarash Rahmani", "authors": "Kia Rahmani, Kartik Nagar, Benjamin Delaware, Suresh Jagannathan", "title": "CLOTHO: Directed Test Generation for Weakly Consistent Database Systems", "comments": "Conditionally accepted to OOPSLA'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational database applications are notoriously difficult to test and debug.\nConcurrent execution of database transactions may violate complex structural\ninvariants that constraint how changes to the contents of one (shared) table\naffect the contents of another. Simplifying the underlying concurrency model is\none way to ameliorate the difficulty of understanding how concurrent accesses\nand updates can affect database state with respect to these sophisticated\nproperties. Enforcing serializable execution of all transactions achieves this\nsimplification, but it comes at a significant price in performance, especially\nat scale, where database state is often replicated to improve latency and\navailability. To address these challenges, this paper presents a novel testing\nframework for detecting serializability violations in (SQL) database-backed\nJava applications executing on weakly-consistent storage systems. We manifest\nour approach in a tool named CLOTHO, that combines a static analyzer and a\nmodel checker to generate abstract executions, discover serializability\nviolations in these executions, and translate them back into concrete test\ninputs suitable for deployment in a test environment. To the best of our\nknowledge, CLOTHO is the first automated test generation facility for\nidentifying serializability anomalies of Java applications intended to operate\nin geo-replicated distributed environments. An experimental evaluation on a set\nof industry-standard benchmarks demonstrates the utility of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 17:57:37 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Rahmani", "Kia", ""], ["Nagar", "Kartik", ""], ["Delaware", "Benjamin", ""], ["Jagannathan", "Suresh", ""]]}, {"id": "1908.05666", "submitter": "Konstantinos Konstantinidis", "authors": "Konstantinos Konstantinidis and Aditya Ramamoorthy", "title": "Resolvable Designs for Speeding up Distributed Computing", "comments": "14 pages, 3 figures, full paper for IEEE TON submission. arXiv admin\n  note: substantial text overlap with arXiv:1802.03049, arXiv:1901.07418", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing frameworks such as MapReduce are often used to process\nlarge computational jobs. They operate by partitioning each job into smaller\ntasks executed on different servers. The servers also need to exchange\nintermediate values to complete the computation. Experimental evidence suggests\nthat this so-called Shuffle phase can be a significant part of the overall\nexecution time for several classes of jobs. Prior work has demonstrated a\nnatural tradeoff between computation and communication whereby running\nredundant copies of jobs can reduce the Shuffle traffic load, thereby leading\nto reduced overall execution times. For a single job, the main drawback of this\napproach is that it requires the original job to be split into a number of\nfiles that grows exponentially in the system parameters. When extended to\nmultiple jobs (with specific function types), these techniques suffer from a\nlimitation of a similar flavor, i.e., they require an exponentially large\nnumber of jobs to be executed. In practical scenarios, these requirements can\nsignificantly reduce the promised gains of the method. In this work, we show\nthat a class of combinatorial structures called resolvable designs can be used\nto develop efficient coded distributed computing schemes for both the single\nand multiple job scenarios considered in prior work. We present both\ntheoretical analysis and exhaustive experimental results (on Amazon EC2\nclusters) that demonstrate the performance advantages of our method. For the\nsingle and multiple job cases, we obtain speed-ups of 4.69x (and 2.6x over\nprior work) and 4.31x over the baseline approach, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:02:28 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 18:14:16 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 00:36:27 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Konstantinidis", "Konstantinos", ""], ["Ramamoorthy", "Aditya", ""]]}, {"id": "1908.05700", "submitter": "Gal Oren", "authors": "Leonid Barenboim, Gal Oren", "title": "Distributed Backup Placement in One Round and its Applications to\n  Maximum Matching Approximation and Self-Stabilization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the distributed backup-placement problem each node of a network has to\nselect one neighbor, such that the maximum number of nodes that make the same\nselection is minimized. This is a natural relaxation of the perfect matching\nproblem, in which each node is selected just by one neighbor. Previous\n(approximate) solutions for backup placement are non-trivial, even for simple\ngraph topologies, such as dense graphs. In this paper we devise an algorithm\nfor dense graph topologies, including unit disk graphs, unit ball graphs, line\ngraphs, graphs with bounded diversity, and many more. Our algorithm requires\njust one round, and is as simple as the following operation. Consider a\ncircular list of neighborhood IDs, sorted in an ascending order, and select the\nID that is next to the selecting vertex ID. Surprisingly, such a simple\none-round strategy turns out to be very efficient for backup placement\ncomputation in dense networks. Not only that it improves the number of rounds\nof the solution, but also the approximation ratio is improved by a\nmultiplicative factor of at least $2$.\n  Our new algorithm has several interesting implications. In particular, it\ngives rise to a $(2 + \\epsilon)$-approximation to maximum matching within\n$O(\\log^* n)$ rounds in dense networks. The resulting algorithm is very simple\nas well, in sharp contrast to previous algorithms that compute such a solution\nwithin this running time. Moreover, these algorithms are applicable to a\nnarrower graph family than our algorithm. For the same graph family, the best\npreviously-known result has $O(\\log {\\Delta} + \\log^* n)$ running time. Another\ninteresting implication is the possibility to execute our backup placement\nalgorithm as-is in the self-stabilizing setting. This makes it possible to\nsimplify and improve other algorithms for the self-stabilizing setting, by\nemploying helpful properties of backup placement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 18:31:16 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Barenboim", "Leonid", ""], ["Oren", "Gal", ""]]}, {"id": "1908.05790", "submitter": "Elliott Slaughter", "authors": "Elliott Slaughter, Wei Wu, Yuankun Fu, Legend Brandenburg, Nicolai\n  Garcia, Wilhem Kautz, Emily Marx, Kaleb S. Morris, Wonchan Lee, Qinglei Cao,\n  George Bosilca, Seema Mirchandaney, Sean Treichler, Patrick McCormick, Alex\n  Aiken", "title": "Task Bench: A Parameterized Benchmark for Evaluating Parallel Runtime\n  Performance", "comments": "14 pages, 13 figures, published in SC'20: Proceedings of the\n  Conference for High Performance Computing, Networking, Storage and Analysis", "journal-ref": null, "doi": "10.5555/3433701.3433783", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Task Bench, a parameterized benchmark designed to explore the\nperformance of parallel and distributed programming systems under a variety of\napplication scenarios. Task Bench lowers the barrier to benchmarking multiple\nprogramming systems by making the implementation for a given system orthogonal\nto the benchmarks themselves: every benchmark constructed with Task Bench runs\non every Task Bench implementation. Furthermore, Task Bench's parameterization\nenables a wide variety of benchmark scenarios that distill the key\ncharacteristics of larger applications.\n  We conduct a comprehensive study with implementations of Task Bench in 15\nprogramming systems on up to 256 Haswell nodes of the Cori supercomputer. We\nintroduce a novel metric, minimum effective task granularity to study the\nbaseline runtime overhead of each system. We show that when running at scale,\n100 {\\mu}s is the smallest granularity that even the most efficient systems can\nreliably support with current technologies. We also study each system's\nscalability, ability to hide communication and mitigate load imbalance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 23:04:30 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 00:54:41 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Slaughter", "Elliott", ""], ["Wu", "Wei", ""], ["Fu", "Yuankun", ""], ["Brandenburg", "Legend", ""], ["Garcia", "Nicolai", ""], ["Kautz", "Wilhem", ""], ["Marx", "Emily", ""], ["Morris", "Kaleb S.", ""], ["Lee", "Wonchan", ""], ["Cao", "Qinglei", ""], ["Bosilca", "George", ""], ["Mirchandaney", "Seema", ""], ["Treichler", "Sean", ""], ["McCormick", "Patrick", ""], ["Aiken", "Alex", ""]]}, {"id": "1908.05792", "submitter": "Wissam Sid-Lakhdar", "authors": "Wissam M. Sid-Lakhdar, Mohsen Mahmoudi Aznaveh, Xiaoye S. Li, James W.\n  Demmel", "title": "Multitask and Transfer Learning for Autotuning Exascale Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask learning and transfer learning have proven to be useful in the\nfield of machine learning when additional knowledge is available to help a\nprediction task. We aim at deriving methods following these paradigms for use\nin autotuning, where the goal is to find the optimal performance parameters of\nan application treated as a black-box function. We show comparative results\nwith state-of-the-art autotuning techniques. For instance, we observe an\naverage $1.5x$ improvement of the application runtime compared to the OpenTuner\nand HpBandSter autotuners. We explain how our approaches can be more suitable\nthan some state-of-the-art autotuners for the tuning of any application in\ngeneral and of expensive exascale applications in particular.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 23:14:54 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Sid-Lakhdar", "Wissam M.", ""], ["Aznaveh", "Mohsen Mahmoudi", ""], ["Li", "Xiaoye S.", ""], ["Demmel", "James W.", ""]]}, {"id": "1908.05855", "submitter": "Masatoshi Hanai", "authors": "Masatoshi Hanai, Toyotaro Suzumura, Wen Jun Tan, Elvis Liu, Georgios\n  Theodoropoulos and Wentong Cai", "title": "Distributed Edge Partitioning for Trillion-edge Graphs", "comments": "VLDB 2020, Code in http://www.masahanai.jp/DistributedNE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Distributed Neighbor Expansion (Distributed NE), a parallel and\ndistributed graph partitioning method that can scale to trillion-edge graphs\nwhile providing high partitioning quality. Distributed NE is based on a new\nheuristic, called parallel expansion, where each partition is constructed in\nparallel by greedily expanding its edge set from a single vertex in such a way\nthat the increase of the vertex cuts becomes local minimal. We theoretically\nprove that the proposed method has the upper bound in the partitioning quality.\nThe empirical evaluation with various graphs shows that the proposed method\nproduces higher-quality partitions than the state-of-the-art distributed graph\npartitioning algorithms. The performance evaluation shows that the space\nefficiency of the proposed method is an order-of-magnitude better than the\nexisting algorithms, keeping its time efficiency comparable. As a result,\nDistributed NE can partition a trillion-edge graph using only 256 machines\nwithin 70 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 05:52:19 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 14:18:04 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Hanai", "Masatoshi", ""], ["Suzumura", "Toyotaro", ""], ["Tan", "Wen Jun", ""], ["Liu", "Elvis", ""], ["Theodoropoulos", "Georgios", ""], ["Cai", "Wentong", ""]]}, {"id": "1908.05891", "submitter": "Xin Yao", "authors": "Xin Yao, Tianchi Huang, Chenglei Wu, Rui-Xiao Zhang, Lifeng Sun", "title": "Federated Learning with Additional Mechanisms on Clients to Reduce\n  Communication Costs", "comments": "This is a combination version of our papers in VCIP 2018 and ICIP\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) enables on-device training over distributed networks\nconsisting of a massive amount of modern smart devices, such as smartphones and\nIoT (Internet of Things) devices. However, the leading optimization algorithm\nin such settings, i.e., federated averaging (FedAvg), suffers from heavy\ncommunication costs and the inevitable performance drop, especially when the\nlocal data is distributed in a non-IID way. To alleviate this problem, we\npropose two potential solutions by introducing additional mechanisms to the\non-device training.\n  The first (FedMMD) is adopting a two-stream model with the MMD (Maximum Mean\nDiscrepancy) constraint instead of a single model in vanilla FedAvg to be\ntrained on devices. Experiments show that the proposed method outperforms\nbaselines, especially in non-IID FL settings, with a reduction of more than 20%\nin required communication rounds.\n  The second is FL with feature fusion (FedFusion). By aggregating the features\nfrom both the local and global models, we achieve higher accuracy at fewer\ncommunication costs. Furthermore, the feature fusion modules offer better\ninitialization for newly incoming clients and thus speed up the process of\nconvergence. Experiments in popular FL scenarios show that our FedFusion\noutperforms baselines in both accuracy and generalization ability while\nreducing the number of required communication rounds by more than 60%.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 08:51:27 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 16:33:58 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yao", "Xin", ""], ["Huang", "Tianchi", ""], ["Wu", "Chenglei", ""], ["Zhang", "Rui-Xiao", ""], ["Sun", "Lifeng", ""]]}, {"id": "1908.05936", "submitter": "Patrick Stotko", "authors": "Patrick Stotko", "title": "stdgpu: Efficient STL-like Data Structures on the GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous advances in parallel computing and graphics hardware opened up\nseveral novel real-time GPU applications in the fields of computer vision,\ncomputer graphics as well as augmented reality (AR) and virtual reality (VR).\nAlthough these applications built upon established open-source frameworks that\nprovide highly optimized algorithms, they often come with custom self-written\ndata structures to manage the underlying data. In this work, we present stdgpu,\nan open-source library which defines several generic GPU data structures for\nfast and reliable data management. Rather than abandoning previous established\nframeworks, our library aims to extend them, therefore bridging the gap between\nCPU and GPU computing. This way, it provides clean and familiar interfaces and\nintegrates seamlessly into new as well as existing projects. We hope to foster\nfurther developments towards unified CPU and GPU computing and welcome\ncontributions from the community.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 11:37:42 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Stotko", "Patrick", ""]]}, {"id": "1908.05940", "submitter": "Tim Soethout", "authors": "Tim Soethout (ING Bank / Centrum Wiskunde & Informatica (CWI),\n  Netherlands), Tijs van der Storm (Centrum Wiskunde & Informatica (CWI) /\n  Universiteit Groningen, Netherlands), Jurgen Vinju (Centrum Wiskunde &\n  Informatica (CWI) / TU Eindhoven, Netherlands)", "title": "Path-Sensitive Atomic Commit: Local Coordination Avoidance for\n  Distributed Transactions", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 1, Article 3", "doi": "10.22152/programming-journal.org/2021/5/3", "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Concurrent objects with asynchronous messaging are an increasingly\npopular way to structure highly available, high performance, large-scale\nsoftware systems. To ensure data-consistency and support synchronization\nbetween objects such systems often use distributed transactions with Two-Phase\nLocking (2PL) for concurrency control and Two-Phase commit (2PC) as atomic\ncommitment protocol. Inquiry In highly available, high-throughput systems, such\nas large banking infrastructure, however, 2PL becomes a bottleneck when objects\nare highly contended, when an object is queuing a lot of messages because of\nlocking.\n  Approach: In this paper we introduce Path-Sensitive Atomic Commit (PSAC) to\naddress this situation. We start from message handlers (or methods), which are\ndecorated with pre- and post-conditions, describing their guards and effect.\n  Knowledge: This allows the PSAC lock mechanism to check whether the effect of\ntwo incoming messages at the same time are independent, and to avoid locking if\nthis is the case. As a result, more messages are directly accepted or rejected,\nand higher overall throughput is obtained.\n  Grounding: We have implemented PSAC for a state machine-based DSL called\nRebel, on top of a runtime based on the Akka actor framework. Our performance\nevaluation shows that PSAC exhibits the same scalability and latency\ncharacteristics as standard 2PL/2PC, and obtains up to 1.8 times median higher\nthroughput in congested scenarios.\n  Importance: We believe PSAC is a step towards enabling organizations to build\nscalable distributed applications, even if their consistency requirements are\nnot embarrassingly parallel.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 11:52:05 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 18:30:12 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Soethout", "Tim", "", "ING Bank / Centrum Wiskunde & Informatica"], ["van der Storm", "Tijs", "", "Centrum Wiskunde & Informatica"], ["Vinju", "Jurgen", "", "Centrum Wiskunde &\n  Informatica"]]}, {"id": "1908.05944", "submitter": "Talha Bin Masood", "authors": "Talha Bin Masood, Tathagata Ray and Vijay Natarajan", "title": "Parallel Computation of Alpha Complex for Biomolecules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alpha complex, a subset of the Delaunay triangulation, has been\nextensively used as the underlying representation for biomolecular structures.\nWe propose a GPU-based parallel algorithm for the computation of the alpha\ncomplex, which exploits the knowledge of typical spatial distribution and sizes\nof atoms in a biomolecule. Unlike existing methods, this algorithm does not\nrequire prior construction of the Delaunay triangulation. The algorithm\ncomputes the alpha complex in two stages. The first stage proceeds in a\nbottom-up fashion and computes a superset of the edges, triangles, and\ntetrahedra belonging to the alpha complex. The false positives from this\nestimation stage are removed in a subsequent pruning stage to obtain the\ncorrect alpha complex. Computational experiments on several biomolecules\ndemonstrate the superior performance of the algorithm, up to a factor of 50\nwhen compared to existing methods that are optimized for biomolecules.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 12:16:12 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 09:24:49 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Masood", "Talha Bin", ""], ["Ray", "Tathagata", ""], ["Natarajan", "Vijay", ""]]}, {"id": "1908.06043", "submitter": "David Williams-Young", "authors": "David B. Williams-Young and Paul G. Beckman and Chao Yang", "title": "A Shift Selection Strategy for Parallel Shift-Invert Spectrum Slicing in\n  Symmetric Self-Consistent Eigenvalue Computation", "comments": "31 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central importance of large scale eigenvalue problems in scientific\ncomputation necessitates the development of massively parallel algorithms for\ntheir solution. Recent advances in dense numerical linear algebra have enabled\nthe routine treatment of eigenvalue problems with dimensions on the order of\nhundreds of thousands on the world's largest supercomputers. In cases where\ndense treatments are not feasible, Krylov subspace methods offer an attractive\nalternative due to the fact that they do not require storage of the problem\nmatrices. However, demonstration of scalability of either of these classes of\neigenvalue algorithms on computing architectures capable of expressing massive\nparallelism is non-trivial due to communication requirements and serial\nbottlenecks, respectively. In this work, we introduce the SISLICE method: a\nparallel shift-invert algorithm for the solution of the symmetric\nself-consistent field (SCF) eigenvalue problem. The SISLICE method drastically\nreduces the communication requirement of current parallel shift-invert\neigenvalue algorithms through various shift selection and migration techniques\nbased on density of states estimation and k-means clustering, respectively.\nThis work demonstrates the robustness and parallel performance of the SISLICE\nmethod on a representative set of SCF eigenvalue problems and outlines research\ndirections which will be explored in future work.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:06:24 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 00:26:46 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Williams-Young", "David B.", ""], ["Beckman", "Paul G.", ""], ["Yang", "Chao", ""]]}, {"id": "1908.06086", "submitter": "Zeyad Al-Odat", "authors": "Zeyad A. Al-Odat, Sudarshan K. Srinivasan, Eman M. Al-Qtiemat, Sana\n  Shuja", "title": "A Reliable IoT-Based Embedded Health Care System for Diabetic Patients", "comments": "11 pages. arXiv admin note: text overlap with arXiv:1812.02357", "journal-ref": "International Journal On Advances in Internet Technology 2019", "doi": null, "report-no": "1942-2652", "categories": "cs.CY cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a reliable health care system for diabetic patients\nbased on the Internet of Things technology. A diabetic health care system with\na hardware implementation is presented. The proposed work employs Alaris 8100\ninfusion pump, Keil LPC-1768 board, and IoT-cloud to monitor the diabetic\npatients. The security of diabetic data over the cloud and the communication\nchannel between health care system components are considered as part of the\nmain contributions of this work. Moreover, an easy way to control and monitor\nthe diabetic insulin pump is implemented. The \\mbox{patient\\textquotesingle s}\nrecords are stored in the cloud using the Keil board that is connected to the\ninfusion pump. The reliability of the proposed scheme is accomplished by\ntesting the system for five performance characteristics (availability,\nconfidentiality, integrity, authentication, and authorization). The Kiel board\nis embedded with Ethernet port and Cortex-M3 micro-controller that controls the\ninsulin infusion pump. The secure hash algorithm and secure socket shell are\nemployed to achieve the reliability components of the proposed scheme. The\nresults show that the proposed design is reliable, secure and authentic\naccording to different test experiments and a case study of the Markov model.\nMoreover, a 99.3\\% availability probability has been achieved after analyzing\nthe case study.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 04:29:52 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Al-Odat", "Zeyad A.", ""], ["Srinivasan", "Sudarshan K.", ""], ["Al-Qtiemat", "Eman M.", ""], ["Shuja", "Sana", ""]]}, {"id": "1908.06089", "submitter": "Gianmarco Mengaldo", "authors": "Gianmarco Mengaldo", "title": "Batch 1: Definition of several Weather & Climate Dwarfs", "comments": "86 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying weather & climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  This deliverable contains the description of the characteristics of the\nweather & climate dwarfs that form key functional components of prediction\nmodels in terms of the science that they encapsulate and in terms of\ncomputational cost they impose on the forecast production. The ESCAPE work flow\nbetween work packages centres on these dwarfs and hence their selection, their\nperformance assessment, code adaptation and optimization is crucial for the\nsuccess of the project. At this stage of ESCAPE, a selection of established and\nnew dwarfs has been made, their documentation been compiled and the software\nbeen made available on the software exchange platform. The selection of dwarfs\nwill be extended throughout the course of the project (see Deliverable D1.2).\n  The current selection includes the spectral transforms, the cloud\nmicrophysics scheme, two and three-dimensional elliptic solvers, a bi-Fourier\nspectral transform, an interpolation needed for the semi-Lagrangian advection\nscheme and a first version of the semi-Lagrangian advection scheme itself. This\ndeliverable includes their scientific description and the guidance for\ninstallation, execution and testing.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 13:05:55 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mengaldo", "Gianmarco", ""]]}, {"id": "1908.06091", "submitter": "Willem Deconinck", "authors": "Willem Deconinck", "title": "Development of Atlas, a flexible data structure framework", "comments": "60 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  In this deliverable report, we present Atlas, a new software library that is\ncurrently being developed at the European Centre for Medium-Range Weather\nForecasts (ECMWF), with the scope of handling data structures required for NWP\napplications in a flexible and massively parallel way. Atlas provides a\nversatile framework for the future development of efficient NWP and climate\napplications on emerging HPC architectures. The applications range from full\nEarth system models, to specific tools required for post-processing weather\nforecast products.\n  Atlas provides data structures for building various numerical strategies to\nsolve equations on the sphere or limited area's on the sphere. These data\nstructures may contain a distribution of points (grid) and, possibly, a\ncomposition of elements (mesh), required to implement the numerical operations\nrequired. Atlas can also represent a given field within a specific spatial\nprojection. Atlas is capable of mapping fields between different grids as part\nof pre- and post-processing stages or as part of coupling processes whose\nrespective fields are discretised on different grids or meshes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:03:07 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Deconinck", "Willem", ""]]}, {"id": "1908.06093", "submitter": "Alastair McKinstry", "authors": "Alastair McKinstry", "title": "Additional key features required for different directives based porting\n  approaches", "comments": "16 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  This report summarizes key features required for OpenMP and OpenACC\ndirectives based on experience in the ESCAPE project. For OpenMP, the latest\npublic draft standard, 5.0, contains the deep copy and multi-level memory\nfeatures desired; for OpenACC, Technical Report 16 summarizes ongoing\ndiscussions beyond the Standard version 2.6. This document includes a summary\nof our recommendations on this approach.\n  Additional work is also desirable in coordinating the runtime and debugging\nwhen both OpenACC and OpenMP directives are used; in particular the\ninteroperability of the new OPDT debugging interface for OpenMP and its\nsemantics in a mixed-directive program.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:22:23 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["McKinstry", "Alastair", ""]]}, {"id": "1908.06094", "submitter": "Carlos Osuna", "authors": "Carlos Osuna", "title": "Report on the performance portability demonstrated for the relevant\n  Weather & Climate Dwarfs", "comments": "32 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  This deliverable provides an evaluation of the work performed within ESCAPE\nto port different dwarfs to accelerators, using different programming models. A\nkey metric of the evaluation is the performance portability of the resulting\nporting efforts. Portability means that a single source code containing the\nnumerical operators can be compiled and run in multiple architectures, while\nperformance portability additionally requires that the single source code runs\nefficiently in all the different architectures.\n  As results of other deliverables like D2.1, D2.4 ESCAPE provides a collection\nof dwarfs ported to different computing architectures like traditional CPUs,\nIntel XeonPhi and NVIDIA GPUs. Additionally D3.3 went through an optimization\nprocess to obtain efficient and energy efficient dwarfs.\n  In this deliverable we present a review of the different programming models\nemployed and their use to port various dwarfs of ESCAPE. A final evaluation of\nthe different approaches based on different metrics like performance\nportability, readability of the numerical methods, efforts to port a dwarf and\nefficiency of the implementation obtained is reported.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:50:59 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Osuna", "Carlos", ""]]}, {"id": "1908.06095", "submitter": "Erwan Raffin", "authors": "Cyril Mazauric, Erwan Raffin, David Guibert", "title": "Recommendations and specifications for data scope analysis tools", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  In today's computer architectures, moving data is considerably more time- and\nenergy consuming than computing on this data. One of the key performance\noptimizations for any application is therefore to minimize data motion and\nmaximize data reuse. Especially on modern supercomputers with very complex and\ndeep memory hierarchies, it is mandatory to take data locality into account.\nEspecially when targeting accelerators with directive systems like OpenACC or\nOpenMP, identifying data scope, access type and data reuse are critical to\nminimize the data transfers from and to the accelerator. Unfortunately,\nmanually identifying data locality information in complex code bases can be a\ntime consuming task and tool support is therefore desirable.\n  In this report we summarize the results of a survey of currently available\ntools that support software developers and performance engineers with data\nlocality information in complex code bases like numerical weather prediction\n(NWP) or climate simulation applications. Based on the survey results we then\nrecommend a tool and specify some extensions for a tool to solve the problems\nencountered in an NWP application.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:10:03 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mazauric", "Cyril", ""], ["Raffin", "Erwan", ""], ["Guibert", "David", ""]]}, {"id": "1908.06096", "submitter": "Erwan Raffin", "authors": "Cyril Mazauric, Erwan Raffin, Xavier Vigouroux, David Guibert, Alex\n  Macfaden, Jacob Poulsen, Per Berg, Alan Gray, Peter Messmer", "title": "Performance report and optimized implementation of Weather & Climate\n  Dwarfs on GPU, MIC and Optalysys Optical Processor", "comments": "75 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  Here we summarize the work performed on optimizations of the dwarfs on CPUs,\nXeon Phi, GPUs and on the Optalysys optical processor. We limit ourselves to a\nsubset of the dwarf configurations and to problem sizes small enough to execute\non a single node. Also, we use time-to-solution as the main performance metric.\nMulti-node optimizations of the dwarfs and energy-specific optimizations are\nbeyond the scope of this report and will be described in Deliverable D3.4. To\ncover the important algorithmic motifs we picked dwarfs related to the\ndynamical core as well as column physics. Specifically, we focused on the\nformulation relevant to spectral codes like ECMWF's IFS code.\n  The main findings of this report are: (a) Acceleration of 1.1x - 2.5x of the\ndwarfs on CPU based systems using compiler directives, (b) order of magnitude\nacceleration of the dwarfs on GPUs (23x for spectral transform, 9x for MPDATA)\nusing data locality optimizations and (c) demonstrated feasibility of a\nspectral transform in a purely optical fashion.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:28:50 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mazauric", "Cyril", ""], ["Raffin", "Erwan", ""], ["Vigouroux", "Xavier", ""], ["Guibert", "David", ""], ["Macfaden", "Alex", ""], ["Poulsen", "Jacob", ""], ["Berg", "Per", ""], ["Gray", "Alan", ""], ["Messmer", "Peter", ""]]}, {"id": "1908.06097", "submitter": "Erwan Raffin", "authors": "Louis Douriez, Alan Gray, David Guibert, Peter Messmer, Erwan Raffin", "title": "Performance report and optimized implementations of Weather & Climate\n  dwarfs on multi-node systems", "comments": "35 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  Here we summarize the work performed on optimizations of the dwarfs focusing\non CPU multi-nodes and multi-GPUs. We limit ourselves to a subset of the dwarf\nconfigurations chosen by the consortium. Intra-node optimizations of the dwarfs\nand energy-specific optimizations have been described in Deliverable D3.3. To\ncover the important algorithmic motifs we picked dwarfs related to the\ndynamical core as well as column physics. Specifically, we focused on the\nformulation relevant to spectral codes like ECMWF's IFS code.\n  The main findings of this report are: (a) Up-to 30% performance gain with CPU\nbased multi-node systems compared to optimized version of dwarfs from task 3.3\n(see D3.3), (b) up to 10X performance gain on multiple GPUs from optimizations\nto keep data resident on the GPU and enable fast inter-GPU communication\nmechanisms, and (c) multi-GPU systems which feature a high-bandwidth all-to-all\ninterconnect topology with NVLink/NVSwitch hardware are particularly well\nsuited to the algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:48:45 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Douriez", "Louis", ""], ["Gray", "Alan", ""], ["Guibert", "David", ""], ["Messmer", "Peter", ""], ["Raffin", "Erwan", ""]]}, {"id": "1908.06098", "submitter": "Micha{\\l} Kulczewski", "authors": "Micha{\\l} Kulczewski, Marek B{\\l}a\\.zewicz, Sebastian Ciesielski", "title": "Projections of achievable performance for Weather & Climate Dwarfs, and\n  for entire NWP applications, on hybrid architectures", "comments": "63 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  This deliverable contains the description of the performance and energy\nmodels for the selected Weather & Climate dwarfs for different hardware\narchitectures, multinode with GPU accelerators in particular. Presented\nperformance models are extension to model provided in Deliverable 3.2. With\nsome further enhancements, they are incorporated in the DCworms simulator. In\nparticular, extended models allow to predict computational and energy\nperformance on different architectures: single and multinodes, equipped with\nCPUs and GPUs accelerators. This allows to provide feasible performance\nprojection at system scale.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:56:37 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kulczewski", "Micha\u0142", ""], ["B\u0142a\u017cewicz", "Marek", ""], ["Ciesielski", "Sebastian", ""]]}, {"id": "1908.06115", "submitter": "Daan Degrauwe", "authors": "Joris Van Bever, Alex McFaden, Zbigniew Piotrowski, Daan Degrauwe", "title": "Report on energy-efficiency evaluation of several NWP model\n  configurations", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  In this deliverable we report on energy consumption measurements of a number\nof NWP models/dwarfs on the Intel E5-2697v4 processor. The chosen energy\nmetrics and energy measurement methods are documented. Energy measurements are\nperformed on the Bi-Fourier dwarf (BiFFT), the Acraneb dwarf, the ALARO 2.5 km\nLocal Area Model reference configuration (B\\'enard et al. 2010, Bubnova et al.\n1995) and on the COSMO-EULAG Local Area Model reference configuration\n(Piotrowski et al. 2018). The results show a U-shaped dependence of the\nconsumed energy on the wall-clock time performance. This shape can be explained\nfrom the dependence of the average power of the compute nodes on the total\nnumber of cores used.\n  We compare the energy consumption of the BiFFT dwarf on the E5-2697v4\nprocessor to that on the Optalysys optical processors. The latter are found to\nbe much less energy costly, but at the same time it is also the only metric\nwhere they outperform the classical CPU. They are non-competitive as far as\nwall-clock time and especially numerical precision are concerned.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:05:34 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Van Bever", "Joris", ""], ["McFaden", "Alex", ""], ["Piotrowski", "Zbigniew", ""], ["Degrauwe", "Daan", ""]]}, {"id": "1908.06116", "submitter": "Daan Degrauwe", "authors": "Joris Van Bever, Geert Smet, Daan Degrauwe", "title": "Report on workflow analysis for specific LAM applications", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  In this deliverable we focus on the RMI-EPS ensemble prediction suite. We\nfirst provide a detailed report on the workflow of the suite in which 5 main\ncategories of jobs are defined; pre-processing, lateral boundary conditions\n(LBCs), data assimilation, forecast and post-processing.\n  Combined Energy and wall-clock time measurements of the entire RMI-EPS suite\nwere performed. They indicate that the wall-clock times are relatively spread\nbetween the various defined job categories, with the forecast accounting for\nthe largest fraction at about 35%. As far as energy consumption is concerned,\nthe forecast part dwarfs everything else and is responsible for up to 99% of\nthe total energy consumption. This means that energy optimizations for the\nforecast part will translate almost proportionally into optimizations of the\nwhole suite, while the maximum theoretical speed-up due to forecast\noptimizations cannot exceed a factor of about 3/2. Therefore, in terms of\nenergy consumption, optimizations should first focus on the forecast part. For\nwall-clock time performance gains, however, optimizations (and possibly\nadditional dwarfs) can be considered for the categories outside of the forecast\npart.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:11:12 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Van Bever", "Joris", ""], ["Smet", "Geert", ""], ["Degrauwe", "Daan", ""]]}, {"id": "1908.06270", "submitter": "Sebastian Brandt", "authors": "Sebastian Brandt, Yannic Maus, Jara Uitto", "title": "A Sharp Threshold Phenomenon for the Distributed Complexity of the\n  Lov\\'asz Local Lemma", "comments": "appeared at PODC 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lov\\'{a}sz Local Lemma (LLL) says that, given a set of bad events that\ndepend on the values of some random variables and where each event happens with\nprobability at most $p$ and depends on at most $d$ other events, there is an\nassignment of the variables that avoids all bad events if the LLL criterion\n$ep(d+1)<1$ is satisfied.\n  In this paper, we study the dependency of the distributed complexity of the\nLLL problem on the chosen LLL criterion. We show that for the fundamental case\nof each random variable of the considered LLL instance being associated with an\nedge of the input graph, that is, each random variable influences at most two\nevents, a sharp threshold phenomenon occurs at $p = 2^{-d}$: we provide a\nsimple deterministic (!) algorithm that matches a known $\\Omega(\\log^* n)$\nlower bound in bounded degree graphs, if $p < 2^{-d}$, whereas for $p \\geq\n2^{-d}$, a known $\\Omega(\\log \\log n)$ randomized and a known $\\Omega(\\log n)$\ndeterministic lower bounds hold.\n  In many applications variables affect more than two events; our main\ncontribution is to extend our algorithm to the case where random variables\ninfluence at most three different bad events. We show that, surprisingly, the\nsharp threshold occurs at the exact same spot, providing evidence for our\nconjecture that this phenomenon always occurs at $p = 2^{-d}$, independent of\nthe number $r$ of events that are affected by a variable. Almost all steps of\nthe proof framework we provide for the case $r=3$ extend directly to the case\nof arbitrary $r$; consequently, our approach serves as a step towards\ncharacterizing the complexity of the LLL under different exponential criteria.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 09:24:35 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 10:47:56 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Brandt", "Sebastian", ""], ["Maus", "Yannic", ""], ["Uitto", "Jara", ""]]}, {"id": "1908.06394", "submitter": "Jieyi Long", "authors": "Jieyi Long", "title": "Nakamoto Consensus with Verifiable Delay Puzzle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new consensus protocol based on verifiable delay\nfunction. First, we introduce the concept of verifiable delay puzzle (VDP),\nwhich resembles the hashing puzzle used in the PoW mechanism but can only be\nsolved sequentially. We then present a VDP implementation based on the\ncontinuous verifiable delay function. Further, we show that VDP can be combined\nwith the Nakamoto consensus in a proof-of-stake/proof-of-delay hybrid protocol.\nWe analyze the persistence and liveness of the protocol, and show that compared\nto PoW, our proposal consumes much less energy; compared to BFT leader-election\nbased consensus algorithms, our proposal achieves better resistance to\nlong-range attacks and DoS attacks targeting the block proposers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 08:11:21 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 11:58:25 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 23:05:54 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 05:01:37 GMT"}, {"version": "v5", "created": "Tue, 20 Jul 2021 15:18:18 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Long", "Jieyi", ""]]}, {"id": "1908.06405", "submitter": "Zhaoming Yin", "authors": "Zhaoming Yin, Anbang Ruan, Ming Wei, Huafeng Li, Kai Yuan, Junqing\n  Wang, Yahui Wang, Ming Ni, Andrew Martin", "title": "StreamNet: A DAG System with Streaming Graph Computing", "comments": "FTC 2020 - Future Technologies Conference 2020 5-6 November 2020 |\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve high throughput in the POW based blockchain systems, researchers\nproposed a series of methods, and DAG is one of the most active and promising\nfields. We designed and implemented the StreamNet, aiming to engineer a\nscalable and endurable DAG system. When attaching a new block in the DAG, only\ntwo tips are selected. One is the parent tip whose definition is the same as in\nConflux[1]; another is using Markov Chain Monte Carlo (MCMC) technique by which\nthe definition is the same as IOTA [2]. We infer a pivotal chain along the path\nof each epoch in the graph, and a total order of the graph could be calculated\nwithout a centralized authority. To scale up, we leveraged the graph streaming\nproperty; high transaction validation speed will be achieved even if the DAG is\ngrowing. To scale out, we designed the direct signal gossip protocol to help\ndisseminate block updates in the network, such that messages can be passed in\nthe network more efficiently. We implemented our system based on IOTA's\nreference code (IRI) and ran comprehensive experiments over the different sizes\nof clusters of multiple network topologies.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 09:25:26 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 01:48:16 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Yin", "Zhaoming", ""], ["Ruan", "Anbang", ""], ["Wei", "Ming", ""], ["Li", "Huafeng", ""], ["Yuan", "Kai", ""], ["Wang", "Junqing", ""], ["Wang", "Yahui", ""], ["Ni", "Ming", ""], ["Martin", "Andrew", ""]]}, {"id": "1908.06414", "submitter": "Ali Dorri", "authors": "Fatemeh MohammadZadeh and Seyed Ali Mirghasemi and Ali Dorri and\n  HamidReza Ahmadifar", "title": "DMap: A Distributed Blockchain-based Framework for Online Mapping in\n  Smart City", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart cities are growing significantly due to the growth of smart connected\nvehicles and Internet of Things (IoT) where a wide range of devices are\nconnected to share data. Online mapping is one of the fundamental services\noffered in smart cities which enables the vehicle owners to find shortest or\nfastest direction toward a destination thus reduces travel cost and air\npollution. However, existing online mapping services rely on centralized\nservers, e.g., Google, which collect data from users to offer service. This\nmethod is unlikely to scale with growth in the smart city participants and\nintroduces privacy concerns and data wall where data of the users is managed by\nbig companies. To address these challenges, in this paper we introduce DMap, a\nblockchain-based platform where the users can share data in an anonymous\ndistributed manner with service providers. To the best of our knowledge, DMap\nis the first distributed blockchain-based solution for online mapping. To\nimprove the scalability of the blockchain, we propose to use edge-computing\nalong with blockchain. To protect against malicious vehicles that may inject\nfalse data, we define a reputation system where the collected data of the\nvehicles is verified by monitoring the neighbor data. We introduce data\nmarketplace where users can trade their data to address data wall challenge.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 10:02:53 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["MohammadZadeh", "Fatemeh", ""], ["Mirghasemi", "Seyed Ali", ""], ["Dorri", "Ali", ""], ["Ahmadifar", "HamidReza", ""]]}, {"id": "1908.06503", "submitter": "Ivy Peng", "authors": "Ivy B. Peng, Maya B. Gokhale, Eric W. Green", "title": "System Evaluation of the Intel Optane Byte-addressable NVM", "comments": null, "journal-ref": "In Proceedings of the International Symposium on Memory Systems,\n  2019", "doi": "10.1145/3357526.3357568", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byte-addressable non-volatile memory (NVM) features high density, DRAM\ncomparable performance, and persistence. These characteristics position NVM as\na promising new tier in the memory hierarchy. Nevertheless, NVM has asymmetric\nread and write performance, and considerably higher write energy than DRAM. Our\nwork provides an in-depth evaluation of the first commercially available\nbyte-addressable NVM -- the Intel Optane DC persistent memory. The first part\nof our study quantifies the latency, bandwidth, power efficiency, and energy\nconsumption under eight memory configurations. We also evaluate the real impact\non in-memory graph processing workloads. Our results show that augmenting NVM\nwith DRAM is essential, and the combination can effectively bridge the\nperformance gap and provide reasonable performance with higher capacity. We\nalso identify NUMA-related performance characteristics for accesses to memory\non a remote socket. In the second part, we employ two fine-grained allocation\npolicies to control traffic distribution between DRAM and NVM. Our results show\nthat bandwidth spilling between DRAM and NVM could provide 2.0x bandwidth and\nenable $20\\%$ larger problems than using DRAM as a cache. Also, write isolation\nbetween DRAM and NVM could save up to 3.9x energy and improves bandwidth by\n3.1x compared to DRAM-cached NVM. We establish a roofline model to explore\npower and energy efficiency at various distributions of read-only traffic. Our\nresults show that NVM requires 1.8x lower power than DRAM for data-intensive\nworkloads. Overall, applications can significantly optimize performance and\npower efficiency by adapting traffic distribution to NVM and DRAM through\nmemory configurations and fine-grained policies to fully exploit the new memory\ndevice.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 19:29:29 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Peng", "Ivy B.", ""], ["Gokhale", "Maya B.", ""], ["Green", "Eric W.", ""]]}, {"id": "1908.06649", "submitter": "Francesco Silvestri", "authors": "Rezaul Chowdhury and Francesco Silvestri and Flavio Vella", "title": "A Computational Model for Tensor Core Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To respond to the need of efficient training and inference of deep neural\nnetworks, a plethora of domain-specific hardware architectures have been\nintroduced, such as Google Tensor Processing Units and NVIDIA Tensor Cores. A\ncommon feature of these architectures is a hardware circuit for efficiently\ncomputing a dense matrix multiplication of a given small size. In order to\nbroaden the class of algorithms that exploit these systems, we propose a\ncomputational model, named the TCU model, that captures the ability to natively\nmultiply small matrices. We then use the TCU model for designing fast\nalgorithms for several problems, including matrix operations (dense and sparse\nmultiplication, Gaussian Elimination), graph algorithms (transitive closure,\nall pairs shortest distances), Discrete Fourier Transform, stencil\ncomputations, integer multiplication, and polynomial evaluation. We finally\nhighlight a relation between the TCU model and the external memory model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 08:59:46 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 07:25:17 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chowdhury", "Rezaul", ""], ["Silvestri", "Francesco", ""], ["Vella", "Flavio", ""]]}, {"id": "1908.06684", "submitter": "Thorsten Wissmann", "authors": "Eric Goubault, Samuel Mimram", "title": "Directed Homotopy in Non-Positively Curved Spaces", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 3 (July 13,\n  2020) lmcs:6634", "doi": "10.23638/LMCS-16(3:4)2020", "report-no": null, "categories": "cs.LO cs.DC math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A semantics of concurrent programs can be given using precubical sets, in\norder to study (higher) commutations between the actions, thus encoding the\n\"geometry\" of the space of possible executions of the program. Here, we study\nthe particular case of programs using only mutexes, which are the most widely\nused synchronization primitive. We show that in this case, the resulting\nprograms have non-positive curvature, a notion that we introduce and study here\nfor precubical sets, and can be thought of as an algebraic analogue of the\nwell-known one for metric spaces. Using this it, as well as categorical\nrewriting techniques, we are then able to show that directed and non-directed\nhomotopy coincide for directed paths in these precubical sets. Finally, we\nstudy the geometric realization of precubical sets in metric spaces, to show\nthat our conditions on precubical sets actually coincide with those for metric\nspaces. Since the category of metric spaces is not cocomplete, we are lead to\nwork with generalized metric spaces and study some of their properties.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 10:32:53 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 18:10:49 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 07:33:38 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 10:09:58 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Goubault", "Eric", ""], ["Mimram", "Samuel", ""]]}, {"id": "1908.06909", "submitter": "Ander Biguri", "authors": "Ander Biguri, Hossein Towsyfyan, Richard Boardman, Thomas Blumensath", "title": "Numerically robust tetrahedron-based tomographic forward and backward\n  projectors on parallel architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  X-ray tomographic reconstruction typically uses voxel basis functions to\nrepresent volumetric images. Due to the structure in voxel basis\nrepresentations, efficient ray-tracing methods exist allowing fast, GPU\naccelerated implementations. Tetrahedral mesh basis functions are a valuable\nalternative to voxel based image representations as they provide flexible,\ninhomogeneous partitionings which can be used to provide reconstructions with\nreduced numbers of elements or with arbitrarily fine object surface\nrepresentations. We thus present a robust parallelizable ray-tracing method for\nvolumetric tetrahedral domains developed specifically for Computed Tomography\nimage reconstruction. Tomographic image reconstruction requires algorithms that\nare robust to numerical errors in floating point arithmetic whilst typical data\nsizes encountered in tomography require the algorithm to be parallelisable in\nGPUs which leads to additional constraints on algorithm choices. Based on these\nconsiderations, this article presents numerical solutions to the design of\nefficient ray-tracing algorithms for the projection and backprojection\noperations. Initial reconstruction results using CAD data to define a\ntriangulation of the domain demonstrate the advantages of our method and\ncontrast tetrahedral mesh based reconstructions to voxel based methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:10:55 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Biguri", "Ander", ""], ["Towsyfyan", "Hossein", ""], ["Boardman", "Richard", ""], ["Blumensath", "Thomas", ""]]}, {"id": "1908.06936", "submitter": "Sameh Abdulah", "authors": "Sameh Abdulah, Yuxiao Li, Jian Cao, Hatem Ltaief, David E. Keyes, Marc\n  G. Genton, Ying Sun", "title": "ExaGeoStatR: A Package for Large-Scale Geostatistics in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel computing in Gaussian process calculation becomes a necessity for\navoiding computational and memory restrictions associated with Geostatistics\napplications. The evaluation of the Gaussian log-likelihood function requires\nO(n^2) storage and O(n^3) operations where n is the number of geographical\nlocations. In this paper, we present ExaGeoStatR, a package for large-scale\nGeostatistics in R that supports parallel computation of the maximum likelihood\nfunction on shared memory, GPU, and distributed systems. The parallelization\ndepends on breaking down the numerical linear algebra operations into a set of\ntasks and rendering them for a task-based programming model. ExaGeoStatR\nsupports several maximum likelihood computation variants such as exact,\nDiagonal Super Tile (DST), and Tile Low-Rank (TLR) approximation besides\nproviding a tool to generate large-scale synthetic datasets which can be used\nto test and compare different approximations methods. The package can be used\ndirectly through the R environment without any C, CUDA, or MPIknowledge. Here,\nwe demonstrate the ExaGeoStatR package by illustrating its implementation\ndetails, analyzing its performance on various parallel architectures, and\nassessing its accuracy using both synthetic datasets and a sea surface\ntemperature dataset. The performance evaluation involves spatial datasets with\nup to 250K observations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:28:17 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Abdulah", "Sameh", ""], ["Li", "Yuxiao", ""], ["Cao", "Jian", ""], ["Ltaief", "Hatem", ""], ["Keyes", "David E.", ""], ["Genton", "Marc G.", ""], ["Sun", "Ying", ""]]}, {"id": "1908.07038", "submitter": "Willem Deconinck", "authors": "Willem Deconinck", "title": "Public release of Atlas under an open source license, which is\n  accelerator enabled and has improved interoperability features", "comments": "29 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1908.06091", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying Weather & Climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  Atlas has been presented in deliverable D1.3. With this deliverable D2.3, a\nfirst version of the Atlas software libraries is publicly released with a\npermissive open-source license. The software is freely available for download,\nand contains a user guide and installation instructions.\n  The Atlas libraries have been carefully designed with the user's perspective\nin mind. Even though Atlas is mainly coded in C++, an equivalent Fortran\ninterface is presented without additional runtime overhead. The Fortran\ninterfaces are provided to accommodate existing NWP and climate models that\ntypically consist of Fortran subroutines. The mixed Fortran/C++ design enhances\ninteroperability between NWP and climate models and novel data management\ntechniques. Atlas provides interoperability with accelerator hardware, and can\nserve as foundation to support higher level abstractions as used in domain\nspecific languages.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:35:56 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Deconinck", "Willem", ""]]}, {"id": "1908.07040", "submitter": "Andreas M\\\"uller", "authors": "Andreas M\\\"uller, Mike Gillard, Kristian Pagh Nielsen, Zbigniew\n  Piotrowski", "title": "Batch 2: Definition of novel Weather & Climate Dwarfs", "comments": "54 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1908.06089", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is one of the deliverable reports created for the ESCAPE\nproject. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather\nPrediction at Exascale. The project develops world-class, extreme-scale\ncomputing capabilities for European operational numerical weather prediction\nand future climate models. This is done by identifying weather & climate dwarfs\nwhich are key patterns in terms of computation and communication (in the spirit\nof the Berkeley dwarfs). These dwarfs are then optimised for different hardware\narchitectures (single and multi-node) and alternative algorithms are explored.\nPerformance portability is addressed through the use of domain specific\nlanguages.\n  This deliverable contains the description of the characteristics of a second\nset of so-called numerical weather & climate prediction dwarfs that form key\nfunctional components of prediction models in terms of the science that they\nencapsulate and in terms of computational cost they impose on the forecast\nproduction. The ESCAPE work flow between work packages centres on these dwarfs\nand hence their selection, their performance assessment, code adaptation and\noptimisation is crucial for the success of the project. These new dwarfs have\nbeen chosen with the purpose of extending the range of computational\ncharacteristic represented by the dwarfs previously selected in batch 1 (see\nDeliverable D1.1). The dwarfs have been made, their documentation has been\ncompiled and the software has been made available on the software exchange\nplatform.\n  The dwarfs in this deliverable include a multigrid elliptic solver, a novel\nadvection scheme for unstructured meshes, an advection scheme for structured\nmeshes and a radiation scheme. This deliverable includes their scientific\ndescription and the guidance for installation, execution and testing.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 14:27:45 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["M\u00fcller", "Andreas", ""], ["Gillard", "Mike", ""], ["Nielsen", "Kristian Pagh", ""], ["Piotrowski", "Zbigniew", ""]]}, {"id": "1908.07315", "submitter": "Jaroslav Opatrny", "authors": "Iman Bagheri, Lata Narayanan, Jaroslav Opatrny", "title": "Evacuation of equilateral triangles by mobile agents of limited\n  communication range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evacuating $k \\geq 2$ mobile agents from a\nunit-sided equilateral triangle through an exit located at an unknown location\non the perimeter of the triangle. The agents are initially located at the\ncentroid of the triangle and they can communicate with other agents at distance\nat most $r$ with $0\\leq r \\leq 1$. An agent can move at speed at most one, and\nfinds the exit only when it reaches the point where the exit is located. The\nagents can collaborate in the search for the exit. The goal of the {\\em\nevacuation problem} is to minimize the evacuation time, defined as the\nworst-case time for {\\em all} the agents to reach the exit. We propose and\nanalyze several algorithms for the problem of evacuation by $k \\geq 2$ agents;\nour results indicate that the best strategy to be used varies depending on the\nvalues of $r$ and $k$. For two agents, we give three algorithms, each of which\nachieves the best performance for different sub-ranges of $r$ in the range $0\n\\leq r \\leq 1$. Finally, we show that for any $r$, evacuation of $k=6\n+2\\lceil(\\frac{1}{r}-1)\\rceil$ agents can be done in time $1+\\sqrt{3}/3$, which\nis optimal in terms of time, and asymptotically optimal in terms of the number\nof agents.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 12:56:16 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Bagheri", "Iman", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""]]}, {"id": "1908.07391", "submitter": "Andreas Kamilaris", "authors": "Andreas Kamilaris, Agusti Fonts and Francesc X. Prenafeta-Boldu", "title": "The Rise of Blockchain Technology in Agriculture and Food Supply Chains", "comments": "Commentary published in Trends in Food Science & Technology Journal", "journal-ref": "Trends in Food Science & Technology Journal 2019", "doi": "10.1016/j.tifs.2019.07.034", "report-no": null, "categories": "cs.CY cs.DC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain is an emerging digital technology allowing ubiquitous financial\ntransactions among distributed untrusted parties, without the need of\nintermediaries such as banks. This article examines the impact of blockchain\ntechnology in agriculture and food supply chain, presents existing ongoing\nprojects and initiatives, and discusses overall implications, challenges and\npotential, with a critical view over the maturity of these projects. Our\nfindings indicate that blockchain is a promising technology towards a\ntransparent supply chain of food, with many ongoing initiatives in various food\nproducts and food-related issues, but many barriers and challenges still exist,\nwhich hinder its wider popularity among farmers and systems. These challenges\ninvolve technical aspects, education, policies and regulatory frameworks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 16:25:59 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Kamilaris", "Andreas", ""], ["Fonts", "Agusti", ""], ["Prenafeta-Boldu", "Francesc X.", ""]]}, {"id": "1908.07420", "submitter": "Antonio Ferrara", "authors": "Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara", "title": "Towards Effective Device-Aware Federated Learning", "comments": "12 pages, AIIA 2019, 18th International Conference of the Italian\n  Association for Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wealth of information produced by social networks, smartphones,\nmedical or financial applications, speculations have been raised about the\nsensitivity of such data in terms of users' personal privacy and data security.\nTo address the above issues, Federated Learning (FL) has been recently proposed\nas a means to leave data and computational resources distributed over a large\nnumber of nodes (clients) where a central coordinating server aggregates only\nlocally computed updates without knowing the original data. In this work, we\nextend the FL framework by pushing forward the state the art in the field on\nseveral dimensions: (i) unlike the original FedAvg approach relying solely on\nsingle criteria (i.e., local dataset size), a suite of domain- and\nclient-specific criteria constitute the basis to compute each local client's\ncontribution, (ii) the multi-criteria contribution of each device is computed\nin a prioritized fashion by leveraging a priority-aware aggregation operator\nused in the field of information retrieval, and (iii) a mechanism is proposed\nfor online-adjustment of the aggregation operator parameters via a local search\nstrategy with backtracking. Extensive experiments on a publicly available\ndataset indicate the merits of the proposed approach compared to standard\nFedAvg baseline.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:12:59 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Deldjoo", "Yashar", ""], ["Di Noia", "Tommaso", ""], ["Ferrara", "Antonio", ""]]}, {"id": "1908.07567", "submitter": "Jianyu Niu", "authors": "Jianyu Niu", "title": "Eunomia: A Permissionless Parallel Chain Protocol Based on Logical Clock", "comments": "19 pages, 5 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging parallel chain protocols represent a breakthrough to address the\nscalability of blockchain. By composing multiple parallel chain instances, the\nwhole systems' throughput can approach the network capacity. How to coordinate\ndifferent chains' blocks and to construct them into a global ordering is\ncritical to the performance of parallel chain protocol. However, the existed\nsolutions use either the global synchronization clock with the single-chain\nbottleneck or pre-defined ordering sequences with distortion of blocks'\ncausality to order blocks. In addition, the prior ordering methods rely on that\nhonest participants faithfully follow the ordering protocol, but remain silent\nfor any denial of ordering (DoR) attack.\n  On the other hand, the conflicting transactions included into the global\nblock sequence will make Simple Payment Verification (SPV) difficult. Clients\nusually need to store a full record of transactions to distinguish the\nconflictions and tell whether transactions are confirmed. However, the\nrequirement for a full record will greatly hinder blockchains' application,\nespecially for mobile scenarios.\n  In this technical report, we propose Eunomia, which leverages logical clock\nand fine-grained UTXO sharding to realize a simple, efficient, secure and\npermissionless parallel chain protocol. By observing the characteristics of the\nparallel chain, we find the blocks ordering issue in parallel chain has many\nsimilarities with the event ordering in the distributed system. Eunomia thus\nadopts \"virtual\" logical clock, which is optimized to have the minimum protocol\noverhead and runs in a distributed way. In addition, Eunomia combines the\nmining incentive with block ordering, providing incentive compatibility against\nDoR attack. What's more, the fine-grained UTXO sharding does well solve the\nconflicting transactions in parallel chain and is shown to be SPV-friendly.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 18:51:51 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Niu", "Jianyu", ""]]}, {"id": "1908.07573", "submitter": "Andrew Prout", "authors": "Andrew Prout, William Arcand, David Bestor, Bill Bergeron, Chansup\n  Byun, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna\n  Klein, Peter Michaleas, Lauren Milechin, Julie Mullen, Antonio Rosa,\n  Siddharth Samsi, Charles Yee, Albert Reuther, Jeremy Kepner", "title": "Securing HPC using Federated Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated authentication can drastically reduce the overhead of basic account\nmaintenance while simultaneously improving overall system security. Integrating\nwith the user's more frequently used account at their primary organization both\nprovides a better experience to the end user and makes account compromise or\nchanges in affiliation more likely to be noticed and acted upon. Additionally,\nwith many organizations transitioning to multi-factor authentication for all\naccount access, the ability to leverage external federated identity management\nsystems provides the benefit of their efforts without the additional overhead\nof separately implementing a distinct multi-factor authentication process. This\npaper describes our experiences and the lessons we learned by enabling\nfederated authentication with the U.S. Government PKI and InCommon Federation,\nscaling it up to the user base of a production HPC system, and the motivations\nbehind those choices. We have received only positive feedback from our users.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 19:11:05 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Prout", "Andrew", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1908.07715", "submitter": "Naoki Yonezawa", "authors": "Naoki Yonezawa", "title": "A sufficient condition for a linear speedup in competitive parallel\n  computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In competitive parallel computing, the identical copies of a code in a phase\nof a sequential program are assigned to processor cores and the result of the\nfastest core is adopted. In the literature, it is reported that a superlinear\nspeedup can be achieved if there is an enough fluctuation among the execution\ntimes consumed by the cores. Competitive parallel computing is a promising\napproach to use a huge amount of cores effectively. However, there is few\ntheoretical studies on speedups which can be achieved by competitive parallel\ncomputing at present. In this paper, we present a behavioral model of\ncompetitive parallel computing and provide a means to predict a speedup which\ncompetitive parallel computing yields through theoretical analyses and\nsimulations. We also found a sufficient condition to provide a linear speedup\nwhich competitive parallel computing yields. More specifically, it is\nsufficient for the execution times which consumed by the cores to follow an\nexponential distribution. In addition, we found that the different\ndistributions which have the identical coefficient of variation (CV) do not\nalways provide the identical speedup. While CV is a convenient measure to\npredict a speedup, it is not enough to provide an exact prediction.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 05:50:44 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Yonezawa", "Naoki", ""]]}, {"id": "1908.07782", "submitter": "Chenghao Hu", "authors": "Chenghao Hu, Jingyan Jiang, Zhi Wang", "title": "Decentralized Federated Learning: A Segmented Gossip Approach", "comments": "Accepted to the 1st International Workshop on Federated Machine\n  Learning for User Privacy and Data Confidentiality (FML'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging concern about data privacy and security has motivated the\nproposal of federated learning, which allows nodes to only synchronize the\nlocally-trained models instead their own original data. Conventional federated\nlearning architecture, inherited from the parameter server design, relies on\nhighly centralized topologies and the assumption of large nodes-to-server\nbandwidths. However, in real-world federated learning scenarios the network\ncapacities between nodes are highly uniformly distributed and smaller than that\nin a datacenter. It is of great challenges for conventional federated learning\napproaches to efficiently utilize network capacities between nodes. In this\npaper, we propose a model segment level decentralized federated learning to\ntackle this problem. In particular, we propose a segmented gossip approach,\nwhich not only makes full utilization of node-to-node bandwidth, but also has\ngood training convergence. The experimental results show that even the training\ntime can be highly reduced as compared to centralized federated learning.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 10:21:43 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Hu", "Chenghao", ""], ["Jiang", "Jingyan", ""], ["Wang", "Zhi", ""]]}, {"id": "1908.07873", "submitter": "Tian Li", "authors": "Tian Li, Anit Kumar Sahu, Ameet Talwalkar, Virginia Smith", "title": "Federated Learning: Challenges, Methods, and Future Directions", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2020.2975749", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning involves training statistical models over remote devices\nor siloed data centers, such as mobile phones or hospitals, while keeping data\nlocalized. Training in heterogeneous and potentially massive networks\nintroduces novel challenges that require a fundamental departure from standard\napproaches for large-scale machine learning, distributed optimization, and\nprivacy-preserving data analysis. In this article, we discuss the unique\ncharacteristics and challenges of federated learning, provide a broad overview\nof current approaches, and outline several directions of future work that are\nrelevant to a wide range of research communities.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 13:53:23 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Li", "Tian", ""], ["Sahu", "Anit Kumar", ""], ["Talwalkar", "Ameet", ""], ["Smith", "Virginia", ""]]}, {"id": "1908.07917", "submitter": "Massimiliano Morrelli", "authors": "Marco Covelli, Massimiliano Morrelli", "title": "Nuova frontiera della classificazione testuale: Big data e calcolo\n  distribuito", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document was created in order to study the algorithms for the\ncategorization of phrases and rank them using the facilities provided by the\nframework Apache Spark. Starting from the study illustrated in the publication\n\"Classifying textual data: shallow, deep and ensemble methods\" by Laura\nAnderlucci, Lucia Guastadisegni, Cinzia Viroli, we wanted to carry out a study\non the possible realization of a solution that uses the distributed environment\nand allows the classification of phrases. Italiano. Il presente documento\npersegue l'obiettivo di studiare gli algoritmi per la categorizzazione di frasi\ne classificarle con l'ausilio delle strutture messe a disposizione dal\nframework Apache Spark. Partendo dallo studio illustrato nella pubblicazione\n\"Classifying textual data: shallow, deep and ensemble methods\" di Laura\nAnderlucci, Lucia Guastadisegni e Cinzia Viroli si \\`e voluto realizzare uno\nstudio sulla possibile implementazione di una soluzione in grado di\nclassificare frasi sfruttando i l'ambiente distribuito.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:27:40 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Covelli", "Marco", ""], ["Morrelli", "Massimiliano", ""]]}, {"id": "1908.07918", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Pankaj Singh, Rakhi Garg, P. K. Mishra", "title": "Mining Association Rules in Various Computing Environments: A Survey", "comments": "14 pages", "journal-ref": "International Journal of Applied Engineering Research 2016; 11(8):\n  5629-5640", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association Rule Mining (ARM) is one of the well know and most researched\ntechnique of data mining. There are so many ARM algorithms have been designed\nthat their counting is a large number. In this paper we have surveyed the\nvarious ARM algorithms in four computing environments. The considered computing\nenvironments are sequential computing, parallel and distributed computing, grid\ncomputing and cloud computing. With the emergence of new computing paradigm,\nARM algorithms have been designed by many researchers to improve the efficiency\nby utilizing the new paradigm. This paper represents the journey of ARM\nalgorithms started from sequential algorithms, and through parallel and\ndistributed, and grid based algorithms to the current state-of-the-art, along\nwith the motives for adopting new machinery.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 11:13:50 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Singh", "Sudhakar", ""], ["Singh", "Pankaj", ""], ["Garg", "Rakhi", ""], ["Mishra", "P. K.", ""]]}, {"id": "1908.07985", "submitter": "Royson Lee", "authors": "Royson Lee, Stylianos I. Venieris, {\\L}ukasz Dudziak, Sourav\n  Bhattacharya, Nicholas D. Lane", "title": "MobiSR: Efficient On-Device Super-Resolution through Heterogeneous\n  Mobile Processors", "comments": "Accepted at the 25th Annual International Conference on Mobile\n  Computing and Networking (MobiCom), 2019", "journal-ref": null, "doi": "10.1145/3300061.3345455", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional networks have demonstrated unprecedented\nperformance in the image restoration task of super-resolution (SR). SR entails\nthe upscaling of a single low-resolution image in order to meet\napplication-specific image quality demands and plays a key role in mobile\ndevices. To comply with privacy regulations and reduce the overhead of cloud\ncomputing, executing SR models locally on-device constitutes a key alternative\napproach. Nevertheless, the excessive compute and memory requirements of SR\nworkloads pose a challenge in mapping SR networks on resource-constrained\nmobile platforms. This work presents MobiSR, a novel framework for performing\nefficient super-resolution on-device. Given a target mobile platform, the\nproposed framework considers popular model compression techniques and traverses\nthe design space to reach the highest performing trade-off between image\nquality and processing speed. At run time, a novel scheduler dispatches\nincoming image patches to the appropriate model-engine pair based on the\npatch's estimated upscaling difficulty in order to meet the required image\nquality with minimum processing latency. Quantitative evaluation shows that the\nproposed framework yields on-device SR designs that achieve an average speedup\nof 2.13x over highly-optimized parallel difficulty-unaware mappings and 4.79x\nover highly-optimized single compute engine implementations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:55:08 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lee", "Royson", ""], ["Venieris", "Stylianos I.", ""], ["Dudziak", "\u0141ukasz", ""], ["Bhattacharya", "Sourav", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "1908.08014", "submitter": "Amir Nakib", "authors": "Asmaa Ghoumari, Amir Nakib", "title": "Graph based adaptive evolutionary algorithm for continuous optimization", "comments": null, "journal-ref": "OLA conference 2018", "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  he greatest weakness of evolutionary algorithms, widely used today, is the\npremature convergence due to the loss of population diversity over generations.\nTo overcome this problem, several algorithms have been proposed, such as the\nGraph-based Evolutionary Algorithm (GEA) \\cite{1} which uses graphs to model\nthe structure of the population, but also memetic or differential evolution\nalgorithms \\cite{2,3}, or diversity-based ones \\cite{4,5} have been designed.\nThese algorithms are based on multi-populations, or often rather focus on the\nself-tuning parameters, however, they become complex to tune because of their\nhigh number of parameters. In this paper, our approach consists of an\nevolutionary algorithm that allows a dynamic adaptation of the search operators\nbased on a graph in order to limit the loss of diversity and reduce the design\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 15:50:05 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Ghoumari", "Asmaa", ""], ["Nakib", "Amir", ""]]}, {"id": "1908.08082", "submitter": "Tim Capes", "authors": "Tim Capes, Vishal Raheja, Mete Kemertas, Iqbal Mohomed", "title": "Dynamic Scheduling of MPI-based Distributed Deep Learning Training Jobs", "comments": null, "journal-ref": "Published at MLSys Workshop @ NeurIPS 2018\n  (https://nips.cc/Conferences/2018/Schedule?showEvent=10919) December 7th,\n  2018", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a general trend towards solving problems suited to deep learning\nwith more complex deep learning architectures trained on larger training sets.\nThis requires longer compute times and greater data parallelization or model\nparallelization. Both data and model parallelism have been historically faster\nin parameter server architectures, but data parallelism is starting to be\nfaster in ring architectures due to algorithmic improvements. In this paper, we\nanalyze the math behind ring architectures and make an informed adaptation of\ndynamic scheduling to ring architectures. To do so, we formulate a non-convex,\nnon-linear, NP-hard integer programming problem and a new efficient doubling\nheuristic for its solution. We build upon Horovod: an open source ring\narchitecture framework over TensorFlow. We show that Horovod jobs have a low\ncost to stop and restart and that stopping and restarting ring architecture\njobs leads to faster completion times. These two facts make dynamic scheduling\nof ring architecture jobs feasible. Lastly, we simulate a scheduler using these\nruns and show a more than halving of average job time on some workload\npatterns.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 18:49:26 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Capes", "Tim", ""], ["Raheja", "Vishal", ""], ["Kemertas", "Mete", ""], ["Mohomed", "Iqbal", ""]]}, {"id": "1908.08098", "submitter": "Waheed Bajwa", "authors": "Zhixiong Yang and Waheed U. Bajwa", "title": "BRIDGE: Byzantine-resilient Decentralized Gradient Descent", "comments": "18 pages, 1 figure, 1 table; preprint of a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.MA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized optimization techniques are increasingly being used to learn\nmachine learning models from data distributed over multiple locations without\ngathering the data at any one location. Unfortunately, methods that are\ndesigned for faultless networks typically fail in the presence of node\nfailures. In particular, Byzantine failures---corresponding to the scenario in\nwhich faulty/compromised nodes are allowed to arbitrarily deviate from an\nagreed-upon protocol---are the hardest to safeguard against in decentralized\nsettings. This paper introduces a Byzantine-resilient decentralized gradient\ndescent (BRIDGE) method for decentralized learning that, when compared to\nexisting works, is more efficient and scalable in higher-dimensional settings\nand that is deployable in networks having topologies that go beyond the star\ntopology. The main contributions of this work include theoretical analysis of\nBRIDGE for strongly convex learning objectives and numerical experiments\ndemonstrating the efficacy of BRIDGE for both convex and nonconvex learning\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 19:49:56 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Yang", "Zhixiong", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1908.08316", "submitter": "Christopher Natoli", "authors": "Christopher Natoli, Jiangshan Yu, Vincent Gramoli, Paulo\n  Esteves-Verissimo", "title": "Deconstructing Blockchains: A Comprehensive Survey on Consensus,\n  Membership and Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is no exaggeration to say that since the introduction of Bitcoin,\nblockchains have become a disruptive technology that has shaken the world.\nHowever, the rising popularity of the paradigm has led to a flurry of proposals\naddressing variations and/or trying to solve problems stemming from the initial\nspecification. This added considerable complexity to the current blockchain\necosystems, amplified by the absence of detail in many accompanying blockchain\nwhitepapers.\n  Through this paper, we set out to explain blockchains in a simple way, taming\nthat complexity through the deconstruction of the blockchain into three simple,\ncritical components common to all known systems: membership selection,\nconsensus mechanism and structure. We propose an evaluation framework with\ninsight into system models, desired properties and analysis criteria, using the\ndecoupled components as criteria. We use this framework to provide clear and\nintuitive overviews of the design principles behind the analyzed systems and\nthe properties achieved. We hope our effort will help clarifying the current\nstate of blockchain proposals and provide directions to the analysis of future\nproposals.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:22:04 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Natoli", "Christopher", ""], ["Yu", "Jiangshan", ""], ["Gramoli", "Vincent", ""], ["Esteves-Verissimo", "Paulo", ""]]}, {"id": "1908.08553", "submitter": "Justin  Reyes", "authors": "Justin A. Reyes, Eduardo R. Mucciolo, Dan Marinescu", "title": "Simulation of Quantum Many-Body Systems on Amazon Cloud", "comments": "25 pages, 11 figures", "journal-ref": "Computer Physics Communications 261 (2021) 107750", "doi": "10.1016/j.cpc.2020.107750", "report-no": null, "categories": "cs.DC cond-mat.str-el quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum many-body systems (QMBs) are some of the most challenging physical\nsystems to simulate numerically. Methods involving approximations for tensor\nnetwork (TN) contractions have proven to be viable alternatives to algorithms\nsuch as quantum Monte Carlo or simulated annealing. However, these methods are\ncumbersome, difficult to implement, and often have significant limitations in\ntheir accuracy and efficiency when considering systems in more than one\ndimension. In this paper, we explore the exact computation of TN contractions\non two-dimensional geometries and present a heuristic improvement of TN\ncontraction that reduces the computing time, the amount of memory, and the\ncommunication time. We run our algorithm for the Ising model using memory\noptimized x1.32x large instances on Amazon Web Services (AWS) Elastic Compute\nCloud (EC2). Our results show that cloud computing is a viable alternative to\nsupercomputers for this class of scientific applications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 18:14:18 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 15:32:38 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Reyes", "Justin A.", ""], ["Mucciolo", "Eduardo R.", ""], ["Marinescu", "Dan", ""]]}, {"id": "1908.08590", "submitter": "Salvatore Di Girolamo", "authors": "Salvatore Di Girolamo, Konstantin Taranov, Andreas Kurth, Michael\n  Schaffner, Timo Schneider, Jakub Ber\\'anek, Maciej Besta, Luca Benini, Duncan\n  Roweth, Torsten Hoefler", "title": "Network-Accelerated Non-Contiguous Memory Transfers", "comments": "In Proceedings of the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC19), Nov. 2019", "journal-ref": null, "doi": "10.1145/3295500.3356189", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications often communicate data that is non-contiguous in the send- or\nthe receive-buffer, e.g., when exchanging a column of a matrix stored in\nrow-major order. While non-contiguous transfers are well supported in HPC\n(e.g., MPI derived datatypes), they can still be up to 5x slower than\ncontiguous transfers of the same size. As we enter the era of network\nacceleration, we need to investigate which tasks to offload to the NIC: In this\nwork we argue that non-contiguous memory transfers can be transparently\nnetworkaccelerated, truly achieving zero-copy communications. We implement and\nextend sPIN, a packet streaming processor, within a Portals 4 NIC SST model,\nand evaluate strategies for NIC-offloaded processing of MPI datatypes, ranging\nfrom datatype-specific handlers to general solutions for any MPI datatype. We\ndemonstrate up to 10x speedup in the unpack throughput of real applications,\ndemonstrating that non-contiguous memory transfers are a first-class candidate\nfor network acceleration.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 20:52:23 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Di Girolamo", "Salvatore", ""], ["Taranov", "Konstantin", ""], ["Kurth", "Andreas", ""], ["Schaffner", "Michael", ""], ["Schneider", "Timo", ""], ["Ber\u00e1nek", "Jakub", ""], ["Besta", "Maciej", ""], ["Benini", "Luca", ""], ["Roweth", "Duncan", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1908.08634", "submitter": "EPTCS", "authors": "Frank Valencia (CNRS-LIX, Ecole Polytechnique de Paris and Univ.\n  Javeriana Cali.)", "title": "Semantic Structures for Spatially-Distributed Multi-Agent Systems", "comments": "In Proceedings EXPRESS/SOS 2019, arXiv:1908.08213. This is an invited\n  contribution to EXPRESS/SOS 2019 based on my invited talk", "journal-ref": "EPTCS 300, 2019, pp. 39-53", "doi": "10.4204/EPTCS.300.3", "report-no": null, "categories": "cs.MA cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial constraint systems (scs) are semantic structures for reasoning about\nspatial and epistemic information in concurrent systems. They have been used to\nreason about beliefs, lies, and group epistemic behaviour inspired by social\nnetworks. They have also been used for proving new results about modal logics\nand giving semantics to process calculi. In this paper we will discuss the\ntheory and main results about scs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 01:55:26 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Valencia", "Frank", "", "CNRS-LIX, Ecole Polytechnique de Paris and Univ.\n  Javeriana Cali."]]}, {"id": "1908.08637", "submitter": "EPTCS", "authors": "Tobias Prehn (Technische Universit\\\"at Berlin), Myron Rotter\n  (Technische Universit\\\"at Berlin)", "title": "Immediate Observation in Mediated Population Protocols", "comments": "In Proceedings EXPRESS/SOS 2019, arXiv:1908.08213", "journal-ref": "EPTCS 300, 2019, pp. 102-113", "doi": "10.4204/EPTCS.300.7", "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the computational power of variants of population\nprotocols (PP), a formalism for distributed systems with anonymous agents\nhaving very limited capabilities. The capabilities of agents are enhanced in\nmediated population protocols (MPP) by recording the states in the edges of the\ninteraction graph. Restricting the interactions to the communication model of\nimmediate observation (IO) reduces the computational power of the resulting\nformalism. We show that this enhancement and restriction, when combined, yield\na model (IOMPP) at least as powerful as the basic PP. The proof requires a\nnovel notion of configurations in the MPP model allowing differentiation of\nagents and uses techniques similar to methods of analyzing encoding criteria,\nnamely operational correspondence. The constructional part of the proof is\ngeneric in a way that all protocols can be translated into the new model\nwithout losing the desirable properties they might have besides a stable\noutput. Furthermore, we illustrate how this approach could be utilized to prove\nour conjecture of IOMPP model being even as expressive as the MPP model. If our\nconjecture holds, this would result in a sharp characterization of the\ncomputational power and reveal the nonnecessity of two-way communication in the\ncontext of mediated population protocols.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 01:57:30 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Prehn", "Tobias", "", "Technische Universit\u00e4t Berlin"], ["Rotter", "Myron", "", "Technische Universit\u00e4t Berlin"]]}, {"id": "1908.08649", "submitter": "Waheed Bajwa", "authors": "Zhixiong Yang, Arpita Gang, and Waheed U. Bajwa", "title": "Adversary-resilient Distributed and Decentralized Statistical Inference\n  and Machine Learning: An Overview of Recent Advances Under the Byzantine\n  Threat Model", "comments": "24 pages, 6 figures, 2 tables; Published in IEEE Signal Processing\n  Magazine, May 2020 (Special Issue on \"Machine Learning From Distributed,\n  Streaming Data\")", "journal-ref": "IEEE Signal Processing Mag., vol. 37, no. 3, pp. 146-159, May 2020", "doi": "10.1109/MSP.2020.2973345", "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the last few decades have witnessed a huge body of work devoted to\ninference and learning in distributed and decentralized setups, much of this\nwork assumes a non-adversarial setting in which individual nodes---apart from\noccasional statistical failures---operate as intended within the algorithmic\nframework. In recent years, however, cybersecurity threats from malicious\nnon-state actors and rogue entities have forced practitioners and researchers\nto rethink the robustness of distributed and decentralized algorithms against\nadversarial attacks. As a result, we now have a plethora of algorithmic\napproaches that guarantee robustness of distributed and/or decentralized\ninference and learning under different adversarial threat models. Driven in\npart by the world's growing appetite for data-driven decision making, however,\nsecuring of distributed/decentralized frameworks for inference and learning\nagainst adversarial threats remains a rapidly evolving research area. In this\narticle, we provide an overview of some of the most recent developments in this\narea under the threat model of Byzantine attacks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 03:23:49 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 17:39:59 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 02:21:10 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Yang", "Zhixiong", ""], ["Gang", "Arpita", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1908.08774", "submitter": "Yikun Ban", "authors": "Yikun Ban, Yuchen Zhou, Xu Cheng, Jiangfang Yi", "title": "Coalesced TLB to Exploit Diverse Contiguity of Memory Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The miss rate of TLB is crucial to the performance of address translation for\nvirtual memory. To reduce the TLB misses, improving translation coverage of TLB\nhas been an primary approach. Many previous works focus on coalescing multiple\ncontiguously mapped pages of the memory mapping into a modified entry, which\nfunction well if the assumed contiguity of memory mapping is given.\nUnfortunately, scenarios of applications are complicated and the produced\ncontiguity diversify. To gain better performance of translation, in this paper,\nwe first introduce a complex but prevalent type of contiguity, mixed\ncontiguity. Then we propose a HW-SW hybrid coalesced TLB structure which works\nwell on all observed types of contiguity including this type. In our\nevaluation, the proposed scheme, K-bit Aligned TLB, outperforms the\nstate-of-the-art work by reducing at lease 27% TLB misses on average over it\nusing 16 benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:34:43 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 22:21:59 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ban", "Yikun", ""], ["Zhou", "Yuchen", ""], ["Cheng", "Xu", ""], ["Yi", "Jiangfang", ""]]}, {"id": "1908.09015", "submitter": "Hien Truong", "authors": "Hien Thi Thu Truong, Miguel Almeida, Ghassan Karame, Claudio Soriente", "title": "Towards Secure and Decentralized Sharing of IoT Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) bears unprecedented security and scalability\nchallenges due to the magnitude of data produced and exchanged by IoT devices\nand platforms. Some of those challenges are currently being addressed by\ncoupling IoT applications with blockchains. However, current blockchain-backed\nIoT systems simply use the blockchain to store access control policies, thereby\nunderutilizing the power of blockchain technology. In this paper, we propose a\nnew framework named Sash that couples IoT platforms with blockchain that\nprovides a number of advantages compared to state of the art. In Sash, the\nblockchain is used to store access control policies and take access control\ndecisions. Therefore, both changes to policies and access requests are\ncorrectly enforced and publicly auditable. Further, we devise a ``data\nmarketplace'' by leveraging the ability of blockchains to handle financial\ntransaction and providing ``by design'' remuneration to data producers.\nFinally, we exploit a special flavor of identity-based encryption to cater for\ncryptography-enforced access control while minimizing the overhead to\ndistribute decryption keys. We prototype Sash by using the FIWARE open source\nIoT platform and the Hyperledger Fabric framework as the blockchain back-end.\nWe also evaluate the performance of our prototype and show that it incurs\ntolerable overhead in realistic deployment settings.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 19:50:40 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Truong", "Hien Thi Thu", ""], ["Almeida", "Miguel", ""], ["Karame", "Ghassan", ""], ["Soriente", "Claudio", ""]]}, {"id": "1908.09042", "submitter": "Hamed Rahimi", "authors": "Parsa Rajabzadeh, Amin Pishevar and Hamed Rahimi", "title": "SIDLE: Semantically Intelligent Distributed Leader Election Algorithm\n  for Wireless Sensor Networks", "comments": "The First International Conference of Smart City, 2019, Apadana\n  University, Shiraz, Iran\n  https://www.civilica.com/Paper-SMARTCITYC01-SMARTCITYC01_100.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the deployment of a group of Wireless Sensor and\nActuator Network (WSAN) for Internet of Thing (IoT) systems in rural regions\ndeployed by a drone dropping sensors and actuators at a certain position as a\nmesh of a hexagonal form. Nodes are heterogeneous in hardware and functionality\nthus not all nodes are able to transfer data directly to the base station.\nPrimitive ones are only capable of collecting local data. However, ones that\nare more sophisticated are equipped with long-range radio telemetry and more\ncomputational power. Power optimization is one of the crucial factors in\ndesigning WSANs. Total power consumption must be minimized, as sensors are\nself-managed. It is not feasible to collect sensors on time bases and recharge\nthe batteries. Therefore, energy consumption optimization and harvesting green\nenergy are other factors that are considered. In this regard, protocols are\ndesigned in a way to support such requirements. The preprocessed data are first\ncollected and combined by the leaders at each hexagonal cell. Then, the\ninformation packets are sent to the head clusters. Consequently, head clusters\nreprocess the received information and depict a better global view of the zone,\nusing a variety of the received information. Finally, the processed information\nis sent to the nearest base station or a mobile drone.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 22:19:15 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 21:06:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Rajabzadeh", "Parsa", ""], ["Pishevar", "Amin", ""], ["Rahimi", "Hamed", ""]]}, {"id": "1908.09048", "submitter": "Subramaniam Venkatraman Krishnan", "authors": "Liqun Shao, Yiwen Zhu, Abhiram Eswaran, Kristin Lieber, Janhavi\n  Mahajan, Minsoo Thigpen, Sudhir Darbha, Siqi Liu, Subru Krishnan, Soundar\n  Srinivasan, Carlo Curino and Konstantinos Karanasos", "title": "Griffon: Reasoning about Job Anomalies with Unlabeled Data in\n  Cloud-based Platforms", "comments": null, "journal-ref": null, "doi": "10.1145/3357223.3362716", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Microsoft's internal big data analytics platform is comprised of hundreds of\nthousands of machines, serving over half a million jobs daily, from thousands\nof users. The majority of these jobs are recurring and are crucial for the\ncompany's operation. Although administrators spend significant effort tuning\nsystem performance, some jobs inevitably experience slowdowns, i.e., their\nexecution time degrades over previous runs. Currently, the investigation of\nsuch slowdowns is a labor-intensive and error-prone process, which costs\nMicrosoft significant human and machine resources, and negatively impacts\nseveral lines of businesses. In this work, we present Griffin, a system we\nbuilt and have deployed in production last year to automatically discover the\nroot cause of job slowdowns. Existing solutions either rely on labeled data\n(i.e., resolved incidents with labeled reasons for job slowdowns), which is in\nmost cases non-existent or non-trivial to acquire, or on time-series analysis\nof individual metrics that do not target specific jobs holistically. In\ncontrast, in Griffin we cast the problem to a corresponding regression one that\npredicts the runtime of a job, and show how the relative contributions of the\nfeatures used to train our interpretable model can be exploited to rank the\npotential causes of job slowdowns. Evaluated over historical incidents, we show\nthat Griffin discovers slowdown causes that are consistent with the ones\nvalidated by domain-expert engineers, in a fraction of the time required by\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 22:57:50 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Shao", "Liqun", ""], ["Zhu", "Yiwen", ""], ["Eswaran", "Abhiram", ""], ["Lieber", "Kristin", ""], ["Mahajan", "Janhavi", ""], ["Thigpen", "Minsoo", ""], ["Darbha", "Sudhir", ""], ["Liu", "Siqi", ""], ["Krishnan", "Subru", ""], ["Srinivasan", "Soundar", ""], ["Curino", "Carlo", ""], ["Karanasos", "Konstantinos", ""]]}, {"id": "1908.09271", "submitter": "Joachim Neu", "authors": "Joachim Neu, Muriel M\\'edard", "title": "Babel Storage: Uncoordinated Content Delivery from Multiple Coded\n  Storage Systems", "comments": null, "journal-ref": null, "doi": "10.1109/GLOBECOM38437.2019.9013383", "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In future content-centric networks, content is identified independently of\nits location. From an end-user's perspective, individual storage systems\ndissolve into a seemingly omnipresent structureless `storage fog'. Content\nshould be delivered oblivious of the network topology, using multiple storage\nsystems simultaneously, and at minimal coordination overhead. Prior works have\naddressed the advantages of error correction coding for distributed storage and\ncontent delivery separately. This work takes a comprehensive approach to\nhighlighting the tradeoff between storage overhead and transmission overhead in\nuncoordinated content delivery from multiple coded storage systems.\n  Our contribution is twofold. First, we characterize the tradeoff between\nstorage and transmission overhead when all participating storage systems employ\nthe same code. Second, we show that the resulting stark inefficiencies can be\navoided when storage systems use diverse codes. What is more, such code\ndiversity is not just technically desirable, but presumably will be the reality\nin the increasingly heterogeneous networks of the future. To this end, we show\nthat a mix of Reed-Solomon, low-density parity-check and random linear network\ncodes achieves close-to-optimal performance at minimal coordination and\noperational overhead.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 08:15:13 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Neu", "Joachim", ""], ["M\u00e9dard", "Muriel", ""]]}, {"id": "1908.09291", "submitter": "Sam Whitlock", "authors": "Sam Whitlock, James Larus, Edouard Bugnion", "title": "Extending TensorFlow's Semantics with Pipelined Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow is a popular cloud computing framework that targets machine\nlearning applications. It separates the specification of application logic (in\na dataflow graph) from the execution of the logic. TensorFlow's native runtime\nexecutes the application with low overhead across a diverse set of hardware\nincluding CPUs, GPUs, and ASICs. Although the underlying dataflow engine\nsupporting these features could be applied to computations beyond machine\nlearning, certain design decisions limit this broader application, such as the\ninability for an application to differentiate between data items across\nconcurrent requests.\n  This paper introduces Pipelined TensorFlow (PTF), a system that extends\nTensorFlow's semantics to provide support for a broader variety of application\nlogic. In particular, PTF supports applications that concurrently process\nfinite batches of data on a single instantiation. PTF adds these semantics by\npartitioning the dataflow graph into a pipeline of smaller graphs and tagging\neach data item with metadata. These smaller graphs are separated by gates: new\ndata structures in PTF that buffer data items between graphs and interpret the\nmetadata to apply the new semantics. PTF's pipeline architecture executes on an\nunmodified TensorFlow runtime, maintaining compatibility with many existing\nTensorFlow library functions. Our evaluation shows that the pipelining\nmechanism of PTF can increase the throughput of a bioinformatics application by\n4$\\times$ while only increasing its latency by 0.13$\\times$. This results in a\nsustained genome alignment and sorting rate of 321 megabases/second, using the\ncompute and I/O resources of 20 computers.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 09:47:50 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Whitlock", "Sam", ""], ["Larus", "James", ""], ["Bugnion", "Edouard", ""]]}, {"id": "1908.09378", "submitter": "Ben Karsin", "authors": "John Iacono, Ben Karsin, Nodari Sitchinava", "title": "A parallel priority queue with fast updates for GPU architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high computational throughput of modern graphics processing units (GPUs)\nmake them the de-facto architecture for high-performance computing\napplications. However, to achieve peak performance, GPUs require highly\nparallel workloads, as well as memory access patterns that exhibit good\nlocality of reference. As a result, many state-of-the-art algorithms and data\nstructures designed for GPUs sacrifice work-optimality to achieve the necessary\nparallelism. Furthermore, some abstract data types are avoided completely due\nto there being no corresponding data structure that performs well on the GPU.\nOne such abstract data type is the priority queue. Many well-known algorithms\nrely on priority queue operations as a building block. While various priority\nqueue structures have been developed that are parallel, cache-aware, or\ncache-oblivious, none has been shown to be efficient on GPUs. In this paper, we\npresent the parBucketHeap, a parallel, cache-efficient data structure designed\nfor modern GPU architectures that supports standard priority queue operations,\nas well as bulk update. We analyze the structure in several well-known\ncomputational models and show that it provides both optimal parallelism and is\ncache-efficient. We implement the parBucketHeap and, using it, we solve the\nsingle-source shortest path (SSSP) problem. Experimental results indicate that,\nfor sufficiently large, dense graphs with high diameter, we out-perform current\nstate-of-the-art SSSP algorithms on the GPU by up to a factor of 5. Unlike\nexisting GPU SSSP algorithms, our approach is work-optimal and places\nsignificantly less load on the GPU, reducing power consumption.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 19:23:42 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Iacono", "John", ""], ["Karsin", "Ben", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "1908.09466", "submitter": "Yanbing Mao", "authors": "Yanbing Mao, Hamidreza Jafarnejadsani, Pan Zhao, Emrah Akyol, and\n  Naira Hovakimyan", "title": "Novel Stealthy Attack and Defense Strategies for Networked Control\n  Systems", "comments": "to appear in IEEE TAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies novel attack and defense strategies, based on a class of\nstealthy attacks, namely the zero-dynamics attack (ZDA), for multi-agent\ncontrol systems. ZDA poses a formidable security challenge since its attack\nsignal is hidden in the null-space of the state-space representation of the\ncontrol system and hence it can evade conventional detection methods. An\nintuitive defense strategy builds on changing the aforementioned representation\nvia switching through a set of carefully crafted topologies. In this paper, we\npropose realistic ZDA variations where the attacker is aware of this\ntopology-switching strategy, and hence employs the following policies to avoid\ndetection: (i) pause, update and resume ZDA according to the knowledge of\nswitching topologies; (ii) cooperate with a concurrent stealthy topology attack\nthat alters network topology at switching times, such that the original ZDA is\nfeasible under the corrupted topology. We first systematically study the\nproposed ZDA variations, and then develop defense strategies against them under\nthe realistic assumption that the defender has no knowledge of attack starting,\npausing, and resuming times and the number of misbehaving agents. Particularly,\nwe characterize conditions for detectability of the proposed ZDA variations, in\nterms of the network topologies to be maintained, the set of agents to be\nmonitored, and the measurements of the monitored agents that should be\nextracted, while simultaneously preserving the privacy of the states of the\nnon-monitored agents. We then propose an attack detection algorithm based on\nthe Luenberger observer, using the characterized detectability conditions. We\nprovide numerical simulation results to demonstrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 04:41:00 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 19:07:49 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mao", "Yanbing", ""], ["Jafarnejadsani", "Hamidreza", ""], ["Zhao", "Pan", ""], ["Akyol", "Emrah", ""], ["Hovakimyan", "Naira", ""]]}, {"id": "1908.09473", "submitter": "Naoki Kitamura", "authors": "Naoki Kitamura, Hirotaka Kitagawa, Yota Otachi and Taisuke Izumi", "title": "Low-Congestion Shortcut and Graph Parameters", "comments": "19pages, 6figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of low-congestion shortcuts is initiated by Ghaffari and Haeupler\n[SODA2016] for addressing the design of CONGEST algorithms running fast in\nrestricted network topologies. Specifically, given a specific graph class $X$,\nan $f$-round algorithm of constructing shortcuts of quality $q$ for any\ninstance in $X$ results in $\\tilde{O}(q + f)$-round algorithms of solving\nseveral fundamental graph problems such as minimum spanning tree and minimum\ncut, for $X$.\n  In this paper, we consider the relationship between the quality of\nlow-congestion shortcuts and three major graph parameters, chordality,\ndiameter, and clique-width. The main contribution of the paper is threefold:\n(1) We show an $O(1)$-round algorithm which constructs a low-congestion\nshortcut with quality $O(kD)$ for any $k$-chordal graph, and prove that the\nquality and running time of this construction is nearly optimal up to\npolylogarithmic factors. (2) We present two algorithms, each of which\nconstructs a low-congestion shortcut with quality $\\tilde{O}(n^{1/4})$ in\n$\\tilde{O}(n^{1/4})$ rounds for graphs of $D=3$, and that with quality\n$\\tilde{O}(n^{1/3})$ in $\\tilde{O}(n^{1/3})$ rounds for graphs of $D=4$\nrespectively. These results obviously deduce two MST algorithms running in\n$\\tilde{O}(n^{1/4})$ and $\\tilde{O}(n^{1/3})$ rounds for $D=3$ and $4$\nrespectively, which almost close the long-standing complexity gap of the MST\nconstruction in small-diameter graphs originally posed by Lotker et al.\n[Distributed Computing 2006]. (3) We show that bounding clique-width does not\nhelp the construction of good shortcuts by presenting a network topology of\nclique-width six where the construction of MST is as expensive as the general\ncase.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:11:24 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kitamura", "Naoki", ""], ["Kitagawa", "Hirotaka", ""], ["Otachi", "Yota", ""], ["Izumi", "Taisuke", ""]]}, {"id": "1908.09606", "submitter": "Grzegorz Kwasniewski", "authors": "Grzegorz Kwasniewski (1), Marko Kabi\\'c (2,3), Maciej Besta (1), Joost\n  VandeVondele (2,3), Raffaele Solc\\`a (2,3), Torsten Hoefler (1) ((1)\n  Department of Computer Science, ETH Zurich, (2) ETH Zurich, (3) Swiss\n  National Supercomputing Centre (CSCS))", "title": "Red-blue pebbling revisited: near optimal parallel matrix-matrix\n  multiplication", "comments": "18 pages, 29 figures, short version submitted to the SC'19 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose COSMA: a parallel matrix-matrix multiplication algorithm that is\nnear communication-optimal for all combinations of matrix dimensions, processor\ncounts, and memory sizes. The key idea behind COSMA is to derive an optimal (up\nto a factor of 0.03\\% for 10MB of fast memory) sequential schedule and then\nparallelize it, preserving I/O optimality. To achieve this, we use the red-blue\npebble game to precisely model MMM dependencies and derive a constructive and\ntight sequential and parallel I/O lower bound proofs. Compared to 2D or 3D\nalgorithms, which fix processor decomposition upfront and then map it to the\nmatrix dimensions, it reduces communication volume by up to $\\sqrt{3}$ times.\nCOSMA outperforms the established ScaLAPACK, CARMA, and CTF algorithms in all\nscenarios up to 12.8x (2.2x on average), achieving up to 88\\% of Piz Daint's\npeak performance. Our work does not require any hand tuning and is maintained\nas an open source implementation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 11:40:17 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 00:24:39 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 15:36:04 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Kwasniewski", "Grzegorz", ""], ["Kabi\u0107", "Marko", ""], ["Besta", "Maciej", ""], ["VandeVondele", "Joost", ""], ["Solc\u00e0", "Raffaele", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1908.10033", "submitter": "Shantanu Sharma", "authors": "Nisha Panwar, Shantanu Sharma, Guoxi Wang, Sharad Mehrotra, Nalini\n  Venkatasubramanian, Mamadou H. Diallo, Ardalan Amiri Sani", "title": "IoT Notary: Sensor Data Attestation in Smart Environment", "comments": "Accepted in IEEE International Symposium on Network Computing and\n  Applications (NCA), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary IoT environments, such as smart buildings, require end-users to\ntrust data-capturing rules published by the systems. There are several reasons\nwhy such a trust is misplaced --- IoT systems may violate the rules\ndeliberately or IoT devices may transfer user data to a malicious third-party\ndue to cyberattacks, leading to the loss of individuals' privacy or service\nintegrity. To address such concerns, we propose IoT Notary, a framework to\nensure trust in IoT systems and applications. IoT Notary provides secure log\nsealing on live sensor data to produce a verifiable `proof-of-integrity,' based\non which a verifier can attest that captured sensor data adheres to the\npublished data-capturing rules. IoT Notary is an integral part of TIPPERS, a\nsmart space system that has been deployed at UCI to provide various real-time\nlocation-based services in the campus. IoT Notary imposes nominal overheads for\nverification, thereby users can verify their data of one day in less than two\nseconds.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 05:10:04 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Wang", "Guoxi", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""], ["Diallo", "Mamadou H.", ""], ["Sani", "Ardalan Amiri", ""]]}, {"id": "1908.10036", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar", "title": "Performance modeling of a distributed file-system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centers have become center of big data processing. Most programs running\nin a data center processes big data. The storage requirements of such programs\ncannot be fulfilled by a single node in the data center, and hence a\ndistributed file system is used where the the storage resource are pooled\ntogether from more than one node and presents a unified view of it to outside\nworld. Optimum performance of these distributed file-systems given a workload\nis of paramount important as disk being the slowest component in the framework.\nOwning to this fact, many big data processing frameworks implement their own\nfile-system to get the optimal performance by fine tuning it for their specific\nworkloads. However, fine-tuning a file system for a particular workload results\nin poor performance for workloads that do not match the profile of desired\nworkload. Hence, these file systems cannot be used for general purpose usage,\nwhere the workload characteristics shows high variation. In this paper we model\nthe performance of a general purpose file-system and analyse the impact of\ntuning the file-system on its performance. Performance of these parallel\nfile-systems are not easy to model because the performance depends on a lot of\nconfiguration parameters, like the network, disk, under lying file system,\nnumber of servers, number of clients, parallel file-system configuration etc.\nWe present a Multiple Linear regression model that can capture the relationship\nbetween the configuration parameters of a file system, hardware configuration,\nworkload configuration (collectively called features) and the performance\nmetrics. We use this to rank the features according to their importance in\ndeciding the performance of the file-system.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 05:38:54 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Kumar", "Sandeep", ""]]}, {"id": "1908.10040", "submitter": "EPTCS", "authors": "Frank S. de Boer (CWI Amsterdam, The Netherlands), Elena Giachino\n  (University of Bologna, Italy), Stijn de Gouw (The Open University, The\n  Netherlands), Reiner H\\\"ahnle (Technical University of Darmstadt, Germany),\n  Einar Broch Johnsen (University of Oslo, Norway), Cosimo Laneve (University\n  of Bologna, Italy), Ka I Pun (Western Norway University of Applied Sciences,\n  University of Oslo, Norway), Gianluigi Zavattaro (University of Bologna,\n  Italy)", "title": "Analysis of SLA Compliance in the Cloud -- An Automated, Model-based\n  Approach", "comments": "In Proceedings VORTEX 2018, arXiv:1908.09302", "journal-ref": "EPTCS 302, 2019, pp. 1-15", "doi": "10.4204/EPTCS.302.1", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service Level Agreements (SLA) are commonly used to specify the quality\nattributes between cloud service providers and the customers. A violation of\nSLAs can result in high penalties. To allow the analysis of SLA compliance\nbefore the services are deployed, we describe in this paper an approach for\nSLA-aware deployment of services on the cloud, and illustrate its workflow by\nmeans of a case study. The approach is based on formal models combined with\nstatic analysis tools and generated runtime monitors. As such, it fits well\nwithin a methodology combining software development with information technology\noperations (DevOps).\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 06:19:26 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["de Boer", "Frank S.", "", "CWI Amsterdam, The Netherlands"], ["Giachino", "Elena", "", "University of Bologna, Italy"], ["de Gouw", "Stijn", "", "The Open University, The\n  Netherlands"], ["H\u00e4hnle", "Reiner", "", "Technical University of Darmstadt, Germany"], ["Johnsen", "Einar Broch", "", "University of Oslo, Norway"], ["Laneve", "Cosimo", "", "University\n  of Bologna, Italy"], ["Pun", "Ka I", "", "Western Norway University of Applied Sciences,\n  University of Oslo, Norway"], ["Zavattaro", "Gianluigi", "", "University of Bologna,\n  Italy"]]}, {"id": "1908.10042", "submitter": "EPTCS", "authors": "Wolfgang Ahrendt (Chalmers University of Technology), Ludovic Henrio\n  (Univ Lyon, EnsL, UCBL, CNRS, Inria, LIP), Wytse Oortwijn (University of\n  Twente)", "title": "Who is to Blame? Runtime Verification of Distributed Objects with Active\n  Monitors", "comments": "In Proceedings VORTEX 2018, arXiv:1908.09302", "journal-ref": "EPTCS 302, 2019, pp. 32-46", "doi": "10.4204/EPTCS.302.3", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since distributed software systems are ubiquitous, their correct functioning\nis crucially important. Static verification is possible in principle, but\nrequires high expertise and effort which is not feasible in many eco-systems.\nRuntime verification can serve as a lean alternative, where monitoring\nmechanisms are automatically generated from property specifications, to check\ncompliance at runtime. This paper contributes a practical solution for powerful\nand flexible runtime verification of distributed, object-oriented applications,\nvia a combination of the runtime verification tool Larva and the active object\nframework ProActive. Even if Larva supports in itself only the generation of\nlocal, sequential monitors, we empower Larva for distributed monitoring by\nconnecting monitors with active objects, turning them into active,\ncommunicating monitors. We discuss how this allows for a variety of monitoring\narchitectures. Further, we show how property specifications, and thereby the\ngenerated monitors, provide a model that splits the blame between the local\nobject and its environment. While Larva itself focuses on monitoring of\ncontrol-oriented properties, we use the Larva front-end StaRVOOrS to also\ncapture data-oriented (pre/post) properties in the distributed monitoring. We\ndemonstrate this approach to distributed runtime verification with a case\nstudy, a distributed key/value store.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 06:20:22 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Ahrendt", "Wolfgang", "", "Chalmers University of Technology"], ["Henrio", "Ludovic", "", "Univ Lyon, EnsL, UCBL, CNRS, Inria, LIP"], ["Oortwijn", "Wytse", "", "University of\n  Twente"]]}, {"id": "1908.10086", "submitter": "Klaus-Tycho Foerster", "authors": "Klaus-Tycho Foerster, Stefan Schmid", "title": "Distributed Consistent Network Updates in SDNs: Local Verification for\n  Global Guarantees", "comments": "Appears in IEEE NCA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While SDNs enable more flexible and adaptive network operations, (logically)\ncentralized reconfigurations introduce overheads and delays, which can limit\nnetwork reactivity. This paper initiates the study of a more distributed\napproach, in which the consistent network updates are implemented by the\nswitches and routers directly in the data plane. In particular, our approach\nleverages concepts from local proof labeling systems, which allows the data\nplane elements to locally check network properties, and we show that this is\nsufficient to obtain global network guarantees. We demonstrate our approach\nconsidering three fundamental use cases, and analyze its benefits in terms of\nperformance and fault-tolerance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 08:52:36 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Foerster", "Klaus-Tycho", ""], ["Schmid", "Stefan", ""]]}, {"id": "1908.10290", "submitter": "Zhi Cao", "authors": "Zhi Cao, Honggang Zhang, Yu Cao, Benyuan Liu", "title": "A Deep Reinforcement Learning Approach to Multi-component Job Scheduling\n  in Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the optimal scheduling of a collection of\nmulti-component application jobs in an edge computing system that consists of\ngeo-distributed edge computing nodes connected through a wide area network. The\nscheduling and placement of application jobs in an edge system is challenging\ndue to the interdependence of multiple components of each job, and the\ncommunication delays between the geographically distributed data sources and\nedge nodes and their dynamic availability. In this paper we explore the\nfeasibility of applying Deep Reinforcement Learning (DRL) based design to\naddress these challenges. We introduce a DRL actor-critic algorithm that aims\nto find an optimal scheduling policy to minimize average job slowdown in the\nedge system. We have demonstrated through simulations that our design\noutperforms a few existing algorithms, based on both synthetic data and a\nGoogle cloud data trace.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 16:13:18 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 04:12:47 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 04:00:16 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Cao", "Zhi", ""], ["Zhang", "Honggang", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""]]}, {"id": "1908.10310", "submitter": "Kazuyuki Shudo", "authors": "Yoshiki Takahashi, Masato Asahara, Kazuyuki Shudo", "title": "A Framework for Model Search Across Multiple Machine Learning\n  Implementations", "comments": "Proc. 15h Int'l eScience Conference (eScience 2019), September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recently devised machine learning (ML) algorithms have shown improved\naccuracy for various predictive problems. Model searches, which explore to find\nan optimal ML algorithm and hyperparameter values for the target problem, play\na critical role in such improvements. During a model search, data scientists\ntypically use multiple ML implementations to construct several predictive\nmodels; however, it takes significant time and effort to employ multiple ML\nimplementations due to the need to learn how to use them, prepare input data in\nseveral different formats, and compare their outputs. Our proposed framework\naddresses these issues by providing simple and unified coding method. It has\nbeen designed with the following two attractive features: i) new machine\nlearning implementations can be added easily via common interfaces between the\nframework and ML implementations and ii) it can be scaled to handle large model\nconfiguration search spaces via profile-based scheduling. The results of our\nevaluation indicate that, with our framework, implementers need only write\n55-144 lines of code to add a new ML implementation. They also show that ours\nwas the fastest framework for the HIGGS dataset, and the second-fastest for the\nSECOM dataset.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 16:35:22 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Takahashi", "Yoshiki", ""], ["Asahara", "Masato", ""], ["Shudo", "Kazuyuki", ""]]}, {"id": "1908.10388", "submitter": "Maxwell Young", "authors": "Qian M. Zhou, Aiden Calvert, Maxwell Young", "title": "Singletons for Simpletons: Revisiting Windowed Backoff using Chernoff\n  Bounds", "comments": "Corrections to first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backoff algorithms are used in many distributed systems where multiple\ndevices contend for a shared resource. For the classic balls-into-bins problem,\nthe number of singletons---those bins with a single ball---is important to the\nanalysis of several backoff algorithms; however, existing analyses employ\nadvanced probabilistic tools to obtain concentration bounds. Here, we show that\nstandard Chernoff bounds can be used instead, and the simplicity of this\napproach is illustrated by re-analyzing some well-known backoff algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:17:24 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 14:28:18 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Zhou", "Qian M.", ""], ["Calvert", "Aiden", ""], ["Young", "Maxwell", ""]]}, {"id": "1908.10574", "submitter": "Stuart Byma", "authors": "Stuart Byma, Akash Dhasade, Adrian Altenhoff, Christophe Dessimoz,\n  James R. Larus", "title": "Parallel and Scalable Precise Clustering for Homologous Protein\n  Discovery", "comments": "11 pages, 11 figures. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new, parallel implementation of clustering and\ndemonstrates its utility in greatly speeding up the process of identifying\nhomologous proteins. Clustering is a technique to reduce the number of\ncomparison needed to find similar pairs in a set of $n$ elements such as\nprotein sequences. Precise clustering ensures that each pair of similar\nelements appears together in at least one cluster, so that similarities can be\nidentified by all-to-all comparison in each cluster rather than on the full\nset. This paper introduces ClusterMerge, a new algorithm for precise clustering\nthat uses transitive relationships among the elements to enable parallel and\nscalable implementations of this approach. We apply ClusterMerge to the\nimportant problem of finding similar amino acid sequences in a collection of\nproteins. ClusterMerge identifies 99.8% of similar pairs found by a full\n$O(n^2)$ comparison, with only half as many operations. More importantly,\nClusterMerge is highly amenable to parallel and distributed computation. Our\nimplementation achieves a speedup of 604$\\times$ on 768 cores (1400$\\times$\nfaster than a comparable single-threaded clustering implementation), a strong\nscaling efficiency of 90%, and a weak scaling efficiency of nearly 100%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:11:38 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Byma", "Stuart", ""], ["Dhasade", "Akash", ""], ["Altenhoff", "Adrian", ""], ["Dessimoz", "Christophe", ""], ["Larus", "James R.", ""]]}, {"id": "1908.10630", "submitter": "Nguyen Truong", "authors": "Nguyen Truong, Kai Sun, Yike Guo", "title": "Blockchain-based Personal Data Management: From Fiction to Solution", "comments": "8 pages, 5 figures; accepted to be published as full paper at IEEE\n  International Symposium on Network Computing and Applications (NCA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging blockchain technology has enabled various decentralised\napplications in a trustless environment without relying on a trusted\nintermediary. It is expected as a promising solution to tackle sophisticated\nchallenges on personal data management, thanks to its advanced features such as\nimmutability, decentralisation and transparency. Although certain approaches\nhave been proposed to address technical difficulties in personal data\nmanagement; most of them only provided preliminary methodological exploration.\nAlarmingly, when utilising Blockchain for developing a personal data management\nsystem, fictions have occurred in existing approaches and been promulgated in\nthe literature. Such fictions are theoretically doable; however, by thoroughly\nbreaking down consensus protocols and transaction validation processes, we\nclarify that such existing approaches are either impractical or highly\ninefficient due to the natural limitations of the blockchain and Smart\nContracts technologies. This encourages us to propose a feasible solution in\nwhich such fictions are reduced by designing a novel system architecture with a\nblockchain-based \"proof of permission\" protocol. We demonstrate the feasibility\nand efficiency of the proposed models by implementing a clinical data sharing\nservice built on top of a public blockchain platform. We believe that our\nresearch resolves existing ambiguity and take a step further on providing a\npractically feasible solution for decentralised personal data management.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 10:40:50 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Truong", "Nguyen", ""], ["Sun", "Kai", ""], ["Guo", "Yike", ""]]}, {"id": "1908.10716", "submitter": "Robbert Van Renesse", "authors": "Robbert van Renesse", "title": "Asynchronous Consensus Without Rounds", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault tolerant consensus protocols usually involve ordered rounds of voting\nbetween a collection of processes. In this paper, we derive a general\nspecification of fault tolerant asynchronous consensus protocols and present a\nclass of consensus protocols that refine this specification without using\nrounds. Crash-tolerant protocols in this class use 3f+1 processes, while\nByzantine-tolerant protocols use 5f+1 processes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 13:27:05 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["van Renesse", "Robbert", ""]]}, {"id": "1908.10743", "submitter": "EPTCS", "authors": "Giorgio Audrito (University of Turin, Italy), Ferruccio Damiani\n  (University of Turin, Italy), Volker Stolz (Western Norway University of\n  Applied Sciences, Norway), Mirko Viroli (University of Bologna, Italy)", "title": "On Distributed Runtime Verification by Aggregate Computing", "comments": "In Proceedings VORTEX 2018, arXiv:1908.09302", "journal-ref": "EPTCS 302, 2019, pp. 47-61", "doi": "10.4204/EPTCS.302.4", "report-no": null, "categories": "cs.SE cs.DC cs.MA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime verification is a computing analysis paradigm based on observing a\nsystem at runtime (to check its expected behaviour) by means of monitors\ngenerated from formal specifications. Distributed runtime verification is\nruntime verification in connection with distributed systems: it comprises both\nmonitoring of distributed systems and using distributed systems for monitoring.\nAggregate computing is a programming paradigm based on a reference computing\nmachine that is the aggregate collection of devices that cooperatively carry\nout a computational process: the details of behaviour, position and number of\ndevices are largely abstracted away, to be replaced with a space-filling\ncomputational environment. In this position paper we argue, by means of simple\nexamples, that aggregate computing is particularly well suited for implementing\ndistributed monitors. Our aim is to foster further research on how to generate\naggregate computing monitors from suitable formal specifications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 06:20:41 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Audrito", "Giorgio", "", "University of Turin, Italy"], ["Damiani", "Ferruccio", "", "University of Turin, Italy"], ["Stolz", "Volker", "", "Western Norway University of\n  Applied Sciences, Norway"], ["Viroli", "Mirko", "", "University of Bologna, Italy"]]}, {"id": "1908.10826", "submitter": "Sean Ovens", "authors": "Sean Ovens and Philipp Woelfel", "title": "Strongly Linearizable Implementations of Snapshots and Other Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability is the gold standard of correctness conditions for shared\nmemory algorithms, and historically has been considered the practical\nequivalent of atomicity. However, it has been shown [1] that replacing atomic\nobjects with linearizable implementations can affect the probability\ndistribution of execution outcomes in randomized algorithms. Thus, linearizable\nobjects are not always suitable replacements for atomic objects. A stricter\ncorrectness condition called strong linearizability has been developed and\nshown to be appropriate for randomized algorithms in a strong adaptive\nadversary model [1]. We devise several new lock-free strongly linearizable\nimplementations from atomic registers. In particular, we give the first\nstrongly linearizable lock-free snapshot implementation that uses bounded\nspace. This improves on the unbounded space solution of Denysyuk and Woelfel\n[2]. As a building block, our algorithm uses a lock-free strongly linearizable\nABA-detecting register. We obtain this object by modifying the wait-free\nlinearizable ABA-detecting register of Aghazadeh and Woelfel [3], which, as we\nshow, is not strongly linearizable. Aspnes and Herlihy [4] identified a wide\nclass types that have wait-free linearizable implementations from atomic\nregisters. These types require that any pair of operations either commute, or\none overwrites the other. Aspnes and Herlihy gave a general wait-free\nlinearizable implementation of such types, employing a wait-free linearizable\nsnapshot object. Replacing that snapshot object with our lock-free strongly\nlinearizable one, we prove that all types in this class have a lock-free\nstrongly linearizable implementation from atomic registers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 16:56:08 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Ovens", "Sean", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1908.10834", "submitter": "Ang Li", "authors": "Tong Geng, Ang Li, Runbin Shi, Chunshu Wu, Tianqi Wang, Yanfei Li,\n  Pouya Haghi, Antonino Tumeo, Shuai Che, Steve Reinhardt, Martin Herbordt", "title": "AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload\n  Rebalancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning systems have been successfully applied to Euclidean data such\nas images, video, and audio. In many applications, however, information and\ntheir relationships are better expressed with graphs. Graph Convolutional\nNetworks (GCNs) appear to be a promising approach to efficiently learn from\ngraph data structures, having shown advantages in many critical applications.\nAs with other deep learning modalities, hardware acceleration is critical. The\nchallenge is that real-world graphs are often extremely large and unbalanced;\nthis poses significant performance demands and design challenges.\n  In this paper, we propose Autotuning-Workload-Balancing GCN (AWB-GCN) to\naccelerate GCN inference. To address the issue of workload imbalance in\nprocessing real-world graphs, three hardware-based autotuning techniques are\nproposed: dynamic distribution smoothing, remote switching, and row remapping.\nIn particular, AWB-GCN continuously monitors the sparse graph pattern,\ndynamically adjusts the workload distribution among a large number of\nprocessing elements (up to 4K PEs), and, after converging, reuses the ideal\nconfiguration. Evaluation is performed using an Intel D5005 FPGA with five\ncommonly-used datasets. Results show that 4K-PE AWB-GCN can significantly\nelevate PE utilization by 7.7x on average and demonstrate considerable\nperformance speedups over CPUs (3255x), GPUs (80.3x), and a prior GCN\naccelerator (5.1x).\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 17:18:49 GMT"}, {"version": "v10", "created": "Fri, 11 Sep 2020 01:50:22 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 00:55:01 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 04:14:59 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 01:27:39 GMT"}, {"version": "v5", "created": "Thu, 30 Apr 2020 04:58:53 GMT"}, {"version": "v6", "created": "Thu, 30 Jul 2020 21:41:06 GMT"}, {"version": "v7", "created": "Fri, 14 Aug 2020 02:01:00 GMT"}, {"version": "v8", "created": "Sun, 6 Sep 2020 16:17:54 GMT"}, {"version": "v9", "created": "Thu, 10 Sep 2020 15:52:32 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Geng", "Tong", ""], ["Li", "Ang", ""], ["Shi", "Runbin", ""], ["Wu", "Chunshu", ""], ["Wang", "Tianqi", ""], ["Li", "Yanfei", ""], ["Haghi", "Pouya", ""], ["Tumeo", "Antonino", ""], ["Che", "Shuai", ""], ["Reinhardt", "Steve", ""], ["Herbordt", "Martin", ""]]}, {"id": "1908.11373", "submitter": "Salonik Resch", "authors": "Salonik Resch, S. Karen Khatamifard, Zamshed Iqbal Chowdhury, Masoud\n  Zabihi, Zhengyang Zhao, Jian-Ping Wang, Sachin S. Sapatnekar, Ulya R.\n  Karpuzcu", "title": "A Machine Learning Accelerator In-Memory for Energy Harvesting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing demand to bring machine learning capabilities to low\npower devices. By integrating the computational power of machine learning with\nthe deployment capabilities of low power devices, a number of new applications\nbecome possible. In some applications, such devices will not even have a\nbattery, and must rely solely on energy harvesting techniques. This puts\nextreme constraints on the hardware, which must be energy efficient and capable\nof tolerating interruptions due to power outages. Here, as a representative\nexample, we propose an in-memory support vector machine learning accelerator\nutilizing non-volatile spintronic memory. The combination of\nprocessing-in-memory and non-volatility provides a key advantage in that\nprogress is effectively saved after every operation. This enables instant shut\ndown and restart capabilities with minimal overhead. Additionally, the\noperations are highly energy efficient leading to low power consumption.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 02:32:05 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Resch", "Salonik", ""], ["Khatamifard", "S. Karen", ""], ["Chowdhury", "Zamshed Iqbal", ""], ["Zabihi", "Masoud", ""], ["Zhao", "Zhengyang", ""], ["Wang", "Jian-Ping", ""], ["Sapatnekar", "Sachin S.", ""], ["Karpuzcu", "Ulya R.", ""]]}, {"id": "1908.11402", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e, Andrzej Pelc", "title": "Want to Gather? No Need to Chatter!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A team of mobile agents, starting from different nodes of an unknown network,\npossibly at different times, have to meet at the same node and declare that\nthey have all met. Agents have different labels and move in synchronous rounds\nalong links of the network. The above task is known as gathering and was\ntraditionally considered under the assumption that when some agents are at the\nsame node then they can talk. In this paper we ask the question of whether this\nability of talking is needed for gathering. The answer turns out to be no.\n  Our main contribution are two deterministic algorithms that always accomplish\ngathering in a much weaker model. We only assume that at any time an agent\nknows how many agents are at the node that it currently occupies but agents do\nnot see the labels of other co-located agents and cannot exchange any\ninformation with them. They also do not see other nodes than the current one.\nOur first algorithm works under the assumption that agents know a priori some\nupper bound N on the network size, and it works in time polynomial in N and in\nthe length l of the smallest label. Our second algorithm does not assume any a\npriori knowledge about the network but its complexity is exponential in the\nnetwork size and in the labels of agents. Its purpose is to show feasibility of\ngathering under this harsher scenario.\n  As a by-product of our techniques we obtain, in the same weak model, the\nsolution of the fundamental problem of leader election among agents. As an\napplication of our result we also solve, in the same model, the well-known\ngossiping problem: if each agent has a message at the beginning, we show how to\nmake all messages known to all agents, even without any a priori knowledge\nabout the network. If agents know an upper bound N on the network size then our\ngossiping algorithm works in time polynomial in N, in l and in the length of\nthe largest message.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 18:08:53 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1908.11450", "submitter": "Siqi Wang", "authors": "Siqi Wang, Anuj Pathania, Tulika Mitra", "title": "Neural Network Inference on Mobile SoCs", "comments": "Accepted to IEEE Design & Test", "journal-ref": "in IEEE Design & Test, vol. 37, no. 5, pp. 50-57, Oct. 2020", "doi": "10.1109/MDAT.2020.2968258", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing demand from mobile Machine Learning (ML) applications\ncalls for evermore powerful on-chip computing resources. Mobile devices are\nempowered with heterogeneous multi-processor Systems-on-Chips (SoCs) to process\nML workloads such as Convolutional Neural Network (CNN) inference. Mobile SoCs\nhouse several different types of ML capable components on-die, such as CPU,\nGPU, and accelerators. These different components are capable of independently\nperforming inference but with very different power-performance characteristics.\nIn this article, we provide a quantitative evaluation of the inference\ncapabilities of the different components on mobile SoCs. We also present\ninsights behind their respective power-performance behavior. Finally, we\nexplore the performance limit of the mobile SoCs by synergistically engaging\nall the components concurrently. We observe that a mobile SoC provides up to 2x\nimprovement with parallel inference when all its components are engaged, as\nopposed to engaging only one component.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 07:13:57 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 16:03:15 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Wang", "Siqi", ""], ["Pathania", "Anuj", ""], ["Mitra", "Tulika", ""]]}, {"id": "1908.11488", "submitter": "Francois Le Gall", "authors": "Taisuke Izumi, Fran\\c{c}ois Le Gall and Fr\\'ed\\'eric Magniez", "title": "Quantum Distributed Algorithm for Triangle Finding in the CONGEST Model", "comments": "13 pages; v2: minor corrections; to appear in the proceedings of\n  STACS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the triangle finding problem in the CONGEST model of\ndistributed computing. Recent works by Izumi and Le Gall (PODC'17), Chang,\nPettie and Zhang (SODA'19) and Chang and Saranurak (PODC'19) have successively\nreduced the classical round complexity of triangle finding (as well as triangle\nlisting) from the trivial upper bound $O(n)$ to $\\tilde O(n^{1/3})$, where~$n$\ndenotes the number of vertices in the graph. In this paper we present a quantum\ndistributed algorithm that solves the triangle finding problem in $\\tilde\nO(n^{1/4})$ rounds in the CONGEST model. This gives another example of quantum\nalgorithm beating the best known classical algorithms in distributed computing.\nOur result also exhibits an interesting phenomenon: while in the classical\nsetting the best known upper bounds for the triangle finding and listing\nproblems are identical, in the quantum setting the round complexities of these\ntwo problems are now $\\tilde O(n^{1/4})$ and $\\tilde \\Theta(n^{1/3})$,\nrespectively. Our result thus shows that triangle finding is easier than\ntriangle listing in the quantum CONGEST model.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 00:10:08 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 02:06:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Izumi", "Taisuke", ""], ["Gall", "Fran\u00e7ois Le", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1908.11501", "submitter": "Manoj Muniswamaiah", "authors": "Manoj Muniswamaiah and Dr. Charles Tappert", "title": "Mobile Cloud Computing in Healthcare Using Dynamic Cloudlets for\n  Energy-Aware Consumption", "comments": null, "journal-ref": null, "doi": "10.5121/csit.2019.91006", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile cloud computing (MCC) has increasingly been adopted in healthcare\nindustry by healthcare professionals (HCPs) which has resulted in the growth of\nmedical software applications for these platforms. There are different\napplications which help HCPs with many important tasks. Mobile cloud computing\nhas helped HCPs in better decision making and improved patient care. MCC\nenables users to acquire the benefit of cloud computing services to meet the\nhealthcare demands. However, the restrictions posed by network bandwidth and\nmobile device capacity has brought challenges with respect to energy\nconsumption and latency delays. In this paper we propose dynamic energy\nconsumption mobile cloud computing model (DEMCCM) which addresses the energy\nconsumption issue by healthcare mobile devices using dynamic cloudlets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 01:41:38 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Muniswamaiah", "Manoj", ""], ["Tappert", "Dr. Charles", ""]]}, {"id": "1908.11551", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Gary S. H. Tan", "title": "Internet-based Adaptive Distributed Simulation of Mobile Ad-hoc Networks", "comments": "Proceedings of the Proceedings of the 2019 Winter Simulation\n  Conference (WSC 2019)", "journal-ref": null, "doi": "10.1109/WSC40007.2019.9004796", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on Internet-based simulation, a form of distributed\nsimulation in which a set of execution units that are physically located around\nthe globe work together to run a simulation model. This setup is very\nchallenging because of the latency/variability of communications. Thus, clever\nmechanisms must be adopted in the distributed simulation, such as the adaptive\npartitioning of the simulated model and load balancing strategies among\nexecution units. We simulate a wireless model over a real Internet-based\ndistributed simulation setup, and evaluate the scalability of the simulator\nwith and without the use of adaptive strategies for both communication overhead\nreduction and load-balancing enhancement. The results confirm the viability of\nour approach to build Internet-based simulations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 06:22:35 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 06:26:13 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Tan", "Gary S. H.", ""]]}, {"id": "1908.11660", "submitter": "Shubhendra Singhal Mr.", "authors": "Shubhendra Pal Singhal, M.Sridevi", "title": "Comparative study of performance of parallel Alpha Beta Pruning for\n  different architectures", "comments": "5 pages, 6 figures, Accepted in 2019 IEEE 9th International Advance\n  Computing Conference(IEEE Xplore)", "journal-ref": "2019 IEEE 9th International Conference on Advanced Computing\n  (IACC), Tiruchirappalli, India, 2019, pp. 115-119", "doi": "10.1109/IACC48062.2019.8971591", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of searching the best possible action depending on various\nstates like state of environment, system goal etc. has been a major area of\nstudy in computer systems. In any search algorithm, searching best possible\nsolution from the pool of every possibility known can lead to the construction\nof the whole state search space popularly called as minimax algorithm. This may\nlead to a impractical time complexities which may not be suitable for real time\nsearching operations. One of the practical solution for the reduction in\ncomputational time is Alpha Beta pruning. Instead of searching for the whole\nstate space, we prune the unnecessary branches, which helps reduce the time by\nsignificant amount. This paper focuses on the various possible implementations\nof the Alpha Beta pruning algorithms and gives an insight of what algorithm can\nbe used for parallelism. Various studies have been conducted on how to make\nAlpha Beta pruning faster. Parallelizing Alpha Beta pruning for the GPUs\nspecific architectures like mesh(CUDA) etc. or shared memory model(OpenMP)\nhelps in the reduction of the computational time. This paper studies the\ncomparison between sequential and different parallel forms of Alpha Beta\npruning and their respective efficiency for the chess game as an application.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 11:36:38 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 11:05:56 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Singhal", "Shubhendra Pal", ""], ["Sridevi", "M.", ""]]}, {"id": "1908.11740", "submitter": "Panagiotis Bouros", "authors": "Dimitrios Tsitsigkos and Panagiotis Bouros and Nikos Mamoulis and\n  Manolis Terrovitis", "title": "Parallel In-Memory Evaluation of Spatial Joins", "comments": "Extended version of the SIGSPATIAL'19 paper under the same title", "journal-ref": "27th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (ACM SIGSPATIAL GIS 2019), Chicago, Illinois,\n  USA, November 5-8, 2019", "doi": "10.1145/3347146.3359343", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial join is a popular operation in spatial database systems and its\nevaluation is a well-studied problem. As main memories become bigger and faster\nand commodity hardware supports parallel processing, there is a need to revamp\nclassic join algorithms which have been designed for I/O-bound processing. In\nview of this, we study the in-memory and parallel evaluation of spatial joins,\nby re-designing a classic partitioning-based algorithm to consider alternative\napproaches for space partitioning. Our study shows that, compared to a\nstraightforward implementation of the algorithm, our tuning can improve\nperformance significantly. We also show how to select appropriate partitioning\nparameters based on data statistics, in order to tune the algorithm for the\ngiven join inputs. Our parallel implementation scales gracefully with the\nnumber of threads reducing the cost of the join to at most one second even for\njoin inputs with tens of millions of rectangles.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 13:48:31 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 08:54:01 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Tsitsigkos", "Dimitrios", ""], ["Bouros", "Panagiotis", ""], ["Mamoulis", "Nikos", ""], ["Terrovitis", "Manolis", ""]]}, {"id": "1908.11779", "submitter": "Eric Veith", "authors": "Eric M.S.P. Veith, Lars Fischer, Martin Tr\\\"oschel, Astrid Nie{\\ss}e", "title": "Analyzing Cyber-Physical Systems from the Perspective of Artificial\n  Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principles of modern cyber-physical system (CPS) analysis are based on\nanalytical methods that depend on whether safety or liveness requirements are\nconsidered. Complexity is abstracted through different techniques, ranging from\nstochastic modelling to contracts. However, both distributed heuristics and\nArtificial Intelligence (AI)-based approaches as well as the user perspective\nor unpredictable effects, such as accidents or the weather, introduce enough\nuncertainty to warrant reinforcement-learning-based approaches. This paper\ncompares traditional approaches in the domain of CPS modelling and analysis\nwith the AI researcher perspective to exploring unknown complex systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 20:57:27 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Veith", "Eric M. S. P.", ""], ["Fischer", "Lars", ""], ["Tr\u00f6schel", "Martin", ""], ["Nie\u00dfe", "Astrid", ""]]}, {"id": "1908.11780", "submitter": "Kunal Lillaney", "authors": "Kunal Lillaney (1), Vasily Tarasov (2), David Pease (2), Randal Burns\n  (1) ((1) Johns Hopkins University, (2) IBM Research-Almaden)", "title": "Towards Marrying Files to Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deal with the constant growth of unstructured data, vendors have deployed\nscalable, resilient, and cost effective object-based storage systems built on\nRESTful web services. However, many applications rely on richer file-system\nAPIs and semantics, and cannot benefit from object stores. This leads to\nstorage sprawl, as object stores are deployed alongside file systems and data\nis accessed and managed across both systems in an ad-hoc fashion. We believe\nthere is a critical need for a transparent merger of objects and files,\nconsolidating data into a single platform. Such a merger would extend the\ncapabilities of both object and file stores while preserving existing semantics\nand interfaces. In this position paper, we examine the viability of unifying\nobject stores and file systems, and the various design tradeoffs that exist.\nThen, using our own implementation of an object-based, POSIX-complete file\nsystem, we experimentally demonstrate several critical design considerations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 17:02:02 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Lillaney", "Kunal", "", "Johns Hopkins University"], ["Tarasov", "Vasily", "", "IBM Research-Almaden"], ["Pease", "David", "", "IBM Research-Almaden"], ["Burns", "Randal", "", "Johns Hopkins University"]]}, {"id": "1908.11781", "submitter": "Yuke Wang", "authors": "Yuke Wang, Boyuan Feng, Gushu Li, Lei Deng, Yuan Xie, Yufei Ding", "title": "AccD: A Compiler-based Framework for Accelerating Distance-related\n  Algorithms on CPU-FPGA Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a promising solution to boost the performance of distance-related\nalgorithms (e.g., K-means and KNN), FPGA-based acceleration attracts lots of\nattention, but also comes with numerous challenges. In this work, we propose\nAccD, a compiler-based framework for accelerating distance-related algorithms\non CPU-FPGA platforms. Specifically, AccD provides a Domain-specific Language\nto unify distance-related algorithms effectively, and an optimizing compiler to\nreconcile the benefits from both the algorithmic optimization on the CPU and\nthe hardware acceleration on the FPGA. The output of AccD is a high-performance\nand power-efficient design that can be easily synthesized and deployed on\nmainstream CPU-FPGA platforms. Intensive experiments show that AccD designs\nachieve 31.42x speedup and 99.63x better energy efficiency on average over\nstandard CPU-based implementations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 19:15:26 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Wang", "Yuke", ""], ["Feng", "Boyuan", ""], ["Li", "Gushu", ""], ["Deng", "Lei", ""], ["Xie", "Yuan", ""], ["Ding", "Yufei", ""]]}, {"id": "1908.11807", "submitter": "Andrey Prokopenko", "authors": "D. Lebrun-Grandi\\'e, A. Prokopenko, B. Turcksin, S.R. Slattery", "title": "ArborX: A Performance Portable Geometric Search Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for geometric objects that are close in space is a fundamental\ncomponent of many applications. The performance of search algorithms comes to\nthe forefront as the size of a problem increases both in terms of total object\ncount as well as in the total number of search queries performed. Scientific\napplications requiring modern leadership-class supercomputers also pose an\nadditional requirement of performance portability, i.e. being able to\nefficiently utilize a variety of hardware architectures. In this paper, we\nintroduce a new open-source C++ search library, ArborX, which we have designed\nfor modern supercomputing architectures. We examine scalable search algorithms\nwith a focus on performance, including a highly efficient parallel bounding\nvolume hierarchy implementation, and propose a flexible interface making it\neasy to integrate with existing applications. We demonstrate the performance\nportability of ArborX on multi-core CPUs and GPUs, and compare it to the\nstate-of-the-art libraries such as Boost.Geometry.Index and nanoflann.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:17:32 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 16:10:40 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Lebrun-Grandi\u00e9", "D.", ""], ["Prokopenko", "A.", ""], ["Turcksin", "B.", ""], ["Slattery", "S. R.", ""]]}, {"id": "1908.11808", "submitter": "Gabriele D'Angelo", "authors": "Stefano Ferretti, Gabriele D'Angelo", "title": "On the Ethereum Blockchain Structure: a Complex Networks Theory\n  Perspective", "comments": null, "journal-ref": null, "doi": "10.1002/cpe.5493", "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the Ethereum blockchain using the complex networks\nmodeling framework. Accounts acting on the blockchain are represented as nodes,\nwhile the interactions among these accounts, recorded on the blockchain, are\ntreated as links in the network. Using this representation, it is possible to\nderive interesting mathematical characteristics that improve the understanding\nof the actual interactions happening in the blockchain. Not only, by looking at\nthe history of the blockchain, it is possible to verify if radical changes in\nthe blockchain evolution happened.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 07:55:19 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "1908.11809", "submitter": "Sudarshan Srinivasan", "authors": "Sudarshan Srinivasan, Pradeep Janedula, Saurabh Dhoble, Sasikanth\n  Avancha, Dipankar Das, Naveen Mellempudi, Bharat Daga, Martin Langhammer,\n  Gregg Baeckler, Bharat Kaul", "title": "High Performance Scalable FPGA Accelerator for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision is the first order knob for achieving higher Artificial\nIntelligence Operations (AI-TOPS). However the algorithmic space for sub-8-bit\nprecision compute is diverse, with disruptive changes happening frequently,\nmaking FPGAs a natural choice for Deep Neural Network inference, In this work\nwe present an FPGA-based accelerator for CNN inference acceleration. We use\n{\\it INT-8-2} compute (with {\\it 8 bit} activation and {2 bit} weights) which\nis recently showing promise in the literature, and which no known ASIC, CPU or\nGPU natively supports today. Using a novel Adaptive Logic Module (ALM) based\ndesign, as a departure from traditional DSP based designs, we are able to\nachieve high performance measurement of 5 AI-TOPS for {\\it Arria10} and project\na performance of 76 AI-TOPS at 0.7 TOPS/W for {\\it Stratix10}. This exceeds\nknown CPU, GPU performance and comes close to best known ASIC (TPU) numbers,\nwhile retaining the versatility of the FPGA platform for other applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 07:13:53 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Srinivasan", "Sudarshan", ""], ["Janedula", "Pradeep", ""], ["Dhoble", "Saurabh", ""], ["Avancha", "Sasikanth", ""], ["Das", "Dipankar", ""], ["Mellempudi", "Naveen", ""], ["Daga", "Bharat", ""], ["Langhammer", "Martin", ""], ["Baeckler", "Gregg", ""], ["Kaul", "Bharat", ""]]}, {"id": "1908.11810", "submitter": "Quan Nguyen Hoang", "authors": "Quan Nguyen, Andre Cronje, Michael Kong, Alex Kampa, George Samman", "title": "StairDag: Cross-DAG Validation For Scalable BFT Consensus", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.03655", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new consensus protocol, so-called \\emph{\\stair}, for\nfast consensus in DAG-based trustless system. In \\stair, we propose a new\napproach to creating local block DAG, namely \\emph{x-DAG} (cross-DAG), on each\nnode. \\emph{\\stair} protocol is based on our Proof-of-Stake StakeDag framework\n\\cite{stakedag} that distinguishes participants into users and validators by\ntheir stake. Both users and validators can create and validate event blocks.\nUnlike StakeDag's DAG, x-DAG ensures that each new block has to have parent\nblocks from both Users and Validators to achieve more safety and liveness. Our\nprotocol leverages a pool of validators to expose more validating power to new\nblocks for faster consensus in a leaderless asynchronous system. Further, our\nframework allows participants to join as observers / monitors, who can retrieve\nDAG for post-validation, but do not participate in onchain validation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 12:49:12 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 10:52:44 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Nguyen", "Quan", ""], ["Cronje", "Andre", ""], ["Kong", "Michael", ""], ["Kampa", "Alex", ""], ["Samman", "George", ""]]}, {"id": "1908.11811", "submitter": "Gabriele D'Angelo", "authors": "Edoardo Rosa, Gabriele D'Angelo, Stefano Ferretti", "title": "Agent-based Simulation of Blockchains", "comments": "Proceedings of the 19-th Asia Simulation Conference (AsiaSim 2019)", "journal-ref": null, "doi": "10.1007/978-981-15-1078-6_10", "report-no": null, "categories": "cs.CR cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe LUNES-Blockchain, an agent-based simulator of\nblockchains that is able to exploit Parallel and Distributed Simulation (PADS)\ntechniques to offer a high level of scalability. To assess the preliminary\nimplementation of our simulator, we provide a simplified modelling of the\nBitcoin protocol and we study the effect of a security attack on the consensus\nprotocol in which a set of malicious nodes implements a filtering denial of\nservice (i.e. Sybil Attack). The results confirm the viability of the\nagent-based modelling of blockchains implemented by means of PADS.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:15:13 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 15:37:25 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Rosa", "Edoardo", ""], ["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""]]}, {"id": "1908.11846", "submitter": "Biqing Fang", "authors": "Biqing Fang, Hai Wan, Shaowei Cai, Zejie Cai", "title": "An Incremental Evaluation Mechanism for the Critical Node Problem", "comments": "There are some typos in our experiments, we need about a month to fix\n  it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Critical Node Problem (CNP) is to identify a subset of nodes in a graph\nwhose removal maximally degrades pairwise connectivity. The CNP is an important\nvariant of the Critical Node Detection Problem (CNDP) with wide applications.\nDue to its NP-hardness for general graphs, most works focus on local search\nalgorithms that can return a good quality solution in a reasonable time.\nHowever, computing the objective function of CNP is a frequent procedure and is\ntime-consuming (with complexity O(|V | + |E|)) during the search, which is a\ncommon problem that previous algorithms suffered from. In this paper, we\npropose a general incremental evaluation mechanism (IEM) to compute the\nobjective function with much lower complexity. In this work, we improved two\nimportant greedy operations with IEM, along with experiments. Finally, we\nevaluate IEM by applying it into an evolutionary algorithm on two popular\nbenchmarks, compared with the state-of-the-art approach. The experimental\nresults showed the significance of IEM.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:51:23 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 05:42:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Fang", "Biqing", ""], ["Wan", "Hai", ""], ["Cai", "Shaowei", ""], ["Cai", "Zejie", ""]]}, {"id": "1908.11848", "submitter": "Bao Xin Chen", "authors": "Xing Zhao and Aijun An and Junfeng Liu and Bao Xin Chen", "title": "Dynamic Stale Synchronous Parallel Distributed Training for Deep\n  Learning", "comments": null, "journal-ref": "2019 IEEE 39th International Conference on Distributed Computing\n  Systems (ICDCS)", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a popular machine learning technique and has been applied to\nmany real-world problems. However, training a deep neural network is very\ntime-consuming, especially on big data. It has become difficult for a single\nmachine to train a large model over large datasets. A popular solution is to\ndistribute and parallelize the training process across multiple machines using\nthe parameter server framework. In this paper, we present a distributed\nparadigm on the parameter server framework called Dynamic Stale Synchronous\nParallel (DSSP) which improves the state-of-the-art Stale Synchronous Parallel\n(SSP) paradigm by dynamically determining the staleness threshold at the run\ntime. Conventionally to run distributed training in SSP, the user needs to\nspecify a particular staleness threshold as a hyper-parameter. However, a user\ndoes not usually know how to set the threshold and thus often finds a threshold\nvalue through trial and error, which is time-consuming. Based on workers'\nrecent processing time, our approach DSSP adaptively adjusts the threshold per\niteration at running time to reduce the waiting time of faster workers for\nsynchronization of the globally shared parameters, and consequently increases\nthe frequency of parameters updates (increases iteration throughput), which\nspeedups the convergence rate. We compare DSSP with other paradigms such as\nBulk Synchronous Parallel (BSP), Asynchronous Parallel (ASP), and SSP by\nrunning deep neural networks (DNN) models over GPU clusters in both homogeneous\nand heterogeneous environments. The results show that in a heterogeneous\nenvironment where the cluster consists of mixed models of GPUs, DSSP converges\nto a higher accuracy much earlier than SSP and BSP and performs similarly to\nASP. In a homogeneous distributed cluster, DSSP has more stable and slightly\nbetter performance than SSP and ASP, and converges much faster than BSP.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 23:03:45 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhao", "Xing", ""], ["An", "Aijun", ""], ["Liu", "Junfeng", ""], ["Chen", "Bao Xin", ""]]}, {"id": "1908.11850", "submitter": "Swapnil Haria", "authors": "Swapnil Haria, Mark D. Hill, Michael M. Swift", "title": "MOD: Minimally Ordered Durable Datastructures for Persistent Memory", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent Memory (PM) makes possible recoverable applications that can\npreserve application progress across system reboots and power failures. Actual\nrecoverability requires careful ordering of cacheline flushes, currently done\nin two extreme ways. On one hand, expert programmers have reasoned deeply about\nconsistency and durability to create applications centered on a single\ncustom-crafted durable datastructure. On the other hand, less-expert\nprogrammers have used software transaction memory (STM) to make atomic one or\nmore updates, albeit at a significant performance cost due largely to ordered\nlog updates.\n  In this work, we propose the middle ground of composable persistent\ndatastructures called Minimally Ordered Durable (MOD) datastructures. MOD is a\nC++ library of several datastructures---currently, map, set, stack, queue and\nvector--- that often perform better than STM and yet are relatively easy to\nuse. They allow multiple updates to one or more datastructures to be atomic\nwith respect to failure. Moreover, we provide a recipe to create more\nrecoverable datastructures.\n  MOD is motivated by our analysis of real Intel Optane PM hardware showing\nthat allowing unordered, overlapping flushes significantly improves\nperformance. MOD reduces ordering by adapting existing techniques for\nout-of-place updates (like shadow paging) with space-reducing structural\nsharing (from functional programming). MOD exposes a Basic interface for single\nupdates and a Composition interface for atomically performing multiple updates.\nRelative to the state-of-the-art Intel PMDK v1.5 STM, MOD improves map, set,\nstack, queue microbenchmark performance by 40%, and speeds up application\nbenchmark performance by 38%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 15:11:31 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Haria", "Swapnil", ""], ["Hill", "Mark D.", ""], ["Swift", "Michael M.", ""]]}]