[{"id": "1803.00045", "submitter": "Mansaf Alam Dr", "authors": "Syed Arshad Ali and Mansaf Alam", "title": "Resource-Aware Min-Min (RAMM) Algorithm for Resource Allocation in Cloud\n  Computing Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource allocation (RA) is a significant aspect in Cloud Computing which\nfacilitates the Cloud resources to Cloud consumers as a metered service. The\nCloud resource manager is responsible to assign available resources to the\ntasks for execution in an effective way that improves system performance,\nreduce response time, reduce makespan and utilize resources efficiently. To\nfulfil these objectives, an effective Tasks Scheduling algorithm is required.\nThe standard Min-Min and Max-Min Task Scheduling Algorithms are available, but\nthese algorithms are not able to produce better makespan and effective resource\nutilization. This paper proposed a Resource-Aware Min-Min (RAMM) Algorithm\nbased on classic Min-Min Algorithm. The RAMM Algorithm selects shortest\nexecution time task and assign it to the resource which takes shortest\ncompletion time. If minimum completion time resource is busy then the RAMM\nAlgorithm selects next minimum completion time resource to reduce waiting time\nof task and better resource utilization. The experiment results show that the\nRAMM Algorithm produces better makespan and load balance than standard Min-Min,\nMax-Min and improved Max-Min Algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 13:57:32 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Ali", "Syed Arshad", ""], ["Alam", "Mansaf", ""]]}, {"id": "1803.00087", "submitter": "Souvik  Sengupta", "authors": "Souvik Sengupta, Jordi Garcia, Xavi Masip-Bruin", "title": "A Literature Survey on Ontology of Different Computing Platforms in\n  Smart Environments", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart environments integrates various types of technologies, including cloud\ncomputing, fog computing, and the IoT paradigm. In such environments, it is\nessential to organize and manage efficiently the broad and complex set of\nheterogeneous resources. For this reason, resources classification and\ncategorization becomes a vital issue in the control system. In this paper we\nmake an exhaustive literature survey about the various computing systems and\narchitectures which defines any type of ontology in the context of smart\nenvironments, considering both, authors that explicitly propose resources\ncategorization and authors that implicitly propose some resources\nclassification as part of their system architecture. As part of this research\nsurvey, we have built a table that summarizes all research works considered,\nand which provides a compact and graphical snapshot of the current\nclassification trends. The goal and primary motivation of this literature\nsurvey has been to understand the current state of the art and identify the\ngaps between the different computing paradigms involved in smart environment\nscenarios. As a result, we have found that it is essential to consider together\nseveral computing paradigms and technologies, and that there is not, yet, any\nresearch work that integrates a merged resources classification, taxonomy or\nontology required in such heterogeneous scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 21:07:54 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Sengupta", "Souvik", ""], ["Garcia", "Jordi", ""], ["Masip-Bruin", "Xavi", ""]]}, {"id": "1803.00355", "submitter": "Georgios Chasparis", "authors": "Georgios C. Chasparis, Vladimir Janjic, Michael Rossbory", "title": "Learning-based Dynamic Pinning of Parallelized Applications in Many-Core\n  Systems", "comments": "arXiv admin note: text overlap with arXiv:1606.08156", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for adaptive, secure and responsive scheduling in a\ngreat range of computing applications, including human-centered and\ntime-critical applications, this paper proposes a scheduling framework that\nseamlessly adds resource-awareness to any parallel application. In particular,\nwe introduce a learning-based framework for dynamic placement of parallel\nthreads to Non-Uniform Memory Access (NUMA) architectures. Decisions are taken\nindependently by each thread in a decentralized fashion that significantly\nreduces computational complexity. The advantage of the proposed learning scheme\nis the ability to easily incorporate any multi-objective criterion and easily\nadapt to performance variations during runtime. Under the multi-objective\ncriterion of maximizing total completed instructions per second (i.e., both\ncomputational and memory-access instructions), we provide analytical guarantees\nwith respect to the expected performance of the parallel application. We also\ncompare the performance of the proposed scheme with the Linux operating system\nscheduler in an extensive set of applications, including both computationally\nand memory intensive ones. We have observed that performance improvement could\nbe significant especially under limited availability of resources and under\nirregular memory-access patterns.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 13:31:18 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 09:06:16 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Chasparis", "Georgios C.", ""], ["Janjic", "Vladimir", ""], ["Rossbory", "Michael", ""]]}, {"id": "1803.00356", "submitter": "Cristina Monni", "authors": "Leonardo Mariani, Cristina Monni, Mauro Pezz\\'e, Oliviero Riganelli\n  and Rui Xin", "title": "Localizing Faults in Cloud Systems", "comments": "12 pages, 8 figures, paper accepted at ICST 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By leveraging large clusters of commodity hardware, the Cloud offers great\nopportunities to optimize the operative costs of software systems, but impacts\nsignificantly on the reliability of software applications. The lack of control\nof applications over Cloud execution environments largely limits the\napplicability of state-of-the-art approaches that address reliability issues by\nrelying on heavyweight training with injected faults. In this paper, we propose\n\\emph(LOUD}, a lightweight fault localization approach that relies on positive\ntraining only, and can thus operate within the constraints of Cloud systems.\n\\emph{LOUD} relies on machine learning and graph theory. It trains machine\nlearning models with correct executions only, and compensates the inaccuracy\nthat derives from training with positive samples, by elaborating the outcome of\nmachine learning techniques with graph theory algorithms. The experimental\nresults reported in this paper confirm that \\emph{LOUD} can localize faults\nwith high precision, by relying only on a lightweight positive training.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 13:40:03 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Mariani", "Leonardo", ""], ["Monni", "Cristina", ""], ["Pezz\u00e9", "Mauro", ""], ["Riganelli", "Oliviero", ""], ["Xin", "Rui", ""]]}, {"id": "1803.00368", "submitter": "Yuan Wang", "authors": "Yuan Wang, Wee Peng Tay, Wuhua Hu", "title": "An Event-based Diffusion LMS Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a wireless sensor network consists of cooperative nodes, each of\nthem keep adapting to streaming data to perform a least-mean-squares\nestimation, and also maintain information exchange among neighboring nodes in\norder to improve performance. For the sake of reducing communication overhead,\nprolonging batter life while preserving the benefits of diffusion cooperation,\nwe propose an energy-efficient diffusion strategy that adopts an event-based\ncommunication mechanism, which allow nodes to cooperate with neighbors only\nwhen necessary. We also study the performance of the proposed algorithm, and\nshow that its network mean error and MSD are bounded in steady state. Numerical\nresults demonstrate that the proposed method can effectively reduce the network\nenergy consumption without sacrificing steady-state network MSD performance\nsignificantly.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 09:51:58 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 11:30:36 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2018 07:37:51 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Wang", "Yuan", ""], ["Tay", "Wee Peng", ""], ["Hu", "Wuhua", ""]]}, {"id": "1803.00422", "submitter": "Daniela Z\\\"oller", "authors": "Daniela Z\\\"oller, Stefan Lenz and Harald Binder", "title": "Distributed regression modeling for selecting markers under data\n  protection constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data protection constraints frequently require a distributed analysis of\ndata, i.e., individual-level data remains at many different sites, but analysis\nnevertheless has to be performed jointly. The corresponding aggregated data is\noften exchanged manually, requiring explicit permission before transfer, i.e.,\nthe number of data calls and the amount of data should be limited. Thus, only\nsimple aggregated summary statistics are typically transferred with just a\nsingle call. This does not allow for more complex tasks such as variable\nselection. As an alternative, we propose a multivariable regression approach\nfor identifying important markers by automatic variable selection based on\naggregated data from different locations in iterative calls. To minimize the\namount of transferred data and the number of calls, we also provide a heuristic\nvariant of the approach. When performing a global data standardization, the\nproposed methods yields the same results as when pooling individual-level data.\nIn a simulation study, the information loss introduced by a local\nstandardization is seen to be minimal. In a typical scenario, the heuristic\ndecreases the number of data calls from more than 10 to 3, rendering manual\ndata releases feasible. To make our approach widely available for application,\nwe provide an implementation on top of the DataSHIELD framework.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 15:04:06 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 14:09:29 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Z\u00f6ller", "Daniela", ""], ["Lenz", "Stefan", ""], ["Binder", "Harald", ""]]}, {"id": "1803.00737", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat, E. A. Bashkov, V. Babkov, C. Titarenko", "title": "Fusion of multispectral satellite imagery using a cluster of graphics\n  processing unit", "comments": "7 pages, 5 Figures, 3 tables", "journal-ref": "International Geoinformatics Research and Development Journal,\n  2013", "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a parallel implementation of existing image fusion methods\non a graphical cluster. Parallel implementations of methods based on discrete\nwavelet transformation (Haars and Daubechies discrete wavelet transform) are\ndeveloped. Experiments were performed on a cluster using GPU and CPU and\nperformance gains were estimated for the use of the developed parallel\nimplementations to process satellite images from satellite Landsat 7. The\nimplementation on a graphic cluster provides performance improvement from 2 to\n18 times. The quality of the considered methods was evaluated by ERGAS and QNR\nmetrics. The results show performance gains and retaining of quality with the\ncluster of GPU compared to the results obtained by the authors and other\nresearchers for a CPU and single GPU.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 07:09:20 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Bashkov", "E. A.", ""], ["Babkov", "V.", ""], ["Titarenko", "C.", ""]]}, {"id": "1803.00989", "submitter": "Lilia Rodrigues Sampaio", "authors": "Lilia Sampaio, F\\'abio Silva, Amanda Souza, Andrey Brito, Pascal\n  Felber", "title": "Secure and Privacy-Aware Data Dissemination for Cloud-Based Applications", "comments": null, "journal-ref": null, "doi": "10.1145/3147213.3147230", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a data dissemination platform that supports data\nsecurity and different privacy levels even when the platform and the data are\nhosted by untrusted infrastructures. The proposed system aims at enabling an\napplication ecosystem that uses off-the-shelf trusted platforms (in this case,\nIntel SGX), so that users may allow or disallow third parties to access the\nlive data stream with a specific sensitivity-level. Moreover, this approach\ndoes not require users to manage the encryption keys directly. Our experiments\nshow that such an approach is indeed practical for medium scale systems, where\nparticipants disseminate small volumes of data at a time, such as in smart\ngrids and IoT environments.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 18:40:13 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Sampaio", "Lilia", ""], ["Silva", "F\u00e1bio", ""], ["Souza", "Amanda", ""], ["Brito", "Andrey", ""], ["Felber", "Pascal", ""]]}, {"id": "1803.01015", "submitter": "Giuseppe Di Molfetta Prof.", "authors": "Pablo Arrighi, Giuseppe Di Molfetta, Iv\\'an M\\'arquez-Mart\\'in,\n  Armando P\\'erez", "title": "The Dirac equation as a quantum walk over the honeycomb and triangular\n  lattices", "comments": "2 figures. Any comments is very welcome! Accepted in PRA", "journal-ref": "Phys. Rev. A 97, 062111 (2018)", "doi": "10.1103/PhysRevA.97.062111", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discrete-time Quantum Walk (QW) is essentially an operator driving the\nevolution of a single particle on the lattice, through local unitaries. Some\nQWs admit a continuum limit, leading to well-known physics partial differential\nequations, such as the Dirac equation. We show that these simulation results\nneed not rely on the grid: the Dirac equation in $(2+1)$--dimensions can also\nbe simulated, through local unitaries, on the honeycomb or the triangular\nlattice. The former is of interest in the study of graphene-like materials. The\nlatter, we argue, opens the door for a generalization of the Dirac equation to\narbitrary discrete surfaces.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 19:19:57 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 21:24:48 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Arrighi", "Pablo", ""], ["Di Molfetta", "Giuseppe", ""], ["M\u00e1rquez-Mart\u00edn", "Iv\u00e1n", ""], ["P\u00e9rez", "Armando", ""]]}, {"id": "1803.01016", "submitter": "Zhiyuan Xu", "authors": "Teng Li, Zhiyuan Xu, Jian Tang, Yanzhi Wang", "title": "Model-Free Control for Distributed Stream Data Processing using Deep\n  Reinforcement Learning", "comments": "14 pages, this paper has been accepted by VLDB 2018", "journal-ref": null, "doi": "10.14778/3184470.3184474", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on general-purpose Distributed Stream Data Processing\nSystems (DSDPSs), which deal with processing of unbounded streams of continuous\ndata at scale distributedly in real or near-real time. A fundamental problem in\na DSDPS is the scheduling problem with the objective of minimizing average\nend-to-end tuple processing time. A widely-used solution is to distribute\nworkload evenly over machines in the cluster in a round-robin manner, which is\nobviously not efficient due to lack of consideration for communication delay.\nModel-based approaches do not work well either due to the high complexity of\nthe system environment. We aim to develop a novel model-free approach that can\nlearn to well control a DSDPS from its experience rather than accurate and\nmathematically solvable system models, just as a human learns a skill (such as\ncooking, driving, swimming, etc). Specifically, we, for the first time, propose\nto leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free\ncontrol in DSDPSs; and present design, implementation and evaluation of a novel\nand highly effective DRL-based control framework, which minimizes average\nend-to-end tuple processing time by jointly learning the system environment via\ncollecting very limited runtime statistics data and making decisions under the\nguidance of powerful Deep Neural Networks. To validate and evaluate the\nproposed framework, we implemented it based on a widely-used DSDPS, Apache\nStorm, and tested it with three representative applications. Extensive\nexperimental results show 1) Compared to Storm's default scheduler and the\nstate-of-the-art model-based method, the proposed framework reduces average\ntuple processing by 33.5% and 14.0% respectively on average. 2) The proposed\nframework can quickly reach a good scheduling solution during online learning,\nwhich justifies its practicability for online control in DSDPSs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 19:30:55 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Li", "Teng", ""], ["Xu", "Zhiyuan", ""], ["Tang", "Jian", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1803.01098", "submitter": "Marwen Zorgui", "authors": "Marwen Zorgui, Robert Mateescu, Filip Blagojevic, Cyril Guyot, and\n  Zhiying Wang", "title": "Storage-Efficient Shared Memory Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of storage-efficient algorithms for emulating atomic\nshared memory over an asynchronous, distributed message-passing system. Our\nfirst algorithm is an atomic single-writer multi-reader algorithm based on a\nnovel erasure-coding technique, termed \\emph{multi-version code}. Next, we\npropose an extension of our single-writer algorithm to a multi-writer\nmulti-reader environment. Our second algorithm combines replication and\nmulti-version code, and is suitable in situations where we expect a large\nnumber of concurrent writes. Moreover, when the number of concurrent writes is\nbounded, we propose a simplified variant of the second algorithm that has a\nsimple structure similar to the single-writer algorithm.\n  Let $N$ be the number of servers, and the shared memory variable be of size 1\nunit. Our algorithms have the following properties:\n  (i) The write operation terminates if the number of server failures is\nbounded by a parameter $f$. The algorithms also guarantee the termination of\nthe read as long as the number of writes concurrent with the read is smaller\nthan a design parameter $\\nu$, and the number of server failures is bounded by\n$f$.\n  (ii) The overall storage size for the first algorithm, and the steady-state\nstorage size for the second algorithm, are all $N/\\lceil \\frac{N-2f}{\\nu}\n\\rceil$ units. Moreover, our simplified variant of the second algorithm\nachieves the worst-case storage cost of $N/\\lceil \\frac{N-2f}{\\nu} \\rceil$,\nasymptotically matching a lower bound by Cadambe et al. for $N \\gg f, \\nu \\le\nf+1$.\n  (iii) The write and read operations only consist of a small number (2 to 3)\nof communication rounds.\n  (iv) For all algorithms, the server maintains a simple data structure. A\nserver only needs to store the information associated with the latest value it\nobserves, similar to replication-based algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 02:54:27 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 04:26:54 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Zorgui", "Marwen", ""], ["Mateescu", "Robert", ""], ["Blagojevic", "Filip", ""], ["Guyot", "Cyril", ""], ["Wang", "Zhiying", ""]]}, {"id": "1803.01237", "submitter": "Vipul Harsh", "authors": "Vipul Harsh, Laxmikant Kale, Edgar Solomonik", "title": "Histogram Sort with Sampling", "comments": "12 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To minimize data movement, state-of-the-art parallel sorting algorithms use\ntechniques based on sampling and histogramming to partition keys prior to\nredistribution. Sampling enables partitioning to be done using a representative\nsubset of the keys, while histogramming enables evaluation and iterative\nimprovement of a given partition. We introduce Histogram sort with sampling\n(HSS), which combines sampling and iterative histogramming to find high quality\npartitions with minimal data movement and high practical performance. Compared\nto the best known (recently introduced) algorithm for finding these partitions,\nour algorithm requires a factor of {\\Theta}(log(p)/ log log(p)) less\ncommunication, and substantially less when compared to standard variants of\nSample sort and Histogram sort. We provide a distributed memory implementation\nof the proposed algorithm, compare its performance to two existing\nimplementations, and provide a brief application study showing benefit of the\nnew algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 20:51:38 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 03:47:49 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Harsh", "Vipul", ""], ["Kale", "Laxmikant", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1803.01256", "submitter": "Yuan Lu", "authors": "Yuan Lu, Qiang Tang, Guiling Wang", "title": "ZebraLancer: Decentralized Crowdsourcing of Human Knowledge atop Open\n  Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and implement the first private and anonymous decentralized\ncrowdsourcing system ZebraLancer, and overcome two fundamental challenges of\ndecentralizing crowdsourcing, i.e., data leakage and identity breach.\n  First, our outsource-then-prove methodology resolves the tension between the\nblockchain transparency and the data confidentiality to guarantee the basic\nutilities/fairness requirements of data crowdsourcing, thus ensuring: (i) a\nrequester will not pay more than what data deserve, according to a policy\nannounced when her task is published via the blockchain; (ii) each worker\nindeed gets a payment based on the policy, if he submits data to the\nblockchain; (iii) the above properties are realized not only without a central\narbiter, but also without leaking the data to the open blockchain. Second, the\ntransparency of blockchain allows one to infer private information about\nworkers and requesters through their participation history. Simply enabling\nanonymity is seemingly attempting but will allow malicious workers to submit\nmultiple times to reap rewards. ZebraLancer also overcomes this problem by\nallowing anonymous requests/submissions without sacrificing accountability. The\nidea behind is a subtle linkability: if a worker submits twice to a task,\nanyone can link the submissions, or else he stays anonymous and unlinkable\nacross tasks. To realize this delicate linkability, we put forward a novel\ncryptographic concept, i.e., the common-prefix-linkable anonymous\nauthentication. We remark the new anonymous authentication scheme might be of\nindependent interest. Finally, we implement our protocol for a common image\nannotation task and deploy it in a test net of Ethereum. The experiment results\nshow the applicability of our protocol atop the existing real-world blockchain.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 22:42:14 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 20:23:27 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 05:17:25 GMT"}, {"version": "v4", "created": "Sun, 17 Feb 2019 23:55:44 GMT"}, {"version": "v5", "created": "Thu, 7 May 2020 17:53:10 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Lu", "Yuan", ""], ["Tang", "Qiang", ""], ["Wang", "Guiling", ""]]}, {"id": "1803.01281", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Siddharth Samsi, William Arcand, David Bestor, Bill\n  Bergeron, Tim Davis, Vijay Gadepally, Michael Houle, Matthew Hubbell, Hayden\n  Jananthan, Michael Jones, Anna Klein, Peter Michaleas, Roger Pearce, Lauren\n  Milechin, Julie Mullen, Andrew Prout, Antonio Rosa, Geoff Sanders, Charles\n  Yee, Albert Reuther", "title": "Design, Generation, and Validation of Extreme Scale Power-Law Graphs", "comments": "8 pages, 6 figures, IEEE IPDPS 2018 Graph Algorithm Building Blocks\n  (GABB) workshop", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00055", "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.PF math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive power-law graphs drive many fields: metagenomics, brain mapping,\nInternet-of-things, cybersecurity, and sparse machine learning. The development\nof novel algorithms and systems to process these data requires the design,\ngeneration, and validation of enormous graphs with exactly known properties.\nSuch graphs accelerate the proper testing of new algorithms and systems and are\na prerequisite for success on real applications. Many random graph generators\ncurrently exist that require realizing a graph in order to know its exact\nproperties: number of vertices, number of edges, degree distribution, and\nnumber of triangles. Designing graphs using these random graph generators is a\ntime-consuming trial-and-error process. This paper presents a novel approach\nthat uses Kronecker products to allow the exact computation of graph properties\nprior to graph generation. In addition, when a real graph is desired, it can be\ngenerated quickly in memory on a parallel computer with no-interprocessor\ncommunication. To test this approach, graphs with $10^{12}$ edges are generated\non a 40,000+ core supercomputer in 1 second and exactly agree with those\npredicted by the theory. In addition, to demonstrate the extensibility of this\napproach, decetta-scale graphs with up to $10^{30}$ edges are simulated in a\nfew minutes on a laptop.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 01:47:49 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kepner", "Jeremy", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Davis", "Tim", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jananthan", "Hayden", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Pearce", "Roger", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Sanders", "Geoff", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1803.01296", "submitter": "Chin-Jung Hsu", "authors": "Chin-Jung Hsu, Vivek Nair, Tim Menzies, Vincent W. Freeh", "title": "Scout: An Experienced Guide to Find the Best Cloud Configuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the right cloud configuration for workloads is an essential step to\nensure good performance and contain running costs. A poor choice of cloud\nconfiguration decreases application performance and increases running cost\nsignificantly. While Bayesian Optimization is effective and applicable to any\nworkloads, it is fragile because performance and workload are hard to model (to\npredict).\n  In this paper, we propose a novel method, SCOUT. The central insight of SCOUT\nis that using prior measurements, even those for different workloads, improves\nsearch performance and reduces search cost. At its core, SCOUT extracts search\nhints (inference of resource requirements) from low-level performance metrics.\nSuch hints enable SCOUT to navigate through the search space more\nefficiently---only spotlight region will be searched.\n  We evaluate SCOUT with 107 workloads on Apache Hadoop and Spark. The\nexperimental results demonstrate that our approach finds better cloud\nconfigurations with a lower search cost than state of the art methods.\n  Based on this work, we conclude that (i) low-level performance information is\nnecessary for finding the right cloud configuration in an effective, efficient\nand reliable way, and (ii) a search method can be guided by historical data,\nthereby reducing cost and improving performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 04:14:17 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Hsu", "Chin-Jung", ""], ["Nair", "Vivek", ""], ["Menzies", "Tim", ""], ["Freeh", "Vincent W.", ""]]}, {"id": "1803.01358", "submitter": "Katina Kralevska", "authors": "Katina Kralevska", "title": "Applied Erasure Coding in Networks and Distributed Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of digital data is rapidly growing. There is an increasing use of\na wide range of computer systems, from mobile devices to large-scale data\ncenters, and important for reliable operation of all computer systems is\nmitigating the occurrence and the impact of errors in digital data. The demand\nfor new ultra-fast and highly reliable coding techniques for data at rest and\nfor data in transit is a major research challenge. Reliability is one of the\nmost important design requirements. The simplest way of providing a degree of\nreliability is by using data replication techniques. However, replication is\nhighly inefficient in terms of capacity utilization. Erasure coding has\ntherefore become a viable alternative to replication since it provides the same\nlevel of reliability as replication with significantly less storage overhead.\nThe present thesis investigates efficient constructions of erasure codes for\ndifferent applications. Methods from both coding and information theory have\nbeen applied to network coding, Optical Packet Switching (OPS) networks and\ndistributed storage systems. The following four issues are addressed: -\nConstruction of binary and non-binary erasure codes; - Reduction of the header\noverhead due to the encoding coefficients in network coding; - Construction and\nimplementation of new erasure codes for large-scale distributed storage systems\nthat provide savings in the storage and network resources compared to\nstate-of-the-art codes; and - Provision of a unified view on Quality of Service\n(QoS) in OPS networks when erasure codes are used, with the focus on Packet\nLoss Rate (PLR), survivability and secrecy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 14:16:00 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Kralevska", "Katina", ""]]}, {"id": "1803.01491", "submitter": "Li Chen", "authors": "Li Chen, Ge Chen, Justinas Lingys, Kai Chen", "title": "Programmable Switch as a Parallel Computing Device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern switches have packet processing capacity of up to multi-tera bits per\nsecond, and they are also becoming more and more programmable. We seek to\nunderstand whether the programmability can translate packet processing capacity\nto computational power for parallel computing applications. In this paper, we\nfirst develop a simple mathematical model to understand the costs and overheads\nof data plane computation. Then we validate the the performance benefits of\noffloading computation to network. Using experiments on real data center\nnetwork, we finnd that offloading computation to the data plane results in up\nto 20x speed-up for a simple Map-Reduce application. Motivated by this, we\npropose a parallel programming framework, p4mr, to help users efficiently\nprogram multiple switches. We successfully build and test a prototype of p4mr\non a simulated testbed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 04:06:53 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Chen", "Li", ""], ["Chen", "Ge", ""], ["Lingys", "Justinas", ""], ["Chen", "Kai", ""]]}, {"id": "1803.01498", "submitter": "Dong Yin", "authors": "Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett", "title": "Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale distributed learning, security issues have become increasingly\nimportant. Particularly in a decentralized environment, some computing units\nmay behave abnormally, or even exhibit Byzantine failures -- arbitrary and\npotentially adversarial behavior. In this paper, we develop distributed\nlearning algorithms that are provably robust against such failures, with a\nfocus on achieving optimal statistical performance. A main result of this work\nis a sharp analysis of two robust distributed gradient descent algorithms based\non median and trimmed mean operations, respectively. We prove statistical error\nrates for three kinds of population loss functions: strongly convex,\nnon-strongly convex, and smooth non-convex. In particular, these algorithms are\nshown to achieve order-optimal statistical error rates for strongly convex\nlosses. To achieve better communication efficiency, we further propose a\nmedian-based distributed algorithm that is provably robust, and uses only one\ncommunication round. For strongly convex quadratic loss, we show that this\nalgorithm achieves the same optimal error rate as the robust distributed\ngradient descent algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 05:04:17 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 06:34:39 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Yin", "Dong", ""], ["Chen", "Yudong", ""], ["Ramchandran", "Kannan", ""], ["Bartlett", "Peter", ""]]}, {"id": "1803.02123", "submitter": "Per Skarin", "authors": "Per Skarin, William T\\\"arneberg, Karl-Erik {\\AA}rzen, Maria Kihl", "title": "Towards Mission-Critical Control at the Edge and Over 5G", "comments": "June 18th: Upload the final version as submitted to IEEE Services\n  [EDGE] 2018 on May 16th (updated abstract and some wording, results\n  unchanged)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of industrial IoT and cloud computing, and the advent of\n5G and edge clouds, there are ambitious expectations on elasticity, economies\nof scale, and fast time to market for demanding use cases in the next\ngeneration of ICT networks. Responsiveness and reliability of wireless\ncommunication links and services in the cloud are set to improve significantly\nas the concept of edge clouds is becoming more prevalent. To enable industrial\nuptake we must provide cloud capacity in the networks but also a sufficient\nlevel of simplicity and self-sustainability in the software platforms. In this\npaper, we present a research test-bed built to study mission-critical control\nover the distributed edge cloud. We evaluate system properties using a\nconventional control application in the form of a Model Predictive Controller.\nOur cloud platform provides the means to continuously operate our\nmission-critical application while seamlessly relocating computations across\ngeographically dispersed compute nodes. Through our use of 5G wireless radio,\nwe allow for mobility and reliably provide compute resources with low latency,\nat the edge. The primary contribution of this paper is a state-of-the art,\nfully operational test-bed showing the potential for merged IoT, 5G, and cloud.\nWe also provide an evaluation of the system while operating a mission-critical\napplication and provide an outlook on a novel research direction.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 11:31:59 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 08:25:56 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Skarin", "Per", ""], ["T\u00e4rneberg", "William", ""], ["\u00c5rzen", "Karl-Erik", ""], ["Kihl", "Maria", ""]]}, {"id": "1803.02216", "submitter": "Dominik Pajak", "authors": "Seth Gilbert, Nancy Lynch, Calvin Newport, Dominik Pajak", "title": "On Simple Back-Off in Unreliable Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study local and global broadcast in the dual graph model,\nwhich describes communication in a radio network with both reliable and\nunreliable links. Existing work proved that efficient solutions to these\nproblems are impossible in the dual graph model under standard assumptions. In\nreal networks, however, simple back-off strategies tend to perform well for\nsolving these basic communication tasks. We address this apparent paradox by\nintroducing a new set of constraints to the dual graph model that better\ngeneralize the slow/fast fading behavior common in real networks. We prove that\nin the context of these new constraints, simple back-off strategies now provide\nefficient solutions to local and global broadcast in the dual graph model. We\nalso precisely characterize how this efficiency degrades as the new constraints\nare reduced down to non-existent, and prove new lower bounds that establish\nthis degradation as near optimal for a large class of natural algorithms. We\nconclude with a preliminary investigation of the performance of these\nstrategies when we include additional generality to the model. These results\nprovide theoretical foundations for the practical observation that simple\nback-off algorithms tend to work well even amid the complicated link dynamics\nof real radio networks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 14:27:45 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 12:37:15 GMT"}, {"version": "v3", "created": "Sun, 16 Dec 2018 03:14:31 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Gilbert", "Seth", ""], ["Lynch", "Nancy", ""], ["Newport", "Calvin", ""], ["Pajak", "Dominik", ""]]}, {"id": "1803.02500", "submitter": "Yogesh Simmhan", "authors": "Yogesh Simmhan, Pushkara Ravindra, Shilpa Chaturvedi, Malati Hegde,\n  Rashmi Ballamajalu", "title": "Towards a Data-driven IoT Software Architecture for Smart City Utilities", "comments": "Pre-print of article to appear in Software: Practice and Experience,\n  Wiley, 2018", "journal-ref": "Software: Practice and Experience, Volume 48, Issue 7, July 2018,\n  Pages 1390-1416", "doi": "10.1002/spe.2580", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is emerging as the next big wave of digital\npresence for billions of devices on the Internet. Smart Cities are practical\nmanifestation of IoT, with the goal of efficient, reliable and safe delivery of\ncity utilities like water, power and transport to residents, through their\nintelligent management. A data-driven IoT Software Platform is essential for\nrealizing manageable and sustainable Smart Utilities, and for novel\napplications to be developed upon them. Here, we propose such a\nservice-oriented software architecture to address two key operational\nactivities in a Smart Utility -- the IoT fabric for resource management, and\nthe data and application platform for decision making. Our design uses open web\nstandards and evolving network protocols, Cloud and edge resources, and\nstreaming Big Data platforms. We motivate our design requirements using the\nsmart water management domain; some of these requirements are unique to\ndeveloping nations. We also validate the architecture within a campus-scale IoT\ntestbed at the Indian Institute of Science (IISc), Bangalore, and present our\nexperiences. Our architecture is scalable to a township or city, while also\ngeneralizable to other Smart Utility domains. Our experiences serves as a\ntemplate for other similar efforts, particularly in emerging markets, and\nhighlights the gaps and opportunities for a data-driven IoT Software\narchitecture for smart cities.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 02:03:46 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Simmhan", "Yogesh", ""], ["Ravindra", "Pushkara", ""], ["Chaturvedi", "Shilpa", ""], ["Hegde", "Malati", ""], ["Ballamajalu", "Rashmi", ""]]}, {"id": "1803.02540", "submitter": "Adam Sealfon", "authors": "Shafi Goldwasser, Rafail Ostrovsky, Alessandra Scafuro and Adam\n  Sealfon", "title": "Population stability: regulating size in the presence of an adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new coordination problem in distributed computing that we call\nthe population stability problem. A system of agents each with limited memory\nand communication, as well as the ability to replicate and self-destruct, is\nsubjected to attacks by a worst-case adversary that can at a bounded rate (1)\ndelete agents chosen arbitrarily and (2) insert additional agents with\narbitrary initial state into the system. The goal is perpetually to maintain a\npopulation whose size is within a constant factor of the target size $N$. The\nproblem is inspired by the ability of complex biological systems composed of a\nmultitude of memory-limited individual cells to maintain a stable population\nsize in an adverse environment. Such biological mechanisms allow organisms to\nheal after trauma or to recover from excessive cell proliferation caused by\ninflammation, disease, or normal development.\n  We present a population stability protocol in a communication model that is a\nsynchronous variant of the population model of Angluin et al. In each round,\npairs of agents selected at random meet and exchange messages, where at least a\nconstant fraction of agents is matched in each round. Our protocol uses\nthree-bit messages and $\\omega(\\log^2 N)$ states per agent. We emphasize that\nour protocol can handle an adversary that can both insert and delete agents, a\nsetting in which existing approximate counting techniques do not seem to apply.\nThe protocol relies on a novel coloring strategy in which the population size\nis encoded in the variance of the distribution of colors. Individual agents can\nlocally obtain a weak estimate of the population size by sampling from the\ndistribution, and make individual decisions that robustly maintain a stable\nglobal population size.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 06:48:35 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Goldwasser", "Shafi", ""], ["Ostrovsky", "Rafail", ""], ["Scafuro", "Alessandra", ""], ["Sealfon", "Adam", ""]]}, {"id": "1803.02657", "submitter": "Subho Sankar Banerjee", "authors": "Subho S. Banerjee, Mohamed El-Hadedy, Jong Bin Lim, Zbigniew T.\n  Kalbarczyk, Deming Chen, Steve Lumetta, Ravishankar K. Iyer", "title": "ASAP: Accelerated Short-Read Alignment on Programmable Hardware", "comments": null, "journal-ref": null, "doi": "10.1109/TC.2018.2875733", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of high-throughput sequencing machines ensures rapid\ngeneration of up to billions of short nucleotide fragments in a short period of\ntime. This massive amount of sequence data can quickly overwhelm today's\nstorage and compute infrastructure. This paper explores the use of hardware\nacceleration to significantly improve the runtime of short-read alignment, a\ncrucial step in preprocessing sequenced genomes. We focus on the Levenshtein\ndistance (edit-distance) computation kernel and propose the ASAP accelerator,\nwhich utilizes the intrinsic delay of circuits for edit-distance computation\nelements as a proxy for computation. Our design is implemented on an Xilinx\nVirtex 7 FPGA in an IBM POWER8 system that uses the CAPI interface for cache\ncoherence across the CPU and FPGA. Our design is $200\\times$ faster than the\nequivalent C implementation of the kernel running on the host processor and\n$2.2\\times$ faster for an end-to-end alignment tool for 120-150 base-pair\nshort-read sequences. Further the design represents a $3760\\times$ improvement\nover the CPU in performance/Watt terms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 05:54:28 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 17:07:30 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Banerjee", "Subho S.", ""], ["El-Hadedy", "Mohamed", ""], ["Lim", "Jong Bin", ""], ["Kalbarczyk", "Zbigniew T.", ""], ["Chen", "Deming", ""], ["Lumetta", "Steve", ""], ["Iyer", "Ravishankar K.", ""]]}, {"id": "1803.02720", "submitter": "Darya Melnyk", "authors": "Darya Melnyk, Yuyi Wang and Roger Wattenhofer", "title": "Byzantine Preferential Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Byzantine agreement problem, n nodes with possibly different input\nvalues aim to reach agreement on a common value in the presence of t < n/3\nByzantine nodes which represent arbitrary failures in the system. This paper\nintroduces a generalization of Byzantine agreement, where the input values of\nthe nodes are preference rankings of three or more candidates. We show that\nconsensus on preferences, which is an important question in social choice\ntheory, complements already known results from Byzantine agreement. In addition\npreferential voting raises new questions about how to approximate consensus\nvectors. We propose a deterministic algorithm to solve Byzantine agreement on\nrankings under a generalized validity condition, which we call Pareto-Validity.\nThese results are then extended by considering a special voting rule which\nchooses the Kemeny median as the consensus vector. For this rule, we derive a\nlower bound on the approximation ratio of the Kemeny median that can be\nguaranteed by any deterministic algorithm. We then provide an algorithm\nmatching this lower bound. To our knowledge, this is the first non-trivial\nmulti-dimensional approach which can tolerate a constant fraction of Byzantine\nnodes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 15:39:18 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Melnyk", "Darya", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1803.02750", "submitter": "Carlos Baquero", "authors": "Vitor Enes, Paulo S\\'ergio Almeida, Carlos Baquero, Jo\\~ao Leit\\~ao", "title": "Efficient Synchronization of State-based CRDTs", "comments": "To be published at the 35th IEEE International Conference on Data\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure high availability in large scale distributed systems, Conflict-free\nReplicated Data Types (CRDTs) relax consistency by allowing immediate query and\nupdate operations at the local replica, with no need for remote\nsynchronization. State-based CRDTs synchronize replicas by periodically sending\ntheir full state to other replicas, which can become extremely costly as the\nCRDT state grows. Delta-based CRDTs address this problem by producing small\nincremental states (deltas) to be used in synchronization instead of the full\nstate. However, current synchronisation algorithms for Delta-based CRDTs induce\nredundant wasteful delta propagation, performing worse than expected, and\nsurprisingly, no better than State-based. In this paper we: 1) identify two\nsources of inefficiency in current synchronization algorithms for delta-based\nCRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)\nexploit join decompositions to obtain optimal deltas and 4) improve the\nefficiency of synchronization algorithms; and finally, 5) evaluate the improved\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 16:28:03 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 16:00:46 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 12:37:17 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Enes", "Vitor", ""], ["Almeida", "Paulo S\u00e9rgio", ""], ["Baquero", "Carlos", ""], ["Leit\u00e3o", "Jo\u00e3o", ""]]}, {"id": "1803.02811", "submitter": "Adam Stooke", "authors": "Adam Stooke and Pieter Abbeel", "title": "Accelerated Methods for Deep Reinforcement Learning", "comments": "v2: -Added game performance statistics summary for algorithm scaling\n  across full Atari game set. -Added full set of learning curves (appendix).\n  -Fixed images to remove phantom borders. -Streamlined some discussion, moved\n  some details to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has achieved many recent successes, yet\nexperiment turn-around time remains a key bottleneck in research and in\npractice. We investigate how to optimize existing deep RL algorithms for modern\ncomputers, specifically for a combination of CPUs and GPUs. We confirm that\nboth policy gradient and Q-value learning algorithms can be adapted to learn\nusing many parallel simulator instances. We further find it possible to train\nusing batch sizes considerably larger than are standard, without negatively\naffecting sample complexity or final performance. We leverage these facts to\nbuild a unified framework for parallelization that dramatically hastens\nexperiments in both classes of algorithm. All neural network computations use\nGPUs, accelerating both data collection and training. Our results include using\nan entire DGX-1 to learn successful strategies in Atari games in mere minutes,\nusing both synchronous and asynchronous algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 18:39:12 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 20:48:11 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Stooke", "Adam", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1803.02933", "submitter": "Cesar A. Uribe", "authors": "C\\'esar A. Uribe and Darina Dvinskikh and Pavel Dvurechensky and\n  Alexander Gasnikov and Angelia Nedi\\'c", "title": "Distributed Computation of Wasserstein Barycenters over Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new \\cu{class-optimal} algorithm for the distributed computation\nof Wasserstein Barycenters over networks. Assuming that each node in a graph\nhas a probability distribution, we prove that every node can reach the\nbarycenter of all distributions held in the network by using local interactions\ncompliant with the topology of the graph. We provide an estimate for the\nminimum number of communication rounds required for the proposed method to\nachieve arbitrary relative precision both in the optimality of the solution and\nthe consensus among all agents for undirected fixed networks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 01:32:06 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 18:01:11 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Uribe", "C\u00e9sar A.", ""], ["Dvinskikh", "Darina", ""], ["Dvurechensky", "Pavel", ""], ["Gasnikov", "Alexander", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1803.03031", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley, Pierre Fraigniaud, Juho Hirvonen, Ami Paz and Mor\n  Perry", "title": "Redundancy in Distributed Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed proofs are mechanisms enabling the nodes of a network to\ncollectivity and efficiently check the correctness of Boolean predicates on the\nstructure of the network, or on data-structures distributed over the nodes\n(e.g., spanning trees or routing tables). We consider mechanisms consisting of\ntwo components: a \\emph{prover} assigning a \\emph{certificate} to each node,\nand a distributed algorithm called \\emph{verifier} that is in charge of\nverifying the distributed proof formed by the collection of all certificates.\n  In this paper, we show that many network predicates have distributed proofs\noffering a high level of redundancy, explicitly or implicitly. We use this\nremarkable property of distributed proofs for establishing perfect tradeoffs\nbetween the \\emph{size of the certificate} stored at every node, and the\n\\emph{number of rounds} of the verification protocol. If we allow every node to\ncommunicate to distance at most $t$, one might expect that the certificate\nsizes can be reduced by a multiplicative factor of at least~$t$. In trees,\ncycles and grids, we show that such tradeoffs can be established for \\emph{all}\nnetwork predicates, i.e., it is always possible to linearly decrease the\ncertificate size. In arbitrary graphs, we show that any part of the\ncertificates common to all nodes can be evenly redistributed among these nodes,\nachieving even a better tradeoff: this common part of the certificate can be\nreduced by the size of a smallest ball of radius $t$ in the network.\n  In addition to these general results, we establish several upper and lower\nbounds on the certificate sizes used for distributed proofs for spanning trees,\nminimum-weight spanning trees, diameter, additive and multiplicative spanners,\nand more, improving and generalizing previous results from the literature.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 10:34:34 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 10:26:45 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 10:41:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Feuilloley", "Laurent", ""], ["Fraigniaud", "Pierre", ""], ["Hirvonen", "Juho", ""], ["Paz", "Ami", ""], ["Perry", "Mor", ""]]}, {"id": "1803.03042", "submitter": "Jonas Lef\\`evre", "authors": "Armando Casta\\~neda (1), Jonas Lef\\`evre (2) and Amitabh Trehan (2)\n  ((1) Instituto de Matem\\'aticas, UNAM, Mexico,(2) Computer Science,\n  Loughborough University, UK)", "title": "Some Problems in Compact Message Passing", "comments": "22 pages, 5 figures, submitted to DISC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper seeks to address the question of designing distributed algorithms\nfor the setting of compact memory i.e. sublinear bits working memory for\narbitrary connected networks. The nodes in our networks may have much lower\ninternal memory as compared to the number of their possible neighbours implying\nthat a node may not be able to store all the IDs of its neighbours. These\nalgorithms are useful for large networks of small devices such as the Internet\nof Things, for wireless or ad-hoc networks, and, in general, as memory\nefficient algorithms. We introduce the Compact Message Passing(CMP) model;an\nextension of the standard message passing model considered at a finer\ngranularity where a node can interleave reads and writes with internal\ncomputations, using a port only once in a round. The interleaving is required\nfor meaningful computations due to the low memory requirement and is akin to a\ndistributed network with nodes executing streaming algorithms. Note that the\ninternal memory size upper bounds the message sizes and hence e.g. for\nlog-memory, the model is weaker than the Congest model; for such models our\nalgorithms will work directly too. We present early results in the CMP model\nfor nodes with log^2-memory. We introduce the concepts of local compact\nfunctions and compact protocols and give solutions for some classic distributed\nproblems (leader election, tree constructions and traversals). We build on\nthese to solve the open problem of compact preprocessing for the compact\nself-healing routing algorithm CompactFTZ posed in Compact Routing Messages in\nSelf-Healing Trees(TCS2017) by designing local compact functions for finding\nparticular subtrees of labeled binary trees. Hence, we introduce the first\nfully compact self-healing routing algorithm. We also give independent fully\ncompact versions of the Forgiving Tree[PODC08] and Thorup-Zwick's tree based\ncompact routing[SPAA01].\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 11:20:08 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 12:44:51 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Casta\u00f1eda", "Armando", ""], ["Lef\u00e8vre", "Jonas", ""], ["Trehan", "Amitabh", ""]]}, {"id": "1803.03094", "submitter": "Adnan Ashraf", "authors": "Adnan Ashraf, Benjamin Byholm, Ivan Porres", "title": "Distributed virtual machine consolidation: A systematic mapping study", "comments": "The manuscript has been accepted for publication in Computer Science\n  Review", "journal-ref": null, "doi": "10.1016/j.cosrev.2018.02.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Virtual Machine (VM) consolidation is an effective technique to\nimprove resource utilization and reduce energy footprint in cloud data centers.\nIt can be implemented in a centralized or a distributed fashion. Distributed VM\nconsolidation approaches are currently gaining popularity because they are\noften more scalable than their centralized counterparts and they avoid a single\npoint of failure.\n  Objective: To present a comprehensive, unbiased overview of the\nstate-of-the-art on distributed VM consolidation approaches.\n  Method: A Systematic Mapping Study (SMS) of the existing distributed VM\nconsolidation approaches.\n  Results: 19 papers on distributed VM consolidation categorized in a variety\nof ways. The results show that the existing distributed VM consolidation\napproaches use four types of algorithms, optimize a number of different\nobjectives, and are often evaluated with experiments involving simulations.\n  Conclusion: There is currently an increasing amount of interest on developing\nand evaluating novel distributed VM consolidation approaches. A number of\nresearch gaps exist where the focus of future research may be directed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 14:10:48 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Ashraf", "Adnan", ""], ["Byholm", "Benjamin", ""], ["Porres", "Ivan", ""]]}, {"id": "1803.03190", "submitter": "Jan Seeger", "authors": "Jan Seeger, Rohit A. Deshmukh, Vasil Sarafov, Arne Br\\\"oring", "title": "Dynamic IoT Choreographies", "comments": "Submitted to IEEE Pervasive Computing's special issue \"IoT\n  communications\". v2: Fixed some typos v3: Extended evaluation section,\n  improved diagrams, small text improvements v4: Accepted version", "journal-ref": "IEEE Pervasive Computing 18.1 (2019): 19-27", "doi": "10.1109/MPRV.2019.2907003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things is growing at a dramatic rate and extending into\nvarious application domains. We have designed, implemented and evaluated a\nresilient and decentralized system using IoT concepts. We applied it to\nmaintaining the functionality of building automation systems, so that new\ndevices can appear and vanish on-the-fly.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 16:32:00 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 08:27:33 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 09:38:59 GMT"}, {"version": "v4", "created": "Mon, 3 Jun 2019 14:36:30 GMT"}, {"version": "v5", "created": "Mon, 22 Jul 2019 08:09:44 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Seeger", "Jan", ""], ["Deshmukh", "Rohit A.", ""], ["Sarafov", "Vasil", ""], ["Br\u00f6ring", "Arne", ""]]}, {"id": "1803.03248", "submitter": "Yannic Maus", "authors": "Mohsen Ghaffari, Juho Hirvonen, Fabian Kuhn, Yannic Maus", "title": "Improved Distributed $\\Delta$-Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized distributed algorithm that computes a\n$\\Delta$-coloring in any non-complete graph with maximum degree $\\Delta \\geq 4$\nin $O(\\log \\Delta) + 2^{O(\\sqrt{\\log\\log n})}$ rounds, as well as a randomized\nalgorithm that computes a $\\Delta$-coloring in $O((\\log \\log n)^2)$ rounds when\n$\\Delta \\in [3, O(1)]$. Both these algorithms improve on an $O(\\log^3 n/\\log\n\\Delta)$-round algorithm of Panconesi and Srinivasan~[STOC'1993], which has\nremained the state of the art for the past 25 years. Moreover, the latter\nalgorithm gets (exponentially) closer to an $\\Omega(\\log\\log n)$ round lower\nbound of Brandt et al.~[STOC'16].\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 18:46:32 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 10:56:05 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Hirvonen", "Juho", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "1803.03288", "submitter": "Sayed Hadi Hashemi", "authors": "Sayed Hadi Hashemi, Sangeetha Abdu Jyothi, Roy H. Campbell", "title": "TicTac: Accelerating Distributed Deep Learning with Communication\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning systems rely on iterative distributed training\nto tackle the increasing complexity of models and input data. The iteration\ntime in these communication-heavy systems depends on the computation time,\ncommunication time and the extent of overlap of computation and communication.\n  In this work, we identify a shortcoming in systems with graph representation\nfor computation, such as TensorFlow and PyTorch, that result in high variance\nin iteration time --- random order of received parameters across workers. We\ndevelop a system, TicTac, to improve the iteration time by fixing this issue in\ndistributed deep learning with Parameter Servers while guaranteeing\nnear-optimal overlap of communication and computation. TicTac identifies and\nenforces an order of network transfers which improves the iteration time using\nprioritization. Our system is implemented over TensorFlow and requires no\nchanges to the model or developer inputs. TicTac improves the throughput by up\nto $37.7\\%$ in inference and $19.2\\%$ in training, while also reducing\nstraggler effect by up to $2.3\\times$. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 20:03:51 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 00:38:36 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Hashemi", "Sayed Hadi", ""], ["Jyothi", "Sangeetha Abdu", ""], ["Campbell", "Roy H.", ""]]}, {"id": "1803.03290", "submitter": "Chen Yuan", "authors": "Yiting Zhao, Chen Yuan, Guangyi Liu, Ilya Grinberg", "title": "Graph-based Preconditioning Conjugate Gradient Algorithm for N-1\n  Contingency Analysis", "comments": "5 pages, 8 figures, Proc. of 2018 IEEE Power and Energy Society\n  General Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contingency analysis (CA) plays a critical role to guarantee operation\nsecurity in the modern power systems. With the high penetration of renewable\nenergy, a real-time and comprehensive N-1 CA is needed as a power system\nanalysis tool to ensure system security. In this paper, a graph-based\npreconditioning conjugate gradient (GPCG) approach is proposed for the nodal\nparallel computing in N-1 CA. To pursue a higher performance in the practical\napplication, the coefficient matrix of the base case is used as the incomplete\nLU (ILU) preconditioner for each N-1 scenario. Additionally, the re-dispatch\nstrategy is employed to handle the islanding issues in CA. Finally, computation\nperformance of the proposed GPCG approach is tested on a real provincial system\nin China.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 20:11:28 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Zhao", "Yiting", ""], ["Yuan", "Chen", ""], ["Liu", "Guangyi", ""], ["Grinberg", "Ilya", ""]]}, {"id": "1803.03444", "submitter": "Dragi Kimovski", "authors": "Dragi Kimovski, Humaira Ijaz, Nishant Surabh, Radu Prodan", "title": "An Adaptive Nature-inspired Fog Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decade, Cloud computing has efficiently exploited the economy\nof scale by providing low cost computational and storage resources over the\nInternet, eventually leading to consolidation of computing resources into large\ndata centers. However, the nascent of the highly decentralized Internet of\nThings (IoT) technologies that cannot effectively utilize the centralized Cloud\ninfrastructures pushes computing towards resource dispersion. Fog computing\nextends the Cloud paradigm by enabling dispersion of the computational and\nstorage resources at the edge of the network in a close proximity to where the\ndata is generated. In its essence, Fog computing facilitates the operation of\nthe limited compute, storage and networking resources physically located close\nto the edge devices. However, the shared complexity of the Fog and the\ninfluence of the recent IoT trends moving towards deploying and interconnecting\nextremely large sets of pervasive devices and sensors, requires exploration of\nadaptive Fog architectural approaches capable of adapting and scaling in\nresponse to the unpredictable load patterns of the distributed IoT\napplications. In this paper we introduce a promising new nature-inspired Fog\narchitecture, named SmartFog, capable of providing low decision making latency\nand adaptive resource management. By utilizing novel algorithms and techniques\nfrom the fields of multi-criteria decision making, graph theory and machine\nlearning we model the Fog as a distributed intelligent processing system,\ntherefore emulating the function of the human brain.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 10:04:17 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Kimovski", "Dragi", ""], ["Ijaz", "Humaira", ""], ["Surabh", "Nishant", ""], ["Prodan", "Radu", ""]]}, {"id": "1803.03482", "submitter": "Marc Shapiro", "authors": "Marc Shapiro (Regal, DELYS), Annette Bieniusa, Peter Zeller, Gustavo\n  Petri (IRIF)", "title": "Ensuring referential integrity under causal consistency", "comments": null, "journal-ref": "PaPoC 2018 - 5th W. on Principles and Practice of Consistency for\n  Distributed Data, Apr 2016, Porto, Portugal", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referential integrity (RI) is an important correctness property of a shared,\ndistributed object storage system. It is sometimes thought that enforcing RI\nrequires a strong form of consistency. In this paper, we argue that causal\nconsistency suffices to maintain RI. We support this argument with pseudocode\nfor a reference CRDT data type that maintains RI under causal consistency.\nQuickCheck has not found any errors in the model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 12:08:46 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Shapiro", "Marc", "", "Regal, DELYS"], ["Bieniusa", "Annette", "", "IRIF"], ["Zeller", "Peter", "", "IRIF"], ["Petri", "Gustavo", "", "IRIF"]]}, {"id": "1803.03620", "submitter": "Lalith Suresh", "authors": "Lalith Suresh, Dahlia Malkhi, Parikshit Gopalan, Ivan Porto Carreiro,\n  Zeeshan Lokhandwala", "title": "Stable and Consistent Membership at Scale with Rapid", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and evaluation of Rapid, a distributed membership\nservice. At Rapid's core is a scheme for multi-process cut detection (CD) that\nrevolves around two key insights: (i) it suspects a failure of a process only\nafter alerts arrive from multiple sources, and (ii) when a group of processes\nexperience problems, it detects failures of the entire group, rather than\nconclude about each process individually. Implementing these insights\ntranslates into a simple membership algorithm with low communication overhead.\n  We present evidence that our strategy suffices to drive unanimous detection\nalmost-everywhere, even when complex network conditions arise, such as one-way\nreachability problems, firewall misconfigurations, and high packet loss.\nFurthermore, we present both empirical evidence and analyses that proves that\nthe almost-everywhere detection happens with high probability. To complete the\ndesign, Rapid contains a leaderless consensus protocol that converts\nmulti-process cut detections into a view-change decision. The resulting\nmembership service works both in fully decentralized as well as logically\ncentralized modes.\n  We present an evaluation of Rapid in moderately scalable cloud settings.\nRapid bootstraps 2000 node clusters 2-5.8x faster than prevailing tools such as\nMemberlist and ZooKeeper, remains stable in face of complex failure scenarios,\nand provides strong consistency guarantees. It is easy to integrate Rapid into\nexisting distributed applications, of which we demonstrate two.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 18:05:02 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Suresh", "Lalith", ""], ["Malkhi", "Dahlia", ""], ["Gopalan", "Parikshit", ""], ["Carreiro", "Ivan Porto", ""], ["Lokhandwala", "Zeeshan", ""]]}, {"id": "1803.03682", "submitter": "Katina Kralevska", "authors": "Danilo Gligoroski, Katina Kralevska, Rune E. Jensen and Per Simonsen", "title": "Network Traffic Driven Storage Repair", "comments": "arXiv admin note: text overlap with arXiv:1701.06664", "journal-ref": null, "doi": "10.1504/IJBDI.2019.100888", "report-no": "Published in International Journal of Big Data Intelligence, 2018", "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we constructed an explicit family of locally repairable and locally\nregenerating codes. Their existence was proven by Kamath et al. but no explicit\nconstruction was given. Our design is based on HashTag codes that can have\ndifferent sub-packetization levels. In this work we emphasize the importance of\nhaving two ways to repair a node: repair only with local parity nodes or repair\nwith both local and global parity nodes. We say that the repair strategy is\nnetwork traffic driven since it is in connection with the concrete system and\ncode parameters: the repair bandwidth of the code, the number of I/O\noperations, the access time for the contacted parts and the size of the stored\nfile. We show the benefits of having repair duality in one practical example\nimplemented in Hadoop. We also give algorithms for efficient repair of the\nglobal parity nodes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 20:16:05 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 15:04:24 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Gligoroski", "Danilo", ""], ["Kralevska", "Katina", ""], ["Jensen", "Rune E.", ""], ["Simonsen", "Per", ""]]}, {"id": "1803.03797", "submitter": "Pawan Kumar", "authors": "Sahithi Rampalli, Natasha Sehgal, Ishita Bindlish, Tanya Tyagi, and\n  Pawan Kumar", "title": "Efficient FPGA Implementation of Conjugate Gradient Methods for\n  Laplacian System using HLS", "comments": "10 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study FPGA based pipelined and superscalar design of two\nvariants of conjugate gradient methods for solving Laplacian equation on a\ndiscrete grid; the first version corresponds to the original conjugate gradient\nalgorithm, and the second version corresponds to a slightly modified version of\nthe same.\n  In conjugate gradient method to solve partial differential equations, matrix\nvector operations are required in each iteration; these operations can be\nimplemented as 5 point stencil operations on the grid without explicitely\nconstructing the matrix. We show that a pipelined and superscalar design using\nhigh level synthesis written in C language leads to a significant reduction in\nlatencies for both methods. When comparing these two, we show that the later\nhas roughly two times lower latency than the former given the same degree of\nsuperscalarity. These reductions in latencies for the newer variant of CG is\ndue to parallel implementations of stencil operation on subdomains of the grid,\nand dut to overlap of these stencil operations with dot product operations. In\na superscalar design, domain needs to be partitioned, and boundary data needs\nto be copied, which requires padding. In 1D partition, the padding latency\nincreases as the number of partitions increase. For a streaming data flow\nmodel, we propose a novel traversal of the grid for 2D domain decomposition\nthat leads to 2 times reduction in latency cost involved with padding compared\nto 1D partitions. Our implementation is roughly 10 times faster than software\nimplementation for linear system of dimension $10000 \\times 10000.$\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 12:05:35 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Rampalli", "Sahithi", ""], ["Sehgal", "Natasha", ""], ["Bindlish", "Ishita", ""], ["Tyagi", "Tanya", ""], ["Kumar", "Pawan", ""]]}, {"id": "1803.03807", "submitter": "Tomer Golomb", "authors": "Tomer Golomb, Yisroel Mirsky and Yuval Elovici", "title": "CIoTA: Collaborative IoT Anomaly Detection via Blockchain", "comments": "Appears in the workshop on Decentralized IoT Security and Standards\n  (DISS) of the Network and Distributed Systems Security Symposium (NDSS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their rapid growth and deployment, Internet of things (IoT) devices\nhave become a central aspect of our daily lives. However, they tend to have\nmany vulnerabilities which can be exploited by an attacker. Unsupervised\ntechniques, such as anomaly detection, can help us secure the IoT devices.\nHowever, an anomaly detection model must be trained for a long time in order to\ncapture all benign behaviors. This approach is vulnerable to adversarial\nattacks since all observations are assumed to be benign while training the\nanomaly detection model.\n  In this paper, we propose CIoTA, a lightweight framework that utilizes the\nblockchain concept to perform distributed and collaborative anomaly detection\nfor devices with limited resources. CIoTA uses blockchain to incrementally\nupdate a trusted anomaly detection model via self-attestation and consensus\namong IoT devices. We evaluate CIoTA on our own distributed IoT simulation\nplatform, which consists of 48 Raspberry Pis, to demonstrate CIoTA's ability to\nenhance the security of each device and the security of the network as a whole.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 13:53:19 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 19:09:08 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Golomb", "Tomer", ""], ["Mirsky", "Yisroel", ""], ["Elovici", "Yuval", ""]]}, {"id": "1803.03885", "submitter": "Kapil Ahuja", "authors": "Rohit Agrawal, Chin Hao Hoo, Kapil Ahuja, and Akash Kumar", "title": "Parallel FPGA Router using Sub-Gradient method and Steiner tree", "comments": "5 pages, double column, 1 figure, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the FPGA (Field Programmable Gate Arrays) design flow, one of the most\ntime-consuming step is the routing of nets. Therefore, there is a need to\naccelerate it. In a recent paper by Hoo et. al., the authors have developed a\nLinear Programming based framework that parallelizes this routing process to\nachieve significant speedups (the algorithm is termed as ParaLaR). However,\nthis approach has certain weaknesses. Namely, the constraints violation by the\nsolution and a local minima that could be improved. We address these two issues\nhere.\n  In our paper, we use this framework and solve it using the Primal-Dual\nsub-gradient method that better exploits the problem properties. We also\npropose a better way to update the size of the step taken by this iterative\nalgorithm. We perform experiments on a set of standard benchmarks, where we\nshow that our algorithm outperforms the standard existing algorithms (VPR and\nParaLaR).\n  We achieve up to 22% improvement in the constraints violation and the\nstandard metric of the minimum channel width when compared with ParaLaR (which\nis same as in VPR). We achieve about 20% savings in another standard metric of\nthe total wire length (when compared with VPR), which is the same as for\nParaLaR. Hence, our algorithm achieves minimum value for all the three\nparameters. Also, the critical path delay for our algorithm is almost same as\ncompared to VPR and ParaLaR. We achieve relative speedups of 3 times when we\nrun a parallel version of our algorithm using 4 threads.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 02:41:06 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 17:46:27 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Agrawal", "Rohit", ""], ["Hoo", "Chin Hao", ""], ["Ahuja", "Kapil", ""], ["Kumar", "Akash", ""]]}, {"id": "1803.03922", "submitter": "Yuechao Pan", "authors": "Yuechao Pan, Roger Pearce and John D. Owens", "title": "Scalable Breadth-First Search on a GPU Cluster", "comments": "12 pages, 13 figures. To appear at IPDPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a GPU cluster, the ratio of high computing power to communication\nbandwidth makes scaling breadth-first search (BFS) on a scale-free graph\nextremely challenging. By separating high and low out-degree vertices, we\npresent an implementation with scalable computation and a model for scalable\ncommunication for BFS and direction-optimized BFS. Our communication model uses\nglobal reduction for high-degree vertices, and point-to-point transmission for\nlow-degree vertices. Leveraging the characteristics of degree separation, we\nreduce the graph size to one third of the conventional edge list\nrepresentation. With several other optimizations, we observe linear weak\nscaling as we increase the number of GPUs, and achieve 259.8 GTEPS on a\nscale-33 Graph500 RMAT graph with 124 GPUs on the latest CORAL early access\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 08:18:51 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 06:37:57 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Pan", "Yuechao", ""], ["Pearce", "Roger", ""], ["Owens", "John D.", ""]]}, {"id": "1803.03951", "submitter": "Ofir Shwartz", "authors": "Ofir Shwartz, Yitzhak Birk", "title": "The Secure Machine: Efficient Secure Execution On Untrusted Platforms", "comments": "A PhD thesis, to appear at the Technion's library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the Secure Machine, SeM for short, a CPU architecture\nextension for secure computing. SeM uses a small amount of in-chip additional\nhardware that monitors key communication channels inside the CPU chip, and only\nacts when required. SeM provides confidentiality and integrity for a secure\nprogram without trusting the platform software or any off-chip hardware. SeM\nsupports existing binaries of single- and multi-threaded applications running\non single- or multi-core, multi-CPU. The performance reduction caused by it is\nonly few percent, most of which is due to the memory encryption layer that is\ncommonly used in many secure architectures.\n  We also developed SeM-Prepare, a software tool that automatically instruments\nexisting applications (binaries) with additional instructions so they can be\nsecurely executed on our architecture without requiring any programming efforts\nor the availability of the desired program`s source code.\n  To enable secure data sharing in shared memory environments, we developed\nSecure Distributed Shared Memory (SDSM), an efficient (time and memory)\nalgorithm for allowing thousands of compute nodes to share data securely while\nrunning on an untrusted computing environment. SDSM shows a negligible\nreduction in performance, and it requires negligible and hardware resources. We\ndeveloped Distributed Memory Integrity Trees, a method for enhancing single\nnode integrity trees for preserving the integrity of a distributed application\nrunning on an untrusted computing environment. We show that our method is\napplicable to existing single node integrity trees such as Merkle Tree, Bonsai\nMerkle Tree, and Intel`s SGX memory integrity engine. All these building blocks\nmay be used together to form a practical secure system, and some can be used in\nconjunction with other secure systems.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 12:09:27 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Shwartz", "Ofir", ""], ["Birk", "Yitzhak", ""]]}, {"id": "1803.04014", "submitter": "Stefano Markidis Prof.", "authors": "Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng,\n  Jeffrey S. Vetter", "title": "NVIDIA Tensor Core Programmability, Performance & Precision", "comments": "This paper has been accepted by the Eighth International Workshop on\n  Accelerators and Hybrid Exascale Systems (AsHES) 2018", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00091", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called\n\"Tensor Core\" that performs one matrix-multiply-and-accumulate on 4x4 matrices\nper clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta\nmicroarchitecture, provides 640 Tensor Cores with a theoretical peak\nperformance of 125 Tflops/s in mixed precision. In this paper, we investigate\ncurrent approaches to program NVIDIA Tensor Cores, their performances and the\nprecision loss due to computation in mixed precision.\n  Currently, NVIDIA provides three different ways of programming\nmatrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply\nAccumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS\nGEMM. After experimenting with different approaches, we found that NVIDIA\nTensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100\nGPU, seven and three times the performance in single and half precision\nrespectively. A WMMA implementation of batched GEMM reaches a performance of 4\nTflops/s. While precision loss due to matrix multiplication with half precision\ninput might be critical in many HPC applications, it can be considerably\nreduced at the cost of increased computation. Our results indicate that HPC\napplications using matrix multiplications can strongly benefit from using of\nNVIDIA Tensor Cores.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 18:55:29 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Markidis", "Stefano", ""], ["Der Chien", "Steven Wei", ""], ["Laure", "Erwin", ""], ["Peng", "Ivy Bo", ""], ["Vetter", "Jeffrey S.", ""]]}, {"id": "1803.04120", "submitter": "Michael Gowanlock", "authors": "Michael Gowanlock, Ben Karsin", "title": "GPU Accelerated Self-join for the Distance Similarity Metric", "comments": "Accepted for Publication in the 4th IEEE International Workshop on\n  High-Performance Big Data, Deep Learning, and Cloud Computing. To appear in\n  the Proceedings of the 32nd IEEE International Parallel and Distributed\n  Processing Symposium Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-join finds all objects in a dataset within a threshold of each other\ndefined by a similarity metric. As such, the self-join is a building block for\nthe field of databases and data mining, and is employed in Big Data\napplications. In this paper, we advance a GPU-efficient algorithm for the\nsimilarity self-join that uses the Euclidean distance metric. The\nsearch-and-refine strategy is an efficient approach for low dimensionality\ndatasets, as index searches degrade with increasing dimension (i.e., the curse\nof dimensionality). Thus, we target the low dimensionality problem, and compare\nour GPU self-join to a search-and-refine implementation, and a state-of-the-art\nparallel algorithm. In low dimensionality, there are several unique challenges\nassociated with efficiently solving the self-join problem on the GPU. Low\ndimensional data often results in higher data densities, causing a significant\nnumber of distance calculations and a large result set. As dimensionality\nincreases, index searches become increasingly exhaustive, forming a performance\nbottleneck. We advance several techniques to overcome these challenges using\nthe GPU. The techniques we propose include a GPU-efficient index that employs a\nbounded search, a batching scheme to accommodate large result set sizes, and a\nreduction in distance calculations through duplicate search removal. Our GPU\nself-join outperforms both search-and-refine and state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 05:28:44 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Gowanlock", "Michael", ""], ["Karsin", "Ben", ""]]}, {"id": "1803.04141", "submitter": "Dimitrios Vasilas", "authors": "Dimitrios Vasilas (LIP6, DELYS), Marc Shapiro (DELYS, LIP6), Bradley\n  King", "title": "A Modular Design for Geo-Distributed Querying", "comments": "5th Workshop on Principles and Practice of Consistency for\n  Distributed Data, Apr 2018, Porto, Portugal. 5th Workshop on Principles and\n  Practice of Consistency for Distributed Data April 23--26, 2018, Porto,\n  Portugal, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most distributed storage systems provide limited abilities for querying data\nby attributes other than their primary keys. Supporting efficient search on\nsecondary attributes is challenging as applications pose varying requirements\nto query processing systems, and no single system design can be suitable for\nall needs. In this paper, we show how to overcome these challenges in order to\nextend distributed data stores to support queries on secondary attributes. We\npropose a modular architecture that is flexible and allows query processing\nsystems to make trade-offs according to different use case requirements. We\ndescribe adap-tive mechanisms that make use of this flexibility to enable query\nprocessing systems to dynamically adjust to query and write operation\nworkloads.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 07:39:56 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Vasilas", "Dimitrios", "", "LIP6, DELYS"], ["Shapiro", "Marc", "", "DELYS, LIP6"], ["King", "Bradley", ""]]}, {"id": "1803.04209", "submitter": "Michael Teng", "authors": "Michael Teng and Frank Wood", "title": "High Throughput Synchronous Distributed Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new, high-throughput, synchronous, distributed, data-parallel,\nstochastic-gradient-descent learning algorithm. This algorithm uses amortized\ninference in a compute-cluster-specific, deep, generative, dynamical model to\nperform joint posterior predictive inference of the mini-batch gradient\ncomputation times of all worker-nodes in a parallel computing cluster. We show\nthat a synchronous parameter server can, by utilizing such a model, choose an\noptimal cutoff time beyond which mini-batch gradient messages from slow workers\nare ignored that maximizes overall mini-batch gradient computations per second.\nIn keeping with earlier findings we observe that, under realistic conditions,\neagerly discarding the mini-batch gradient computations of stragglers not only\nincreases throughput but actually increases the overall rate of convergence as\na function of wall-clock time by virtue of eliminating idleness. The principal\nnovel contribution and finding of this work goes beyond this by demonstrating\nthat using the predicted run-times from a generative model of cluster worker\nperformance to dynamically adjust the cutoff improves substantially over the\nstatic-cutoff prior art, leading to, among other things, significantly reduced\ndeep neural net training times on large computer clusters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 11:51:38 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Teng", "Michael", ""], ["Wood", "Frank", ""]]}, {"id": "1803.04211", "submitter": "B\\'erenger Bramas", "authors": "B\\'erenger Bramas", "title": "Increasing the Degree of Parallelism Using Speculative Execution in\n  Task-based Runtime Systems", "comments": "24 pages, https://peerj.com/articles/cs-183/", "journal-ref": "PeerJ Computer Science 5:e183 2019", "doi": "10.7717/peerj-cs.183", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Task-based programming models have demonstrated their efficiency in the\ndevelopment of scientific applications on modern high-performance platforms.\nThey allow delegation of the management of parallelization to the runtime\nsystem (RS), which is in charge of the data coherency, the scheduling, and the\nassignment of the work to the computational units. However, some applications\nhave a limited degree of parallelism such that no matter how efficient the RS\nimplementation, they may not scale on modern multicore CPUs. In this paper, we\npropose using speculation to unleash the parallelism when it is uncertain if\nsome tasks will modify data, and we formalize a new methodology to enable\nspeculative execution in a graph of tasks. This description is partially\nimplemented in our new C++ RS called SPETABARU, which is capable of executing\ntasks in advance if some others are not certain to modify the data. We study\nthe behavior of our approach to compute Monte Carlo and replica exchange Monte\nCarlo simulations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 11:59:19 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 08:00:05 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Bramas", "B\u00e9renger", ""]]}, {"id": "1803.04237", "submitter": "Diego Didona Dr", "authors": "Diego Didona, Rachid Guerraoui, Jingjing Wang, Willy Zwaenepoel", "title": "Causal Consistency and Latency Optimality: Friend or Foe?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal consistency is an attractive consistency model for replicated data\nstores. It is provably the strongest model that tolerates partitions, it avoids\nthe long latencies associated with strong consistency, and, especially when\nusing read-only transactions, it prevents many of the anomalies of weaker\nconsistency models. Recent work has shown that causal consistency allows\n\"latency-optimal\" read-only transactions, that are nonblocking, single-version\nand single-round in terms of communication. On the surface, this latency\noptimality is very appealing, as the vast majority of applications are assumed\nto have read-dominated workloads.\n  In this paper, we show that such \"latency-optimal\" read-only transactions\ninduce an extra overhead on writes, the extra overhead is so high that\nperformance is actually jeopardized, even in read-dominated workloads. We show\nthis result from a practical and a theoretical angle.\n  First, we present a protocol that implements \"almost laten- cy-optimal\" ROTs\nbut does not impose on the writes any of the overhead of latency-optimal\nprotocols. In this protocol, ROTs are nonblocking, one version and can be\nconfigured to use either two or one and a half rounds of client-server\ncommunication. We experimentally show that this protocol not only provides\nbetter throughput, as expected, but also surprisingly better latencies for all\nbut the lowest loads and most read-heavy workloads.\n  Then, we prove that the extra overhead imposed on writes by latency-optimal\nread-only transactions is inherent, i.e., it is not an artifact of the design\nwe consider, and cannot be avoided by any implementation of latency-optimal\nread-only transactions. We show in particular that this overhead grows linearly\nwith the number of clients.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 13:18:05 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Didona", "Diego", ""], ["Guerraoui", "Rachid", ""], ["Wang", "Jingjing", ""], ["Zwaenepoel", "Willy", ""]]}, {"id": "1803.04270", "submitter": "He Li", "authors": "He Li, Song Guo, Chentao Wu, Jie Li", "title": "FDRC: Flow-Driven Rule Caching Optimization in Software Defined\n  Networking", "comments": null, "journal-ref": "Proceedings of 2015 IEEE International Conference on\n  Communications (ICC), pp.5777-5782", "doi": "10.1109/ICC.2015.7249243", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the sharp growth of cloud services and their possible combinations, the\nscale of data center network traffic has an inevitable explosive increasing in\nrecent years. Software defined network (SDN) provides a scalable and flexible\nstructure to simplify network traffic management. It has been shown that\nTernary Content Addressable Memory (TCAM) management plays an important role on\nthe performance of SDN. However, previous literatures, in point of view on rule\nplacement strategies, are still insufficient to provide high scalability for\nprocessing large flow sets with a limited TCAM size. So caching is a brand new\nmethod for TCAM management which can provide better performance than rule\nplacement. In this paper, we propose FDRC, an efficient flow-driven rule\ncaching algorithm to optimize the cache replacement in SDN-based networks.\nDifferent from the previous packet-driven caching algorithm, FDRC is\ncharacterized by trying to deal with the challenges of limited cache size\nconstraint and unpredictable flows. In particular, we design a caching\nalgorithm with low-complexity to achieve high cache hit ratio by prefetching\nand special replacement strategy for predictable and unpredictable flows,\nrespectively. By conducting extensive simulations, we demonstrate that our\nproposed caching algorithm significantly outperforms FIFO and least recently\nused (LRU) algorithms under various network settings.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 14:11:15 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Li", "He", ""], ["Guo", "Song", ""], ["Wu", "Chentao", ""], ["Li", "Jie", ""]]}, {"id": "1803.04277", "submitter": "He Li", "authors": "He Li, Hai Jin", "title": "SDPMN: Privacy Preserving MapReduce Network Using SDN", "comments": null, "journal-ref": "Proceedings of 2014 International Conference on Cloud Computing\n  and Big Data, pp.109-115", "doi": "10.1109/CCBD.2014.30", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce is a popular programming model and an associated implementation for\nparallel processing big data in the distributed environment. Since large scaled\nMapReduce data centers usually provide services to many users, it is an\nessential problem to preserve the privacy between different applications in the\nsame network. In this paper, we propose SDPMN, a framework that using\n\\textit{software defined network} (SDN) to distinguish the network between each\napplication, which is a manageable and scalable method. We design this\nframework based on the existing SDN structure and Hadoop networks. Since the\nrule space of each SDN device is limited, we also propose the rule placement\noptimization for this framework to maximize the hardware supported isolated\napplication networks. We state this problem in a general MapReduce network and\ndesign a heuristic algorithm to find the solution. From the simulation based\nevaluation, with our algorithm, the given network can support more privacy\npreserving application networks with SDN switches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 14:22:02 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Li", "He", ""], ["Jin", "Hai", ""]]}, {"id": "1803.04292", "submitter": "Bertil Chapuis", "authors": "Bertil Chapuis, Benoit Garbinato", "title": "Geodabs: Trajectory Indexing Meets Fingerprinting at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding trajectories and discovering motifs that are similar in large\ndatasets is a central problem for a wide range of applications. Solutions\naddressing this problem usually rely on spatial indexing and on the computation\nof a similarity measure in polynomial time. Although effective in the context\nof sparse trajectory datasets, this approach is too expensive in the context of\ndense datasets, where many trajectories potentially match with a given query.\nIn this paper, we apply fingerprinting, a copy-detection mechanism used in the\ncontext of textual data, to trajectories. To this end, we fingerprint\ntrajectories with geodabs, a construction based on geohash aimed at trajectory\nfingerprinting. We demonstrate that by relying on the properties of a space\nfilling curve geodabs can be used to build sharded inverted indexes. We show\nhow normalization affects precision and recall, two key measures in information\nretrieval. We then demonstrate that the probabilistic nature of fingerprinting\nhas a marginal effect on the quality of the results. Finally, we evaluate our\nmethod in terms of performances and show that, in contrast with existing\nmethods, it is not affected by the density of the trajectory dataset and that\nit can be efficiently distributed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 14:48:06 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Chapuis", "Bertil", ""], ["Garbinato", "Benoit", ""]]}, {"id": "1803.04378", "submitter": "Mehdi Ghatee Dr.", "authors": "Arash Raeisi Gahrouei and Mehdi Ghatee", "title": "Effective Implementation of GPU-based Revised Simplex algorithm applying\n  new memory management and cycle avoidance strategies", "comments": "27 pages, 6 Tables, 10 Figures, Extracted from a PhD research program\n  in Department of Computer Science of Amirkabir University of Technology,\n  Tehran, Iran", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Processing Units (GPUs) with high computational capabilities used as\nmodern parallel platforms to deal with complex computational problems. We use\nthis platform to solve large-scale linear programing problems by revised\nsimplex algorithm. To implement this algorithm, we propose some new memory\nmanagement strategies. In addition, to avoid cycling because of degeneracy\nconditions, we use a tabu rule for entering variable selection in the revised\nsimplex algorithm. To evaluate this algorithm, we consider two sets of\nbenchmark problems and compare the speedup factors for these problems. The\ncomparisons demonstrate that the proposed method is highly effective and solve\nthe problems with the maximum speedup factors 165.2 and 65.46 with respect to\nthe sequential version and Matlab Linprog solver respectively.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 17:11:31 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Gahrouei", "Arash Raeisi", ""], ["Ghatee", "Mehdi", ""]]}, {"id": "1803.04432", "submitter": "Manuel P\\\"oter BSc", "authors": "Manuel P\\\"oter and Jesper Larsson Tr\\\"aff", "title": "Memory Models for C/C++ Programmers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memory model is the crux of the concurrency semantics of shared-memory\nsystems. It defines the possible values that a read operation is allowed to\nreturn for any given set of write operations performed by a concurrent program,\nthereby defining the basic semantics of shared variables. It is therefore\nimpossible to meaningfully reason about a program or any part of the\nprogramming language implementation without an unambiguous memory model.\n  This note provides a brief introduction into the topic of memory models,\nexplaining why it is essential for concurrent programs and covering well known\nmemory models from sequential consistency to those of the x86 and ARM/POWER\nCPUs. Section 4 is fully dedicated to the C++11 memory model, explaining how it\ncan be used to write concurrent code that is not only correct and portable, but\nalso efficient by utilizing the relaxed memory models of modern architectures.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 18:08:03 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["P\u00f6ter", "Manuel", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1803.04442", "submitter": "Krzysztof Rzadca", "authors": "Grzegorz Milka, Krzysztof Rzadca", "title": "Dfuntest: A Testing Framework for Distributed Applications", "comments": "PPAM 2017 Proceedings", "journal-ref": null, "doi": "10.1007/978-3-319-78024-5_35", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New ideas in distributed systems (algorithms or protocols) are commonly\ntested by simulation, because experimenting with a prototype deployed on a\nrealistic platform is cumbersome. However, a prototype not only measures\nperformance but also verifies assumptions about the underlying system. We\ndeveloped dfuntest - a testing framework for distributed applications that\ndefines abstractions and test structure, and automates experiments on\ndistributed platforms. Dfuntest aims to be jUnit's analogue for distributed\napplications; a framework that enables the programmer to write robust and\nflexible scenarios of experiments. Dfuntest requires minimal bindings that\nspecify how to deploy and interact with the application. Dfuntest's\nabstractions allow execution of a scenario on a single machine, a cluster, a\ncloud, or any other distributed infrastructure, e.g. on PlanetLab. A scenario\nis a procedure; thus, our framework can be used both for functional tests and\nfor performance measurements. We show how to use dfuntest to deploy our DHT\nprototype on 60 PlanetLab nodes and verify whether the prototype maintains a\ncorrect topology.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 18:25:59 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Milka", "Grzegorz", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "1803.04513", "submitter": "Dimitris Sakavalas", "authors": "Dimitris Sakavalas, Lewis Tseng, Nitin H. Vaidya", "title": "Effects of Topology Knowledge and Relay Depth on Asynchronous Consensus", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a point-to-point message-passing network. We are interested in the\nasynchronous crash-tolerant consensus problem in incomplete networks. We study\nthe feasibility and efficiency of approximate consensus under different\nrestrictions on topology knowledge and the relay depth, i.e., the maximum\nnumber of hops any message can be relayed. These two constraints are common in\nlarge-scale networks, and are used to avoid memory overload and network\ncongestion respectively. Specifically, for different values of integers k, k ,\nwe consider that each node knows all its neighbors of at most k-hop distance\n(k-hop topology knowledge), and the relay depth is k . We consider both\ndirected and undirected graphs. More concretely, we answer the following main\nquestion in asynchronous systems:\n  What is a tight condition on the underlying communication graphs for\nachieving approximate consensus if each node has only a k-hop topology\nknowledge and relay depth k?\n  To prove that the necessary conditions presented in the paper are also\nsufficient, we have developed algorithms that achieve consensus in graphs\nsatisfying those conditions:\n  -The first class of algorithms requires k-hop topology knowledge and relay\ndepth k. Unlike prior algorithms, these algorithms do not flood the network,\nand each node does not need the full topology knowledge. We show how the\nconvergence time and the message complexity of those algorithms is affected by\nk, providing the respective upper bounds.\n  -The second set of algorithms requires only one-hop neighborhood knowledge,\ni.e., immediate incoming and outgoing neighbors, but needs to flood the network\n(i.e., relay depth is n, where n is the number of nodes). One result that may\nbe of independent interest is a topology discovery mechanism to learn and\n\"estimate\" the topology in asynchronous directed networks with crash faults.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 20:30:46 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 17:49:56 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 22:02:51 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Sakavalas", "Dimitris", ""], ["Tseng", "Lewis", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1803.04631", "submitter": "Xiaolong Xie", "authors": "Xiaolong Xie, Yun Liang, Xiuhong Li, Wei Tan", "title": "CuLDA_CGS: Solving Large-scale LDA Problems on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation(LDA) is a popular topic model. Given the fact\nthat the input corpus of LDA algorithms consists of millions to billions of\ntokens, the LDA training process is very time-consuming, which may prevent the\nusage of LDA in many scenarios, e.g., online service. GPUs have benefited\nmodern machine learning algorithms and big data analysis as they can provide\nhigh memory bandwidth and computation power. Therefore, many frameworks, e.g.\nTen- sorFlow, Caffe, CNTK, support to use GPUs for accelerating the popular\nmachine learning data-intensive algorithms. However, we observe that LDA\nsolutions on GPUs are not satisfying.\n  In this paper, we present CuLDA_CGS, a GPU-based efficient and scalable\napproach to accelerate large-scale LDA problems. CuLDA_CGS is designed to\nefficiently solve LDA problems at high throughput. To it, we first delicately\ndesign workload partition and synchronization mechanism to exploit the benefits\nof mul- tiple GPUs. Then, we offload the LDA sampling process to each\nindividual GPU by optimizing from the sampling algorithm, par- allelization,\nand data compression perspectives. Evaluations show that compared with\nstate-of-the-art LDA solutions, CuLDA_CGS outperforms them by a large margin\n(up to 7.3X) on a single GPU. CuLDA_CGS is able to achieve extra 3.0X speedup\non 4 GPUs. The source code is publicly available on https://github.com/cuMF/\nCuLDA_CGS.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 05:44:40 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Xie", "Xiaolong", ""], ["Liang", "Yun", ""], ["Li", "Xiuhong", ""], ["Tan", "Wei", ""]]}, {"id": "1803.04725", "submitter": "Jingzhao Wang", "authors": "Jingzhao Wang, Tinghan Wang and Yuan Luo", "title": "Storage and Repair Bandwidth Tradeoff for Distributed Storage Systems\n  with Clusters and Separate Nodes", "comments": null, "journal-ref": "Sci. China Inf. Sci. (2018) 61: 100303", "doi": "10.1007/s11432-018-9499-0", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal tradeoff between node storage and repair bandwidth is an\nimportant issue for distributed storage systems (DSSs). As for realistic DSSs\nwith clusters, when repairing a failed node, it is more efficient to download\nmore data from intra-cluster nodes than from cross-cluster nodes. Therefore, it\nis meaningful to differentiate the repair bandwidth from intra-cluster and\ncross-cluster. For cluster DSSs the tradeoff has been considered with special\nrepair assumptions where all the alive nodes are utilized to repair a failed\nnode. In this paper, we investigate the optimal tradeoff for cluster DSSs under\nmore general storage/repair parameters. Furthermore, a regenerating code\nconstruction strategy achieving the points in the optimal tradeoff curve is\nproposed for cluster DSSs with specific parameters as a numerical example.\nMoreover, the influence of separate nodes for the tradeoff is also considered\nfor DSSs with clusters and separated nodes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 10:57:12 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Wang", "Jingzhao", ""], ["Wang", "Tinghan", ""], ["Luo", "Yuan", ""]]}, {"id": "1803.04780", "submitter": "EPTCS", "authors": "Onoriode Uviase, Gerald Kotonya", "title": "IoT Architectural Framework: Connection and Integration Framework for\n  IoT Systems", "comments": "In Proceedings ALP4IoT 2017, arXiv:1802.00976", "journal-ref": "EPTCS 264, 2018, pp. 1-17", "doi": "10.4204/EPTCS.264.1", "report-no": null, "categories": "cs.DC cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of the Internet of Things (IoT) has since seen a growing\ninterest in architectural design and adaptive frameworks to promote the\nconnection between heterogeneous IoT devices and IoT systems. The most widely\nfavoured software architecture in IoT is the Service Oriented Architecture\n(SOA), which aims to provide a loosely coupled systems to leverage the use and\nreuse of IoT services at the middle-ware layer, to minimise system integration\nproblems. However, despite the flexibility offered by SOA, the challenges of\nintegrating, scaling and ensuring resilience in IoT systems persist. One of the\nkey causes of poor integration in IoT systems is the lack of an intelligent,\nconnection-aware framework to support interaction in IoT systems. This paper\nreviews existing architectural frameworks for integrating IoT devices and\nidentifies the key areas that require further research improvements. The paper\nconcludes by proposing a possible solution based on microservice. The proposed\nIoT integration framework benefits from an intelligent API layer that employs\nan external service assembler, service auditor, service monitor and service\nrouter component to coordinate service publishing, subscription, decoupling and\nservice combination within the architecture.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 04:09:36 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Uviase", "Onoriode", ""], ["Kotonya", "Gerald", ""]]}, {"id": "1803.04781", "submitter": "Tushar Semwal", "authors": "Tushar Semwal, Shashi Shekhar Jha and Shivashankar B. Nair", "title": "On Ordering Multi-Robot Task Executions within a Cyber Physical System", "comments": "28 pages, 14 figures, accepted in ACM TAAS", "journal-ref": "Tushar Semwal, Shashi Shekhar Jha, and Shivashankar B. Nair. 2017.\n  On Ordering Multi-Robot Task Executions within a Cyber Physical System. ACM\n  Trans. Auton. Adapt. Syst. 12, 4, Article 20 (November 2017), 27 pages", "doi": "10.1145/3124677", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With robots entering the world of Cyber Physical Systems (CPS), ordering the\nexecution of allocated tasks during run-time becomes crucial. This is so\nbecause, in a real world, there can be several physical tasks that use shared\nresources that need to be executed concurrently. In this paper, we propose a\nmechanism to solve this issue of ordering task executions within a CPS which\ninherently handles mutual exclusion. The mechanism caters to a decentralized\nand distributed CPS comprising nodes such as computers, robots and sensor\nnodes, and uses mobile software agents that knit through them to aid the\nexecution of the various tasks while also ensuring mutual exclusion of shared\nresources. The computations, communications and control, are achieved through\nthese mobile agents. Physical execution of the tasks is performed by the robots\nin an asynchronous and pipelined manner without the use of a clock. The\nmechanism also features addition and deletion of tasks and insertion and\nremoval of robots facilitating \\textit{On-The-Fly Programming}. As an\napplication, a Warehouse Management System as a CPS has been implemented. The\npaper concludes with the results and discussions on using the mechanism in both\nemulated and real world environments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 06:11:11 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Semwal", "Tushar", ""], ["Jha", "Shashi Shekhar", ""], ["Nair", "Shivashankar B.", ""]]}, {"id": "1803.04782", "submitter": "Bin Yu", "authors": "Bin Yu, Ke Zhu, Kaiteng Wu, Michael Zhang", "title": "Improved OpenCL-based Implementation of Social Field Pedestrian Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cond-mat.other", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two aspects of improvements are proposed for the OpenCL-based implementation\nof the social field pedestrian model. In the aspect of algorithm, a method\nbased on the idea of divide-and-conquer is devised in order to overcome the\nproblem of global memory depletion when fields are of a larger size. This is of\nimportance for the study of finer pedestrian walking behavior, which usually\nrequires larger fields. In the aspect of computation, the OpenCL heterogeneous\nframework is thoroughly studied. Factors that may affect the numerical\nefficiency are evaluated, with regarding to the social field model previously\nproposed. This includes usage of local memory, deliberate patch of data\nstructures for avoidance of bank conflicts, and so on. Numerical experiments\ndisclose that the numerical efficiency is brought to an even higher level.\nCompared to the CPU model and the previous GPU model, the current GPU model can\nbe at most 71.56 and 13.3 times faster respectively so that it is more\nqualified to be a core engine for analysis of super-large scale crowd.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 07:49:30 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 05:16:44 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Yu", "Bin", ""], ["Zhu", "Ke", ""], ["Wu", "Kaiteng", ""], ["Zhang", "Michael", ""]]}, {"id": "1803.04783", "submitter": "Fabian Schuiki", "authors": "Fabian Schuiki, Michael Schaffner, Frank K. G\\\"urkaynak, Luca Benini", "title": "A Scalable Near-Memory Architecture for Training Deep Neural Networks on\n  Large In-Memory Datasets", "comments": "14 pages, submitted to IEEE Transactions on Computers journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most investigations into near-memory hardware accelerators for deep neural\nnetworks have primarily focused on inference, while the potential of\naccelerating training has received relatively little attention so far. Based on\nan in-depth analysis of the key computational patterns in state-of-the-art\ngradient-based training methods, we propose an efficient near-memory\nacceleration engine called NTX that can be used to train state-of-the-art deep\nconvolutional neural networks at scale. Our main contributions are: (i) a loose\ncoupling of RISC-V cores and NTX co-processors reducing offloading overhead by\n7x over previously published results; (ii) an optimized IEEE754 compliant data\npath for fast high-precision convolutions and gradient propagation; (iii)\nevaluation of near-memory computing with NTX embedded into residual area on the\nLogic Base die of a Hybrid Memory Cube; and (iv) a scaling analysis to meshes\nof HMCs in a data center scenario. We demonstrate a 2.7x energy efficiency\nimprovement of NTX over contemporary GPUs at 4.4x less silicon area, and a\ncompute performance of 1.2 Tflop/s for training large state-of-the-art networks\nwith full floating-point precision. At the data center scale, a mesh of NTX\nachieves above 95% parallel and energy efficiency, while providing 2.1x energy\nsavings or 3.1x performance improvement over a GPU-based system.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 09:28:22 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 08:28:37 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 14:56:53 GMT"}, {"version": "v4", "created": "Wed, 17 Oct 2018 10:25:49 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Schuiki", "Fabian", ""], ["Schaffner", "Michael", ""], ["G\u00fcrkaynak", "Frank K.", ""], ["Benini", "Luca", ""]]}, {"id": "1803.04784", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Renewing computing paradigms for more efficient parallelization of\n  single-threads", "comments": "28 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing is still based on the 70-years old paradigms introduced by von\nNeumann. The need for more performant, comfortable and safe computing forced to\ndevelop and utilize several tricks both in hardware and software. Till now\ntechnology enabled to increase performance without changing the basic computing\nparadigms. The recent stalling of single-threaded computing performance,\nhowever, requires to redesign computing to be able to provide the expected\nperformance. To do so, the computing paradigms themselves must be scrutinized.\nThe limitations caused by the too restrictive interpretation of the computing\nparadigms are demonstrated, an extended computing paradigm introduced, ideas\nabout changing elements of the computing stack suggested, some implementation\ndetails of both hardware and software discussed. The resulting new computing\nstack offers considerably higher computing throughput, simplified hardware\narchitecture, drastically improved real-time behavior and in general,\nsimplified and more efficient computing stack.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:43:58 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1803.04785", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat, Yuriy O. Ivanov, Aladdein M. Amro", "title": "Model Oriented Scheduling Algorithm for The Hardware-In-The-Loop\n  Simulation", "comments": "10 pages, 5 figures", "journal-ref": "International Journal of Electronics Communication and Computer\n  Engineering, 2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for designing software for dynamical systems\nsimulation. An algorithm is proposed to obtain a schedule for calculating each\nphase variable of a stiff system of differential equations. The problem is\nclassified as a fixed-priority pre-emptive scheduling of periodic tasks. The\nBranch-and-Bound algorithm is modified to minimize the defined utilization\nfunction and to optimize the scheduling process for a numerical solver. A\nprogram for the experimental schedule is implemented solving a job-shop problem\nthat proved the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 20:37:28 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Ivanov", "Yuriy O.", ""], ["Amro", "Aladdein M.", ""]]}, {"id": "1803.04786", "submitter": "Prasanna Kansakar", "authors": "Prasanna Kansakar and Arslan Munir", "title": "A Design Space Exploration Methodology for Parameter Optimization in\n  Multicore Processors", "comments": "Published in IEEE Transactions on Parallel and Distributed Systems.\n  arXiv admin note: text overlap with arXiv:1802.05123", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems ( Volume:\n  29, Issue: 1, Jan. 1 2018 ) Page(s): 2 - 15", "doi": "10.1109/TPDS.2017.2745580", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for application-specific design of multicore/manycore processing\nplatforms is evident with computing systems finding use in diverse application\ndomains. In order to tailor multicore/manycore processors for application\nspecific requirements, a multitude of processor design parameters have to be\ntuned accordingly which involves rigorous and extensive design space\nexploration over large search spaces. In this paper, we propose an efficient\nmethodology for design space exploration. We evaluate our methodology over two\nsearch spaces - small and large, using a cycle-accurate simulator (ESESC) and a\nstandard set of PARSEC and SPLASH-2 benchmarks. For the smaller design space,\nwe compare results obtained from our design space exploration methodology with\nresults obtained from fully exhaustive search. The results show that solution\nquality obtained from our methodology are within 1.35% - 3.69% of the results\nobtained from fully exhaustive search while only exploring 2.74% - 3% of the\ndesign space. For larger design space, we compare solution quality of different\nresults obtained by varying the number of tunable processor design parameters\nincluded in the exhaustive search phase of our methodology. The results show\nthat including more number of tunable parameters in the exhaustive search phase\nof our methodology greatly improves solution quality.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:43:05 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Kansakar", "Prasanna", ""], ["Munir", "Arslan", ""]]}, {"id": "1803.04970", "submitter": "Johannes Holke", "authors": "Johannes Holke", "title": "Scalable Algorithms for Parallel Tree-based Adaptive Mesh Refinement\n  with General Element Types", "comments": "200 Pages, dissertation, 58 figures, Bonn (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we develop, discuss and implement algorithms for scalable\nparallel tree-based adaptive mesh refinement (AMR) using space-filling curves\n(SFCs). We create an AMR software that works independently of the used element\ntype, such as for example lines, triangles, tetrahedra, quadrilaterals,\nhexahedra, and prisms. Along with a detailed mathematical discussion, this\nrequires the implementation as a numerical software and its validation, as well\nas scalability tests on current supercomputers. For triangular and tetrahedral\nelements (simplices) with red-refinement (1:4 in 2D, 1:8 in 3D), we develop a\nnew SFC index, the tetrahedral Morton index (TM-index). Its construction is\nsimilar to the Morton index for quadrilaterals/hexahedra, as it is also based\non bitwise interleaving the coordinates of a certain vertex of the simplex, the\nanchor node. We develop and demonstrate a new simplicial SFC and create a fast\nand scalable tree-based AMR software that offers a flexibility and generality\nthat was previously not available.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 17:23:03 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 14:05:57 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 16:21:54 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Holke", "Johannes", ""]]}, {"id": "1803.05069", "submitter": "Maofan Yin", "authors": "Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, Ittai\n  Abraham", "title": "HotStuff: BFT Consensus in the Lens of Blockchain", "comments": "a shorter version of this paper has been published in PODC'19, which\n  does not include interpretation of other protocols using the framework,\n  system evaluation or additional proofs in appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HotStuff, a leader-based Byzantine fault-tolerant replication\nprotocol for the partially synchronous model. Once network communication\nbecomes synchronous, HotStuff enables a correct leader to drive the protocol to\nconsensus at the pace of actual (vs. maximum) network delay--a property called\nresponsiveness--and with communication complexity that is linear in the number\nof replicas. To our knowledge, HotStuff is the first partially synchronous BFT\nreplication protocol exhibiting these combined properties. HotStuff is built\naround a novel framework that forms a bridge between classical BFT foundations\nand blockchains. It allows the expression of other known protocols (DLS, PBFT,\nTendermint, Casper), and ours, in a common framework.\n  Our deployment of HotStuff over a network with over 100 replicas achieves\nthroughput and latency comparable to that of BFT-SMaRt, while enjoying linear\ncommunication footprint during leader failover (vs. quadratic with BFT-SMaRt).\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 23:01:05 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 15:39:12 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 18:21:08 GMT"}, {"version": "v4", "created": "Tue, 2 Apr 2019 00:48:38 GMT"}, {"version": "v5", "created": "Wed, 5 Jun 2019 04:26:20 GMT"}, {"version": "v6", "created": "Tue, 23 Jul 2019 05:19:36 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Yin", "Maofan", ""], ["Malkhi", "Dahlia", ""], ["Reiter", "Michael K.", ""], ["Gueta", "Guy Golan", ""], ["Abraham", "Ittai", ""]]}, {"id": "1803.05255", "submitter": "Blesson Varghese", "authors": "Cihat Baktir, Cagatay Sonmez, Cem Ersoy, Atay Ozgovde, and Blesson\n  Varghese", "title": "Addressing the Challenges in Federating Edge Resources", "comments": "Book Chapter accepted to the Fog and Edge Computing: Principles and\n  Paradigms; Editors Buyya, Srirama", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book chapter considers how Edge deployments can be brought to bear in a\nglobal context by federating them across multiple geographic regions to create\na global Edge-based fabric that decentralizes data center computation. This is\ncurrently impractical, not only because of technical challenges, but is also\nshrouded by social, legal and geopolitical issues. In this chapter, we discuss\ntwo key challenges - networking and management in federating Edge deployments.\nAdditionally, we consider resource and modeling challenges that will need to be\naddressed for a federated Edge.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 13:16:44 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Baktir", "Cihat", ""], ["Sonmez", "Cagatay", ""], ["Ersoy", "Cem", ""], ["Ozgovde", "Atay", ""], ["Varghese", "Blesson", ""]]}, {"id": "1803.05320", "submitter": "Farhad Merchant", "authors": "Farhad Merchant, Tarun Vatwani, Anupam Chattopadhyay, Soumyendu Raha,\n  S K Nandy, Ranjani Narayan, and Rainer Leupers", "title": "Efficient Realization of Givens Rotation through Algorithm-Architecture\n  Co-design for Acceleration of QR Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient realization of Generalized Givens Rotation (GGR) based\nQR factorization that achieves 3-100x better performance in terms of\nGflops/watt over state-of-the-art realizations on multicore, and General\nPurpose Graphics Processing Units (GPGPUs). GGR is an improvement over\nclassical Givens Rotation (GR) operation that can annihilate multiple elements\nof rows and columns of an input matrix simultaneously. GGR takes 33% lesser\nmultiplications compared to GR. For custom implementation of GGR, we identify\nmacro operations in GGR and realize them on a Reconfigurable Data-path (RDP)\ntightly coupled to pipeline of a Processing Element (PE). In PE, GGR attains\nspeed-up of 1.1x over Modified Householder Transform (MHT) presented in the\nliterature. For parallel realization of GGR, we use REDEFINE, a scalable\nmassively parallel Coarse-grained Reconfigurable Architecture, and show that\nthe speed-up attained is commensurate with the hardware resources in REDEFINE.\nGGR also outperforms General Matrix Multiplication (gemm) by 10% in-terms of\nGflops/watt which is counter-intuitive.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 14:41:52 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 08:41:53 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Merchant", "Farhad", ""], ["Vatwani", "Tarun", ""], ["Chattopadhyay", "Anupam", ""], ["Raha", "Soumyendu", ""], ["Nandy", "S K", ""], ["Narayan", "Ranjani", ""], ["Leupers", "Rainer", ""]]}, {"id": "1803.05397", "submitter": "Can Karakus", "authors": "Can Karakus, Yifan Sun, Suhas Diggavi, Wotao Yin", "title": "Redundancy Techniques for Straggler Mitigation in Distributed\n  Optimization and Learning", "comments": "39 pages, 14 figures. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of distributed optimization and learning systems is bottlenecked\nby \"straggler\" nodes and slow communication links, which significantly delay\ncomputation. We propose a distributed optimization framework where the dataset\nis \"encoded\" to have an over-complete representation with built-in redundancy,\nand the straggling nodes in the system are dynamically left out of the\ncomputation at every iteration, whose loss is compensated by the embedded\nredundancy. We show that oblivious application of several popular optimization\nalgorithms on encoded data, including gradient descent, L-BFGS, proximal\ngradient under data parallelism, and coordinate descent under model\nparallelism, converge to either approximate or exact solutions of the original\nproblem when stragglers are treated as erasures. These convergence results are\ndeterministic, i.e., they establish sample path convergence for arbitrary\nsequences of delay patterns or distributions on the nodes, and are independent\nof the tail behavior of the delay distribution. We demonstrate that equiangular\ntight frames have desirable properties as encoding matrices, and propose\nefficient mechanisms for encoding large-scale data. We implement the proposed\ntechnique on Amazon EC2 clusters, and demonstrate its performance over several\nlearning problems, including matrix factorization, LASSO, ridge regression and\nlogistic regression, and compare the proposed method with uncoded,\nasynchronous, and data replication strategies.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 16:48:08 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Karakus", "Can", ""], ["Sun", "Yifan", ""], ["Diggavi", "Suhas", ""], ["Yin", "Wotao", ""]]}, {"id": "1803.05575", "submitter": "Zhuolun Xiang", "authors": "Zhuolun Xiang, Nitin H. Vaidya", "title": "Global Stabilization for Causally Consistent Partial Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causally consistent distributed storage systems have received significant\nattention recently due to the potential for providing high throughput and\ncausality guarantees. {\\em Global stabilization} is a technique established for\nachieving causal consistency in distributed multi-version key-value store\nsystems, adopted by the previous work such as GentleRain\n\\cite{Du2014GentleRainCA} and Cure \\cite{akkoorath2016cure}. Intuitively, this\napproach serializes all updates by their physical time and computes the\n``Global Stable Time'' which is a time point $t$ such that versions with\ntimestamp $\\leq t$ can be returned to the client without violating causality.\nHowever, all previous designs with global stabilization assume {\\em full\nreplication}, where each data center stores a full copy of data, and each\nclient is restricted to access servers within one data center. In this paper,\nwe propose a theoretical framework to support {\\em general partial replication}\nwith causal consistency via global stabilization, where each server can store\nan arbitrary subset of the data, and each client is allowed to communicate with\nany subset of the servers and migrate among them without extra delays. We\npropose an algorithm that implements causal consistency for distributed\nmulti-version key-value stores with general partially replication. We prove the\noptimality of the Global Stable Time computation in our algorithm regarding the\nremote update visibility latency, i.e. how fast update from a remote server is\nvisible to the client, under general partial replication. We also provide\ntrade-offs to further optimize the remote update visibility by introducing\nextra delays during client's migration. Simulation results on the performance\nof our algorithm compared to the previous work are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 02:40:21 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 16:34:27 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 15:03:13 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Xiang", "Zhuolun", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1803.05587", "submitter": "Chin-Jung Hsu", "authors": "Chin-Jung Hsu, Vivek Nair, Tim Menzies, Vincent Freeh", "title": "Micky: A Cheaper Alternative for Selecting Cloud Instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most cloud computing optimizers explore and improve one workload at a time.\nWhen optimizing many workloads, the single-optimizer approach can be\nprohibitively expensive. Accordingly, we examine \"collective optimizer\" that\nconcurrently explore and improve a set of workloads significantly reducing the\nmeasurement costs. Our large-scale empirical study shows that there is often a\nsingle cloud configuration which is surprisingly near-optimal for most\nworkloads. Consequently, we create a collective-optimizer, MICKY, that\nreformulates the task of finding the near-optimal cloud configuration as a\nmulti-armed bandit problem. MICKY efficiently balances exploration (of new\ncloud configurations) and exploitation (of known good cloud configuration). Our\nexperiments show that MICKY can achieve on average 8.6 times reduction in\nmeasurement cost as compared to the state-of-the-art method while finding\nnear-optimal solutions.\n  Hence we propose MICKY as the basis of a practical collective optimization\nmethod for finding good cloud configurations (based on various constraints such\nas budget and tolerance to near-optimal configurations).\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 04:14:03 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hsu", "Chin-Jung", ""], ["Nair", "Vivek", ""], ["Menzies", "Tim", ""], ["Freeh", "Vincent", ""]]}, {"id": "1803.05636", "submitter": "Vassilis Papataxiarhis", "authors": "Vassilis Papataxiarhis and Stathes Hadjiefthymiades", "title": "Event Correlation and Forecasting over Multivariate Streaming Sensor\n  Data", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event management in sensor networks is a multidisciplinary field involving\nseveral steps across the processing chain. In this paper, we discuss the major\nsteps that should be performed in real- or near real-time event handling\nincluding event detection, correlation, prediction and filtering. First, we\ndiscuss existing univariate and multivariate change detection schemes for the\nonline event detection over sensor data. Next, we propose an online event\ncorrelation scheme that intends to unveil the internal dynamics that govern the\noperation of a system and are responsible for the generation of various types\nof events. We show that representation of event dependencies can be\naccommodated within a probabilistic temporal knowledge representation framework\nthat allows the formulation of rules. We also address the important issue of\nidentifying outdated dependencies among events by setting up a time-dependent\nframework for filtering the extracted rules over time. The proposed theory is\napplied on the maritime domain and is validated through extensive\nexperimentation with real sensor streams originating from large-scale sensor\nnetworks deployed in ships.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 08:46:23 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Papataxiarhis", "Vassilis", ""], ["Hadjiefthymiades", "Stathes", ""]]}, {"id": "1803.05880", "submitter": "Jeffrey Daily", "authors": "Jeff Daily, Abhinav Vishnu, Charles Siegel, Thomas Warfel, Vinay\n  Amatya", "title": "GossipGraD: Scalable Deep Learning using Gossip Communication based\n  Asynchronous Gradient Descent", "comments": "13 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GossipGraD - a gossip communication protocol based\nStochastic Gradient Descent (SGD) algorithm for scaling Deep Learning (DL)\nalgorithms on large-scale systems. The salient features of GossipGraD are: 1)\nreduction in overall communication complexity from {\\Theta}(log(p)) for p\ncompute nodes in well-studied SGD to O(1), 2) model diffusion such that compute\nnodes exchange their updates (gradients) indirectly after every log(p) steps,\n3) rotation of communication partners for facilitating direct diffusion of\ngradients, 4) asynchronous distributed shuffle of samples during the\nfeedforward phase in SGD to prevent over-fitting, 5) asynchronous communication\nof gradients for further reducing the communication cost of SGD and GossipGraD.\nWe implement GossipGraD for GPU and CPU clusters and use NVIDIA GPUs (Pascal\nP100) connected with InfiniBand, and Intel Knights Landing (KNL) connected with\nAries network. We evaluate GossipGraD using well-studied dataset ImageNet-1K\n(~250GB), and widely studied neural network topologies such as GoogLeNet and\nResNet50 (current winner of ImageNet Large Scale Visualization Research\nChallenge (ILSVRC)). Our performance evaluation using both KNL and Pascal GPUs\nindicates that GossipGraD can achieve perfect efficiency for these datasets and\ntheir associated neural network topologies. Specifically, for ResNet50,\nGossipGraD is able to achieve ~100% compute efficiency using 128 NVIDIA Pascal\nP100 GPUs - while matching the top-1 classification accuracy published in\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:32:16 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Daily", "Jeff", ""], ["Vishnu", "Abhinav", ""], ["Siegel", "Charles", ""], ["Warfel", "Thomas", ""], ["Amatya", "Vinay", ""]]}, {"id": "1803.05935", "submitter": "Zhangxin Zhou", "authors": "Zhangxin Zhou, Chen Yuan, Ziyan Yao, Jiangpeng Dai, Guangyi Liu,\n  Renchang Dai, Zhiwei Wang, Garng M. Huang", "title": "CIM/E Oriented Graph Database Model Architecture and Parallel Network\n  Topology Processing", "comments": "To be published (Accepted) in: Proceedings of the Power and Energy\n  Society General Meeting (PESGM), Portland, OR, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CIM/E is an easy and efficient electric power model exchange standard between\ndifferent Energy Management System vendors. With the rapid growth of data size\nand system complexity, the traditional relational database is not the best\noption to store and process the data. In contrast, the graph database and graph\ncomputation show their potential advantages to handle the power system data and\nperform real-time data analytics and computation. The graph concept fits power\ngrid data naturally because of the fundamental structure similarity. Vertex and\nedge in the graph database can act as both a parallel storage unit and a\ncomputation unit. In this paper, the CIM/E data is modeled into the graph\ndatabase. Based on this model, the parallel network topology processing\nalgorithm is established and conducted by applying graph computation. The\nmodeling and parallel network topology processing have been demonstrated in the\nmodified IEEE test cases and practical Sichuan power network. The processing\nefficiency is greatly improved using the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 18:28:15 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Zhou", "Zhangxin", ""], ["Yuan", "Chen", ""], ["Yao", "Ziyan", ""], ["Dai", "Jiangpeng", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""], ["Huang", "Garng M.", ""]]}, {"id": "1803.06089", "submitter": "Florin Rusu", "authors": "Weijie Zhao, Florin Rusu, Bin Dong, Kesheng Wu, Anna Y. Q. Ho, and\n  Peter Nugent", "title": "Distributed Caching for Complex Querying of Raw Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As applications continue to generate multi-dimensional data at exponentially\nincreasing rates, fast analytics to extract meaningful results is becoming\nextremely important. The database community has developed array databases that\nalleviate this problem through a series of techniques. In-situ mechanisms\nprovide direct access to raw data in the original format---without loading and\npartitioning. Parallel processing scales to the largest datasets. In-memory\ncaching reduces latency when the same data are accessed across a workload of\nqueries. However, we are not aware of any work on distributed caching of\nmulti-dimensional raw arrays. In this paper, we introduce a distributed\nframework for cost-based caching of multi-dimensional arrays in native format.\nGiven a set of files that contain portions of an array and an online query\nworkload, the framework computes an effective caching plan in two stages.\nFirst, the plan identifies the cells to be cached locally from each of the\ninput files by continuously refining an evolving R-tree index. In the second\nstage, an optimal assignment of cells to nodes that collocates dependent cells\nin order to minimize the overall data transfer is determined. We design cache\neviction and placement heuristic algorithms that consider the historical query\nworkload. A thorough experimental evaluation over two real datasets in three\nfile formats confirms the superiority -- by as much as two orders of magnitude\n-- of the proposed framework over existing techniques in terms of cache\noverhead and workload execution time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 06:33:24 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Zhao", "Weijie", ""], ["Rusu", "Florin", ""], ["Dong", "Bin", ""], ["Wu", "Kesheng", ""], ["Ho", "Anna Y. Q.", ""], ["Nugent", "Peter", ""]]}, {"id": "1803.06178", "submitter": "Krzysztof Rzadca", "authors": "Milosz Pacholczyk, Krzysztof Rzadca", "title": "Fair non-monetary scheduling in federated clouds", "comments": "Accepted to CrossCloud'18: 5th Workshop on CrossCloud Infrastructures\n  & Platforms", "journal-ref": null, "doi": "10.1145/3195870.3195873", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a hybrid cloud, individual cloud service providers (CSPs) often have\nincentive to use each other's resources to off-load peak loads or place load\ncloser to the end user. However, CSPs have to keep track of contributions and\ngains in order to disincentivize long-term free-riding. We show CloudShare, a\ndistributed version of a load balancing algorithm DirectCloud based on the\nShapley value---a powerful fairness concept from game theory. CloudShare\ncoordinates CSPs by a ZooKeeper-based coordination layer; each CSP runs a\nbroker that interacts with local resources (such as Kubernetes-managed\nclusters). We quantitatively evaluate our implementation by simulation. The\nresults confirm that CloudShare generates on the average more fair schedules\nthan the popular FairShare algorithm. We believe our results show an viable\nalternative to monetary methods based on, e.g., spot markets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 11:45:27 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Pacholczyk", "Milosz", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "1803.06333", "submitter": "Celestine D\\\"unner", "authors": "Celestine D\\\"unner, Thomas Parnell, Dimitrios Sarigiannis, Nikolas\n  Ioannou, Andreea Anghel, Gummadi Ravi, Madhusudanan Kandasamy, Haralampos\n  Pozidis", "title": "Snap ML: A Hierarchical Framework for Machine Learning", "comments": "in Proceedings of the Thirty-Second Conference on Neural Information\n  Processing Systems (NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new software framework for fast training of generalized linear\nmodels. The framework, named Snap Machine Learning (Snap ML), combines recent\nadvances in machine learning systems and algorithms in a nested manner to\nreflect the hierarchical architecture of modern computing systems. We prove\ntheoretically that such a hierarchical system can accelerate training in\ndistributed environments where intra-node communication is cheaper than\ninter-node communication. Additionally, we provide a review of the\nimplementation of Snap ML in terms of GPU acceleration, pipelining,\ncommunication patterns and software architecture, highlighting aspects that\nwere critical for achieving high performance. We evaluate the performance of\nSnap ML in both single-node and multi-node environments, quantifying the\nbenefit of the hierarchical scheme and the data streaming functionality, and\ncomparing with other widely-used machine learning software frameworks. Finally,\nwe present a logistic regression benchmark on the Criteo Terabyte Click Logs\ndataset and show that Snap ML achieves the same test loss an order of magnitude\nfaster than any of the previously reported results, including those obtained\nusing TensorFlow and scikit-learn.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:37:12 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 11:30:36 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 17:17:55 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["D\u00fcnner", "Celestine", ""], ["Parnell", "Thomas", ""], ["Sarigiannis", "Dimitrios", ""], ["Ioannou", "Nikolas", ""], ["Anghel", "Andreea", ""], ["Ravi", "Gummadi", ""], ["Kandasamy", "Madhusudanan", ""], ["Pozidis", "Haralampos", ""]]}, {"id": "1803.06341", "submitter": "Jingjing Wang", "authors": "Diego Didona, Rachid Guerraoui, Jingjing Wang, Willy Zwaenepoel", "title": "Distributed Transactions: Dissecting the Nightmare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed storage systems are transactional and a lot of work has been\ndevoted to optimizing their performance, especially the performance of\nread-only transactions that are considered the most frequent in practice. Yet,\nthe results obtained so far are rather disappointing, and some of the design\ndecisions seem contrived. This paper contributes to explaining this state of\naffairs by proving intrinsic limitations of transactional storage systems, even\nthose that need not ensure strong consistency but only causality.\n  We first consider general storage systems where some transactions are\nread-only and some also involve write operations. We show that even read-only\ntransactions cannot be \"fast\": their operations cannot be executed within one\nround-trip message exchange between a client seeking an object and the server\nstoring it. We then consider systems (as sometimes implemented today) where all\ntransactions are read-only, i.e., updates are performed as individual\noperations outside transactions. In this case, read-only transactions can\nindeed be \"fast\", but we prove that they need to be \"visible\". They induce\ninherent updates on the servers, which in turn impact their overall\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:55:46 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Didona", "Diego", ""], ["Guerraoui", "Rachid", ""], ["Wang", "Jingjing", ""], ["Zwaenepoel", "Willy", ""]]}, {"id": "1803.06354", "submitter": "Jimmy Lin", "authors": "Youngbin Kim and Jimmy Lin", "title": "Serverless Data Analytics with Flint", "comments": "Published in the Proceedings of the 2018 IEEE 11th International\n  Conference on Cloud Computing (CLOUD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless architectures organized around loosely-coupled function\ninvocations represent an emerging design for many applications. Recent work\nmostly focuses on user-facing products and event-driven processing pipelines.\nIn this paper, we explore a completely different part of the application space\nand examine the feasibility of analytical processing on big data using a\nserverless architecture. We present Flint, a prototype Spark execution engine\nthat takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model.\nWith Flint, a developer uses PySpark exactly as before, but without needing an\nactual Spark cluster. We describe the design, implementation, and performance\nof Flint, along with the challenges associated with serverless analytics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 18:02:27 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 00:51:26 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Kim", "Youngbin", ""], ["Lin", "Jimmy", ""]]}, {"id": "1803.06443", "submitter": "Hanlin Tang", "authors": "Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, Ji Liu", "title": "Communication Compression for Decentralized Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing distributed learning systems is an art of balancing between\ncomputation and communication. There have been two lines of research that try\nto deal with slower networks: {\\em communication compression} for low bandwidth\nnetworks, and {\\em decentralization} for high latency networks. In this paper,\nWe explore a natural question: {\\em can the combination of both techniques lead\nto a system that is robust to both bandwidth and latency?}\n  Although the system implication of such combination is trivial, the\nunderlying theoretical principle and algorithm design is challenging: unlike\ncentralized algorithms, simply compressing exchanged information, even in an\nunbiased stochastic way, within the decentralized network would accumulate the\nerror and fail to converge. In this paper, we develop a framework of\ncompressed, decentralized training and propose two different strategies, which\nwe call {\\em extrapolation compression} and {\\em difference compression}. We\nanalyze both algorithms and prove both converge at the rate of $O(1/\\sqrt{nT})$\nwhere $n$ is the number of workers and $T$ is the number of iterations,\nmatching the convergence rate for full precision, centralized training. We\nvalidate our algorithms and find that our proposed algorithm outperforms the\nbest of merely decentralized and merely quantized algorithm significantly for\nnetworks with {\\em both} high latency and low bandwidth.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 01:51:09 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 00:15:49 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 23:10:01 GMT"}, {"version": "v4", "created": "Mon, 31 Dec 2018 21:20:01 GMT"}, {"version": "v5", "created": "Thu, 31 Jan 2019 20:20:32 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Tang", "Hanlin", ""], ["Gan", "Shaoduo", ""], ["Zhang", "Ce", ""], ["Zhang", "Tong", ""], ["Liu", "Ji", ""]]}, {"id": "1803.06561", "submitter": "Chen Yu", "authors": "Chen Yu, Bojan Karlas, Jie Zhong, Ce Zhang, Ji Liu", "title": "AutoML from Service Provider's Perspective: Multi-device, Multi-tenant\n  Model Selection with GP-EI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AutoML has become a popular service that is provided by most leading cloud\nservice providers today. In this paper, we focus on the AutoML problem from the\n\\emph{service provider's perspective}, motivated by the following practical\nconsideration: When an AutoML service needs to serve {\\em multiple users} with\n{\\em multiple devices} at the same time, how can we allocate these devices to\nusers in an efficient way? We focus on GP-EI, one of the most popular\nalgorithms for automatic model selection and hyperparameter tuning, used by\nsystems such as Google Vizer. The technical contribution of this paper is the\nfirst multi-device, multi-tenant algorithm for GP-EI that is aware of\n\\emph{multiple} computation devices and multiple users sharing the same set of\ncomputation devices. Theoretically, given $N$ users and $M$ devices, we obtain\na regret bound of $O((\\text{\\bf {MIU}}(T,K) + M)\\frac{N^2}{M})$, where\n$\\text{\\bf {MIU}}(T,K)$ refers to the maximal incremental uncertainty up to\ntime $T$ for the covariance matrix $K$. Empirically, we evaluate our algorithm\non two applications of automatic model selection, and show that our algorithm\nsignificantly outperforms the strategy of serving users independently.\nMoreover, when multiple computation devices are available, we achieve\nnear-linear speedup when the number of users is much larger than the number of\ndevices.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 19:56:18 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 01:02:26 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2018 02:59:46 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Yu", "Chen", ""], ["Karlas", "Bojan", ""], ["Zhong", "Jie", ""], ["Zhang", "Ce", ""], ["Liu", "Ji", ""]]}, {"id": "1803.06784", "submitter": "Fausto Milletari", "authors": "Fausto Milletari, Johann Frei, Seyed-Ahmad Ahmadi", "title": "TOMAAT: volumetric medical image analysis as a cloud service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been recently applied to a multitude of computer vision and\nmedical image analysis problems. Although recent research efforts have improved\nthe state of the art, most of the methods cannot be easily accessed, compared\nor used by either researchers or the general public. Researchers often publish\ntheir code and trained models on the internet, but this does not always enable\nthese approaches to be easily used or integrated in stand-alone applications\nand existing workflows. In this paper we propose a framework which allows easy\ndeployment and access of deep learning methods for segmentation through a\ncloud-based architecture. Our approach comprises three parts: a server, which\nwraps trained deep learning models and their pre- and post-processing data\npipelines and makes them available on the cloud; a client which interfaces with\nthe server to obtain predictions on user data; a service registry that informs\nclients about available prediction endpoints that are available in the cloud.\nThese three parts constitute the open-source TOMAAT framework.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 02:21:36 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 09:19:03 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Milletari", "Fausto", ""], ["Frei", "Johann", ""], ["Ahmadi", "Seyed-Ahmad", ""]]}, {"id": "1803.06845", "submitter": "Richard McClatchey", "authors": "Syeda ZarAfshan Gohera, Peter Bloodsworth, Raihan Ur Rasool, Richard\n  McClatchey", "title": "Cloud Provider Capacity Augmentation Through Automated Resource\n  Bartering", "comments": "26 pages, 15 figures, 6 tables", "journal-ref": "Future Generation Computer Systems Vol 81 pp 203-218 April 2018", "doi": "10.1016/j.future.2017.09.080", "report-no": null, "categories": "cs.SE cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing interest in Cloud Computing places a heavy workload on cloud\nproviders which is becoming increasingly difficult for them to manage with\ntheir primary datacenter infrastructures. Resource limitations can make\nproviders vulnerable to significant reputational damage and it often forces\ncustomers to select services from the larger, more established companies,\nsometimes at a higher price. Funding limitations, however, commonly prevent\nemerging and even established providers from making continual investment in\nhardware speculatively assuming a certain level of growth in demand. As an\nalternative, they may strive to use the current inter-cloud resource sharing\nplatforms which mainly rely on monetary payments and thus putting pressure on\nalready stretched cash flows. To address such issues, we have designed and\nimplemented a new multi-agent based Cloud Resource Bartering System (CRBS) that\nfosters the management and bartering of pooled resources without requiring\ncostly financial transactions between providers. Agents in CRBS not only\nstrengthen the trading relationship among providers but also enable them to\nhandle surges in demand with their primary setup. Unlike existing systems, CRBS\nassigns resources by considering resource urgency which comparatively improves\ncustomers satisfaction and the resource utilization rate by more than 50%.The\nevaluation results provide evidence that our system assists providers to timely\nacquire the additional resources and to maintain sustainable service delivery.\nWe conclude that the existence of such a system is economically beneficial for\ncloud providers and enables them to adapt to fluctuating workloads.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 09:39:06 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Gohera", "Syeda ZarAfshan", ""], ["Bloodsworth", "Peter", ""], ["Rasool", "Raihan Ur", ""], ["McClatchey", "Richard", ""]]}, {"id": "1803.06867", "submitter": "Richard McClatchey", "authors": "Khawar Hasham, Kamran Munir, Richard McClatchey", "title": "Cloud Infrastructure Provenance Collection and Management to Reproduce\n  Scientific Workflow Execution", "comments": "59 pages, 21 figures, 4 algorithms, 6 tables, Future Generation\n  Computer Systems. March 2018", "journal-ref": null, "doi": "10.1016/j.future.2017.07.015", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Cloud computing provides a new computing paradigm for\nscientific workflow execution. It provides dynamic, on-demand and scalable\nresources that enable the processing of complex workflow-based experiments.\nWith the ever growing size of the experimental data and increasingly complex\nprocessing workflows, the need for reproducibility has also become essential.\nProvenance has been thought of a mechanism to verify a workflow and to provide\nworkflow reproducibility. One of the obstacles in reproducing an experiment\nexecution is the lack of information about the execution infrastructure in the\ncollected provenance. This information becomes critical in the context of Cloud\nin which resources are provisioned on-demand and by specifying resource\nconfigurations. Therefore, a mechanism is required that enables capturing of\ninfrastructure information along with the provenance of workflows executing on\nthe Cloud to facilitate the re-creation of execution environment on the Cloud.\nThis paper presents a framework, ReCAP, along with the proposed mapping\napproaches that aid in capturing the Cloud-aware provenance information and\nhelp in re-provisioning the execution resource on the Cloud with similar\nconfigurations. Experimental evaluation has shown the impact of different\nresource configurations on the workflow execution performance, therefore\njustifies the need for collecting such provenance information in the context of\nCloud. The evaluation has also demonstrated that the proposed mapping\napproaches can capture Cloud information in various Cloud usage scenarios\nwithout causing performance overhead and can also enable the re-provisioning of\nresources on Cloud. Experiments were conducted using workflows from different\nscientific domains such as astronomy and neuroscience to demonstrate the\napplicability of this research for different workflows.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 10:52:43 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hasham", "Khawar", ""], ["Munir", "Kamran", ""], ["McClatchey", "Richard", ""]]}, {"id": "1803.06924", "submitter": "Gabor Kecskemeti", "authors": "Gabor Kecskemeti, Zsolt Nemeth, Attila Kertesz, Rajiv Ranjan", "title": "Cloud Workload Prediction based on Workflow Execution Time Discrepancies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrastructure as a service clouds hide the complexity of maintaining the\nphysical infrastructure with a slight disadvantage: they also hide their\ninternal working details. Should users need knowledge about these details e.g.,\nto increase the reliability or performance of their applications, they would\nneed solutions to detect behavioural changes in the underlying system. Existing\nruntime solutions for such purposes offer limited capabilities as they are\nmostly restricted to revealing weekly or yearly behavioural periodicity in the\ninfrastructure. This article proposes a technique for predicting generic\nbackground workload by means of simulations that are capable of providing\nadditional knowledge of the underlying private cloud systems in order to\nsupport activities like cloud orchestration or workflow enactment. Our\ntechnique uses long-running scientific workflows and their behaviour\ndiscrepancies and tries to replicate these in a simulated cloud with known\n(trace-based) workloads. We argue that the better we can mimic the current\ndiscrepancies the better we can tell expected workloads in the near future on\nthe real life cloud. We evaluated the proposed prediction approach with a\nbiochemical application on both real and simulated cloud infrastructures. The\nproposed algorithm has shown to produce significantly (~20%) better workload\npredictions for the future of simulated clouds than random workload selection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 13:55:33 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Kecskemeti", "Gabor", ""], ["Nemeth", "Zsolt", ""], ["Kertesz", "Attila", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1803.06973", "submitter": "Lorenzo Posani", "authors": "Lorenzo Posani, Alessio Paccoia, Marco Moschettini", "title": "The carbon footprint of distributed cloud storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ICT (Information Communication Technologies) ecosystem is estimated to be\nresponsible, as of today, for 10% of the total worldwide energy demand -\nequivalent to the combined energy production of Germany and Japan. Cloud\nstorage, mainly operated through large and densely-packed data centers,\nconstitutes a non-negligible part of it. However, since the cloud is a\nfast-inflating market and the energy-efficiency of data centers is mostly an\ninsensitive issue for the collectivity, its carbon footprint shows no signs of\nslowing down. In this paper, we analyze a novel paradigm for cloud storage\n(implemented by Cubbit, http://cubbit.io), in which data are stored and\ndistributed over a network of p2p-interacting ARM-based single-board devices.\nWe compare Cubbit's distributed cloud to the traditional centralized solution\nin terms of environmental footprint and energy efficiency. We demonstrate that,\ncompared to the centralized cloud, the distributed architecture of Cubbit has a\ncarbon footprint reduced of a 77% factor for data storage and of a 50% factor\nfor data transfers. These results provide an example of how a radical paradigm\nshift in a large-reach technology can benefit both the final consumer as well\nas our society as a whole.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 15:02:21 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 17:21:41 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 13:44:39 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Posani", "Lorenzo", ""], ["Paccoia", "Alessio", ""], ["Moschettini", "Marco", ""]]}, {"id": "1803.07068", "submitter": "Hanlin Tang", "authors": "Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, Ji Liu", "title": "D$^2$: Decentralized Training over Decentralized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While training a machine learning model using multiple workers, each of which\ncollects data from their own data sources, it would be most useful when the\ndata collected from different workers can be {\\em unique} and {\\em different}.\nIronically, recent analysis of decentralized parallel stochastic gradient\ndescent (D-PSGD) relies on the assumption that the data hosted on different\nworkers are {\\em not too different}. In this paper, we ask the question: {\\em\nCan we design a decentralized parallel stochastic gradient descent algorithm\nthat is less sensitive to the data variance across workers?} In this paper, we\npresent D$^2$, a novel decentralized parallel stochastic gradient descent\nalgorithm designed for large data variance \\xr{among workers} (imprecisely,\n\"decentralized\" data). The core of D$^2$ is a variance blackuction extension of\nthe standard D-PSGD algorithm, which improves the convergence rate from\n$O\\left({\\sigma \\over \\sqrt{nT}} + {(n\\zeta^2)^{\\frac{1}{3}} \\over\nT^{2/3}}\\right)$ to $O\\left({\\sigma \\over \\sqrt{nT}}\\right)$ where $\\zeta^{2}$\ndenotes the variance among data on different workers. As a result, D$^2$ is\nrobust to data variance among workers. We empirically evaluated D$^2$ on image\nclassification tasks where each worker has access to only the data of a limited\nset of labels, and find that D$^2$ significantly outperforms D-PSGD.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 17:59:11 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 00:13:16 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Tang", "Hanlin", ""], ["Lian", "Xiangru", ""], ["Yan", "Ming", ""], ["Zhang", "Ce", ""], ["Liu", "Ji", ""]]}, {"id": "1803.07588", "submitter": "Shi Pu", "authors": "Shi Pu, Wei Shi, Jinming Xu, and Angelia Nedi\\'c", "title": "A Push-Pull Gradient Method for Distributed Optimization in Networks", "comments": "Accepted in CDC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on solving a distributed convex optimization problem\nin a network, where each agent has its own convex cost function and the goal is\nto minimize the sum of the agents' cost functions while obeying the network\nconnectivity structure. In order to minimize the sum of the cost functions, we\nconsider a new distributed gradient-based method where each node maintains two\nestimates, namely, an estimate of the optimal decision variable and an estimate\nof the gradient for the average of the agents' objective functions. From the\nviewpoint of an agent, the information about the decision variable is pushed to\nthe neighbors, while the information about the gradients is pulled from the\nneighbors (hence giving the name \"push-pull gradient method\"). The method\nunifies the algorithms with different types of distributed architecture,\nincluding decentralized (peer-to-peer), centralized (master-slave), and\nsemi-centralized (leader-follower) architecture. We show that the algorithm\nconverges linearly for strongly convex and smooth objective functions over a\ndirected static network. In our numerical test, the algorithm performs well\neven for time-varying directed networks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 18:25:14 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 23:51:46 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 16:13:55 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Pu", "Shi", ""], ["Shi", "Wei", ""], ["Xu", "Jinming", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1803.07680", "submitter": "Robail Yasrab Dr.", "authors": "Robail Yasrab", "title": "PaaS Cloud: The Business Perspective", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation of PaaS technology accomplishes the true promise of\nobject-oriented and 4GLs development with less effort. Now PaaS is becoming one\nof the core technical services for application development organizations. PaaS\noffers a resourceful and agile approach to develop, operate and deploy\napplications in a cost-effective manner. It is now turning out to be one of the\npreferred choices throughout the world, especially for globally distributed\ndevelopment environment. However it still lacks the scale of popularity and\nacceptance which Software-as-a-Service (SaaS) and Infrastructure-as-a-Service\n(IaaS) have attained. PaaS offers a promising future with novel technology\narchitecture and evolutionary development approach. In this article, we\nidentify the strengths, weaknesses, opportunities and threats for the PaaS\nindustry. We then identify the various issues that will affect the different\nstakeholders of PaaS industry. This research will outline a set of\nrecommendations for the PaaS practitioners to better manage this technology.\nFor PaaS technology researchers, we also outline the number of research areas\nthat need attention in coming future. Finally, we also included an online\nsurvey to outline PaaS technology market leaders. This will facilitate PaaS\ntechnology practitioners to have a more deep insight into market trends and\ntechnologies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 22:37:09 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Yasrab", "Robail", ""]]}, {"id": "1803.07722", "submitter": "Awais Khan", "authors": "Awais Khan, Chang-Gyu Lee, Prince Hamandawana, Sungyong Park, Youngjae\n  Kim", "title": "A Robust Fault-Tolerant and Scalable Cluster-wide Deduplication for\n  Shared-Nothing Storage Systems", "comments": "6 Pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deduplication has been largely employed in distributed storage systems to\nimprove space efficiency. Traditional deduplication research ignores the design\nspecifications of shared-nothing distributed storage systems such as no central\nmetadata bottleneck, scalability, and storage rebalancing. Further,\ndeduplication introduces transactional changes, which are prone to errors in\nthe event of a system failure, resulting in inconsistencies in data and\ndeduplication metadata. In this paper, we propose a robust, fault-tolerant and\nscalable cluster-wide deduplication that can eliminate duplicate copies across\nthe cluster. We design a distributed deduplication metadata shard which\nguarantees performance scalability while preserving the design constraints of\nshared- nothing storage systems. The placement of chunks and deduplication\nmetadata is made cluster-wide based on the content fingerprint of chunks. To\nensure transactional consistency and garbage identification, we employ a\nflag-based asynchronous consistency mechanism. We implement the proposed\ndeduplication on Ceph. The evaluation shows high disk-space savings with\nminimal performance degradation as well as high robustness in the event of\nsudden server failure.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 02:45:35 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Khan", "Awais", ""], ["Lee", "Chang-Gyu", ""], ["Hamandawana", "Prince", ""], ["Park", "Sungyong", ""], ["Kim", "Youngjae", ""]]}, {"id": "1803.07741", "submitter": "Shi Pu", "authors": "Shi Pu and Angelia Nedi\\'c", "title": "A Distributed Stochastic Gradient Tracking Method", "comments": "Accepted in CDC 2018. Extended (journal) version can be found at\n  arXiv:1805.11454", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of distributed multi-agent optimization\nover a network, where each agent possesses a local cost function that is smooth\nand strongly convex. The global objective is to find a common solution that\nminimizes the average of all cost functions. Assuming agents only have access\nto unbiased estimates of the gradients of their local cost functions, we\nconsider a distributed stochastic gradient tracking method. We show that, in\nexpectation, the iterates generated by each agent are attracted to a\nneighborhood of the optimal solution, where they accumulate exponentially fast\n(under a constant step size choice). More importantly, the limiting (expected)\nerror bounds on the distance of the iterates from the optimal solution decrease\nwith the network size, which is a comparable performance to a centralized\nstochastic gradient algorithm. Numerical examples further demonstrate the\neffectiveness of the method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 04:05:25 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 23:02:57 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 16:27:33 GMT"}, {"version": "v4", "created": "Thu, 1 Aug 2019 16:20:56 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Pu", "Shi", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1803.07877", "submitter": "Percival Lucena", "authors": "Percival Lucena, Alecio P. D. Binotto, Fernanda da Silva Momo, Henry\n  Kim", "title": "A Case Study for Grain Quality Assurance Tracking based on a Blockchain\n  Business Network", "comments": null, "journal-ref": "Proceedings of the Symposium on Foundations and Applications of\n  Blockchain (FAB 18) March 9, 2018, Los Angeles, California, USA", "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the key processes in Agriculture is quality measurement throughout the\ntransportation of grains along its complex supply chain. This procedure is\nsuitable for failures, such as delays to final destinations, poor monitoring,\nand frauds. To address the grain quality measurement challenge through the\ntransportation chain, novel technologies, such as Distributed Ledger and\nBlockchain, can bring more efficiency and resilience to the process.\nParticularly, Blockchain is a new type of distributed database in which\ntransactions are securely appended using cryptography and hashed pointers.\nThose transactions can be generated and ruled by special network-embedded\nsoftware -- known as smart contracts -- that may be public to all nodes of the\nnetwork or may be private to a specific set of peer nodes. This paper analyses\nthe implementation of Blockchain technology targeting grain quality assurance\ntracking in a real scenario. Preliminary results support a potential demand for\na Blockchain-based certification that would lead to an added valuation of\naround 15% for GM-free soy in the scope of a Grain Exporter Business Network in\nBrazil.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 12:19:52 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Lucena", "Percival", ""], ["Binotto", "Alecio P. D.", ""], ["Momo", "Fernanda da Silva", ""], ["Kim", "Henry", ""]]}, {"id": "1803.08228", "submitter": "Awais Khan", "authors": "Awais Khan, Taeuk Kim, Hyunki Byun, Youngjae Kim, Sungyong Park, Hyogi\n  Sim", "title": "SCISPACE: A Scientific Collaboration Workspace for File Systems in\n  Geo-Distributed HPC Data Centers", "comments": "8 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future terabit networks are committed to dramatically improving big data\nmotion between geographically dispersed HPC data centers.The scientific\ncommunity takes advantage of the terabit networks such as DOE's ESnet and\naccelerates the trend to build a small world of collaboration between\ngeospatial HPC data centers. It improves information and resource sharing for\njoint simulation and analysis between the HPC data centers. In this paper, we\npropose to build SCISPACE (Scientific Collaboration Workspace) for\ncollaborative data centers. It provides a global view of information shared\nfrom multiple geo-distributed HPC data centers under a single workspace.\nSCISPACE supports native data-access to gain high-performance when data read or\nwrite is required in native data center namespace. It is accomplished by\nintegrating a metadata export protocol. To optimize scientific collaborations\nacross HPC data centers, SCISPACE implements search and discovery service. To\nevaluate, we configured two geo-distributed small-scale HPC data centers\nconnected via high-speed Infiniband network, equipped with LustreFS. We show\nthe feasibility of SCISPACE using real scientific datasets and applications.\nThe evaluation results show average 36\\% performance boost when the proposed\nnative-data access is employed in collaborations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 05:18:18 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Khan", "Awais", ""], ["Kim", "Taeuk", ""], ["Byun", "Hyunki", ""], ["Kim", "Youngjae", ""], ["Park", "Sungyong", ""], ["Sim", "Hyogi", ""]]}, {"id": "1803.08426", "submitter": "Erick Lavoie", "authors": "Erick Lavoie, Laurie Hendren, Frederic Desprez, Miguel Correia", "title": "Pando: Personal Volunteer Computing in Browsers", "comments": "14 pages, 12 figures, 2 tables", "journal-ref": null, "doi": "10.1145/3361525.3361539", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The large penetration and continued growth in ownership of personal\nelectronic devices represents a freely available and largely untapped source of\ncomputing power. To leverage those, we present Pando, a new volunteer computing\ntool based on a declarative concurrent programming model and implemented using\nJavaScript, WebRTC, and WebSockets. This tool enables a dynamically varying\nnumber of failure-prone personal devices contributed by volunteers to\nparallelize the application of a function on a stream of values, by using the\ndevices' browsers. We show that Pando can provide throughput improvements\ncompared to a single personal device, on a variety of compute-bound\napplications including animation rendering and image processing. We also show\nthe flexibility of our approach by deploying Pando on personal devices\nconnected over a local network, on Grid5000, a French-wide computing grid in a\nvirtual private network, and seven PlanetLab nodes distributed in a wide area\nnetwork over Europe.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 16:05:10 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 14:42:05 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 12:31:57 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lavoie", "Erick", ""], ["Hendren", "Laurie", ""], ["Desprez", "Frederic", ""], ["Correia", "Miguel", ""]]}, {"id": "1803.08432", "submitter": "Carsten Burstedde", "authors": "Carsten Burstedde", "title": "Parallel tree algorithms for AMR and non-standard data access", "comments": "32 pages, 14 algorithms, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce several parallel algorithms operating on a distributed forest of\nadaptive quadtrees/octrees. They are targeted at large-scale applications\nrelying on data layouts that are more complex than required for standard finite\nelements, such as hp-adaptive Galerkin methods, particle tracking and\nsemi-Lagrangian schemes, and in-situ post-processing and visualization.\nSpecifically, we design algorithms to derive an adapted worker forest based on\nsparse data, to identify owner processes in a top-down search of remote\nobjects, and to allow for variable process counts and per-element data sizes in\npartitioning and parallel file I/O. We demonstrate the algorithms' usability\nand performance in the context of a particle tracking example that we scale to\n21e9 particles and 64Ki MPI processes on the Juqueen supercomputer, and we\ndescribe the construction of a parallel assembly of variably sized spheres in\nspace creating up to 768e9 elements on the Juwels supercomputer.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 16:15:56 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 13:56:28 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 18:07:26 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Burstedde", "Carsten", ""]]}, {"id": "1803.08601", "submitter": "Carl Yang", "authors": "Carl Yang, Aydin Buluc and John D. Owens", "title": "Design Principles for Sparse Matrix Multiplication on the GPU", "comments": "16 pages, 7 figures, International European Conference on Parallel\n  and Distributed Computing (Euro-Par) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement two novel algorithms for sparse-matrix dense-matrix\nmultiplication (SpMM) on the GPU. Our algorithms expect the sparse input in the\npopular compressed-sparse-row (CSR) format and thus do not require expensive\nformat conversion. While previous SpMM work concentrates on thread-level\nparallelism, we additionally focus on latency hiding with instruction-level\nparallelism and load-balancing. We show, both theoretically and experimentally,\nthat the proposed SpMM is a better fit for the GPU than previous approaches. We\nidentify a key memory access pattern that allows efficient access into both\ninput and output matrices that is crucial to getting excellent performance on\nSpMM. By combining these two ingredients---(i) merge-based load-balancing and\n(ii) row-major coalesced memory access---we demonstrate a 4.1x peak speedup and\na 31.7% geomean speedup over state-of-the-art SpMM implementations on\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 22:31:17 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 06:30:45 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Yang", "Carl", ""], ["Buluc", "Aydin", ""], ["Owens", "John D.", ""]]}, {"id": "1803.08605", "submitter": "Minxian Xu", "authors": "Minxian Xu, Adel Nadjaran Toosi, Rajkumar Buyya", "title": "iBrownout: An Integrated Approach for Managing Energy and Brownout in\n  Container-based Clouds", "comments": "14 pages, in press", "journal-ref": "IEEE Transactions on Sustainable Computing, 2018", "doi": "10.1109/TSUSC.2018.2808493", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption of Cloud data centers has been a major concern of many\nresearchers, and one of the reasons for huge energy consumption of Clouds lies\nin the inefficient utilization of computing resources. Besides energy\nconsumption, another challenge of data centers is the unexpected loads, which\nleads to the overloads and performance degradation. Compared with VM\nconsolidation and Dynamic Voltage Frequency Scaling that cannot function well\nwhen the whole data center is overloaded, brownout has shown to be a promising\ntechnique to handle both overloads and energy consumption through dynamically\ndeactivating application optional components, which are also identified as\ncontainers/microservices. In this work, we propose an integrated approach to\nmanage energy consumption and brownout in container-based cloud data centers.\n\\color{black} We also evaluate our proposed scheduling policies with real\ntraces in a prototype system. The results show that our approach reduces about\n40%, 20% and 10% energy than the approach without power-saving techniques,\nbrownout-overbooking approach and auto-scaling approach respectively while\nensuring Quality of Service.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 23:03:34 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Xu", "Minxian", ""], ["Toosi", "Adel Nadjaran", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1803.08609", "submitter": "Mohammad Roohitavaf", "authors": "Mohammad Roohitavaf and Sandeep Kulkarni", "title": "Toward Adaptive Causal Consistency for Replicated Data Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal consistency for key-value stores has two main requirements (1) do not\nmake a version visible if some of its dependencies are invisible as it may\nviolate causal consistency in the future and (2) make a version visible as soon\nas possible so that clients have the most recent information (to the extent\nfeasible). These two requirements conflict with each other. Existing key-value\nstores that provide causal consistency (or detection of causal violation)\nutilize a static approach in the trade-off between these requirements.\nDepending upon the choice, it assists some applications and penalizes some\napplications. We propose an alternative where the system provides a set of\ntracking groups and checking groups. This allows the application to choose the\nsettings that are most suitable for that application. Furthermore, these groups\ncan be dynamically changed based on application requirements.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 23:12:55 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Roohitavaf", "Mohammad", ""], ["Kulkarni", "Sandeep", ""]]}, {"id": "1803.08617", "submitter": "Yihan Sun", "authors": "Naama Ben-David and Guy E. Blelloch and Yihan Sun and Yuanhao Wei", "title": "Multiversion Concurrency with Bounded Delay and Precise Garbage\n  Collection", "comments": null, "journal-ref": null, "doi": "10.1145/3323165.3323185", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in bounding the number of instructions taken\nto process transactions. The main result is a multiversion transactional system\nthat supports constant delay (extra instructions beyond running in isolation)\nfor all read-only transactions, delay equal to the number of processes for\nwriting transactions that are not concurrent with other writers, and\nlock-freedom for concurrent writers. The system supports precise garbage\ncollection in that versions are identified for collection as soon as the last\ntransaction releases them. As far as we know these are first results that bound\ndelays for multiple readers and even a single writer. The approach is\nparticularly useful in situations where read-transactions dominate write\ntransactions, or where write transactions come in as streams or batches and can\nbe processed by a single writer (possibly in parallel).\n  The approach is based on using functional data structures to support multiple\nversions, and an efficient solution to the Version Maintenance (VM) problem for\nacquiring, updating and releasing versions. Our solution to the VM problem is\nprecise, safe and wait-free (PSWF).\n  We experimentally validate our approach by applying it to balanced tree data\nstructures for maintaining ordered maps. We test the transactional system using\nmultiple algorithms for the VM problem, including our PSWF VM algorithm, and\nimplementations with weaker guarantees based on epochs, hazard pointers, and\nread-copy-update. To evaluate the functional data structure for concurrency and\nmulti-versioning, we implement batched updates for functional tree structures\nand compare the performance with state-of-the-art concurrent data structures\nfor balanced trees. The experiments indicate our approach works well in\npractice over a broad set of criteria.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 00:22:27 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 04:34:03 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 22:35:09 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ben-David", "Naama", ""], ["Blelloch", "Guy E.", ""], ["Sun", "Yihan", ""], ["Wei", "Yuanhao", ""]]}, {"id": "1803.08694", "submitter": "Zhiyuan Jiang", "authors": "Zhiyuan Jiang and Bhaskar Krishnamachari and Sheng Zhou and Zhisheng\n  Niu", "title": "SENATE: A Permissionless Byzantine Consensus Protocol in Wireless\n  Networks", "comments": "Submitted to IEEE International Conference on Blockchain 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.CY cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blockchain technology has achieved tremendous success in open\n(permissionless) decentralized consensus by employing proof-of-work (PoW) or\nits variants, whereby unauthorized nodes cannot gain disproportionate impact on\nconsensus beyond their computational power. However, PoW-based systems incur a\nhigh delay and low throughput, making them ineffective in dealing with\nreal-time applications. On the other hand, byzantine fault-tolerant (BFT)\nconsensus algorithms with better delay and throughput performance have been\nemployed in closed (permissioned) settings to avoid Sybil attacks. In this\npaper, we present Sybil-proof wirelEss Network coordinAte based byzanTine\nconsEnsus (SENATE), which is based on the conventional BFT consensus framework\nyet works in open systems of wireless devices where faulty nodes may launch\nSybil attacks. As in a Senate in the legislature where the quota of senators\nper state (district) is a constant irrespective with the population of the\nstate, \"senators\" in SENATE are selected from participating distributed nodes\nbased on their wireless network coordinates (WNC) with a fixed number of nodes\nper district in the WNC space. Elected senators then participate in the\nsubsequent consensus reaching process and broadcast the result. Thereby, SENATE\nis proof against Sybil attacks since pseudonyms of a faulty node are likely to\nbe adjacent in the WNC space and hence fail to be elected.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 08:59:33 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Jiang", "Zhiyuan", ""], ["Krishnamachari", "Bhaskar", ""], ["Zhou", "Sheng", ""], ["Niu", "Zhisheng", ""]]}, {"id": "1803.08833", "submitter": "Elena Pastorelli", "authors": "Elena Pastorelli, Pier Stanislao Paolucci, Francesco Simula, Andrea\n  Biagioni, Fabrizio Capuani, Paolo Cretaro, Giulia De Bonis, Francesca Lo\n  Cicero, Alessandro Lonardo, Michele Martinelli, Luca Pontisso, Piero Vicini,\n  Roberto Ammendola", "title": "Gaussian and exponential lateral connectivity on distributed spiking\n  neural network simulation", "comments": "9 pages, 9 figures, added reference to final peer reviewed version on\n  conference paper and DOI", "journal-ref": null, "doi": "10.1109/PDP2018.2018.00110", "report-no": null, "categories": "cs.DC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We measured the impact of long-range exponentially decaying intra-areal\nlateral connectivity on the scaling and memory occupation of a distributed\nspiking neural network simulator compared to that of short-range Gaussian\ndecays. While previous studies adopted short-range connectivity, recent\nexperimental neurosciences studies are pointing out the role of longer-range\nintra-areal connectivity with implications on neural simulation platforms.\nTwo-dimensional grids of cortical columns composed by up to 11 M point-like\nspiking neurons with spike frequency adaption were connected by up to 30 G\nsynapses using short- and long-range connectivity models. The MPI processes\ncomposing the distributed simulator were run on up to 1024 hardware cores,\nhosted on a 64 nodes server platform. The hardware platform was a cluster of\nIBM NX360 M5 16-core compute nodes, each one containing two Intel Xeon Haswell\n8-core E5-2630 v3 processors, with a clock of 2.40 G Hz, interconnected through\nan InfiniBand network, equipped with 4x QDR switches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 15:21:42 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 14:54:34 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Pastorelli", "Elena", ""], ["Paolucci", "Pier Stanislao", ""], ["Simula", "Francesco", ""], ["Biagioni", "Andrea", ""], ["Capuani", "Fabrizio", ""], ["Cretaro", "Paolo", ""], ["De Bonis", "Giulia", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Pontisso", "Luca", ""], ["Vicini", "Piero", ""], ["Ammendola", "Roberto", ""]]}, {"id": "1803.08841", "submitter": "Nikola Konstantinov", "authors": "Dan Alistarh, Christopher De Sa, Nikola Konstantinov", "title": "The Convergence of Stochastic Gradient Descent in Asynchronous Shared\n  Memory", "comments": "To be published in PoDC 2018; 18 pages, 1 figure; Changes: added\n  pseudocode for Algorithm 2, some references and corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is a fundamental algorithm in machine\nlearning, representing the optimization backbone for training several classic\nmodels, from regression to neural networks. Given the recent practical focus on\ndistributed machine learning, significant work has been dedicated to the\nconvergence properties of this algorithm under the inconsistent and noisy\nupdates arising from execution in a distributed environment. However,\nsurprisingly, the convergence properties of this classic algorithm in the\nstandard shared-memory model are still not well-understood.\n  In this work, we address this gap, and provide new convergence bounds for\nlock-free concurrent stochastic gradient descent, executing in the classic\nasynchronous shared memory model, against a strong adaptive adversary. Our\nresults give improved upper and lower bounds on the \"price of asynchrony\" when\nexecuting the fundamental SGD algorithm in a concurrent setting. They show that\nthis classic optimization tool can converge faster and with a wider range of\nparameters than previously known under asynchronous iterations. At the same\ntime, we exhibit a fundamental trade-off between the maximum delay in the\nsystem and the rate at which SGD can converge, which governs the set of\nparameters under which this algorithm can still work efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 15:32:42 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 16:14:39 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Alistarh", "Dan", ""], ["De Sa", "Christopher", ""], ["Konstantinov", "Nikola", ""]]}, {"id": "1803.08917", "submitter": "Zeyuan Allen-Zhu", "authors": "Dan Alistarh, Zeyuan Allen-Zhu, Jerry Li", "title": "Byzantine Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of distributed stochastic optimization in an\nadversarial setting where, out of the $m$ machines which allegedly compute\nstochastic gradients every iteration, an $\\alpha$-fraction are Byzantine, and\ncan behave arbitrarily and adversarially. Our main result is a variant of\nstochastic gradient descent (SGD) which finds $\\varepsilon$-approximate\nminimizers of convex functions in $T = \\tilde{O}\\big( \\frac{1}{\\varepsilon^2 m}\n+ \\frac{\\alpha^2}{\\varepsilon^2} \\big)$ iterations. In contrast, traditional\nmini-batch SGD needs $T = O\\big( \\frac{1}{\\varepsilon^2 m} \\big)$ iterations,\nbut cannot tolerate Byzantine failures. Further, we provide a lower bound\nshowing that, up to logarithmic factors, our algorithm is\ninformation-theoretically optimal both in terms of sampling complexity and time\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 17:58:54 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Alistarh", "Dan", ""], ["Allen-Zhu", "Zeyuan", ""], ["Li", "Jerry", ""]]}, {"id": "1803.09004", "submitter": "Xiaofan Zhang", "authors": "Chuanhao Zhuge, Xinheng Liu, Xiaofan Zhang, Sudeep Gummadi, Jinjun\n  Xiong, Deming Chen", "title": "Face Recognition with Hybrid Efficient Convolution Algorithms on FPGAs", "comments": "This paper is accepted in GLSVLSI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks have become a Swiss knife in solving\ncritical artificial intelligence tasks. However, deploying deep CNN models for\nlatency-critical tasks remains to be challenging because of the complex nature\nof CNNs. Recently, FPGA has become a favorable device to accelerate deep CNNs\nthanks to its high parallel processing capability and energy efficiency. In\nthis work, we explore different fast convolution algorithms including Winograd\nand Fast Fourier Transform (FFT), and find an optimal strategy to apply them\ntogether on different types of convolutions. We also propose an optimization\nscheme to exploit parallelism on novel CNN architectures such as Inception\nmodules in GoogLeNet. We implement a configurable IP-based face recognition\nacceleration system based on FaceNet using High-Level Synthesis. Our\nimplementation on a Xilinx Ultrascale device achieves 3.75x latency speedup\ncompared to a high-end NVIDIA GPU and surpasses previous FPGA results\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 22:42:57 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhuge", "Chuanhao", ""], ["Liu", "Xinheng", ""], ["Zhang", "Xiaofan", ""], ["Gummadi", "Sudeep", ""], ["Xiong", "Jinjun", ""], ["Chen", "Deming", ""]]}, {"id": "1803.09102", "submitter": "Anshu Shukla", "authors": "Nanjangud C. Narendra, Sambit Nayak, Anshu Shukla", "title": "Managing Large-Scale Transient Data in IoT Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive availability of streaming data is driving interest in\ndistributed Fast Data platforms for streaming applications. Such\nlatency-sensitive applications need to respond to dynamism in the input rates\nand task behavior using scale-in and -out on elastic Cloud resources. Platforms\nlike Apache Storm do not provide robust capabilities for responding to such\ndynamism and for rapid task migration across VMs. We propose several dataflow\ncheckpoint and migration approaches that allow a running streaming dataflow to\nmigrate, without any loss of in-flight messages or their internal tasks states,\nwhile reducing the time to recover and stabilize. We implement and evaluate\nthese migration strategies on Apache Storm using micro and application\ndataflows for scaling in and out on up to 2-21 Azure VMs. Our results show that\nwe can migrate dataflows of large sizes within 50 sec, in comparison to Storm's\ndefault approach that takes over 100 sec. We also find that our approaches\nstabilize the application much earlier and there is no failure and\nre-processing of messages.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 12:49:51 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Narendra", "Nanjangud C.", ""], ["Nayak", "Sambit", ""], ["Shukla", "Anshu", ""]]}, {"id": "1803.09342", "submitter": "Gregory Price", "authors": "Gregory Michael Price", "title": "DMTCP Checkpoint/Restart of MPI Programs via Proxies", "comments": "5 pages, technical report, initial exploratory research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MPI accomplishes portable, standardized message-passing between processes by\nexposing a standard API that hides the implementation of the underlying\nmechanism for message passing. Until now, checkpointing an MPI program required\nknowledge of these underlying mechanisms. Through the addition of a proxy, we\ndemonstrate that MPI programs can be checkpointed and restarted regardless of\nthe MPI implementation utilized. Further, proxies may enable MPI programs to be\ncheckpointed on one MPI implementation, and restarted on another.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 21:16:21 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Price", "Gregory Michael", ""]]}, {"id": "1803.09504", "submitter": "Krzysztof Malarz", "authors": "M. Kotwica, P. Gronek, K. Malarz", "title": "Efficient space virtualisation for Hoshen--Kopelman algorithm", "comments": null, "journal-ref": "International Journal of Modern Physics C 30 (2019) 1950055", "doi": "10.1142/S0129183119500554", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the efficient space virtualisation for Hoshen--Kopelman\nalgorithm is presented. We observe minimal parallel overhead during\ncomputations, due to negligible communication costs. The proposed algorithm is\napplied for computation of random-site percolation thresholds for four\ndimensional simple cubic lattice with sites' neighbourhoods containing\nnext-next-nearest neighbours (3NN). The obtained percolation thresholds are\n$p_C(\\text{NN})=0.19680(23)$, $p_C(\\text{2NN})=0.08410(23)$,\n$p_C(\\text{3NN})=0.04540(23)$, $p_C(\\text{2NN+NN})=0.06180(23)$,\n$p_C(\\text{3NN+NN})=0.04000(23)$, $p_C(\\text{3NN+2NN})=0.03310(23)$,\n$p_C(\\text{3NN+2NN+NN})=0.03190(23)$, where 2NN and NN stand for next-nearest\nneighbours and nearest neighbours, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 10:57:59 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:39:06 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Kotwica", "M.", ""], ["Gronek", "P.", ""], ["Malarz", "K.", ""]]}, {"id": "1803.09553", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley and Juho Hirvonen", "title": "Local verification of global proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the cost of local and global proofs on distributed\nverification. In this setting the nodes of a distributed system are provided\nwith a nondeterministic proof for the correctness of the state of the system,\nand the nodes need to verify this proof by looking at only their local\nneighborhood in the system.\n  Previous works have studied the model where each node is given its own,\npossibly unique, part of the proof as input. The cost of a proof is the maximum\nsize of an individual label. We compare this model to a model where each node\nhas access to the same global proof, and the cost is the size of this global\nproof.\n  It is easy to see that a global proof can always include all of the local\nproofs, and every local proof can be a copy of the global proof. We show that\nthere exists properties that exhibit these relative proof sizes, and also\nproperties that are somewhere in between. In addition, we introduce a new lower\nbound technique and use it to prove a tight lower bound on the complexity of\nreversing distributed decision and establish a link between communication\ncomplexity and distributed proof complexity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 12:48:22 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Feuilloley", "Laurent", ""], ["Hirvonen", "Juho", ""]]}, {"id": "1803.09584", "submitter": "Radhika Jagtap", "authors": "Alexandra Ferreron, Radhika Jagtap, Sascha Bischoff, Roxana Rusitoru", "title": "Crossing the Architectural Barrier: Evaluating Representative Regions of\n  Parallel HPC Applications", "comments": "2017 IEEE International Symposium on Performance Analysis of Systems\n  and Software (ISPASS)", "journal-ref": null, "doi": "10.1109/ISPASS.2017.7975275", "report-no": null, "categories": "cs.PF cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exascale computing will get mankind closer to solving important social,\nscientific and engineering problems. Due to high prototyping costs, High\nPerformance Computing (HPC) system architects make use of simulation models for\ndesign space exploration and hardware-software co-design. However, as HPC\nsystems reach exascale proportions, the cost of simulation increases, since\nsimulators themselves are largely single-threaded. Tools for selecting\nrepresentative parts of parallel applications to reduce running costs are\nwidespread, e.g., BarrierPoint achieves this by analysing, in simulation,\nabstract characteristics such as basic blocks and reuse distances. However,\narchitectures new to HPC have a limited set of tools available.\n  In this work, we provide an independent cross-architectural evaluation on\nreal hardware - across Intel and ARM - of the BarrierPoint methodology, when\napplied to parallel HPC proxy applications. We present both cases: when the\nmethodology can be applied and when it cannot. In the former case, results show\nthat we can predict the performance of full application execution by running\nshorter representative sections. In the latter case, we dive into the\nunderlying issues and suggest improvements. We demonstrate a total simulation\ntime reduction of up to 178x, whilst keeping the error below 2.3% for both\ncycles and instructions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 19:40:03 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ferreron", "Alexandra", ""], ["Jagtap", "Radhika", ""], ["Bischoff", "Sascha", ""], ["Rusitoru", "Roxana", ""]]}, {"id": "1803.09737", "submitter": "In\\^es Almeida", "authors": "In\\^es Almeida and Jo\\~ao Xavier", "title": "DJAM: distributed Jacobi asynchronous method for learning personal\n  models", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": "10.1109/LSP.2018.2859596", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing data collected by a network of agents often boils down to solving\nan optimization problem. The distributed nature of these problems calls for\nmethods that are, themselves, distributed. While most collaborative learning\nproblems require agents to reach a common (or consensus) model, there are\nsituations in which the consensus solution may not be optimal. For instance,\nagents may want to reach a compromise between agreeing with their neighbors and\nminimizing a personal loss function. We present DJAM, a Jacobi-like distributed\nalgorithm for learning personalized models. This method is\nimplementation-friendly: it has no hyperparameters that need tuning, it is\nasynchronous, and its updates only require single-neighbor interactions. We\nprove that DJAM converges with probability one to the solution, provided that\nthe personal loss functions are strongly convex and have Lipschitz gradient. We\nthen give evidence that DJAM is on par with state-of-the-art methods: our\nmethod reaches a solution with error similar to the error of a carefully tuned\nADMM in about the same number of single-neighbor interactions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 17:53:56 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 16:54:17 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Almeida", "In\u00eas", ""], ["Xavier", "Jo\u00e3o", ""]]}, {"id": "1803.09876", "submitter": "Jie Yan", "authors": "Jie Yan, Zhang Yang, Aiqing Zhang, Zeyao Mo", "title": "JSweep: A Patch-centric Data-driven Approach for Parallel Sweeps on\n  Large-scale Meshes", "comments": "10 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In mesh-based numerical simulations, sweep is an important computation\npattern. During sweeping a mesh, computations on cells are strictly ordered by\ndata dependencies in given directions. Due to such a serial order,\nparallelizing sweep is challenging, especially for unstructured and deforming\nstructured meshes. Meanwhile, recent high-fidelity multi-physics simulations of\nparticle transport, including nuclear reactor and inertial confinement fusion,\nrequire {\\em sweeps} on large scale meshes with billions of cells and hundreds\nof directions.\n  In this paper, we present JSweep, a parallel data-driven computational\nframework integrated in the JAxMIN infrastructure. The essential of JSweep is a\ngeneral patch-centric data-driven abstraction, coupled with a high performance\nruntime system leveraging hybrid parallelism of MPI+threads and achieving\ndynamic communication on contemporary multi-core clusters. Built on JSweep, we\nimplement a representative data-driven algorithm, Sn transport, featuring\noptimizations of vertex clustering, multi-level priority strategy and\npatch-angle parallelism. Experimental evaluation with two real-world\napplications on structured and unstructured meshes respectively, demonstrates\nthat JSweep can scale to tens of thousands of processor cores with reasonable\nparallel efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 03:24:57 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yan", "Jie", ""], ["Yang", "Zhang", ""], ["Zhang", "Aiqing", ""], ["Mo", "Zeyao", ""]]}, {"id": "1803.09877", "submitter": "Lingjiao Chen", "authors": "Lingjiao Chen and Hongyi Wang and Zachary Charles and Dimitris\n  Papailiopoulos", "title": "DRACO: Byzantine-resilient Distributed Training via Redundant Gradients", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed model training is vulnerable to byzantine system failures and\nadversarial compute nodes, i.e., nodes that use malicious updates to corrupt\nthe global model stored at a parameter server (PS). To guarantee some form of\nrobustness, recent work suggests using variants of the geometric median as an\naggregation rule, in place of gradient averaging. Unfortunately, median-based\nrules can incur a prohibitive computational overhead in large-scale settings,\nand their convergence guarantees often require strong assumptions. In this\nwork, we present DRACO, a scalable framework for robust distributed training\nthat uses ideas from coding theory. In DRACO, each compute node evaluates\nredundant gradients that are used by the parameter server to eliminate the\neffects of adversarial updates. DRACO comes with problem-independent robustness\nguarantees, and the model that it trains is identical to the one trained in the\nadversary-free setup. We provide extensive experiments on real datasets and\ndistributed setups across a variety of large-scale models, where we show that\nDRACO is several times, to orders of magnitude faster than median-based\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 03:34:25 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 05:38:33 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2018 02:10:56 GMT"}, {"version": "v4", "created": "Fri, 22 Jun 2018 02:47:53 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Chen", "Lingjiao", ""], ["Wang", "Hongyi", ""], ["Charles", "Zachary", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "1803.09984", "submitter": "Aur\\'elien Bellet", "authors": "Pierre Dellenbach, Aur\\'elien Bellet, Jan Ramon", "title": "Hiding in the Crowd: A Massively Distributed Algorithm for Private\n  Averaging with Malicious Adversaries", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of personal data collected in our everyday interactions with\nconnected devices offers great opportunities for innovative services fueled by\nmachine learning, as well as raises serious concerns for the privacy of\nindividuals. In this paper, we propose a massively distributed protocol for a\nlarge set of users to privately compute averages over their joint data, which\ncan then be used to learn predictive models. Our protocol can find a solution\nof arbitrary accuracy, does not rely on a third party and preserves the privacy\nof users throughout the execution in both the honest-but-curious and malicious\nadversary models. Specifically, we prove that the information observed by the\nadversary (the set of maliciours users) does not significantly reduce the\nuncertainty in its prediction of private values compared to its prior belief.\nThe level of privacy protection depends on a quantity related to the Laplacian\nmatrix of the network graph and generally improves with the size of the graph.\nFurthermore, we design a verification procedure which offers protection against\nmalicious users joining the service with the goal of manipulating the outcome\nof the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 09:35:29 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Dellenbach", "Pierre", ""], ["Bellet", "Aur\u00e9lien", ""], ["Ramon", "Jan", ""]]}, {"id": "1803.10359", "submitter": "Gesualdo Scutari", "authors": "Ye Tian, Ying Sun, and Gesualdo Scutari", "title": "Achieving Linear Convergence in Distributed Asynchronous Multi-agent\n  Optimization", "comments": "Part of this work has been presented to Allerton 2018; first version\n  posted on arxiv on March 2018; revised Nov. 2018. To appear on IEEE Trans. on\n  Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This papers studies multi-agent (convex and \\emph{nonconvex}) optimization\nover static digraphs. We propose a general distributed \\emph{asynchronous}\nalgorithmic framework whereby i) agents can update their local variables as\nwell as communicate with their neighbors at any time, without any form of\ncoordination; and ii) they can perform their local computations using\n(possibly) delayed, out-of-sync information from the other agents. Delays need\nnot be known to the agent or obey any specific profile, and can also be\ntime-varying (but bounded). The algorithm builds on a tracking mechanism that\nis robust against asynchrony (in the above sense), whose goal is to estimate\nlocally the average of agents' gradients. When applied to strongly convex\nfunctions, we prove that it converges at an R-linear (geometric) rate as long\nas the step-size is {sufficiently small}. A sublinear convergence rate is\nproved, when nonconvex problems and/or diminishing, {\\it uncoordinated}\nstep-sizes are considered. To the best of our knowledge, this is the first\ndistributed algorithm with provable geometric convergence rate in such a\ngeneral asynchronous setting. Preliminary numerical results demonstrate the\nefficacy of the proposed algorithm and validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 00:04:43 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 18:56:49 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 15:08:00 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 13:05:13 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Tian", "Ye", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "1803.10494", "submitter": "Pierre-Louis Roman", "authors": "Davide Frey, Marc X. Makkes, Pierre-Louis Roman, Fran\\c{c}ois\n  Ta\\\"iani, Spyros Voulgaris", "title": "Dietcoin: shortcutting the Bitcoin verification process for your\n  smartphone", "comments": "HAL link: https://hal.inria.fr/hal-01743995v1", "journal-ref": null, "doi": null, "report-no": "RR-9162", "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchains have a storage scalability issue. Their size is not bounded and\nthey grow indefinitely as time passes. As of August 2017, the Bitcoin\nblockchain is about 120 GiB big while it was only 75 GiB in August 2016. To\nbenefit from Bitcoin full security model, a bootstrapping node has to download\nand verify the entirety of the 120 GiB. This poses a challenge for low-resource\ndevices such as smartphones. Thankfully, an alternative exists for such devices\nwhich consists of downloading and verifying just the header of each block. This\npartial block verification enables devices to reduce their bandwidth\nrequirements from 120 GiB to 35 MiB. However, this drastic decrease comes with\na safety cost implied by a partial block verification. In this work, we enable\nlow-resource devices to fully verify subchains of blocks without having to pay\nthe onerous price of a full chain download and verification; a few additional\nMiB of bandwidth suffice. To do so, we propose the design of diet nodes that\ncan securely query full nodes for shards of the UTXO set, which is needed to\nperform full block verification and can otherwise only be built by sequentially\nparsing the chain.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 09:54:51 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Frey", "Davide", ""], ["Makkes", "Marc X.", ""], ["Roman", "Pierre-Louis", ""], ["Ta\u00efani", "Fran\u00e7ois", ""], ["Voulgaris", "Spyros", ""]]}, {"id": "1803.10726", "submitter": "Aravind Acharya", "authors": "Aravind Acharya, Uday Bondhugula, Albert Cohen", "title": "An Approach for Finding Permutations Quickly: Fusion and Dimension\n  matching", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polyhedral compilers can perform complex loop optimizations that improve\nparallelism and cache behaviour of loops in the input program. These\ntransformations result in significant performance gains on modern processors\nwhich have large compute power and deep memory hierarchies. The paper,\n\"Polyhedral Auto-transformation with No Integer Linear Programming\", identifies\nissues that adversely affect scalability of polyhedral transformation\nframeworks; in particular the Pluto algorithm. The construction and solving of\na complex Integer Linear Programming (ILP) problem increases the time taken by\na polyhedral compiler significantly. The paper presents two orthogonal ideas,\nwhich together overcome the scalability issues in the affine scheduling\nproblem. It first relaxes the ILP to a Linear Programming (LP) problem, thereby\nsolving a cheaper algorithm. To overcome the sub-optimalities that arise due to\nthis relaxation, the affine scheduling problem is decomposed into following\nthree components: (1) Fusion and dimension matching, (2) Loop scaling and\nshifting, and (3) Loop skewing. This new auto-transformation framework,\npluto-lp-dfp, significantly improves the time taken by the Pluto algorithm\nwithout sacrificing performance of the generated code. This report first\nprovides proofs for the theoretical claims made in the paper surrounding\nrelaxed LP formulation of the Pluto algorithm. The second part of the report\ndescribes an approach to find good loop fusion (or distribution) and loop\npermutations that enable tileability. This short report serves as the\nsupplementary material for the paper.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 17:03:26 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Acharya", "Aravind", ""], ["Bondhugula", "Uday", ""], ["Cohen", "Albert", ""]]}, {"id": "1803.10836", "submitter": "Bilal Akil", "authors": "Bilal Akil, Ying Zhou, Uwe R\\\"ohm", "title": "Technical Report: On the Usability of Hadoop MapReduce, Apache Spark &\n  Apache Flink for Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": "School of IT, University of Sydney, Tech. Rep. 709", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data processing platforms for cloud computing are important tools\nfor large-scale data analytics. Apache Hadoop MapReduce has become the de facto\nstandard in this space, though its programming interface is relatively\nlow-level, requiring many implementation steps even for simple analysis tasks.\nThis has led to the development of advanced dataflow oriented platforms, most\nprominently Apache Spark and Apache Flink. Those platforms not only aim to\nimprove performance through improved in-memory processing, but in particular\nprovide built-in high-level data processing functionality, such as filtering\nand join operators, which should make data analysis tasks easier to develop\nthan with plain Hadoop MapReduce. But is this indeed the case?\n  This paper compares three prominent distributed data processing platforms:\nApache Hadoop MapReduce; Apache Spark; and Apache Flink, from a usability\nperspective. We report on the design, execution and results of a usability\nstudy with a cohort of masters students, who were learning and working with all\nthree platforms in order to solve different use cases set in a data science\ncontext. Our findings show that Spark and Flink are preferred platforms over\nMapReduce. Among participants, there was no significant difference in perceived\npreference or development time between both Spark and Flink as platforms for\nbatch-oriented big data analysis. This study starts an exploration of the\nfactors that make big data platforms more - or less - effective for users in\ndata science.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 20:13:18 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Akil", "Bilal", ""], ["Zhou", "Ying", ""], ["R\u00f6hm", "Uwe", ""]]}, {"id": "1803.10901", "submitter": "Bikram Karmakar", "authors": "Bikram Karmakar and Indranil Mukhopadhyay", "title": "Statistical Validity and Consistency of Big Data Analytics: A General\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Informatics and technological advancements have triggered generation of huge\nvolume of data with varied complexity in its management and analysis. Big Data\nanalytics is the practice of revealing hidden aspects of such data and making\ninferences from it. Although storage, retrieval and management of Big Data seem\npossible through efficient algorithm and system development, concern about\nstatistical consistency remains to be addressed in view of its specific\ncharacteristics. Since Big Data does not conform to standard analytics, we need\nproper modification of the existing statistical theory and tools. Here we\npropose, with illustrations, a general statistical framework and an algorithmic\nprinciple for Big Data analytics that ensure statistical accuracy of the\nconclusions. The proposed framework has the potential to push forward\nadvancement of Big Data analytics in the right direction. The\npartition-repetition approach proposed here is broad enough to encompass all\npractical data analytic problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 02:15:03 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Karmakar", "Bikram", ""], ["Mukhopadhyay", "Indranil", ""]]}, {"id": "1803.10943", "submitter": "Cai Chen", "authors": "Cai Chen, Manyuan Zhang, Huanzhi Zhang, Zhenyun Huang, Yong Li", "title": "Privacy-preserving Sensory Data Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a large scale of various wireless sensor networks have been\ndeployed for basic scientific works. Massive data loss is so common that there\nis a great demand for data recovery. While data recovery methods fulfil the\nrequirement of accuracy, the potential privacy leakage caused by them concerns\nus a lot. Thus the major challenge of sensory data recovery is the issue of\neffective privacy preservation. Existing algorithms can either accomplish\naccurate data recovery or solve privacy issue, yet no single design is able to\naddress these two problems simultaneously. Therefore in this paper, we propose\na novel approach Privacy-Preserving Compressive Sensing with Multi-Attribute\nAssistance (PPCS-MAA). It applies PPCS scheme to sensory data recovery, which\ncan effectively encrypts sensory data without decreasing accuracy, because it\nmaintains the homomorphic obfuscation property for compressive sensing. In\naddition, multiple environmental attributes from sensory datasets usually have\nstrong correlation so that we design a MultiAttribute Assistance (MAA)\ncomponent to leverage this feature for better recovery accuracy. Combining PPCS\nwith MAA, the novel recovery scheme can provide reliable privacy with high\naccuracy. Firstly, based on two real datasets, IntelLab and GreenOrbs, we\nreveal the inherited low-rank features as the ground truth and find such\nmulti-attribute correlation. Secondly, we develop a PPCS-MAA algorithm to\npreserve privacy and optimize the recovery accuracy. Thirdly, the results of\nreal data-driven simulations show that the algorithm outperforms the existing\nsolutions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 07:25:33 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Chen", "Cai", ""], ["Zhang", "Manyuan", ""], ["Zhang", "Huanzhi", ""], ["Huang", "Zhenyun", ""], ["Li", "Yong", ""]]}, {"id": "1803.11036", "submitter": "Jie Xu", "authors": "Jie Xu", "title": "Regain Sliding super point from distributed edge routers by GPU", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding super point is a special host defined under sliding time window with\nwhich there are huge other hosts contact. It plays important roles in network\nsecurity and management. But how to detect them in real time from nowadays\nhigh-speed network which contains several distributed routers is a hard task.\nDistributed sliding super point detection requires an algorithm that can\nestimate the number of contacting hosts incrementally, scan packets faster than\ntheir flowing speed and reconstruct sliding super point at the end of a time\nperiod. But no existing algorithm satisfies these three requirements\nsimultaneously. To solve this problem, this paper firstly proposed a\ndistributed sliding super point detection algorithm running on GPU. The\nadvantage of this algorithm comes from a novel sliding estimator, which can\nestimate contacting host number incrementally under a sliding window, and a set\nof reversible hash functions, by which sliding super points could be regained\nwithout storing additional data such as IP list. There are two main procedures\nin this algorithm: packets scanning and sliding super points reconstruction.\nBoth could run parallel without any data reading conflict. When deployed on a\nlow cost GPU, this algorithm could deal with traffic with bandwidth as high as\n680 Gb/s. A real world core network traffic is used to evaluate the performance\nof this sliding super point detection algorithm on a cheap GPU, Nvidia GTX950\nwith 4 GB graphic memory. Experiments comparing with other algorithms under\ndiscrete time window show that this algorithm has the highest accuracy. Under\nsliding time widow, this algorithm has the same performance as in discrete time\nwindow, where no other algorithms can work.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 12:52:07 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Xu", "Jie", ""]]}, {"id": "1803.11050", "submitter": "EPTCS", "authors": "Pujie Han, Zhengjun Zhai, Brian Nielsen, Ulrik Nyman", "title": "A Modeling Framework for Schedulability Analysis of Distributed Avionics\n  Systems", "comments": "In Proceedings MARS/VPT 2018, arXiv:1803.08668", "journal-ref": "EPTCS 268, 2018, pp. 150-168", "doi": "10.4204/EPTCS.268.5", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a modeling framework for schedulability analysis of\ndistributed integrated modular avionics (DIMA) systems that consist of\nspatially distributed ARINC-653 modules connected by a unified AFDX network. We\nmodel a DIMA system as a set of stopwatch automata (SWA) in UPPAAL to analyze\nits schedulability by classical model checking (MC) and statistical model\nchecking (SMC). The framework has been designed to enable three types of\nanalysis: global SMC, global MC, and compositional MC. This allows an effective\nmethodology including (1) quick schedulability falsification using global SMC\nanalysis, (2) direct schedulability proofs using global MC analysis in simple\ncases, and (3) strict schedulability proofs using compositional MC analysis for\nlarger state space. The framework is applied to the analysis of a concrete DIMA\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 20:59:53 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Han", "Pujie", ""], ["Zhai", "Zhengjun", ""], ["Nielsen", "Brian", ""], ["Nyman", "Ulrik", ""]]}, {"id": "1803.11211", "submitter": "Theophanis Hadjistasi", "authors": "Chryssis Georgiou, Theophanis Hadjistasi, Nicolas Nicolaou and\n  Alexander A. Schwarzmann", "title": "Unleashing and Speeding Up Readers in Atomic Object Implementations", "comments": "arXif admin note: substantial text overlap with arXiv:1610.08373", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing efficient emulations of atomic read/write objects in asynchronous,\ncrash-prone, message-passing systems is an important problem in distributed\ncomputing. Communication latency is a factor that typically dominates the\nperformance of message-passing systems, consequently the efficiency of\nalgorithms implementing atomic objects is measured in terms of the number of\ncommunication exchanges involved in each read and write operation. The seminal\nresult of Attiya, Bar-Noy, and Dolev established that two pairs of\ncommunication exchanges, or equivalently two round-trip communications, are\nsufficient. Subsequent research examined the possibility of implementations\nthat involve less than four exchanges. The work of Dutta et al. showed that for\nsingle-writer/multiple-reader (SWMR) settings two exchanges are sufficient,\nprovided that the number of readers is severely constrained with respect to the\nnumber of object replicas in the system and the number of replica failures, and\nalso showed that no two exchange implementations of\nmultiple-writer/multiple-reader (MWMR) objects are possible. Later research\nfocused on providing implementations that remove the constraint on the number\nof readers, while having read and write operations that use variable number of\ncommunication exchanges, specifically two, three, or four exchanges.\n  This work presents two advances in the state-of-the-art in this area.\nSpecifically, for SWMR and MWMR systems algorithms are given in which read\noperations take two or three exchanges. This improves on prior works where read\noperations took either (a) three exchanges, or (b) two or four exchanges. The\nnumber of readers in the new algorithms is unconstrained, and write operations\ntake the same number of exchanges as in prior work (two for SWMR and four for\nMWMR settings). The correctness of algorithms is rigorously argued.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 18:18:58 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Georgiou", "Chryssis", ""], ["Hadjistasi", "Theophanis", ""], ["Nicolaou", "Nicolas", ""], ["Schwarzmann", "Alexander A.", ""]]}, {"id": "1803.11389", "submitter": "Jinhwan Park", "authors": "Wonyong Sung, Jinhwan Park", "title": "Single Stream Parallelization of Recurrent Neural Networks for Low Power\n  and Fast Inference", "comments": "Submitted to International Conference on Embedded Computer Systems:\n  Architectures, MOdeling and Simulation (SAMOS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural network algorithms show high performance in many applications,\ntheir efficient inference on mobile and embedded systems are of great\ninterests. When a single stream recurrent neural network (RNN) is executed for\na personal user in embedded systems, it demands a large amount of DRAM accesses\nbecause the network size is usually much bigger than the cache size and the\nweights of an RNN are used only once at each time step. We overcome this\nproblem by parallelizing the algorithm and executing it multiple time steps at\na time. This approach also reduces the power consumption by lowering the number\nof DRAM accesses. QRNN (Quasi Recurrent Neural Networks) and SRU (Simple\nRecurrent Unit) based recurrent neural networks are used for implementation.\nThe experiments for SRU showed about 300% and 930% of speed-up when the numbers\nof multi time steps are 4 and 16, respectively, in an ARM CPU based system.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 09:15:07 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Sung", "Wonyong", ""], ["Park", "Jinhwan", ""]]}, {"id": "1803.11399", "submitter": "Tim Dykes", "authors": "Tim Dykes, Amr Hassan, Claudio Gheller, Darren Croton, Mel Krokos", "title": "Interactive 3D Visualization for Theoretical Virtual Observatories", "comments": "10 Pages, 13 Figures, Accepted for Publication in Monthly Notices of\n  the Royal Astronomical Society", "journal-ref": null, "doi": "10.1093/mnras/sty855", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Observatories (VOs) are online hubs of scientific knowledge. They\nencompass a collection of platforms dedicated to the storage and dissemination\nof astronomical data, from simple data archives to e-research platforms\noffering advanced tools for data exploration and analysis. Whilst the more\nmature platforms within VOs primarily serve the observational community, there\nare also services fulfilling a similar role for theoretical data. Scientific\nvisualization can be an effective tool for analysis and exploration of datasets\nmade accessible through web platforms for theoretical data, which often contain\nspatial dimensions and properties inherently suitable for visualization via\ne.g. mock imaging in 2d or volume rendering in 3d. We analyze the current state\nof 3d visualization for big theoretical astronomical datasets through\nscientific web portals and virtual observatory services. We discuss some of the\nchallenges for interactive 3d visualization and how it can augment the workflow\nof users in a virtual observatory context. Finally we showcase a lightweight\nclient-server visualization tool for particle-based datasets allowing\nquantitative visualization via data filtering, highlighting two example use\ncases within the Theoretical Astrophysical Observatory.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 10:05:04 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Dykes", "Tim", ""], ["Hassan", "Amr", ""], ["Gheller", "Claudio", ""], ["Croton", "Darren", ""], ["Krokos", "Mel", ""]]}, {"id": "1803.11440", "submitter": "Avi Mendelson", "authors": "Ori Chalak, Cai Weiguang, Li Wei, Fang Lei, Zheng Libing, Wang\n  Jintang, Wu Zuguang, Gu Xiongli, Wang Haibin, Avi Mendelson", "title": "ScaleSimulator: A Fast and Cycle-Accurate Parallel Simulator for\n  Architectural Exploration", "comments": "Was published in SIMUTools 2017\n  https://drive.google.com/file/d/0B-bj84Yl7TM4R0NJRC16dnUxX0U/view", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of next generation computer systems should be supported by simulation\ninfrastructure that must achieve a few contradictory goals such as fast\nexecution time, high accuracy, and enough flexibility to allow comparison\nbetween large numbers of possible design points. Most existing architecture\nlevel simulators are designed to be flexible and to execute the code in\nparallel for greater efficiency, but at the cost of scarified accuracy. This\npaper presents the ScaleSimulator simulation environment, which is based on a\nnew design methodology whose goal is to achieve near cycle accuracy while still\nbeing flexible enough to simulate many different future system architectures\nand efficient enough to run meaningful workloads. We achieve these goals by\nmaking the parallelism a first-class citizen in our methodology. Thus, this\npaper focuses mainly on the ScaleSimulator design points that enable better\nparallel execution while maintaining the scalability and cycle accuracy of a\nsimulated architecture. The paper indicates that the new proposed\nScaleSimulator tool can (1) efficiently parallelize the execution of a\ncycle-accurate architecture simulator, (2) efficiently simulate complex\narchitectures (e.g., out-of-order CPU pipeline, cache coherency protocol, and\nnetwork) and massive parallel systems, and (3) use meaningful workloads, such\nas full simulation of OLTP benchmarks, to examine future architectural choices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 18:20:20 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Chalak", "Ori", ""], ["Weiguang", "Cai", ""], ["Wei", "Li", ""], ["Lei", "Fang", ""], ["Libing", "Zheng", ""], ["Jintang", "Wang", ""], ["Zuguang", "Wu", ""], ["Xiongli", "Gu", ""], ["Haibin", "Wang", ""], ["Mendelson", "Avi", ""]]}]