[{"id": "1107.0234", "submitter": "Miguel Mosteiro", "authors": "Antonio Fern\\'andez Anta and Miguel A. Mosteiro and Jorge Ram\\'on\n  Mu\\~noz", "title": "Unbounded Contention Resolution in Multiple-Access Channels", "comments": "21 pages, 1 figure. To appear in DISC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A frequent problem in settings where a unique resource must be shared among\nusers is how to resolve the contention that arises when all of them must use\nit, but the resource allows only for one user each time. The application of\nefficient solutions for this problem spans a myriad of settings such as radio\ncommunication networks or databases. For the case where the number of users is\nunknown, recent work has yielded fruitful results for local area networks and\nradio networks, although either a (possibly loose) upper bound on the number of\nusers needs to be known, or the solution is suboptimal, or it is only implicit\nor embedded in other problems, with bounds proved only asymptotically. In this\npaper, under the assumption that collision detection or information on the\nnumber of contenders is not available, we present a novel protocol for\ncontention resolution in radio networks, and we recreate a protocol previously\nused for other problems, tailoring the constants for our needs. In contrast\nwith previous work, both protocols are proved to be optimal up to a small\nconstant factor and with high probability for big enough number of contenders.\nAdditionally, the protocols are evaluated and contrasted with the previous work\nby extensive simulations. The evaluation shows that the complexity bounds\nobtained by the analysis are rather tight, and that both protocols proposed\nhave small and predictable complexity for many system sizes (unlike previous\nproposals).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 14:16:33 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Anta", "Antonio Fern\u00e1ndez", ""], ["Mosteiro", "Miguel A.", ""], ["Mu\u00f1oz", "Jorge Ram\u00f3n", ""]]}, {"id": "1107.0538", "submitter": "Antonio Wendell De Oliveira Rodrigues", "authors": "Antonio Wendell De Oliveira Rodrigues (INRIA Lille - Nord Europe),\n  Fr\\'ed\\'eric Guyomarc'H (INRIA Lille - Nord Europe), Jean-Luc Dekeyser (INRIA\n  Lille - Nord Europe), Yvonnick Le Menach (L2EP)", "title": "Automatic Multi-GPU Code Generation applied to Simulation of Electrical\n  Machines", "comments": "Compumag 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electrical and electronic engineering has used parallel programming to\nsolve its large scale complex problems for performance reasons. However, as\nparallel programming requires a non-trivial distribution of tasks and data,\ndevelopers find it hard to implement their applications effectively. Thus, in\norder to reduce design complexity, we propose an approach to generate code for\nhybrid architectures (e.g. CPU + GPU) using OpenCL, an open standard for\nparallel programming of heterogeneous systems. This approach is based on Model\nDriven Engineering (MDE) and the MARTE profile, standard proposed by Object\nManagement Group (OMG). The aim is to provide resources to non-specialists in\nparallel programming to implement their applications. Moreover, thanks to model\nreuse capacity, we can add/change functionalities or the target architecture.\nConsequently, this approach helps industries to achieve their time-to-market\nconstraints and confirms by experimental tests, performance improvements using\nmulti-GPU environments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2011 06:13:51 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Rodrigues", "Antonio Wendell De Oliveira", "", "INRIA Lille - Nord Europe"], ["Guyomarc'H", "Fr\u00e9d\u00e9ric", "", "INRIA Lille - Nord Europe"], ["Dekeyser", "Jean-Luc", "", "INRIA\n  Lille - Nord Europe"], ["Menach", "Yvonnick Le", "", "L2EP"]]}, {"id": "1107.1072", "submitter": "Aniket Kate", "authors": "Michael Backes, Ian Goldberg, Aniket Kate, and Tomas Toft", "title": "Adding Query Privacy to Robust DHTs", "comments": "To appear at ACM ASIACCS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in anonymous communication over distributed hash tables (DHTs) has\nincreased in recent years. However, almost all known solutions solely aim at\nachieving sender or requestor anonymity in DHT queries. In many application\nscenarios, it is crucial that the queried key remains secret from intermediate\npeers that (help to) route the queries towards their destinations. In this\npaper, we satisfy this requirement by presenting an approach for providing\nprivacy for the keys in DHT queries.\n  We use the concept of oblivious transfer (OT) in communication over DHTs to\npreserve query privacy without compromising spam resistance. Although our\nOT-based approach can work over any DHT, we concentrate on communication over\nrobust DHTs that can tolerate Byzantine faults and resist spam. We choose the\nbest-known robust DHT construction, and employ an efficient OT protocol\nwell-suited for achieving our goal of obtaining query privacy over robust DHTs.\nFinally, we compare the performance of our privacy-preserving protocols with\ntheir more privacy-invasive counterparts. We observe that there is no increase\nin the message complexity and only a small overhead in the computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 09:40:37 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2012 08:45:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Backes", "Michael", ""], ["Goldberg", "Ian", ""], ["Kate", "Aniket", ""], ["Toft", "Tomas", ""]]}, {"id": "1107.1089", "submitter": "Antonio Fern\\'andez Anta", "authors": "Andr\\'es Sevilla and Alberto Mozo and Antonio Fern\\'andez Anta", "title": "Node Sampling using Random Centrifugal Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling a network with a given probability distribution has been identified\nas a useful operation. In this paper we propose distributed algorithms for\nsampling networks, so that nodes are selected by a special node, called the\n\\emph{source}, with a given probability distribution. All these algorithms are\nbased on a new class of random walks, that we call Random Centrifugal Walks\n(RCW). A RCW is a random walk that starts at the source and always moves away\nfrom it.\n  Firstly, an algorithm to sample any connected network using RCW is proposed.\nThe algorithm assumes that each node has a weight, so that the sampling process\nmust select a node with a probability proportional to its weight. This\nalgorithm requires a preprocessing phase before the sampling of nodes. In\nparticular, a minimum diameter spanning tree (MDST) is created in the network,\nand then nodes' weights are efficiently aggregated using the tree. The good\nnews are that the preprocessing is done only once, regardless of the number of\nsources and the number of samples taken from the network. After that, every\nsample is done with a RCW whose length is bounded by the network diameter.\n  Secondly, RCW algorithms that do not require preprocessing are proposed for\ngrids and networks with regular concentric connectivity, for the case when the\nprobability of selecting a node is a function of its distance to the source.\n  The key features of the RCW algorithms (unlike previous Markovian approaches)\nare that (1) they do not need to warm-up (stabilize), (2) the sampling always\nfinishes in a number of hops bounded by the network diameter, and (3) it\nselects a node with the exact probability distribution.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 10:46:33 GMT"}, {"version": "v2", "created": "Sun, 20 May 2012 04:10:29 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2012 17:39:50 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Sevilla", "Andr\u00e9s", ""], ["Mozo", "Alberto", ""], ["Anta", "Antonio Fern\u00e1ndez", ""]]}, {"id": "1107.1127", "submitter": "Hrishikesh Sharma", "authors": "Shreeniwas Sapre, Hrishikesh Sharma, Abhishek Patil, B. S. Adiga and\n  Sachin Patkar", "title": "Finite Projective Geometry based Fast, Conflict-free Parallel Matrix\n  Computations", "comments": "32 pages, to be submitted to some distributed and parallel computing\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix computations, especially iterative PDE solving (and the sparse matrix\nvector multiplication subproblem within) using conjugate gradient algorithm,\nand LU/Cholesky decomposition for solving system of linear equations, form the\nkernel of many applications, such as circuit simulators, computational fluid\ndynamics or structural analysis etc. The problem of designing approaches for\nparallelizing these computations, to get good speedups as much as possible as\nper Amdahl's law, has been continuously researched upon. In this paper, we\ndiscuss approaches based on the use of finite projective geometry graphs for\nthese two problems. For the problem of conjugate gradient algorithm, the\napproach looks at an alternative data distribution based on projective-geometry\nconcepts. It is proved that this data distribution is an optimal data\ndistribution for scheduling the main problem of dense matrix-vector\nmultiplication. For the problem of parallel LU/Cholesky decomposition of\ngeneral matrices, the approach is motivated by the recently published scheme\nfor interconnects of distributed systems, perfect difference networks. We find\nthat projective-geometry based graphs indeed offer an exciting way of\nparallelizing these computations, and in fact many others. Moreover, their\napplications ranges from architectural ones (interconnect choice) to\nalgorithmic ones (data distributions).\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 11:18:54 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Sapre", "Shreeniwas", ""], ["Sharma", "Hrishikesh", ""], ["Patil", "Abhishek", ""], ["Adiga", "B. S.", ""], ["Patkar", "Sachin", ""]]}, {"id": "1107.1200", "submitter": "EPTCS", "authors": "Bogdan Aman (''A.I. Cuza'' University of Iasi, Romania), Gabriel\n  Ciobanu (Institute of Computer Science, Romanian Academy, Iasi)", "title": "Time Delays in Membrane Systems and Petri Nets", "comments": "In Proceedings QAPL 2011, arXiv:1107.0746", "journal-ref": "EPTCS 57, 2011, pp. 47-60", "doi": "10.4204/EPTCS.57.4", "report-no": null, "categories": "cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timing aspects in formalisms with explicit resources and parallelism are\ninvestigated, and it is presented a formal link between timed membrane systems\nand timed Petri nets with localities. For both formalisms, timing does not\nincrease the expressive power; however both timed membrane systems and timed\nPetri nets are more flexible in describing molecular phenomena where time is a\ncritical resource. We establish a link between timed membrane systems and timed\nPetri nets with localities, and prove an operational correspondence between\nthem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 17:54:46 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Aman", "Bogdan", "", "''A.I. Cuza'' University of Iasi, Romania"], ["Ciobanu", "Gabriel", "", "Institute of Computer Science, Romanian Academy, Iasi"]]}, {"id": "1107.1222", "submitter": "EPTCS", "authors": "David Balduzzi (MPI for Intelligent Systems)", "title": "On the information-theoretic structure of distributed measurements", "comments": "In Proceedings DCM 2011, arXiv:1207.6821", "journal-ref": "EPTCS 88, 2012, pp. 28-42", "doi": "10.4204/EPTCS.88.3", "report-no": null, "categories": "cs.IT cs.DC cs.NE math.CT math.IT nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal structure of a measuring device, which depends on what its\ncomponents are and how they are organized, determines how it categorizes its\ninputs. This paper presents a geometric approach to studying the internal\nstructure of measurements performed by distributed systems such as\nprobabilistic cellular automata. It constructs the quale, a family of sections\nof a suitably defined presheaf, whose elements correspond to the measurements\nperformed by all subsystems of a distributed system. Using the quale we\nquantify (i) the information generated by a measurement; (ii) the extent to\nwhich a measurement is context-dependent; and (iii) whether a measurement is\ndecomposable into independent submeasurements, which turns out to be equivalent\nto context-dependence. Finally, we show that only indecomposable measurements\nare more informative than the sum of their submeasurements.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 19:21:30 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2012 02:06:15 GMT"}], "update_date": "2012-08-01", "authors_parsed": [["Balduzzi", "David", "", "MPI for Intelligent Systems"]]}, {"id": "1107.1545", "submitter": "Gabriel Terejanu", "authors": "Gabriel Terejanu, Yang Cheng, Tarunraj Singh, Peter D. Scott", "title": "Comparison of SCIPUFF Plume Prediction with Particle Filter Assimilated\n  Prediction for Dipole Pride 26 Data", "comments": "The Chemical and Biological Defense Physical Science and Technology\n  Conference, New Orleans, November 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the application of a particle filter for data\nassimilation in the context of puff-based dispersion models. Particle filters\nprovide estimates of the higher moments, and are well suited for strongly\nnonlinear and/or non-Gaussian models. The Gaussian puff model SCIPUFF, is used\nin predicting the chemical concentration field after a chemical incident. This\nmodel is highly nonlinear and evolves with variable state dimension and, after\nsufficient time, high dimensionality. While the particle filter formalism\nnaturally supports variable state dimensionality high dimensionality represents\na challenge in selecting an adequate number of particles, especially for the\nBootstrap version. We present an implementation of the Bootstrap particle\nfilter and compare its performance with the SCIPUFF predictions. Both the model\nand the Particle Filter are evaluated on the Dipole Pride 26 experimental data.\nSince there is no available ground truth, the data has been divided in two\nsets: training and testing. We show that even with a modest number of\nparticles, the Bootstrap particle filter provides better estimates of the\nconcentration field compared with the process model, without excessive increase\nin computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 01:59:45 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Terejanu", "Gabriel", ""], ["Cheng", "Yang", ""], ["Singh", "Tarunraj", ""], ["Scott", "Peter D.", ""]]}, {"id": "1107.1627", "submitter": "Zhiying Wang", "authors": "Zhiying Wang, Itzhak Tamo, Jehoshua Bruck", "title": "On Codes for Optimal Rebuilding Access", "comments": "Submitted to Allerton 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MDS (maximum distance separable) array codes are widely used in storage\nsystems due to their computationally efficient encoding and decoding\nprocedures. An MDS code with r redundancy nodes can correct any r erasures by\naccessing (reading) all the remaining information in both the systematic nodes\nand the parity (redundancy) nodes. However, in practice, a single erasure is\nthe most likely failure event; hence, a natural question is how much\ninformation do we need to access in order to rebuild a single storage node? We\ndefine the rebuilding ratio as the fraction of remaining information accessed\nduring the rebuilding of a single erasure. In our previous work we showed that\nthe optimal rebuilding ratio of 1/r is achievable (using our newly constructed\narray codes) for the rebuilding of any systematic node, however, all the\ninformation needs to be accessed for the rebuilding of the parity nodes.\nNamely, constructing array codes with a rebuilding ratio of 1/r was left as an\nopen problem. In this paper, we solve this open problem and present array codes\nthat achieve the lower bound of 1/r for rebuilding any single systematic or\nparity node.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 13:21:28 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Wang", "Zhiying", ""], ["Tamo", "Itzhak", ""], ["Bruck", "Jehoshua", ""]]}, {"id": "1107.1851", "submitter": "Dohan Kim", "authors": "Dohan Kim", "title": "Task swapping networks in distributed systems", "comments": "This is a preprint of a paper whose final and definite form is\n  published in: Int. J. Comput. Math. 90 (2013), 2221-2243 (DOI:\n  10.1080/00207160.2013.772985)", "journal-ref": "Int. J. Comput. Math. 90 (2013), 2221-2243", "doi": "10.1080/00207160.2013.772985", "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we propose task swapping networks for task reassignments by\nusing task swappings in distributed systems. Some classes of task reassignments\nare achieved by using iterative local task swappings between software agents in\ndistributed systems. We use group-theoretic methods to find a minimum-length\nsequence of adjacent task swappings needed from a source task assignment to a\ntarget task assignment in a task swapping network of several well-known\ntopologies.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2011 11:35:16 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2011 22:48:59 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2013 15:22:29 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Kim", "Dohan", ""]]}, {"id": "1107.1866", "submitter": "Dohan Kim", "authors": "Dohan Kim", "title": "Priority-based task reassignments in hierarchical 2D mesh-connected\n  systems using tableaux", "comments": "Preprint of an article published in Discrete Mathematics, Algorithms\n  and Applications Vol. 6, No. 2 (2014) 1450022 (16 pages), (c) World\n  Scientific Publishing Company, DOI: 10.1142/S1793830914500220", "journal-ref": "Discrete Mathematics, Algorithms and Applications Vol. 6, No. 2\n  (2014) 1450022 (16 pages)", "doi": "10.1142/S1793830914500220", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Task reassignments in 2D mesh-connected systems (2D-MSs) have been researched\nfor several decades. We propose a hierarchical 2D mesh-connected system\n(2D-HMS) in order to exploit the regular nature of a 2D-MS. In our approach\npriority-based task assignments and reassignments in a 2D-HMS are represented\nby tableaux and their algorithms. We show how task relocations for a\npriority-based task reassignment in a 2D-HMS are reduced to a jeu de taquin\nslide.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2011 15:48:34 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2011 22:46:09 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2013 12:31:18 GMT"}, {"version": "v4", "created": "Fri, 21 Mar 2014 02:54:59 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Kim", "Dohan", ""]]}, {"id": "1107.1932", "submitter": "Sleiman Rabah", "authors": "Sleiman Rabah and Dan Ni and Payam Jahanshahi and Luis Felipe Guzman", "title": "Current State and Challenges of Automatic Planning in Web Service\n  Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a survey on the current state of Web Service Compositions\nand the difficulties and solutions to automated Web Service Compositions. This\nfirst gives a definition of Web Service Composition and the motivation and goal\nof it. It then explores into why we need automated Web Service Compositions and\nformally defines the domains. Techniques and solutions are proposed by the\npapers we surveyed to solve the current difficulty of automated Web Service\nComposition. Verification and future work is discussed at the end to further\nextend the topic.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 04:24:48 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Rabah", "Sleiman", ""], ["Ni", "Dan", ""], ["Jahanshahi", "Payam", ""], ["Guzman", "Luis Felipe", ""]]}, {"id": "1107.1937", "submitter": "Stefano Ferretti Stefano Ferretti", "authors": "Stefano Ferretti, Vittorio Ghini", "title": "Scale-Free Opportunistic Networks: is it Possible?", "comments": "A revised version of the paper will appear in Proc. of the 8th\n  International Workshop on Mobile Peer-to-Peer Computing - IEEE International\n  Conference on Pervasive Computing and Communications (PERCOM 2012), Lugano,\n  Switzerland, IEEE, March 2012", "journal-ref": null, "doi": "10.1109/PerComW.2012.6197590", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coupling of scale-free networks with mobile unstructured networks is\ncertainly unusual. In mobile networks, connections active at a given instant\nare constrained by the geographical distribution of mobile nodes, and by the\nlimited signal strength of the wireless technology employed to build the ad-hoc\noverlay. This is in contrast with the presence of hubs, typical of scale-free\nnets. However, opportunistic (mobile) networks possess the distinctive feature\nto be delay tolerant; mobile nodes implement a store, carry and forward\nstrategy that permits to disseminate data based on a multi-hop route, which is\nbuilt in time, when nodes encounter other ones while moving. In this paper, we\nconsider opportunistic networks as evolving graphs where links represent\ncontacts among nodes arising during a (non-instantaneous) time interval. We\ndiscuss a strategy to control the way nodes manage contacts and build\n\"opportunistic overlays\". Based on such an approach, interesting overlays can\nbe obtained, shaped following given desired topologies, such as scale-free\nones.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 05:03:45 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 10:14:44 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ferretti", "Stefano", ""], ["Ghini", "Vittorio", ""]]}, {"id": "1107.2443", "submitter": "Taisuke Izumi", "authors": "Jun Hosoda, Juraj Hromkovic, Taisuke Izumi, Horotaka Ono, Monika\n  Steinova, Koichi Wada", "title": "On the Approximability and Hardness of Minimum Topic Connected Overlay\n  and Its Special Instances", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of designing a scalable overlay network to support\ndecentralized topic-based pub/sub communication, the Minimum Topic-Connected\nOverlay problem (Min-TCO in short) has been investigated: Given a set of t\ntopics and a collection of n users together with the lists of topics they are\ninterested in, the aim is to connect these users to a network by a minimum\nnumber of edges such that every graph induced by users interested in a common\ntopic is connected. It is known that Min-TCO is NP-hard and approximable within\nO(log t) in polynomial time. In this paper, we further investigate the problem\nand some of its special instances. We give various hardness results for\ninstances where the number of topics in which an user is interested in is\nbounded by a constant, and also for the instances where the number of users\ninterested in a common topic is constant. For the latter case, we present a\nfirst constant approximation algorithm. We also present some polynomial-time\nalgorithms for very restricted instances of Min-TCO.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 00:51:53 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Hosoda", "Jun", ""], ["Hromkovic", "Juraj", ""], ["Izumi", "Taisuke", ""], ["Ono", "Horotaka", ""], ["Steinova", "Monika", ""], ["Wada", "Koichi", ""]]}, {"id": "1107.2526", "submitter": "Pascal Bianchi", "authors": "Pascal Bianchi and J\\'er\\'emie Jakubowicz", "title": "Convergence of a Multi-Agent Projected Stochastic Gradient Algorithm for\n  Non-Convex Optimization", "comments": "IEEE Transactions on Automatic Control 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for the convergence analysis of a class of\ndistributed constrained non-convex optimization algorithms in multi-agent\nsystems. The aim is to search for local minimizers of a non-convex objective\nfunction which is supposed to be a sum of local utility functions of the\nagents. The algorithm under study consists of two steps: a local stochastic\ngradient descent at each agent and a gossip step that drives the network of\nagents to a consensus. Under the assumption of decreasing stepsize, it is\nproved that consensus is asymptotically achieved in the network and that the\nalgorithm converges to the set of Karush-Kuhn-Tucker points. As an important\nfeature, the algorithm does not require the double-stochasticity of the gossip\nmatrices. It is in particular suitable for use in a natural broadcast scenario\nfor which no feedback messages between agents are required. It is proved that\nour result also holds if the number of communications in the network per unit\nof time vanishes at moderate speed as time increases, allowing for potential\nsavings of the network's energy. Applications to power allocation in wireless\nad-hoc networks are discussed. Finally, we provide numerical results which\nsustain our claims.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 11:21:14 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2012 14:32:03 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2013 20:55:03 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Bianchi", "Pascal", ""], ["Jakubowicz", "J\u00e9r\u00e9mie", ""]]}, {"id": "1107.2990", "submitter": "Sotirios Kentros", "authors": "Sotirios Kentros and Aggelos Kiayias", "title": "Solving the At-Most-Once Problem with Nearly Optimal Effectiveness", "comments": "Updated Version. A Brief Announcement was published in PODC 2011. An\n  Extended Abstract was published in the proceeding of ICDCN 2012. A full\n  version was published in Theoretical Computer Science, Volume 496, 22 July\n  2013, Pages 69 - 88", "journal-ref": "Theoretical Computer Science, Volume 496, 22 July 2013, Pages\n  69-88, ISSN 0304-3975", "doi": "10.1016/j.tcs.2013.04.017", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present and analyze a wait-free deterministic algorithm for solving the\nat-most-once problem: how m shared-memory fail-prone processes perform\nasynchronously n jobs at most once. Our algorithmic strategy provides for the\nfirst time nearly optimal effectiveness, which is a measure that expresses the\ntotal number of jobs completed in the worst case. The effectiveness of our\nalgorithm equals n-2m+2. This is up to an additive factor of m close to the\nknown effectiveness upper bound n-m+1 over all possible algorithms and improves\non the previously best known deterministic solutions that have effectiveness\nonly n-log m o(n). We also present an iterative version of our algorithm that\nfor any $m = O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$ is both\neffectiveness-optimal and work-optimal, for any constant $\\epsilon > 0$. We\nthen employ this algorithm to provide a new algorithmic solution for the\nWrite-All problem which is work optimal for any\n$m=O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 04:24:38 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 00:32:22 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Kentros", "Sotirios", ""], ["Kiayias", "Aggelos", ""]]}, {"id": "1107.3129", "submitter": "Anwitaman Datta", "authors": "Frederique Oggier, Anwitaman Datta", "title": "Homomorphic Self-repairing Codes for Agile Maintenance of Distributed\n  Storage Systems", "comments": "arXiv admin note: significant text overlap with arXiv:1008.0064", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data storage systems are essential to deal with the need to store\nmassive volumes of data. In order to make such a system fault-tolerant, some\nform of redundancy becomes crucial, incurring various overheads - most\nprominently in terms of storage space and maintenance bandwidth requirements.\nErasure codes, originally designed for communication over lossy channels,\nprovide a storage efficient alternative to replication based redundancy,\nhowever entailing high communication overhead for maintenance, when some of the\nencoded fragments need to be replenished in news ones after failure of some\nstorage devices. We propose as an alternative a new family of erasure codes\ncalled self-repairing codes (SRC) taking into account the peculiarities of\ndistributed storage systems, specifically the maintenance process. SRC has the\nfollowing salient features: (a) encoded fragments can be repaired directly from\nother subsets of encoded fragments by downloading less data than the size of\nthe complete object, ensuring that (b) a fragment is repaired from a fixed\nnumber of encoded fragments, the number depending only on how many encoded\nblocks are missing and independent of which specific blocks are missing. This\npaper lays the foundations by defining the novel self-repairing codes,\nelaborating why the defined characteristics are desirable for distributed\nstorage systems. Then homomorphic self-repairing codes (HSRC) are proposed as a\nconcrete instance, whose various aspects and properties are studied and\ncompared - quantitatively or qualitatively with respect to other codes\nincluding traditional erasure codes as well as other recent codes designed\nspecifically for storage applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 18:46:33 GMT"}], "update_date": "2011-12-25", "authors_parsed": [["Oggier", "Frederique", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1107.3166", "submitter": "Barlas O\\u{g}uz", "authors": "Barlas O\\u{g}uz, Venkat Anantharam, Ilkka Norros", "title": "Stable, scalable, decentralized P2P file sharing with non-altruistic\n  peers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P2P systems provide a scalable solution for distributing large files in a\nnetwork. The file is split into many chunks, and peers contact other peers to\ncollect missing chunks to eventually complete the entire file. The so-called\n`rare chunk' phenomenon, where a single chunk becomes rare and prevents peers\nfrom completing the file, is a threat to the stability of such systems.\nPractical systems such as BitTorrent overcome this issue by requiring a global\nsearch for the rare chunk, which necessitates a centralized mechanism. We\ndemonstrate a new system based on an approximate rare-chunk rule, allowing for\ncompletely distributed file sharing while retaining scalability and stability.\nWe assume non-altruistic peers and the seed is required to make only a minimal\ncontribution.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 21:03:08 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["O\u011fuz", "Barlas", ""], ["Anantharam", "Venkat", ""], ["Norros", "Ilkka", ""]]}, {"id": "1107.3682", "submitter": "Jun Wu", "authors": "Jun Wu and Shigeru Shimamoto", "title": "Context-Capture Multi-Valued Decision Fusion With Fault Tolerant\n  Capability For Wireless Sensor Networks", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": "10.5121/ijwmn.2011.3310", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless sensor networks (WSNs) are usually utilized to perform decision\nfusion of event detection. Current decision fusion schemes are based on binary\nvalued decision and do not consider bursty contextcapture. However, bursty\ncontext and multi-valued data are important characteristics of WSNs. One on\nhand, the local decisions from sensors usually have bursty and contextual\ncharacteristics. Fusion center must capture the bursty context information from\nthe sensors. On the other hand, in practice, many applications need to process\nmulti-valued data, such as temperature and reflection level used for lightening\nprediction. To address these challenges, the Markov modulated Poisson process\n(MMPP) and multi-valued logic are introduced into WSNs to perform\ncontext-capture multi-valued decision fusion. The overall decision fusion is\ndecomposed into two parts. The first part is the context-capture model for WSNs\nusing superposition MMPP. Through this procedure, the fusion center has a\nhigher probability to get useful local decisions from sensors. The second one\nis focused on multi-valued decision fusion. Fault detection can also be\nperformed based on MVL. Once the fusion center detects the faulty nodes, all\ntheir local decisions are removed from the computation of the likelihood\nratios. Finally, we evaluate the capability of context-capture and fault\ntolerant. The result supports the usefulness of our scheme.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 10:50:36 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Wu", "Jun", ""], ["Shimamoto", "Shigeru", ""]]}, {"id": "1107.3734", "submitter": "Nicolas Gast", "authors": "Marc Tchiboukdjian, Nicolas Gast, Denis Trystram", "title": "Decentralized List Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical list scheduling is a very popular and efficient technique for\nscheduling jobs in parallel and distributed platforms. It is inherently\ncentralized. However, with the increasing number of processors, the cost for\nmanaging a single centralized list becomes too prohibitive. A suitable approach\nto reduce the contention is to distribute the list among the computational\nunits: each processor has only a local view of the work to execute. Thus, the\nscheduler is no longer greedy and standard performance guarantees are lost.\n  The objective of this work is to study the extra cost that must be paid when\nthe list is distributed among the computational units. We first present a\ngeneral methodology for computing the expected makespan based on the analysis\nof an adequate potential function which represents the load unbalance between\nthe local lists. We obtain an equation on the evolution of the potential by\ncomputing its expected decrease in one step of the schedule. Our main theorem\nshows how to solve such equations to bound the makespan. Then, we apply this\nmethod to several scheduling problems, namely, for unit independent tasks, for\nweighted independent tasks and for tasks with precendence constraints. More\nprecisely, we prove that the time for scheduling a global workload W composed\nof independent unit tasks on m processors is equal to W/m plus an additional\nterm proportional to log_2 W. We provide a lower bound which shows that this is\noptimal up to a constant. This result is extended to the case of weighted\nindependent tasks. In the last setting, precedence task graphs, our analysis\nleads to an improvement on the bound of Arora et al. We finally provide some\nexperiments using a simulator. The distribution of the makespan is shown to fit\nexisting probability laws. The additive term is shown by simulation to be\naround 3 \\log_2 W confirming the tightness of our analysis.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 15:13:23 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Tchiboukdjian", "Marc", ""], ["Gast", "Nicolas", ""], ["Trystram", "Denis", ""]]}, {"id": "1107.3765", "submitter": "Jordan Boyd-Graber", "authors": "Ke Zhai, Jordan Boyd-Graber, and Nima Asadi", "title": "Using Variational Inference and MapReduce to Scale Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for\nexploring document collections. Because of the increasing prevalence of large\ndatasets, there is a need to improve the scalability of inference of LDA. In\nthis paper, we propose a technique called ~\\emph{MapReduce LDA} (Mr. LDA) to\naccommodate very large corpus collections in the MapReduce framework. In\ncontrast to other techniques to scale inference for LDA, which use Gibbs\nsampling, we use variational inference. Our solution efficiently distributes\ncomputation and is relatively simple to implement. More importantly, this\nvariational implementation, unlike highly tuned and specialized\nimplementations, is easily extensible. We demonstrate two extensions of the\nmodel possible with this scalable framework: informed priors to guide topic\ndiscovery and modeling topics from a multilingual corpus.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 16:32:22 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Zhai", "Ke", ""], ["Boyd-Graber", "Jordan", ""], ["Asadi", "Nima", ""]]}, {"id": "1107.4660", "submitter": "V\\'ictor M. L\\'opez Mill\\'an", "authors": "V\\'ictor L\\'opez Mill\\'an and Vicent Cholvi and Luis L\\'opez and\n  Antonio Fern\\'andez Anta", "title": "Reducing Search Lengths with Locally Precomputed Partial Random Walks", "comments": "The contents in this articule have suffered major changes. It has\n  been replaced by \"Improving Resource Location with Locally Precomputed\n  Partial Random Walks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random walks can be used to search a complex networks for a desired resource.\nTo reduce the number of hops necessary to find the resource, we propose a\nsearch mechanism based on building random walks connecting together partial\nwalks that have been precomputed at each network node in an initial stage. The\nresources found in each partial walk are registered in its associated Bloom\nfilter. Searches can then jump over partial nodes in which the resource is not\nlocated, significantly reducing search length. However, additional unnecessary\nhops come from false positives at the Bloom filters. The analytic model\nprovided predicts the expected search length of this mechanism, the optimal\nsize of the partial walks and the corresponding optimal (shortest) expected\nsearch length. Simulation experiments are used to validate these predictions\nand to assess the impact of the number of partial walks precomputed in each\nnode.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2011 06:17:04 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2012 22:13:51 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2013 12:17:19 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Mill\u00e1n", "V\u00edctor L\u00f3pez", ""], ["Cholvi", "Vicent", ""], ["L\u00f3pez", "Luis", ""], ["Anta", "Antonio Fern\u00e1ndez", ""]]}, {"id": "1107.4724", "submitter": "Pablo Chico de Guzman Huerta", "authors": "Pablo Chico de Guzm\\'an, Amadeo Casas, Manuel Carro and Manuel V.\n  Hermenegildo", "title": "Parallel Backtracking with Answer Memoing for Independent\n  And-Parallelism", "comments": "19 pages, 15 figures, uses tlp style", "journal-ref": "Theory and Practice of Logic Programming (2011) volume 11, issue\n  4-5, pages 555-574", "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal-level Independent and-parallelism (IAP) is exploited by scheduling for\nsimultaneous execution two or more goals which will not interfere with each\nother at run time. This can be done safely even if such goals can produce\nmultiple answers. The most successful IAP implementations to date have used\nrecomputation of answers and sequentially ordered backtracking. While in\nprinciple simplifying the implementation, recomputation can be very inefficient\nif the granularity of the parallel goals is large enough and they produce\nseveral answers, while sequentially ordered backtracking limits parallelism.\nAnd, despite the expected simplification, the implementation of the classic\nschemes has proved to involve complex engineering, with the consequent\ndifficulty for system maintenance and extension, while still frequently running\ninto the well-known trapped goal and garbage slot problems. This work presents\nan alternative parallel backtracking model for IAP and its implementation. The\nmodel features parallel out-of-order (i.e., non-chronological) backtracking and\nrelies on answer memoization to reuse and combine answers. We show that this\napproach can bring significant performance advantages. Also, it can bring some\nsimplification to the important engineering task involved in implementing the\nbacktracking mechanism of previous approaches.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2011 06:28:05 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["de Guzm\u00e1n", "Pablo Chico", ""], ["Casas", "Amadeo", ""], ["Carro", "Manuel", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1107.4940", "submitter": "Dejan Kovachev", "authors": "Dejan Kovachev, Yiwei Cao and Ralf Klamma", "title": "Mobile Cloud Computing: A Comparison of Application Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Cloud computing is an emerging concept combining many fields of computing.\nThe foundation of cloud computing is the delivery of services, software and\nprocessing capacity over the Internet, reducing cost, increasing storage,\nautomating systems, decoupling of service delivery from underlying technology,\nand providing flexibility and mobility of information. However, the actual\nrealization of these benefits is far from being achieved for mobile\napplications and open many new research questions. In order to better\nunderstand how to facilitate the building of mobile cloud-based applications,\nwe have surveyed existing work in mobile computing through the prism of cloud\ncomputing principles. We give a definition of mobile cloud coputing and provide\nan overview of the results from this review, in particular, models of mobile\ncloud applications. We also highlight research challenges in the area of mobile\ncloud computing. We conclude with recommendations for how this better\nunderstanding of mobile cloud computing can help building more powerful mobile\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 13:17:13 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Kovachev", "Dejan", ""], ["Cao", "Yiwei", ""], ["Klamma", "Ralf", ""]]}, {"id": "1107.5279", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, K. V. Rashmi and P. Vijay Kumar", "title": "Information-theoretically Secure Regenerating Codes for Distributed\n  Storage", "comments": "Globecom 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regenerating codes are a class of codes for distributed storage networks that\nprovide reliability and availability of data, and also perform efficient node\nrepair. Another important aspect of a distributed storage network is its\nsecurity. In this paper, we consider a threat model where an eavesdropper may\ngain access to the data stored in a subset of the storage nodes, and possibly\nalso, to the data downloaded during repair of some nodes. We provide explicit\nconstructions of regenerating codes that achieve information-theoretic secrecy\ncapacity in this setting.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 18:09:52 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Shah", "Nihar B.", ""], ["Rashmi", "K. V.", ""], ["Kumar", "P. Vijay", ""]]}, {"id": "1107.5419", "submitter": "Florian Huc", "authors": "Sebastien Gambs and Rachid Guerraoui and Hamza Harkous and Florian Huc\n  and Anne-Marie Kermarrec", "title": "Scalable and Secure Aggregation in Distributed Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing an aggregation function in a\n\\emph{secure} and \\emph{scalable} way. Whereas previous distributed solutions\nwith similar security guarantees have a communication cost of $O(n^3)$, we\npresent a distributed protocol that requires only a communication complexity of\n$O(n\\log^3 n)$, which we prove is near-optimal. Our protocol ensures perfect\nsecurity against a computationally-bounded adversary, tolerates\n$(1/2-\\epsilon)n$ malicious nodes for any constant $1/2 > \\epsilon > 0$ (not\ndepending on $n$), and outputs the exact value of the aggregated function with\nhigh probability.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 09:09:38 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2011 12:10:07 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2011 14:14:26 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Gambs", "Sebastien", ""], ["Guerraoui", "Rachid", ""], ["Harkous", "Hamza", ""], ["Huc", "Florian", ""], ["Kermarrec", "Anne-Marie", ""]]}, {"id": "1107.5645", "submitter": "Quan Yu", "authors": "Quan Yu, Kenneth W. Shum, Chi Wan Sung", "title": "Minimization of Storage Cost in Distributed Storage Systems with Repair\n  Consideration", "comments": "5 pages, 4 figures, to appear in Proc. IEEE GLOBECOM, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a distributed storage system, the storage costs of different storage\nnodes, in general, can be different. How to store a file in a given set of\nstorage nodes so as to minimize the total storage cost is investigated. By\nanalyzing the min-cut constraints of the information flow graph, the feasible\nregion of the storage capacities of the nodes can be determined. The storage\ncost minimization can then be reduced to a linear programming problem, which\ncan be readily solved. Moreover, the tradeoff between storage cost and\nrepair-bandwidth is established.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2011 08:40:55 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Yu", "Quan", ""], ["Shum", "Kenneth W.", ""], ["Sung", "Chi Wan", ""]]}, {"id": "1107.5951", "submitter": "Matthew Knepley", "authors": "Dave A. May, Matthew G. Knepley", "title": "Optimal, scalable forward models for computing gravity anomalies", "comments": "38 pages, 13 figures; accepted by Geophysical Journal International", "journal-ref": "Geophysical Journal International, 187(1):161-177, 2011", "doi": "10.1111/j.1365-246X.2011.05167.x", "report-no": null, "categories": "cs.CE cs.DC physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe three approaches for computing a gravity signal from a density\nanomaly. The first approach consists of the classical \"summation\" technique,\nwhilst the remaining two methods solve the Poisson problem for the\ngravitational potential using either a Finite Element (FE) discretization\nemploying a multilevel preconditioner, or a Green's function evaluated with the\nFast Multipole Method (FMM). The methods utilizing the PDE formulation\ndescribed here differ from previously published approaches used in gravity\nmodeling in that they are optimal, implying that both the memory and\ncomputational time required scale linearly with respect to the number of\nunknowns in the potential field. Additionally, all of the implementations\npresented here are developed such that the computations can be performed in a\nmassively parallel, distributed memory computing environment. Through numerical\nexperiments, we compare the methods on the basis of their discretization error,\nCPU time and parallel scalability. We demonstrate the parallel scalability of\nall these techniques by running forward models with up to $10^8$ voxels on\n1000's of cores.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 12:31:59 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["May", "Dave A.", ""], ["Knepley", "Matthew G.", ""]]}, {"id": "1107.6014", "submitter": "Anissa Lamani", "authors": "Alain Cournier (MIS), Swan Dubois (LIP6), Anissa Lamani (MIS), Franck\n  Petit (LIP6), Vincent Villain (MIS)", "title": "Snap-Stabilizing Message Forwarding Algorithm on Tree Topologies", "comments": "2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the message forwarding problem that consists in\nmanaging the network resources that are used to forward messages. Previous\nworks on this problem provide solutions that either use a significant number of\nbuffers (that is n buffers per processor, where n is the number of processors\nin the network) making the solution not scalable or, they reserve all the\nbuffers from the sender to the receiver to forward only one message %while\nusing D buffers (where D refers to the diameter of the network) . The only\nsolution that uses a constant number of buffers per link was introduced in [1].\nHowever the solution works only on a chain networks. In this paper, we propose\na snap-stabilizing algorithm for the message forwarding problem that uses the\nsame complexity on the number of buffers as [1] and works on tree topologies.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 16:37:44 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Cournier", "Alain", "", "MIS"], ["Dubois", "Swan", "", "LIP6"], ["Lamani", "Anissa", "", "MIS"], ["Petit", "Franck", "", "LIP6"], ["Villain", "Vincent", "", "MIS"]]}]