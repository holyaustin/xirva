[{"id": "1909.00047", "submitter": "Anis Elgabli", "authors": "Anis Elgabli, Jihong Park, Amrit S. Bedi, Mehdi Bennis, Vaneet\n  Aggarwal", "title": "GADMM: Fast and Communication Efficient Framework for Distributed\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT cs.NI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the data is distributed across multiple servers, lowering the\ncommunication cost between the servers (or workers) while solving the\ndistributed learning problem is an important problem and is the focus of this\npaper. In particular, we propose a fast, and communication-efficient\ndecentralized framework to solve the distributed machine learning (DML)\nproblem. The proposed algorithm, Group Alternating Direction Method of\nMultipliers (GADMM) is based on the Alternating Direction Method of Multipliers\n(ADMM) framework. The key novelty in GADMM is that it solves the problem in a\ndecentralized topology where at most half of the workers are competing for the\nlimited communication resources at any given time. Moreover, each worker\nexchanges the locally trained model only with two neighboring workers, thereby\ntraining a global model with a lower amount of communication overhead in each\nexchange. We prove that GADMM converges to the optimal solution for convex loss\nfunctions, and numerically show that it converges faster and more\ncommunication-efficient than the state-of-the-art communication-efficient\nalgorithms such as the Lazily Aggregated Gradient (LAG) and dual averaging, in\nlinear and logistic regression tasks on synthetic and real datasets.\nFurthermore, we propose Dynamic GADMM (D-GADMM), a variant of GADMM, and prove\nits convergence under the time-varying network topology of the workers.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 19:39:37 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 10:18:43 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 12:06:09 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Elgabli", "Anis", ""], ["Park", "Jihong", ""], ["Bedi", "Amrit S.", ""], ["Bennis", "Mehdi", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1909.00083", "submitter": "Run Chen", "authors": "Run Chen and Andrew L. Liu", "title": "A Distributed Algorithm for Large-scale Convex Quadratically Constrained\n  Quadratic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Jacobi-style distributed algorithm to solve convex,\nquadratically constrained quadratic programs (QCQPs), which arise from a broad\nrange of applications. While small to medium-sized convex QCQPs can be solved\nefficiently by interior-point algorithms, large-scale problems pose significant\nchallenges to traditional algorithms that are mainly designed to be implemented\non a single computing unit. The exploding volume of data (and hence, the\nproblem size), however, may overwhelm any such units. In this paper, we propose\na distributed algorithm for general, non-separable, large-scale convex QCQPs,\nusing a novel idea of predictor-corrector primal-dual update with an adaptive\nstep size. The algorithm enables distributed storage of data as well as\nparallel distributed computing. We establish the conditions for the proposed\nalgorithm to converge to a global optimum, and implement our algorithm on a\ncomputer cluster with multiple nodes using Message Passing Interface (MPI). The\nnumerical experiments are conducted on data sets of various scales from\ndifferent applications, and the results show that our algorithm exhibits\nfavorable scalability for solving large-scale problems.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 22:35:43 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 18:24:55 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 17:50:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Chen", "Run", ""], ["Liu", "Andrew L.", ""]]}, {"id": "1909.00084", "submitter": "Subru Krishnan", "authors": "Ashvin Agrawal, Rony Chatterjee, Carlo Curino, Avrilia Floratou, Neha\n  Gowdal, Matteo Interlandi, Alekh Jindal, Kostantinos Karanasos, Subru\n  Krishnan, Brian Kroth, Jyoti Leeka, Kwanghyun Park, Hiren Patel, Olga Poppe,\n  Fotis Psallidas, Raghu Ramakrishnan, Abhishek Roy, Karla Saur, Rathijit Sen,\n  Markus Weimer, Travis Wright, Yiwen Zhu", "title": "Cloudy with high chance of DBMS: A 10-year prediction for\n  Enterprise-Grade ML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning (ML) has proven itself in high-value web applications such\nas search ranking and is emerging as a powerful tool in a much broader range of\nenterprise scenarios including voice recognition and conversational\nunderstanding for customer support, autotuning for videoconferencing,\nintelligent feedback loops in large-scale sysops, manufacturing and autonomous\nvehicle management, complex financial predictions, just to name a few.\nMeanwhile, as the value of data is increasingly recognized and monetized,\nconcerns about securing valuable data and risks to individual privacy have been\ngrowing. Consequently, rigorous data management has emerged as a key\nrequirement in enterprise settings. How will these trends (ML growing\npopularity, and stricter data governance) intersect? What are the unmet\nrequirements for applying ML in enterprise settings? What are the technical\nchallenges for the DB community to solve? In this paper, we present our vision\nof how ML and database systems are likely to come together, and early steps we\ntake towards making this vision a reality.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 22:37:15 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 23:48:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Agrawal", "Ashvin", ""], ["Chatterjee", "Rony", ""], ["Curino", "Carlo", ""], ["Floratou", "Avrilia", ""], ["Gowdal", "Neha", ""], ["Interlandi", "Matteo", ""], ["Jindal", "Alekh", ""], ["Karanasos", "Kostantinos", ""], ["Krishnan", "Subru", ""], ["Kroth", "Brian", ""], ["Leeka", "Jyoti", ""], ["Park", "Kwanghyun", ""], ["Patel", "Hiren", ""], ["Poppe", "Olga", ""], ["Psallidas", "Fotis", ""], ["Ramakrishnan", "Raghu", ""], ["Roy", "Abhishek", ""], ["Saur", "Karla", ""], ["Sen", "Rathijit", ""], ["Weimer", "Markus", ""], ["Wright", "Travis", ""], ["Zhu", "Yiwen", ""]]}, {"id": "1909.00155", "submitter": "Lei He", "authors": "Shengwen Liang, Ying Wang, Member, IEEE, Cheng Liu, Lei He, Huawei Li,\n  Senior Member, IEEE, and, Xiaowei Li, Senior Member, IEEE", "title": "EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) emerge as a powerful approach to process\nnon-euclidean data structures and have been proved powerful in various\napplication domains such as social networks and e-commerce. While such graph\ndata maintained in real-world systems can be extremely large and sparse, thus\nemploying GNNs to deal with them requires substantial computational and memory\noverhead, which induces considerable energy and resource cost on CPUs and GPUs.\nIn this work, we present a specialized accelerator architecture, EnGN, to\nenable high-throughput and energy-efficient processing of large-scale GNNs. The\nproposed EnGN is designed to accelerate the three key stages of GNN\npropagation, which is abstracted as common computing patterns shared by typical\nGNNs. To support the key stages simultaneously, we propose the\nring-edge-reduce(RER) dataflow that tames the poor locality of\nsparsely-and-randomly connected vertices, and the RER PE-array to practice RER\ndataflow. In addition, we utilize a graph tiling strategy to fit large graphs\ninto EnGN and make good use of the hierarchical on-chip buffers through\nadaptive computation reordering and tile scheduling. Overall, EnGN achieves\nperformance speedup by 1802.9X, 19.75X, and 2.97X and energy efficiency by\n1326.35X, 304.43X, and 6.2X on average compared to CPU, GPU, and a\nstate-of-the-art GCN accelerator HyGCN, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 07:12:59 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 02:08:40 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 11:34:10 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Liang", "Shengwen", ""], ["Wang", "Ying", ""], ["Member", "", ""], ["IEEE", "", ""], ["Liu", "Cheng", ""], ["He", "Lei", ""], ["Li", "Huawei", ""], ["Member", "Senior", ""], ["IEEE", "", ""], ["and", "", ""], ["Li", "Xiaowei", ""], ["Member", "Senior", ""], ["IEEE", "", ""]]}, {"id": "1909.00394", "submitter": "Stanislav Polyakov", "authors": "Julia Dubenskaya and Stanislav Polyakov", "title": "Improving the Effective Utilization of Supercomputer Resources by Adding\n  Low-Priority Containerized Jobs", "comments": "11 pages, 5 figures", "journal-ref": "CEUR Workshop Proceedings. - 2019. - Vol. 2406. - P. 43-53", "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to utilize idle computational resources of\nsupercomputers. The idea is to maintain an additional queue of low-priority\nnon-parallel jobs and execute them in containers, using container migration\ntools to break the execution down into separate intervals. We propose a\ncontainer management system that can maintain this queue and interact with the\nsupercomputer scheduler. We conducted a series of experiments simulating\nsupercomputer scheduler and the proposed system. The experiments demonstrate\nthat the proposed system increases the effective utilization of supercomputer\nresources under most of the conditions, in some cases significantly improving\nthe performance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 13:25:38 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Dubenskaya", "Julia", ""], ["Polyakov", "Stanislav", ""]]}, {"id": "1909.00553", "submitter": "Prashant Jayaprakash Nair", "authors": "Seokin Hong, Bulent Abali, Alper Buyuktosunoglu, Michael B. Healy, and\n  Prashant J. Nair", "title": "Touch\\'e: Towards Ideal and Efficient Cache Compression By Mitigating\n  Tag Area Overheads", "comments": "Keywords: Compression, Caches, Tag Array, Data Array, Hashing", "journal-ref": "Proceedings of the 52nd Annual IEEE/ACM International Symposium on\n  Microarchitecture, October 2019, Pages 453-465", "doi": "10.1145/3352460.3358281", "report-no": null, "categories": "cs.AR cs.DC cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression is seen as a simple technique to increase the effective cache\ncapacity. Unfortunately, compression techniques either incur tag area overheads\nor restrict data placement to only include neighboring compressed cache blocks\nto mitigate tag area overheads. Ideally, we should be able to place arbitrary\ncompressed cache blocks without any placement restrictions and tag area\noverheads.\n  This paper proposes Touch\\'e, a framework that enables storing multiple\narbitrary compressed cache blocks within a physical cacheline without any tag\narea overheads. The Touch\\'e framework consists of three components. The first\ncomponent, called the ``Signature'' (SIGN) engine, creates shortened signatures\nfrom the tag addresses of compressed blocks. Due to this, the SIGN engine can\nstore multiple signatures in each tag entry. On a cache access, the physical\ncacheline is accessed only if there is a signature match (which has a\nnegligible probability of false positive). The second component, called the\n``Tag Appended Data'' (TADA) mechanism, stores the full tag addresses with\ndata. TADA enables Touch\\'e to detect false positive signature matches by\nensuring that the actual tag address is available for comparison. The third\ncomponent, called the ``Superblock Marker'' (SMARK) mechanism, uses a unique\nmarker in the tag entry to indicate the occurrence of compressed cache blocks\nfrom neighboring physical addresses in the same cacheline. Touch\\'e is\ncompletely hardware-based and achieves an average speedup of 12\\% (ideal 13\\%)\nwhen compared to an uncompressed baseline.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 05:39:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hong", "Seokin", ""], ["Abali", "Bulent", ""], ["Buyuktosunoglu", "Alper", ""], ["Healy", "Michael B.", ""], ["Nair", "Prashant J.", ""]]}, {"id": "1909.00560", "submitter": "Hailiang Zhao", "authors": "Shuiguang Deng, Hailiang Zhao, Weijia Fang, Jianwei Yin, Schahram\n  Dustdar, Albert Y. Zomaya", "title": "Edge Intelligence: The Confluence of Edge Computing and Artificial\n  Intelligence", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": "10.1109/JIOT.2020.2984887", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the rapid developments in communication technologies and the surge\nin the use of mobile devices, a brand-new computation paradigm, Edge Computing,\nis surging in popularity. Meanwhile, Artificial Intelligence (AI) applications\nare thriving with the breakthroughs in deep learning and the many improvements\nin hardware architectures. Billions of data bytes, generated at the network\nedge, put massive demands on data processing and structural optimization. Thus,\nthere exists a strong demand to integrate Edge Computing and AI, which gives\nbirth to Edge Intelligence. In this paper, we divide Edge Intelligence into AI\nfor edge (Intelligence-enabled Edge Computing) and AI on edge (Artificial\nIntelligence on Edge). The former focuses on providing more optimal solutions\nto key problems in Edge Computing with the help of popular and effective AI\ntechnologies while the latter studies how to carry out the entire process of\nbuilding AI models, i.e., model training and inference, on the edge. This paper\nprovides insights into this new inter-disciplinary field from a broader\nperspective. It discusses the core concepts and the research road-map, which\nshould provide the necessary background for potential future research\ninitiatives in Edge Intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 06:37:13 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 11:28:38 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Deng", "Shuiguang", ""], ["Zhao", "Hailiang", ""], ["Fang", "Weijia", ""], ["Yin", "Jianwei", ""], ["Dustdar", "Schahram", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1909.00562", "submitter": "Junya Ono", "authors": "Junya Ono, Masao Utiyama, Eiichiro Sumita", "title": "Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent\n  Neural Network Machine Translation", "comments": "9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduction of training time is an important issue in many tasks like patent\ntranslation involving neural networks. Data parallelism and model parallelism\nare two common approaches for reducing training time using multiple graphics\nprocessing units (GPUs) on one machine. In this paper, we propose a hybrid\ndata-model parallel approach for sequence-to-sequence (Seq2Seq) recurrent\nneural network (RNN) machine translation. We apply a model parallel approach to\nthe RNN encoder-decoder part of the Seq2Seq model and a data parallel approach\nto the attention-softmax part of the model. We achieved a speed-up of 4.13 to\n4.20 times when using 4 GPUs compared with the training speed when using 1 GPU\nwithout affecting machine translation accuracy as measured in terms of BLEU\nscores.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 06:41:34 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 06:12:02 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Ono", "Junya", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "1909.00709", "submitter": "Aur\\'elien Cavelan", "authors": "Aur\\'elien Cavelan and Florina M. Ciorba", "title": "Algorithm-Based Fault Tolerance for Parallel Stencil Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase in HPC systems size and complexity, together with increasing\non-chip transistor density, power limitations, and number of components, render\nmodern HPC systems subject to soft errors. Silent data corruptions (SDCs) are\ntypically caused by such soft errors in the form of bit-flips in the memory\nsubsystem and hinder the correctness of scientific applications. This work\naddresses the problem of protecting a class of iterative computational kernels,\ncalled stencils, against SDCs when executing on parallel HPC systems. Existing\nSDC detection and correction methods are in general either inaccurate,\ninefficient, or targeting specific application classes that do not include\nstencils. This work proposes a novel algorithm-based fault tolerance (ABFT)\nmethod to protect scientific applications that contain arbitrary stencil\ncomputations against SDCs. The ABFT method can be applied both online and\noffline to accurately detect and correct SDCs in 2D and 3D parallel stencil\ncomputations. We present a formal model for the proposed method including\ntheorems and proofs for the computation of the associated checksums as well as\nerror detection and correction. We experimentally evaluate the use of the\nproposed ABFT method on a real 3D stencil-based application (HotSpot3D) via a\nfault-injection, detection, and correction campaign. Results show that the\nproposed ABFT method achieves less than 8% overhead compared to the performance\nof the unprotected stencil application. Moreover, it accurately detects and\ncorrects SDCs. While the offline ABFT version corrects errors more accurately,\nit may incur a small additional overhead than its online counterpart.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 13:26:18 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Cavelan", "Aur\u00e9lien", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1909.00844", "submitter": "Krzysztof Nowicki", "authors": "Mohsen Ghaffari, Krzysztof Nowicki, Mikkel Thorup", "title": "Faster Algorithms for Edge Connectivity via Random $2$-Out Contractions", "comments": "algorithms and data structures, graph algorithms, edge connectivity,\n  out-contractions, randomized algorithms, distributed algorithms, massively\n  parallel computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple new randomized contraction approach to the global minimum\ncut problem for simple undirected graphs. The contractions exploit 2-out edge\nsampling from each vertex rather than the standard uniform edge sampling. We\ndemonstrate the power of our new approach by obtaining better algorithms for\nsequential, distributed, and parallel models of computation. Our end results\ninclude the following randomized algorithms for computing edge connectivity\nwith high probability:\n  -- Two sequential algorithms with complexities $O(m \\log n)$ and $O(m+n\n\\log^3 n)$. These improve on a long line of developments including a celebrated\n$O(m \\log^3 n)$ algorithm of Karger [STOC'96] and the state of the art $O(m\n\\log^2 n (\\log\\log n)^2)$ algorithm of Henzinger et al. [SODA'17]. Moreover,\nour $O(m+n \\log^3 n)$ algorithm is optimal whenever $m = \\Omega(n \\log^3 n)$.\nWithin our new time bounds, whp, we can also construct the cactus\nrepresentation of all minimal cuts.\n  -- An $\\~O(n^{0.8} D^{0.2} + n^{0.9})$ round distributed algorithm, where D\ndenotes the graph diameter. This improves substantially on a recent\nbreakthrough of Daga et al. [STOC'19], which achieved a round complexity of\n$\\~O(n^{1-1/353}D^{1/353} + n^{1-1/706})$, hence providing the first sublinear\ndistributed algorithm for exactly computing the edge connectivity.\n  -- The first $O(1)$ round algorithm for the massively parallel computation\nsetting with linear memory per machine.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:49:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Nowicki", "Krzysztof", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1909.00899", "submitter": "Roman Snytsar", "authors": "Roman Snytsar", "title": "De(con)struction of the lazy-F loop: improving performance of Smith\n  Waterman alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Striped variation of the Smith-Waterman algorithm is known as extremely\nefficient and easily adaptable for the SIMD architectures. However, the\npotential for improvement has not been exhausted yet. The popular Lazy-F loop\nheuristic requires additional memory access operations, and the worst-case\nperformance of the loop could be as bad as the nonvectorized version. We\ndemonstrate the progression of the lazy-F loop transformations that improve the\nloop performance, and ultimately eliminate the loop completely. Our algorithm\nachieves the best asymptotic performance of all scan-based SW algorithms\nO(n/p+log(p)), and is very efficient in practice.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 00:37:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Snytsar", "Roman", ""]]}, {"id": "1909.01149", "submitter": "Srinivas Eswar", "authors": "Srinivas Eswar, Koby Hayashi, Grey Ballard, Ramakrishnan Kannan,\n  Michael A. Matheson, Haesun Park", "title": "PLANC: Parallel Low Rank Approximation with Non-negativity Constraints", "comments": "arXiv admin note: text overlap with arXiv:1806.07985", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of low-rank approximation of massive dense\nnon-negative tensor data, for example to discover latent patterns in video and\nimaging applications. As the size of data sets grows, single workstations are\nhitting bottlenecks in both computation time and available memory. We propose a\ndistributed-memory parallel computing solution to handle massive data sets,\nloading the input data across the memories of multiple nodes and performing\nefficient and scalable parallel algorithms to compute the low-rank\napproximation. We present a software package called PLANC (Parallel Low Rank\nApproximation with Non-negativity Constraints), which implements our solution\nand allows for extension in terms of data (dense or sparse, matrices or tensors\nof any order), algorithm (e.g., from multiplicative updating techniques to\nalternating direction method of multipliers), and architecture (we exploit GPUs\nto accelerate the computation in this work).We describe our parallel\ndistributions and algorithms, which are careful to avoid unnecessary\ncommunication and computation, show how to extend the software to include new\nalgorithms and/or constraints, and report efficiency and scalability results\nfor both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 15:09:53 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Eswar", "Srinivas", ""], ["Hayashi", "Koby", ""], ["Ballard", "Grey", ""], ["Kannan", "Ramakrishnan", ""], ["Matheson", "Michael A.", ""], ["Park", "Haesun", ""]]}, {"id": "1909.01162", "submitter": "Zuphit Fidelman", "authors": "Zuphit Fidelman", "title": "A Generic Sharding Scheme for Blockchain Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis introduces a formal general framework for scaling blockchain\nprotocols by sharding. The framework is modular and it can be adjusted for\ndifferent needs or sets of assumptions. We prove that sharded protocols\nobtained by following our scheme (with correct modules in place) live up to the\nsame safety and liveness guarantees as their non-sharded counterparts. The\nproof is general and relies on well-defined specifications of certain\ncomponents. This lays the ground for simple proofs of correctness for sharded\nprotocols obtained by following the proposed scheme.\n  The framework is not left as an obscure specification of some high level\nstructure; explicit use is demonstrated by applying it to shard Algorand. As\npart of this concrete construction, a tamper-proof mechanism to assign nodes to\nshards is introduced. This mechanism is constructed by using verifiable random\nfunctions and can safely withstand a powerful adaptive adversary.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:25:55 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Fidelman", "Zuphit", ""]]}, {"id": "1909.01241", "submitter": "Chansup Byun", "authors": "Chansup Byun, Jeremy Kepner, William Arcand, David Bestor, Bill\n  Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones,\n  Anna Klein, Peter Michaleas, Julie Mullen, Andrew Prout, Antonio Rosa,\n  Siddharth Samsi, Charles Yee, Albert Reuther", "title": "Large Scale Parallelization Using File-Based Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel and new file-based communication\narchitecture using the local filesystem for large scale parallelization. This\nnew approach eliminates the issues with filesystem overload and resource\ncontention when using the central filesystem for large parallel jobs. The new\napproach incurs additional overhead due to inter-node message file transfers\nwhen both the sending and receiving processes are not on the same node.\nHowever, even with this additional overhead cost, its benefits are far greater\nfor the overall cluster operation in addition to the performance enhancement in\nmessage communications for large scale parallel jobs. For example, when running\na 2048-process parallel job, it achieved about 34 times better performance with\nMPI_Bcast() when using the local filesystem. Furthermore, since the security\nfor transferring message files is handled entirely by using the secure copy\nprotocol (scp) and the file system permissions, no additional security measures\nor ports are required other than those that are typically required on an HPC\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 15:13:06 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Byun", "Chansup", ""], ["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1909.01279", "submitter": "Philipp Witte", "authors": "Philipp A. Witte, Mathias Louboutin, Henryk Modzelewski, Charles\n  Jones, James Selvage, Felix J. Herrmann", "title": "An Event-Driven Approach to Serverless Seismic Imaging in the Cloud", "comments": "Submitted to IEEE Transactions on Parallel and Distributed Systems.\n  August 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting the cloud for high-performance computing (HPC) is a challenging\ntask, as software for HPC applications hinges on fast network connections and\nis sensitive to hardware failures. Using cloud infrastructure to recreate\nconventional HPC clusters is therefore in many cases an infeasible solution for\nmigrating HPC applications to the cloud. As an alternative to the generic lift\nand shift approach, we consider the specific application of seismic imaging and\ndemonstrate a serverless and event-driven approach for running large-scale\ninstances of this problem in the cloud. Instead of permanently running compute\ninstances, our workflow is based on a serverless architecture with high\nthroughput batch computing and event-driven computations, in which\ncomputational resources are only running as long as they are utilized. We\ndemonstrate that this approach is very flexible and allows for resilient and\nnested levels of parallelization, including domain decomposition for solving\nthe underlying partial differential equations. While the event-driven approach\nintroduces some overhead as computational resources are repeatedly restarted,\nit inherently provides resilience to instance shut-downs and allows a\nsignificant reduction of cost by avoiding idle instances, thus making the cloud\na viable alternative to on-premise clusters for large-scale seismic imaging.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:19:24 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Witte", "Philipp A.", ""], ["Louboutin", "Mathias", ""], ["Modzelewski", "Henryk", ""], ["Jones", "Charles", ""], ["Selvage", "James", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "1909.01392", "submitter": "Matheus D'E\\c{c}a Torquato De Melo", "authors": "Matheus Torquato, Marco Vieira", "title": "Towards Models for Availability and Security Evaluation of Cloud\n  Computing with Moving Target Defense", "comments": "Student Forum paper of the 15th European Dependable Computing\n  Conference (EDCC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security is one of the most relevant concerns in cloud computing. With the\nevolution of cyber-security threats, developing innovative techniques to thwart\nattacks is of utmost importance. One recent method to improve cloud computing\nsecurity is Moving Target Defense (MTD). MTD makes use of dynamic\nreconfiguration in virtualized environments to \"confuse\" attackers or to\nnullify their knowledge about the system state. However, there is still no\nconsolidated mechanism to evaluate the trade-offs between availability and\nsecurity when using MTD on cloud computing. The evaluation through measurements\nis complex as one needs to deal with unexpected events as failures and attacks.\nTo overcome this challenge, we intend to propose a set of models to evaluate\nthe availability and security of MTD in cloud computing environments. The\nexpected results include the quantification of availability and security levels\nunder different conditions (e.g., different software aging rates, varying\nworkloads, different attack intensities).\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:38:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Torquato", "Matheus", ""], ["Vieira", "Marco", ""]]}, {"id": "1909.01473", "submitter": "Qinmeng Zou", "authors": "Frederic Magoules and Qinmeng Zou", "title": "Asynchronous Time-Parallel Method based on Laplace Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laplace transform method has proved to be very efficient and easy to\nparallelize for the solution of time-dependent problems. However, the\nsynchronization delay among processors implies an upper bound on the expectable\nacceleration factor, which leads to a lot of wasted time. In this paper, we\npropose an original asynchronous Laplace transform method formalized for\nquasilinear problems based on the well-known Gaver-Stehfest algorithm. Parallel\nexperiments show the convergence of our new method, as well as several\ninteresting properties compared with the classical algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 22:19:46 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Magoules", "Frederic", ""], ["Zou", "Qinmeng", ""]]}, {"id": "1909.01597", "submitter": "Kristian Hinnenthal", "authors": "John Augustine, Kristian Hinnenthal, Fabian Kuhn, Christian\n  Scheideler, Philipp Schneider", "title": "Shortest Paths in a Hybrid Network Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a communication model for hybrid networks, where nodes have\naccess to two different communication modes: a local mode where communication\nis only possible between specific pairs of nodes, and a global mode where\ncommunication between any pair of nodes is possible. This can be motivated, for\ninstance, by wireless networks in which we combine direct device-to-device\ncommunication (e.g., using WiFi) with communication via a shared infrastructure\n(like base stations, the cellular network, or satellites).\n  Typically, communication over short-range connections is cheaper and can be\ndone at a much higher rate. Hence, we are focusing here on the LOCAL model (in\nwhich the nodes can exchange an unbounded amount of information in each round)\nfor the local connections while for the global communication we assume the\nso-called node-capacitated clique model, where in each round every node can\nexchange only $O(\\log n)$-bit messages with just $O(\\log n)$ other nodes.\n  In order to explore the power of combining local and global communication, we\nstudy the impact of hybrid communication on the complexity of computing\nshortest paths in the graph given by the local connections. Specifically, for\nthe all-pairs shortest paths problem (APSP), we show that an exact solution can\nbe computed in time $\\tilde O\\big(n^{2/3}\\big)$ and that approximate solutions\ncan be computed in time $\\tilde \\Theta\\big(\\!\\sqrt{n}\\big)$. For the\nsingle-source shortest paths problem (SSSP), we show that an exact solution can\nbe computed in time $\\tilde O\\big(\\!\\sqrt{\\mathsf{SPD}}\\big)$, where\n$\\mathsf{SPD}$ denotes the shortest path diameter. We further show that a\n$\\big(1\\!+\\!o(1)\\big)$-approximate solution can be computed in time $\\tilde\nO\\big(n^{1/3}\\big)$. Additionally, we show that for every constant\n$\\varepsilon>0$, it is possible to compute an $O(1)$-approximate solution in\ntime $\\tilde O(n^\\varepsilon)$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 07:50:40 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Augustine", "John", ""], ["Hinnenthal", "Kristian", ""], ["Kuhn", "Fabian", ""], ["Scheideler", "Christian", ""], ["Schneider", "Philipp", ""]]}, {"id": "1909.01756", "submitter": "Srinivas Kota Reddy", "authors": "Kota Srinivas Reddy and Nikhil Karamchandani", "title": "Rate-Memory Trade-off for Multi-access Coded Caching with Uncoded\n  Placement", "comments": "Accepted in Transactions on Communications. Preliminary works\n  appeared and presented in SPCOM and ISIT", "journal-ref": null, "doi": "10.1109/TCOMM.2020.2980817", "report-no": null, "categories": "cs.IT cs.DC cs.IR math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a multi-access variant of the popular coded caching framework, which\nconsists of a central server with a catalog of $N$ files, $K$ caches with\nlimited memory $M$, and $K$ users such that each user has access to $L$\nconsecutive caches with a cyclic wrap-around and requests one file from the\ncentral server's catalog. The server assists in file delivery by transmitting a\nmessage of size $R$ over a shared error-free link and the goal is to\ncharacterize the optimal rate-memory trade-off. This setup was studied\npreviously by Hachem et al., where an achievable rate and an\ninformation-theoretic lower bound were derived. However, the multiplicative gap\nbetween them was shown to scale linearly with the access degree $L$ and thus\norder-optimality could not be established.\n  A series of recent works have used a natural mapping of the coded caching\nproblem to the well-known index coding problem to derive tighter\ncharacterizations of the optimal rate-memory trade-off under the additional\nassumption that the caches store uncoded content. We follow a similar strategy\nfor the multi-access framework and provide new bounds for the optimal\nrate-memory trade-off $R^*(M)$ over all uncoded placement policies. In\nparticular, we derive a new achievable rate for any $L \\ge 1$ and a new lower\nbound, which works for any uncoded placement policy and $L \\ge K/2$. We then\nestablish that the (multiplicative) gap between the new achievable rate and the\nlower bound is at most $2$ independent of all parameters, thus establishing an\norder-optimal characterization of $R^*(M)$ for any $L\\ge K/2$. This is a\nsignificant improvement over the previously known gap result, albeit under the\nrestriction of uncoded placement policies. Finally, we also characterize\n$R^*(M)$ exactly for a few special cases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:56:40 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 18:55:17 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Reddy", "Kota Srinivas", ""], ["Karamchandani", "Nikhil", ""]]}, {"id": "1909.01786", "submitter": "Andrea Formisano", "authors": "Agostino Dovier and Andrea Formisano and Flavio Vella", "title": "GPU-based parallelism for ASP-solving", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) has become, the paradigm of choice in the field\nof logic programming and non-monotonic reasoning. Thanks to the availability of\nefficient solvers, ASP has been successfully employed in a large number of\napplication domains. The term GPU-computing indicates a recent programming\nparadigm aimed at enabling the use of modern parallel Graphical Processing\nUnits (GPUs) for general purpose computing. In this paper we describe an\napproach to ASP-solving that exploits GPU parallelism. The design of a\nGPU-based solver poses various challenges due to the peculiarities of GPUs'\nsoftware and hardware architectures and to the intrinsic nature of the\nsatisfiability problem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 13:28:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Dovier", "Agostino", ""], ["Formisano", "Andrea", ""], ["Vella", "Flavio", ""]]}, {"id": "1909.01957", "submitter": "Anisur Molla Rahaman", "authors": "Ajay D. Kshemkalyani and Anisur Rahaman Molla and Gokarna Sharma", "title": "Dispersion of Mobile Robots in the Global Communication Model", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dispersion problem on graphs asks $k\\leq n$ robots placed initially\narbitrarily on the nodes of an $n$-node anonymous graph to reposition\nautonomously to reach a configuration in which each robot is on a distinct node\nof the graph. This problem is of significant interest due to its relationship\nto other fundamental robot coordination problems, such as exploration,\nscattering, load balancing etc. In this paper, we consider dispersion in the\n{\\em global communication} model where a robot can communicate with any other\nrobot in the graph (but the graph is unknown to robots). We provide three novel\ndeterministic algorithms, two for arbitrary graphs and one for arbitrary trees,\nin a synchronous setting where all robots perform their actions in every time\nstep. For arbitrary graphs, our first algorithm is based on a DFS traversal and\nguarantees $O(\\min(m,k\\Delta))$ steps runtime using $\\Theta(\\log\n(\\max(k,\\Delta)))$ bits at each robot, where $m$ is the number of edges and\n$\\Delta$ is the maximum degree of the graph. The second algorithm for arbitrary\ngraphs is based on a BFS traversal and guarantees $O( \\max(D,k) \\Delta\n(D+\\Delta))$ steps runtime using $O(\\max(D,\\Delta \\log k))$ bits at each robot,\nwhere $D$ is the diameter of the graph. The algorithm for arbitrary trees is\nalso based on a BFS travesal and guarantees $O(D\\max(D,k))$ steps runtime using\n$O(\\max(D,\\Delta \\log k))$ bits at each robot. Our results are significant\nimprovements compared to the existing results established in the {\\em local\ncommunication} model where a robot can communication only with other robots\npresent at the same node. Particularly, the DFS-based algorithm is optimal for\nboth memory and time in constant-degree arbitrary graphs. The BFS-based\nalgorithm for arbitrary trees is optimal with respect to runtime when $k\\leq\nO(D)$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:33:09 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Kshemkalyani", "Ajay D.", ""], ["Molla", "Anisur Rahaman", ""], ["Sharma", "Gokarna", ""]]}, {"id": "1909.01980", "submitter": "Duong Nguyen", "authors": "Duong Nguyen, Aleksey Charapko, Sandeep S Kulkarni and Murat Demirbas", "title": "Using Weaker Consistency Models with Monitoring and Recovery for\n  Improving Performance of Key-Value Stores", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.11453,\n  arXiv:1801.07319", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency properties provided by most key-value stores can be classified\ninto sequential consistency and eventual consistency. The former is easier to\nprogram with but suffers from lower performance whereas the latter suffers from\npotential anomalies while providing higher performance. We focus on the problem\nof what a designer should do if he/she has an algorithm that works correctly\nwith sequential consistency but is faced with an underlying key-value store\nthat provides a weaker consistency. We propose a detect-rollback based\napproach: The designer identifies a correctness predicate, say $P$, and\ncontinues to run the protocol, as our system monitors $P$. If $P$ is violated\n(because of weaker consistency), the system rolls back and resumes the\ncomputation at a state where $P$ holds.\n  We evaluate this approach with graph-based applications running on the\nVoldemort key-value store. Our experiments with deployment on Amazon AWS EC2\ninstances shows that using eventual consistency with monitoring can provide a\n$50\\%$ -- $80\\%$ increase in throughput when compared with sequential\nconsistency. We also observe that the overhead of the monitoring itself was low\n(typically less than $4\\%$) and the latency of detecting violations was small.\nIn particular, in a scenario designed to intentionally cause a large number of\nviolations, more than $99.9\\%$ of violations were detected in less than 50\nmilliseconds in regional networks, and in less than 3 seconds in global\nnetworks.\n  We find that for some applications, frequent rollback can cause the program\nusing eventual consistency to effectively \\textit{stall}. We propose alternate\nmechanisms for dealing with re-occurring rollbacks. Overall, for applications\nconsidered in this paper, we find that even with rollback, eventual consistency\nprovides better performance than using sequential consistency.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 03:44:14 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Nguyen", "Duong", ""], ["Charapko", "Aleksey", ""], ["Kulkarni", "Sandeep S", ""], ["Demirbas", "Murat", ""]]}, {"id": "1909.02053", "submitter": "Samuel S. Ogden", "authors": "Samuel S. Ogden and Tian Guo", "title": "ModiPick: SLA-aware Accuracy Optimization For Mobile Deep Inference", "comments": "11 pages (13 with citations), 9 figures. Expansion of work done for\n  PhD research qualifier presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile applications are increasingly leveraging complex deep learning models\nto deliver features, e.g., image recognition, that require high prediction\naccuracy. Such models can be both computation and memory-intensive, even for\nnewer mobile devices, and are therefore commonly hosted in powerful remote\nservers. However, current cloud-based inference services employ static model\nselection approach that can be suboptimal for satisfying application SLAs\n(service level agreements), as they fail to account for inherent dynamic mobile\nenvironment.\n  We introduce a cloud-based technique called ModiPick that dynamically selects\nthe most appropriate model for each inference request, and adapts its selection\nto match different SLAs and execution time budgets that are caused by variable\nmobile environments. The key idea of ModiPick is to make inference speed and\naccuracy trade-offs at runtime with a pool of managed deep learning models. As\nsuch, ModiPick masks unpredictable inference time budgets and therefore meets\nSLA targets, while improving accuracy within mobile network constraints. We\nevaluate ModiPick through experiments based on prototype systems and through\nsimulations. We show that ModiPick achieves comparable inference accuracy to a\ngreedy approach while improving SLA adherence by up to 88.5%.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 18:50:16 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Ogden", "Samuel S.", ""], ["Guo", "Tian", ""]]}, {"id": "1909.02061", "submitter": "Salem Alqahtani", "authors": "Salem Alqahtani and Murat Demirbas", "title": "Performance Analysis and Comparison of Distributed Machine Learning\n  Systems", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8846938", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has permeated through many aspects of computing/processing\nsystems in recent years. While distributed training architectures/frameworks\nare adopted for training large deep learning models quickly, there has not been\na systematic study of the communication bottlenecks of these architectures and\ntheir effects on the computation cycle time and scalability. In order to\nanalyze this problem for synchronous Stochastic Gradient Descent (SGD) training\nof deep learning models, we developed a performance model of computation time\nand communication latency under three different system architectures: Parameter\nServer (PS), peer-to-peer (P2P), and Ring allreduce (RA). To complement and\ncorroborate our analytical models with quantitative results, we evaluated the\ncomputation and communication performance of these system architectures of the\nsystems via experiments performed with Tensorflow and Horovod frameworks. We\nfound that the system architecture has a very significant effect on the\nperformance of training. RA-based systems achieve scalable performance as they\nsuccessfully decouple network usage from the number of workers in the system.\nIn contrast, 1PS systems suffer from low performance due to network congestion\nat the parameter server side. While P2P systems fare better than 1PS systems,\nthey still suffer from significant network bottleneck. Finally, RA systems also\nexcel by virtue of overlapping computation time and communication time, which\nPS and P2P architectures fail to achieve.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:09:38 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Alqahtani", "Salem", ""], ["Demirbas", "Murat", ""]]}, {"id": "1909.02092", "submitter": "Sanidhya Kashyap", "authors": "Sanidhya Kashyap and Dai Qin and Steve Byan and Virendra J. Marathe\n  and Sanketh Nalli", "title": "Correct, Fast Remote Persistence", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence of updates to remote byte-addressable persistent memory (PM),\nusing RDMA operations (RDMA updates), is a poorly understood subject.\nVisibility of RDMA updates on the remote server is not the same as persistence\nof those updates. The remote server's configuration has significant\nimplications on what it means for RDMA updates to be persistent on the remote\nserver. This leads to significant implications on methods needed to correctly\npersist those updates. This paper presents a comprehensive taxonomy of system\nconfigurations and the corresponding methods to ensure correct remote\npersistence of RDMA updates. We show that the methods for correct, fast remote\npersistence vary dramatically, with corresponding performance trade offs,\nbetween different remote server configurations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 20:28:40 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Kashyap", "Sanidhya", ""], ["Qin", "Dai", ""], ["Byan", "Steve", ""], ["Marathe", "Virendra J.", ""], ["Nalli", "Sanketh", ""]]}, {"id": "1909.02119", "submitter": "Subho Sankar Banerjee", "authors": "Subho S Banerjee, Saurabh Jha, Zbigniew T. Kalbarczyk, Ravishankar K.\n  Iyer", "title": "Inductive-bias-driven Reinforcement Learning For Efficient Schedules in\n  Heterogeneous Clusters", "comments": "Scheduling, Bayesian, POMDP, Sampling, Deep Reinforcement Learning,\n  Accelerators, FPGA, GPU", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning, Vienna, Austria, PMLR 119, 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of scheduling of workloads onto heterogeneous processors (e.g.,\nCPUs, GPUs, FPGAs) is of fundamental importance in modern data centers. Current\nsystem schedulers rely on application/system-specific heuristics that have to\nbe built on a case-by-case basis. Recent work has demonstrated ML techniques\nfor automating the heuristic search by using black-box approaches which require\nsignificant training data and time, which make them challenging to use in\npractice. This paper presents Symphony, a scheduling framework that addresses\nthe challenge in two ways: (i) a domain-driven Bayesian reinforcement learning\n(RL) model for scheduling, which inherently models the resource dependencies\nidentified from the system architecture; and (ii) a sampling-based technique to\ncompute the gradients of a Bayesian model without performing full probabilistic\ninference. Together, these techniques reduce both the amount of training data\nand the time required to produce scheduling policies that significantly\noutperform black-box approaches by up to 2.2x.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 21:19:14 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 11:10:04 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Banerjee", "Subho S", ""], ["Jha", "Saurabh", ""], ["Kalbarczyk", "Zbigniew T.", ""], ["Iyer", "Ravishankar K.", ""]]}, {"id": "1909.02125", "submitter": "Maruti Mudunuru", "authors": "B. Ahmmed, M. K. Mudunuru, S. Karra, S. C. James, H. S. Viswanathan,\n  and J. A. Dunbar", "title": "PFLOTRAN-SIP: A PFLOTRAN Module for Simulating Spectral-Induced\n  Polarization of Electrical Impedance Data", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.DC cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spectral induced polarization (SIP) is a non-intrusive geophysical method\nthat is widely used to detect sulfide minerals, clay minerals, metallic\nobjects, municipal wastes, hydrocarbons, and salinity intrusion. However, SIP\nis a static method that cannot measure the dynamics of flow and solute/species\ntransport in the subsurface. To capture these dynamics, the data collected with\nthe SIP technique needs to be coupled with fluid flow and reactive-transport\nmodels. To our knowledge, currently, there is no simulator in the open-source\nliterature that couples fluid flow, solute transport, and SIP process models to\nanalyze geoelectrical signatures in a large-scale system. A massively parallel\nsimulation framework (PFLOTRAN-SIP) was built to couple SIP data to fluid flow\nand solute transport processes. This framework built on the PFLOTRAN-E4D\nsimulator that couples PFLOTRAN and E4D, without sacrificing computational\nperformance. PFLOTRAN solves the coupled flow and solute transport process\nmodels to estimate solute concentrations, which were used in Archie's model to\ncompute bulk electrical conductivities at near-zero frequency. These bulk\nelectrical conductivities were modified using the Cole-Cole model to account\nfor frequency dependence. Using the estimated frequency-dependent bulk\nconductivities, E4D simulated the real and complex electrical potential signals\nfor selected frequencies for SIP. The PFLOTRAN-SIP framework was demonstrated\nthrough a synthetic tracer-transport model simulating tracer concentration and\nelectrical impedances for four frequencies. Later, SIP inversion estimated bulk\nelectrical conductivities by matching electrical impedances for each specified\nfrequency. The estimated bulk electrical conductivities were consistent with\nthe simulated tracer concentrations from the PFLOTRAN-SIP forward model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 21:43:46 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 01:18:17 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Ahmmed", "B.", ""], ["Mudunuru", "M. K.", ""], ["Karra", "S.", ""], ["James", "S. C.", ""], ["Viswanathan", "H. S.", ""], ["Dunbar", "J. A.", ""]]}, {"id": "1909.02127", "submitter": "Leyuan Wang", "authors": "Leyuan Wang and John D. Owens", "title": "Fast BFS-Based Triangle Counting on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method to compute triangle counting on\nGPUs. Unlike previous formulations of graph matching, our approach is BFS-based\nby traversing the graph in an all-source-BFS manner and thus can be mapped onto\nGPUs in a massively parallel fashion. Our implementation uses the Gunrock\nprogramming model and we evaluate our implementation in runtime and memory\nconsumption compared with previous state-of-the-art work. We sustain a peak\ntraversed-edges-per-second (TEPS) rate of nearly 10 GTEPS. Our algorithm is the\nmost scalable and parallel among all existing GPU implementations and also\noutperforms all existing CPU distributed implementations. This work\nspecifically focuses on leveraging our implementation on the triangle counting\nproblem for the Subgraph Isomorphism Graph Challenge 2019, demonstrating a\ngeometric mean speedup over the 2018 champion of 3.84x.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 21:46:51 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wang", "Leyuan", ""], ["Owens", "John D.", ""]]}, {"id": "1909.02362", "submitter": "Mehmet Emre Ozfatura", "authors": "Mehdi Salehi Heydar Abad and Emre Ozfatura and Deniz Gunduz and Ozgur\n  Ercetin", "title": "Hierarchical Federated Learning Across Heterogeneous Cellular Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study collaborative machine learning (ML) across wireless devices, each\nwith its own local dataset. Offloading these datasets to a cloud or an edge\nserver to implement powerful ML solutions is often not feasible due to latency,\nbandwidth and privacy constraints. Instead, we consider federated edge learning\n(FEEL), where the devices share local updates on the model parameters rather\nthan their datasets. We consider a heterogeneous cellular network (HCN), where\nsmall cell base stations (SBSs) orchestrate FL among the mobile users (MUs)\nwithin their cells, and periodically exchange model updates with the macro base\nstation (MBS) for global consensus. We employ gradient sparsification and\nperiodic averaging to increase the communication efficiency of this\nhierarchical federated learning (FL) framework. We then show using CIFAR-10\ndataset that the proposed hierarchical learning solution can significantly\nreduce the communication latency without sacrificing the model accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 12:42:38 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Abad", "Mehdi Salehi Heydar", ""], ["Ozfatura", "Emre", ""], ["Gunduz", "Deniz", ""], ["Ercetin", "Ozgur", ""]]}, {"id": "1909.02363", "submitter": "Shantenu Jha", "authors": "Geoffrey Fox, Shantenu Jha", "title": "Understanding ML driven HPC: Applications and Infrastructure", "comments": "Invited talk to \"Visionary Track\" at IEEE eScience 2019. arXiv admin\n  note: text overlap with arXiv:1806.04731 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently outlined the vision of \"Learning Everywhere\" which captures the\npossibility and impact of how learning methods and traditional HPC methods can\nbe coupled together. A primary driver of such coupling is the promise that\nMachine Learning (ML) will give major performance improvements for traditional\nHPC simulations. Motivated by this potential, the ML around HPC class of\nintegration is of particular significance. In a related follow-up paper, we\nprovided an initial taxonomy for integrating learning around HPC methods. In\nthis paper, which is part of the Learning Everywhere series, we discuss \"how\"\nlearning methods and HPC simulations are being integrated to enhance effective\nperformance of computations. This paper identifies several modes ---\nsubstitution, assimilation, and control, in which learning methods integrate\nwith HPC simulations and provide representative applications in each mode. This\npaper discusses some open research questions and we hope will motivate and\nclear the ground for MLaroundHPC benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 12:47:48 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Fox", "Geoffrey", ""], ["Jha", "Shantenu", ""]]}, {"id": "1909.02625", "submitter": "An Xu", "authors": "An Xu, Zhouyuan Huo, Heng Huang", "title": "Diversely Stale Parameters for Efficient Training of CNNs", "comments": "Layer-wise Staleness, Parallel Training, Convolutional Neural\n  Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The backpropagation algorithm is the most popular algorithm training neural\nnetworks nowadays. However, it suffers from the forward locking, backward\nlocking and update locking problems, especially when a neural network is so\nlarge that its layers are distributed across multiple devices. Existing\nsolutions either can only handle one locking problem or lead to severe accuracy\nloss or memory inefficiency. Moreover, none of them consider the straggler\nproblem among devices. In this paper, we propose Layer-wise Staleness and a\nnovel efficient training algorithm, Diversely Stale Parameters (DSP), which can\naddress all these challenges without loss of accuracy nor memory issue. We also\nanalyze the convergence of DSP with two popular gradient-based methods and\nprove that both of them are guaranteed to converge to critical points for\nnon-convex problems. Finally, extensive experimental results on training deep\nconvolutional neural networks demonstrate that our proposed DSP algorithm can\nachieve significant training speedup with stronger robustness and better\ngeneralization than compared methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:41:06 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 17:31:57 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Xu", "An", ""], ["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "1909.02712", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang and Keyou You", "title": "Decentralized Stochastic Gradient Tracking for Non-convex Empirical Risk\n  Minimization", "comments": "This paper has been revised and theoretical results are improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a decentralized stochastic gradient tracking (DSGT)\nalgorithm for non-convex empirical risk minimization problems over a\npeer-to-peer network of nodes, which is in sharp contrast to the existing DSGT\nonly for convex problems. To ensure exact convergence and handle the variance\namong decentralized datasets, each node performs a stochastic gradient (SG)\ntracking step by using a mini-batch of samples, where the batch size is\ndesigned to be proportional to the size of the local dataset. We explicitly\nevaluate the convergence rate of DSGT with respect to the number of iterations\nin terms of algebraic connectivity of the network, mini-batch size, gradient\nvariance, etc. Under certain conditions, we further show that DSGT has a\nnetwork independence property in the sense that the network topology only\naffects the convergence rate up to a constant factor. Hence, the convergence\nrate of DSGT can be comparable to the centralized SGD method. Moreover, a\nlinear speedup of DSGT with respect to the number of nodes is achievable for\nsome scenarios. Numerical experiments for neural networks and logistic\nregression problems on CIFAR-10 finally illustrate the advantages of DSGT.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 05:05:45 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 05:28:08 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 08:38:02 GMT"}, {"version": "v4", "created": "Fri, 28 Aug 2020 11:46:28 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Zhang", "Jiaqi", ""], ["You", "Keyou", ""]]}, {"id": "1909.02724", "submitter": "Mohamed Wahib", "authors": "Peng Chen, Mohamed Wahib, Shinichiro Takizawa, Ryousei Takano, Satoshi\n  Matsuoka", "title": "iFDK: A Scalable Framework for Instant High-resolution Image\n  Reconstruction", "comments": "ACM/IEEE Proceedings of the International Conference for High\n  Performance Computing, Networking, Storage and Analysis (SC'19)", "journal-ref": null, "doi": "10.1145/3295500.3356163", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed Tomography (CT) is a widely used technology that requires\ncompute-intense algorithms for image reconstruction. We propose a novel\nback-projection algorithm that reduces the projection computation cost to 1/6\nof the standard algorithm. We also propose an efficient implementation that\ntakes advantage of the heterogeneity of GPU-accelerated systems by overlapping\nthe filtering and back-projection stages on CPUs and GPUs, respectively.\nFinally, we propose a distributed framework for high-resolution image\nreconstruction on state-of-the-art GPU-accelerated supercomputers. The\nframework relies on an elaborate interleave of MPI collective communication\nsteps to achieve scalable communication. Evaluation on a single Tesla V100 GPU\ndemonstrates that our back-projection kernel performs up to 1.6x faster than\nthe standard FDK implementation. We also demonstrate the scalability and\ninstantaneous CT capability of the distributed framework by using up to 2,048\nV100 GPUs to solve 4K and 8K problems within 30 seconds and 2 minutes,\nrespectively (including I/O).\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 05:55:41 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Chen", "Peng", ""], ["Wahib", "Mohamed", ""], ["Takizawa", "Shinichiro", ""], ["Takano", "Ryousei", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1909.02765", "submitter": "Zhuoran Ji", "authors": "Zhuoran Ji", "title": "ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution\n  Neural Network Inference on Mobile GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural networks are widely used for mobile applications. However,\nGPU convolution algorithms are designed for mini-batch neural network training,\nthe single-image convolution neural network inference algorithm on mobile GPUs\nis not well-studied. After discussing the usage difference and examining the\nexisting convolution algorithms, we proposed the HNTMP convolution algorithm.\nThe HNTMP convolution algorithm achieves $14.6 \\times$ speedup than the most\npopular \\textit{im2col} convolution algorithm, and $2.30 \\times$ speedup than\nthe fastest existing convolution algorithm (direct convolution) as far as we\nknow.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 08:36:05 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 06:04:24 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ji", "Zhuoran", ""]]}, {"id": "1909.02791", "submitter": "Yuzhe Luo", "authors": "Yuzhe Luo, Xin Yu", "title": "An Automatic Debugging Tool of Instruction-Driven Multicore Systems with\n  Synchronization Points", "comments": "Have been sent to ITC_Asia 2019, ITC_Asia 2019 paper 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracing back the instruction execution sequence to debug a multicore system\ncan be very time-consuming because the relationships of the instructions can be\nvery complex. For instructions that cannot be checked by the environment\nimmediately after their executions, the errors they triggered can propagate\nthrough the instruction execution sequence. Our task is to find the\nerror-triggered instructions automatically. This paper presents an automatic\ndebugging tool that can leverage the synchronization points in the instruction\nexecution sequences of the multicore system being verified to locate the\ninstruction which results in simulation error automatically. To evaluate the\nperformance of the debugging tool, we analyze the complexity of the algorithms\nand count the number of instructions executed to locate the aimed instruction.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 09:27:38 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Luo", "Yuzhe", ""], ["Yu", "Xin", ""]]}, {"id": "1909.02852", "submitter": "Nachshon Cohen", "authors": "Yoav Zuriel, Michal Friedman, Gali Sheffi, Nachshon Cohen, Erez\n  Petrank", "title": "Efficient Lock-Free Durable Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-volatile memory is expected to co-exist or replace DRAM in upcoming\narchitectures. Durable concurrent data structures for non-volatile memories are\nessential building blocks for constructing adequate software for use with these\narchitectures. In this paper, we propose a new approach for durable concurrent\nsets and use this approach to build the most efficient durable hash tables\navailable today. Evaluation shows a performance improvement factor of up to\n3.3x over existing technology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:21:12 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 16:56:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zuriel", "Yoav", ""], ["Friedman", "Michal", ""], ["Sheffi", "Gali", ""], ["Cohen", "Nachshon", ""], ["Petrank", "Erez", ""]]}, {"id": "1909.02865", "submitter": "Muhammad Samir Khan", "authors": "Muhammad Samir Khan and Nitin Vaidya", "title": "Asynchronous Byzantine Consensus on Undirected Graphs under Local\n  Broadcast Model", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.11677", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we look at Byzantine consensus in asynchronous systems under the\nlocal broadcast model. In the local broadcast model, a message sent by any node\nis received identically by all of its neighbors in the communication network,\npreventing a faulty node from transmitting conflicting information to different\nneighbors. Our recent work has shown that in the synchronous setting, network\nconnectivity requirements for Byzantine consensus are lower under the local\nbroadcast model as compared to the classical point-to-point communication\nmodel. Here we show that the same is not true in the asynchronous setting, and\nthe network requirements for Byzantine consensus stays the same under local\nbroadcast as under point-to-point communication model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 21:23:24 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Khan", "Muhammad Samir", ""], ["Vaidya", "Nitin", ""]]}, {"id": "1909.02871", "submitter": "Stephan G\\\"unther", "authors": "Stephan M. G\\\"unther, Nicolas Appel and Georg Carle", "title": "Galois Field Arithmetics for Linear Network Coding using AVX512\n  Instruction Set Extensions", "comments": "6 pages, 2 figures, the updated finite field library is available\n  under the LGPL at https://moep80211.net/plink/libmoepgf-avx512", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear network coding requires arithmetic operations over Galois fields, more\nspecifically over finite extension fields. While coding over GF(2) reduces to\nsimple XOR operations, this field is less preferred for practical applications\nof random linear network coding due to high chances of linear dependencies and\ntherefore redundant coded packets. Coding over larger fields such as GF(16) and\nGF(256) does not have that issue, but is significantly slower. SIMD vector\nextensions of processors such as AVX2 on x86-based systems or NEON on ARM-based\ndevices offer the potential to increase performance by orders of magnitude.\n  In this paper we present an implementation of different algorithms and Galois\nfields based on the AVX512 instruction set extension and integrate it into the\nfinite field library libmoepgf. We compare the performance of the new\nimplementation to the reference implementation based on AVX2, showing a\nsignificant increase in throughput. In addition, we provide a survey of the\nbest possible coding performance offered by a variety of different platforms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 09:20:59 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["G\u00fcnther", "Stephan M.", ""], ["Appel", "Nicolas", ""], ["Carle", "Georg", ""]]}, {"id": "1909.02873", "submitter": "Felix Ongati", "authors": "Felix Ongati and Dr. Eng. Lawrence Muchemi", "title": "Big Data Intelligence Using Distributed Deep Neural Networks", "comments": "Computational Intelligence Postgraduate Research Project. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large amount of data is often required to train and deploy useful machine\nlearning models in industry. Smaller enterprises do not have the luxury of\naccessing enough data for machine learning, For privacy sensitive fields such\nas banking, insurance and healthcare, aggregating data to a data warehouse\nposes a challenge of data security and limited computational resources. These\nchallenges are critical when developing machine learning algorithms in\nindustry. Several attempts have been made to address the above challenges by\nusing distributed learning techniques such as federated learning over disparate\ndata stores in order to circumvent the need for centralised data aggregation.\nThis paper proposes an improved algorithm to securely train deep neural\nnetworks over several data sources in a distributed way, in order to eliminate\nthe need to centrally aggregate the data and the need to share the data thus\npreserving privacy. The proposed method allows training of deep neural networks\nusing data from multiple de-linked nodes in a distributed environment and to\nsecure the representation shared during training. Only a representation of the\ntrained models (network architecture and weights) are shared. The algorithm was\nevaluated on existing healthcare patients data and the performance of this\nimplementation was compared to that of a regular deep neural network trained on\na single centralised architecture. This algorithm will pave a way for\ndistributed training of neural networks on privacy sensitive applications where\nraw data may not be shared directly or centrally aggregating this data in a\ndata warehouse is not feasible.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 06:52:52 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Ongati", "Felix", ""], ["Muchemi", "Dr. Eng. Lawrence", ""]]}, {"id": "1909.02895", "submitter": "Zolt\\'an Andr\\'as Lux", "authors": "Zolt\\'an Andr\\'as Lux, Felix Beierle, Sebastian Zickau, Sebastian\n  G\\\"ond\\\"or", "title": "Full-text Search for Verifiable Credential Metadata on Distributed\n  Ledgers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-sovereign Identity (SSI) powered by distributed ledger technologies\nenables more flexible and faster digital identification workflows, while at the\nsame time limiting the control and influence of central authorities. However, a\nglobal identity solution must be able to handle myriad credential types from\nmillions of issuing organizations. As metadata about types of digital\ncredentials is readable by everyone on the public permissioned ledger with\nHyperledger Indy, anyone could find relevant and trusted credential types for\ntheir use cases by looking at the records on the blockchain. To this date, no\nefficient full-text search mechanism exists that would allow users to search\nfor credential types in a simple and efficient fashion tightly integrated into\ntheir applications. In this work, we propose a full-text search framework based\non the publicly available metadata on the Hyperledger Indy ledger for\nretrieving matching credential types. The proposed solution is able to find\ncredential types based on textual input from the user by using a full-text\nsearch engine and maintaining a local copy of the ledger. Thus, we do not need\nto rely on information about credentials coming from a very large candidate\npool of third parties we would need to trust, such as the website of a company\ndisplaying its own identifier and a list of issued credentials. We have also\nproven the feasiblity of the concept by implementing and evaluating a prototype\nof the full-text credential metadata search service.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:31:13 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Lux", "Zolt\u00e1n Andr\u00e1s", ""], ["Beierle", "Felix", ""], ["Zickau", "Sebastian", ""], ["G\u00f6nd\u00f6r", "Sebastian", ""]]}, {"id": "1909.02986", "submitter": "Aryaman Gupta", "authors": "Aryaman Gupta, Ulrik G\\\"unther, Pietro Incardona, Ata Deniz Aydin,\n  Raimund Dachselt, Stefan Gumhold, Ivo F. Sbalzarini", "title": "A Proposed Framework for Interactive Virtual Reality In Situ\n  Visualization of Parallel Numerical Simulations", "comments": "2 pages, 2 figures, accepted at IEEE Large Data Analysis and\n  Visualization (LDAV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computer simulations progress to increasingly complex, non-linear, and\nthree-dimensional systems and phenomena, intuitive and immediate visualization\nof their results is becoming crucial. While Virtual Reality (VR) and Natural\nUser Interfaces (NUIs) have been shown to improve understanding of complex 3D\ndata, their application to live in situ visualization and computational\nsteering is hampered by performance requirements. Here, we present the design\nof a software framework for interactive VR in situ visualization of parallel\nnumerical simulations, as well as a working prototype implementation. Our\ndesign is targeted towards meeting the performance requirements for VR, and our\nwork is packaged in a framework that allows for easy instrumentation of\nsimulations. Our preliminary results inform about the technical feasibility of\nthe architecture, as well as the challenges that remain.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:03:12 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Gupta", "Aryaman", ""], ["G\u00fcnther", "Ulrik", ""], ["Incardona", "Pietro", ""], ["Aydin", "Ata Deniz", ""], ["Dachselt", "Raimund", ""], ["Gumhold", "Stefan", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1909.03026", "submitter": "Jonas Traub", "authors": "Jonas Traub, Jorge-Arnulfo Quian\\'e-Ruiz, Zoi Kaoudi, Volker Markl\n  (Technische Universit\\\"at Berlin, German Research Center for Artificial\n  Intelligence (DFKI))", "title": "Agora: A Unified Asset Ecosystem Going Beyond Marketplaces and Cloud\n  Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data, algorithms, and compute/storage infrastructure are key assets that\ndrive data science and artificial intelligence applications. As providing all\nthese assets requires a huge investment, data science and artificial\nintelligence technologies are currently dominated by a small number of\nproviders who can afford these investments. This leads to lock-in effects and\nhinders features that require a flexible exchange of assets among users. In\nthis vision paper, we present Agora, a unified asset ecosystem. The Agora\nsystem provides the technical infrastructure that allows for offering and using\ndata and algorithms, as well as physical infrastructure components. Agora is\ndesigned as an open ecosystem of asset marketplaces and provides to a broad\naudience not only data but the entire data value chain (including computational\nresources and human expertise). Agora (i) leverages a fine-grained exchange of\nassets, (ii) allows for combining assets to novel applications, and (iii)\nflexibly executes such applications on available resources. As a result, Agora\novercomes lock-in effects and removes entry barriers for new asset providers.\nIn contrast to existing data management systems, Agora operates in a heavily\ndecentralized and dynamic environment: Data, algorithms, and even compute\nresources are dynamically created, modified, and removed by different\nstakeholders. Agora presents novel research directions for the data management\ncommunity as a whole: It requires to combine our traditional expertise in\nscalable data processing and management with infrastructure provisioning as\nwell as economic and application aspects of data, algorithms, and\ninfrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 17:22:28 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 14:35:22 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 07:23:17 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Traub", "Jonas", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Kaoudi", "Zoi", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Markl", "Volker", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"]]}, {"id": "1909.03057", "submitter": "Matteo Turilli", "authors": "Matteo Turilli, Andre Merzky, Thomas Naughton, Wael Elwasif, Shantenu\n  Jha", "title": "Characterizing the Performance of Executing Many-tasks on Summit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific workloads are comprised of many tasks, where each task is an\nindependent simulation or analysis of data. The execution of millions of tasks\non heterogeneous HPC platforms requires scalable dynamic resource management\nand multi-level scheduling. RADICAL-Pilot (RP) -- an implementation of the\nPilot abstraction, addresses these challenges and serves as an effective\nruntime system to execute workloads comprised of many tasks. In this paper, we\ncharacterize the performance of executing many tasks using RP when interfaced\nwith JSM and PRRTE on Summit: RP is responsible for resource management and\ntask scheduling on acquired resource; JSM or PRRTE enact the placement of\nlaunching of scheduled tasks. Our experiments provide lower bounds on the\nperformance of RP when integrated with JSM and PRRTE. Specifically, for\nworkloads comprised of homogeneous single-core, 15 minutes-long tasks we find\nthat: PRRTE scales better than JSM for > O(1000) tasks; PRRTE overheads are\nnegligible; and PRRTE supports optimizations that lower the impact of overheads\nand enable resource utilization of 63% when executing O(16K), 1-core tasks over\n404 compute nodes.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 20:58:19 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Turilli", "Matteo", ""], ["Merzky", "Andre", ""], ["Naughton", "Thomas", ""], ["Elwasif", "Wael", ""], ["Jha", "Shantenu", ""]]}, {"id": "1909.03111", "submitter": "Omar S Navarro Leija", "authors": "Omar S Navarro Leija, Alan Jeffrey", "title": "Lightweight Record-and-Replay for Intermittent Tests Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present lightweight record-and-replay (RR). In contrast to\ntraditional \"fully deterministic\" RR solutions, lightweight RR focuses on\nhandling nondeterminism arising from thread communication for programs with\nconcurrent, message-passing architectures. By decreasing nondeterminism in\nprograms, lightweight RR decreases the number of intermittent failures in\nprogram's test suites. We evaluated the effectiveness of lightweight RR on\nServo, a highly concurrent web browser. Our evaluation shows lightweight RR is\neffective at greatly reducing intermittent failures for some tests, but not\nothers. Lightweight RR performance overhead remains a work in progress, but log\nsizes are quite small. We believe with further work lightweight RR could prove\nuseful for lowering nondeterminism in programs at a negligible performance\noverhead.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 20:00:17 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Leija", "Omar S Navarro", ""], ["Jeffrey", "Alan", ""]]}, {"id": "1909.03130", "submitter": "Lalith Suresh", "authors": "Lalith Suresh, Joao Loff, Faria Kalim, Nina Narodytska, Leonid Ryzhyk,\n  Sahan Gamage, Brian Oki, Zeeshan Lokhandwala, Mukesh Hira, Mooly Sagiv", "title": "Automating Cluster Management with Weave", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern cluster management systems like Kubernetes and Openstack grapple with\nhard combinatorial optimization problems: load balancing, placement,\nscheduling, and configuration. Currently, developers tackle these problems by\ndesigning custom application-specific algorithms---an approach that is proving\nunsustainable, as ad-hoc solutions both perform poorly and introduce\noverwhelming complexity to the system, making it challenging to add important\nnew features.\n  We propose a radically different architecture, where programmers drive\ncluster management tasks declaratively, using SQL queries over cluster state\nstored in a relational database. These queries capture in a natural way both\nconstraints on the cluster configuration as well as optimization objectives.\nWhen a cluster reconfiguration is required at runtime, our tool, called Weave,\nsynthesizes an encoding of these queries into an optimization model, which it\nsolves using an off-the-shelf solver.\n  We demonstrate Weave's efficacy by powering three production-grade systems\nwith it: a Kubernetes scheduler, a virtual machine management solution, and a\ndistributed transactional datastore. Using Weave, we expressed complex cluster\nmanagement policies in under 20 lines of SQL, easily added new features to\nthese existing systems, and significantly improved placement quality and\nconvergence times.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 21:13:36 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Suresh", "Lalith", ""], ["Loff", "Joao", ""], ["Kalim", "Faria", ""], ["Narodytska", "Nina", ""], ["Ryzhyk", "Leonid", ""], ["Gamage", "Sahan", ""], ["Oki", "Brian", ""], ["Lokhandwala", "Zeeshan", ""], ["Hira", "Mukesh", ""], ["Sagiv", "Mooly", ""]]}, {"id": "1909.03167", "submitter": "Rohan Achar", "authors": "Rohan Achar, Pritha Dawn and Cristina V. Lopes", "title": "GoTcha: An Interactive Debugger for GoT-Based Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Debugging distributed systems is hard. Most of the techniques that have been\ndeveloped for debugging such systems use either extensive model checking, or\npostmortem analysis of logs and traces. Interactive debugging is typically a\ntool that is only effective in single threaded and single process applications,\nand is rarely applied to distributed systems. While the live observation of\nstate changes using interactive debuggers is effective, it comes with a host of\nproblems in distributed scenarios. In this paper, we discuss the requirements\nan interactive debugger for distributed systems should meet, the role the\nunderlying distributed model plays in facilitating the debugger, and the\nimplementation of our interactive debugger: GoTcha. GoTcha is a browser based\ninteractive debugger for distributed systems built on the Global Object Tracker\n(GoT) programming model. We show how the GoT model facilitates the debugger,\nand the features that the debugger can offer. We also demonstrate a typical\ndebugging workflow.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 01:53:45 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Achar", "Rohan", ""], ["Dawn", "Pritha", ""], ["Lopes", "Cristina V.", ""]]}, {"id": "1909.03231", "submitter": "Tiziano De Matteis", "authors": "Tiziano De Matteis, Johannes de Fine Licht, Jakub Ber\\'anek and\n  Torsten Hoefler", "title": "Streaming Message Interface: High-Performance Distributed Memory\n  Programming on Reconfigurable Hardware", "comments": null, "journal-ref": null, "doi": "10.1145/3295500.3356201", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed memory programming is the established paradigm used in\nhigh-performance computing (HPC) systems, requiring explicit communication\nbetween nodes and devices. When FPGAs are deployed in distributed settings,\ncommunication is typically handled either by going through the host machine,\nsacrificing performance, or by streaming across fixed device-to-device\nconnections, sacrificing flexibility. We present Streaming Message Interface\n(SMI), a communication model and API that unifies explicit message passing with\na hardware-oriented programming model, facilitating minimal-overhead, flexible,\nand productive inter-FPGA communication. Instead of bulk transmission, messages\nare streamed across the network during computation, allowing communication to\nbe seamlessly integrated into pipelined designs. We present a high-level\nsynthesis implementation of SMI targeting a dedicated FPGA interconnect,\nexposing runtime-configurable routing with support for arbitrary network\ntopologies, and implement a set of distributed memory benchmarks. Using SMI,\nprogrammers can implement distributed, scalable HPC programs on reconfigurable\nhardware, without deviating from best practices for hardware design.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 09:53:54 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["De Matteis", "Tiziano", ""], ["Licht", "Johannes de Fine", ""], ["Ber\u00e1nek", "Jakub", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1909.03263", "submitter": "Eric Goubault", "authors": "Eric Goubault and Marijana Lazic and Jeremy Ledent and Sergio Rajsbaum", "title": "A dynamic epistemic logic analysis of the equality negation task", "comments": null, "journal-ref": "Long version of DaLI 2019 conference paper", "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the solvability of the equality negation task in a\nsimple wait-free model where processes communicate by reading and writing\nshared variables or exchanging messages. In this task, two processes start with\na private input value in the set {0,1,2}, and after communicating, each one\nmust decide a binary output value, so that the outputs of the processes are the\nsame if and only if the input values of the processes are different. This task\nis already known to be unsolvable; our goal here is to prove this result using\nthe dynamic epistemic logic (DEL) approach introduced by Goubault, Ledent and\nRajsbaum in GandALF 2018. We show that in fact, there is no epistemic logic\nformula that explains why the task is unsolvable. We fix this issue by\nextending the language of our DEL framework, which allows us to construct such\na formula, and discuss its utility.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 12:45:14 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Goubault", "Eric", ""], ["Lazic", "Marijana", ""], ["Ledent", "Jeremy", ""], ["Rajsbaum", "Sergio", ""]]}, {"id": "1909.03296", "submitter": "Ege Korkan", "authors": "Ege Korkan, Hassib Belhaj Hassine, Verena Eileen Schlott, Sebastian\n  K\\\"abisch, Sebastian Steinhorst", "title": "WoTify: A platform to bring Web of Things to your devices", "comments": "6 pages, 9 figures, Second W3C Workshop on the Web of Things, 3-5\n  June 2019, Munich, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) has already taken off, together with many Web of\nThings (WoT) off-the-shelf devices, such as Philips Hue lights and platforms\nsuch as Azure IoT. These devices and platforms define their own way of\ndescribing the interactions with the devices and do not support the recently\npublished WoT standards by World Wide Web Consortium (W3C). On the other hand,\nmany hardware components that are popular in developer and maker communities\nlack a programming language independent platform to integrate these components\ninto the WoT, similar to npm and pip for software packages. To solve these\nproblems and nurture the adoption of the W3C WoT, in this paper, we propose a\nplatform to WoTify either existing hardware by downloading new software in them\nor already existing IoT and WoT devices by describing them with a Thing\nDescription.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 16:11:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Korkan", "Ege", ""], ["Hassine", "Hassib Belhaj", ""], ["Schlott", "Verena Eileen", ""], ["K\u00e4bisch", "Sebastian", ""], ["Steinhorst", "Sebastian", ""]]}, {"id": "1909.03314", "submitter": "John-Paul Robinson", "authors": "John-Paul Robinson, Thomas Anthony, Ravi Tripathi, Sara A. Sims,\n  Kristina M. Visscher, Purushotham V. Bangalore", "title": "Analyzing the HCP Datasets using GPUs: The Anatomy of a Science\n  Engagement", "comments": "6 pages, 3 figures, PEARC '18: Practice and Experience in Advanced\n  Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": null, "doi": "10.1145/3219104.3219163", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper documents the experience improving the performance of a data\nprocessing workflow for analysis of the Human Connectome Project's HCP900 data\nset. It describes how network and compute bottlenecks were discovered and\nresolved during the course of a science engagement. A series of computational\nenhancements to the stock FSL BedpostX workflow are described. These\nenhancements migrated the workflow from a slow serial execution of computations\nresulting from Slurm scheduler incompatibilities to eventual execution on GPU\nresources, going from a 21-day execution on a single CPU core to a 2 hour\nexecution on a GPU. This workflow contributed a vital use-case to the build-out\nof the campus compute cluster with additional GPUs and resulted in enhancements\nto network bandwidth. It also shares insights on potential improvements to\ndistribution of scientific software to avoid stagnation in site-specific\ndeployment decisions. The discussion highlights the advantages of open licenses\nand popular code collaboration sites like GitHub.com in feeding contributions\nupstream.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 18:12:52 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Robinson", "John-Paul", ""], ["Anthony", "Thomas", ""], ["Tripathi", "Ravi", ""], ["Sims", "Sara A.", ""], ["Visscher", "Kristina M.", ""], ["Bangalore", "Purushotham V.", ""]]}, {"id": "1909.03346", "submitter": "K. R. Jayaram", "authors": "K. R. Jayaram", "title": "Elastic Remote Methods", "comments": "ACM MIDDLEWARE 2013", "journal-ref": "Middleware 2013. Lecture Notes in Computer Science, vol 8275.\n  Springer, Berlin, Heidelberg", "doi": "10.1007/978-3-642-45065-5_8", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For distributed applications to take full advantage of cloud computing\nsystems, we need middleware systems\n  that allow developers to build elasticity management components right into\nthe applications.\n  This paper describes the design and implementation of ElasticRMI, a\nmiddleware system that (1) enables application developers to dynamically change\nthe number of (server) objects available to handle remote method invocations\nwith respect to the application's workload, without requiring major changes to\nclients (invokers) of remote methods, (2) enables flexible elastic scaling by\nallowing developers to use a combination of resource utilization metrics and\nfine-grained application-specific information like the properties of internal\ndata structures to drive scaling decisions, (3) provides a high-level\nprogramming framework that handles elasticity at the level of classes and\nobjects, masking low-level platform specific tasks (like provisioning VM\nimages) from the developer, and (4) increases the portability of ElasticRMI\napplications across different private data centers/IaaS clouds through Apache\nMesos.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 22:20:07 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Jayaram", "K. R.", ""]]}, {"id": "1909.03359", "submitter": "Gurbinder Gill", "authors": "Gurbinder Gill (1), Roshan Dathathri (1), Saeed Maleki (2), Madan\n  Musuvathi (2), Todd Mytkowicz (2), Olli Saarikivi (2) ((1) The University of\n  Texas at Austin, (2) Microsoft Research)", "title": "Distributed Training of Embeddings using Graph Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications today, such as NLP, network analysis, and code analysis,\nrely on semantically embedding objects into low-dimensional fixed-length\nvectors. Such embeddings naturally provide a way to perform useful downstream\ntasks, such as identifying relations among objects or predicting objects for a\ngiven context, etc. Unfortunately, the training necessary for accurate\nembeddings is usually computationally intensive and requires processing large\namounts of data. Furthermore, distributing this training is challenging. Most\nembedding training uses stochastic gradient descent (SGD), an \"inherently\"\nsequential algorithm. Prior approaches to parallelizing SGD do not honor these\ndependencies and thus potentially suffer poor convergence.\n  This paper presents a distributed training framework for a class of\napplications that use Skip-gram-like models to generate embeddings. We call\nthis class Any2Vec and it includes Word2Vec, DeepWalk, and Node2Vec among\nothers. We first formulate Any2Vec training algorithm as a graph application\nand leverage the state-of-the-art distributed graph analytics framework,\nD-Galois. We adapt D-Galois to support dynamic graph generation and\nrepartitioning, and incorporate novel communication optimizations. Finally, we\nintroduce a novel way to combine gradients during distributed training to\nprevent accuracy loss. We show that our framework, called GraphAny2Vec, matches\non a cluster of 32 hosts the accuracy of the state-of-the-art shared-memory\nimplementations of Word2Vec and Vertex2Vec on 1 host, and gives a geo-mean\nspeedup of 12x and 5x respectively. Furthermore, GraphAny2Vec is on average 2x\nfaster than the state-of-the-art distributed Word2Vec implementation, DMTK, on\n32 hosts. We also show the superiority of our Gradient Combiner independent of\nGraphAny2Vec by incorporating it in DMTK, which raises its accuracy by > 30%.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 01:06:03 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 00:34:44 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gill", "Gurbinder", ""], ["Dathathri", "Roshan", ""], ["Maleki", "Saeed", ""], ["Musuvathi", "Madan", ""], ["Mytkowicz", "Todd", ""], ["Saarikivi", "Olli", ""]]}, {"id": "1909.03432", "submitter": "Moshe Sulamy", "authors": "Yehuda Afek and Itay Harel and Amit Jacob-Fanani and Moshe Sulamy", "title": "Consensus in Equilibrium: Can One Against All Decide Fairly?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is there an equilibrium for distributed consensus when all agents except one\ncollude to steer the decision value towards their preference? If an equilibrium\nexists, then an $n-1$ size coalition cannot do better by deviating from the\nalgorithm, even if it prefers a different decision value. We show that an\nequilibrium exists under this condition only if the number of agents in the\nnetwork is odd and the decision is binary (among two possible input values).\nThat is, in this framework we provide a separation between binary and\nmulti-valued consensus. Moreover, the input and output distribution must be\nuniform, regardless of the communication model (synchronous or asynchronous).\nFurthermore, we define a new problem - Resilient Input Sharing (RIS), and use\nit to find an {\\em iff} condition for the $(n-1)$-resilient equilibrium for\ndeterministic binary consensus, essentially showing that an equilibrium for\ndeterministic consensus is equivalent to each agent learning all the other\ninputs in some strong sense. Finally, we note that $(n-2)$-resilient\nequilibrium for binary consensus is possible for any $n$. The case of\n$(n-2)$-resilient equilibrium for \\emph{multi-valued} consensus is left open.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 11:29:15 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Afek", "Yehuda", ""], ["Harel", "Itay", ""], ["Jacob-Fanani", "Amit", ""], ["Sulamy", "Moshe", ""]]}, {"id": "1909.03518", "submitter": "Fabrizio Romano Genovese", "authors": "Fabrizio Genovese", "title": "The Essence of Petri Net Gluings", "comments": "20 pages, 4 pages appendix, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.DC cs.FL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many categorical frameworks have been proposed to formalize the idea of\ngluing Petri nets with each other. Such frameworks model net gluings in terms\nof sharing of resources or synchronization of transitions. Interpretations\ngiven to these gluings are more or less satisfactory when we consider Petri\nnets with a semantics attached to them.\n  In this work, we define a framework to compose Petri nets together in such a\nway that their semantics is respected. In addition to this, we show how our\nframework generalizes the previously defined ones.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 17:33:08 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 11:52:24 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Genovese", "Fabrizio", ""]]}, {"id": "1909.03555", "submitter": "Maciej Pawlik", "authors": "Maciej Pawlik, Kamil Figiela, Maciej Malawski", "title": "Performance considerations on execution of large scale workflow\n  applications on cloud functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function-as-a-Service is a novel type of cloud service used for creating\ndistributed applications and utilizing computing resources. Application\ndeveloper supplies source code of cloud functions, which are small applications\nor application components, while the service provider is responsible for\nprovisioning the infrastructure, scaling and exposing a REST style API. This\nenvironment seems to be adequate for running scientific workflows, which in\nrecent years, have become an established paradigm for implementing and\npreserving complex scientific processes. In this paper, we present work done on\nevaluating three major FaaS providers (Amazon, Google, IBM) as a platform for\nrunning scientific workflows. The experiments were performed with a dedicated\nbenchmarking framework, which consisted of instrumented workflow execution\nengine. The testing load was implemented as a large scale bag-of-tasks style\nworkflow, where task count reached 5120 running in parallel. The studied\nparameters include raw performance, efficiency of infrastructure provisioning,\noverhead introduced by the API and network layers, as well as aspects related\nto run time accounting. Conclusions include insights into available\nperformance, expressed as raw GFlops values and charts depicting relation of\nperformance to function size. The infrastructure provisioning proved to be\ngoverned by parallelism and rate limits, which can be deducted from included\ncharts. The overhead imposed by using a REST API proved to be a significant\ncontribution to overall run time of individual tasks, and possibly the whole\nworkflow. The paper ends with pointing out possible future work, which includes\nbuilding performance models and designing a dedicated scheduling algorithms for\nrunning scientific workflows on FaaS.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 22:21:49 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Pawlik", "Maciej", ""], ["Figiela", "Kamil", ""], ["Malawski", "Maciej", ""]]}, {"id": "1909.03632", "submitter": "Kazutomo Yoshii", "authors": "Kazutomo Yoshii, John Tramm, Andrew Siegel, Pete Beckman", "title": "Improving the scalabiliy of neutron cross-section lookup codes on\n  multicore NUMA system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the XSBench proxy application, a memory-intensive OpenMP program, to\nexplore the source of on-node scalability degradation of a popular Monte Carlo\n(MC) reactor physics benchmark on non-uniform memory access (NUMA) systems. As\nbackground, we present the details of XSBench, a performance abstraction \"proxy\napp\" for the full MC simulation, as well as the internal design of the Linux\nkernel. We explain how the physical memory allocation inside the kernel affects\nthe multicore scalability of XSBench. On a sixteen-core, two-socket NUMA\ntestbed, the scaling efficiency is improved from a nonoptimized 70% to an\noptimized 95%, and the optimized version consumes 25% less energy than does the\nnonoptimized version. In addition to the NUMA optimization we evaluate a\npage-size optimization to XSBench and observe a 1.5x performance improvement,\ncompared with a nonoptimized one.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 04:59:57 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Yoshii", "Kazutomo", ""], ["Tramm", "John", ""], ["Siegel", "Andrew", ""], ["Beckman", "Pete", ""]]}, {"id": "1909.03848", "submitter": "Zvezdin Besarabov", "authors": "Zvezdin Besarabov and Todor Kolev", "title": "Distributed creation of Machine learning agents for Blockchain analysis", "comments": "arXiv admin note: text overlap with arXiv:1810.06696", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating efficient deep neural networks involves repetitive manual\noptimization of the topology and the hyperparameters. This human intervention\nsignificantly inhibits the process. Recent publications propose various Neural\nArchitecture Search (NAS) algorithms that automate this work. We have applied a\ncustomized NAS algorithm with network morphism and Bayesian optimization to the\nproblem of cryptocurrency predictions, where it achieved results on par with\nour best manually designed models. This is consistent with the findings of\nother teams, while several known experiments suggest that given enough\ncomputing power, NAS algorithms can surpass state-of-the-art neural network\nmodels designed by humans. In this paper, we propose a blockchain network\nprotocol that incentivises independent computing nodes to run NAS algorithms\nand compete in finding better neural network models for a particular task. If\nimplemented, such network can be an autonomous and self-improving source of\nmachine learning models, significantly boosting and democratizing the access to\nAI capabilities for many industries.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:52:03 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Besarabov", "Zvezdin", ""], ["Kolev", "Todor", ""]]}, {"id": "1909.03947", "submitter": "Gabriel Laberge", "authors": "G. Laberge and S. Shirzad and P. Diehl and H. Kaiser and S. Prudhomme\n  and A. Lemoine", "title": "Scheduling optimization of parallel linear algebra algorithms using\n  Supervised Learning", "comments": "Accepted at HPCML19", "journal-ref": null, "doi": "10.1109/MLHPC49564.2019.00009", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear algebra algorithms are used widely in a variety of domains, e.g\nmachine learning, numerical physics and video games graphics. For all these\napplications, loop-level parallelism is required to achieve high performance.\nHowever, finding the optimal way to schedule the workload between threads is a\nnon-trivial problem because it depends on the structure of the algorithm being\nparallelized and the hardware the executable is run on. In the realm of\nAsynchronous Many Task runtime systems, a key aspect of the scheduling problem\nis predicting the proper chunk-size, where the chunk-size is defined as the\nnumber of iterations of a for-loop assigned to a thread as one task. In this\npaper, we study the applications of supervised learning models to predict the\nchunk-size which yields maximum performance on multiple parallel linear algebra\noperations using the HPX backend of Blaze's linear algebra library. More\nprecisely, we generate our training and tests sets by measuring performance of\nthe application with different chunk-sizes for multiple linear algebra\noperations; vector-addition, matrix-vector-multiplication, matrix-matrix\naddition and matrix-matrix-multiplication. We compare the use of logistic\nregression, neural networks and decision trees with a newly developed decision\ntree based model in order to predict the optimal value for chunk-size. Our\nresults show that classical decision trees and our custom decision tree model\nare able to forecast a chunk-size which results in good performance for the\nlinear algebra operations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:54:35 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 19:10:27 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Laberge", "G.", ""], ["Shirzad", "S.", ""], ["Diehl", "P.", ""], ["Kaiser", "H.", ""], ["Prudhomme", "S.", ""], ["Lemoine", "A.", ""]]}, {"id": "1909.03990", "submitter": "Nibesh Shrestha", "authors": "Nibesh Shrestha, Mohan Kumar", "title": "Revisiting EZBFT: A Decentralized Byzantine Fault Tolerant Protocol with\n  Speculation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we revisit EZBFT[2] and present safety, liveness and execution\nconsistency violations in the protocol. To demonstrate these violations, we\npresent simple scenarios, involving only four replicas, two clients, and one or\ntwo owner changes. We also note shortcomings of the presented TLA+\nspecification used to model check the proposed protocol.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:00:17 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Shrestha", "Nibesh", ""], ["Kumar", "Mohan", ""]]}, {"id": "1909.04532", "submitter": "Haibo Yang", "authors": "Haibo Yang, Xin Zhang, Minghong Fang, and Jia Liu", "title": "Byzantine-Resilient Stochastic Gradient Descent for Distributed\n  Learning: A Lipschitz-Inspired Coordinate-wise Median Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the resilience of distributed algorithms based on\nstochastic gradient descent (SGD) in distributed learning with potentially\nByzantine attackers, who could send arbitrary information to the parameter\nserver to disrupt the training process. Toward this end, we propose a new\nLipschitz-inspired coordinate-wise median approach (LICM-SGD) to mitigate\nByzantine attacks. We show that our LICM-SGD algorithm can resist up to half of\nthe workers being Byzantine attackers, while still converging almost surely to\na stationary region in non-convex settings. Also, our LICM-SGD method does not\nrequire any information about the number of attackers and the Lipschitz\nconstant, which makes it attractive for practical implementations. Moreover,\nour LICM-SGD method enjoys the optimal $O(md)$ computational time-complexity in\nthe sense that the time-complexity is the same as that of the standard SGD\nunder no attacks. We conduct extensive experiments to show that our LICM-SGD\nalgorithm consistently outperforms existing methods in training multi-class\nlogistic regression and convolutional neural networks with MNIST and CIFAR-10\ndatasets. In our experiments, LICM-SGD also achieves a much faster running time\nthanks to its low computational time-complexity.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:45:21 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Yang", "Haibo", ""], ["Zhang", "Xin", ""], ["Fang", "Minghong", ""], ["Liu", "Jia", ""]]}, {"id": "1909.04539", "submitter": "Andrew Gloster", "authors": "Andrew Gloster, Enda Carroll, Miguel Bustamante, Lennon O'Naraigh", "title": "Efficient Interleaved Batch Matrix Solvers for CUDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new methodology for data accesses when solving\nbatches of Tridiagonal and Pentadiagonal matrices that all share the same LHS\nmatrix. By only storing one copy of this matrix there is a significant\nreduction in storage overheads and the authors show that there is also a\nperformance increase in terms of compute time. These two results combined lead\nto an overall more efficient implementation over the current state of the art\nalgorithms cuThomasBatch and cuPentBatch, allowing for a greater number of\nsystems to be solved on a single GPU.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:53:16 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 15:33:51 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gloster", "Andrew", ""], ["Carroll", "Enda", ""], ["Bustamante", "Miguel", ""], ["O'Naraigh", "Lennon", ""]]}, {"id": "1909.04544", "submitter": "Michael Feldmann M. Sc.", "authors": "Michael Feldmann, Thorsten G\\\"otte, Christian Scheideler", "title": "A Loosely Self-stabilizing Protocol for Randomized Congestion Control\n  with Logarithmic Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider congestion control in peer-to-peer distributed systems. The\nproblem can be reduced to the following scenario: Consider a set $V$ of $n$\npeers (called clients in this paper) that want to send messages to a fixed\ncommon peer (called server in this paper). We assume that each client $v \\in V$\nsends a message with probability $p(v) \\in [0,1)$ and the server has a capacity\nof $\\sigma \\in \\mathbb{N}$, i.e., it can recieve at most $\\sigma$ messages per\nround and excess messages are dropped. The server can modify these\nprobabilities when clients send messages. Ideally, we wish to converge to a\nstate with $\\sum p(v) = \\sigma$ and $p(v) = p(w)$ for all $v,w \\in V$. We\npropose a loosely self-stabilizing protocol with a slightly relaxed legimate\nstate. Our protocol lets the system converge from any initial state to a state\nwhere $\\sum p(v) \\in \\left[\\sigma \\pm \\epsilon\\right]$ and $|p(v)-p(w)| \\in\nO(\\frac{1}{n})$. This property is then maintained for\n$\\Omega(n^{\\mathfrak{c}})$ rounds in expectation. In particular, the initial\nclient probabilities and server variables are not necessarily well-defined,\ni.e., they may have arbitrary values. Our protocol uses only $O(W + \\log n)$\nbits of memory where $W$ is length of node identifers, making it very\nlightweight. Finally we state a lower bound on the convergence time an see that\nour protocol performs asymptotically optimal (up to some polylogarithmic\nfactor).\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:57:49 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 08:56:38 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Feldmann", "Michael", ""], ["G\u00f6tte", "Thorsten", ""], ["Scheideler", "Christian", ""]]}, {"id": "1909.04548", "submitter": "Minsoo Rhu", "authors": "Yujeong Choi, Minsoo Rhu", "title": "PREMA: A Predictive Multi-task Scheduling Algorithm For Preemptible\n  Neural Processing Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To amortize cost, cloud vendors providing DNN acceleration as a service to\nend-users employ consolidation and virtualization to share the underlying\nresources among multiple DNN service requests. This paper makes a case for a\n\"preemptible\" neural processing unit (NPU) and a \"predictive\" multi-task\nscheduler to meet the latency demands of high-priority inference while\nmaintaining high throughput. We evaluate both the mechanisms that enable NPUs\nto be preemptible and the policies that utilize them to meet scheduling\nobjectives. We show that preemptive NPU multi-tasking can achieve an average\n7.8x, 1.4x, and 4.8x improvement in latency, throughput, and SLA satisfaction,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:34:31 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Choi", "Yujeong", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1909.04550", "submitter": "Jianshen Liu", "authors": "Jianshen Liu, Philip Kufeldt, Carlos Maltzahn", "title": "MBWU: Benefit Quantification for Data Access Function Offloading", "comments": "16 pages, 11 figures", "journal-ref": "HPC I/O in the Data Center Workshop, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The storage industry is considering new kinds of storage devices that support\ndata access function offloading, i.e. the ability to perform data access\nfunctions on the storage device itself as opposed to performing it on a\nseparate compute system to which the storage device is connected. But what is\nthe benefit of offloading to a storage device that is controlled by an embedded\nplatform, very different from a host platform? To quantify the benefit, we need\na measurement methodology that enables apple-to-apple comparisons between\ndifferent platforms. We propose a Media-based Work Unit (MBWU, pronounced\n\"MibeeWu\"), and an MBWU-based measurement methodology to standardize the\nplatform efficiency evaluation so as to quantify the benefit of offloading. To\ndemonstrate the merit of this methodology, we implemented a prototype to\nautomate quantifying the benefit of offloading the key-value data access\nfunction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 04:15:33 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Liu", "Jianshen", ""], ["Kufeldt", "Philip", ""], ["Maltzahn", "Carlos", ""]]}, {"id": "1909.04551", "submitter": "Hyunbin Park", "authors": "Hyunbin Park, Dohyun Kim, and Shiho Kim", "title": "TMA: Tera-MACs/W Neural Hardware Inference Accelerator with a\n  Multiplier-less Massive Parallel Processor", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally intensive Inference tasks of Deep neural networks have\nenforced revolution of new accelerator architecture to reduce power consumption\nas well as latency. The key figure of merit in hardware inference accelerators\nis the number of multiply-and-accumulation operations per watt (MACs/W), where,\nthe state-of-the-arts MACs/W remains several hundreds Giga-MACs/W. We propose a\nTera-MACS/W neural hardware inference Accelerator (TMA) with 8-bit activations\nand scalable integer weights less than 1-byte. The architectures main feature\nis configurable neural processing element for matrix-vector operations. The\nproposed neural processing element has Multiplier-less Massive Parallel\nProcessor to work without any multiplications, which makes it attractive for\nenergy efficient high-performance neural network applications. We benchmark our\nsystems latency, power, and performance using Alexnet trained on ImageNet.\nFinally, we compared our accelerators throughput and power consumption to the\nprior works. The proposed accelerator outperforms the state of the art in terms\nof energy and area achieving 2.3 TMACS/W@1.0 V, 65 nm CMOS technology.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 14:18:13 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Park", "Hyunbin", ""], ["Kim", "Dohyun", ""], ["Kim", "Shiho", ""]]}, {"id": "1909.04715", "submitter": "Ahmed Khaled", "authors": "Ahmed Khaled and Konstantin Mishchenko and Peter Richt\\'arik", "title": "First Analysis of Local GD on Heterogeneous Data", "comments": "NeurIPS 2019 Workshop on Federated Learning for Data Privacy and\n  Confidentiality. 11 pages, 4 lemmas, 1 theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first convergence analysis of local gradient descent for\nminimizing the average of smooth and convex but otherwise arbitrary functions.\nProblems of this form and local gradient descent as a solution method are of\nimportance in federated learning, where each function is based on private data\nstored by a user on a mobile device, and the data of different users can be\narbitrarily heterogeneous. We show that in a low accuracy regime, the method\nhas the same communication complexity as gradient descent.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 19:46:13 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 13:25:03 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Khaled", "Ahmed", ""], ["Mishchenko", "Konstantin", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1909.04716", "submitter": "Ahmed Khaled", "authors": "Ahmed Khaled and Peter Richt\\'arik", "title": "Gradient Descent with Compressed Iterates", "comments": "NeurIPS 2019 Workshop on Federated Learning for Data Privacy and\n  Confidentiality. 10 pages, 1 algorithm, 1 theorem, 5 lemmas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a new type of stochastic first order method: gradient\ndescent with compressed iterates (GDCI). GDCI in each iteration first\ncompresses the current iterate using a lossy randomized compression technique,\nand subsequently takes a gradient step. This method is a distillation of a key\ningredient in the current practice of federated learning, where a model needs\nto be compressed by a mobile device before it is sent back to a server for\naggregation. Our analysis provides a step towards closing the gap between the\ntheory and practice of federated learning, and opens the possibility for many\nextensions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 19:52:09 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 13:35:45 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Khaled", "Ahmed", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1909.04746", "submitter": "Ahmed Khaled", "authors": "Ahmed Khaled and Konstantin Mishchenko and Peter Richt\\'arik", "title": "Tighter Theory for Local SGD on Identical and Heterogeneous Data", "comments": "To appear in AISTATS 2020. 31 pages, 1 algorithm, 5 theorems, 6\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new analysis of local SGD, removing unnecessary assumptions and\nelaborating on the difference between two data regimes: identical and\nheterogeneous. In both cases, we improve the existing theory and provide values\nof the optimal stepsize and optimal number of local iterations. Our bounds are\nbased on a new notion of variance that is specific to local SGD methods with\ndifferent data. The tightness of our results is guaranteed by recovering known\nstatements when we plug $H=1$, where $H$ is the number of local steps. The\nempirical evidence further validates the severe impact of data heterogeneity on\nthe performance of local SGD.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 20:47:10 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 08:17:47 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 21:27:41 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Khaled", "Ahmed", ""], ["Mishchenko", "Konstantin", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1909.04783", "submitter": "Samuel S. Ogden", "authors": "Samuel S. Ogden and Tian Guo", "title": "Characterizing the Deep Neural Networks Inference Performance of Mobile\n  Applications", "comments": "11 pages (12 with references and bios), 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's mobile applications are increasingly leveraging deep neural networks\nto provide novel features, such as image and speech recognitions. To use a\npre-trained deep neural network, mobile developers can either host it in a\ncloud server, referred to as cloud-based inference, or ship it with their\nmobile application, referred to as on-device inference. In this work, we\ninvestigate the inference performance of these two common approaches on both\nmobile devices and public clouds, using popular convolutional neural networks.\nOur measurement study suggests the need for both on-device and cloud-based\ninferences for supporting mobile applications. In particular, newer mobile\ndevices is able to run mobile-optimized CNN models in reasonable time. However,\nfor older mobile devices or to use more complex CNN models, mobile applications\nshould opt in for cloud-based inference. We further demonstrate that variable\nnetwork conditions can lead to poor cloud-based inference end-to-end time. To\nsupport efficient cloud-based inference, we propose a CNN model selection\nalgorithm called CNNSelect that dynamically selects the most appropriate CNN\nmodel for each inference request, and adapts its selection to match different\nSLAs and execution time budgets that are caused by variable mobile\nenvironments. The key idea of CNNSelect is to make inference speed and accuracy\ntrade-offs at runtime using a set of CNN models. We demonstrated that CNNSelect\nsmoothly improves inference accuracy while maintaining SLA attainment in 88.5%\nmore cases than a greedy baseline.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 22:33:10 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ogden", "Samuel S.", ""], ["Guo", "Tian", ""]]}, {"id": "1909.04885", "submitter": "Michael Kaufmann", "authors": "Michael Kaufmann, Kornilios Kourtis, Celestine Mendler-D\\\"unner,\n  Adrian Sch\\\"upbach, Thomas Parnell", "title": "Addressing Algorithmic Bottlenecks in Elastic Machine Learning with\n  Chicle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning training is one of the most common and important\nworkloads running on data centers today, but it is rarely executed alone.\nInstead, to reduce costs, computing resources are consolidated and shared by\ndifferent applications. In this scenario, elasticity and proper load balancing\nare vital to maximize efficiency, fairness, and utilization. Currently, most\ndistributed training frameworks do not support the aforementioned properties. A\nfew exceptions that do support elasticity, imitate generic distributed\nframeworks and use micro-tasks. In this paper we illustrate that micro-tasks\nare problematic for machine learning applications, because they require a high\ndegree of parallelism which hinders the convergence of distributed training at\na pure algorithmic level (i.e., ignoring overheads and scalability\nlimitations). To address this, we propose Chicle, a new elastic distributed\ntraining framework which exploits the nature of machine learning algorithms to\nimplement elasticity and load balancing without micro-tasks. We use Chicle to\ntrain deep neural network as well as generalized linear models, and show that\nChicle achieves performance competitive with state of the art rigid frameworks,\nwhile efficiently enabling elastic execution and dynamic load balancing.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 07:37:05 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kaufmann", "Michael", ""], ["Kourtis", "Kornilios", ""], ["Mendler-D\u00fcnner", "Celestine", ""], ["Sch\u00fcpbach", "Adrian", ""], ["Parnell", "Thomas", ""]]}, {"id": "1909.04934", "submitter": "Nanjangud Narendra PhD", "authors": "James Kempf, Sambit Nayak, Remi Robert, Jim Feng, Kunal Rajan\n  Deshmukh, Anshu Shukla, Aleksandra Obeso Duque, Nanjangud Narendra, and Johan\n  Sj\\\"oberg", "title": "The Nubo Virtual Services Marketplace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a virtual services marketplace, called Nubo,\ndesigned to connect buyers of virtual services (or tenants) with providers of\nthose services on a cloud computing platform. The marketplace is implemented as\na collection of distributed microservices along with a marketplace portal that\nruns as a Web application. The heart of Nubo is the Saranyu tenant and service\nmanagement microservice. Saranyu is a decentralized application (dApp) built on\ntop of the J.P. Morgan Quorum blockchain. Tenant and service accounts are\nrepresented as static (nonnegotiable) smart contracts written in the Solidity\nlanguage. Quorum provides a tamper evident and tamper resistant distributed\nledger, whereby multiple cloud and service providers can co-operate to provide\nservice resources to tenants in a trustworthy fashion. Services offer resources\nconsisting of a collection of attributes describing what the tenant can\nconsume, and tenants subscribe to service resources through the Nubo\nMarketplace portal. The Service Manager microservice provides multitenant\nsupport for containerized services built for deployment and orchestration using\nDocker that were originally not designed to be managed through Saranyu. We\ndiscuss our design goals for Nubo, describe the overall architecture, discuss\nsome details on how Saranyu uses the blockchain and smart contracts, and\nprovide comprehensive performance and scalability data measured on the Saranyu\nREST API. The results indicate Saranyu is competitive with published results\nfor comparable operations on the Havana release of OpenStack Keystone, but\nSaranyu provides a much richer collection of tenant and service management\nfunctionality than Keystone.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 09:25:16 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 15:54:08 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Kempf", "James", ""], ["Nayak", "Sambit", ""], ["Robert", "Remi", ""], ["Feng", "Jim", ""], ["Deshmukh", "Kunal Rajan", ""], ["Shukla", "Anshu", ""], ["Duque", "Aleksandra Obeso", ""], ["Narendra", "Nanjangud", ""], ["Sj\u00f6berg", "Johan", ""]]}, {"id": "1909.04945", "submitter": "Ayesha Abdul Majeed Ms", "authors": "Ayesha Abdul Majeed, Peter Kilpatrick, Ivor Spence, and Blesson\n  Varghese", "title": "Performance Estimation of Container-Based Cloud-to-Fog Offloading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing offloads latency critical application services running on the\nCloud in close proximity to end-user devices onto resources located at the edge\nof the network. The research in this paper is motivated towards characterising\nand estimating the time taken to offload a service using containers, which is\ninvestigated in the context of the `Save and Load' container migration\ntechnique. To this end, the research addresses questions such as whether fog\noffloading can be accurately modelled and which system and network related\nparameters influence offloading. These are addressed by exploring a catalogue\nof 21 different metrics both at the system and process levels that is used as\ninput to four estimation techniques using collective model and individual\nmodels to predict the time taken for offloading. The study is pursued by\ncollecting over 1.1 million data points and the preliminary results indicate\nthat offloading can be modelled accurately.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 09:47:48 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Majeed", "Ayesha Abdul", ""], ["Kilpatrick", "Peter", ""], ["Spence", "Ivor", ""], ["Varghese", "Blesson", ""]]}, {"id": "1909.05020", "submitter": "Jemin George", "authors": "Jemin George, Prudhvi Gurram", "title": "Distributed Deep Learning with Event-Triggered Communication", "comments": "arXiv admin note: substantial text overlap with arXiv:1908.06693", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Distributed Event-Triggered Stochastic GRAdient Descent\n(DETSGRAD) algorithm for solving non-convex optimization problems typically\nencountered in distributed deep learning. We propose a novel communication\ntriggering mechanism that would allow the networked agents to update their\nmodel parameters aperiodically and provide sufficient conditions on the\nalgorithm step-sizes that guarantee the asymptotic mean-square convergence. The\nalgorithm is applied to a distributed supervised-learning problem, in which a\nset of networked agents collaboratively train their individual neural networks\nto recognize handwritten digits in images, while aperiodically sharing the\nmodel parameters with their one-hop neighbors. Results indicate that all agents\nreport similar performance that is also comparable to the performance of a\ncentrally trained neural network, while the event-triggered communication\nprovides significant reduction in inter-agent communication. Results also show\nthat the proposed algorithm allows the individual agents to recognize the\ndigits even though the training data corresponding to all the digits are not\nlocally available to each agent.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 20:11:40 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["George", "Jemin", ""], ["Gurram", "Prudhvi", ""]]}, {"id": "1909.05059", "submitter": "He Huang", "authors": "He Huang and Chaowei Yuan", "title": "Generalized Optimal Two-way Relays Subsets Pairings in Cloud-based\n  Region Cognitive Networks", "comments": "This paper has some flaws that should be modified, so we want to\n  withdraw it firstly!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Communication reliability improving is one of most important research\nrequirements in cognitive networks, as 5G communications technology rapidly\ndevelop nowadays. In this paper, we propose generalized optimal cloud-based\nregion relays subsets paring model in underlay dual-hop cognitive networks,\nthis unified model reveals three relays nodes characteristics of cloud-based\ncooperative networks in the nearby area- A subset, only needs receiving for\nfirst hop, B subset, just receive and forward, C subset, only forwarding. In\naddition, this generalized model can be converted into various classical relay\nselection algorithms when A, B and C subsets are taken as special selection\nvalues [Table II]. Furthermore, we put forward optimal relays subsets pairings\nand replacement algorithm flowchart to improve minimum outage probability (OP)\nfor communication reliability, and prove that optimal relays subsets pairings\n(A, B, C) will better guarantee reliability of communication, comparing other\npopular relays selection schemes. Simulation results show that optimal relays\npaired subsets are exist and this generalized algorithm enormously reduces OP,\ncomparing other selection algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:57:30 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 13:20:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Huang", "He", ""], ["Yuan", "Chaowei", ""]]}, {"id": "1909.05073", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma,\n  Bin Ren, Yanzhi Wang", "title": "PCONV: The Missing but Desirable Sparsity in DNN Weight Pruning for\n  Real-time Execution on Mobile Devices", "comments": "To appear in Proceedings of the 34th AAAI Conference on Artificial\n  Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression techniques on Deep Neural Network (DNN) have been widely\nacknowledged as an effective way to achieve acceleration on a variety of\nplatforms, and DNN weight pruning is a straightforward and effective method.\nThere are currently two mainstreams of pruning methods representing two\nextremes of pruning regularity: non-structured, fine-grained pruning can\nachieve high sparsity and accuracy, but is not hardware friendly; structured,\ncoarse-grained pruning exploits hardware-efficient structures in pruning, but\nsuffers from accuracy drop when the pruning rate is high. In this paper, we\nintroduce PCONV, comprising a new sparsity dimension, -- fine-grained pruning\npatterns inside the coarse-grained structures. PCONV comprises two types of\nsparsities, Sparse Convolution Patterns (SCP) which is generated from\nintra-convolution kernel pruning and connectivity sparsity generated from\ninter-convolution kernel pruning. Essentially, SCP enhances accuracy due to its\nspecial vision properties, and connectivity sparsity increases pruning rate\nwhile maintaining balanced workload on filter computation. To deploy PCONV, we\ndevelop a novel compiler-assisted DNN inference framework and execute PCONV\nmodels in real-time without accuracy compromise, which cannot be achieved in\nprior work. Our experimental results show that, PCONV outperforms three\nstate-of-art end-to-end DNN frameworks, TensorFlow-Lite, TVM, and Alibaba\nMobile Neural Network with speedup up to 39.2x, 11.4x, and 6.3x, respectively,\nwith no accuracy loss. Mobile devices can achieve real-time inference on\nlarge-scale DNNs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 03:58:29 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 01:33:36 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 00:18:07 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2020 19:39:06 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Ma", "Xiaolong", ""], ["Guo", "Fu-Ming", ""], ["Niu", "Wei", ""], ["Lin", "Xue", ""], ["Tang", "Jian", ""], ["Ma", "Kaisheng", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1909.05125", "submitter": "Luis Mu\\~noz-Gonz\\'alez", "authors": "Luis Mu\\~noz-Gonz\\'alez, Kenneth T. Co, Emil C. Lupu", "title": "Byzantine-Robust Federated Machine Learning through Adaptive Model\n  Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables training collaborative machine learning models at\nscale with many participants whilst preserving the privacy of their datasets.\nStandard federated learning techniques are vulnerable to Byzantine failures,\nbiased local datasets, and poisoning attacks. In this paper we introduce\nAdaptive Federated Averaging, a novel algorithm for robust federated learning\nthat is designed to detect failures, attacks, and bad updates provided by\nparticipants in a collaborative model. We propose a Hidden Markov Model to\nmodel and learn the quality of model updates provided by each participant\nduring training. In contrast to existing robust federated learning schemes, we\npropose a robust aggregation rule that detects and discards bad or malicious\nlocal model updates at each training iteration. This includes a mechanism that\nblocks unwanted participants, which also increases the computational and\ncommunication efficiency. Our experimental evaluation on 4 real datasets show\nthat our algorithm is significantly more robust to faulty, noisy and malicious\nparticipants, whilst being computationally more efficient than other\nstate-of-the-art robust federated learning methods such as Multi-KRUM and\ncoordinate-wise median.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 15:19:36 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Co", "Kenneth T.", ""], ["Lupu", "Emil C.", ""]]}, {"id": "1909.05204", "submitter": "Oded Naor", "authors": "Oded Naor, Mathieu Baudet, Dahlia Malkhi, Alexander Spiegelman", "title": "Cogsworth: Byzantine View Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for Byzantine fault tolerance (BFT) in the partial synchrony\nsetting divide the local state of the nodes into views, and the transition from\none view to the next dictates a leader change. In order to provide liveness,\nall honest nodes need to stay in the same view for a sufficiently long time.\nThis requires \\emph{view synchronization}, a requisite of BFT that we extract\nand formally define here.\n  Existing approaches for Byzantine view synchronization incur quadratic\ncommunication (in $n$, the number of parties). A cascade of $O(n)$ view changes\nmay thus result in $O(n^3)$ communication complexity. This paper presents a new\nByzantine view synchronization algorithm named Cogsworth, that has\noptimistically linear communication complexity and constant latency. Faced with\nbenign failures, Cogsworth has expected linear communication and constant\nlatency.\n  The result here serves as an important step towards reaching solutions that\nhave overall quadratic communication, the known lower bound on Byzantine fault\ntolerant consensus. Cogsworth is particularly useful for a family of BFT\nprotocols that already exhibit linear communication under various\ncircumstances, but suffer quadratic overhead due to view synchronization.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:50:28 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 21:14:55 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 17:09:00 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Naor", "Oded", ""], ["Baudet", "Mathieu", ""], ["Malkhi", "Dahlia", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "1909.05349", "submitter": "Ayoosh Bansal", "authors": "Ayoosh Bansal, Jayati Singh, Yifan Hao, Jen-Yang Wen, Renato Mancuso,\n  and Marco Caccamo", "title": "Cache Where you Want! Reconciling Predictability and Coherent Caching", "comments": "13 pages, 10 figures, v2 update includes overview section with formal\n  solution definition. This is a long version of a prior publication", "journal-ref": "2020 9th Mediterranean Conference on Embedded Computing (MECO),\n  2020, pp. 1-6", "doi": "10.1109/MECO49872.2020.9134262", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time and cyber-physical systems need to interact with and respond to\ntheir physical environment in a predictable time. While multicore platforms\nprovide incredible computational power and throughput, they also introduce new\nsources of unpredictability. Large fluctuations in latency to access data\nshared between multiple cores is an important contributor to the overall\nexecution-time variability. In addition to the temporal unpredictability\nintroduced by caching, parallel applications with data shared across multiple\ncores also pay additional latency overheads due to data coherence. Analyzing\nthe impact of data coherence on the worst-case execution-time of real-time\napplications is challenging because only scarce implementation details are\nrevealed by manufacturers. This paper presents application level control for\ncaching data at different levels of the cache hierarchy. The rationale is that\nby caching data only in shared cache it is possible to bypass private caches.\nThe access latency to data present in caches becomes independent of its\ncoherence state. We discuss the existing architectural support as well as the\nrequired hardware and OS modifications to support the proposed cacheability\ncontrol. We evaluate the system on an architectural simulator. We show that the\nworst case execution time for a single memory write request is reduced by 52%.\nBenchmark evaluations show that proposed technique has a minimal impact on\naverage performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 20:47:58 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 23:34:25 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bansal", "Ayoosh", ""], ["Singh", "Jayati", ""], ["Hao", "Yifan", ""], ["Wen", "Jen-Yang", ""], ["Mancuso", "Renato", ""], ["Caccamo", "Marco", ""]]}, {"id": "1909.05350", "submitter": "Sebastian U. Stich", "authors": "Sebastian U. Stich, Sai Praneeth Karimireddy", "title": "The Error-Feedback Framework: Better Rates for SGD with Delayed\n  Gradients and Compressed Communication", "comments": "Submitted 9/19, Published 9/20", "journal-ref": "Journal of Machine Learning Research (JMLR), 21(237):1-36, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze (stochastic) gradient descent (SGD) with delayed updates on smooth\nquasi-convex and non-convex functions and derive concise, non-asymptotic,\nconvergence rates. We show that the rate of convergence in all cases consists\nof two terms: (i) a stochastic term which is not affected by the delay, and\n(ii) a higher order deterministic term which is only linearly slowed down by\nthe delay. Thus, in the presence of noise, the effects of the delay become\nnegligible after a few iterations and the algorithm converges at the same\noptimal rate as standard SGD. This result extends a line of research that\nshowed similar results in the asymptotic regime or for strongly-convex\nquadratic functions only. We further show similar results for SGD with more\nintricate form of delayed gradients -- compressed gradients under error\ncompensation and for local~SGD where multiple workers perform local steps\nbefore communicating with each other. In all of these settings, we improve upon\nthe best known rates. These results show that SGD is robust to compressed\nand/or delayed stochastic gradient updates. This is in particular important for\ndistributed parallel implementations, where asynchronous and communication\nefficient methods are the key to achieve linear speedups for optimization with\nmultiple devices.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 20:54:49 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 15:44:47 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Stich", "Sebastian U.", ""], ["Karimireddy", "Sai Praneeth", ""]]}, {"id": "1909.05480", "submitter": "Sa\\v{s}o Stanovnik", "authors": "Sa\\v{s}o Stanovnik and Matija Cankar", "title": "On the similarities and differences between the Cloud, Fog and the Edge", "comments": "Euro-Par 2019, F2C-DP workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of edge and fog computing is growing, but there are still many\ninconsistent and loosely-defined terms in current literature. With many\narticles comparing theoretical architectures and evaluating implementations,\nthere is a need to understand the underlying meaning of information condensed\ninto fog, edge, and similar terms. Through our review of current literature, we\ndiscuss these differences and extract key characteristics for basic concepts\nthat appear throughout. The similarities to existing IaaS, PaaS and SaaS models\nare presented, contrasted against similar models modified for the specifics of\nedge devices and workloads.\n  We also evaluate the different aspects existing evaluation and comparison\nworks investigate, including the compute, networking, storage, security, and\nease-of-use capabilities of the target implementations. Following that, we make\na broad overview of currently available commercial and open-source platforms\nimplementing the edge or fog paradigms, identifying key players, successful\nniche actors and general trends for feature-level and technical development of\nthese platforms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:45:16 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Stanovnik", "Sa\u0161o", ""], ["Cankar", "Matija", ""]]}, {"id": "1909.05531", "submitter": "Isabelly Rocha", "authors": "Robert Birke, Isabelly Rocha, Juan Perez, Valerio Schiavoni, Pascal\n  Felber, Lydia Y. Chen", "title": "Differential Approximation and Sprinting for Multi-Priority Big Data\n  Engines", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's big data clusters based on the MapReduce paradigm are capable of\nexecuting analysis jobs with multiple priorities, providing differential\nlatency guarantees. Traces from production systems show that the latency\nadvantage of high-priority jobs comes at the cost of severe latency degradation\nof low-priority jobs as well as daunting resource waste caused by repetitive\neviction and re-execution of low-priority jobs. We advocate a new resource\nmanagement design that exploits the idea of differential approximation and\nsprinting. The unique combination of approximation and sprinting avoids the\neviction of low-priority jobs and its consequent latency degradation and\nresource waste. To this end, we designed, implemented and evaluated DiAS, an\nextension of the Spark processing engine to support deflate jobs by dropping\ntasks and to sprint jobs. Our experiments on scenarios with two and three\npriority classes indicate that DiAS achieves up to 90% and 60% latency\nreduction for low- and high-priority jobs, respectively. DiAS not only\neliminates resource waste but also (surprisingly) lowers energy consumption up\nto 30% at only a marginal accuracy loss for low-priority jobs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 09:32:47 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 09:41:02 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Birke", "Robert", ""], ["Rocha", "Isabelly", ""], ["Perez", "Juan", ""], ["Schiavoni", "Valerio", ""], ["Felber", "Pascal", ""], ["Chen", "Lydia Y.", ""]]}, {"id": "1909.05537", "submitter": "Philippe Qu\\'einnec", "authors": "Armando Casta\\~neda, Aur\\'elie Hurault, Philippe Qu\\'einnec, Matthieu\n  Roy", "title": "Tasks in Modular Proofs of Concurrent Algorithms", "comments": "Long version of paper in 21st International Symposium on\n  Stabilization, Safety, and Security of Distributed Systems (SSS 2019)", "journal-ref": null, "doi": "10.1007/978-3-030-34992-9_6", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Proving correctness of distributed or concurrent algorithms is a\nmind-challenging and complex process. Slight errors in the reasoning are\ndifficult to find, calling for computer-checked proof systems. In order to\nbuild computer-checked proofs with usual tools, such as Coq or TLA+, having\nsequential specifications of all base objects that are used as building blocks\nin a given algorithm is a requisite to provide a modular proof built by\ncomposition. Alas, many concurrent objects do not have a sequential\nspecification. This article describes a systematic method to transform any\ntask, a specification method that captures concurrent one-shot distributed\nproblems, into a sequential specification involving two calls, Set and Get.\nThis transformation allows system designers to compose proofs, thus providing a\nframework for modular computer-checked proofs of algorithms designed using\ntasks and sequential objects as building blocks. The Moir&Anderson\nimplementation of renaming using splitters is an iconic example of such\nalgorithms designed by composition.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 09:43:47 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 09:22:25 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Casta\u00f1eda", "Armando", ""], ["Hurault", "Aur\u00e9lie", ""], ["Qu\u00e9innec", "Philippe", ""], ["Roy", "Matthieu", ""]]}, {"id": "1909.05576", "submitter": "Gadi Taubenfeld", "authors": "Michel Raynal, Gadi Taubenfeld", "title": "Fully Anonymous Shared Memory Algorithms", "comments": "Full version of SSS2019 BA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process anonymity has been studied for a long time. Memory anonymity is more\nrecent. In an anonymous memory system, there is no a priori agreement among the\nprocesses on the names of the shared registers they access. This article\nintroduces the fully anonymous model, namely a model in which both the\nprocesses and the memory are anonymous. It is shown that fundamental problems\nsuch as mutual exclusion, consensus, and its weak version called set agreement,\ncan be solved despite full anonymity, the first in a failure-free system, the\nothers in the presence of any number of process crashes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:30:42 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 13:27:45 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Raynal", "Michel", ""], ["Taubenfeld", "Gadi", ""]]}, {"id": "1909.05617", "submitter": "Piotr Dziurzanski", "authors": "Leandro Soares Indrusiak, Piotr Dziurzanski, Shuai Zhao", "title": "Proceedings of the International Workshop on Reconfigurable and\n  Communication-centric Cyber-Physical Systems (ReCoCyPS 2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume represents the proceedings of the International Workshop on\nReconfigurable and Communication-centric Cyber-Physical Systems (ReCoCyPS\n2019), co-located with the 14th International Symposium on Reconfigurable\nCommunication-centric Systems-on-Chip (ReCoSoC 2019) on July 2-3 2019, York,\nUnited Kingdom.\n  The workshop is organised by the EU-funded SAFIRE project consortium.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 16:00:51 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Indrusiak", "Leandro Soares", ""], ["Dziurzanski", "Piotr", ""], ["Zhao", "Shuai", ""]]}, {"id": "1909.05821", "submitter": "Alexander Hentschel", "authors": "Alexander Hentschel, Dieter Shirley, Layne Lafrance", "title": "Flow: Separating Consensus and Compute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughput limitations of existing blockchain architectures are one of the\nmost significant hurdles for their wide-spread adoption. Attempts to address\nthis challenge include layer-2 solutions, such as Bitcoin's Lightning or\nEthereum's Plasma network, that move work off the main chain. Another prominent\ntechnique is sharding, i.e., breaking the network into many interconnected\nnetworks. However, these scaling approaches significantly increase the\ncomplexity of the programming model by breaking ACID guarantees increasing the\ncost and time for application development.\n  In this paper, we describe a novel approach where we split the work\ntraditionally assigned to cryptocurrency miners into two different node roles.\nSpecifically, the selection and ordering of transactions are performed\nindependently from their execution. The focus of this paper is to formalize the\nsplit of consensus and computation, and prove that this approach increases\nthroughput without compromising security.\n  In contrast to most existing proposals, our approach achieves scaling via\nseparation of concerns, i.e., better utilization of network resources, rather\nthan sharding. This approach allows established programming paradigms for smart\ncontracts (which generally assume transactional atomicity) to persist without\nintroducing additional complexity. We present simulations on a proof-of-concept\nnetwork of 32 globally distributed nodes. While the consensus algorithm was\nidentical in all simulations (a 2-step-commit protocol with rotating block\nproposer), block computation was either included in a consensus nodes' regular\noperations (conventional architecture) or delegated to specialized execution\nnodes (separation of concerns). Separation of concerns enables our system to\nachieve a throughput increase by a factor of 56 compared to conventional\narchitectures without loss of safety or decentralization.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:28:42 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Hentschel", "Alexander", ""], ["Shirley", "Dieter", ""], ["Lafrance", "Layne", ""]]}, {"id": "1909.05832", "submitter": "Alexander Hentschel", "authors": "Alexander Hentschel, Dieter Shirley, Layne Lafrance, Maor Zamski", "title": "Flow: Separating Consensus and Compute -- Execution Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughput limitations of existing blockchain architectures are well\ndocumented and are one of the most significant hurdles for their wide-spread\nadoption. In our previous proof-of-concept work, we have shown that separating\ncomputation from consensus can provide a significant throughput increase\nwithout compromising security. In our architecture, Consensus Nodes only define\nthe transaction order but do not execute transactions. Instead, computing the\nblock result is delegated to compute-optimized Execution Nodes, and dedicated\nVerification Nodes check the computation result. During normal operation,\nConsensus Nodes do not inspect the computation but oversee that participating\nnodes execute their tasks with due diligence and adjudicate potential result\nchallenges. While the architecture can significantly increase throughput,\nVerification Nodes still have to duplicate the computation fully. In this\npaper, we refine the architecture such that result verification is distributed\nand parallelized across many Verification Nodes. The full architecture\nsignificantly increases throughput and delegates the computation work to the\nspecialized Execution Nodes and the onus of checking it to a variety of less\npowerful Verification Nodes. We provide a full protocol specification of the\nverification process, including challenges to faulty computation results and\nthe resulting adjudication process. Furthermore, we formally prove liveness and\nsafety of the system.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:38:48 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Hentschel", "Alexander", ""], ["Shirley", "Dieter", ""], ["Lafrance", "Layne", ""], ["Zamski", "Maor", ""]]}, {"id": "1909.05973", "submitter": "EPTCS", "authors": "Diego Marmsoler (Technische Universit\\\"at M\\\"unchen), Ana Petrovska\n  (Technische Universit\\\"at M\\\"unchen)", "title": "Detecting Architectural Erosion using Runtime Verification", "comments": "In Proceedings ICE 2019, arXiv:1909.05242", "journal-ref": "EPTCS 304, 2019, pp. 97-114", "doi": "10.4204/EPTCS.304.7", "report-no": null, "categories": "cs.SE cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The architecture of a system captures important design decisions for the\nsystem. Over time, changes in a system's implementation may lead to violations\nof specific design decisions. This problem is common in industry and known as\narchitectural erosion. Since it may have severe consequences on the quality of\na system, research has focused on the development of tools and techniques to\naddress the presented problem. As of today, most of the approaches to detect\narchitectural erosion employ static analysis techniques. While these techniques\nare well-suited for the analysis of static architectures, they reach their\nlimit when it comes to dynamic architectures. Thus, in this paper, we propose\nan alternative approach based on runtime verification. To this end, we propose\na systematic way to translate a formal specification of architectural\nconstraints to monitors, which can be used to detect violations of these\nconstraints. The approach is implemented in Eclipse/EMF, demonstrated through a\nrunning example, and evaluated using two case studies.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 22:25:13 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Marmsoler", "Diego", "", "Technische Universit\u00e4t M\u00fcnchen"], ["Petrovska", "Ana", "", "Technische Universit\u00e4t M\u00fcnchen"]]}, {"id": "1909.06040", "submitter": "Yanghua Peng", "authors": "Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu, Chen Meng, Wei Lin", "title": "DL2: A Deep Learning-driven Scheduler for Deep Learning Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more companies have deployed machine learning (ML) clusters, where\ndeep learning (DL) models are trained for providing various AI-driven services.\nEfficient resource scheduling is essential for maximal utilization of expensive\nDL clusters. Existing cluster schedulers either are agnostic to ML workload\ncharacteristics, or use scheduling heuristics based on operators' understanding\nof particular ML framework and workload, which are less efficient or not\ngeneral enough. In this paper, we show that DL techniques can be adopted to\ndesign a generic and efficient scheduler. DL2 is a DL-driven scheduler for DL\nclusters, targeting global training job expedition by dynamically resizing\nresources allocated to jobs. DL2 advocates a joint supervised learning and\nreinforcement learning approach: a neural network is warmed up via offline\nsupervised learning based on job traces produced by the existing cluster\nscheduler; then the neural network is plugged into the live DL cluster,\nfine-tuned by reinforcement learning carried out throughout the training\nprogress of the DL jobs, and used for deciding job resource allocation in an\nonline fashion. By applying past decisions made by the existing cluster\nscheduler in the preparatory supervised learning phase, our approach enables a\nsmooth transition from existing scheduler, and renders a high-quality scheduler\nin minimizing average training completion time. We implement DL2 on Kubernetes\nand enable dynamic resource scaling in DL jobs on MXNet. Extensive evaluation\nshows that DL2 outperforms fairness scheduler (i.e., DRF) by 44.1% and expert\nheuristic scheduler (i.e., Optimus) by 17.5% in terms of average job completion\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:30:11 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Peng", "Yanghua", ""], ["Bao", "Yixin", ""], ["Chen", "Yangrui", ""], ["Wu", "Chuan", ""], ["Meng", "Chen", ""], ["Lin", "Wei", ""]]}, {"id": "1909.06055", "submitter": "Shantenu Jha", "authors": "Andre Luckow and Shantenu Jha", "title": "Performance Characterization and Modeling of Serverless and HPC\n  Streaming Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiment-in-the-Loop Computing (EILC) requires support for numerous types\nof processing and the management of heterogeneous infrastructure over a dynamic\nrange of scales: from the edge to the cloud and HPC, and intermediate\nresources. Serverless is an emerging service that combines high-level\nmiddleware services, such as distributed execution engines for managing tasks,\nwith low-level infrastructure. It offers the potential of usability and\nscalability, but adds to the complexity of managing heterogeneous and dynamic\nresources. In response, we extend Pilot-Streaming to support serverless\nplatforms. Pilot-Streaming provides a unified abstraction for resource\nmanagement for HPC, cloud, and serverless, and allocates resource containers\nindependent of the application workload removing the need to write\nresource-specific code. Understanding of the performance and scaling\ncharacteristics of streaming applications and infrastructure presents another\nchallenge for EILC. StreamInsight provides insight into the performance of\nstreaming applications and infrastructure, their selection, configuration and\nscaling behavior. Underlying StreamInsight is the universal scalability law,\nwhich permits the accurate quantification of scalability properties of\nstreaming applications. Using experiments on HPC and AWS Lambda, we demonstrate\nthat StreamInsight provides an accurate model for a variety of application\ncharacteristics, e.g., machine learning model sizes and resource\nconfigurations.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 06:39:08 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Luckow", "Andre", ""], ["Jha", "Shantenu", ""]]}, {"id": "1909.06096", "submitter": "Philipp Samfass", "authors": "Philipp Samfass, Tobias Weinzierl, Dominic E. Charrier, Michael Bader", "title": "Lightweight Task Offloading Exploiting MPI Wait Times for Parallel\n  Adaptive Mesh Refinement", "comments": null, "journal-ref": null, "doi": "10.1002/cpe.5916", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing the workload of sophisticated simulations is inherently difficult,\nsince we have to balance both computational workload and memory footprint over\nmeshes that can change any time or yield unpredictable cost per mesh entity,\nwhile modern supercomputers and their interconnects start to exhibit\nfluctuating performance. We propose a novel lightweight balancing technique for\nMPI+X to accompany traditional, prediction-based load balancing. It is a\nreactive diffusion approach that uses online measurements of MPI idle time to\nmigrate tasks temporarily from overloaded to underemployed ranks. Tasks are\ndeployed to ranks which otherwise would wait, processed with high priority, and\nmade available to the overloaded ranks again. This migration is non-persistent.\nOur approach hijacks idle time to do meaningful work and is totally\nnon-blocking, asynchronous and distributed without a global data view. Tests\nwith a seismic simulation code developed in the ExaHyPE engine uncover the\nmethod's potential. We found speed-ups of up to 2-3 for ill-balanced scenarios\nwithout logical modifications of the code base and show that the strategy is\ncapable to react quickly to temporarily changing workload or node performance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 09:12:14 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 17:33:09 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Samfass", "Philipp", ""], ["Weinzierl", "Tobias", ""], ["Charrier", "Dominic E.", ""], ["Bader", "Michael", ""]]}, {"id": "1909.06129", "submitter": "Akram Saeed Aqlan Alhammadi", "authors": "Akram Saeed Aqlan Alhammadi, Dr.V. Vasanthi", "title": "Mr-moslo: vm consolidation using multiple regression multi-objective\n  seven-spot ladybird optimization for host overload detection", "comments": null, "journal-ref": "International Journal of Intelligent Engineering and Systems,\n  Vol.13, No.2, 2020", "doi": "10.22266/ijies2020.0430.03", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual Machine (VM)consolidation is a crucial process in improving the\nutilization of the resource in cloud computing services.As the cloud data\ncenters consume high electrical power,the operational costs and carbon dioxide\nreleases increases.The inefficient usage of the resources is the main reason\nfor these problems and VM consolidation is a viable solution.VM consolidation\nincludes host overload/under-load detection,VM selection and VM placement\nprocesses.Most existing host overload/under-load detection approaches of VM\nconsolidation uses CPU utilization only for the determining host load.In this\npaper,three resources namely CPU utilization,memory utilization and bandwidth\nutilization are used for host overload detection and an adaptive regression\nbased model called Multiple Regression Multi-Objective Seven-Spot Ladybird\nOptimization(MR-MOSLO) is proposed.This model is based on combining the\nbenefits of adaptive threshold based and regression based host overload\ndetection algorithms.This approach of combining these features provide more\nadvantages for threshold setting in dynamic environments with accurate\nprediction of host overloading.For this purpose, initially,Multiple Regression\n(MR)algorithm is used which relay on CPU utilization,memory utilization and\nbandwidth utilization for estimation of the host load conditions.Then a\nMulti-Objective Seven-Spot Ladybird Optimization(MOSLO)algorithm is introduced\nto select the upper and lower threshold limits for host utilization.Based on\nthese algorithms,the host overload/under-load is detected with high accuracy\nand less power consumption.The simulations are conducted in CloudSim tool and\nthe empirical results shows that the proposed MR-MOSLO algorithm detects the\nhost overload efficiently with reasonably similar energy and SLA values while\ncomparatively lesser SLATAH,PDM,SLAV and ESV values than most of the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 10:29:30 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 04:53:09 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Alhammadi", "Akram Saeed Aqlan", ""], ["Vasanthi", "Dr. V.", ""]]}, {"id": "1909.06369", "submitter": "Richard Jiang", "authors": "Bing Xu, Tobechukwu Agbele, Qiang Ni and Richard Jiang", "title": "Biometric Blockchain: A Secure Solution for Intelligent Vehicle Data\n  Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intelligent vehicle (IV) has become a promising technology that could\nrevolutionize our life in smart cities sooner or later. However, it yet suffers\nfrom many security vulnerabilities. Traditional security methods are incapable\nto secure the IV data sharing against malicious attacks. Blockchain, as\nexpected by both research and industry communities, has emerged as a good\nsolution to address these issues. The major issues in IV data sharing are\ntrust, data accuracy and reliability of data sharing in the communication\nchannel. Blockchain technology, previously working for the cryptocurrency, has\nrecently applied to build trust and reliability in peer-to-peer networks with\nsimilar topologies of IV data sharing. In this chapter, we present a new\nframework, namely biometric blockchain (BBC), for secure IV data sharing. In\nour new scheme, biometric information is exploited as a cue to record who is\nresponsible in the data sharing activities, while the proposed BBC technology\nserves as the backbone of the IV data-sharing architecture. Hence, the proposed\nBBC technology provides a more reliable trust environment between the vehicles\nwhile personal identities are traceable in the proposed new scheme.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 19:23:01 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Xu", "Bing", ""], ["Agbele", "Tobechukwu", ""], ["Ni", "Qiang", ""], ["Jiang", "Richard", ""]]}, {"id": "1909.06420", "submitter": "Patrick Totzke", "authors": "Corto Mascle, Mahsa Shirmohammadi, Patrick Totzke", "title": "Controlling a Random Population is EXPTIME-hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bertrand et al. [1] (LMCS 2019) describe two-player zero-sum games in which\none player tries to achieve a reachability objective in $n$ games (on the same\nfinite arena) simultaneously by broadcasting actions, and where the opponent\nhas full control of resolving non-deterministic choices. They show EXPTIME\ncompleteness for the question if such games can be won for every number $n$ of\ngames.\n  We consider the almost-sure variant in which the opponent randomizes their\nactions, and where the player tries to achieve the reachability objective\neventually with probability one. The lower bound construction in [1] does not\ndirectly carry over to this randomized setting. In this note we show EXPTIME\nhardness for the almost-sure problem by reduction from Countdown Games.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 19:51:57 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mascle", "Corto", ""], ["Shirmohammadi", "Mahsa", ""], ["Totzke", "Patrick", ""]]}, {"id": "1909.06435", "submitter": "Camilo Rocha", "authors": "Carlos Pinz\\'on, Camilo Rocha, Jorge Finke", "title": "A Random Network Model for the Analysis of Blockchain Designs with\n  Communication Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a random network model for blockchains, a distributed\nhierarchical data structure of blocks that has found several applications in\nvarious industries. The model is parametric on two probability distribution\nfunctions governing block production and communication delay, which are key to\ncapture the complexity of the mechanism used to synchronize the many\ndistributed local copies of a blockchain. The proposed model is equipped with\nsimulation algorithms for both bounded and unbounded number of distributed\ncopies of the blockchain. They are used to study fast blockchain systems, i.e.,\nblockchains in which the average time of block production can match the average\ntime of message broadcasting used for blockchain synchronization. In\nparticular, the model and the algorithms are useful to understand efficiency\ncriteria associated with fast blockchains for identifying, e.g., when\nincreasing the block production will have negative impact on the stability of\nthe distributed data structure given the network's broadcast delay.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:26:02 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Pinz\u00f3n", "Carlos", ""], ["Rocha", "Camilo", ""], ["Finke", "Jorge", ""]]}, {"id": "1909.06494", "submitter": "Victor Zakhary", "authors": "Victor Zakhary, Divyakant Agrawal, Amr El Abbadi", "title": "Transactional Smart Contracts in Blockchain Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents TXSC, a framework that provides smart contract developers\nwith transaction primitives. These primitives allow developers to write smart\ncontracts without the need to reason about the anomalies that can arise due to\nconcurrent smart contract function executions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 00:36:13 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zakhary", "Victor", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1909.06496", "submitter": "Saraju Mohanty", "authors": "Saraju P. Mohanty, Venkata P. Yanambaka, Elias Kougianos and Deepak\n  Puthal", "title": "PUFchain: Hardware-Assisted Blockchain for Sustainable Simultaneous\n  Device and Data Security in the Internet of Everything (IoE)", "comments": "37 pages, 22 figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the first-ever blockchain which can simultaneously\nhandle device and data security, which is important for the emerging\nInternet-of-Everything (IoE). This article presents a unique concept of\nblockchain that integrates hardware security primitives called Physical\nUnclonable Functions (PUFs) to solve scalability, latency, and energy\nrequirement challenges and is called PUFchain. Data management and security\n(and privacy) of data, devices, and individuals, are some of the issues in the\nIoE architectures that need to be resolved. Integrating the blockchain into the\nIoE environment can help solve these issues and helps in the aspects of data\nstorage and security. This article introduces a new blockchain architecture\ncalled PUFchain and introduces a new consensus algorithm called \"Proof of\nPUF-Enabled Authentication\" (PoP) for deployment in PUFchain. The proposed PoP\nis the PUF integration into our previously proposed Proof-of-Authentication\n(PoAh) consensus algorithm and can be called \"Hardware-Assisted\nProof-of-Authentication (HA-PoAh)\". However, PUF integration is possible in the\nexisting and new consensus algorithms. PoP utilizes PUFs which are responsible\nfor generating a unique key that cannot be cloned and hence provide the highest\nlevel of security. A PUF uses the nanoelectronic manufacturing variations that\nare introduced during the fabrication of an integrated circuit to generate the\nkeys. Hence, once generated from a PUF module, the keys cannot be cloned or\ngenerated from any other module. PUFchain uses a PUF and Hashing module which\nperforms the necessary cryptographic functions. Hence the mining process is\noffloaded to the hardware module which reduces the processing times. PoP is\napproximately 1,000X faster than the well-established Proof-of-Work (PoW) and\n5X faster than Proof-of-Authentication (PoAh).\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 00:58:39 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mohanty", "Saraju P.", ""], ["Yanambaka", "Venkata P.", ""], ["Kougianos", "Elias", ""], ["Puthal", "Deepak", ""]]}, {"id": "1909.06499", "submitter": "Siyuan Gu", "authors": "Siyuan Gu, Deke Guo, Guoming Tang, Lailong Luo, Yuchen Sun, Xueshan\n  Luo", "title": "HyEdge: Optimal Request Scheduling in Hybrid Edge Computing Environment", "comments": "11 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of Internet of Things (IoT) devices and the arrival\nof the 5G era, edge computing has become an attractive paradigm to serve\nend-users and provide better QoS. Many efforts have been done to provision some\nmerging public network services at the edge. We reveal that it is very common\nthat specific users call for private and isolated edge services to preserve\ndata privacy and enable other security intentions. However, it still remains\nopen to fulfill such kind of mixed requests in edge computing. In this paper,\nwe propose the framework of hybrid edge computing to offer both public and\nprivate edge services systematically. To fully exploit the benefits of this\nnovel framework, we define the problem of optimal request scheduling over a\ngiven placement solution of hybrid edge servers, so as to minimize the response\ndelay. This problem is further modeled as a mixed integer non-linear problem\n(MINLP), which is typically NP-hard. Accordingly, we propose the\npartition-based optimization method, which can efficiently solve this NP-hard\nproblem via the problem decomposition and the branch and bound strategies. We\nfinally conduct extensive evaluations with a real-world dataset to measure the\nperformance of our methods. The results indicate that the proposed method\nachieves elegant performance with low computation complexity.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 01:25:11 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gu", "Siyuan", ""], ["Guo", "Deke", ""], ["Tang", "Guoming", ""], ["Luo", "Lailong", ""], ["Sun", "Yuchen", ""], ["Luo", "Xueshan", ""]]}, {"id": "1909.06526", "submitter": "K. R. Jayaram", "authors": "K. R. Jayaram, Vinod Muthusamy, Parijat Dube, Vatche Ishakian, Chen\n  Wang, Benjamin Herta, Scott Boag, Diana Arroyo, Asser Tantawi, Archit Verma,\n  Falk Pollok, Rania Khalaf", "title": "FfDL : A Flexible Multi-tenant Deep Learning Platform", "comments": "MIDDLEWARE 2019", "journal-ref": null, "doi": "10.1145/3361525.3361538", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) is becoming increasingly popular in several application\ndomains and has made several new application features involving computer\nvision, speech recognition and synthesis, self-driving automobiles, drug\ndesign, etc. feasible and accurate. As a result, large scale on-premise and\ncloud-hosted deep learning platforms have become essential infrastructure in\nmany organizations. These systems accept, schedule, manage and execute DL\ntraining jobs at scale.\n  This paper describes the design, implementation and our experiences with\nFfDL, a DL platform used at IBM. We describe how our design balances\ndependability with scalability, elasticity, flexibility and efficiency. We\nexamine FfDL qualitatively through a retrospective look at the lessons learned\nfrom building, operating, and supporting FfDL; and quantitatively through a\ndetailed empirical evaluation of FfDL, including the overheads introduced by\nthe platform for various deep learning models, the load and performance\nobserved in a real case study using FfDL within our organization, the frequency\nof various faults observed including unanticipated faults, and experiments\ndemonstrating the benefits of various scheduling policies. FfDL has been\nopen-sourced.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 04:02:45 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Jayaram", "K. R.", ""], ["Muthusamy", "Vinod", ""], ["Dube", "Parijat", ""], ["Ishakian", "Vatche", ""], ["Wang", "Chen", ""], ["Herta", "Benjamin", ""], ["Boag", "Scott", ""], ["Arroyo", "Diana", ""], ["Tantawi", "Asser", ""], ["Verma", "Archit", ""], ["Pollok", "Falk", ""], ["Khalaf", "Rania", ""]]}, {"id": "1909.06535", "submitter": "Zhimin Gao", "authors": "Zhimin Gao, Lei Xu, Keshav Kasichainula, Lin Chen, Bogdan Carbunar,\n  Weidong Shi", "title": "Private and Atomic Exchange of Assets over Zero Knowledge Based Payment\n  Ledger", "comments": "Single column, 19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin brings a new type of digital currency that does not rely on a central\nsystem to maintain transactions. By benefiting from the concept of\ndecentralized ledger, users who do not know or trust each other can still\nconduct transactions in a peer-to-peer manner. Inspired by Bitcoin, other\ncryptocurrencies were invented in recent years such as Ethereum, Dash, Zcash,\nMonero, Grin, etc. Some of these focus on enhancing privacy for instance crypto\nnote or systems that apply the similar concept of encrypted notes used for\ntransactions to enhance privacy (e.g., Zcash, Monero). However, there are few\nmechanisms to support the exchange of privacy-enhanced notes or assets on the\nchain, and at the same time preserving the privacy of the exchange operations.\nExisting approaches for fair exchanges of assets with privacy mostly rely on\noff-chain/side-chain, escrow or centralized services. Thus, we propose a\nsolution that supports oblivious and privacy-protected fair exchange of crypto\nnotes or privacy enhanced crypto assets. The technology is demonstrated by\nextending zero-knowledge based crypto notes. To address \"privacy\" and\n\"multi-currency\", we build a new zero-knowledge proving system and extend note\nformat with new property to represent various types of tokenized assets or\ncryptocurrencies. By extending the payment protocol, exchange operations are\nrealized through privacy enhanced transactions (e.g., shielded transactions).\nBased on the possible scenarios during the exchange operation, we add new\nconstraints and conditions to the zero-knowledge proving system used for\nvalidating transactions publicly.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 05:14:26 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gao", "Zhimin", ""], ["Xu", "Lei", ""], ["Kasichainula", "Keshav", ""], ["Chen", "Lin", ""], ["Carbunar", "Bogdan", ""], ["Shi", "Weidong", ""]]}, {"id": "1909.06587", "submitter": "Jun Zhao", "authors": "Jun Zhao, Jing Tang, Li Zengxiang, Huaxiong Wang, Kwok-Yan Lam,\n  Kaiping Xue", "title": "An Analysis of Blockchain Consistency in Asynchronous Networks: Deriving\n  a Neat Bound", "comments": "This paper appears in the Proceedings of IEEE International\n  Conference on Distributed Computing Systems (ICDCS) 2020. Please feel free to\n  contact us for questions or remarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal analyses of blockchain protocols have received much attention\nrecently. Consistency results of Nakamoto's blockchain protocol are often\nexpressed in a quantity $c$, which denotes the expected number of network\ndelays before some block is mined. With $\\mu$ (resp., $\\nu$) denoting the\nfraction of computational power controlled by benign miners (resp., the\nadversary), where $\\mu + \\nu = 1$, we prove for the first time that to ensure\nthe consistency property of Nakamoto's blockchain protocol in an asynchronous\nnetwork, it suffices to have $c$ to be just slightly greater than\n$\\frac{2\\mu}{\\ln (\\mu/\\nu)}$. Such a result is both neater and stronger than\nexisting ones. In the proof, we formulate novel Markov chains which\ncharacterize the numbers of mined blocks in different rounds.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 12:43:53 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 14:59:35 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhao", "Jun", ""], ["Tang", "Jing", ""], ["Zengxiang", "Li", ""], ["Wang", "Huaxiong", ""], ["Lam", "Kwok-Yan", ""], ["Xue", "Kaiping", ""]]}, {"id": "1909.06662", "submitter": "Christian G\\\"ottel", "authors": "Christian G\\\"ottel, Pascal Felber and Valerio Schiavoni", "title": "iperfTZ: Understanding Network Bottlenecks for TrustZone-based Trusted\n  Applications", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": "In: M. Ghaffari, M. Nesterenko, S. Tixeuil, S. Tucci, Y. Yamauchi\n  (eds) SSS 2019. Lecture Notes in Computer Science, vol 11914. Springer, Cham", "doi": "10.1007/978-3-030-34992-9_15", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing availability of hardware-based trusted execution environments\n(TEEs) in commodity processors has recently advanced support (i.e., design,\nimplementation and deployment frameworks) for network-based secure services.\nExamples of such TEEs include ARM TrustZone or Intel SGX, largely available in\nembedded, mobile and server-grade processors. TEEs shield services from\ncompromised hosts, malicious users or powerful attackers. TEE-enabled devices\nare largely being deployed on the edge of the network, paving the way for\nlarge-scale deployments of trusted applications. These applications allow\nprocessing and disseminating sensitive data without having to trust cloud\nproviders. However, uncovering network performance limitations of such trusted\napplications is difficult and currently lacking, despite the interest and\nreliance by developers and system deployers. iperfTZ is an open-source tool to\nuncover network performance bottlenecks rooted at the design and implementation\nof trusted applications for ARM TrustZone and underlying runtime systems. Our\nevaluation based on micro-benchmarks shows current trade-offs for trusted\napplications, both from a network as well as an energy perspective; an often\noverlooked yet relevant aspect for edge-based deployments.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 19:38:43 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 15:53:03 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["G\u00f6ttel", "Christian", ""], ["Felber", "Pascal", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1909.06811", "submitter": "Ran Gelles", "authors": "Yagel Ashkenazi, Ran Gelles, Amir Leshem", "title": "Noisy Beeping Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce noisy beeping networks, where nodes have limited communication\ncapabilities, namely, they can only emit energy or sense the channel for\nenergy. Furthermore, imperfections may cause devices to malfunction with some\nfixed probability when sensing the channel, which amounts to deducing a noisy\nreceived transmission. Such noisy networks have implications for\nultra-lightweight sensor networks and biological systems.\n  We show how to compute tasks in a noise-resilient manner over noisy beeping\nnetworks of arbitrary structure. In particular, we transform any algorithm that\nassumes a noiseless beeping network (of size $n$) into a noise-resilient\nversion while incurring a multiplicative overhead of only $O(\\log n)$ in its\nround complexity, with high probability. We show that our coding is optimal for\nsome tasks, such as node-coloring of a clique.\n  We further show how to simulate a large family of algorithms designed for\ndistributed networks in the CONGEST($B$) model over a noisy beeping network.\nThe simulation succeeds with high probability and incurs an asymptotic\nmultiplicative overhead of $O(B\\cdot \\Delta \\cdot \\min(n,\\Delta^2))$ in the\nround complexity, where $\\Delta$ is the maximal degree of the network. The\noverhead is tight for certain graphs, e.g., a clique. Further, this simulation\nimplies a constant overhead coding for constant-degree networks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 15:00:36 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ashkenazi", "Yagel", ""], ["Gelles", "Ran", ""], ["Leshem", "Amir", ""]]}, {"id": "1909.06842", "submitter": "Yuxin Wang", "authors": "Yuxin Wang, Qiang Wang, Shaohuai Shi, Xin He, Zhenheng Tang, Kaiyong\n  Zhao, Xiaowen Chu", "title": "Benchmarking the Performance and Energy Efficiency of AI Accelerators\n  for AI Training", "comments": "Revised some minor issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become widely used in complex AI applications. Yet,\ntraining a deep neural network (DNNs) model requires a considerable amount of\ncalculations, long running time, and much energy. Nowadays, many-core AI\naccelerators (e.g., GPUs and TPUs) are designed to improve the performance of\nAI training. However, processors from different vendors perform dissimilarly in\nterms of performance and energy consumption. To investigate the differences\namong several popular off-the-shelf processors (i.e., Intel CPU, NVIDIA GPU,\nAMD GPU, and Google TPU) in training DNNs, we carry out a comprehensive\nempirical study on the performance and energy efficiency of these processors by\nbenchmarking a representative set of deep learning workloads, including\ncomputation-intensive operations, classical convolutional neural networks\n(CNNs), recurrent neural networks (LSTM), Deep Speech 2, and Transformer.\nDifferent from the existing end-to-end benchmarks which only present the\ntraining time, We try to investigate the impact of hardware, vendor's software\nlibrary, and deep learning framework on the performance and energy consumption\nof AI training. Our evaluation methods and results not only provide an\ninformative guide for end-users to select proper AI accelerators, but also\nexpose some opportunities for the hardware vendors to improve their software\nlibrary.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 17:30:05 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 06:25:01 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 19:55:20 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 17:28:19 GMT"}, {"version": "v5", "created": "Thu, 21 Nov 2019 05:30:27 GMT"}, {"version": "v6", "created": "Sat, 23 May 2020 17:32:51 GMT"}, {"version": "v7", "created": "Tue, 26 May 2020 01:56:31 GMT"}, {"version": "v8", "created": "Fri, 10 Jul 2020 10:42:11 GMT"}, {"version": "v9", "created": "Thu, 8 Oct 2020 18:29:34 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wang", "Yuxin", ""], ["Wang", "Qiang", ""], ["Shi", "Shaohuai", ""], ["He", "Xin", ""], ["Tang", "Zhenheng", ""], ["Zhao", "Kaiyong", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1909.06849", "submitter": "Mahla Rahati-Quchani", "authors": "Mahla Rahati-Quchani, Saeid Abrishami, Mehdi Feizi", "title": "An Efficient Mechanism for Computation Offloading in Mobile-Edge\n  Computing", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) is a promising technology that provides cloud and\nIT services within the proximity of the mobile user. With the increasing number\nof mobile applications, mobile devices (MD) encounter limitations of their\nresources, such as battery life and computation capacity. The computation\noffloading in MEC can help mobile users to reduce battery usage and speed up\ntask execution. Although there are many solutions for offloading in MEC, most\nusually only employ one MEC server for improving mobile device energy\nconsumption and execution time. Instead of conventional centralized\noptimization methods, the current paper considers a decentralized optimization\nmechanism between MEC servers and users. In particular, an assignment mechanism\ncalled school choice is employed to assist heterogeneous users to select\ndifferent MEC operators in a distributed environment. With this mechanism, each\nuser can benefit from minimizing the price and energy consumption of executing\ntasks while also meeting the specified deadline. The present research has\ndesigned an efficient mechanism for a computation offloading scheme that\nachieves minimal price and energy consumption under latency constraints.\nNumerical results demonstrate that the proposed algorithm can attain efficient\nand successful computation offloading.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 18:01:07 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 19:12:54 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Rahati-Quchani", "Mahla", ""], ["Abrishami", "Saeid", ""], ["Feizi", "Mehdi", ""]]}, {"id": "1909.06895", "submitter": "Tam\\'as Lukovszki", "authors": "Attila Hideg, Tamas Lukovszki", "title": "Asynchronous Filling by Myopic Luminous Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of filling an unknown area represented by an\narbitrary connected graph of $n$ vertices by mobile luminous robots. In this\nproblem, the robots enter the graph one-by-one through a specific vertex,\ncalled the Door, and they eventually have to cover all vertices of the graph\nwhile avoiding collisions. The robots are anonymous and make decisions driven\nby the same local rule of behavior. They have limited persistent memory and\nlimited visibility range. We investigate the Filling problem in the\nasynchronous model.\n  We assume that the robots know an upper bound $\\Delta$ on the maximum degree\nof the graph before entering. We present an algorithm solving the asynchronous\nFilling problem with robots having $1$ hop visibility range, $O(\\log\\Delta)$\nbits of persistent storage, and $\\Delta+4$ colors, including the color when the\nlight is off. We analyze the algorithm in terms of asynchronous rounds, where a\nround means the smallest time interval in which each robot, which has not yet\nfinished the algorithm, has been activated at least once. We show that this\nalgorithm needs $O(n^2)$ asynchronous rounds. Our analysis provides the first\nasymptotic upper bound on the running time in terms of asynchronous rounds.\n  Then we show how the number of colors can be reduced to $O(1)$ at the cost of\nthe running time. The algorithm with $1$ hop visibility range, $O(\\log \\Delta)$\nbits of persistent memory, and $O(1)$ colors needs $O(n^2\\log \\Delta)$ rounds.\nWe show how the running time can be improved by robots with a visibility range\nof $2$ hops, $O(\\log \\Delta)$ bits of persistent memory, and $\\Delta + 4$\ncolors (including the color when the light is off). We show that the algorithm\nneeds $O(n)$ asynchronous rounds. Finally, we show how to extend our solution\nto the $k$-Door case, $k\\geq 2$, by using $\\Delta + k + 4$ colors, including\nthe color when the light is off.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 22:04:54 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 20:12:30 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Hideg", "Attila", ""], ["Lukovszki", "Tamas", ""]]}, {"id": "1909.06951", "submitter": "Kiwan Maeng", "authors": "Kiwan Maeng, Alexei Colin, Brandon Lucia", "title": "Alpaca: Intermittent Execution without Checkpoints", "comments": "Extended version of an OOPSLA 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of energy harvesting devices creates the potential for\nbatteryless sensing and computing devices. Such devices operate only\nintermittently, as energy is available, presenting a number of challenges for\nsoftware developers. Programmers face a complex design space requiring\nreasoning about energy, memory consistency, and forward progress. This paper\nintroduces Alpaca, a low-overhead programming model for intermittent computing\non energy-harvesting devices. Alpaca programs are composed of a sequence of\nuser-defined tasks. The Alpaca runtime preserves execution progress at the\ngranularity of a task. The key insight in Alpaca is the privatization of data\nshared between tasks. Updates of shared values in a task are privatized and\nonly committed to main memory on successful execution of the task, ensuring\nthat data remain consistent despite power failures. Alpaca provides a familiar\nprogramming interface and a highly efficient runtime model. We also present an\nalternate version of Alpaca, Alpaca-undo, that uses undo-logging and rollback\ninstead of privatization and commit. We implemented a prototype of both\nversions of Alpaca as an extension to C with an LLVM compiler pass. We\nevaluated Alpaca, and directly compared to three systems from prior work.\nAlpacaconsistently improves performance compared to the previous systems, by up\nto 23.8x, while also improving memory footprint in many cases, by up to 17.6x.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:55:37 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Maeng", "Kiwan", ""], ["Colin", "Alexei", ""], ["Lucia", "Brandon", ""]]}, {"id": "1909.07166", "submitter": "Smriti Prathapan", "authors": "Kaushik Velusamy, Smriti Prathapan, Milton Halem", "title": "Exploring the Behavior of Coherent Accelerator Processor Interface\n  (CAPI) on IBM Power8+ Architecture and FlashSystem 900", "comments": "18 pages, 7 figures, 3 tables, Accepted for publication at 2019\n  International Workshop on OpenPOWER for HPC (IWOPH19) International\n  Supercomputing Conference HPC Frankfurt, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Coherent Accelerator Processor Interface (CAPI) is a general term for the\ninfrastructure that provides high throughput and low latency path to the flash\nstorage connected to the IBM POWER 8+ System. CAPI accelerator card is attached\ncoherently as a peer to the Power8+ processor. This removes the overhead and\ncomplexity of the IO subsystem and allows the accelerator to operate as part of\nan application. In this paper, we present the results of experiments on IBM\nFlashSystem900 (FS900) with CAPI accelerator card using the \"CAPI-Flash IBM\nData Engine for NoSQL Software\" Library. This library provides the application,\na direct access to the underlying flash storage through user space APIs, to\nmanage and access the data in flash. This offloads kernel IO driver\nfunctionality to dedicated CAPI FPGA accelerator hardware. We conducted\nexperiments to analyze the performance of FS900 with CAPI accelerator card,\nusing the Key Value Layer APIs, employing NASA's MODIS Land Surface Reflectance\ndataset as a large dataset use case. We performed Read and Write operations on\ndatasets of size ranging from 1MB to 3TB by varying the number of threads. We\nthen compared this performance with other heterogeneous storage and memory\ndevices such as NVM, SSD and RAM, without using the CAPI Accelerator in\nsynchronous and asynchronous file IO modes of operations. The results indicate\nthat FS900 & CAPI, together with the metadata cache in RAM, delivers the\nhighest IO/s and OP/s for read operations. This was higher than just using RAM,\nalong with utilizing lesser CPU resources. Among FS900, SSD and NVM, FS900 had\nthe highest write IO/s. Another important observation is that, when the size of\nthe input dataset exceeds the capacity of RAM, and when the data access is\nnon-uniform and sparse, FS900 with CAPI would be a cost-effective alternative.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:45:37 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Velusamy", "Kaushik", ""], ["Prathapan", "Smriti", ""], ["Halem", "Milton", ""]]}, {"id": "1909.07168", "submitter": "Anthony Boulmier", "authors": "Anthony Boulmier, Franck Raynaud, Nabil Abdennadher, Bastien Chopard", "title": "On the Benefits of Anticipating Load Imbalance for Performance\n  Optimization of Parallel Applications", "comments": "Accepted at IEEE Cluster 2019, Albuquerque New Mexico, 9 pages, 5\n  figures,", "journal-ref": null, "doi": "10.1109/CLUSTER.2019.8890998", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parallel iterative applications, computational efficiency is essential for\naddressing large problems. Load imbalance is one of the major performance\ndegradation factors of parallel applications. Therefore, distributing,\ncleverly, and as evenly as possible, the workload among processing elements\n(PE) maximizes application performance. So far, the standard load balancing\nmethod consists in distributing the workload evenly between PEs and, when load\nimbalance appears, redistributing the extra load from overloaded PEs to\nunderloaded PEs. However, this does not anticipate the load imbalance growth\nthat may continue during the next iterations. In this paper, we present a first\nstep toward a novel philosophy of load balancing that unloads the PEs that will\nbe overloaded in the near future to let the application rebalance itself via\nits own dynamics. Herein, we present a formal definition of our new approach\nusing a simple mathematical model and discuss its advantages compared to the\nstandard load balancing method. In addition to the theoretical study, we apply\nour method to an application that reproduces the computation of a fluid model\nwith non-uniform erosion. The performance validates the benefit of anticipating\nload imbalance. We observed up to 16% performance improvement compared to the\nstandard load balancing method.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 12:57:01 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Boulmier", "Anthony", ""], ["Raynaud", "Franck", ""], ["Abdennadher", "Nabil", ""], ["Chopard", "Bastien", ""]]}, {"id": "1909.07190", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda and Arjun Guha", "title": "Model-Based Warp Overlapped Tiling for Image Processing Programs on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific languages that execute image processing pipelineson GPUs,\nsuch as Halide and Forma, operate by 1) dividing the image into overlapped\ntiles, and 2) fusing loops to improve memory locality. However, current\napproaches have limitations: 1) they require intra thread block\nsynchronization, which has a non-trivial cost, 2) they must choose between\nsmall tiles that require more overlapped computations or large tiles that\nincrease shared memory access (and lowers occupancy), and 3) their\nautoscheduling algorithms use simplified GPU models that can result in\ninefficient global memory accesses. We present a new approach for executing\nimage processing pipelines on GPUs that addresses these limitations as follows.\n1) We fuse loops to form overlapped tiles that fit in a single warp, which\nallows us to use lightweight warp synchronization. 2) We introduce hybrid\ntiling, which stores overlapped regions in a combination of thread-local\nregisters and shared memory. Thus hybrid tiling either increases occupancy by\ndecreasing shared memory usage or decreases overlapping computations using\nlarger tiles. 3) We present an automatic loop fusion algorithm that considers\nseveral factors that affect the performance of GPU kernels. We implement these\ntechniques in PolyMage-GPU, which is a new GPU backend for PolyMage. Our\napproach produces code that is faster than Halide's manual schedules: 1.65x\nfaster on an NVIDIA GTX 1080Ti and 1.33 faster on an NVIDIA Tesla V100.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 13:34:25 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 14:51:53 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Jangda", "Abhinav", ""], ["Guha", "Arjun", ""]]}, {"id": "1909.07251", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley", "title": "Note on distributed certification of minimum spanning trees", "comments": "A note explaining a known result. (New version, with minor changes.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed proof (also known as local certification, or proof-labeling\nscheme) is a mechanism to certify that the solution to a graph problem is\ncorrect. It takes the form of an assignment of labels to the nodes, that can be\nchecked locally. There exists such a proof for the minimum spanning tree\nproblem, using $O(\\log n \\log W)$ bit labels (where $n$ is the number of nodes\nin the graph, and $W$ is the largest weight of an edge). This is due to Korman\nand Kutten who describe it in concise and formal manner in [Korman and Kutten\n07]. In this note, we propose a more intuitive description of the result, as\nwell as a gentle introduction to the problem.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:55:31 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 15:36:31 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Feuilloley", "Laurent", ""]]}, {"id": "1909.07372", "submitter": "Prayitno Prayitno", "authors": "Karisma Trinanda Putra, Jing-Doo Wang, Eko Prasetyo, Prayitno", "title": "Modeling Traffic Congestion with Spatiotemporal Big Data for An\n  Intelligent Freeway Monitoring System", "comments": "The article was published without the co-Author's notice, and it is\n  withdrawn due to his objection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traffic congestion is a complex, nonlinear spatiotemporal modeling problem.\nBy collecting and analyzing a vast quantity and different categories of\ninformation, traffic flow, and road congestion can be predicted and controlled\non an intelligent transportation system. This report provides an analysis of\ntraveling time across Taiwan from North to South, vice versa. We analyze\ntraffic in a national freeway between Tainan and Kaohsiung section, which\nrepresents the common trip of the population in Southern Taiwan. The data is\nrecorded using the Electronic Toll Collection System (ETC) provided by Ministry\nof Transportation in Taiwan. We use MapReduce framework to process data into a\nsmaller task which can be distributed on several computer clusters to speed up\nthe process. The results show that the spatiotemporal model of traffic flow is\nstrongly influenced by direction, working hour, and holidays with a recurring\npattern for each week. The distinctive pattern inside the spatiotemporal\ndataset can be used on an AI-powered decision-making system for future\ndevelopment.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 07:26:39 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 21:30:34 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Putra", "Karisma Trinanda", ""], ["Wang", "Jing-Doo", ""], ["Prasetyo", "Eko", ""], ["Prayitno", "", ""]]}, {"id": "1909.07433", "submitter": "Clinton Ehrlich", "authors": "Clinton Ehrlich, Anna Guzova", "title": "KRNC: New Foundations for Permissionless Byzantine Consensus and Global\n  Monetary Stability", "comments": "104 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies biomimetic engineering to the problem of permissionless\nByzantine consensus and achieves results that surpass the prior state of the\nart by four orders of magnitude. It introduces a biologically inspired\nasymmetric Sybil-resistance mechanism, Proof-of-Balance, which can replace\nsymmetric Proof-of-Work and Proof-of-Stake weighting schemes.\n  The biomimetic mechanism is incorporated into a permissionless blockchain\nprotocol, Key Retroactivity Network Consensus (\"KRNC\"), which delivers ~40,000\ntimes the security and speed of today's decentralized ledgers. KRNC allows the\nfiat money that the public already owns to be upgraded with cryptographic\ninflation protection, eliminating the problems inherent in bootstrapping new\ncurrencies like Bitcoin and Ethereum.\n  The paper includes two independently significant contributions to the\nliterature. First, it replaces the non-structural axioms invoked in prior work\nwith a new formal method for reasoning about trust, liveness, and safety from\nfirst principles. Second, it demonstrates how two previously overlooked\nexploits, book-prize attacks and pseudo-transfer attacks, collectively\nundermine the security guarantees of all prior permissionless ledgers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 18:43:35 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 20:15:13 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 02:05:55 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 03:55:55 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ehrlich", "Clinton", ""], ["Guzova", "Anna", ""]]}, {"id": "1909.07437", "submitter": "Hyoukjun Kwon", "authors": "Hyoukjun Kwon, Liangzhen Lai, Michael Pellauer, Tushar Krishna,\n  Yu-Hsin Chen, Vikas Chandra", "title": "Heterogeneous Dataflow Accelerators for Multi-DNN Workloads", "comments": "This paper is accepted at HPCA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging AI-enabled applications such as augmented/virtual reality (AR/VR)\nleverage multiple deep neural network (DNN) models for sub-tasks such as object\ndetection, hand tracking, and so on. Because of the diversity of the sub-tasks,\nthe layers within and across the DNN models are highly heterogeneous in\noperation and shape. Such layer heterogeneity is a challenge for a fixed\ndataflow accelerator (FDA) that employs a fixed dataflow on a single\naccelerator substrate since each layer prefers different dataflows (computation\norder and parallelization) and tile sizes. Reconfigurable DNN accelerators\n(RDAs) have been proposed to adapt their dataflows to diverse layers to address\nthe challenge. However, the dataflow flexibility in RDAs is enabled at the area\nand energy costs of expensive hardware structures (switches, controller, etc.)\nand per-layer reconfiguration.\n  Alternatively, this work proposes a new class of accelerators, heterogeneous\ndataflow accelerators (HDAs), which deploys multiple sub-accelerators each\nsupporting a different dataflow. HDAs enable coarser-grained dataflow\nflexibility than RDAs with higher energy efficiency and lower area cost\ncomparable to FDAs. To exploit such benefits, hardware resource partitioning\nacross sub-accelerators and layer execution schedule need to be carefully\noptimized. Therefore, we also present Herald, which co-optimizes hardware\npartitioning and layer execution schedule. Using Herald on a suite of AR/VR and\nMLPerf workloads, we identify a promising HDA architecture, Maelstrom, which\ndemonstrates 65.3% lower latency and 5.0% lower energy than the best FDAs and\n22.0% lower energy at the cost of 20.7% higher latency than a state-of-the-art\nRDA. The results suggest that HDA is an alternative class of Pareto-optimal\naccelerators to RDA with strength in energy, which can be a better choice than\nRDAs depending on the use cases.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:46:13 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 19:05:20 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 13:23:56 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2020 02:27:29 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Kwon", "Hyoukjun", ""], ["Lai", "Liangzhen", ""], ["Pellauer", "Michael", ""], ["Krishna", "Tushar", ""], ["Chen", "Yu-Hsin", ""], ["Chandra", "Vikas", ""]]}, {"id": "1909.07439", "submitter": "Jeroen B\\'edorf", "authors": "Jeroen B\\'edorf, Simon Portegies Zwart", "title": "Bonsai-SPH: A GPU accelerated astrophysical Smoothed Particle\n  Hydrodynamics code", "comments": "Updated intro and multi-GPU sections. 23 pages, 9 figures. Submission\n  to SciPost", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the smoothed-particle hydrodynamics simulation code, Bonsai-SPH,\nwhich is a continuation of our previously developed gravity-only hierarchical\n$N$-body code (called Bonsai). The code is optimized for Graphics Processing\nUnit (GPU) accelerators which enables researchers to take advantage of these\npowerful computational resources. Bonsa-SPH produces simulation results\ncomparable with state-of-the-art, CPU based, codes, but using an order of\nmagnitude less computation time. The code is freely available online and the\ndetails are described in this work.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 19:16:25 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 15:40:14 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["B\u00e9dorf", "Jeroen", ""], ["Zwart", "Simon Portegies", ""]]}, {"id": "1909.07452", "submitter": "Paritosh Ramanan", "authors": "Paritosh Ramanan, Kiyoshi Nakayama", "title": "BAFFLE : Blockchain Based Aggregator Free Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aspect of Federated Learning (FL) is the requirement of a centralized\naggregator to maintain and update the global model. However, in many cases\norchestrating a centralized aggregator might be infeasible due to numerous\noperational constraints. In this paper, we introduce BAFFLE, an aggregator\nfree, blockchain driven, FL environment that is inherently decentralized.\nBAFFLE leverages Smart Contracts (SC) to coordinate the round delineation,\nmodel aggregation and update tasks in FL. BAFFLE boosts computational\nperformance by decomposing the global parameter space into distinct chunks\nfollowed by a score and bid strategy. In order to characterize the performance\nof BAFFLE, we conduct experiments on a private Ethereum network and use the\ncentralized and aggregator driven methods as our benchmark. We show that BAFFLE\nsignificantly reduces the gas costs for FL on the blockchain as compared to a\ndirect adaptation of the aggregator based method. Our results also show that\nBAFFLE achieves high scalability and computational efficiency while delivering\nsimilar accuracy as the benchmark methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 19:47:26 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 23:00:13 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 17:57:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ramanan", "Paritosh", ""], ["Nakayama", "Kiyoshi", ""]]}, {"id": "1909.07453", "submitter": "Pierre Tholoniat", "authors": "Pierre Tholoniat, Vincent Gramoli", "title": "Formal Verification of Blockchain Byzantine Fault Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To implement a blockchain, the trend is now to integrate a non-trivial\nByzantine fault tolerant consensus algorithm instead of the seminal idea of\nwaiting to receive blocks to decide upon the longest branch. After a decade of\nexistence, blockchains trade now large amounts of valuable assets and a simple\ndisagreement could lead to disastrous losses. Unfortunately, Byzantine\nconsensus solutions used in blockchains are at best proved correct \"by hand\" as\nwe are not aware of any of them having been formally verified. In this paper,\nwe propose two contributions: (i) we illustrate the severity of the problem by\nlisting six vulnerabilities of blockchain consensus including two new\ncounter-examples; (ii) we then formally verify two Byzantine fault tolerant\ncomponents of Red Belly Blockchain using the ByMC model checker. First, we\nspecify a simple broadcast primitive in 116 lines of code that is verified in\n40 seconds on a 2-core Intel machine. Then, we specify a blockchain consensus\nalgorithm in 276 lines of code that is verified in 17 minutes on a 64-core AMD\nmachine using MPI. To conclude, we argue that it has now become both relatively\nsimple and crucial to formally verify the correctness of blockchain consensus\nprotocols.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 19:47:48 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 18:31:05 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Tholoniat", "Pierre", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1909.07588", "submitter": "Tianyi Chen", "authors": "Jun Sun, Tianyi Chen, Georgios B. Giannakis, and Zaiyue Yang", "title": "Communication-Efficient Distributed Learning via Lazily Aggregated\n  Quantized Gradients", "comments": "Accepted in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper develops a novel aggregated gradient approach for\ndistributed machine learning that adaptively compresses the gradient\ncommunication. The key idea is to first quantize the computed gradients, and\nthen skip less informative quantized gradient communications by reusing\noutdated gradients. Quantizing and skipping result in `lazy' worker-server\ncommunications, which justifies the term Lazily Aggregated Quantized gradient\nthat is henceforth abbreviated as LAQ. Our LAQ can provably attain the same\nlinear convergence rate as the gradient descent in the strongly convex case,\nwhile effecting major savings in the communication overhead both in transmitted\nbits as well as in communication rounds. Empirically, experiments with real\ndata corroborate a significant communication reduction compared to existing\ngradient- and stochastic gradient-based algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 04:53:24 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Sun", "Jun", ""], ["Chen", "Tianyi", ""], ["Giannakis", "Georgios B.", ""], ["Yang", "Zaiyue", ""]]}, {"id": "1909.07673", "submitter": "Marcelo Pasin", "authors": "Leonardo R. Rodrigues, Marcelo Pasin, Omir C. Alves Jr., Charles C.\n  Miers, Mauricio A. Pillon, Pascal Felber, Guilherme P. Koslovski", "title": "Network-Aware Container Scheduling in Multi-Tenant Data Center", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681). Preprint of paper to appear at\n  2019 IEEE GLOBECOM, Global Communications Conference. 9-13 December 2019,\n  Waikaloa (HI), USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network management on multi-tenant container-based data centers has critical\nimpact on performance. Tenants encapsulate applications in containers\nabstracting away details on hosting infrastructures, and entrust data centers\nmanagement framework with the provisioning of network QoS requirements. In this\npaper, we propose a network-aware multi-criteria container scheduler to jointly\nprocess containers and network requirements. We introduce a new Mixed Integer\nLinear Programming formulation for network-aware scheduling encompassing both\ntenants and providers metrics. We describe two GPU-accelerated modules to\naddress the complexity barrier of the problem and efficiently process\nscheduling requests. Our experiments show that our scheduling approach\naccounting for both network and containers outperforms traditional algorithms\nused by containers orchestrators.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:33:26 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Rodrigues", "Leonardo R.", ""], ["Pasin", "Marcelo", ""], ["Alves", "Omir C.", "Jr."], ["Miers", "Charles C.", ""], ["Pillon", "Mauricio A.", ""], ["Felber", "Pascal", ""], ["Koslovski", "Guilherme P.", ""]]}, {"id": "1909.07817", "submitter": "Shantenu Jha", "authors": "Hyungro Lee, Heng Ma, Matteo Turilli, Debsindhu Bhowmik, Shantenu Jha,\n  Arvind Ramanathan", "title": "DeepDriveMD: Deep-Learning Driven Adaptive Molecular Simulations for\n  Protein Folding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulations of biological macromolecules play an important role in\nunderstanding the physical basis of a number of complex processes such as\nprotein folding. Even with increasing computational power and evolution of\nspecialized architectures, the ability to simulate protein folding at atomistic\nscales still remains challenging. This stems from the dual aspects of high\ndimensionality of protein conformational landscapes, and the inability of\natomistic molecular dynamics (MD) simulations to sufficiently sample these\nlandscapes to observe folding events. Machine learning/deep learning (ML/DL)\ntechniques, when combined with atomistic MD simulations offer the opportunity\nto potentially overcome these limitations by: (1) effectively reducing the\ndimensionality of MD simulations to automatically build latent representations\nthat correspond to biophysically relevant reaction coordinates (RCs), and (2)\ndriving MD simulations to automatically sample potentially novel conformational\nstates based on these RCs. We examine how coupling DL approaches with MD\nsimulations can fold small proteins effectively on supercomputers. In\nparticular, we study the computational costs and effectiveness of scaling\nDL-coupled MD workflows by folding two prototypical systems, viz., Fs-peptide\nand the fast-folding variant of the villin head piece protein. We demonstrate\nthat a DL driven MD workflow is able to effectively learn latent\nrepresentations and drive adaptive simulations. Compared to traditional\nMD-based approaches, our approach achieves an effective performance gain in\nsampling the folded states by at least 2.3x. Our study provides a quantitative\nbasis to understand how DL driven MD simulations, can lead to effective\nperformance gains and reduced times to solution on supercomputing resources.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:58:47 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Lee", "Hyungro", ""], ["Ma", "Heng", ""], ["Turilli", "Matteo", ""], ["Bhowmik", "Debsindhu", ""], ["Jha", "Shantenu", ""], ["Ramanathan", "Arvind", ""]]}, {"id": "1909.07865", "submitter": "Daniele De Sensi", "authors": "Daniele De Sensi, Salvatore Di Girolamo, Torsten Hoefler", "title": "Mitigating Network Noise on Dragonfly Networks through Application-Aware\n  Routing", "comments": "Accepted at The International Conference for High Performance\n  Computing Networking, Storage, and Analysis (SC '19)", "journal-ref": "Published in Proceedings of The International Conference for High\n  Performance Computing Networking, Storage, and Analysis (SC '19) (2019)", "doi": "10.1145/3295500.3356196", "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System noise can negatively impact the performance of HPC systems, and the\ninterconnection network is one of the main factors contributing to this\nproblem. To mitigate this effect, adaptive routing sends packets on non-minimal\npaths if they are less congested. However, while this may mitigate interference\ncaused by congestion, it also generates more traffic since packets traverse\nadditional hops, causing in turn congestion on other applications and on the\napplication itself. In this paper, we first describe how to estimate network\nnoise. By following these guidelines, we show how noise can be reduced by using\nrouting algorithms which select minimal paths with a higher probability. We\nexploit this knowledge to design an algorithm which changes the probability of\nselecting minimal paths according to the application characteristics. We\nvalidate our solution on microbenchmarks and real-world applications on two\nsystems relying on a Dragonfly interconnection network, showing noise reduction\nand performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:54:09 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["De Sensi", "Daniele", ""], ["Di Girolamo", "Salvatore", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1909.08006", "submitter": "Yuanjing Shi", "authors": "Yuanjing Shi, Zhaoxing Li", "title": "Leyenda: An Adaptive, Hybrid Sorting Algorithm for Large Scale Data with\n  Limited Memory", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is the one of the fundamental tasks of modern data management\nsystems. With Disk I/O being the most-accused performance bottleneck and more\ncomputation-intensive workloads, it has come to our attention that in\nheterogeneous environment, performance bottleneck may vary among different\ninfrastructure. As a result, sort kernels need to be adaptive to changing\nhardware conditions. In this paper, we propose Leyenda, a hybrid, parallel and\nefficient Radix Most-Significant-Bit (MSB) MergeSort algorithm, with\nutilization of local thread-level CPU cache and efficient disk/memory I/O.\nLeyenda is capable of performing either internal or external sort efficiently,\nbased on different I/O and processing conditions. We benchmarked Leyenda with\nthree different workloads from Sort Benchmark, targeting three unique use\ncases, including internal, partially in-memory and external sort, and we found\nLeyenda to outperform GNU's parallel in-memory quick/merge sort implementations\nby up to three times. Leyenda is also ranked the second best external sort\nalgorithm on ACM 2019 SIGMOD programming contest and forth overall.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:10:04 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Shi", "Yuanjing", ""], ["Li", "Zhaoxing", ""]]}, {"id": "1909.08029", "submitter": "Qinyi Luo", "authors": "Qinyi Luo, Jiaao He, Youwei Zhuo, Xuehai Qian", "title": "Heterogeneity-Aware Asynchronous Decentralized Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed deep learning training usually adopts All-Reduce as the\nsynchronization mechanism for data parallel algorithms due to its high\nperformance in homogeneous environment. However, its performance is bounded by\nthe slowest worker among all workers, and is significantly slower in\nheterogeneous situations. AD-PSGD, a newly proposed synchronization method\nwhich provides numerically fast convergence and heterogeneity tolerance,\nsuffers from deadlock issues and high synchronization overhead. Is it possible\nto get the best of both worlds - designing a distributed training method that\nhas both high performance as All-Reduce in homogeneous environment and good\nheterogeneity tolerance as AD-PSGD? In this paper, we propose Ripples, a\nhigh-performance heterogeneity-aware asynchronous decentralized training\napproach. We achieve the above goal with intensive synchronization\noptimization, emphasizing the interplay between algorithm and system\nimplementation. To reduce synchronization cost, we propose a novel\ncommunication primitive Partial All-Reduce that allows a large group of workers\nto synchronize quickly. To reduce synchronization conflict, we propose static\ngroup scheduling in homogeneous environment and simple techniques (Group Buffer\nand Group Division) to avoid conflicts with slightly reduced randomness. Our\nexperiments show that in homogeneous environment, Ripples is 1.1 times faster\nthan the state-of-the-art implementation of All-Reduce, 5.1 times faster than\nParameter Server and 4.3 times faster than AD-PSGD. In a heterogeneous setting,\nRipples shows 2 times speedup over All-Reduce, and still obtains 3 times\nspeedup over the Parameter Server baseline.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:57:49 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Luo", "Qinyi", ""], ["He", "Jiaao", ""], ["Zhuo", "Youwei", ""], ["Qian", "Xuehai", ""]]}, {"id": "1909.08069", "submitter": "Hong-Ning Dai Prof.", "authors": "Hong-Ning Dai and Raymond Chi-Wing Wong and Hao Wang and Zibin Zheng\n  and Athanasios V. Vasilakos", "title": "Big Data Analytics for Large Scale Wireless Networks: Challenges and\n  Opportunities", "comments": "29 pages, 14 figures, 8 tables", "journal-ref": "ACM Computing Surveys, 2019", "doi": "10.1145/3337065", "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The wide proliferation of various wireless communication systems and wireless\ndevices has led to the arrival of big data era in large scale wireless\nnetworks. Big data of large scale wireless networks has the key features of\nwide variety, high volume, real-time velocity and huge value leading to the\nunique research challenges that are different from existing computing systems.\nIn this paper, we present a survey of the state-of-art big data analytics (BDA)\napproaches for large scale wireless networks. In particular, we categorize the\nlife cycle of BDA into four consecutive stages: Data Acquisition, Data\nPreprocessing, Data Storage and Data Analytics. We then present a detailed\nsurvey of the technical solutions to the challenges in BDA for large scale\nwireless networks according to each stage in the life cycle of BDA. Moreover,\nwe discuss the open research issues and outline the future directions in this\npromising area.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 13:25:23 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Dai", "Hong-Ning", ""], ["Wong", "Raymond Chi-Wing", ""], ["Wang", "Hao", ""], ["Zheng", "Zibin", ""], ["Vasilakos", "Athanasios V.", ""]]}, {"id": "1909.08096", "submitter": "Dumitrel Loghin", "authors": "Dumitrel Loghin, Shaofeng Cai, Gang Chen, Tien Tuan Anh Dinh, Feiyi\n  Fan, Qian Lin, Janice Ng, Beng Chin Ooi, Xutao Sun, Quang-Trung Ta, Wei Wang,\n  Xiaokui Xiao, Yang Yang, Meihui Zhang, Zhonghua Zhang", "title": "The Disruptions of 5G on Data-driven Technologies and Applications", "comments": "19 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With 5G on the verge of being adopted as the next mobile network, there is a\nneed to analyze its impact on the landscape of computing and data management.\nIn this paper, we analyze the impact of 5G on both traditional and emerging\ntechnologies and project our view on future research challenges and\nopportunities. With a predicted increase of 10-100x in bandwidth and 5-10x\ndecrease in latency, 5G is expected to be the main enabler for smart cities,\nsmart IoT and efficient healthcare, where machine learning is conducted at the\nedge. In this context, we investigate how 5G can help the development of\nfederated learning. Network slicing, another key feature of 5G, allows running\nmultiple isolated networks on the same physical infrastructure. However,\nsecurity remains the main concern in the context of virtualization,\nmulti-tenancy and high device density. Formal verification of 5G networks can\nbe applied to detect security issues in massive virtualized environments. In\nsummary, 5G will make the world even more densely and closely connected. What\nwe have experienced in 4G connectivity will pale in comparison to the vast\namounts of possibilities engendered by 5G.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 23:27:18 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 06:20:25 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 03:29:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Loghin", "Dumitrel", ""], ["Cai", "Shaofeng", ""], ["Chen", "Gang", ""], ["Dinh", "Tien Tuan Anh", ""], ["Fan", "Feiyi", ""], ["Lin", "Qian", ""], ["Ng", "Janice", ""], ["Ooi", "Beng Chin", ""], ["Sun", "Xutao", ""], ["Ta", "Quang-Trung", ""], ["Wang", "Wei", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yang", ""], ["Zhang", "Meihui", ""], ["Zhang", "Zhonghua", ""]]}, {"id": "1909.08263", "submitter": "EPTCS", "authors": "Marco De Bortoli (Graz University of Technology)", "title": "Distributed Answer Set Coloring: Stable Models Computation via Graph\n  Coloring", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 441-451", "doi": "10.4204/EPTCS.306.60", "report-no": null, "categories": "cs.DC cs.AI cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a famous logic language for knowledge\nrepresentation, which has been really successful in the last years, as\nwitnessed by the great interest into the development of efficient solvers for\nASP. Yet, the great request of resources for certain types of problems, as the\nplanning ones, still constitutes a big limitation for problem solving.\nParticularly, in the case the program is grounded before the resolving phase,\nan exponential blow up of the grounding can generate a huge ground file,\ninfeasible for single machines with limited resources, thus preventing even the\ndiscovering of a single non-optimal solution. To address this problem, in this\npaper we present a distributed approach to ASP solving, exploiting distributed\ncomputation benefits in order to overcome the just explained limitations. The\nhere presented tool, which is called Distributed Answer Set Coloring (DASC), is\na pure solver based on the well-known Graph Coloring algorithm. DASC is part of\na bigger project aiming to bring logic programming into a distributed system,\nstarted in 2017 by Federico Igne with mASPreduce and continued in 2018 by\nPietro Totis with a distributed grounder. In this paper we present a low level\nimplementation of the Graph Coloring algorithm, via the Boost and MPI libraries\nfor C++. Finally, we provide a few results of the very first working version of\nour tool, at the moment without any strong optimization or heuristic.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:16:15 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["De Bortoli", "Marco", "", "Graz University of Technology"]]}, {"id": "1909.08329", "submitter": "Renjie Gu", "authors": "Renjie Gu, Shuo Yang, Fan Wu", "title": "Distributed Machine Learning on Mobile Devices: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, mobile devices have gained increasingly development with\nstronger computation capability and larger storage. Some of the\ncomputation-intensive machine learning and deep learning tasks can now be run\non mobile devices. To take advantage of the resources available on mobile\ndevices and preserve users' privacy, the idea of mobile distributed machine\nlearning is proposed. It uses local hardware resources and local data to solve\nmachine learning sub-problems on mobile devices, and only uploads computation\nresults instead of original data to contribute to the optimization of the\nglobal model. This architecture can not only relieve computation and storage\nburden on servers, but also protect the users' sensitive information. Another\nbenefit is the bandwidth reduction, as various kinds of local data can now\nparticipate in the training process without being uploaded to the server. In\nthis paper, we provide a comprehensive survey on recent studies of mobile\ndistributed machine learning. We survey a number of widely-used mobile\ndistributed machine learning methods. We also present an in-depth discussion on\nthe challenges and future directions in this area. We believe that this survey\ncan demonstrate a clear overview of mobile distributed machine learning and\nprovide guidelines on applying mobile distributed machine learning to real\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:09:02 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Gu", "Renjie", ""], ["Yang", "Shuo", ""], ["Wu", "Fan", ""]]}, {"id": "1909.08369", "submitter": "Taisuke Izumi", "authors": "Shimon Bitton, Yuval Emek, Taisuke Izumi, Shay Kutten", "title": "Message Reduction in the Local Model is a Free Lunch", "comments": "Appeared at International Symposium on Distributed Computing (DISC)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new \\emph{spanner} construction algorithm is presented, working under the\n\\emph{LOCAL} model with unique edge IDs. Given an $n$-node communication graph,\na spanner with a constant stretch and $O (n^{1 + \\varepsilon})$ edges (for an\narbitrarily small constant $\\varepsilon > 0$) is constructed in a constant\nnumber of rounds sending $O (n^{1 + \\varepsilon})$ messages whp. Consequently,\nwe conclude that every $t$-round LOCAL algorithm can be transformed into an $O\n(t)$-round LOCAL algorithm that sends $O (t \\cdot n^{1 + \\varepsilon})$\nmessages whp. This improves upon all previous message-reduction schemes for\nLOCAL algorithms that incur a $\\log^{\\Omega (1)} n$ blow-up of the round\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 11:33:59 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Bitton", "Shimon", ""], ["Emek", "Yuval", ""], ["Izumi", "Taisuke", ""], ["Kutten", "Shay", ""]]}, {"id": "1909.08458", "submitter": "Mathias Bourgoin", "authors": "Victor Allombert, Mathias Bourgoin, Julien Tesson", "title": "Introduction to the Tezos Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tezos is an innovative blockchain that improves on several aspects compared\nto more established blockchains. It offers an original proof-of-stake consensus\nalgorithm and can be used as a decentralized smart contract platform. It has\nthe capacity to amend its own economic protocol through a voting mechanism and\nfocuses on formal methods to improve safety.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 14:07:08 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Allombert", "Victor", ""], ["Bourgoin", "Mathias", ""], ["Tesson", "Julien", ""]]}, {"id": "1909.08704", "submitter": "Michael Salim", "authors": "Michael A. Salim, Thomas D. Uram, J. Taylor Childers, Prasanna\n  Balaprakash, Venkatram Vishwanath, Michael E. Papka", "title": "Balsam: Automated Scheduling and Execution of Dynamic, Data-Intensive\n  HPC Workflows", "comments": "SC '18: 8th Workshop on Python for High-Performance and Scientific\n  Computing (PyHPC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Balsam service to manage high-throughput task scheduling and\nexecution on supercomputing systems. Balsam allows users to populate a task\ndatabase with a variety of tasks ranging from simple independent tasks to\ndynamic multi-task workflows. With abstractions for the local resource\nscheduler and MPI environment, Balsam dynamically packages tasks into ensemble\njobs and manages their scheduling lifecycle. The ensembles execute in a pilot\n\"launcher\" which (i) ensures concurrent, load-balanced execution of arbitrary\nserial and parallel programs with heterogeneous processor requirements, (ii)\nrequires no modification of user applications, (iii) is tolerant of task-level\nfaults and provides several options for error recovery, (iv) stores provenance\ndata (e.g task history, error logs) in the database, (v) supports dynamic\nworkflows, in which tasks are created or killed at runtime. Here, we present\nthe design and Python implementation of the Balsam service and launcher. The\nefficacy of this system is illustrated using two case studies: hyperparameter\noptimization of deep neural networks, and high-throughput single-point quantum\nchemistry calculations. We find that the unique combination of flexible\njob-packing and automated scheduling with dynamic (pilot-managed) execution\nfacilitates excellent resource utilization. The scripting overheads typically\nneeded to manage resources and launch workflows on supercomputers are\nsubstantially reduced, accelerating workflow development and execution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 20:57:44 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Salim", "Michael A.", ""], ["Uram", "Thomas D.", ""], ["Childers", "J. Taylor", ""], ["Balaprakash", "Prasanna", ""], ["Vishwanath", "Venkatram", ""], ["Papka", "Michael E.", ""]]}, {"id": "1909.08719", "submitter": "Ranvir Rana", "authors": "Giulia Fanti, Jiantao Jiao, Ashok Makkuva, Sewoong Oh, Ranvir Rana,\n  Pramod Viswanath", "title": "Barracuda: The Power of $\\ell$-polling in Proof-of-Stake Blockchains", "comments": "ACM Mobihoc 2019, Best paper award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A blockchain is a database of sequential events that is maintained by a\ndistributed group of nodes. A key consensus problem in blockchains is that of\ndetermining the next block (data element) in the sequence. Many blockchains\naddress this by electing a new node to propose each new block. The new block is\n(typically) appended to the tip of the proposer's local blockchain, and\nsubsequently broadcast to the rest of the network. Without network delay (or\nadversarial behavior), this procedure would give a perfect chain, since each\nproposer would have the same view of the blockchain. A major challenge in\npractice is forking. Due to network delays, a proposer may not yet have the\nmost recent block, and may, therefore, create a side chain that branches from\nthe middle of the main chain. Forking reduces throughput, since only one a\nsingle main chain can survive, and all other blocks are discarded. We propose a\nnew P2P protocol for blockchains called Barracuda, in which each proposer,\nprior to proposing a block, polls $\\ell$ other nodes for their local blocktree\ninformation. Under a stochastic network model, we prove that this lightweight\nprimitive improves throughput as if the entire network were a factor of $\\ell$\nfaster. We provide guidelines on how to implement Barracuda in practice,\nguaranteeing robustness against several real-world factors.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 21:43:24 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Fanti", "Giulia", ""], ["Jiao", "Jiantao", ""], ["Makkuva", "Ashok", ""], ["Oh", "Sewoong", ""], ["Rana", "Ranvir", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1909.08969", "submitter": "Roch Guerin", "authors": "R. Guerin", "title": "When Two is Worse Than One", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is concerned with the impact on job latency of splitting a token\nbucket into multiple sub-token buckets with equal aggregate parameters and\noffered the same job arrival process. The situation commonly arises in\ndistributed computing environments where job arrivals are rate controlled (each\njob needs one token to enter the system), but capacity limitations call for\ndistributing jobs across multiple compute resources with scalability\nconsiderations preventing the use of a centralized rate control component (each\ncompute resource is responsible for monitoring and enforcing that the job\nstream it receives conforms to a certain traffic envelope). The question we\naddress is to what extent splitting a token bucket into multiple sub-token\nbuckets that individually rate control a subset of the original arrival process\naffects job latency, when jobs wait for a token whenever the token bucket is\nempty upon their arrival. Our contribution is to establish that independent of\nthe job arrival process and how jobs are distributed across compute resources\n(and sub-token buckets), splitting a token bucket always increases the sum of\njob latencies in the token buckets, and consequently the average job latency.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:21:00 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Guerin", "R.", ""]]}, {"id": "1909.08978", "submitter": "Hope Mogale Mr", "authors": "Hope Mogale, Michael Esiefarienrhe, Naison Gasela, Lucia Letlonkane", "title": "Accelerating Green Computing with Hybrid Asymmetric Multicore\n  Architectures and Safe Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel strategy for accelerating green computing by\nutilizing and adopting the Hybrid Asymmetric Multicore Architectures (HAMA)\nmodel with Safe Parallelism. Most of the modern computing is serial and\ncontributes to the global footprint of energy consumption. These impacts are\noften witnessed and experienced in many server farms and cloud computing\nplatforms where the majority of the world's information resides. Evidently in\nthis paper we present a novel strategy that can help decelerate the global\nfootprint of energy consumption caused by computing. Through our strategy we\nprove that by adopting HAMA and utilizing safe parallelism energy consumption\nper computation can be minimized.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:26:51 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Mogale", "Hope", ""], ["Esiefarienrhe", "Michael", ""], ["Gasela", "Naison", ""], ["Letlonkane", "Lucia", ""]]}, {"id": "1909.09002", "submitter": "Ruben Becker", "authors": "Ruben Becker, Yuval Emek, Christoph Lenzen", "title": "Low Diameter Graph Decompositions by Approximate Distance Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many models for large-scale computation, decomposition of the problem is\nkey to efficient algorithms. For distance-related graph problems, it is often\ncrucial that such a decomposition results in clusters of small diameter, while\nthe probability that an edge is cut by the decomposition scales linearly with\nthe length of the edge. There is a large body of literature on low diameter\ngraph decomposition with small edge cutting probabilities, with all existing\ntechniques heavily building on single source shortest paths (SSSP)\ncomputations. Unfortunately, in many theoretical models for large-scale\ncomputations, the SSSP task constitutes a complexity bottleneck. Therefore, it\nis desirable to replace exact SSSP computations with approximate ones. However\nthis imposes a fundamental challenge since the existing constructions of such\ndecompositions inherently rely on the subtractive form of the triangle\ninequality. The current paper overcomes this obstacle by developing a technique\ntermed blurry ball growing. By combining this technique with a clever\nalgorithmic idea of Miller et al. (SPAA 13), we obtain a construction of low\ndiameter decompositions with small edge cutting probabilities which replaces\nexact SSSP computations by (a small number of) approximate ones. The utility of\nour approach is showcased by deriving efficient algorithms that work in the\nCongest, PRAM, and semi-streaming models of computation. As an application, we\nobtain metric tree embedding algorithms in the vein of Bartal (FOCS 96) whose\ncomputational complexities in these models are optimal up to polylogarithmic\nfactors. Our embeddings have the additional useful property that the tree can\nbe mapped back to the original graph such that each edge is \"used\" only O(log\nn) times, which is of interest for capacitated problems and simulating Congest\nalgorithms on the tree into which the graph is embedded.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:59:50 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Becker", "Ruben", ""], ["Emek", "Yuval", ""], ["Lenzen", "Christoph", ""]]}, {"id": "1909.09145", "submitter": "Praneeth Vepakomma", "authors": "Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, Ramesh Raskar", "title": "Detailed comparison of communication efficiency of split learning and\n  federated learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare communication efficiencies of two compelling distributed machine\nlearning approaches of split learning and federated learning. We show useful\nsettings under which each method outperforms the other in terms of\ncommunication efficiency. We consider various practical scenarios of\ndistributed learning setup and juxtapose the two methods under various\nreal-life scenarios. We consider settings of small and large number of clients\nas well as small models (1M - 6M parameters), large models (10M - 200M\nparameters) and very large models (1 Billion-100 Billion parameters). We show\nthat increasing number of clients or increasing model size favors split\nlearning setup over the federated while increasing the number of data samples\nwhile keeping the number of clients or model size low makes federated learning\nmore communication efficient.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 21:43:33 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Singh", "Abhishek", ""], ["Vepakomma", "Praneeth", ""], ["Gupta", "Otkrist", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1909.09213", "submitter": "EPTCS", "authors": "Fabio Tardivo (New Mexico State University)", "title": "Experimenting with Constraint Programming on GPU", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 427-432", "doi": "10.4204/EPTCS.306.58", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of my PhD thesis is on exploring parallel approaches to efficiently\nsolve problems modeled by constraints and presenting a new proposal. Current\nsolvers are very advanced; they are carefully designed to effectively manage\nthe high-level problems' description and include refined strategies to avoid\nuseless work. Despite this, finding a solution can take an unacceptable amount\nof time. Parallelization can mitigate this problem when the instance of the\nproblem modeled is large, as it happens in real world problems. It is done by\npropagating constraints in parallel and concurrently exploring different parts\nof the search space. I am developing on a constraint solver that exploits the\nmany cores available on Graphics Processing Units (GPU) to speed up the search.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 19:43:51 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Tardivo", "Fabio", "", "New Mexico State University"]]}, {"id": "1909.09252", "submitter": "Devanshu Arya", "authors": "Devanshu Arya, Stevan Rudinac and Marcel Worring", "title": "HyperLearn: A Distributed Approach for Representation Learning in\n  Datasets With Many Modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal datasets contain an enormous amount of relational information,\nwhich grows exponentially with the introduction of new modalities. Learning\nrepresentations in such a scenario is inherently complex due to the presence of\nmultiple heterogeneous information channels. These channels can encode both (a)\ninter-relations between the items of different modalities and (b)\nintra-relations between the items of the same modality. Encoding multimedia\nitems into a continuous low-dimensional semantic space such that both types of\nrelations are captured and preserved is extremely challenging, especially if\nthe goal is a unified end-to-end learning framework. The two key challenges\nthat need to be addressed are: 1) the framework must be able to merge complex\nintra and inter relations without losing any valuable information and 2) the\nlearning model should be invariant to the addition of new and potentially very\ndifferent modalities. In this paper, we propose a flexible framework which can\nscale to data streams from many modalities. To that end we introduce a\nhypergraph-based model for data representation and deploy Graph Convolutional\nNetworks to fuse relational information within and across modalities. Our\napproach provides an efficient solution for distributing otherwise extremely\ncomputationally expensive or even unfeasible training processes across\nmultiple-GPUs, without any sacrifices in accuracy. Moreover, adding new\nmodalities to our model requires only an additional GPU unit keeping the\ncomputational time unchanged, which brings representation learning to truly\nmultimodal datasets. We demonstrate the feasibility of our approach in the\nexperiments on multimedia datasets featuring second, third and fourth order\nrelations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 22:45:21 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Arya", "Devanshu", ""], ["Rudinac", "Stevan", ""], ["Worring", "Marcel", ""]]}, {"id": "1909.09357", "submitter": "Mark Burgess", "authors": "Mark Burgess", "title": "Locality, Statefulness, and Causality in Distributed Information Systems\n  (Concerning the Scale Dependence Of System Promises)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several popular best-practice manifestos for IT design and architecture use\nterms like `stateful', `stateless', `shared nothing', etc, and describe `fact\nbased' or `functional' descriptions of causal evolution to describe computer\nprocesses, especially in cloud computing. The concepts are used ambiguously and\nsometimes in contradictory ways, which has led to many imprecise beliefs about\ntheir implications. This paper outlines the simple view of state and causation\nin Promise Theory, which accounts for the scaling of processes and the\nrelativity of different observers in a natural way. It's shown that the\nconcepts of statefulness or statelessness are artifacts of observational scale\nand causal bias towards functional evaluation. If we include feedback loops,\nrecursion, and process convergence, which appear acausal to external observers,\nthe arguments about (im)mutable state need to be modified in a scale-dependent\nway. In most cases the intended focus of such remarks is not terms like\n`statelessness' but process predictability. A simple principle may be\nsubstituted in most cases as a guide to system design: the principle the\nseparation of dynamic scales.\n  Understanding data reliance and the ability to keep stable promises is of\ncrucial importance to the consistency of data pipelines, and distributed\nclient-server interactions, albeit in different ways. With increasingly data\nintensive processes over widely separated distributed deployments, e.g. in the\nInternet of Things and AI applications, the effects of instability need a more\ncareful treatment.\n  These notes are part of an initiative to engage with thinkers and\npractitioners towards a more rational and disciplined language for systems\nengineering for era of ubiquitous extended-cloud computing.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 07:34:43 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Burgess", "Mark", ""]]}, {"id": "1909.09417", "submitter": "Stefan Vlaski", "authors": "Stefan Vlaski, Lieven Vandenberghe, Ali H. Sayed", "title": "Regularized Diffusion Adaptation via Conjugate Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to develop and study a distributed strategy for\nPareto optimization of an aggregate cost consisting of regularized risks. Each\nrisk is modeled as the expectation of some loss function with unknown\nprobability distribution while the regularizers are assumed deterministic, but\nare not required to be differentiable or even continuous. The individual,\nregularized, cost functions are distributed across a strongly-connected network\nof agents and the Pareto optimal solution is sought by appealing to a\nmulti-agent diffusion strategy. To this end, the regularizers are smoothed by\nmeans of infimal convolution and it is shown that the Pareto solution of the\napproximate, smooth problem can be made arbitrarily close to the solution of\nthe original, non-smooth problem. Performance bounds are established under\nconditions that are weaker than assumed before in the literature, and hence\napplicable to a broader class of adaptation and learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:39:45 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Vlaski", "Stefan", ""], ["Vandenberghe", "Lieven", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1909.09457", "submitter": "Jian-Jia Chen", "authors": "Niklas Ueter, Georg von der Brueggen, Jian-Jia Chen, Tulika Mitra, and\n  Vanchinathan Venkataramani", "title": "Simultaneous Progressing Switching Protocols for Timing Predictable\n  Real-Time Network-on-Chips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many-core systems require inter-core communication, and network-on-chips\n(NoCs) have been demonstrated to provide good scalability. However, not only\nthe distributed structure but also the link switching on the NoCs have imposed\na great challenge in the design and analysis for real-time systems. With\nscalability and flexibility in mind, the existing link switching protocols\nusually consider each single link to be scheduled independently, e.g., the\nworm-hole switching protocol. The flexibility of such link-based arbitrations\nallows each packet to be distributed over multiple routers but also increases\nthe number of possible link states (the number of flits in a buffer) that have\nto be considered in the worst-case timing analysis.\n  For achieving timing predictability, we propose less flexible switching\nprotocols, called \\emph{\\Simultaneous Progressing Switching Protocols}\n(SP$^2$), in which the links used by a flow \\emph{either} all simultaneously\ntransmit one flit (if it exists) of this flow \\emph{or} none of them transmits\nany flit of this flow. Such an \\emph{all-or-nothing} property of the SP$^2$\nrelates the scheduling behavior on the network to the uniprocessor\nself-suspension scheduling problem. We provide rigorous proofs which confirm\nthe equivalence of these two problems. Moreover, our approaches are not limited\nto any specific underlying routing protocols, which are usually constructed for\ndeadlock avoidance instead of timing predictability. We demonstrate the\nanalytical dominance of the fixed-priority $SP^2$ over some of the existing\nsufficient schedulability analysis for fixed-priority wormhole switched\nnetwork-on-chips.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 07:01:34 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 06:56:01 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Ueter", "Niklas", ""], ["von der Brueggen", "Georg", ""], ["Chen", "Jian-Jia", ""], ["Mitra", "Tulika", ""], ["Venkataramani", "Vanchinathan", ""]]}, {"id": "1909.09463", "submitter": "Suryanarayana Murthy Durbhakula", "authors": "Suryanarayana Murthy Durbhakula", "title": "Cache Optimization for Sharing Intensive Workloads on Multi-socket\n  Multi-core servers", "comments": "arXiv admin note: substantial text overlap with arXiv:1908.04249", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major chip manufacturers have all introduced multicore microprocessors.\nMulti-socket systems built from these processors are used for running various\nserver applications. Depending on the application, remote cache-to-cache\ntransfers can severely impact the performance of such workloads. This paper\npresents a cache optimization that can cut down remote cache-to-cache\ntransfers. By keeping track of remote cache lines loaded from remote caches\ninto last-level-cache and by biasing the cache replacement policy towards such\nremote cache lines we can reduce the number of cache misses. This in turn\nresults in improvement of overall performance. I present the design details in\nthis paper. I do a qualitative comparison of various solutions to the problem\nof performance impact of remote cache-to-cache transfers. This work can be\nextended by doing a quantitative evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 14:04:55 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Durbhakula", "Suryanarayana Murthy", ""]]}, {"id": "1909.09472", "submitter": "Shengshan Hu", "authors": "Shengshan Hu, Chengjun Cai, Qian Wang, Cong Wang, Minghui Li, Zhibo\n  Wang, Dengpan Ye", "title": "Augmenting Encrypted Search: A Decentralized Service Realization with\n  Enforced Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searchable symmetric encryption (SSE) allows the data owner to outsource an\nencrypted database to a remote server in a private manner while maintaining the\nability for selectively search. So far, most existing solutions focus on an\nhonest-but-curious server, while security designs against a malicious server\nhave not drawn enough attention. A few recent works have attempted to construct\nverifiable SSE that enables the data owner to verify the integrity of search\nresults. Nevertheless, these verification mechanisms are highly dependent on\nspecific SSE schemes, and fail to support complex queries. A general\nverification mechanism is desired that can be applied to all SSE schemes.\n  In this work, instead of concentrating on a central server, we explore the\npotential of the smart contract, an emerging blockchain-based decentralized\ntechnology, and construct decentralized SSE schemes where the data owner can\nreceive correct search results with assurance without worrying about potential\nwrongdoings of a malicious server. We study both public and private blockchain\nenvironments and propose two designs with a trade-off between security and\nefficiency. To better support practical applications, the multi-user setting of\nSSE is further investigated where the data owner allows authenticated users to\nsearch keywords in shared documents. We implement prototypes of our two designs\nand present experiments and evaluations to demonstrate the practicability of\nour decentralized SSE schemes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 12:49:14 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Hu", "Shengshan", ""], ["Cai", "Chengjun", ""], ["Wang", "Qian", ""], ["Wang", "Cong", ""], ["Li", "Minghui", ""], ["Wang", "Zhibo", ""], ["Ye", "Dengpan", ""]]}, {"id": "1909.09600", "submitter": "Bj\\\"orn Brandenburg", "authors": "Bj\\\"orn B. Brandenburg", "title": "Multiprocessor Real-Time Locking Protocols: A Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically survey the literature on analytically sound multiprocessor\nreal-time locking protocols from 1988 until 2018, covering the following\ntopics: progress mechanisms that prevent the lock-holder preemption problem,\nspin-lock protocols, binary semaphore protocols, independence-preserving (or\nfully preemptive) locking protocols, reader-writer and k-exclusion\nsynchronization, support for nested critical sections, and implementation and\nsystem-integration aspects. A special focus is placed on the\nsuspension-oblivious and suspension-aware analysis approaches for semaphore\nprotocols, their respective notions of priority inversion, optimality criteria,\nlower bounds on maximum priority-inversion blocking, and matching\nasymptotically optimal locking protocols.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 16:26:25 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Brandenburg", "Bj\u00f6rn B.", ""]]}, {"id": "1909.09765", "submitter": "Xiang Li", "authors": "Yuhang Liu, Luming Wang, Xiang Li, Yang Wang, Mingyu Chen, Yungang Bao", "title": "Gene-Patterns: Should Architecture be Customized for Each Application?", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing architectural support is crucial for newly arising applications to\nachieve high performance and high system efficiency. Currently there is a trend\nin designing accelerators for special applications, while arguably a debate is\nsparked whether we should customize architecture for each application. In this\nstudy, we introduce what we refer to as Gene-Patterns, which are the base\npatterns of diverse applications. We present a Recursive Reduce methodology to\nidentify the hotspots, and a HOtspot Trace Suite (HOTS) is provided for the\nresearch community. We first extract the hotspot patterns, and then, remove the\nredundancy to obtain the base patterns. We find that although the number of\napplications is huge and ever-increasing, the amount of base patterns is\nrelatively small, due to the similarity among the patterns of diverse\napplications. The similarity stems not only from the algorithms but also from\nthe data structures. We build the Periodic Table of Memory Access Patterns\n(PT-MAP), where the indifference curves are analogous to the energy levels in\nphysics, and memory performance optimization is essentially an energy level\ntransition. We find that inefficiency results from the mismatch between some of\nthe base patterns and the micro-architecture of modern processors. We have\nidentified the key micro-architecture demands of the base patterns. The\nGene-Pattern concept, methodology, and toolkit will facilitate the design of\nboth hardware and software for the matching between architectures and\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 03:16:04 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 14:34:53 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Liu", "Yuhang", ""], ["Wang", "Luming", ""], ["Li", "Xiang", ""], ["Wang", "Yang", ""], ["Chen", "Mingyu", ""], ["Bao", "Yungang", ""]]}, {"id": "1909.09771", "submitter": "Pawan Kumar", "authors": "Abhinav Aggarwal, Shivam Kakkar, Pawan Kumar", "title": "Multithreaded Filtering Preconditioner for Diffusion Equation on\n  Structured Grid", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel and nested version of a frequency filtering preconditioner is\nproposed for linear systems corresponding to diffusion equation on a structured\ngrid. The proposed preconditioner is found to be robust with respect to jumps\nin the diffusion coefficients. The storage requirement for the preconditioner\nis O(N),where N is number of rows of matrix, hence, a fairly large problem of\nsize more than 42 million unknowns has been solved on a quad core machine with\n64GB RAM. The parallelism is achieved using twisted factorization and SIMD\noperations. The preconditioner achieves a speedup of 3.3 times on a quad core\nprocessor clocked at 4.2 GHz, and compared to a well known algebraic multigrid\nmethod, it is significantly faster in both setup and solve times for diffusion\nequations with jumps.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 04:15:31 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Aggarwal", "Abhinav", ""], ["Kakkar", "Shivam", ""], ["Kumar", "Pawan", ""]]}, {"id": "1909.09845", "submitter": "Yu Chen", "authors": "Alem Fitwi, Yu Chen, Sencun Zhu", "title": "A Lightweight Blockchain-based Privacy Protection for Smart Surveillance\n  at the Edge", "comments": "Presented at the International Workshop on Lightweight Blockchain for\n  Edge Intelligence and Security (LightChain), held in conjunction with the 2nd\n  IEEE International Conference on Blockchain (Blockchain-2019), Atlanta, USA,\n  July 14 - 17, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Witnessing the increasingly pervasive deployment of security video\nsurveillance systems(VSS), more and more individuals have become concerned with\nthe issues of privacy violations. While the majority of the public have a\nfavorable view of surveillance in terms of crime deterrence, individuals do not\naccept the invasive monitoring of their private life. To date, however, there\nis not a lightweight and secure privacy-preserving solution for video\nsurveillance systems. The recent success of blockchain (BC) technologies and\ntheir applications in the Internet of Things (IoT) shed a light on this\nchallenging issue. In this paper, we propose a Lightweight, Blockchain-based\nPrivacy protection (Lib-Pri) scheme for surveillance cameras at the edge. It\nenables the VSS to perform surveillance without compromising the privacy of\npeople captured in the videos. The Lib-Pri system transforms the deployed VSS\ninto a system that functions as a federated blockchain network capable of\ncarrying out integrity checking, blurring keys management, feature sharing, and\nvideo access sanctioning. The policy-based enforcement of privacy measures is\ncarried out at the edge devices for real-time video analytics without\ncluttering the network.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 15:32:22 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Fitwi", "Alem", ""], ["Chen", "Yu", ""], ["Zhu", "Sencun", ""]]}, {"id": "1909.09925", "submitter": "Andrey Kuehlkamp", "authors": "Evan Brinckman, Andrey Kuehlkamp, Jarek Nabrzyski, Ian J. Taylor", "title": "Techniques and Applications for Crawling, Ingesting and Analyzing\n  Blockchain Data", "comments": "Manuscript accepted for publication at ICTC 2019 (ictc.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the public Ethereum network surpasses half a billion transactions and\nenterprise Blockchain systems becoming highly capable of meeting the demands of\nglobal deployments, production Blockchain applications are fast becoming\ncommonplace across a diverse range of business and scientific verticals. In\nthis paper, we reflect on work we have been conducting recently surrounding the\ningestion, retrieval and analysis of Blockchain data. We describe the scaling\nand semantic challenges when extracting Blockchain data in a way that preserves\nthe original metadata of each transaction by cross referencing the Smart\nContract interface with the on-chain data. We then discuss a scientific use\ncase in the area of Scientific workflows by describing how we can harvest data\nfrom tasks and dependencies in a generic way. We then discuss how crawled\npublic blockchain data can be analyzed using two unsupervised machine learning\nalgorithms, which are designed to identify outlier accounts or smart contracts\nin the system. We compare and contrast the two machine learning methods and\ncross correlate with public Websites to illustrate the effectiveness such\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 01:38:08 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Brinckman", "Evan", ""], ["Kuehlkamp", "Andrey", ""], ["Nabrzyski", "Jarek", ""], ["Taylor", "Ian J.", ""]]}, {"id": "1909.09927", "submitter": "Shengyu Fan", "authors": "Weizhi Xu, Shengyu Fan, Hui Yu, Xin Fu", "title": "Accelerating convolutional neural network by exploiting sparsity on GPUs", "comments": "submitted to IEEE Transactions on Neural Networks and Learning\n  Systems (IEEE-TNNLS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) is an important deep learning method. The\nconvolution operation takes a large proportion of the total execution time for\nCNN. Feature maps for convolution operation are usually sparse. Multiplications\nand additions for zero values in the feature map are useless for convolution\nresults. In addition, the convolution layer and pooling layer are computed\nseparately in traditional methods, which leads to frequent data transfer\nbetween CPU and GPU. Based on these observations, we propose two new methods to\naccelerate CNN on GPUs. The first method focuses on accelerating convolution\noperation and reducing the calculation of zero values. The second method\ncombines the operations of one convolution layer with the following pooling\nlayer to effectively reduce traffic between CPU and GPU. For the first method,\nwe extract some convolution layers from LeNet, AlexNet, and GoogLeNet, and can\nachieve up to 3.6X speedup over cuDNN for the single-layer convolution on GPU.\nExperiment on VGG-19 achieves 3.5X speedup over cuDNN for convolution operation\non average. For the second method, the experiment on VGG-19 achieves 4.3X\nspeedup over cuDNN on average.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 01:47:10 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 03:26:26 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 02:57:16 GMT"}, {"version": "v4", "created": "Mon, 3 May 2021 04:03:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xu", "Weizhi", ""], ["Fan", "Shengyu", ""], ["Yu", "Hui", ""], ["Fu", "Xin", ""]]}, {"id": "1909.09937", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang, Keyou You, and Kai Cai", "title": "Distributed Dual Gradient Tracking for Resource Allocation in Unbalanced\n  Networks", "comments": "Accepted by IEEE Transactions on Signal Processing. This version\n  fixed some typos in the accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a distributed dual gradient tracking algorithm (DDGT) to\nsolve resource allocation problems over an unbalanced network, where each node\nin the network holds a private cost function and computes the optimal resource\nby interacting only with its neighboring nodes. Our key idea is the novel use\nof the distributed push-pull gradient algorithm (PPG) to solve the dual problem\nof the resource allocation problem. To study the convergence of the DDGT, we\nfirst establish the sublinear convergence rate of PPG for non-convex objective\nfunctions, which advances the existing results on PPG as they require the\nstrong-convexity of objective functions. Then we show that the DDGT converges\nlinearly for strongly convex and Lipschitz smooth cost functions, and\nsublinearly without the Lipschitz smoothness. Finally, experimental results\nsuggest that DDGT outperforms existing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 04:04:29 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 05:58:07 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 07:50:42 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhang", "Jiaqi", ""], ["You", "Keyou", ""], ["Cai", "Kai", ""]]}, {"id": "1909.10000", "submitter": "Nan Gao", "authors": "Dongwei Li, Shuliang Wang, Nan Gao, Qiang He, Yun Yang", "title": "Cutting the Unnecessary Long Tail: Cost-Effective Big Data Clustering in\n  the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering big data often requires tremendous computational resources where\ncloud computing is undoubtedly one of the promising solutions. However, the\ncomputation cost in the cloud can be unexpectedly high if it cannot be managed\nproperly. The long tail phenomenon has been observed widely in the big data\nclustering area, which indicates that the majority of time is often consumed in\nthe middle to late stages in the clustering process. In this research, we try\nto cut the unnecessary long tail in the clustering process to achieve a\nsufficiently satisfactory accuracy at the lowest possible computation cost. A\nnovel approach is proposed to achieve cost-effective big data clustering in the\ncloud. By training the regression model with the sampling data, we can make\nwidely used k-means and EM (Expectation-Maximization) algorithms stop\nautomatically at an early point when the desired accuracy is obtained.\nExperiments are conducted on four popular data sets and the results demonstrate\nthat both k-means and EM algorithms can achieve high cost-effectiveness in the\ncloud with our proposed approach. For example, in the case studies with the\nmuch more efficient k-means algorithm, we find that achieving a 99% accuracy\nneeds only 47.71%-71.14% of the computation cost required for achieving a 100%\naccuracy while the less efficient EM algorithm needs 16.69%-32.04% of the\ncomputation cost. To put that into perspective, in the United States land use\nclassification example, our approach can save up to $94,687.49 for the\ngovernment in each use.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 13:22:24 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Li", "Dongwei", ""], ["Wang", "Shuliang", ""], ["Gao", "Nan", ""], ["He", "Qiang", ""], ["Yang", "Yun", ""]]}, {"id": "1909.10004", "submitter": "Debasish Pattanayak", "authors": "Debasish Pattanayak, John Augustine, Partha Sarathi Mandal", "title": "Randomized Gathering of Asynchronous Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the widely researched \\textit{gathering} problem for two\nrobots in a scenario which allows randomization in the asynchronous scheduling\nmodel. The scheduler is considered to be the adversary which determines the\nactivation schedule of the robots. The adversary comes in two flavors, namely,\noblivious and adaptive, based on the knowledge of the outcome of random bits.\nThe robots follow \\textit{wait-look-compute-move} cycle.\n  In this paper, we classify the problems based on the capability of the\nadversary to control the parameters such as wait time, computation delay and\nthe speed of robots and check the feasibility of gathering in terms of\nadversarial knowledge and capabilities. The main contributions include the\npossibility of gathering for an oblivious adversary with (i) zero computation\ndelay; (ii) the sum of wait time and computation delay is more than a positive\nvalue. We complement the possibilities with an impossibility.\n  We show that it is impossible for the robots to gather against an adaptive\nadversary with non-negative wait time and non-negative computation delay.\nFinally, we also extend our algorithm for multiple robots with merging.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 13:34:39 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Pattanayak", "Debasish", ""], ["Augustine", "John", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "1909.10152", "submitter": "Dumitrel Loghin", "authors": "Beng Chin Ooi and Gang Chen and Dumitrel Loghin and Wei Wang and\n  Meihui Zhang", "title": "5G: Agent for Further Digital Disruptive Transformations", "comments": "Published in the Bulletin of the Technical Committee on Data\n  Engineering (http://sites.computer.org/debull/A19sept/p9.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fifth-generation (5G) mobile communication technologies are on the way to\nbe adopted as the next standard for mobile networking. It is therefore timely\nto analyze the impact of 5G on the landscape of computing, in particular, data\nmanagement and data-driven technologies. With a predicted increase of\n10-100$\\times$ in bandwidth and 5-10$\\times$ decrease in latency, 5G is\nexpected to be the main enabler for edge computing which includes accessing\ncloud-like services, as well as conducting machine learning at the edge. In\nthis paper, we examine the impact of 5G on both traditional and emerging\ntechnologies, and discuss research challenges and opportunities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 04:36:42 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ooi", "Beng Chin", ""], ["Chen", "Gang", ""], ["Loghin", "Dumitrel", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""]]}, {"id": "1909.10194", "submitter": "Roberto Saltini", "authors": "Roberto Saltini, David Hyland-Wood", "title": "IBFT 2.0: A Safe and Live Variation of the IBFT Blockchain Consensus\n  Protocol for Eventually Synchronous Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present IBFT 2.0 (Istanbul BFT 2.0), which is a\nProof-of-Authority (PoA) Byzantine-fault-tolerant (BFT) blockchain consensus\nprotocols that (i) ensures immediate finality, (ii) is robust in an eventually\nsynchronous network model and (iii) features a dynamic validator set. IBFT 2.0,\nas the name suggests, builds upon the IBFT blockchain consensus protocol\nretaining all of the original features while addressing the safety and liveness\nlimitations described in one of our previous works. In this paper, we present a\nhigh-level description of the IBFT 2.0 protocol and related robustness proof.\nFormal specification of the protocol and related formal proofs will be subject\nof a separate body of work. We also envision a separate work that will provide\ndetailed implementation specifications for IBFT 2.0.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 07:36:17 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Saltini", "Roberto", ""], ["Hyland-Wood", "David", ""]]}, {"id": "1909.10389", "submitter": "Luca Canali", "authors": "Matteo Migliorini, Riccardo Castellotti, Luca Canali, Marco Zanetti", "title": "Machine Learning Pipelines with Modern Big Data Tools for High Energy\n  Physics", "comments": "This is a pre-print of an article published in Computing and Software\n  for Big Science. The final authenticated version is available online at\n  https://rdcu.be/b4Wk9", "journal-ref": "Comput Softw Big Sci 4, 8 (2020)", "doi": "10.1007/s41781-020-00040-0", "report-no": null, "categories": "cs.DC cs.LG hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The effective utilization at scale of complex machine learning (ML)\ntechniques for HEP use cases poses several technological challenges, most\nimportantly on the actual implementation of dedicated end-to-end data\npipelines. A solution to these challenges is presented, which allows training\nneural network classifiers using solutions from the Big Data and data science\necosystems, integrated with tools, software, and platforms common in the HEP\nenvironment. In particular, Apache Spark is exploited for data preparation and\nfeature engineering, running the corresponding (Python) code interactively on\nJupyter notebooks. Key integrations and libraries that make Spark capable of\ningesting data stored using ROOT format and accessed via the XRootD protocol,\nare described and discussed. Training of the neural network models, defined\nusing the Keras API, is performed in a distributed fashion on Spark clusters by\nusing BigDL with Analytics Zoo and also by using TensorFlow, notably for\ndistributed training on CPU and GPU resourcess. The implementation and the\nresults of the distributed training are described in detail in this work.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 14:31:44 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 15:03:35 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 13:21:43 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 12:19:08 GMT"}, {"version": "v5", "created": "Tue, 16 Jun 2020 13:46:57 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Migliorini", "Matteo", ""], ["Castellotti", "Riccardo", ""], ["Canali", "Luca", ""], ["Zanetti", "Marco", ""]]}, {"id": "1909.10513", "submitter": "Prayitno Prayitno", "authors": "Eko Prasetyo, Prayitno, Jing-Doo Wang, Karisma Trinanda Putra", "title": "Visualization and Travel Time Extraction System for the Statistics of\n  TDCS Travel using MapReduce Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, extracting some information as a knowledge from big data is very\nchallenging activity. The size of data is very huge and it requires some\nspecial techniques and adequate processing hardware. It is also applied in\nvehicles transportation data at Taiwan National Freeway from the Traffic Data\nCollection System (TDCS). The results of this extraction will be very useful if\nit can be used by the community. So that the delivery of information extracted\nfrom large data that is easily understood becomes a necessary thing.\nPresentation of results using images / visuals will make it easier for people\nto interpret the information provided. In this project, an interactive\nvisualization of the results of extracting statistical information is attempted\nto be provided. The results can be used by users to support the decision making\nof road users in determining the appropriate time when going through the road\npieces around the Taichung City. This visualization of the statistics will help\npeople who want to predict the travel time around Taichung City.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 08:35:49 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Prasetyo", "Eko", ""], ["Prayitno", "", ""], ["Wang", "Jing-Doo", ""], ["Putra", "Karisma Trinanda", ""]]}, {"id": "1909.10593", "submitter": "Mahdi Miraz", "authors": "Md Mehedi Hassan Onik and Mahdi H. Miraz", "title": "Performance Analytical Comparison of Blockchain-as-a-Service (BaaS)\n  Platforms", "comments": null, "journal-ref": "Springer Nature LNICST Series, vol. 285, Online ISBN:\n  978-3-030-23942-8, Print ISBN: 978-3-030-23943-5, Series Print ISSN:\n  1867-8211, Series Online ISSN: 1867-822X, pp. 3-18, August 2019", "doi": "10.1007/978-3-030-23943-5_1", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both blockchain technologies and cloud computing are contemporary emerging\ntechnologies. While the application of Blockchain technologies is being spread\nbeyond cryptocurrency, cloud computing is also seeing a paradigm shift to meet\nthe needs of the 4th industrial revolution (Industry 4.0). New technological\nad-vancement, especially by the fusion of these two, such as\nBlockchain-as-a-Service (BaaS), is considered to be able to significantly\ngenerate values to the en-terprises. This article surveys the current status of\nBaaS in terms of technological development, applications, market potentials and\nso forth. An evaluative judge-ment, comparing amongst various BaaS platforms,\nhas been presented, along with the trajectory of adoption, challenges and risk\nfactors. Finally, the study suggests standardisation of available BaaS\nplatforms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 03:54:53 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Onik", "Md Mehedi Hassan", ""], ["Miraz", "Mahdi H.", ""]]}, {"id": "1909.10644", "submitter": "Mayra Samaniego Mrs", "authors": "Mayra Samaniego, Cristian Espana, Ralph Deters", "title": "Suspicious Transactions in Smart Spaces", "comments": "Accepted in the HICSS 53 conference\n  (https://hicss.hawaii.edu/program-hicss53/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT systems have enabled ubiquitous communication in physical spaces, making\nthem smart Nowadays, there is an emerging concern about evaluating suspicious\ntransactions in smart spaces. Suspicious transactions might have a logical\nstructure, but they are not correct under the present contextual information of\nsmart spaces. This research reviews suspicious transactions in smart spaces and\nevaluates the characteristics of blockchain technology to manage them.\nAdditionally, this research presents a blockchain-based system model with the\nnovel idea of iContracts (interactive contracts) to enable contextual\nevaluation through proof-of-provenance to detect suspicious transactions in\nsmart spaces.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 22:36:37 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Samaniego", "Mayra", ""], ["Espana", "Cristian", ""], ["Deters", "Ralph", ""]]}, {"id": "1909.10818", "submitter": "Florian Scheidegger", "authors": "Florian Scheidegger, Luca Benini, Costas Bekas, Cristiano Malossi", "title": "Constrained deep neural network architecture search for IoT devices\n  accounting hardware calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks achieve outstanding results in challenging image\nclassification tasks. However, the design of network topologies is a complex\ntask and the research community makes a constant effort in discovering\ntop-accuracy topologies, either manually or employing expensive architecture\nsearches. In this work, we propose a unique narrow-space architecture search\nthat focuses on delivering low-cost and fast executing networks that respect\nstrict memory and time requirements typical of Internet-of-Things (IoT)\nnear-sensor computing platforms. Our approach provides solutions with\nclassification latencies below 10ms running on a $35 device with 1GB RAM and\n5.6GFLOPS peak performance. The narrow-space search of floating-point models\nimproves the accuracy on CIFAR10 of an established IoT model from 70.64% to\n74.87% respecting the same memory constraints. We further improve the accuracy\nto 82.07% by including 16-bit half types and we obtain the best accuracy of\n83.45% by extending the search with model optimized IEEE 754 reduced types. To\nthe best of our knowledge, we are the first that empirically demonstrate on\nover 3000 trained models that running with reduced precision pushes the Pareto\noptimal front by a wide margin. Under a given memory constraint, accuracy is\nimproved by over 7% points for half and over 1% points further for running with\nthe best model individual format.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:27:33 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Scheidegger", "Florian", ""], ["Benini", "Luca", ""], ["Bekas", "Costas", ""], ["Malossi", "Cristiano", ""]]}, {"id": "1909.10853", "submitter": "Tobias Weinzierl", "authors": "Tobias Weinzierl", "title": "A high-level characterisation and generalisation of\n  communication-avoiding programming techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's hardware's explosion of concurrency plus the explosion of data we\nbuild upon in both machine learning and scientific simulations have\nmultifaceted impact on how we write our codes. They have changed our notion of\nperformance and, hence, of what a good code is: Good code has, first of all, to\nbe able to exploit the unprecedented levels of parallelism. To do so, it has to\nmanage to move the compute data into the compute facilities on time. As\ncommunication and memory bandwidth cannot keep pace with the growth in compute\ncapabilities and as latency increases---at least relative to what the hardware\ncould do---communication-avoiding techniques gain importance. We characterise\nand classify the field of communication-avoiding algorithms. A review of some\nexamples of communication-avoiding programming by means of our new terminology\nshows that we are well-advised to broaden our notion of\n\"communication-avoiding\" and to look beyond numerical linear algebra. An\nabstraction, generalisation and weakening of the term enriches our toolset of\nhow to tackle the data movement challenges. Through this, we eventually gain\naccess to a richer set of tools that we can use to deliver proper code for\ncurrent and upcoming hardware generations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:54:55 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 08:15:17 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Weinzierl", "Tobias", ""]]}, {"id": "1909.10888", "submitter": "Yu Chen", "authors": "Ronghua Xu, Gowri Sankar Ramachandran, Yu Chen, Bhaskar Krishnamachari", "title": "BlendSM-DDM: BLockchain-ENabled Secure Microservices for Decentralized\n  Data Marketplaces", "comments": "Accepted and to be presented at the 2nd International Workshop on\n  CLockchain Enabled Sustainable Smart Cities (BLESS 2019), held in conjunction\n  with the 5th IEEE International Smart Cities Conference (ISC2 2019),\n  Casablanca, Morocco, October 14 - 17, 2019. arXiv admin note: text overlap\n  with arXiv:1902.10567", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To promote the benefits of the Internet of Things (IoT) in smart communities\nand smart cities, a real-time data marketplace middleware platform, called the\nIntelligent IoT Integrator (I3), has been recently proposed. While facilitating\nthe easy exchanges of real-time IoT data streams between device owners and\nthird-party applications through the marketplace, I3 is presently a monolithic,\ncentralized platform for a single community. Although the service oriented\narchitecture (SOA) has been widely adopted in the IoT and cyber-physical\nsystems (CPS), it is difficult for a monolithic architecture to provide\nscalable, inter-operable and extensible services for large numbers of\ndistributed IoT devices and different application vendors. Traditional security\nsolutions rely on a centralized authority, which can be a performance\nbottleneck or susceptible to a single point of failure. Inspired by\ncontainerized microservices and blockchain technology, this paper proposed a\nBLockchain-ENabled Secure Microservices for Decentralized Data Marketplaces\n(BlendSM-DDM). Within a permissioned blockchain network, a microservices based\nsecurity mechanism is introduced to secure data exchange and payment among\nparticipants in the marketplace. BlendSM-DDM is able to offer a decentralized,\nscalable and auditable data exchanges for the data marketplace.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 15:24:23 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Xu", "Ronghua", ""], ["Ramachandran", "Gowri Sankar", ""], ["Chen", "Yu", ""], ["Krishnamachari", "Bhaskar", ""]]}, {"id": "1909.10948", "submitter": "Yu Chen", "authors": "Ronghua Xu, Yu Chen, Erik Blasch, Genshe Chen", "title": "Microchain: A Hybrid Consensus Mechanism for Lightweight Distributed\n  Ledger for IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blockchain and smart contract enabled security mechanism for IoT\napplications has been reported recently for urban, financial, and network\nservices. However, due to the power-intensive and a low-throughput consensus\nmechanism in existing blockchain, like Bitcoin and Ethereum, there are still\nchallenges in integrating blockchain technology into resource-constrained IoT\nplatforms. In this paper, Microchain, based on a hybrid Proof-of-Credit\n(PoC)-Voting-based Chain Finality (VCF) consensus protocol, is proposed to\nprovide a secure, scalable and lightweight distributed ledger for IoT systems.\nBy using a bias-resistant randomness protocol and a cryptographic sortition\nalgorithm, a random subset of nodes are selected as a final committee to\nperform the consensus protocol. The hybrid consensus mechanism relies on PoC, a\npure Proof of stake (PoS) protocol, to determine whether or not a participant\nis qualified to propose a block, given a fair initial distribution of the\ncredit assignment. The voting-based chain finality protocol is responsible for\nfinalizing a history of blocks by resolving conflicting checkpoint and\nselecting a unique chain. A proof-of-conception prototype is implemented and\ntested on a physical network environment. The experimental results verify that\nthe Micorchain is able to offer a partially decentralized, scalable and\nlightweight distributed ledger protocol for IoT applications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:19:59 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Xu", "Ronghua", ""], ["Chen", "Yu", ""], ["Blasch", "Erik", ""], ["Chen", "Genshe", ""]]}, {"id": "1909.10964", "submitter": "Fanrong Li", "authors": "Fanrong Li, Zitao Mo, Peisong Wang, Zejian Liu, Jiayun Zhang, Gang Li,\n  Qinghao Hu, Xiangyu He, Cong Leng, Yang Zhang, Jian Cheng", "title": "A System-Level Solution for Low-Power Object Detection", "comments": "Accepted by ICCV 2019 Low-Power Computer Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has made impressive progress in recent years with the help\nof deep learning. However, state-of-the-art algorithms are both computation and\nmemory intensive. Though many lightweight networks are developed for a\ntrade-off between accuracy and efficiency, it is still a challenge to make it\npractical on an embedded device. In this paper, we present a system-level\nsolution for efficient object detection on a heterogeneous embedded device. The\ndetection network is quantized to low bits and allows efficient implementation\nwith shift operators. In order to make the most of the benefits of low-bit\nquantization, we design a dedicated accelerator with programmable logic. Inside\nthe accelerator, a hybrid dataflow is exploited according to the heterogeneous\nproperty of different convolutional layers. We adopt a straightforward but\nresource-friendly column-prior tiling strategy to map the computation-intensive\nconvolutional layers to the accelerator that can support arbitrary feature\nsize. Other operations can be performed on the low-power CPU cores, and the\nentire system is executed in a pipelined manner. As a case study, we evaluate\nour object detection system on a real-world surveillance video with input size\nof 512x512, and it turns out that the system can achieve an inference speed of\n18 fps at the cost of 6.9W (with display) with an mAP of 66.4 verified on the\nPASCAL VOC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:45:43 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 13:57:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Li", "Fanrong", ""], ["Mo", "Zitao", ""], ["Wang", "Peisong", ""], ["Liu", "Zejian", ""], ["Zhang", "Jiayun", ""], ["Li", "Gang", ""], ["Hu", "Qinghao", ""], ["He", "Xiangyu", ""], ["Leng", "Cong", ""], ["Zhang", "Yang", ""], ["Cheng", "Jian", ""]]}, {"id": "1909.11013", "submitter": "Jo\\~ao Santos", "authors": "Jo\\~ao Amaral Santos, Pedro R. M. In\\'acio, Bruno M. Silva", "title": "Towards the Uses of Blockchain in Mobile Health Services and\n  Applications: A Survey", "comments": "Re-organization of the paper is required until it is published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of Bitcoin and blockchain, the growth and adaptation of\ncryptographic features and capabilities were quickly extended to new and\nunderexplored areas, such as healthcare. Currently, blockchain is being\nimplemented mainly as a mechanism to secure Electronic Health Records (EHRs).\nHowever, new studies have shown that this technology can be a powerful tool in\nempowering patients to control their own health data, as well for enabling a\nfool-proof health data history and establishing medical responsibility. With\nthe advent of mobile health (m-Health) sustained on service-oriented\narchitectures, the adaptation of blockchain mechanisms into m-Health\napplications creates the possibility for a more decentralized and available\nhealthcare service. Hence, this paper presents a review of the current security\nbest practices for m-Health including blockchain technologies in healthcare.\nMoreover, it discusses and elaborates on identified open-issues and\npotentialities regarding the uses of Blockchain. Finally, the paper proposes\nconceptual solutions for future blockchain implementations for m-Health\nServices and Applications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 15:50:22 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 15:00:21 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Santos", "Jo\u00e3o Amaral", ""], ["In\u00e1cio", "Pedro R. M.", ""], ["Silva", "Bruno M.", ""]]}, {"id": "1909.11058", "submitter": "Abdullah Yousafzai", "authors": "Abdullah Yousafzai, Ibrar Yaqoob, Muhammad Imran, Abdullah Gani, and\n  Rafidah Md Noor", "title": "Process migration-based computational offloading framework for\n  IoT-supported mobile edge/cloud computing", "comments": "IoT, Edge Computing, Computational Offloading Framework, 2 Algorithms\n  one for resource constraint client and another for edge server, and a unique\n  and easy to setup real IoT testbed idea using of the shelf smartphones, 12\n  pages, 12 figures", "journal-ref": null, "doi": "10.1109/JIOT.2019.2943176", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices have become an indispensable component of Internet of Things\n(IoT). However, these devices have resource constraints in processing\ncapabilities, battery power, and storage space, thus hindering the execution of\ncomputation-intensive applications that often require broad bandwidth,\nstringent response time, long battery life, and heavy computing power. Mobile\ncloud computing and mobile edge computing (MEC) are emerging technologies that\ncan meet the aforementioned requirements using offloading algorithms. In this\npaper, we analyze the effect of platform-dependent native applications on\ncomputational offloading in edge networks and propose a lightweight process\nmigration-based computational offloading framework. The proposed framework does\nnot require application binaries at edge servers and thus seamlessly migrates\nnative applications. The proposed framework is evaluated using an experimental\ntestbed. Numerical results reveal that the proposed framework saves almost 44%\nof the execution time and 84% of the energy consumption. Hence, the proposed\nframework shows profound potential for resource-intensive IoT application\nprocessing in MEC.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:16:45 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Yousafzai", "Abdullah", ""], ["Yaqoob", "Ibrar", ""], ["Imran", "Muhammad", ""], ["Gani", "Abdullah", ""], ["Noor", "Rafidah Md", ""]]}, {"id": "1909.11069", "submitter": "Alexandre Strapa\\c{c}\\~ao Guedes Vianna", "authors": "Alexandre Vianna and Waldemar Ferreira and Kiev Gama", "title": "An Exploratory Study of How Specialists Deal with Testing in Data Stream\n  Processing Applications", "comments": "ACM/IEEE International Symposium on Empirical Software Engineering\n  and Measurement (ESEM) ESEM 2019 Porto de Galinhas, Brazil September\n  19th-20th, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Background] Nowadays, there is a massive growth of data volume and speed in\nmany types of systems. It introduces new needs for infrastructure and\napplications that have to handle streams of data with low latency and high\nthroughput. Testing applications that process such data streams has become a\nsignificant challenge for engineers. Companies are adopting different\napproaches to dealing with this issue. Some have developed their own solutions\nfor testing, while others have adopted a combination of existing testing\ntechniques. There is no consensus about how or in which contexts such solutions\ncan be implemented. [Aims] To the best of our knowledge, there is no\nconsolidated literature on that topic. The present paper is an attempt to fill\nthis gap by conducting an exploratory study with practitioners. [Method] We\nused qualitative methods in this research, in particular interviews and survey.\nWe interviewed 12 professionals who work in projects related to data streams,\nand also administered a questionnaire with other 105 professionals. The\ninterviews went through a transcription and coding process, and the\nquestionnaires were analysed to reinforce findings. [Results] This study\npresents current practices around software testing in data stream processing\napplications. These practices involve methodologies, techniques, and tools.\n[Conclusions] Our main contribution is a compendium of alternatives for many of\nthe challenges that arise when testing streaming applications from a\nstate-of-the-practice perspective.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:45:14 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Vianna", "Alexandre", ""], ["Ferreira", "Waldemar", ""], ["Gama", "Kiev", ""]]}, {"id": "1909.11085", "submitter": "Libin Lu", "authors": "Libin Lu, Matthew J. Morse, Abtin Rahimian, Georg Stadler, Denis Zorin", "title": "Scalable Simulation of Realistic Volume Fraction Red Blood Cell Flows\n  through Vascular Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3295500.3356203", "report-no": null, "categories": "cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution blood flow simulations have potential for developing better\nunderstanding biophysical phenomena at the microscale, such as vasodilation,\nvasoconstriction and overall vascular resistance. To this end, we present a\nscalable platform for the simulation of red blood cell (RBC) flows through\ncomplex capillaries by modeling the physical system as a viscous fluid with\nimmersed deformable particles. We describe a parallel boundary integral\nequation solver for general elliptic partial differential equations, which we\napply to Stokes flow through blood vessels. We also detail a parallel collision\navoiding algorithm to ensure RBCs and the blood vessel remain contact-free. We\nhave scaled our code on Stampede2 at the Texas Advanced Computing Center up to\n34,816 cores. Our largest simulation enforces a contact-free state between four\nbillion surface elements and solves for three billion degrees of freedom on one\nmillion RBCs and a blood vessel composed from two million patches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 23:31:54 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Lu", "Libin", ""], ["Morse", "Matthew J.", ""], ["Rahimian", "Abtin", ""], ["Stadler", "Georg", ""], ["Zorin", "Denis", ""]]}, {"id": "1909.11147", "submitter": "Uri Zwick", "authors": "Jacob Holm, Valerie King, Mikkel Thorup, Or Zamir, Uri Zwick", "title": "Random $k$-out subgraph leaves only $O(n/k)$ inter-component edges", "comments": "22 pages, 1 figure, to appear at FOCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each vertex of an arbitrary simple graph on $n$ vertices chooses $k$ random\nincident edges. What is the expected number of edges in the original graph that\nconnect different connected components of the sampled subgraph? We prove that\nthe answer is $O(n/k)$, when $k\\ge c\\log n$, for some large enough $c$. We\nconjecture that the same holds for smaller values of $k$, possibly for any\n$k\\ge 2$. Such a result is best possible for any $k\\ge 2$. As an application,\nwe use this sampling result to obtain a one-way communication protocol with\n\\emph{private} randomness for finding a spanning forest of a graph in which\neach vertex sends only ${O}(\\sqrt{n}\\log n)$ bits to a referee.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 19:34:38 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Holm", "Jacob", ""], ["King", "Valerie", ""], ["Thorup", "Mikkel", ""], ["Zamir", "Or", ""], ["Zwick", "Uri", ""]]}, {"id": "1909.11150", "submitter": "Nouamane Laanait", "authors": "Nouamane Laanait, Joshua Romero, Junqi Yin, M. Todd Young, Sean\n  Treichler, Vitalii Starchenko, Albina Borisevich, Alex Sergeev, Michael\n  Matheson", "title": "Exascale Deep Learning for Scientific Inverse Problems", "comments": "13 pages, 9 figures. Under review by the Systems and Machine Learning\n  (SysML) Conference (SysML '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci cs.DC physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce novel communication strategies in synchronous distributed Deep\nLearning consisting of decentralized gradient reduction orchestration and\ncomputational graph-aware grouping of gradient tensors. These new techniques\nproduce an optimal overlap between computation and communication and result in\nnear-linear scaling (0.93) of distributed training up to 27,600 NVIDIA V100\nGPUs on the Summit Supercomputer. We demonstrate our gradient reduction\ntechniques in the context of training a Fully Convolutional Neural Network to\napproximate the solution of a longstanding scientific inverse problem in\nmaterials imaging. The efficient distributed training on a dataset size of 0.5\nPB, produces a model capable of an atomically-accurate reconstruction of\nmaterials, and in the process reaching a peak performance of 2.15(4)\nEFLOPS$_{16}$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 19:40:59 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Laanait", "Nouamane", ""], ["Romero", "Joshua", ""], ["Yin", "Junqi", ""], ["Young", "M. Todd", ""], ["Treichler", "Sean", ""], ["Starchenko", "Vitalii", ""], ["Borisevich", "Albina", ""], ["Sergeev", "Alex", ""], ["Matheson", "Michael", ""]]}, {"id": "1909.11238", "submitter": "Jingyu He", "authors": "Jingyu He, Yao Xiao, Corina Bogdan, Shahin Nazarian, Paul Bogdan", "title": "Design Methodology for Energy Efficient Unmanned Aerial Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a load-balancing approach to analyze and partition\nthe UAV perception and navigation intelligence (PNI) code for parallel\nexecution, as well as assigning each parallel computational task to a\nprocessing element in an Network-on-chip (NoC) architecture such that the total\ncommunication energy is minimized and congestion is reduced. First, we\nconstruct a data dependency graph (DDG) by converting the PNI high level\nprogram into Low Level Virtual Machine (LLVM) Intermediate Representation (IR).\nSecond, we propose a scheduling algorithm to partition the PNI application into\nclusters such that (1) inter-cluster communication is minimized, (2) NoC energy\nis reduced and (3) the workloads of different cores are balanced for maximum\nparallel execution. Finally, an energy-aware mapping scheme is adopted to\nassign clusters onto tile-based NoCs. We validate this approach with a drone\nself-navigation application and the experimental results show that our optimal\n32-core design achieves an average 82% energy savings and 4.7x performance\nspeedup against the state-of-art flight controller.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 00:13:21 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 17:49:37 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["He", "Jingyu", ""], ["Xiao", "Yao", ""], ["Bogdan", "Corina", ""], ["Nazarian", "Shahin", ""], ["Bogdan", "Paul", ""]]}, {"id": "1909.11261", "submitter": "Lei Yang", "authors": "Lei Yang, Vivek Bagaria, Gerui Wang, Mohammad Alizadeh, David Tse,\n  Giulia Fanti, Pramod Viswanath", "title": "Prism: Scaling Bitcoin by 10,000x", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is the first fully decentralized permissionless blockchain protocol\nand achieves a high level of security: the ledger it maintains has guaranteed\nliveness and consistency properties as long as the adversary has less compute\npower than the honest nodes. However, its throughput is only 7 transactions per\nsecond and the confirmation latency can be up to hours. Prism is a new\nblockchain protocol which is designed to achieve a natural scaling of Bitcoin's\nperformance while maintaining its full security guarantees. We present an\nimplementation of Prism which achieves a throughput of 70,000 transactions per\nsecond and confirmation latencies of tens of seconds.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 02:42:25 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:00:55 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yang", "Lei", ""], ["Bagaria", "Vivek", ""], ["Wang", "Gerui", ""], ["Alizadeh", "Mohammad", ""], ["Tse", "David", ""], ["Fanti", "Giulia", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1909.11365", "submitter": "Clement Mommessin", "authors": "Olivier Beaumont, Louis-claude Canon, Lionel Eyraud-Dubois, Giorgio\n  Lucarelli, Loris Marchal, Cl\\'ement Mommessin, Bertrand Simon, Denis Trystram", "title": "Scheduling on Two Types of Resources: a Survey", "comments": null, "journal-ref": "ACM Computing Survey, Vol. 53, No. 3, 2020", "doi": "10.1145/3387110", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution in the design of modern parallel platforms leads to revisit the\nscheduling jobs on distributed heterogeneous resources. The goal of this survey\nis to present the main existing algorithms, to classify them based on their\nunderlying principles and to propose unified implementations to enable their\nfair comparison, both in terms of running time and quality of schedules, on a\nlarge set of common benchmarks that we made available for the community. Beyond\nthis comparison, our goal is also to understand the main difficulties that\nheterogeneity conveys and the shared principles that guide the design of\nefficient algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 09:28:57 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 13:42:22 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Beaumont", "Olivier", ""], ["Canon", "Louis-claude", ""], ["Eyraud-Dubois", "Lionel", ""], ["Lucarelli", "Giorgio", ""], ["Marchal", "Loris", ""], ["Mommessin", "Cl\u00e9ment", ""], ["Simon", "Bertrand", ""], ["Trystram", "Denis", ""]]}, {"id": "1909.11469", "submitter": "Vinu Joseph", "authors": "Mark Van der Merwe, Vinu Joseph, Ganesh Gopalakrishnan", "title": "Message Scheduling for Performant, Many-Core Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Belief Propagation (BP) is a message-passing algorithm for approximate\ninference over Probabilistic Graphical Models (PGMs), finding many applications\nsuch as computer vision, error-correcting codes, and protein-folding. While\ngeneral, the convergence and speed of the algorithm has limited its practical\nuse on difficult inference problems. As an algorithm that is highly amenable to\nparallelization, many-core Graphical Processing Units (GPUs) could\nsignificantly improve BP performance. Improving BP through many-core systems is\nnon-trivial: the scheduling of messages in the algorithm strongly affects\nperformance. We present a study of message scheduling for BP on GPUs. We\ndemonstrate that BP exhibits a tradeoff between speed and convergence based on\nparallelism and show that existing message schedulings are not able to utilize\nthis tradeoff. To this end, we present a novel randomized message scheduling\napproach, Randomized BP (RnBP), which outperforms existing methods on the GPU.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 05:19:33 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Van der Merwe", "Mark", ""], ["Joseph", "Vinu", ""], ["Gopalakrishnan", "Ganesh", ""]]}, {"id": "1909.11590", "submitter": "Soujanya Ponnapalli", "authors": "Soujanya Ponnapalli, Aashaka Shah, Amy Tai, Souvik Banerjee, Vijay\n  Chidambaram, Dahlia Malkhi, Michael Wei", "title": "Rainblock: Faster Transaction Processing in Public Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public blockchains like Ethereum use Merkle trees to verify transactions\nreceived from untrusted servers before applying them to the blockchain. We\nempirically show that the low throughput of such blockchains is due to the I/O\nbottleneck associated with using Merkle trees for processing transactions. We\npresent RAINBLOCK, a new architecture for public blockchains that increases\nthroughput without affecting security. RAINBLOCK achieves this by tackling the\nI/O bottleneck on two fronts: first, decoupling transaction processing from\nI/O, and removing I/O from the critical path; second, reducing I/O\namplification by customizing storage for blockchains. RAINBLOCK uses a novel\nvariant of the Merkle tree, the Distributed Sharded Merkle tree (DSM-TREE) to\nstore system state. We evaluate RAINBLOCK using workloads based on public\nEthereum traces (including smart contracts) and show that RAINBLOCK processes\n20K transactions per second in a geo-distributed setting with four regions\nspread across three continents.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:28:51 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 10:33:26 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 18:51:12 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ponnapalli", "Soujanya", ""], ["Shah", "Aashaka", ""], ["Tai", "Amy", ""], ["Banerjee", "Souvik", ""], ["Chidambaram", "Vijay", ""], ["Malkhi", "Dahlia", ""], ["Wei", "Michael", ""]]}, {"id": "1909.11694", "submitter": "Sinan Aksoy", "authors": "Sinan G. Aksoy, Paul Bruillard, Stephen J. Young, Mark Raugas", "title": "Ramanujan Graphs and the Spectral Gap of Supercomputing Topologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph eigenvalues play a fundamental role in controlling structural\nproperties, such as bisection bandwidth, diameter, and fault tolerance, which\nare critical considerations in the design of supercomputing interconnection\nnetworks. This motivates considering graphs with optimal spectral expansion,\ncalled Ramanujan graphs, as potential candidates for interconnection networks.\nIn this work, we explore this possibility by comparing Ramanujan graph\nproperties against those of a wide swath of current and proposed supercomputing\ntopologies. We derive analytic expressions for the spectral gap, bisection\nbandwidth, and diameter of these topologies, some of which were previously\nunknown. We find the spectral gap of existing topologies are well-separated\nfrom the optimal achievable by Ramanujan topologies, suggesting the potential\nutility of adopting Ramanujan graphs as interconnection networks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:27:57 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 00:13:31 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Aksoy", "Sinan G.", ""], ["Bruillard", "Paul", ""], ["Young", "Stephen J.", ""], ["Raugas", "Mark", ""]]}, {"id": "1909.11704", "submitter": "Klaus Reuter", "authors": "Luka Stanisic, Klaus Reuter", "title": "MPCDF HPC Performance Monitoring System: Enabling Insight via\n  Job-Specific Analysis", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on the design and implementation of the HPC performance\nmonitoring system deployed to continuously monitor performance metrics of all\njobs on the HPC systems at the Max Planck Computing and Data Facility (MPCDF).\nThereby it reveals important information to various stakeholders, in particular\nto users, application support, system administrators, and management. On each\ncompute node, hardware and software performance monitoring data is collected by\nour newly developed lightweight open-source hpcmd middleware which builds upon\nstandard Linux tools. The data is transported via rsyslog, and aggregated and\nprocessed by a Splunk system, enabling detailed per-cluster and per-job\ninteractive analysis in a web browser. Additionally, performance reports are\nprovided to the users as PDF files. Finally, we report on practical experience\nand benefits from large-scale deployments on MPCDF HPC systems, demonstrating\nhow our solution can be useful to any HPC center.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:40:57 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Stanisic", "Luka", ""], ["Reuter", "Klaus", ""]]}, {"id": "1909.11762", "submitter": "Derek Schafer", "authors": "Derek Schafer, Sheikh Ghafoor, Daniel Holmes, Martin Ruefenacht,\n  Anthony Skjellum", "title": "Extending the Message Passing Interface (MPI) with User-Level Schedules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composability is one of seven reasons for the long-standing and continuing\nsuccess of MPI. Extending MPI by composing its operations with user-level\noperations provides useful integration with the progress engine and completion\nnotification methods of MPI. However, the existing extensibility mechanism in\nMPI (generalized requests) is not widely utilized and has significant\ndrawbacks.\n  MPI can be generalized via scheduled communication primitives, for example,\nby utilizing implementation techniques from existing MPI-3 nonblocking\ncollectives and from forthcoming MPI-4 persistent and partitioned APIs.\nNon-trivial schedules are used internally in some MPI libraries; but, they are\nnot accessible to end-users.\n  Message-based communication patterns can be built as libraries on top of MPI.\nSuch libraries can have comparable implementation maturity and potentially\nhigher performance than MPI library code, but do not require intimate knowledge\nof the MPI implementation. Libraries can provide performance-portable\ninterfaces that cross MPI implementation boundaries. The ability to compose\nadditional user-defined operations using the same progress engine benefits all\nkinds of general purpose HPC libraries.\n  We propose a definition for MPI schedules: a user-level programming model\nsuitable for creating persistent collective communication composed with new\napplication-specific sequences of user-defined operations managed by MPI and\nfully integrated with MPI progress and completion notification. The API\nproposed offers a path to standardization for extensible communication\nschedules involving user-defined operations. Our approach has the potential to\nintroduce event-driven programming into MPI (beyond the tools interface),\nalthough connecting schedules with events comprises future work.\n  Early performance results described here are promising and indicate strong\noverlap potential.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 20:49:11 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Schafer", "Derek", ""], ["Ghafoor", "Sheikh", ""], ["Holmes", "Daniel", ""], ["Ruefenacht", "Martin", ""], ["Skjellum", "Anthony", ""]]}, {"id": "1909.11958", "submitter": "Muhammad Shahbaz", "authors": "Sean Choi, Muhammad Shahbaz, Balaji Prabhakar, and Mendel Rosenblum", "title": "$\\lambda$-NIC: Interactive Serverless Compute on Programmable SmartNICs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in serverless compute, a cloud computing model\nthat automates infrastructure resource-allocation and management while billing\ncustomers only for the resources they use. Workloads like stream processing\nbenefit from high elasticity and fine-grain pricing of these serverless\nframeworks. However, so far, limited concurrency and high latency of server\nCPUs prohibit many interactive workloads (e.g., web servers and database\nclients) from taking advantage of serverless compute to achieve high\nperformance.\n  In this paper, we argue that server CPUs are ill-suited to run serverless\nworkloads (i.e., lambdas) and present $\\lambda$-NIC, an open-source framework,\nthat runs interactive workloads directly on a SmartNIC; more specifically an\nASIC-based NIC that consists of a dense grid of Network Processing Unit (NPU)\ncores. $\\lambda$-NIC leverages SmartNIC's proximity to the network and a vast\narray of NPU cores to simultaneously run thousands of lambdas on a single NIC\nwith strict tail-latency guarantees. To ease development and deployment of\nlambdas, $\\lambda$-NIC exposes an event-based programming abstraction,\nMatch+Lambda, and a machine model that allows developers to compose and execute\nlambdas on SmartNICs easily. Our evaluation shows that $\\lambda$-NIC achieves\nup to 880x and 736x improvements in workloads' response latency and throughput,\nrespectively, while significantly reducing host CPU and memory usage.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:50:51 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Choi", "Sean", ""], ["Shahbaz", "Muhammad", ""], ["Prabhakar", "Balaji", ""], ["Rosenblum", "Mendel", ""]]}, {"id": "1909.11985", "submitter": "Yidi Eddy WUwuwu", "authors": "Yidi Wu, Kaihao Ma, Xiao Yan, Zhi Liu, Zhenkun Cai, Yuzhen Huang,\n  James Cheng, Han Yuan, Fan Yu", "title": "Elastic deep learning in multi-tenant GPU cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to support elasticity, i.e., the ability to dynamically adjust\nthe parallelism (number of GPUs), for deep neural network (DNN) training.\nElasticity can benefit multi-tenant GPU cluster management in many ways, e.g.,\nachieving various scheduling objectives (e.g., job throughput, job completion\ntime, GPU efficiency) according to cluster load variations, maximizing the use\nof transient idle resources, performance profiling, job migration, and\nstraggler mitigation. However, existing parallelism adjustment strategies incur\nhigh overheads, which hinder many applications from making effective use of\nelasticity. We propose EDL to enable low-overhead elastic deep learning with a\nsimple API. We present techniques that are necessary to reduce the overhead of\nparallelism adjustments, such as stop-free scaling and dynamic data pipeline.\nWe also demonstrate that EDL can indeed bring significant benefits to the\nabove-listed applications in GPU cluster management.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 09:04:07 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 07:42:44 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wu", "Yidi", ""], ["Ma", "Kaihao", ""], ["Yan", "Xiao", ""], ["Liu", "Zhi", ""], ["Cai", "Zhenkun", ""], ["Huang", "Yuzhen", ""], ["Cheng", "James", ""], ["Yuan", "Han", ""], ["Yu", "Fan", ""]]}, {"id": "1909.12291", "submitter": "Steven Young", "authors": "Robert M. Patton, J. Travis Johnston, Steven R. Young, Catherine D.\n  Schuman, Thomas E. Potok, Derek C. Rose, Seung-Hwan Lim, Junghoon Chae, Le\n  Hou, Shahira Abousamra, Dimitris Samaras, Joel Saltz", "title": "Exascale Deep Learning to Accelerate Cancer Research", "comments": "Submitted to IEEE Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, through the use of neural networks, has demonstrated\nremarkable ability to automate many routine tasks when presented with\nsufficient data for training. The neural network architecture (e.g. number of\nlayers, types of layers, connections between layers, etc.) plays a critical\nrole in determining what, if anything, the neural network is able to learn from\nthe training data. The trend for neural network architectures, especially those\ntrained on ImageNet, has been to grow ever deeper and more complex. The result\nhas been ever increasing accuracy on benchmark datasets with the cost of\nincreased computational demands. In this paper we demonstrate that neural\nnetwork architectures can be automatically generated, tailored for a specific\napplication, with dual objectives: accuracy of prediction and speed of\nprediction. Using MENNDL--an HPC-enabled software stack for neural architecture\nsearch--we generate a neural network with comparable accuracy to\nstate-of-the-art networks on a cancer pathology dataset that is also $16\\times$\nfaster at inference. The speedup in inference is necessary because of the\nvolume and velocity of cancer pathology data; specifically, the previous\nstate-of-the-art networks are too slow for individual researchers without\naccess to HPC systems to keep pace with the rate of data generation. Our new\nmodel enables researchers with modest computational resources to analyze newly\ngenerated data faster than it is collected.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 17:53:26 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Patton", "Robert M.", ""], ["Johnston", "J. Travis", ""], ["Young", "Steven R.", ""], ["Schuman", "Catherine D.", ""], ["Potok", "Thomas E.", ""], ["Rose", "Derek C.", ""], ["Lim", "Seung-Hwan", ""], ["Chae", "Junghoon", ""], ["Hou", "Le", ""], ["Abousamra", "Shahira", ""], ["Samaras", "Dimitris", ""], ["Saltz", "Joel", ""]]}, {"id": "1909.12326", "submitter": "Yuang Jiang", "authors": "Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee,\n  Kin K. Leung, Leandros Tassiulas", "title": "Model Pruning Enables Efficient Federated Learning on Edge Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) allows model training from local data collected by\nedge/mobile devices, while preserving data privacy. A challenge is that client\ndevices in FL usually have much more limited computation and communication\nresources compared to servers in a datacenter. To overcome this challenge, we\npropose PruneFL -- a novel FL approach with adaptive and distributed parameter\npruning, which adapts the model size during FL to reduce both communication and\ncomputation overhead and minimize the overall training time, while maintaining\na similar accuracy as the original model. PruneFL includes initial pruning at a\nselected client and further pruning as part of the FL process. The model size\nis adapted during this process, which includes maximizing the approximate\nempirical risk reduction divided by the time of one FL round. Our experiments\nwith various datasets on edge devices (e.g., Raspberry Pi) show that: (i) we\nsignificantly reduce the training time compared to conventional FL and various\nother pruning-based methods; (ii) the pruned model converges to an accuracy\nthat is very similar to the original model but has a much smaller size, and it\nis also a lottery ticket of the original model.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 18:32:33 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 21:16:43 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 03:01:22 GMT"}, {"version": "v4", "created": "Fri, 23 Oct 2020 03:35:42 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Jiang", "Yuang", ""], ["Wang", "Shiqiang", ""], ["Valls", "Victor", ""], ["Ko", "Bong Jun", ""], ["Lee", "Wei-Han", ""], ["Leung", "Kin K.", ""], ["Tassiulas", "Leandros", ""]]}, {"id": "1909.12469", "submitter": "Jaqueline Brito", "authors": "Jaqueline J. Brito, Thiago Mosqueiro, Jeremy Rotman, Victor Xue,\n  Douglas J. Chapski, Juan De la Hoz, Paulo Matias, Lana Martin, Alex\n  Zelikovsky, Matteo Pellegrinni, Serghei Mangul", "title": "Telescope: an interactive tool for managing large scale analysis from\n  mobile devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world of big data, computational analysis has become a key driver\nof biomedical research. Recent exponential growth in the volume of available\nomics data has reshaped the landscape of contemporary biology, creating demand\nfor a continuous feedback loop that seamlessly integrates experimental biology\ntechniques and bioinformatics tools. High-performance computational facilities\nare capable of processing considerable volumes of data, yet often lack an\neasy-to-use interface to guide the user in supervising and adjusting\nbioinformatics analysis in real-time. Here we report the development of\nTelescope, a novel interactive tool that interfaces with high-performance\ncomputational clusters to deliver an intuitive user interface for controlling\nand monitoring bioinformatics analyses in real-time. Telescope was designed to\nnatively operate with a simple and straightforward interface using Web 2.0\ntechnology compatible with most modern devices (e.g., tablets and personal\nsmartphones). Telescope provides a modern and elegant solution to integrate\ncomputational analyses into the experimental environment of biomedical\nresearch. Additionally, it allows biomedical researchers to leverage the power\nof large computational facilities in a user-friendly manner. Telescope is\nfreely available at https://github.com/Mangul-Lab-USC/telescope.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:21:49 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 19:28:27 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 19:08:44 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Brito", "Jaqueline J.", ""], ["Mosqueiro", "Thiago", ""], ["Rotman", "Jeremy", ""], ["Xue", "Victor", ""], ["Chapski", "Douglas J.", ""], ["De la Hoz", "Juan", ""], ["Matias", "Paulo", ""], ["Martin", "Lana", ""], ["Zelikovsky", "Alex", ""], ["Pellegrinni", "Matteo", ""], ["Mangul", "Serghei", ""]]}, {"id": "1909.12533", "submitter": "Anastasia Kruchinina", "authors": "Anastasia Kruchinina, Elias Rudberg, Emanuel H. Rubensson", "title": "Efficient computation of the density matrix with error control on\n  distributed computer systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recursive polynomial expansion for construction of a density matrix\napproximation with rigorous error control [J. Chem. Phys. 128, 074106 (2008)]\nis implemented in the quantum chemistry program Ergo [SoftwareX 7, 107 (2018)]\nusing the Chunks and Tasks matrix library [Parallel Comput. 57, 87 (2016)]. The\nexpansion is based on second-order polynomials and accelerated by the\nscale-and-fold technique [J. Chem. Theory Comput. 7, 1233 (2011)]. We evaluate\nthe performance of the implementation by computing the density matrix from the\nFock matrix in the large-scale self-consistent field calculations. We\ndemonstrate that the amount of communicated data per worker process tends to a\nconstant with increasing system size and number of computer nodes such that the\namount of work per worker process is fixed.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:36:38 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Kruchinina", "Anastasia", ""], ["Rudberg", "Elias", ""], ["Rubensson", "Emanuel H.", ""]]}, {"id": "1909.12684", "submitter": "Daniele Cesarini", "authors": "Daniele Cesarini, Andrea Bartolini, Andrea Borghesi, Carlo Cavazzoni,\n  Mathieu Luisier, Luca Benini", "title": "COUNTDOWN Slack: a Run-time Library to Reduce Energy Footprint in\n  Large-scale MPI Applications", "comments": "13 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power consumption of supercomputers is a major challenge for system\nowners, users, and society. It limits the capacity of system installations, it\nrequires large cooling infrastructures, and it is the cause of a large carbon\nfootprint. Reducing power during application execution without changing the\napplication source code or increasing time-to-completion is highly desirable in\nreal-life high-performance computing scenarios. The power management run-time\nframeworks proposed in the last decade are based on the assumption that the\nduration of communication and application phases in an MPI application can be\npredicted and used at run-time to trade-off communication slack with power\nconsumption. In this manuscript, we first show that this assumption is too\ngeneral and leads to mispredictions, slowing down applications, thereby\njeopardizing the claimed benefits. We then propose a new approach based on (i)\nthe separation of communication phases and slack during MPI calls and (ii) a\ntimeout algorithm to cope with the hardware power management latency, which\njointly makes it possible to achieve performance-neutral power saving in MPI\napplications without requiring labor-intensive and risky application source\ncode modifications. We validate our approach in a tier-1 production environment\nwith widely adopted scientific applications. Our approach has a\ntime-to-completion overhead lower than 1%, while it successfully exploits slack\nin communication phases to achieve an average energy saving of 10%. If we focus\non a large-scale application runs, the proposed approach achieves 22% energy\nsaving with an overhead of only 0.4%. With respect to state-of-the-art\napproaches, COUNTDOWN Slack is the only that always leads to an energy saving\nwith negligible overhead (<3%).\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 13:40:01 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Cesarini", "Daniele", ""], ["Bartolini", "Andrea", ""], ["Borghesi", "Andrea", ""], ["Cavazzoni", "Carlo", ""], ["Luisier", "Mathieu", ""], ["Benini", "Luca", ""]]}, {"id": "1909.12934", "submitter": "Mansi M", "authors": "G. Hall, M. Mansi, I. Makrant", "title": "Novel method for handling Ethereum attack", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Block-chain world is very dynamic and there is need for strong governance and\nunderlying technology architecture to be robust to face challenges. This paper\nconsiders Ethereum, a leading block chain. We deep dive into the nature of this\nblock chain, wherein for software upgrades forks are performed. They types of\nforks and impact is discussed. A specific Ethereum hack led to a hard fork and\nfocus is provided on understanding the hack and overcoming it from a novel\napproach. The current model has been unable to handle multiple Ethereum\nattacks. Thus the current approach is compared against a novel approach\nproviding a security and scaling solution. Here the architecture draws upon\ncombining block-chain layers into operating system level. The approach can have\ntremendous benefits to block chain world and improve the way decentralized\napplication teams perform. The benefits of the novel architecture is discussed.\nThe approach helps safe guard block chain projects, making them safer and chain\nagnostic.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 17:05:59 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 06:51:23 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Hall", "G.", ""], ["Mansi", "M.", ""], ["Makrant", "I.", ""]]}, {"id": "1909.12966", "submitter": "Daniel Reynolds", "authors": "Daniel R. Reynolds and David J. Gardner and Cody J. Balos and Carol S.\n  Woodward", "title": "SUNDIALS Multiphysics+MPIManyVector Performance Testing", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "LLNL-TR-791538", "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we document performance test results on a SUNDIALS-based\nmultiphysics demonstration application. We aim to assess the large-scale\nparallel performance of new capabilities that have been added to the SUNDIALS\nsuite of time integrators and nonlinear solvers in recent years under funding\nfrom both the Exascale Computing Project (ECP) and the Scientific Discovery\nthrough Advanced Scientific (SciDAC) program, specifically: (a) SUNDIALS' new\nMPIManyVector module, that allows extreme flexibility in how a solution\n\"vector\" is staged on computational resources, (b) ARKode's new multirate\nintegration module, MRIStep, allowing high-order accurate calculations that\nsubcycle \"fast\" processes within \"slow\" ones, (c) SUNDIALS' new flexible linear\nsolver interfaces, that allow streamlined specification of problem-specific\nlinear solvers, and (d) SUNDIALS' new N_Vector additions of \"fused\" vector\noperations (to increase arithmetic intensity) and separation of reduction\noperations into \"local\" and \"global\" versions (to reduce latency by combining\nmultiple reductions into a single MPI_Allreduce call). We anticipate that\nsubsequent reports will extend this work to investigate a variety of other new\nfeatures, including SUNDIALS' generic SUNNonlinearSolver interface and\naccelerator-enabled N_Vector modules, and upcoming MRIStep extensions to\nsupport custom \"fast\" integrators (that leverage problem structure) and IMEX\nintegration of the \"slow\" time scale (to add diffusion).\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 21:48:40 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Reynolds", "Daniel R.", ""], ["Gardner", "David J.", ""], ["Balos", "Cody J.", ""], ["Woodward", "Carol S.", ""]]}, {"id": "1909.13014", "submitter": "Amirhossein Reisizadeh", "authors": "Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie,\n  Ramtin Pedarsani", "title": "FedPAQ: A Communication-Efficient Federated Learning Method with\n  Periodic Averaging and Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed framework according to which a model is\ntrained over a set of devices, while keeping data localized. This framework\nfaces several systems-oriented challenges which include (i) communication\nbottleneck since a large number of devices upload their local updates to a\nparameter server, and (ii) scalability as the federated network consists of\nmillions of devices. Due to these systems challenges as well as issues related\nto statistical heterogeneity of data and privacy concerns, designing a provably\nefficient federated learning method is of significant importance yet it remains\nchallenging. In this paper, we present FedPAQ, a communication-efficient\nFederated Learning method with Periodic Averaging and Quantization. FedPAQ\nrelies on three key features: (1) periodic averaging where models are updated\nlocally at devices and only periodically averaged at the server; (2) partial\ndevice participation where only a fraction of devices participate in each round\nof the training; and (3) quantized message-passing where the edge nodes\nquantize their updates before uploading to the parameter server. These features\naddress the communications and scalability challenges in federated learning. We\nalso show that FedPAQ achieves near-optimal theoretical guarantees for strongly\nconvex and non-convex loss functions and empirically demonstrate the\ncommunication-computation tradeoff provided by our method.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 03:10:53 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 02:38:39 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 21:37:38 GMT"}, {"version": "v4", "created": "Sun, 7 Jun 2020 19:09:29 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Jadbabaie", "Ali", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "1909.13380", "submitter": "Krishna Kumar", "authors": "Krishna Kumar, Jeffrey Salmond, Shyamini Kularathna, Christopher\n  Wilkes, Ezra Tjung, Giovanna Biscontin, Kenichi Soga", "title": "Scalable and modular material point method for large-scale simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe a new scalable and modular material point method\n(MPM) code developed for solving large-scale problems in continuum mechanics.\nThe MPM is a hybrid Eulerian-Lagrangian approach, which uses both moving\nmaterial points and computational nodes on a background mesh. The MPM has been\nsuccessfully applied to solve large-deformation problems such as landslides,\nfailure of slopes, concrete flows, etc. Solving these large-deformation\nproblems result in the material points actively moving through the mesh.\nDeveloping an efficient parallelisation scheme for the MPM code requires\ndynamic load-balancing techniques for both the material points and the\nbackground mesh. This paper describes the data structures and algorithms\nemployed to improve the performance and portability of the MPM code. An\nobject-oriented programming paradigm is adopted to modularise the MPM code. The\nUnified Modelling Language (UML) diagram of the MPM code structure is shown in\nFigure 1.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 22:13:29 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kumar", "Krishna", ""], ["Salmond", "Jeffrey", ""], ["Kularathna", "Shyamini", ""], ["Wilkes", "Christopher", ""], ["Tjung", "Ezra", ""], ["Biscontin", "Giovanna", ""], ["Soga", "Kenichi", ""]]}, {"id": "1909.13391", "submitter": "Jayanth Regatti", "authors": "Jayanth Regatti, Gaurav Tendolkar, Yi Zhou, Abhishek Gupta, Yingbin\n  Liang", "title": "Distributed SGD Generalizes Well Under Asynchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of fully synchronized distributed systems has faced a\nbottleneck due to the big data trend, under which asynchronous distributed\nsystems are becoming a major popularity due to their powerful scalability. In\nthis paper, we study the generalization performance of stochastic gradient\ndescent (SGD) on a distributed asynchronous system. The system consists of\nmultiple worker machines that compute stochastic gradients which are further\nsent to and aggregated on a common parameter server to update the variables,\nand the communication in the system suffers from possible delays. Under the\nalgorithm stability framework, we prove that distributed asynchronous SGD\ngeneralizes well given enough data samples in the training optimization. In\nparticular, our results suggest to reduce the learning rate as we allow more\nasynchrony in the distributed system. Such adaptive learning rate strategy\nimproves the stability of the distributed algorithm and reduces the\ncorresponding generalization error. Then, we confirm our theoretical findings\nvia numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 22:35:42 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Regatti", "Jayanth", ""], ["Tendolkar", "Gaurav", ""], ["Zhou", "Yi", ""], ["Gupta", "Abhishek", ""], ["Liang", "Yingbin", ""]]}, {"id": "1909.13403", "submitter": "Zinan Lin", "authors": "Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, Vyas Sekar", "title": "Using GANs for Sharing Networked Time Series Data: Challenges, Initial\n  Promise, and Open Questions", "comments": "Published in IMC 2020. 20 pages, 26 figures", "journal-ref": null, "doi": "10.1145/3419394.3423643", "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited data access is a longstanding barrier to data-driven research and\ndevelopment in the networked systems community. In this work, we explore if and\nhow generative adversarial networks (GANs) can be used to incentivize data\nsharing by enabling a generic framework for sharing synthetic datasets with\nminimal expert knowledge. As a specific target, our focus in this paper is on\ntime series datasets with metadata (e.g., packet loss rate measurements with\ncorresponding ISPs). We identify key challenges of existing GAN approaches for\nsuch workloads with respect to fidelity (e.g., long-term dependencies, complex\nmultidimensional relationships, mode collapse) and privacy (i.e., existing\nguarantees are poorly understood and can sacrifice fidelity). To improve\nfidelity, we design a custom workflow called DoppelGANger (DG) and demonstrate\nthat across diverse real-world datasets (e.g., bandwidth measurements, cluster\nrequests, web sessions) and use cases (e.g., structural characterization,\npredictive modeling, algorithm comparison), DG achieves up to 43% better\nfidelity than baseline models. Although we do not resolve the privacy problem\nin this work, we identify fundamental challenges with both classical notions of\nprivacy and recent advances to improve the privacy properties of GANs, and\nsuggest a potential roadmap for addressing these challenges. By shedding light\non the promise and challenges, we hope our work can rekindle the conversation\non workflows for data sharing.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 00:13:19 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 06:39:40 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 15:45:27 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2020 01:20:02 GMT"}, {"version": "v5", "created": "Sun, 17 Jan 2021 04:54:51 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Lin", "Zinan", ""], ["Jain", "Alankar", ""], ["Wang", "Chen", ""], ["Fanti", "Giulia", ""], ["Sekar", "Vyas", ""]]}, {"id": "1909.13548", "submitter": "Sai Dayapule", "authors": "Fan Yao, Kathy Ngyugen, Sai Santosh Dayapule, Jingxin Wu, Bingqian Lu,\n  Suresh Subramaniam, and Guru Venkataramani", "title": "HolDCSim: A Holistic Simulator for Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing based systems, that span data centers, are commonly deployed\nto offer high performance for user service requests. As data centers continue\nto expand, computer architects and system designers are facing many challenges\non how to balance resource utilization efficiency, server and network\nperformance, energy consumption and quality-of-service (QoS) demands from the\nusers. To develop effective data center management policies, it becomes\nessential to have an in-depth understanding and synergistic control of the\nvarious sub-components inside large scale computing systems, that include both\ncomputation and communication resources. In this paper, we propose HolDCSim, a\nlight-weight, holistic, extensible, event-driven data center simulation\nplatform that effectively models both server and network architectures.\nHolDCSim can be used in a variety of data center system studies including\njob/task scheduling, resource provisioning, global and local server farm power\nmanagement, and network and server performance analysis. We demonstrate the\ndesign of our simulation infrastructure, and illustrate the usefulness of our\nframework with several case studies that analyze server/network performance and\nenergy efficiency. We also perform validation on real machines to verify our\nsimulator.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 09:24:40 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 14:13:21 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yao", "Fan", ""], ["Ngyugen", "Kathy", ""], ["Dayapule", "Sai Santosh", ""], ["Wu", "Jingxin", ""], ["Lu", "Bingqian", ""], ["Subramaniam", "Suresh", ""], ["Venkataramani", "Guru", ""]]}, {"id": "1909.13560", "submitter": "Lorenc Kapllani", "authors": "Lorenc Kapllani and Long Teng", "title": "Multistep schemes for solving backward stochastic differential equations\n  on GPU", "comments": "24 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to parallelize the multistep scheme for the\nnumerical approximation of the backward stochastic differential equations\n(BSDEs) in order to achieve both, a high accuracy and a reduction of the\ncomputation time as well. In the multistep scheme the computations at each grid\npoint are independent and this fact motivates us to select massively parallel\nGPU computing using CUDA. In our investigations we identify performance\nbottlenecks and apply appropriate optimization techniques for reducing the\ncomputation time, using a uniform domain. Finally, some examples with financial\napplications are provided to demonstrate the achieved acceleration on GPUs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 09:49:03 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 16:17:06 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Kapllani", "Lorenc", ""], ["Teng", "Long", ""]]}, {"id": "1909.13639", "submitter": "Ameer Haj-Ali", "authors": "Ameer Haj-Ali, Nesreen K. Ahmed, Ted Willke, Sophia Shao, Krste\n  Asanovic, Ion Stoica", "title": "NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges arising when compilers vectorize loops for today's\nSIMD-compatible architectures is to decide if vectorization or interleaving is\nbeneficial. Then, the compiler has to determine how many instructions to pack\ntogether and how many loop iterations to interleave. Compilers are designed\ntoday to use fixed-cost models that are based on heuristics to make\nvectorization decisions on loops. However, these models are unable to capture\nthe data dependency, the computation graph, or the organization of\ninstructions. Alternatively, software engineers often hand-write the\nvectorization factors of every loop. This, however, places a huge burden on\nthem, since it requires prior experience and significantly increases the\ndevelopment time. In this work, we explore a novel approach for handling loop\nvectorization and propose an end-to-end solution using deep reinforcement\nlearning (RL). We conjecture that deep RL can capture different instructions,\ndependencies, and data structures to enable learning a sophisticated model that\ncan better predict the actual performance cost and determine the optimal\nvectorization factors. We develop an end-to-end framework, from code to\nvectorization, that integrates deep RL in the LLVM compiler. Our proposed\nframework takes benchmark codes as input and extracts the loop codes. These\nloop codes are then fed to a loop embedding generator that learns an embedding\nfor these loops. Finally, the learned embeddings are used as input to a Deep RL\nagent, which determines the vectorization factors for all the loops. We further\nextend our framework to support multiple supervised learning methods. We\nevaluate our approaches against the currently used LLVM vectorizer and loop\npolyhedral optimization techniques. Our experiments show 1.29X-4.73X\nperformance speedup compared to baseline and only 3% worse than the brute-force\nsearch on a wide range of benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 12:29:09 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 12:29:38 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 22:57:00 GMT"}, {"version": "v4", "created": "Sat, 4 Jan 2020 09:11:03 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Haj-Ali", "Ameer", ""], ["Ahmed", "Nesreen K.", ""], ["Willke", "Ted", ""], ["Shao", "Sophia", ""], ["Asanovic", "Krste", ""], ["Stoica", "Ion", ""]]}, {"id": "1909.13654", "submitter": "Tian Zhao", "authors": "Tian Zhao, Yaqi Zhang, Kunle Olukotun", "title": "Serving Recurrent Neural Networks Efficiently with a Spatial Accelerator", "comments": null, "journal-ref": "Proceedings of the 2 nd SysML Conference, Palo Alto, CA, USA,\n  2019. Copyright 2019 by the author(s)", "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) applications form a major class of AI-powered,\nlow-latency data center workloads. Most execution models for RNN acceleration\nbreak computation graphs into BLAS kernels, which lead to significant\ninter-kernel data movement and resource underutilization. We show that by\nsupporting more general loop constructs that capture design parameters in\naccelerators, it is possible to improve resource utilization using cross-kernel\noptimization without sacrificing programmability. Such abstraction level\nenables a design space search that can lead to efficient usage of on-chip\nresources on a spatial architecture across a range of problem sizes. We\nevaluate our optimization strategy on such abstraction with DeepBench using a\nconfigurable spatial accelerator. We demonstrate that this implementation\nprovides a geometric speedup of 30x in performance, 1.6x in area, and 2x in\npower efficiency compared to a Tesla V100 GPU, and a geometric speedup of 2x\ncompared to Microsoft Brainwave implementation on a Stratix 10 FPGA.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 00:55:17 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhao", "Tian", ""], ["Zhang", "Yaqi", ""], ["Olukotun", "Kunle", ""]]}, {"id": "1909.13670", "submitter": "Se Kwon Lee", "authors": "Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, Vijay\n  Chidambaram", "title": "RECIPE : Converting Concurrent DRAM Indexes to Persistent-Memory Indexes", "comments": "3pages: Added one more reference", "journal-ref": null, "doi": "10.1145/3341301.3359635", "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Recipe, a principled approach for converting concurrent DRAM\nindexes into crash-consistent indexes for persistent memory (PM). The main\ninsight behind Recipe is that isolation provided by a certain class of\nconcurrent in-memory indexes can be translated with small changes to\ncrash-consistency when the same index is used in PM. We present a set of\nconditions that enable the identification of this class of DRAM indexes, and\nthe actions to be taken to convert each index to be persistent. Based on these\nconditions and conversion actions, we modify five different DRAM indexes based\non B+ trees, tries, radix trees, and hash tables to their crash-consistent PM\ncounterparts. The effort involved in this conversion is minimal, requiring\n30-200 lines of code. We evaluated the converted PM indexes on Intel DC\nPersistent Memory, and found that they outperform state-of-the-art,\nhand-crafted PM indexes in multi-threaded workloads by up-to 5.2x. For example,\nwe built P-CLHT, our PM implementation of the CLHT hash table by modifying only\n30 LOC. When running YCSB workloads, P-CLHT performs up to 2.4x better than\nCacheline-Conscious Extendible Hashing (CCEH), the state-of-the-art PM hash\ntable.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 02:21:18 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:59:31 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 04:03:04 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2019 18:23:08 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Lee", "Se Kwon", ""], ["Mohan", "Jayashree", ""], ["Kashyap", "Sanidhya", ""], ["Kim", "Taesoo", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "1909.13672", "submitter": "Robert Kl\\\"ofkorn", "authors": "Peter Bastian, Markus Blatt, Andreas Dedner, Nils-Arne Dreier,\n  Christian Engwer, Ren\\'e Fritze, Carsten Gr\\\"aser, Christoph Gr\\\"uninger,\n  Dominic Kempf, Robert Kl\\\"ofkorn, Mario Ohlberger, Oliver Sander", "title": "The DUNE Framework: Basic Concepts and Recent Developments", "comments": "69 pages, 14 figures, 4 tables and various code examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the basic concepts and the module structure of the\nDistributed and Unified Numerics Environment and reflects on recent\ndevelopments and general changes that happened since the release of the first\nDune version in 2007 and the main papers describing that state [1, 2]. This\ndiscussion is accompanied with a description of various advanced features, such\nas coupling of domains and cut cells, grid modifications such as adaptation and\nmoving domains, high order discretizations and node level performance,\nnon-smooth multigrid methods, and multiscale methods. A brief discussion on\ncurrent and future development directions of the framework concludes the paper.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 13:15:53 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 10:42:21 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 16:27:51 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bastian", "Peter", ""], ["Blatt", "Markus", ""], ["Dedner", "Andreas", ""], ["Dreier", "Nils-Arne", ""], ["Engwer", "Christian", ""], ["Fritze", "Ren\u00e9", ""], ["Gr\u00e4ser", "Carsten", ""], ["Gr\u00fcninger", "Christoph", ""], ["Kempf", "Dominic", ""], ["Kl\u00f6fkorn", "Robert", ""], ["Ohlberger", "Mario", ""], ["Sander", "Oliver", ""]]}, {"id": "1909.13772", "submitter": "Martin Bauer", "authors": "Martin Bauer, Sebastian Eibl, Christian Godenschwager, Nils Kohl,\n  Michael Kuron, Christoph Rettinger, Florian Schornbaum, Christoph\n  Schwarzmeier, Dominik Th\\\"onnes, Harald K\\\"ostler, Ulrich R\\\"ude", "title": "waLBerla: A block-structured high-performance framework for multiphysics\n  simulations", "comments": null, "journal-ref": null, "doi": "10.1016/j.camwa.2020.01.007", "report-no": null, "categories": "cs.DC cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming current supercomputers efficiently is a challenging task.\nMultiple levels of parallelism on the core, on the compute node, and between\nnodes need to be exploited to make full use of the system. Heterogeneous\nhardware architectures with accelerators further complicate the development\nprocess. waLBerla addresses these challenges by providing the user with highly\nefficient building blocks for developing simulations on block-structured grids.\nThe block-structured domain partitioning is flexible enough to handle complex\ngeometries, while the structured grid within each block allows for highly\nefficient implementations of stencil-based algorithms. We present several\nexample applications realized with waLBerla, ranging from lattice Boltzmann\nmethods to rigid particle simulations. Most importantly, these methods can be\ncoupled together, enabling multiphysics simulations. The framework uses\nmeta-programming techniques to generate highly efficient code for CPUs and GPUs\nfrom a symbolic method formulation. To ensure software quality and performance\nportability, a continuous integration toolchain automatically runs an extensive\ntest suite encompassing multiple compilers, hardware architectures, and\nsoftware configurations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 15:13:22 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Bauer", "Martin", ""], ["Eibl", "Sebastian", ""], ["Godenschwager", "Christian", ""], ["Kohl", "Nils", ""], ["Kuron", "Michael", ""], ["Rettinger", "Christoph", ""], ["Schornbaum", "Florian", ""], ["Schwarzmeier", "Christoph", ""], ["Th\u00f6nnes", "Dominik", ""], ["K\u00f6stler", "Harald", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1909.13839", "submitter": "Sami Alabed", "authors": "Sami Alabed", "title": "RLCache: Automated Cache Management Using Reinforcement Learning", "comments": "MPhil Thesis, 76 pages, Reinforcement Learning, Multi-agent,\n  multi-task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the use of reinforcement learning to guide a general\npurpose cache manager decisions. Cache managers directly impact the overall\nperformance of computer systems. They govern decisions about which objects\nshould be cached, the duration they should be cached for, and decides on which\nobjects to evict from the cache if it is full. These three decisions impact\nboth the cache hit rate and size of the storage that is needed to achieve that\ncache hit rate. An optimal cache manager will avoid unnecessary operations,\nmaximise the cache hit rate which results in fewer round trips to a slower\nbackend storage system, and minimise the size of storage needed to achieve a\nhigh hit-rate.\n  This project investigates using reinforcement learning in cache management by\ndesigning three separate agents for each of the cache manager tasks.\nFurthermore, the project investigates two advanced reinforcement learning\narchitectures for multi-decision problems: a single multi-task agent and a\nmulti-agent. We also introduce a framework to simplify the modelling of\ncomputer systems problems as a reinforcement learning task. The framework\nabstracts delayed experiences observations and reward assignment in computer\nsystems while providing a flexible way to scale to multiple agents.\n  Simulation results based on an established database benchmark system show\nthat reinforcement learning agents can achieve a higher cache hit rate over\nheuristic driven algorithms while minimising the needed space. They are also\nable to adapt to a changing workload and dynamically adjust their caching\nstrategy accordingly. The proposed cache manager model is generic and\napplicable to other types of caches, such as file system caches. This project\nis the first, to our knowledge, to model cache manager decisions as a\nmulti-task control problem.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:03:51 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Alabed", "Sami", ""]]}]