[{"id": "2010.00096", "submitter": "Sergio Rajsbaum", "authors": "Carole Delporte, Hugues Fauconnier, Sergio Rajsbaum, Michel Raynal", "title": "$t$-Resilient $k$-Immediate Snapshot and its Relation with Agreement\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An immediate snapshot object is a high level communication object, built on\ntop of a read/write distributed system in which all except one processes may\ncrash. It allows a process to write a value and obtain a set of values that\nrepresent a snapshot of the values written to the object, occurring immediately\nafter the write step.\n  Considering an $n$-process model in which up to $t$ processes may crash, this\npaper introduces first the $k$-resilient immediate snapshot object, which is a\nnatural generalization of the basic immediate snapshot (which corresponds to\nthe case $k=t=n-1$). In addition to the set containment properties of the basic\nimmediate snapshot, a $k$-resilient immediate snapshot object requires that\neach set returned to a process contains at least $(n-k)$ pairs.\n  The paper first shows that, for $k,t<n-1$, $k$-resilient immediate snapshot\nis impossible in asynchronous read/write systems. %Then the paper investigates\nthe space of objects that %are impossible to solve in $n$-process $t$-crash\nread/write systems. Then the paper investigates a model of computation where\nthe processes communicate with each other by accessing $k$-immediate snapshot\nobjects, and shows that this model is stronger than the $t$-crash model.\nConsidering the space of $x$-set agreement problems (which are impossible to\nsolve in systems such that $x\\leq t$), the paper shows then that $x$-set\nagreement can be solved in read/write systems enriched with $k$-immediate\nsnapshot objects for $x=\\max(1,t+k-(n-2))$. It also shows that, in these\nsystems, $k$-resilient immediate snapshot and consensus are equivalent when\n$1\\leq t<n/2$ and $t\\leq k\\leq (n-1)-t$. Hence, %thanks to the problem map it\nprovides, the paper establishes strong relations linking fundamental\ndistributed computing objects (one related to communication, the other to\nagreement), which are impossible to solve in pure read/write systems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 20:22:49 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Delporte", "Carole", ""], ["Fauconnier", "Hugues", ""], ["Rajsbaum", "Sergio", ""], ["Raynal", "Michel", ""]]}, {"id": "2010.00130", "submitter": "Sergi Abadal", "authors": "Sergi Abadal, Akshay Jain, Robert Guirado, Jorge L\\'opez-Alonso,\n  Eduard Alarc\\'on", "title": "Computing Graph Neural Networks: A Survey from Algorithms to\n  Accelerators", "comments": "35 pages, 9 figures, 8 tables, 188 references", "journal-ref": "ACM Computing Surveys, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have exploded onto the machine learning scene in\nrecent years owing to their capability to model and learn from graph-structured\ndata. Such an ability has strong implications in a wide variety of fields whose\ndata is inherently relational, for which conventional neural networks do not\nperform well. Indeed, as recent reviews can attest, research in the area of\nGNNs has grown rapidly and has lead to the development of a variety of GNN\nalgorithm variants as well as to the exploration of groundbreaking applications\nin chemistry, neurology, electronics, or communication networks, among others.\nAt the current stage of research, however, the efficient processing of GNNs is\nstill an open challenge for several reasons. Besides of their novelty, GNNs are\nhard to compute due to their dependence on the input graph, their combination\nof dense and very sparse operations, or the need to scale to huge graphs in\nsome applications. In this context, this paper aims to make two main\ncontributions. On the one hand, a review of the field of GNNs is presented from\nthe perspective of computing. This includes a brief tutorial on the GNN\nfundamentals, an overview of the evolution of the field in the last decade, and\na summary of operations carried out in the multiple phases of different GNN\nalgorithm variants. On the other hand, an in-depth analysis of current software\nand hardware acceleration schemes is provided, from which a hardware-software,\ngraph-aware, and communication-centric vision for GNN accelerators is\ndistilled.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 22:29:27 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:46:45 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 09:39:35 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Abadal", "Sergi", ""], ["Jain", "Akshay", ""], ["Guirado", "Robert", ""], ["L\u00f3pez-Alonso", "Jorge", ""], ["Alarc\u00f3n", "Eduard", ""]]}, {"id": "2010.00239", "submitter": "Laercio Lima Pilla", "authors": "La\\'ercio Lima Pilla (ParSys - LRI)", "title": "Optimal Task Assignment to Heterogeneous Federated Learning Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning provides new opportunities for training machine learning\nmodels while respecting data privacy. This technique is based on heterogeneous\ndevices that work together to iteratively train a model while never sharing\ntheir own data. Given the synchronous nature of this training, the performance\nof Federated Learning systems is dictated by the slowest devices, also known as\nstragglers. In this paper, we investigate the problem of minimizing the\nduration of Federated Learning rounds by controlling how much data each device\nuses for training. We formulate this problem as a makespan minimization problem\nwith identical, independent, and atomic tasks that have to be assigned to\nheterogeneous resources with non-decreasing cost functions while respecting\nlower and upper limits of tasks per resource. Based on this formulation, we\npropose a polynomial-time algorithm named OLAR and prove that it provides\noptimal schedules. We evaluate OLAR in an extensive experimental evaluation\nusing simulation that includes comparisons to other algorithms from the state\nof the art and new extensions to them. Our results indicate that OLAR provides\noptimal solutions with a small execution time. They also show that the presence\nof lower and upper limits of tasks per resource erase any benefits that\nsuboptimal heuristics could provide in terms of algorithm execution time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 07:58:48 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Pilla", "La\u00e9rcio Lima", "", "ParSys - LRI"]]}, {"id": "2010.00283", "submitter": "Nick Brown", "authors": "Nick Brown, Brian Bainbridge, Ciar\\'an Beggan, Susan Macmillan,\n  William Brown, Brian Hamilton", "title": "Modelling the earth's geomagnetic environment on Cray machines using\n  PETSc and SLEPc", "comments": "This is the pre-peer reviewed version of the following article:\n  Modelling the earth's geomagnetic environment on Cray machines using PETSc\n  and SLEPc, which has been published in final form at\n  http://dx.doi.org/10.1002/cpe.5660", "journal-ref": null, "doi": "10.1002/cpe.5660", "report-no": null, "categories": "cs.DC physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The British Geological Survey's global geomagnetic model, Model of the\nEarth's Magnetic Environment (MEME), is an important tool for calculating the\nearth's magnetic field, which is continually in flux. Whilst the ability to\ncollect data from ground based observation sites and satellites has grown, the\nmemory bound nature of the code has proved a limitation in modelling problem\nsizes required by modern science. In this paper we describe work replacing the\nbespoke, sequential, eigen-solver with that of the SLEPc package for solving\nthe system of normal equations. This work had a dual purpose, to break through\nthe memory limit of the code, and thus support the modelling of much larger\nsystems, by supporting execution on distributed machines, and to improve\nperformance. But when adopting SLEPc it was not just the solving of the normal\nequations, but also fundamentally how we build and distribute the data\nstructures. We describe an approach for building symmetric matrices in a way\nthat provides good load balance and avoids the need for close co-ordination\nbetween processes or replication of work. We also study the memory bound nature\nof the code and combine detailed profiling with software cache prefetching to\nsignificantly optimise. Performance and scaling are explored on ARCHER, a Cray\nXC30, where we achieve a speed up for the solver of 294 times by replacing the\nmodel's bespoke approach with SLEPc. This work also provided the ability to\nmodel much larger system sizes, up to 100,000 model coefficients. Some of the\nchallenges of modelling systems of this large scale are explored, and\nmitigations including hybrid MPI+OpenMP along with the use of iterative solvers\nare also considered. The result of this work is a modern MEME model that is not\nonly capable of simulating problem sizes demanded by state of the art\ngeomagnetism but also acts as further evidence to the utility of the SLEPc\nlibary.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:21:48 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Brown", "Nick", ""], ["Bainbridge", "Brian", ""], ["Beggan", "Ciar\u00e1n", ""], ["Macmillan", "Susan", ""], ["Brown", "William", ""], ["Hamilton", "Brian", ""]]}, {"id": "2010.00289", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "Weighing up the new kid on the block: Impressions of using Vitis for HPC\n  software development", "comments": "Pre-print of Weighing up the new kid on the block: Impressions of\n  using Vitis for HPC software development, paper in 30th International\n  Conference on Field Programmable Logic and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of reconfigurable computing, and FPGAs in particular, has strong\npotential in the field of High Performance Computing (HPC). However the\ntraditionally high barrier to entry when it comes to programming this\ntechnology has, until now, precluded widespread adoption. To popularise\nreconfigurable computing with communities such as HPC, Xilinx have recently\nreleased the first version of Vitis, a platform aimed at making the programming\nof FPGAs much more a question of software development rather than hardware\ndesign. However a key question is how well this technology fulfils the aim, and\nwhether the tooling is mature enough such that software developers using FPGAs\nto accelerate their codes is now a more realistic proposition, or whether it\nsimply increases the convenience for existing experts. To examine this question\nwe use the Himeno benchmark as a vehicle for exploring the Vitis platform for\nbuilding, executing and optimising HPC codes, describing the different steps\nand potential pitfalls of the technology. The outcome of this exploration is a\ndemonstration that, whilst Vitis is an excellent step forwards and\nsignificantly lowers the barrier to entry in developing codes for FPGAs, it is\nnot a silver bullet and an underlying understanding of dataflow style\nalgorithmic design and appreciation of the architecture is still key to\nobtaining good performance on reconfigurable architectures.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:36:24 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2010.00330", "submitter": "Renan Souza", "authors": "Renan Souza, Leonardo G. Azevedo, V\\'itor Louren\\c{c}o, Elton Soares,\n  Raphael Thiago, Rafael Brand\\~ao, Daniel Civitarese, Emilio Vital Brazil,\n  Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco A.\n  S. Netto", "title": "Workflow Provenance in the Lifecycle of Scientific Machine Learning", "comments": "21 pages, 10 figures, Under review in a scientific journal since June\n  30th, 2020. arXiv admin note: text overlap with arXiv:1910.04223", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has already fundamentally changed several businesses.\nMore recently, it has also been profoundly impacting the computational science\nand engineering domains, like geoscience, climate science, and health science.\nIn these domains, users need to perform comprehensive data analyses combining\nscientific data and ML models to provide for critical requirements, such as\nreproducibility, model explainability, and experiment data understanding.\nHowever, scientific ML is multidisciplinary, heterogeneous, and affected by the\nphysical constraints of the domain, making such analyses even more challenging.\nIn this work, we leverage workflow provenance techniques to build a holistic\nview to support the lifecycle of scientific ML. We contribute with (i)\ncharacterization of the lifecycle and taxonomy for data analyses; (ii) design\nprinciples to build this view, with a W3C PROV compliant data representation\nand a reference system architecture; and (iii) lessons learned after an\nevaluation in an Oil & Gas case using an HPC cluster with 393 nodes and 946\nGPUs. The experiments show that the principles enable queries that integrate\ndomain semantics with ML models while keeping low overhead (<1%), high\nscalability, and an order of magnitude of query acceleration under certain\nworkloads against without our representation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 13:09:48 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Souza", "Renan", ""], ["Azevedo", "Leonardo G.", ""], ["Louren\u00e7o", "V\u00edtor", ""], ["Soares", "Elton", ""], ["Thiago", "Raphael", ""], ["Brand\u00e3o", "Rafael", ""], ["Civitarese", "Daniel", ""], ["Brazil", "Emilio Vital", ""], ["Moreno", "Marcio", ""], ["Valduriez", "Patrick", ""], ["Mattoso", "Marta", ""], ["Cerqueira", "Renato", ""], ["Netto", "Marco A. S.", ""]]}, {"id": "2010.00341", "submitter": "Michael Rodler", "authors": "Michael Rodler, Wenting Li, Ghassan O. Karame, Lucas Davi", "title": "EVMPatch: Timely and Automated Patching of Ethereum Smart Contracts", "comments": "A slightly shorter version of this paper will be published at USENIX\n  Security Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent attacks exploiting errors in smart contract code had devastating\nconsequences thereby questioning the benefits of this technology. It is\ncurrently highly challenging to fix errors and deploy a patched contract in\ntime. Instant patching is especially important since smart contracts are always\nonline due to the distributed nature of blockchain systems. They also manage\nconsiderable amounts of assets, which are at risk and often beyond recovery\nafter an attack. Existing solutions to upgrade smart contracts depend on manual\nand error-prone processes. This paper presents a framework, called EVMPatch, to\ninstantly and automatically patch faulty smart contracts. EVMPatch features a\nbytecode rewriting engine for the popular Ethereum blockchain, and\ntransparently/automatically rewrites common off-the-shelf contracts to\nupgradable contracts. The proof-of-concept implementation of EVMPatch\nautomatically hardens smart contracts that are vulnerable to integer\nover/underflows and access control errors, but can be easily extended to cover\nmore bug classes. Our extensive evaluation on 14,000 real-world (vulnerable)\ncontracts demonstrate that our approach successfully blocks attack transactions\nlaunched on these contracts, while keeping the intended functionality of the\ncontract intact. We perform a study with experienced software developers,\nshowing that EVMPatch is practical, and reduces the time for converting a given\nSolidity smart contract to an upgradable contract by 97.6 %, while ensuring\nfunctional equivalence to the original contract.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:19:01 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 11:38:36 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Rodler", "Michael", ""], ["Li", "Wenting", ""], ["Karame", "Ghassan O.", ""], ["Davi", "Lucas", ""]]}, {"id": "2010.00350", "submitter": "Busra Tegin", "authors": "Busra Tegin, Tolga M. Duman", "title": "Blind Federated Learning at the Wireless Edge with Low-Resolution ADC\n  and DAC", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study collaborative machine learning systems where a massive dataset is\ndistributed across independent workers which compute their local gradient\nestimates based on their own datasets. Workers send their estimates through a\nmultipath fading multiple access channel with orthogonal frequency division\nmultiplexing to mitigate the frequency selectivity of the channel. We assume\nthat there is no channel state information (CSI) at the workers, and the\nparameter server (PS) employs multiple antennas to align the received signals.\nTo reduce the power consumption and the hardware costs, we employ\ncomplex-valued low-resolution digital-to-analog converters (DACs) and\nanalog-to-digital converters (ADCs), at the transmitter and the receiver sides,\nrespectively, and study the effects of practical low-cost DACs and ADCs on the\nlearning performance. Our theoretical analysis shows that the impairments\ncaused by low-resolution DACs and ADCs, including those of one-bit DACs and\nADCs, do not prevent the convergence of the federated learning algorithm, and\nthe multipath channel effects vanish when a sufficient number of antennas are\nused at the PS. We also validate our theoretical results via simulations, and\ndemonstrate that using low-resolution, even one-bit, DACs and ADCs causes only\na slight decrease in the learning accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:31:41 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:19:26 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Tegin", "Busra", ""], ["Duman", "Tolga M.", ""]]}, {"id": "2010.00422", "submitter": "Rupert Nash", "authors": "Rupert W. Nash, Nick Brown, Michael R. Crusoe, Max Kontak", "title": "Supercomputing with MPI meets the Common Workflow Language standards: an\n  experience report", "comments": "Submitted to 15th Workshop on Workflows in Support of Large-Scale\n  Science (WORKS20)", "journal-ref": null, "doi": "10.1109/WORKS51914.2020.00008", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of standards-based workflows is still somewhat unusual by\nhigh-performance computing users. In this paper we describe the experience of\nusing the Common Workflow Language (CWL) standards to describe the execution,\nin parallel, of MPI-parallelised applications. In particular, we motivate and\ndescribe the simple extension to the specification which was required, as well\nas our implementation of this within the CWL reference runner. We discuss some\nof the unexpected benefits, such as simple use of HPC-oriented performance\nmeasurement tools, and CWL software requirements interfacing with HPC module\nsystems. We close with a request for comment from the community on how these\nfeatures could be adopted within versions of the CWL standards.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:11:52 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Nash", "Rupert W.", ""], ["Brown", "Nick", ""], ["Crusoe", "Michael R.", ""], ["Kontak", "Max", ""]]}, {"id": "2010.00501", "submitter": "Isabelly Rocha", "authors": "Isabelly Rocha, Nathaniel Morris, Lydia Y. Chen, Pascal Felber, Robert\n  Birke, Valerio Schiavoni", "title": "PipeTune: Pipeline Parallelism of Hyper and System Parameters Tuning for\n  Deep Learning Clusters", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN learning jobs are common in today's clusters due to the advances in AI\ndriven services such as machine translation and image recognition. The most\ncritical phase of these jobs for model performance and learning cost is the\ntuning of hyperparameters. Existing approaches make use of techniques such as\nearly stopping criteria to reduce the tuning impact on learning cost. However,\nthese strategies do not consider the impact that certain hyperparameters and\nsystems parameters have on training time. This paper presents PipeTune, a\nframework for DNN learning jobs that addresses the trade-offs between these two\ntypes of parameters. PipeTune takes advantage of the high parallelism and\nrecurring characteristics of such jobs to minimize the learning cost via a\npipelined simultaneous tuning of both hyper and system parameters. Our\nexperimental evaluation using three different types of workloads indicates that\nPipeTune achieves up to 22.6% reduction and 1.7x speed up on tuning and\ntraining time, respectively. PipeTune not only improves performance but also\nlowers energy consumption up to 29%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:47:55 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 09:21:43 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Rocha", "Isabelly", ""], ["Morris", "Nathaniel", ""], ["Chen", "Lydia Y.", ""], ["Felber", "Pascal", ""], ["Birke", "Robert", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2010.00753", "submitter": "Kate Donahue", "authors": "Kate Donahue and Jon Kleinberg", "title": "Model-sharing Games: Analyzing Federated Learning Under Voluntary\n  Participation", "comments": "Accepted at AAAI 2021. Supplemental code at\n  https://github.com/kpdonahue/model_sharing_games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a setting where agents, each with access to their own\ndata source, combine models from local data to create a global model. If agents\nare drawing their data from different distributions, though, federated learning\nmight produce a biased global model that is not optimal for each agent. This\nmeans that agents face a fundamental question: should they choose the global\nmodel or their local model? We show how this situation can be naturally\nanalyzed through the framework of coalitional game theory.\n  We propose the following game: there are heterogeneous players with different\nmodel parameters governing their data distribution and different amounts of\ndata they have noisily drawn from their own distribution. Each player's goal is\nto obtain a model with minimal expected mean squared error (MSE) on their own\ndistribution. They have a choice of fitting a model based solely on their own\ndata, or combining their learned parameters with those of some subset of the\nother players. Combining models reduces the variance component of their error\nthrough access to more data, but increases the bias because of the\nheterogeneity of distributions.\n  Here, we derive exact expected MSE values for problems in linear regression\nand mean estimation. We then analyze the resulting game in the framework of\nhedonic game theory; we study how players might divide into coalitions, where\neach set of players within a coalition jointly construct model(s). We analyze\nthree methods of federation, modeling differing degrees of customization. In\nuniform federation, the agents collectively produce a single model. In\ncoarse-grained federation, each agent can weight the global model together with\ntheir local model. In fine-grained federation, each agent can flexibly combine\nmodels from all other agents in the federation. For each method, we analyze the\nstable partitions of players into coalitions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 02:36:23 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 13:53:39 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 14:41:56 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Donahue", "Kate", ""], ["Kleinberg", "Jon", ""]]}, {"id": "2010.00754", "submitter": "Neil J. Gunther", "authors": "Neil J. Gunther", "title": "P = FS: Parallel is Just Fast Serial", "comments": "7 pages, 3 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that parallel processing with homogeneous processors is logically\nequivalent to fast serial processing. The reverse proposition can also be used\nto identify obscure opportunities for applying parallelism. To our knowledge,\nthis theorem has not been previously reported in the queueing theory\nliterature. A plausible explanation is offered for why this might be. The basic\nhomogeneous theorem is also extended to optimizing the latency of heterogenous\nparallel arrays.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 02:39:07 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gunther", "Neil J.", ""]]}, {"id": "2010.00911", "submitter": "Yotam Feldman", "authors": "Yotam M. Y. Feldman and Artem Khyzha and Constantin Enea and Adam\n  Morrison and Aleksandar Nanevski and Noam Rinetzky and Sharon Shoham", "title": "Proving Highly-Concurrent Traversals Correct", "comments": "Extended version of a paper appearing in OOPSLA'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern highly-concurrent search data structures, such as search trees, obtain\nmulti-core scalability and performance by having operations traverse the data\nstructure without any synchronization. As a result, however, these algorithms\nare notoriously difficult to prove linearizable, which requires identifying a\npoint in time in which the traversal's result is correct. The problem is that\ntraversing the data structure as it undergoes modifications leads to complex\nbehaviors, necessitating intricate reasoning about all interleavings of reads\nby traversals and writes mutating the data structure.\n  In this paper, we present a general proof technique for proving\nunsynchronized traversals correct in a significantly simpler manner, compared\nto typical concurrent reasoning and prior proof techniques. Our framework\nrelies only on sequential properties} of traversals and on a conceptually\nsimple and widely-applicable condition about the ways an algorithm's writes\nmutate the data structure. Establishing that a target data structure satisfies\nour condition requires only simple concurrent reasoning, without considering\ninteractions of writes and reads. This reasoning can be further simplified by\nusing our framework.\n  To demonstrate our technique, we apply it to prove several interesting and\nchallenging concurrent binary search trees: the logical-ordering AVL tree, the\nCitrus tree, and the full contention-friendly tree. Both the logical-ordering\ntree and the full contention-friendly tree are beyond the reach of previous\napproaches targeted at simplifying linearizability proofs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:35:27 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Feldman", "Yotam M. Y.", ""], ["Khyzha", "Artem", ""], ["Enea", "Constantin", ""], ["Morrison", "Adam", ""], ["Nanevski", "Aleksandar", ""], ["Rinetzky", "Noam", ""], ["Shoham", "Sharon", ""]]}, {"id": "2010.00914", "submitter": "Hao Chen", "authors": "Hao Chen, Yu Ye, Ming Xiao, Mikael Skoglund and H. Vincent Poor", "title": "Coded Stochastic ADMM for Decentralized Consensus Optimization with Edge\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data, including applications with high security requirements, are often\ncollected and stored on multiple heterogeneous devices, such as mobile devices,\ndrones and vehicles. Due to the limitations of communication costs and security\nrequirements, it is of paramount importance to extract information in a\ndecentralized manner instead of aggregating data to a fusion center. To train\nlarge-scale machine learning models, edge/fog computing is often leveraged as\nan alternative to centralized learning. We consider the problem of learning\nmodel parameters in a multi-agent system with data locally processed via\ndistributed edge nodes. A class of mini-batch stochastic alternating direction\nmethod of multipliers (ADMM) algorithms is explored to develop the distributed\nlearning model. To address two main critical challenges in distributed\nnetworks, i.e., communication bottleneck and straggler nodes (nodes with slow\nresponses), error-control-coding based stochastic incremental ADMM is\ninvestigated. Given an appropriate mini-batch size, we show that the mini-batch\nstochastic ADMM based method converges in a rate of $O(\\frac{1}{\\sqrt{k}})$,\nwhere $k$ denotes the number of iterations. Through numerical experiments, it\nis revealed that the proposed algorithm is communication-efficient, rapidly\nresponding and robust in the presence of straggler nodes compared with state of\nthe art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:41:59 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Chen", "Hao", ""], ["Ye", "Yu", ""], ["Xiao", "Ming", ""], ["Skoglund", "Mikael", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2010.01175", "submitter": "Lun Wang", "authors": "Lun Wang, Qi Pang, Shuai Wang and Dawn Song", "title": "Towards Bidirectional Protection in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior efforts in enhancing federated learning (FL) security fall into two\ncategories. At one end of the spectrum, some work uses secure aggregation\ntechniques to hide the individual client's updates and only reveal the\naggregated global update to a malicious server that strives to infer the\nclients' privacy from their updates. At the other end of the spectrum, some\nwork uses Byzantine-robust FL protocols to suppress the influence of malicious\nclients' updates. We present a federated learning protocol F2ED-LEARNING,\nwhich, for the first time, offers bidirectional defense to simultaneously\ncombat against the malicious centralized server and Byzantine malicious\nclients. To defend against Byzantine malicious clients, F2ED-LEARNING provides\ndimension-free estimation error by employing and calibrating a well-studied\nrobust mean estimator FilterL2. F2ED-LEARNING also leverages secure aggregation\nto protect clients from a malicious server. One key challenge of F2ED-LEARNING\nis to address the incompatibility between FilterL2 and secure aggregation\nschemes. Concretely, FilterL2 has to check the individual updates from clients\nwhereas secure aggregation hides those updates from the malicious server. To\nthis end, we propose a practical and highly effective solution to split the\nclients into shards, where F2ED-LEARNING securely aggregates each shard's\nupdate and launches FilterL2 on updates from different shards. The evaluation\nshows that F2ED-LEARNING consistently achieves optimal or close-to-optimal\nperformance and outperforms five secure FL protocols under five popular\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 19:37:02 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 20:24:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Lun", ""], ["Pang", "Qi", ""], ["Wang", "Shuai", ""], ["Song", "Dawn", ""]]}, {"id": "2010.01243", "submitter": "Yae Jee Cho", "authors": "Yae Jee Cho and Jianyu Wang and Gauri Joshi", "title": "Client Selection in Federated Learning: Convergence Analysis and\n  Power-of-Choice Selection Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed optimization paradigm that enables a\nlarge number of resource-limited client nodes to cooperatively train a model\nwithout data sharing. Several works have analyzed the convergence of federated\nlearning by accounting of data heterogeneity, communication and computation\nlimitations, and partial client participation. However, they assume unbiased\nclient participation, where clients are selected at random or in proportion of\ntheir data sizes. In this paper, we present the first convergence analysis of\nfederated optimization for biased client selection strategies, and quantify how\nthe selection bias affects convergence speed. We reveal that biasing client\nselection towards clients with higher local loss achieves faster error\nconvergence. Using this insight, we propose Power-of-Choice, a communication-\nand computation-efficient client selection framework that can flexibly span the\ntrade-off between convergence speed and solution bias. Our experiments\ndemonstrate that Power-of-Choice strategies converge up to 3 $\\times$ faster\nand give $10$% higher test accuracy than the baseline random selection.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 01:04:17 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Cho", "Yae Jee", ""], ["Wang", "Jianyu", ""], ["Joshi", "Gauri", ""]]}, {"id": "2010.01387", "submitter": "Balaji Arun", "authors": "Balaji Arun and Binoy Ravindran", "title": "DuoBFT: Resilience vs. Efficiency Trade-off in Byzantine Fault Tolerance", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents DuoBFT, a Byzantine fault-tolerant protocol that provides\ntwo features: Cheaper Resilience and Dual Fault Assumptions. First, by\nenhancing a fraction of replicas in the system with trusted components, DuoBFT\nenables commit decisions in the Hybrid fault model with quorums that are about\nhalf the size of regular byzantine quorums. Second, DuoBFT exposes both the\nHybrid and BFT fault models to the clients and lets them make commit decisions\nunder either of these models.\n  We first enable a notion called Flexible Quorums in the Hybrid fault model by\nrevisiting the quorum intersection requirements in hybrid protocols. We apply\nthe Flexible Quorums technique to MinBFT, a state of the art hybrid protocol,\nand consequently build on it to achieve DuoBFT.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 16:46:43 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Arun", "Balaji", ""], ["Ravindran", "Binoy", ""]]}, {"id": "2010.01415", "submitter": "Ben Wiederhake", "authors": "Christoph Lenzen and Ben Wiederhake", "title": "TRIX: Low-Skew Pulse Propagation for Fault-Tolerant Hardware", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vast majority of hardware architectures use a carefully timed reference\nsignal to clock their computational logic. However, standard distribution\nsolutions are not fault-tolerant. In this work, we present a simple grid\nstructure as a more reliable clock propagation method and study it by means of\nsimulation experiments. Fault-tolerance is achieved by forwarding clock pulses\non arrival of the second of three incoming signals from the previous layer.\n  A key question is how well neighboring grid nodes are synchronized, even\nwithout faults. Analyzing the clock skew under typical-case conditions is\nhighly challenging. Because the forwarding mechanism involves taking the\nmedian, standard probabilistic tools fail, even when modeling link delays just\nby unbiased coin flips.\n  Our statistical approach provides substantial evidence that this system\nperforms surprisingly well. Specifically, in an \"infinitely wide\" grid of\nheight~$H$, the delay at a pre-selected node exhibits a standard deviation of\n$O(H^{1/4})$ ($\\approx 2.7$ link delay uncertainties for $H=2000$) and skew\nbetween adjacent nodes of $o(\\log \\log H)$ ($\\approx 0.77$ link delay\nuncertainties for $H=2000$). We conclude that the proposed system is a very\npromising clock distribution method. This leads to the open problem of a\nstochastic explanation of the tight concentration of delays and skews. More\ngenerally, we believe that understanding our very simple abstraction of the\nsystem is of mathematical interest in its own right.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 19:27:49 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lenzen", "Christoph", ""], ["Wiederhake", "Ben", ""]]}, {"id": "2010.01423", "submitter": "Merav Parter", "authors": "Yael Hitron, Cameron Musco and Merav Parter", "title": "Spiking Neural Networks Through the Lens of Streaming Algorithms", "comments": "To appear in DISC'20, shorten abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of biological neural networks from the perspective of\nstreaming algorithms. Like computers, human brains suffer from memory\nlimitations which pose a significant obstacle when processing large scale and\ndynamically changing data. In computer science, these challenges are captured\nby the well-known streaming model, which can be traced back to Munro and\nPaterson `78 and has had significant impact in theory and beyond. In the\nclassical streaming setting, one must compute some function $f$ of a stream of\nupdates $\\mathcal{S} = \\{u_1,\\ldots,u_m\\}$, given restricted single-pass access\nto the stream. The primary complexity measure is the space used by the\nalgorithm.\n  We take the first steps towards understanding the connection between\nstreaming and neural algorithms. On the upper bound side, we design neural\nalgorithms based on known streaming algorithms for fundamental tasks, including\ndistinct elements, approximate median, heavy hitters, and more. The number of\nneurons in our neural solutions almost matches the space bounds of the\ncorresponding streaming algorithms. As a general algorithmic primitive, we show\nhow to implement the important streaming technique of linear sketching\nefficient in spiking neural networks. On the lower bound side, we give a\ngeneric reduction, showing that any space-efficient spiking neural network can\nbe simulated by a space-efficiently streaming algorithm. This reduction lets us\ntranslate streaming-space lower bounds into nearly matching neural-space lower\nbounds, establishing a close connection between these two models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 20:31:52 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hitron", "Yael", ""], ["Musco", "Cameron", ""], ["Parter", "Merav", ""]]}, {"id": "2010.01503", "submitter": "Merav Parter", "authors": "Merav Parter", "title": "Distributed Constructions of Dual-Failure Fault-Tolerant Distance\n  Preservers", "comments": "To appear in DISC'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault tolerant distance preservers (spanners) are sparse subgraphs that\npreserve (approximate) distances between given pairs of vertices under edge or\nvertex failures. So-far, these structures have been studied mainly from a\ncentralized viewpoint. Despite the fact fault tolerant preservers are mainly\nmotivated by the error-prone nature of distributed networks, not much is known\non the distributed computational aspects of these structures.\n  In this paper, we present distributed algorithms for constructing fault\ntolerant distance preservers and $+2$ additive spanners that are resilient to\nat most \\emph{two edge} faults. Prior to our work, the only non-trivial\nconstructions known were for the \\emph{single} fault and \\emph{single source}\nsetting by [Ghaffari and Parter SPAA'16].\n  Our key technical contribution is a distributed algorithm for computing\ndistance preservers w.r.t. a subset $S$ of source vertices, resilient to two\nedge faults. The output structure contains a BFS tree $BFS(s,G \\setminus\n\\{e_1,e_2\\})$ for every $s \\in S$ and every $e_1,e_2 \\in G$. The distributed\nconstruction of this structure is based on a delicate balance between the edge\ncongestion (formed by running multiple BFS trees simultaneously) and the\nsparsity of the output subgraph. No sublinear-round algorithms for constructing\nthese structures have been known before.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 08:04:07 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Parter", "Merav", ""]]}, {"id": "2010.01542", "submitter": "Nick Brown", "authors": "Ludovic Anthony Richard Capelli, Nick Brown, Jonathan Mark Bull", "title": "iPregel: Strategies to Deal with an Extreme Form of Irregularity in\n  Vertex-Centric Graph Processing", "comments": "Preprint of paper submitted to 9th Workshop on Irregular\n  Applications: Architectures and Algorithms (IA3)", "journal-ref": "In 2019 IEEE/ACM 9th Workshop on Irregular Applications:\n  Architectures and Algorithms (IA3) (pp. 45-50). IEEE", "doi": "10.1109/IA349570.2019.00013", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, the vertex-centric programming model has attracted\nsignificant attention in the world of graph processing, resulting in the\nemergence of a number of vertex-centric frameworks. Its simple programming\ninterface, where computation is expressed from a vertex point of view, offers\nboth ease of programming to the user and inherent parallelism for the\nunderlying framework to leverage. However, vertex-centric programs represent an\nextreme form of irregularity, both inter and intra core. This is because they\nexhibit a variety of challenges from a workload that may greatly vary across\nsupersteps, through fine-grain synchronisations, to memory accesses that are\nunpredictable both in terms of quantity and location. In this paper, we explore\nthree optimisations which address these irregular challenges; a hybrid combiner\ncarefully coupling lock-free and lock-based combinations, the partial\nexternalisation of vertex structures to improve locality and the shift to an\nedge-centric representation of the workload. The optimisations were integrated\ninto the iPregel vertex-centric framework, enabling the evaluation of each\noptimisation in the context of graph processing across three general purpose\nbenchmarks common in the vertex-centric community, each run on four publicly\navailable graphs covering all orders of magnitude from a million to a billion\nedges. The result of this work is a set of techniques which we believe not only\nprovide a significant performance improvement in vertex-centric graph\nprocessing, but are also applicable more generally to irregular applications.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:28:24 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Capelli", "Ludovic Anthony Richard", ""], ["Brown", "Nick", ""], ["Bull", "Jonathan Mark", ""]]}, {"id": "2010.01543", "submitter": "Nick Brown", "authors": "Gordon Gibb, Rupert Nash, Nick Brown, Bianca Prodan", "title": "The Technologies Required for Fusing HPC and Real-Time Data to Support\n  Urgent Computing", "comments": "Preprint of paper in 2019 IEEE/ACM HPC for Urgent Decision Making\n  (UrgentHPC)", "journal-ref": "In 2019 IEEE/ACM HPC for Urgent Decision Making (UrgentHPC) (pp.\n  24-34). IEEE", "doi": "10.1109/UrgentHPC49580.2019.00009", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of High Performance Computing (HPC) to compliment urgent decision\nmaking in the event of disasters is an important future potential use of\nsupercomputers. However, the usage modes involved are rather different from how\nHPC has been used traditionally. As such, there are many obstacles that need to\nbe overcome, not least the unbounded wait times in the batch system queues, to\nmake the use of HPC in disaster response practical. In this paper, we present\nhow the VESTEC project plans to overcome these issues and develop a working\nprototype of an urgent computing control system. We describe the requirements\nfor such a system and analyse the different technologies available that can be\nleveraged to successfully build such a system. We finally explore the design of\nthe VESTEC system and discuss ongoing challenges that need to be addressed to\nrealise a production level system.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:28:54 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gibb", "Gordon", ""], ["Nash", "Rupert", ""], ["Brown", "Nick", ""], ["Prodan", "Bianca", ""]]}, {"id": "2010.01545", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "Exploring the acceleration of the Met Office NERC Cloud model using\n  FPGAs", "comments": "Preprint of article in proceedings, ISC High Performance 2019.\n  Lecture Notes in Computer Science, vol 11887", "journal-ref": "In International Conference on High Performance Computing 2019 Jun\n  16 (pp. 567-586). Springer, Cham", "doi": "10.1007/978-3-030-34356-9_43", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Field Programmable Gate Arrays (FPGAs) to accelerate computational\nkernels has the potential to be of great benefit to scientific codes and the\nHPC community in general. With the recent developments in FPGA programming\ntechnology, the ability to port kernels is becoming far more accessible.\nHowever, to gain reasonable performance from this technology it is not enough\nto simple transfer a code onto the FPGA, instead the algorithm must be\nrethought and recast in a data-flow style to suit the target architecture. In\nthis paper we describe the porting, via HLS, of one of the most computationally\nintensive kernels of the Met Office NERC Cloud model (MONC), an atmospheric\nmodel used by climate and weather researchers, onto an FPGA. We describe in\ndetail the steps taken to adapt the algorithm to make it suitable for the\narchitecture and the impact this has on kernel performance. Using a PCIe\nmounted FPGA with on-board DRAM, we consider the integration on this kernel\nwithin a larger infrastructure and explore the performance characteristics of\nour approach in contrast to Intel CPUs that are popular in modern HPC machines,\nover problem sizes involving very large grids. The result of this work is an\nexperience report detailing the challenges faced and lessons learnt in porting\nthis complex computational kernel to FPGAs, as well as exploring the role that\nFPGAs can play and their fundamental limits in accelerating traditional HPC\nworkloads.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:30:04 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2010.01547", "submitter": "Nick Brown", "authors": "Nick Brown, David Dolman", "title": "It's all about data movement: Optimising FPGA data access to boost\n  performance", "comments": "Preprint of article in 2019 IEEE/ACM International Workshop on\n  Heterogeneous High-performance Reconfigurable Computing (H2RC)", "journal-ref": "In 2019 IEEE/ACM International Workshop on Heterogeneous\n  High-performance Reconfigurable Computing (H2RC) IEEE", "doi": "10.1109/H2RC49586.2019.00006", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of reconfigurable computing, and FPGAs in particular, to accelerate\ncomputational kernels has the potential to be of great benefit to scientific\ncodes and the HPC community in general. However, whilst recent advanced in FPGA\ntooling have made the physical act of programming reconfigurable architectures\nmuch more accessible, in order to gain good performance the entire algorithm\nmust be rethought and recast in a dataflow style. Reducing the cost of data\nmovement for all computing devices is critically important, and in this paper\nwe explore the most appropriate techniques for FPGAs. We do this by describing\nthe optimisation of an existing FPGA implementation of an atmospheric model's\nadvection scheme. By taking an FPGA code that was over four times slower than\nrunning on the CPU, mainly due to data movement overhead, we describe the\nprofiling and optimisation strategies adopted to significantly reduce the\nruntime and bring the performance of our FPGA kernels to a much more practical\nlevel for real-world use. The result of this work is a set of techniques,\nsteps, and lessons learnt that we have found significantly improves the\nperformance of FPGA based HPC codes and that others can adopt in their own\ncodes to achieve similar results.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:30:22 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Brown", "Nick", ""], ["Dolman", "David", ""]]}, {"id": "2010.01548", "submitter": "Nick Brown", "authors": "Maurice Jamieson, Nick Brown", "title": "High level programming abstractions for leveraging hierarchical memories\n  with micro-core architectures", "comments": "Accepted manuscript of paper in Journal of Parallel and Distributed\n  Computing 138", "journal-ref": "In Journal of Parallel and Distributed Computing. 2020 Apr\n  1;138:128-38", "doi": "10.1016/j.jpdc.2019.11.011", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-core architectures combine many low memory, low power computing cores\ntogether in a single package. These are attractive for use as accelerators but\ndue to limited on-chip memory and multiple levels of memory hierarchy, the way\nin which programmers offload kernels needs to be carefully considered. In this\npaper we use Python as a vehicle for exploring the semantics and abstractions\nof higher level programming languages to support the offloading of\ncomputational kernels to these devices. By moving to a pass by reference model,\nalong with leveraging memory kinds, we demonstrate the ability to easily and\nefficiently take advantage of multiple levels in the memory hierarchy, even\nones that are not directly accessible to the micro-cores. Using a machine\nlearning benchmark, we perform experiments on both Epiphany-III and MicroBlaze\nbased micro-cores, demonstrating the ability to compute with data sets of\narbitrarily large size. To provide context of our results, we explore the\nperformance and power efficiency of these technologies, demonstrating that\nwhilst these two micro-core technologies are competitive within their own\nembedded class of hardware, there is still a way to go to reach HPC class GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:31:12 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jamieson", "Maurice", ""], ["Brown", "Nick", ""]]}, {"id": "2010.01668", "submitter": "Pete Blacker Mr", "authors": "Peter Blacker, Christopher Paul Bridges, Simon Hadfield", "title": "Diagonal Memory Optimisation for Machine Learning on Micro-controllers", "comments": "10 Page, journal pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning spreads into more and more application areas, micro\ncontrollers and low power CPUs are increasingly being used to perform inference\nwith machine learning models. The capability to deploy onto these limited\nhardware targets is enabling machine learning models to be used across a\ndiverse range of new domains. Optimising the inference process on these targets\nposes different challenges from either desktop CPU or GPU implementations,\nwhere the small amounts of RAM available on these targets sets limits on size\nof models which can be executed. Analysis of the memory use patterns of eleven\nmachine learning models was performed. Memory load and store patterns were\nobserved using a modified version of the Valgrind debugging tool, identifying\nmemory areas holding values necessary for the calculation as inference\nprogressed. These analyses identified opportunities optimise the memory use of\nthese models by overlapping the input and output buffers of individual tensor\noperations. Three methods are presented which can calculate the safe overlap of\ninput and output buffers for tensor operations. Ranging from a computationally\nexpensive approach with the ability to operate on compiled layer operations, to\na versatile analytical solution which requires access to the original source\ncode of the layer. The diagonal memory optimisation technique is described and\nshown to achieve memory savings of up to 34.5% when applied to eleven common\nmodels. Micro-controller targets are identified where it is only possible to\ndeploy some models if diagonal memory optimisation is used.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:45:55 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 21:40:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Blacker", "Peter", ""], ["Bridges", "Christopher Paul", ""], ["Hadfield", "Simon", ""]]}, {"id": "2010.01941", "submitter": "Dixon Vimalajeewa", "authors": "Dixon Vimalajeewa, Subhasis Thakur, John Breslin, Donagh P. Berry,\n  Sasitharan Balasubramaniam", "title": "Block Chain and Internet of Nano-Things for Optimizing Chemical Sensing\n  in Smart Farming", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Internet of Things (IoT) with the Internet of Nano Things (IoNT)\ncan further expand decision making systems (DMS) to improve reliability as it\nprovides a new spectrum of more granular level data to make decisions. However,\ngrowing concerns such as data security, transparency and processing capability\nchallenge their use in real-world applications. DMS integrated with Block Chain\n(BC) technology can contribute immensely to overcome such challenges. The use\nof IoNT and IoT along with BC for making DMS has not yet been investigated.\nThis study proposes a BC-powered IoNT (BC-IoNT) system for sensing chemicals\nlevel in the context of farm management. This is a critical application for\nsmart farming, which aims to improve sustainable farm practices through\ncontrolled delivery of chemicals. BC-IoNT system includes a novel machine\nlearning model formed by using the Langmuir molecular binding model and the\nBayesian theory, and is used as a smart contract for sensing the level of the\nchemicals. A credit model is used to quantify the traceability and credibility\nof farms to determine if they are compliant with the chemical standards. The\naccuracy of detecting the chemicals of the distributed BC-IoNT approach was\n>90% and the centralized approach was <80%. Also, the efficiency of sensing the\nlevel of chemicals depends on the sampling frequency and variability in\nchemical level among farms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 12:12:33 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Vimalajeewa", "Dixon", ""], ["Thakur", "Subhasis", ""], ["Breslin", "John", ""], ["Berry", "Donagh P.", ""], ["Balasubramaniam", "Sasitharan", ""]]}, {"id": "2010.02043", "submitter": "Jannik Castenow", "authors": "Jannik Castenow, Peter Kling, Till Knollmann, Friedhelm Meyer auf der\n  Heide", "title": "A Discrete and Continuous Study of the Max-Chain-Formation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing robot formation problems seek a target formation of a certain\n\\emph{minimal} and, thus, efficient structure. Examples include the Gathering\nand the Chain-Formation problem. In this work, we study formation problems that\ntry to reach a \\emph{maximal} structure, supporting for example an efficient\ncoverage in exploration scenarios. A recent example is the NASA Shapeshifter\nproject, which describes how the robots form a relay chain along which gathered\ndata from extraterrestrial cave explorations may be sent to a home base.\n  As a first step towards understanding such maximization tasks, we introduce\nand study the Max-Chain-Formation problem, where $n$ robots are ordered along a\nwinding, potentially self-intersecting chain and must form a connected,\nstraight line of maximal length connecting its two endpoints. We propose and\nanalyze strategies in a discrete and in a continuous time model. In the\ndiscrete case, we give a complete analysis if all robots are initially\ncollinear, showing that the worst-case time to reach an\n$\\varepsilon$-approximation is upper bounded by $\\mathcal{O}(n^2 \\cdot \\log\n(n/\\varepsilon))$ and lower bounded by $\\Omega(n^2 \\cdot~\\log\n(1/\\varepsilon))$. If one endpoint of the chain remains stationary, this result\ncan be extended to the non-collinear case. If both endpoints move, we identify\na family of instances whose runtime is unbounded. For the continuous model, we\ngive a strategy with an optimal runtime bound of $\\Theta(n)$. Avoiding an\nunbounded runtime similar to the discrete case relies crucially on a\ncounter-intuitive aspect of the strategy: slowing down the endpoints while all\nother robots move at full speed. Surprisingly, we can show that a similar trick\ndoes not work in the discrete model.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:20:53 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Castenow", "Jannik", ""], ["Kling", "Peter", ""], ["Knollmann", "Till", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "2010.02124", "submitter": "Karl Palmskog", "authors": "Elaine Li, Karl Palmskog, Mircea Sebe, Grigore Ro\\c{s}u", "title": "Specification of the Giskard Consensus Protocol", "comments": "15 pages plus 2 pages as appendix, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Giskard consensus protocol is used to validate transactions and\ncomputations in the PlatON network. In this paper, we provide a rigorous\nspecification of Giskard, suitable to serve as a reference in protocol\nimplementation and in formal verification. Using our specification, we prove\nthat the protocol guarantees several notable safety properties.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:10:33 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Li", "Elaine", ""], ["Palmskog", "Karl", ""], ["Sebe", "Mircea", ""], ["Ro\u015fu", "Grigore", ""]]}, {"id": "2010.02147", "submitter": "Pei Peng", "authors": "Pei Peng, Emina Soljanin and Philip Whiting", "title": "Diversity/Parallelism Trade-off in Distributed Systems with Redundancy", "comments": "Described several open problems and outlined future directions, added\n  references, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As numerous machine learning and other algorithms increase in complexity and\ndata requirements, distributed computing becomes necessary to satisfy the\ngrowing computational and storage demands, because it enables parallel\nexecution of smaller tasks that make up a large computing job. However, random\nfluctuations in task service times lead to straggling tasks with long execution\ntimes. Redundancy, in the form of task replication and erasure coding, provides\ndiversity that allows a job to be completed when only a subset of redundant\ntasks is executed, thus removing the dependency on the straggling tasks. In\nsituations of constrained resources (here a fixed number of parallel servers),\nincreasing redundancy reduces the available resources for parallelism. In this\npaper, we characterize the diversity vs. parallelism trade-off and identify the\noptimal strategy, among replication, coding and splitting, which minimizes the\nexpected job completion time. We consider three common service time\ndistributions and establish three models that describe scaling of these\ndistributions with the task size. We find that different distributions with\ndifferent scaling models operate optimally at different levels of redundancy,\nand thus may require very different code rates.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:52:46 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 22:04:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Peng", "Pei", ""], ["Soljanin", "Emina", ""], ["Whiting", "Philip", ""]]}, {"id": "2010.02164", "submitter": "Kevin Yang", "authors": "Kevin Yang, Violet Yao, John DeNero, Dan Klein", "title": "A Streaming Approach For Efficient Batched Beam Search", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient batching strategy for variable-length decoding on GPU\narchitectures. During decoding, when candidates terminate or are pruned\naccording to heuristics, our streaming approach periodically \"refills\" the\nbatch before proceeding with a selected subset of candidates. We apply our\nmethod to variable-width beam search on a state-of-the-art machine translation\nmodel. Our method decreases runtime by up to 71% compared to a fixed-width beam\nsearch baseline and 17% compared to a variable-width baseline, while matching\nbaselines' BLEU. Finally, experiments show that our method can speed up\ndecoding in other domains, such as semantic and syntactic parsing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:13:34 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 22:22:27 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yang", "Kevin", ""], ["Yao", "Violet", ""], ["DeNero", "John", ""], ["Klein", "Dan", ""]]}, {"id": "2010.02208", "submitter": "Simon Bliudze", "authors": "Anton B. Ivanov, Simon Bliudze", "title": "Robust Software Development for University-Built Satellites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellites and other complex systems now become more and more software\ndependent. Even nanosatellites have complexity that can be compared to\nscientific instruments launched to Mars. COTS components and subsystems may now\nbe purchased to support payload development. On the contrary, the software has\nto be adapted to the new payload and, consequently, hardware architecture\nselected for the satellite. There is not a rigorous and robust way to design\nsoftware for CubeSats or small satellites yet. In this paper, we will briefly\nreview some existing systems and present our approach, which based on\nBehaviour-Interaction-Priority (BIP) framework. We will describe our experience\nin implementing fight software simulation and testing in the Swiss CubETH\nCubeSat project. We will conclude with lessons learned and future utilization\nof BIP for hardware testing and simulation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 18:44:05 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ivanov", "Anton B.", ""], ["Bliudze", "Simon", ""]]}, {"id": "2010.02348", "submitter": "Gaurav Menghani", "authors": "Gaurav Menghani, Anil Harwani, Yash Londhe, Kalpesh Kagresha", "title": "An Easy-to-Use-and-Deploy Grid Computing Framework", "comments": "Presented at HiPC 2010 Student Research Symposium\n  (https://hipc.org/hipc2010/studentsymposium.php)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A few grid-computing tools are available for public use. However, such\nsystems are usually quite complex and require several man-months to set up. In\ncase the user wishes to set-up an ad-hoc grid in a small span of time, such\ntools cannot be used. Moreover, the complex services they provide, like,\nreliable file transfer, extra layers of security etc., act as an overhead to\nperformance in case the network is small and reliable.\n  In this paper we describe the structure of our grid-computing framework,\nwhich can be implemented and used, easily on a moderate sized network.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:44:33 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Menghani", "Gaurav", ""], ["Harwani", "Anil", ""], ["Londhe", "Yash", ""], ["Kagresha", "Kalpesh", ""]]}, {"id": "2010.02361", "submitter": "E. Wes Bethel", "authors": "E. Wes Bethel and David Camp and Talita Perciano and Colleen Heinemann", "title": "Performance Analysis of Traditional and Data-Parallel Primitive\n  Implementations of Visualization and Analysis Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurements of absolute runtime are useful as a summary of performance when\nstudying parallel visualization and analysis methods on computational platforms\nof increasing concurrency and complexity. We can obtain even more insights by\nmeasuring and examining more detailed measures from hardware performance\ncounters, such as the number of instructions executed by an algorithm\nimplemented in a particular way, the amount of data moved to/from memory,\nmemory hierarchy utilization levels via cache hit/miss ratios, and so forth.\nThis work focuses on performance analysis on modern multi-core platforms of\nthree different visualization and analysis kernels that are implemented in\ndifferent ways: one is \"traditional\", using combinations of C++ and VTK, and\nthe other uses a data-parallel approach using VTK-m. Our performance study\nconsists of measurement and reporting of several different hardware performance\ncounters on two different multi-core CPU platforms. The results reveal\ninteresting performance differences between these two different approaches for\nimplementing these kernels, results that would not be apparent using runtime as\nthe only metric.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:07:24 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Bethel", "E. Wes", ""], ["Camp", "David", ""], ["Perciano", "Talita", ""], ["Heinemann", "Colleen", ""]]}, {"id": "2010.02372", "submitter": "Filip Hanzely", "authors": "Filip Hanzely, Slavom\\'ir Hanzely, Samuel Horv\\'ath, Peter Richt\\'arik", "title": "Lower Bounds and Optimal Algorithms for Personalized Federated Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the optimization formulation of personalized\nfederated learning recently introduced by Hanzely and Richt\\'arik (2020) which\nwas shown to give an alternative explanation to the workings of local {\\tt SGD}\nmethods. Our first contribution is establishing the first lower bounds for this\nformulation, for both the communication complexity and the local oracle\ncomplexity. Our second contribution is the design of several optimal methods\nmatching these lower bounds in almost all regimes. These are the first provably\noptimal methods for personalized federated learning. Our optimal methods\ninclude an accelerated variant of {\\tt FedProx}, and an accelerated\nvariance-reduced version of {\\tt FedAvg}/Local {\\tt SGD}. We demonstrate the\npractical superiority of our methods through extensive numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:29:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Hanzely", "Filip", ""], ["Hanzely", "Slavom\u00edr", ""], ["Horv\u00e1th", "Samuel", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2010.02436", "submitter": "Mikhail Nesterenko", "authors": "Joseph Oglio, Kendric Hood, Gokarna Sharma and Mikhail Nesterenko", "title": "Byzantine Geoconsensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and investigate the consensus problem for a set of $N$ processes\nembedded on the $d$-dimensional plane, $d\\geq 2$, which we call the {\\em\ngeoconsensus} problem. The processes have unique coordinates and can\ncommunicate with each other through oral messages. In contrast to the\nliterature where processes are individually considered Byzantine, it is\nconsidered that all processes covered by a finite-size convex fault area $F$\nare Byzantine and there may be one or more processes in a fault area. Similarly\nas in the literature where correct processes do not know which processes are\nByzantine, it is assumed that the fault area location is not known to the\ncorrect processes. We prove that the geoconsensus is impossible if all\nprocesses may be covered by at most three areas where one is a fault area.\n  Considering the 2-dimensional embedding, on the constructive side, for $M\n\\geq 1$ fault areas $F$ of arbitrary shape with diameter $D$, we present a\nconsensus algorithm that tolerates $f\\leq N-(2M+1)$ Byzantine processes\nprovided that there are $9M+3$ processes with pairwise distance between them\ngreater than $D$. For square $F$ with side $\\ell$, we provide a consensus\nalgorithm that lifts this pairwise distance requirement and tolerates $f\\leq\nN-15M$ Byzantine processes given that all processes are covered by at least\n$22M$ axis aligned squares of the same size as $F$. For a circular $F$ of\ndiameter $\\ell$, this algorithm tolerates $f\\leq N-57M$ Byzantine processes if\nall processes are covered by at least $85M$ circles. We then extend these\nresults to various size combinations of fault and non-fault areas as well as\n$d$-dimensional process embeddings, $d\\geq 3$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 02:31:18 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Oglio", "Joseph", ""], ["Hood", "Kendric", ""], ["Sharma", "Gokarna", ""], ["Nesterenko", "Mikhail", ""]]}, {"id": "2010.02486", "submitter": "Manish Kumar", "authors": "Yefim Dinitz, Shlomi Dolev, Manish Kumar", "title": "Local Deal-Agreement Based Monotonic Distributed Algorithms for Load\n  Balancing in General Graphs", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer networks, participants may cooperate in processing tasks, so that\nloads are balanced among them. We present local distributed algorithms that\n(repeatedly) use local imbalance criteria to transfer loads concurrently across\nthe participants of the system, iterating until all loads are balanced. Our\nalgorithms are based on a short local deal-agreement communication of\nproposal/deal, based on the neighborhood loads. They converge monotonically,\nalways providing a better state as the execution progresses. Besides, our\nalgorithms avoid making loads temporarily negative. Thus, they may be\nconsidered anytime ones, in the sense that they can be stopped at any time\nduring the execution. We show that our synchronous load balancing algorithms\nachieve $\\epsilon$-Balanced state for the continuous setting and 1-Balanced\nstate for the discrete setting in all graphs, within $O(n D \\log(n\nK/\\epsilon))$ and $O(n D \\log(n K/D) + n D^2)$ time, respectively, where $n$ is\nthe number of nodes, $K$ is the initial discrepancy, $D$ is the graph diameter,\nand $\\epsilon$ is the final discrepancy. Our other monotonic synchronous and\nasynchronous algorithms for the discrete setting are generalizations of the\nfirst presented algorithms, where load balancing is performed concurrently with\nmore than one neighbor. These algorithms arrive at a 1-Balanced state in time\n$O(n K^2)$ in general graphs, but have a potential to be faster as the loads\nare balanced among all neighbors, rather than with only one; we describe a\nscenario that demonstrates the potential for a fast ($O(1)$) convergence. Our\nasynchronous algorithm avoids the need to wait for the slowest participants'\nactivity prior to making the next load balancing steps as synchronous settings\nrestrict. We also introduce a self-stabilizing version of our asynchronous\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:27:52 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Dinitz", "Yefim", ""], ["Dolev", "Shlomi", ""], ["Kumar", "Manish", ""]]}, {"id": "2010.02769", "submitter": "Claude Tadonki Dr. HDR", "authors": "Claude Tadonki", "title": "Conceptual and Technical Challenges for High Performance Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Performance Computing (HPC) aims at providing reasonably fast computing\nsolutions to scientific and real life problems. The advent of multicore\narchitectures is noticeable in the HPC history, because it has brought the\nunderlying parallel programming concept into common considerations. At a larger\nscale, there is a keen interest in building or hosting frontline\nsupercomputers; the Top500 ranking is a nice illustration of this (implicit)\nracing. Supercomputers, as well as ordinary computers, have fallen in price for\nyears while gaining processing power. We clearly see that, what commonly\nsprings up in mind when it comes to HPC is computer capability. However, when\ngoing deeper into the topic, especially on large-scale problems, it appears\nthat the processing speed by itself is no longer sufficient. Indeed, the real\nconcern of HPC users is the time-to-output. Thus, we need to study each\nimportant aspect in the critical path between inputs and outputs. The first\nstep is clearly the method, which is a conjunction of modelling with specific\nconsiderations (hypothesis, simplifications, constraints, to name a few) and a\ncorresponding algorithm, which could be numerical and/or non numerical. Then\ncomes the topic of programming, which should yield a skillful mapping of the\nalgorithm onto HPC machines. Based on multicore processors, probably enhanced\nwith acceleration units, current generation of supercomputers is rated to\ndeliver an increasing peak performance, the Exascale era being the current\nhorizon. However, getting a high fraction of the available peak performance is\nmore and more difficult. The Design of an efficient code that scales well on a\nsupercomputer is a non-trivial task. The present note will discuss the\naforementioned points, interleaved with commented contributions from the\nliterature and our personal views.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:38:38 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Tadonki", "Claude", ""]]}, {"id": "2010.02838", "submitter": "Shagun Sodhani", "authors": "Shagun Sodhani, Olivier Delalleau, Mahmoud Assran, Koustuv Sinha,\n  Nicolas Ballas, Michael Rabbat", "title": "A Closer Look at Codistillation for Distributed Training", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Codistillation has been proposed as a mechanism to share knowledge among\nconcurrently trained models by encouraging them to represent the same function\nthrough an auxiliary loss. This contrasts with the more commonly used\nfully-synchronous data-parallel stochastic gradient descent methods, where\ndifferent model replicas average their gradients (or parameters) at every\niteration and thus maintain identical parameters. We investigate codistillation\nin a distributed training setup, complementing previous work which focused on\nextremely large batch sizes. Surprisingly, we find that even at moderate batch\nsizes, models trained with codistillation can perform as well as models trained\nwith synchronous data-parallel methods, despite using a much weaker\nsynchronization mechanism. These findings hold across a range of batch sizes\nand learning rate schedules, as well as different kinds of models and datasets.\nObtaining this level of accuracy, however, requires properly accounting for the\nregularization effect of codistillation, which we highlight through several\nempirical observations. Overall, this work contributes to a better\nunderstanding of codistillation and how to best take advantage of it in a\ndistributed computing environment.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:01:34 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 01:51:36 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sodhani", "Shagun", ""], ["Delalleau", "Olivier", ""], ["Assran", "Mahmoud", ""], ["Sinha", "Koustuv", ""], ["Ballas", "Nicolas", ""], ["Rabbat", "Michael", ""]]}, {"id": "2010.03012", "submitter": "Bita Hasheminezhad", "authors": "Bita Hasheminezhad, Shahrzad Shirzad, Nanmiao Wu, Patrick Diehl,\n  Hannes Schulz, Hartmut Kaiser", "title": "Towards a Scalable and Distributed Infrastructure for Deep Learning\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1109/DLS51937.2020.00008", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent scaling up approaches to training deep neural networks have\nproven to be effective, the computational intensity of large and complex\nmodels, as well as the availability of large-scale datasets, require deep\nlearning frameworks to utilize scaling out techniques. Parallelization\napproaches and distribution requirements are not considered in the preliminary\ndesigns of most available distributed deep learning frameworks, and most of\nthem still are not able to perform effective and efficient fine-grained\ninter-node communication. We present Phylanx that has the potential to\nalleviate these shortcomings. Phylanx offers a productivity-oriented frontend\nwhere user Python code is translated to a futurized execution tree that can be\nexecuted efficiently on multiple nodes using the C++ standard library for\nparallelism and concurrency (HPX), leveraging fine-grained threading and an\nactive messaging task-based runtime system.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 20:38:47 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 00:18:24 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Hasheminezhad", "Bita", ""], ["Shirzad", "Shahrzad", ""], ["Wu", "Nanmiao", ""], ["Diehl", "Patrick", ""], ["Schulz", "Hannes", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "2010.03035", "submitter": "Le Xu", "authors": "Le Xu, Shivaram Venkataraman, Indranil Gupta, Luo Mai, and Rahul\n  Potharaju", "title": "Move Fast and Meet Deadlines: Fine-grained Real-time Stream Processing\n  with Cameo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource provisioning in multi-tenant stream processing systems faces the\ndual challenges of keeping resource utilization high (without\nover-provisioning), and ensuring performance isolation. In our common\nproduction use cases, where streaming workloads have to meet latency targets\nand avoid breaching service-level agreements, existing solutions are incapable\nof handling the wide variability of user needs. Our framework called Cameo uses\nfine-grained stream processing (inspired by actor computation models), and is\nable to provide high resource utilization while meeting latency targets. Cameo\ndynamically calculates and propagates priorities of events based on user\nlatency targets and query semantics. Experiments on Microsoft Azure show that\ncompared to state-of-the-art, the Cameo framework: i) reduces query latency by\n2.7X in single tenant settings, ii) reduces query latency by 4.6X in\nmulti-tenant scenarios, and iii) weathers transient spikes of workload.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:09:19 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Xu", "Le", ""], ["Venkataraman", "Shivaram", ""], ["Gupta", "Indranil", ""], ["Mai", "Luo", ""], ["Potharaju", "Rahul", ""]]}, {"id": "2010.03144", "submitter": "Sihuan Li", "authors": "Sihuan Li, Sheng Di, Kai Zhao, Xin Liang, Zizhong Chen, Franck\n  Cappello", "title": "SDC Resilient Error-bounded Lossy Compressor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy compression is one of the most important strategies to resolve the big\nscience data issue, however, little work was done to make it resilient against\nsilent data corruptions (SDC). In fact, SDC is becoming non-negligible because\nof exa-scale computing demand on complex scientific simulations with vast\nvolume of data being produced or in some particular instruments/devices (such\nas interplanetary space probe) that need to transfer large amount of data in an\nerror-prone environment. In this paper, we propose an SDC resilient\nerror-bounded lossy compressor upon the SZ compression framework. Specifically,\nwe adopt a new independent-block-wise model that decomposes the entire dataset\ninto many independent sub-blocks to compress. Then, we design and implement a\nseries of error detection/correction strategies based on SZ. We are the first\nto extend algorithm-based fault tolerance (ABFT) to lossy compression. Our\nproposed solution incurs negligible execution overhead without soft errors. It\nkeeps the correctness of decompressed data still bounded within user's\nrequirement with a very limited degradation of compression ratios upon soft\nerrors.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 03:59:25 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Li", "Sihuan", ""], ["Di", "Sheng", ""], ["Zhao", "Kai", ""], ["Liang", "Xin", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "2010.03166", "submitter": "Hongkuan Zhou", "authors": "Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal\n  Kannan and Viktor Prasanna", "title": "Accurate, Efficient and Scalable Training of Graph Neural Networks", "comments": "43 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1810.11899", "journal-ref": "Journal of Parallel and Distributed Computing, Volume 147, January\n  2021, Pages 166-183", "doi": "10.1016/j.jpdc.2020.08.011", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are powerful deep learning models to generate\nnode embeddings on graphs. When applying deep GNNs on large graphs, it is still\nchallenging to perform training in an efficient and scalable way. We propose a\nnovel parallel training framework. Through sampling small subgraphs as\nminibatches, we reduce training workload by orders of magnitude compared with\nstate-of-the-art minibatch methods. We then parallelize the key computation\nsteps on tightly-coupled shared memory systems. For graph sampling, we exploit\nparallelism within and across sampler instances, and propose an efficient data\nstructure supporting concurrent accesses from samplers. The parallel sampler\ntheoretically achieves near-linear speedup with respect to number of processing\nunits. For feature propagation within subgraphs, we improve cache utilization\nand reduce DRAM traffic by data partitioning. Our partitioning is a\n2-approximation strategy for minimizing the communication cost compared to the\noptimal. We further develop a runtime scheduler to reorder the training\noperations and adjust the minibatch subgraphs to improve parallel performance.\nFinally, we generalize the above parallelization strategies to support multiple\ntypes of GNN models and graph samplers. The proposed training outperforms the\nstate-of-the-art in scalability, efficiency and accuracy simultaneously. On a\n40-core Xeon platform, we achieve 60x speedup (with AVX) in the sampling step\nand 20x speedup in the feature propagation step, compared to the serial\nimplementation. Our algorithm enables fast training of deeper GNNs, as\ndemonstrated by orders of magnitude speedup compared to the Tensorflow\nimplementation. We open-source our code at\nhttps://github.com/GraphSAINT/GraphSAINT.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:06:23 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zeng", "Hanqing", ""], ["Zhou", "Hongkuan", ""], ["Srivastava", "Ajitesh", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor", ""]]}, {"id": "2010.03289", "submitter": "Ke Yang", "authors": "Hao Chen, Ke Yang, Stefano Giovanni Rizzo, Giovanna Vantini, Phillip\n  Taylor, Xiaosong Ma, Sanjay Chawla", "title": "QarSUMO: A Parallel, Congestion-optimized Traffic Simulator", "comments": "Fix a typo in Figure 9", "journal-ref": null, "doi": "10.1145/3397536.3422274", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic simulators are important tools for tasks such as urban planning and\ntransportation management. Microscopic simulators allow per-vehicle movement\nsimulation, but require longer simulation time. The simulation overhead is\nexacerbated when there is traffic congestion and most vehicles move slowly.\nThis in particular hurts the productivity of emerging urban computing studies\nbased on reinforcement learning, where traffic simulations are heavily and\nrepeatedly used for designing policies to optimize traffic related tasks.\n  In this paper, we develop QarSUMO, a parallel, congestion-optimized version\nof the popular SUMO open-source traffic simulator. QarSUMO performs high-level\nparallelization on top of SUMO, to utilize powerful multi-core servers and\nenables future extension to multi-node parallel simulation if necessary. The\nproposed design, while partly sacrificing speedup, makes QarSUMO compatible\nwith future SUMO improvements. We further contribute such an improvement by\nmodifying the SUMO simulation engine for congestion scenarios where the update\ncomputation of consecutive and slow-moving vehicles can be simplified.\n  We evaluate QarSUMO with both real-world and synthetic road network and\ntraffic data, and examine its execution time as well as simulation accuracy\nrelative to the original, sequential SUMO.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 09:10:42 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 20:19:59 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chen", "Hao", ""], ["Yang", "Ke", ""], ["Rizzo", "Stefano Giovanni", ""], ["Vantini", "Giovanna", ""], ["Taylor", "Phillip", ""], ["Ma", "Xiaosong", ""], ["Chawla", "Sanjay", ""]]}, {"id": "2010.03536", "submitter": "Salvatore Di Girolamo", "authors": "Salvatore Di Girolamo, Andreas Kurth, Alexandru Calotoiu, Thomas Benz,\n  Timo Schneider, Jakub Ber\\'anek, Luca Benini, Torsten Hoefler", "title": "PsPIN: A high-performance low-power architecture for flexible in-network\n  compute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of offloading data and control tasks to the network is becoming\nincreasingly important, especially if we consider the faster growth of network\nspeed when compared to CPU frequencies. In-network compute alleviates the host\nCPU load by running tasks directly in the network, enabling additional\ncomputation/communication overlap and potentially improving overall application\nperformance. However, sustaining bandwidths provided by next-generation\nnetworks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model\nfor in-NIC compute, where users specify handler functions that are executed on\nthe NIC, for each incoming packet belonging to a given message or flow. It\nenables a CUDA-like acceleration, where the NIC is equipped with lightweight\nprocessing elements that process network packets in parallel. We investigate\nthe architectural specialties that a sPIN NIC should provide to enable\nhigh-performance, low-power, and flexible packet processing. We introduce\nPsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V\narchitecture and designed according to the identified architectural\nspecialties. We investigate the performance of PsPIN with cycle-accurate\nsimulations, showing that it can process packets at 400 Gbit/s for several use\ncases, introducing minimal latencies (26 ns for 64 B packets) and occupying a\ntotal area of 18.5 mm 2 (22 nm FDSOI).\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:32:57 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 10:28:40 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 09:31:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Di Girolamo", "Salvatore", ""], ["Kurth", "Andreas", ""], ["Calotoiu", "Alexandru", ""], ["Benz", "Thomas", ""], ["Schneider", "Timo", ""], ["Ber\u00e1nek", "Jakub", ""], ["Benini", "Luca", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.03660", "submitter": "Robert Schreiber", "authors": "Kamil Rocki, Dirk Van Essendelft, Ilya Sharapov, Robert Schreiber,\n  Michael Morrison, Vladimir Kibardin, Andrey Portnoy, Jean Francois Dietiker,\n  Madhava Syamlal, and Michael James", "title": "Fast Stencil-Code Computation on a Wafer-Scale Processor", "comments": "SC 20: The International Conference for High Performance Computing,\n  Networking, Storage, and Analysis, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of CPU-based and GPU-based systems is often low for PDE\ncodes, where large, sparse, and often structured systems of linear equations\nmust be solved. Iterative solvers are limited by data movement, both between\ncaches and memory and between nodes. Here we describe the solution of such\nsystems of equations on the Cerebras Systems CS-1, a wafer-scale processor that\nhas the memory bandwidth and communication latency to perform well. We achieve\n0.86 PFLOPS on a single wafer-scale system for the solution by BiCGStab of a\nlinear system arising from a 7-point finite difference stencil on a 600 X 595 X\n1536 mesh, achieving about one third of the machine's peak performance. We\nexplain the system, its architecture and programming, and its performance on\nthis problem and related problems. We discuss issues of memory capacity and\nfloating point precision. We outline plans to extend this work towards full\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 21:23:07 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Rocki", "Kamil", ""], ["Van Essendelft", "Dirk", ""], ["Sharapov", "Ilya", ""], ["Schreiber", "Robert", ""], ["Morrison", "Michael", ""], ["Kibardin", "Vladimir", ""], ["Portnoy", "Andrey", ""], ["Dietiker", "Jean Francois", ""], ["Syamlal", "Madhava", ""], ["James", "Michael", ""]]}, {"id": "2010.03805", "submitter": "Giulia Cisotto", "authors": "Sergio Martiradonna, Giulia Cisotto, Gennaro Boggia, Giuseppe Piro,\n  Lorenzo Vangelista, and Stefano Tomasin", "title": "Cascaded WLAN-FWA Networking and Computing Architecture for Pervasive\n  In-Home Healthcare", "comments": null, "journal-ref": null, "doi": "10.1109/MWC.001.2000330", "report-no": null, "categories": "cs.NI cs.CY cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pervasive healthcare is a promising assisted-living solution for chronic\npatients. However, current cutting-edge communication technologies are not able\nto strictly meet the requirements of these applications, especially in the case\nof life-threatening events. To bridge this gap, this paper proposes a new\narchitecture to support indoor healthcare monitoring, with a focus on epileptic\npatients. Several novel elements are introduced. The first element is the\ncascading of a WLAN and a cellular network, where IEEE 802.11ax is used for the\nwireless local area network to collect physiological and environmental data\nin-home and 5G-enabled Fixed Wireless Access links transfer them to a remote\nhospital. The second element is the extension of the network slicing concept to\nthe WLAN, and the introduction of two new slice types to support both regular\nmonitoring and emergency handling. Moreover, the inclusion of local computing\ncapabilities at the WLAN router, together with a mobile edge computing\nresource, represents a further architectural enhancement. Local computation is\nrequired to trigger not only health-related alarms, but also the network\nslicing change in case of emergency: in fact, proper radio resource scheduling\nis necessary for the cascaded networks to handle healthcare traffic together\nwith other promiscuous everyday communication services. Numerical results\ndemonstrate the effectiveness of the proposed approach while highlighting the\nperformance gain achieved with respect to baseline solutions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:16:00 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Martiradonna", "Sergio", ""], ["Cisotto", "Giulia", ""], ["Boggia", "Gennaro", ""], ["Piro", "Giuseppe", ""], ["Vangelista", "Lorenzo", ""], ["Tomasin", "Stefano", ""]]}, {"id": "2010.03869", "submitter": "Shaan Mathur", "authors": "Shaan Mathur and Rafail Ostrovsky", "title": "A Combinatorial Characterization of Self-Stabilizing Population\n  Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We fully characterize self-stabilizing functions in population protocols for\ncomplete interaction graphs. In particular, we investigate self-stabilization\nin systems of $n$ finite state agents in which a malicious scheduler selects an\narbitrary sequence of pairwise interactions under a global fairness condition.\nWe show a necessary and sufficient condition for self-stabilization.\nSpecifically we show that functions without certain set-theoretic conditions\nare impossible to compute in a self-stabilizing manner. Our main contribution\nis in the converse, where we construct a self-stabilizing protocol for all\nother functions that meet this characterization. Our positive construction uses\nDickson's Lemma to develop the notion of the root set, a concept that turns out\nto fundamentally characterize self-stabilization in this model. We believe it\nmay lend to characterizing self-stabilization in more general models as well.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:50:45 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 20:56:56 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Mathur", "Shaan", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "2010.03960", "submitter": "K. R. Chowdhary", "authors": "K. R. Chowdhary, Rajendra Purohit", "title": "Diagnosing Distributed Systems through Log Data Analysis", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-based analysis and trouble-shooting has remained prevalent and\ncommonly used approach for centralized and time-haring systems. However, for\nparallel and distributed systems where happen-before relations are not directly\navailable between the events, it become a challenge to fully depend on\nlog-based analysis in such instances. This article attempts to provide\nsolutions using log-based performance analysis of centralized system, and\ndemonstrates the results and their effectiveness, as well presents the\nchallenges and proposes solutions for performance analysis in distributed and\nparallel systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 13:29:20 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Chowdhary", "K. R.", ""], ["Purohit", "Rajendra", ""]]}, {"id": "2010.04106", "submitter": "Patrick Diehl", "authors": "Nikunj Gupta and Steve R. Brandt and Bibek Wagle and Nanmiao and\n  Alireza Kheirkhahan and Patrick Diehl and Hartmut Kaiser and Felix W. Baumann", "title": "Deploying a Task-based Runtime System on Raspberry Pi Clusters", "comments": null, "journal-ref": null, "doi": "10.1109/ESPM251964.2020.00007", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arm technology is becoming increasingly important in HPC. Recently, Fugaku,\nan \\arm-based system, was awarded the number one place in the Top500 list.\nRaspberry Pis provide an inexpensive platform to become familiar with this\narchitecture. However, Pis can also be useful on their own. Here we describe\nour efforts to configure and benchmark the use of a Raspberry Pi cluster with\nthe HPX/Phylanx platform (normally intended for use with HPC applications) and\ndocument the lessons we learned. First, we highlight the required changes in\nthe configuration of the Pi to gain performance. Second, we explore how limited\nmemory bandwidth limits the use of all cores in our shared memory benchmarks.\nThird, we evaluate whether low network bandwidth affects distributed\nperformance. Fourth, we discuss the power consumption and the resulting\ntrade-off in cost of operation and performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 16:43:53 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 22:47:31 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gupta", "Nikunj", ""], ["Brandt", "Steve R.", ""], ["Wagle", "Bibek", ""], ["Nanmiao", "", ""], ["Kheirkhahan", "Alireza", ""], ["Diehl", "Patrick", ""], ["Kaiser", "Hartmut", ""], ["Baumann", "Felix W.", ""]]}, {"id": "2010.04312", "submitter": "Eric Rozner", "authors": "Erika Hunhoff, Shazal Irshad, Vijay Thurimella, Ali Tariq, Eric Rozner", "title": "Proactive Serverless Function Resource Management", "comments": "To appear in the Sixth International Workshop on Serverless Computing\n  (WoSC6) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new primitive to serverless language runtimes called\nfreshen. With freshen, developers or providers specify functionality to perform\nbefore a given function executes. This proactive technique allows for overheads\nassociated with serverless functions to be mitigated at execution time, which\nimproves function responsiveness. We show various predictive opportunities\nexist to run freshen within reasonable time windows. A high-level design and\nimplementation are described, along with preliminary results to show the\npotential benefits of our scheme.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 00:50:18 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Hunhoff", "Erika", ""], ["Irshad", "Shazal", ""], ["Thurimella", "Vijay", ""], ["Tariq", "Ali", ""], ["Rozner", "Eric", ""]]}, {"id": "2010.04321", "submitter": "Alexandra DeLucia", "authors": "Alexandra DeLucia, Elisabeth Moore", "title": "Analyzing HPC Support Tickets: Experience and Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance computing (HPC) user support teams are the first line of\ndefense against large-scale problems, as they are often the first to learn of\nproblems reported by users. Developing tools to better assist support teams in\nsolving user problems and tracking issue trends is critical for maintaining\nsystem health. Our work examines the Los Alamos National Laboratory HPC Consult\nTeam's user support ticketing system and develops proof of concept tools to\nautomate tasks such as category assignment and similar ticket recommendation.\nWe also generate new categories for reporting and discuss ideas to improve\nfuture ticketing systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 01:40:47 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["DeLucia", "Alexandra", ""], ["Moore", "Elisabeth", ""]]}, {"id": "2010.04400", "submitter": "Quentin Bramas", "authors": "Quentin Bramas (ICube, UNISTRA, ICUBE-R\\'eseaux), Anissa Lamani\n  (ICube, UNISTRA, ICUBE-R\\'eseaux), S\\'ebastien Tixeuil (SU, NPA)", "title": "Stand Up Indulgent Rendezvous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two mobile oblivious robots that evolve in a continuous Euclidean\nspace. We require the two robots to solve the rendezvous problem (meeting in\nfinite time at the same location, not known beforehand) despite the possibility\nthat one of those robots crashes unpredictably. The rendezvous is stand up\nindulgent in the sense that when a crash occurs, the correct robot must still\nmeet the crashed robot on its last position. We characterize the system\nassumptions that enable problem solvability, and present a series of algorithms\nthat solve the problem for the possible cases.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:12:36 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bramas", "Quentin", "", "ICube, UNISTRA, ICUBE-R\u00e9seaux"], ["Lamani", "Anissa", "", "ICube, UNISTRA, ICUBE-R\u00e9seaux"], ["Tixeuil", "S\u00e9bastien", "", "SU, NPA"]]}, {"id": "2010.04406", "submitter": "Haikun Liu", "authors": "Haikun Liu, Di Chen, Hai Jin, Xiaofei Liao, Bingsheng He, Kan Hu, Yu\n  Zhang", "title": "A Survey of Non-Volatile Main Memory Technologies: State-of-the-Arts,\n  Practices, and Future Directions", "comments": "36 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Volatile Main Memories (NVMMs) have recently emerged as promising\ntechnologies for future memory systems. Generally, NVMMs have many desirable\nproperties such as high density, byte-addressability, non-volatility, low cost,\nand energy efficiency, at the expense of high write latency, high write power\nconsumption and limited write endurance. NVMMs have become a competitive\nalternative of Dynamic Random Access Memory (DRAM), and will fundamentally\nchange the landscape of memory systems. They bring many research opportunities\nas well as challenges on system architectural designs, memory management in\noperating systems (OSes), and programming models for hybrid memory systems. In\nthis article, we first revisit the landscape of emerging NVMM technologies, and\nthen survey the state-of-the-art studies of NVMM technologies. We classify\nthose studies with a taxonomy according to different dimensions such as memory\narchitectures, data persistence, performance improvement, energy saving, and\nwear leveling. Second, to demonstrate the best practices in building NVMM\nsystems, we introduce our recent work of hybrid memory system designs from the\ndimensions of architectures, systems, and applications. At last, we present our\nvision of future research directions of NVMMs and shed some light on design\nchallenges and opportunities.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:29:53 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Liu", "Haikun", ""], ["Chen", "Di", ""], ["Jin", "Hai", ""], ["Liao", "Xiaofei", ""], ["He", "Bingsheng", ""], ["Hu", "Kan", ""], ["Zhang", "Yu", ""]]}, {"id": "2010.04414", "submitter": "Guixiang Ma", "authors": "Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin\n  Nazarian, Paul Bogdan", "title": "A Vertex Cut based Framework for Load Balancing and Parallelism\n  Optimization in Multi-core Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level applications, such as machine learning, are evolving from simple\nmodels based on multilayer perceptrons for simple image recognition to much\ndeeper and more complex neural networks for self-driving vehicle control\nsystems.The rapid increase in the consumption of memory and computational\nresources by these models demands the use of multi-core parallel systems to\nscale the execution of the complex emerging applications that depend on them.\nHowever, parallel programs running on high-performance computers often suffer\nfrom data communication bottlenecks, limited memory bandwidth, and\nsynchronization overhead due to irregular critical sections. In this paper, we\npropose a framework to reduce the data communication and improve the\nscalability and performance of these applications in multi-core systems. We\ndesign a vertex cut framework for partitioning LLVM IR graphs into clusters\nwhile taking into consideration the data communication and workload balance\namong clusters. First, we construct LLVM graphs by compiling high-level\nprograms into LLVM IR, instrumenting code to obtain the execution order of\nbasic blocks and the execution time for each memory operation, and analyze data\ndependencies in dynamic LLVM traces. Next, we formulate the problem as Weight\nBalanced $p$-way Vertex Cut, and propose a generic and flexible framework,\nwherein four different greedy algorithms are proposed for solving this problem.\nLastly, we propose a memory-centric run-time mapping of the linear time\ncomplexity to map clusters generated from the vertex cut algorithms onto a\nmulti-core platform. We conclude that our best algorithm, WB-Libra, provides\nperformance improvements of 1.56x and 1.86x over existing state-of-the-art\napproaches for 8 and 1024 clusters running on a multi-core platform,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:54:28 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ma", "Guixiang", ""], ["Xiao", "Yao", ""], ["Willke", "Theodore L.", ""], ["Ahmed", "Nesreen K.", ""], ["Nazarian", "Shahin", ""], ["Bogdan", "Paul", ""]]}, {"id": "2010.04424", "submitter": "Jannik Castenow", "authors": "Jannik Castenow, Jonas Harbig, Daniel Jung, Till Knollmann, Friedhelm\n  Meyer auf der Heide", "title": "Gathering in Linear Time: A Closed Chain of Disoriented & Luminous\n  Robots with Limited Visibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the following question related to the Gathering problem\nof $n$ autonomous, mobile robots in the Euclidean plane: Is it possible to\nsolve Gathering of robots that do not agree on any axis of their coordinate\nsystems (disoriented robots) and see other robots only up to a constant\ndistance (limited visibility) in $o(n^2)$ fully synchronous rounds? The best\nknown algorithm that solves Gathering of disoriented robots with limited\nvisibility assuming oblivious robots needs $\\Theta(n^2)$ rounds [SPAA'11]. The\nlower bound for this algorithm even holds in a simplified closed chain model,\nwhere each robot has exactly two neighbors and the chain connections form a\ncycle. The only existing algorithms achieving a linear number of rounds for\ndisoriented robots assume robots that are located on a two dimensional grid\n[IPDPS'16] and [SPAA'16]. Both algorithms make use of locally visible lights\n(the LUMINOUS model).\n  In this work, we show for the closed chain model, that $n$ disoriented robots\nwith limited visibility in the Euclidean plane can be gathered in\n$\\Theta\\left(n\\right)$ rounds assuming the LUMINOUS model. The lights are used\nto initiate and perform so-called runs along the chain. For the start of such\nruns, locally unique robots need to be determined. In contrast to the grid\n[IPDPS'16], this is not possible in every configuration in the Euclidean plane.\nBased on the theory of isogonal polygons by Gr\\\"unbaum, we identify the class\nof isogonal configurations in which no such locally unique robots can be\nidentified. Our solution combines two algorithms: The first one gathers\nisogonal configurations; it works without any lights. The second one works for\nnon-isogonal configurations; it identifies locally unique robots to start runs,\nusing a constant number of lights. Interleaving these algorithms solves the\nGathering problem in $\\mathcal{O}(n)$ rounds.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:16:14 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Castenow", "Jannik", ""], ["Harbig", "Jonas", ""], ["Jung", "Daniel", ""], ["Knollmann", "Till", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "2010.04700", "submitter": "Jamie Alnasir PhD", "authors": "Jamie J Alnasir", "title": "Distributed Computing in a Pandemic: A Review of Technologies Available\n  for Tackling COVID-19", "comments": "20 pages (14 excl. refs), 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The current COVID-19 global pandemic caused by the SARS-CoV-2 betacoronavirus\nhas resulted in over a million deaths and is having a grave socio-economic\nimpact, hence there is an urgency to find solutions to key research challenges.\nMuch of this COVID-19 research depends on distributed computing. In this\narticle, I review distributed architectures -- various types of clusters, grids\nand clouds -- that can be leveraged to perform these tasks at scale, at\nhigh-throughput, with a high degree of parallelism, and which can also be used\nto work collaboratively. High-performance computing (HPC) clusters will be used\nto carry out much of this work. Several bigdata processing tasks used in\nreducing the spread of SARS-CoV-2 require high-throughput approaches, and a\nvariety of tools, which Hadoop and Spark offer, even using commodity hardware.\nExtremely large-scale COVID-19 research has also utilised some of the world's\nfastest supercomputers, such as IBM's SUMMIT -- for ensemble docking\nhigh-throughput screening against SARS-CoV-2 targets for drug-repurposing, and\nhigh-throughput gene analysis -- and Sentinel, an XPE-Cray based system used to\nexplore natural products. Grid computing has facilitated the formation of the\nworld's first Exascale grid computer. This has accelerated COVID-19 research in\nmolecular dynamics simulations of SARS-CoV-2 spike protein interactions through\nmassively-parallel computation and was performed with over 1 million volunteer\ncomputing devices using the Folding@home platform. Grids and clouds both can\nalso be used for international collaboration by enabling access to important\ndatasets and providing services that allow researchers to focus on research\nrather than on time-consuming data-management tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:40:11 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 18:37:15 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 15:10:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Alnasir", "Jamie J", ""]]}, {"id": "2010.04773", "submitter": "Anup Das", "authors": "Twisha Titirsha and Anup Das", "title": "Thermal-Aware Compilation of Spiking Neural Networks to Neuromorphic\n  Hardware", "comments": "Accepted for publication at LCPC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hardware implementation of neuromorphic computing can significantly improve\nperformance and energy efficiency of machine learning tasks implemented with\nspiking neural networks (SNNs), making these hardware platforms particularly\nsuitable for embedded systems and other energy-constrained environments. We\nobserve that the long bitlines and wordlines in a crossbar of the hardware\ncreate significant current variations when propagating spikes through its\nsynaptic elements, which are typically designed with non-volatile memory (NVM).\nSuch current variations create a thermal gradient within each crossbar of the\nhardware, depending on the machine learning workload and the mapping of neurons\nand synapses of the workload to these crossbars. \\mr{This thermal gradient\nbecomes significant at scaled technology nodes and it increases the leakage\npower in the hardware leading to an increase in the energy consumption.} We\npropose a novel technique to map neurons and synapses of SNN-based machine\nlearning workloads to neuromorphic hardware. We make two novel contributions.\nFirst, we formulate a detailed thermal model for a crossbar in a neuromorphic\nhardware incorporating workload dependency, where the temperature of each\nNVM-based synaptic cell is computed considering the thermal contributions from\nits neighboring cells. Second, we incorporate this thermal model in the mapping\nof neurons and synapses of SNN-based workloads using a hill-climbing heuristic.\nThe objective is to reduce the thermal gradient in crossbars. We evaluate our\nneuron and synapse mapping technique using 10 machine learning workloads for a\nstate-of-the-art neuromorphic hardware. We demonstrate an average 11.4K\nreduction in the average temperature of each crossbar in the hardware, leading\nto a 52% reduction in the leakage power consumption (11% lower total energy\nconsumption) compared to a performance-oriented SNN mapping technique.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:29:14 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 21:26:34 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Titirsha", "Twisha", ""], ["Das", "Anup", ""]]}, {"id": "2010.04828", "submitter": "Feng Li", "authors": "Feng Li, Dali Wang, Feng Yan, Fengguang Song", "title": "ElasticBroker: Combining HPC with Cloud to Provide Realtime Insights\n  into Simulations", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large-scale scientific simulations, it is expensive to store raw\nsimulation results to perform post-analysis. To minimize expensive I/O,\n\"in-situ\" analysis is often used, where analysis applications are tightly\ncoupled with scientific simulations and can access and process the simulation\nresults in memory. Increasingly, scientific domains employ Big Data approaches\nto analyze simulations for scientific discoveries. However, it remains a\nchallenge to organize, transform, and transport data at scale between the two\nsemantically different ecosystems (HPC and Cloud systems). In an effort to\naddress these challenges, we design and implement the ElasticBroker software\nframework, which bridges HPC and Cloud applications to form an \"in-situ\"\nscientific workflow. Instead of writing simulation results to parallel file\nsystems, ElasticBroker performs data filtering, aggregation, and format\nconversions to close the gap between an HPC ecosystem and a distinct Cloud\necosystem. To achieve this goal, ElasticBroker reorganizes simulation snapshots\ninto continuous data streams and send them to the Cloud. In the Cloud, we\ndeploy a distributed stream processing service to perform online data analysis.\nIn our experiments, we use ElasticBroker to setup and execute a cross-ecosystem\nscientific workflow, which consists of a parallel computational fluid dynamics\n(CFD) simulation running on a supercomputer, and a parallel dynamic mode\ndecomposition (DMD) analysis application running in a Cloud computing platform.\nOur results show that running scientific workflows consisting of decoupled HPC\nand Big Data jobs in their native environments with ElasticBroker, can achieve\nhigh quality of service, good scalability, and provide high-quality analytics\nfor ongoing simulations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 22:12:55 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 02:55:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Feng", ""], ["Wang", "Dali", ""], ["Yan", "Feng", ""], ["Song", "Fengguang", ""]]}, {"id": "2010.04902", "submitter": "Konstantinos Konstantinidis", "authors": "Konstantinos Konstantinidis, Aditya Ramamoorthy", "title": "ByzShield: An Efficient and Robust System for Distributed Training", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of large scale models on distributed clusters is a critical\ncomponent of the machine learning pipeline. However, this training can easily\nbe made to fail if some workers behave in an adversarial (Byzantine) fashion\nwhereby they return arbitrary results to the parameter server (PS). A plethora\nof existing papers consider a variety of attack models and propose robust\naggregation and/or computational redundancy to alleviate the effects of these\nattacks. In this work we consider an omniscient attack model where the\nadversary has full knowledge about the gradient computation assignments of the\nworkers and can choose to attack (up to) any q out of K worker nodes to induce\nmaximal damage. Our redundancy-based method ByzShield leverages the properties\nof bipartite expander graphs for the assignment of tasks to workers; this helps\nto effectively mitigate the effect of the Byzantine behavior. Specifically, we\ndemonstrate an upper bound on the worst case fraction of corrupted gradients\nbased on the eigenvalues of our constructions which are based on mutually\northogonal Latin squares and Ramanujan graphs. Our numerical experiments\nindicate over a 36% reduction on average in the fraction of corrupted gradients\ncompared to the state of the art. Likewise, our experiments on training\nfollowed by image classification on the CIFAR-10 dataset show that ByzShield\nhas on average a 20% advantage in accuracy under the most sophisticated\nattacks. ByzShield also tolerates a much larger fraction of adversarial nodes\ncompared to prior work.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 04:41:53 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 05:12:44 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Konstantinidis", "Konstantinos", ""], ["Ramamoorthy", "Aditya", ""]]}, {"id": "2010.04938", "submitter": "Alexander Kryukov", "authors": "Alexander Kryukov, Igor Bychkov, Elena Korosteleva, Andrey Mikhailov,\n  and Minh-Duc Nguyen", "title": "AstroDS -- A Distributed Storage for Astrophysics of Cosmic Rays.\n  Current Status", "comments": "Proceedings of the 4th International Workshop on Data Life Cycle in\n  Physics (DLC2020), Moscow, Russia, June 8-10, 2020", "journal-ref": "CEUR Workshop Proceedings, Vol-2679, pages 32-42, 2020", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the processing of scientific data in astroparticle physics is\nbased on various distributed technologies, the most common of which are Grid\nand cloud computing. The most frequently discussed approaches are focused on\nlarge and even very large scientific experiments, such as Cherenkov Telescope\nArray. We, by contrast, offer a solution designed for small to medium\nexperiments such as TAIGA. In such experiments, as a rule, historically\ndeveloped specific data processing methods and specialized software are used.\nWe have specifically designed a distributed (cloud) data storage for\nastroparticle physics data collaboration in medium-sized experiments. In this\narticle, we discuss the current state of our work using the example of the\nTAIGA and CASCADE experiments. A feature of our approach is that we provide our\nusers with scientific data in the form to which they are accustomed to in\neveryday work on local resources.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 08:22:43 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kryukov", "Alexander", ""], ["Bychkov", "Igor", ""], ["Korosteleva", "Elena", ""], ["Mikhailov", "Andrey", ""], ["Nguyen", "Minh-Duc", ""]]}, {"id": "2010.05031", "submitter": "Lucia Pons", "authors": "Lucia Pons, Josu\\'e Feliu, Jos\\'e Puche, Chaoyi Huang, Salvador Petit,\n  Julio Pons, Mar\\'ia E. G\\'omez, Julio Sahuquillo", "title": "Understanding Cloud Workloads Performance in a Production like\n  Environment", "comments": "16 pages, 17 figures. Submitted to Journal of Parallel and\n  Distributed Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding inter-VM interference is of paramount importance to provide a\nsound knowledge and understand where performance degradation comes from in the\ncurrent public cloud. With this aim, this paper devises a workload taxonomy\nthat classifies applications according to how the major system resources affect\ntheir performance (e.g., tail latency) as a function of the level of load\n(e.g., QPS). After that, we present three main studies addressing three major\nconcerns to improve the cloud performance: impact of the level of load on\nperformance, impact of hyper-threading on performance, and impact of limiting\nthe major system resources (e.g., last level cache) on performance. In all\nthese studies we identified important findings that we hope help cloud\nproviders improve their system utilization.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 15:31:49 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Pons", "Lucia", ""], ["Feliu", "Josu\u00e9", ""], ["Puche", "Jos\u00e9", ""], ["Huang", "Chaoyi", ""], ["Petit", "Salvador", ""], ["Pons", "Julio", ""], ["G\u00f3mez", "Mar\u00eda E.", ""], ["Sahuquillo", "Julio", ""]]}, {"id": "2010.05037", "submitter": "Samuel Hsia", "authors": "Samuel Hsia, Udit Gupta, Mark Wilkening, Carole-Jean Wu, Gu-Yeon Wei\n  and David Brooks", "title": "Cross-Stack Workload Characterization of Deep Recommendation Systems", "comments": "Published in 2020 IEEE International Symposium on Workload\n  Characterization (IISWC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based recommendation systems form the backbone of most\npersonalized cloud services. Though the computer architecture community has\nrecently started to take notice of deep recommendation inference, the resulting\nsolutions have taken wildly different approaches - ranging from near memory\nprocessing to at-scale optimizations. To better design future hardware systems\nfor deep recommendation inference, we must first systematically examine and\ncharacterize the underlying systems-level impact of design decisions across the\ndifferent levels of the execution stack. In this paper, we characterize eight\nindustry-representative deep recommendation models at three different levels of\nthe execution stack: algorithms and software, systems platforms, and hardware\nmicroarchitectures. Through this cross-stack characterization, we first show\nthat system deployment choices (i.e., CPUs or GPUs, batch size granularity) can\ngive us up to 15x speedup. To better understand the bottlenecks for further\noptimization, we look at both software operator usage breakdown and CPU\nfrontend and backend microarchitectural inefficiencies. Finally, we model the\ncorrelation between key algorithmic model architecture features and hardware\nbottlenecks, revealing the absence of a single dominant algorithmic component\nbehind each hardware bottleneck.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 16:11:43 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hsia", "Samuel", ""], ["Gupta", "Udit", ""], ["Wilkening", "Mark", ""], ["Wu", "Carole-Jean", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""]]}, {"id": "2010.05098", "submitter": "Matthew Ding", "authors": "Matthew Ding", "title": "Relay Protocol for Approximate Byzantine Consensus", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate byzantine consensus is a fundamental problem of distributed\ncomputing. This paper presents a novel algorithm for approximate byzantine\nconsensus, called Relay-ABC. The algorithm allows machines to achieve\napproximate consensus to arbitrary exactness in the presence of byzantine\nfailures. The algorithm relies on the usage of a relayed messaging system and\nsigned messages with unforgeable signatures that are unique to each node. The\nuse of signatures and relays allows the strict necessary network conditions of\ntraditional approximate byzantine consensus algorithms to be circumvented.\n  We also provide theoretical guarantees of validity and convergence for\nRelay-ABC. To do this, we utilize the idea that the iteration of states in the\nnetwork can be modeled by a sequence of transition matrices. We extend previous\nmethods, which use transition matrices to prove ABC convergence, by having each\nstate vector model not just one iteration, but a set of $D$ iterations, where\n$D$ is a diameter property of the graph. This allows us to accurately model the\ndelays of messages inherent within the relay system.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 21:37:06 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 19:07:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ding", "Matthew", ""]]}, {"id": "2010.05187", "submitter": "Kostas Kolomvatsos", "authors": "Panagiotis Oikonomou, Anna Karanika, Christos Anagnostopoulos, Kostas\n  Kolomvatsos", "title": "On the Road from Edge Computing to the Edge Mesh", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, we are witnessing the advent of the Internet of Things (EC) with\nnumerous devices performing interactions between them or with end users. The\nhuge number of devices leads to huge volumes of collected data that demand the\nappropriate processing. The 'legacy' approach is to rely on Cloud where\nincreased computational resources can be adopted to realize any processing.\nHowever, even if the communication with the Cloud back end lasts for some\nseconds there are cases where problems in the network or the need for\nsupporting real time applications require a reduced latency in the provision of\nresponses/outcomes. Edge Computing (EC) comes into the scene as the 'solver' of\nthe latency problem (and not only). Any processing can be performed close to\ndata sources, i.e., at EC nodes having direct connection with IoT devices.\nHence, an ecosystem of processing nodes can be present at the edge of the\nnetwork giving the opportunity to apply novel services upon the collected data.\nVarious challenges should be met before we talk about a fully automated\necosystem where EC nodes can cooperate or understand the status of them and the\nenvironment to be capable of efficiently serving end users or applications. In\nthis paper, we perform a survey of the relevant research activities targeting\nto support the vision of Edge Mesh (EM), i.e., a 'cover' of intelligence upon\nthe EC infrastructure. We present all the parts of the EC/EM framework starting\nfrom the necessary hardware and discussing research outcomes in every aspect of\nEC nodes functioning. We present technologies and theories adopted for data,\ntasks and resource management while discussing how (deep) machine learning and\noptimization techniques are adopted to solve various problems. Our aim is to\nprovide a starting point for novel research to conclude efficient\nservices/applications opening up the path to realize the future EC form.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 08:00:10 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Oikonomou", "Panagiotis", ""], ["Karanika", "Anna", ""], ["Anagnostopoulos", "Christos", ""], ["Kolomvatsos", "Kostas", ""]]}, {"id": "2010.05222", "submitter": "Xiang An", "authors": "Xiang An, Xuhan Zhu, Yang Xiao, Lan Wu, Ming Zhang, Yuan Gao, Bin Qin,\n  Debing Zhang, Ying Fu", "title": "Partial FC: Training 10 Million Identities on a Single Machine", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has been an active and vital topic among computer vision\ncommunity for a long time. Previous researches mainly focus on loss functions\nused for facial feature extraction network, among which the improvements of\nsoftmax-based loss functions greatly promote the performance of face\nrecognition. However, the contradiction between the drastically increasing\nnumber of face identities and the shortage of GPU memories is gradually\nbecoming irreconcilable. In this paper, we thoroughly analyze the optimization\ngoal of softmax-based loss functions and the difficulty of training massive\nidentities. We find that the importance of negative classes in softmax function\nin face representation learning is not as high as we previously thought. The\nexperiment demonstrates no loss of accuracy when training with only 10\\%\nrandomly sampled classes for the softmax-based loss functions, compared with\ntraining with full classes using state-of-the-art models on mainstream\nbenchmarks. We also implement a very efficient distributed sampling algorithm,\ntaking into account model accuracy and training efficiency, which uses only\neight NVIDIA RTX2080Ti to complete classification tasks with tens of millions\nof identities. The code of this paper has been made available\nhttps://github.com/deepinsight/insightface/tree/master/recognition/partial_fc.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 11:15:26 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 05:25:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["An", "Xiang", ""], ["Zhu", "Xuhan", ""], ["Xiao", "Yang", ""], ["Wu", "Lan", ""], ["Zhang", "Ming", ""], ["Gao", "Yuan", ""], ["Qin", "Bin", ""], ["Zhang", "Debing", ""], ["Fu", "Ying", ""]]}, {"id": "2010.05233", "submitter": "Shaoshan Liu", "authors": "Jinliang Xie, Jie Tang, Shaoshan Liu", "title": "An Energy-Efficient High Definition Map Data Distribution Mechanism for\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Driving is now the promising future of transportation. As one\nbasis for autonomous driving, High Definition Map (HD map) provides\nhigh-precision descriptions of the environment, therefore it enables more\naccurate perception and localization while improving the efficiency of path\nplanning. However, an extremely large amount of map data needs to be\ntransmitted during driving, thus posing great challenge for real-time and\nsafety requirements for autonomous driving. To this end, we first demonstrate\nhow the existing data distribution mechanism can support HD map services.\nFurthermore, considering the constraints of vehicle power, vehicle speed, base\nstation bandwidth, etc., we propose a HD map data distribution mechanism on top\nof Vehicle-to-Infrastructure (V2I) data transmission. By this mechanism, the\nmap provision task is allocated to the selected RSU nodes and transmits\nproportionate HD map data cooperatively. Their works on map data loading aims\nto provide in-time HD map data service with optimized in-vehicle energy\nconsumption. Finally, we model the selection of RSU nodes into a partial\nknapsack problem and propose a greedy strategy-based data transmission\nalgorithm. Experimental results confirm that within limited energy consumption,\nthe proposed mechanism can ensure HD map data service by coordinating multiple\nRSUs with the shortest data transmission time.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 12:22:48 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Xie", "Jinliang", ""], ["Tang", "Jie", ""], ["Liu", "Shaoshan", ""]]}, {"id": "2010.05337", "submitter": "Da Zheng", "authors": "Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song,\n  Quan Gan, Zheng Zhang, George Karypis", "title": "DistDGL: Distributed Graph Neural Network Training for Billion-Scale\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNN) have shown great success in learning from\ngraph-structured data. They are widely used in various applications, such as\nrecommendation, fraud detection, and search. In these domains, the graphs are\ntypically large, containing hundreds of millions of nodes and several billions\nof edges. To tackle this challenge, we develop DistDGL, a system for training\nGNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the\nDeep Graph Library (DGL), a popular GNN development framework. DistDGL\ndistributes the graph and its associated data (initial features and embeddings)\nacross the machines and uses this distribution to derive a computational\ndecomposition by following an owner-compute rule. DistDGL follows a synchronous\ntraining approach and allows ego-networks forming the mini-batches to include\nnon-local nodes. To minimize the overheads associated with distributed\ncomputations, DistDGL uses a high-quality and light-weight min-cut graph\npartitioning algorithm along with multiple balancing constraints. This allows\nit to reduce communication overheads and statically balance the computations.\nIt further reduces the communication by replicating halo nodes and by using\nsparse embedding updates. The combination of these design choices allows\nDistDGL to train high-quality models while achieving high parallel efficiency\nand memory scalability. We demonstrate our optimizations on both inductive and\ntransductive GNN models. Our results show that DistDGL achieves linear speedup\nwithout compromising model accuracy and requires only 13 seconds to complete a\ntraining epoch for a graph with 100 million nodes and 3 billion edges on a\ncluster with 16 machines.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 20:22:26 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 19:38:52 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zheng", "Da", ""], ["Ma", "Chao", ""], ["Wang", "Minjie", ""], ["Zhou", "Jinjing", ""], ["Su", "Qidong", ""], ["Song", "Xiang", ""], ["Gan", "Quan", ""], ["Zhang", "Zheng", ""], ["Karypis", "George", ""]]}, {"id": "2010.05447", "submitter": "Swaroop Reddy Basireddy", "authors": "B Swaroopa Reddy and G V V Sharma", "title": "Scalable Consensus Protocols for PoW based Blockchain and blockDAG", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two models for scaling the transaction throughput\nin Proof-of-Work (PoW) based blockchain networks. In the first approach, a\nmathematical model has derived for optimal transaction throughput for PoW based\nlongest chain rule blockchain. In this approach, the blockchain Peer-to-Peer\n(P2P) network is considered as Erd\\\"os-R\\'enyi random network topology. This\napproach is however limited by the block creation rate, the results suggest\nthat the rate beyond an optimal point can result in unfairness in the system.\nThe second approach is a new consensus protocol proposed by considering the\nledger as a Directed Acyclic Graph (DAG) called blockDAG instead of a chain of\nblocks. In this framework, we follow a two-step strategy that makes the system\nrobust enough to handle the double-spend attacks. The first step involves the\ndevelopment of an unsupervised learning graph clustering algorithm for\nseparating the blocks created by an attacker. In the second step, the attackers\nblocks are eliminated and the remaining blocks are arranged in topological\norder by honest clients which makes the blockDAG system suitable for smart\ncontract applications found in Internet of Things (IoT) services. The\nSimulation results demonstrate a significant improvement in the transaction\nthroughput compared to bitcoin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 04:36:37 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 08:45:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Reddy", "B Swaroopa", ""], ["Sharma", "G V V", ""]]}, {"id": "2010.05448", "submitter": "Swaroop Reddy Basireddy", "authors": "Swaroopa Reddy B", "title": "securePrune:Secure block pruning in UTXO based blockchains using\n  Accumulators", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a scheme called securePrune for reducing the\nstorage space of a full node and synchronization time of bootstrapping nodes\njoining the Peer-to-Peer (P2P) network in an Unspent Transaction Outputs (UTXO)\nbased blockchain like bitcoin using RSA accumulators. The size of the bitcoin\nblockchain is growing linearly with transactions. We propose a new block\nstructure to represent the state of a blockchain also called UTXO set by\nincluding an accumulator of a state in the block header and proofs of knowledge\nfor inclusion and deletion of the transactions of the current block in the\nblock. In our scheme, the miners periodically release a snapshot of the\nblockchain state. The other full nodes in the network, securely prune the\nhistorical blocks after attaining the required number of confirmations for the\nsnapshot block, which in turn confirms the snapshot of the state through an\naccumulator specified in the block header and proofs inside the block. The\nsecure and periodic pruning of the old blocks, reduce the synchronization time\nfor a new node joining into the network. The simulation results demonstrate a\nsignificant reduction in the storage space of a full node and bootstrapping\ncost of the new nodes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 04:41:49 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["B", "Swaroopa Reddy", ""]]}, {"id": "2010.05489", "submitter": "Elad Michael Schiller (PhD)", "authors": "Oskar Lundstr\\\"om and Michel Raynal and Elad Michael Schiller", "title": "Self-Stabilizing Indulgent Zero-degrading Binary Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guerraoui proposed an indulgent solution for the binary consensus problem.\nNamely, he showed that an arbitrary behavior of the failure detector never\nviolates safety requirements even if it compromises liveness. Consensus\nimplementations are often used in a repeated manner. Dutta and Guerraoui\nproposed a zero-degrading solution, \\ie during system runs in which the failure\ndetector behaves perfectly, a node failure during one consensus instance has no\nimpact on the performance of future instances.\n  Our study, which focuses on indulgent zero-degrading binary consensus, aims\nat the design of an even more robust communication abstraction. We do so\nthrough the lenses of self-stabilization - a very strong notion of\nfault-tolerance. In addition to node and communication failures,\nself-stabilizing algorithms can recover after the occurrence of arbitrary\ntransient faults; these faults represent any violation of the assumptions\naccording to which the system was designed to operate (as long as the algorithm\ncode stays intact).\n  This work proposes the first, to the best of our knowledge, self-stabilizing\nalgorithm for indulgent zero-degrading binary consensus for time-free\nmessage-passing systems prone to detectable process failures. The proposed\nalgorithm has an O(1) stabilization time (in terms of asynchronous cycles) from\narbitrary transient faults. Since the proposed solution uses an {\\Omega}\nfailure detector, we also present the first, to the best of our knowledge,\nself-stabilizing asynchronous {\\Omega} failure detector, which is a variation\non the one by Most\\'efaoui, Mourgaya, and Raynal.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:22:01 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lundstr\u00f6m", "Oskar", ""], ["Raynal", "Michel", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "2010.05498", "submitter": "Junya Nakamura", "authors": "Junya Nakamura, Sayaka Kamei, Yukiko Yamauchi", "title": "Evacuation from Various Types of Finite 2D Square Grid Fields by a\n  Metamorphic Robotic System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A metamorphic robotic system (MRS) is composed of anonymous, memoryless, and\nautonomous modules that execute an identical distributed algorithm to move\nwhile keeping the connectivity of the modules. For an MRS, the number of\nmodules required to solve a given task is an important complexity measure.\nHere, we consider evacuation from a finite two-dimensional square grid field by\nan MRS. This study aims to establish the minimum number of modules required to\nsolve the evacuation problem under several conditions. We consider a\nrectangular field surrounded by walls with at least one exit. Our results show\nthat two modules are necessary and sufficient for evacuation from any\nrectangular field if equipped with a global compass, which provides the modules\nwith a common sense of direction. After that, we focus on the case of modules\nwithout a global compass and show that four (resp. seven) modules are necessary\nand sufficient for restricted (resp. any) initial shapes of an MRS. We also\nshow that two modules are sufficient when an MRS is touching a wall in an\ninitial configuration. Then, we clarify the condition to stop an MRS after\nevacuation of a rectangular field. Finally, we extend these results to mazes\nand convex fields.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:52:34 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 14:25:38 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 14:32:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Nakamura", "Junya", ""], ["Kamei", "Sayaka", ""], ["Yamauchi", "Yukiko", ""]]}, {"id": "2010.05675", "submitter": "Patrick Lambein-Monette", "authors": "Bernadette Charron-Bost and Patrick Lambein-Monette", "title": "Average Consensus: A Little Learning Goes A Long Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When networked systems of autonomous agents carry out complex tasks, the\ncontrol and coordination sought after generally depend on a few fundamental\ncontrol primitives. Chief among these primitives is consensus, where agents are\nto converge to a common estimate within the range of initial values, which\nbecomes average consensus when the joint limit should be the average of the\ninitial values. To provide reliable services that are easy to deploy, these\nprimitives should operate even when the network is subject to frequent and\nunpredictable changes. Moreover, they should mobilize few computational\nresources so that low powered, deterministic, and anonymous agents can partake\nin the network. In this stringent adversarial context, we investigate the\ndistributed implementation of these primitives over networks with\nbidirectional, but potentially short-lived, communication links. Inspired by\nthe classic EqualNeighbor and Metropolis agreement rules for multi-agent\nsystems, we design distributed algorithms for consensus and average consensus,\nwhich we show to operate in polynomial time in a synchronous temporal model.\nThese algorithms are fully distributed, requiring neither symmetry-breaking\ndevices such as unique identifiers, nor global control or knowledge of the\nnetwork. Our strategy consists in making agents learn simple structural\nparameters of the network -- namely, their largest degrees -- which constitutes\nenough information to build simple update rules, implementable locally with\nlittle computational and memory overhead.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 13:15:33 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Charron-Bost", "Bernadette", ""], ["Lambein-Monette", "Patrick", ""]]}, {"id": "2010.05680", "submitter": "Jiarui Fang", "authors": "Jiarui Fang, Yang Yu, Chengduo Zhao, Jie Zhou", "title": "TurboTransformers: An Efficient GPU Serving System For Transformer\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transformer is the most critical algorithm innovation of the Nature\nLanguage Processing (NLP) field in recent years. Unlike the Recurrent Neural\nNetwork (RNN) models, Transformers can process on dimensions of sequence\nlengths in parallel, therefore leading to better accuracy on long sequences.\nHowever, efficient deployments of them for online services in data centers\nequipped with GPUs are not easy. First, more computation introduced by\ntransformer structures makes it more challenging to meet the latency and\nthroughput constraints of serving. Second, NLP tasks take in sentences of\nvariable length. The variability of input dimensions brings a severe problem to\nefficient memory management and serving optimization.\n  This paper designed a transformer serving system called TurboTransformers,\nwhich consists of a computing runtime and a serving framework to solve the\nabove challenges. Three innovative features make it stand out from other\nsimilar works. An efficient parallel algorithm is proposed for GPU-based batch\nreduction operations, like Softmax and LayerNorm, major hot spots besides BLAS\nroutines. A memory allocation algorithm, which better balances the memory\nfootprint and allocation/free efficiency, is designed for variable-length input\nsituations. A serving framework equipped with a new batch scheduler using\ndynamic programming achieves the optimal throughput on variable-length\nrequests. The system can achieve the state-of-the-art transformer model serving\nperformance on GPU platforms and can be seamlessly integrated into your PyTorch\ncode with a few lines of code.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:28:38 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:30:31 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 12:33:49 GMT"}, {"version": "v4", "created": "Sat, 20 Feb 2021 08:54:32 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Fang", "Jiarui", ""], ["Yu", "Yang", ""], ["Zhao", "Chengduo", ""], ["Zhou", "Jie", ""]]}, {"id": "2010.05693", "submitter": "Enes Krijestorac", "authors": "Enes Krijestorac, Agon Memedi, Takamasa Higuchi, Seyhan Ucar, Onur\n  Altintas, Danijela Cabric", "title": "Hybrid Vehicular and Cloud Distributed Computing: A Case for Cooperative\n  Perception", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the use of hybrid offloading of computing tasks\nsimultaneously to edge servers (vertical offloading) via LTE communication and\nto nearby cars (horizontal offloading) via V2V communication, in order to\nincrease the rate at which tasks are processed compared to local processing.\nOur main contribution is an optimized resource assignment and scheduling\nframework for hybrid offloading of computing tasks. The framework optimally\nutilizes the computational resources in the edge and in the micro cloud, while\ntaking into account communication constraints and task requirements. While\ncooperative perception is the primary use case of our framework, the framework\nis applicable to other cooperative vehicular applications with high computing\ndemand and significant transmission overhead. The framework is tested in a\nsimulated environment built on top of car traces and communication rates\nexported from the Veins vehicular networking simulator. We observe a\nsignificant increase in the processing rate of cooperative perception sensor\nframes when hybrid offloading with optimized resource assignment is adopted.\nFurthermore, the processing rate increases with V2V connectivity as more\ncomputing tasks can be offloaded horizontally.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 00:29:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Krijestorac", "Enes", ""], ["Memedi", "Agon", ""], ["Higuchi", "Takamasa", ""], ["Ucar", "Seyhan", ""], ["Altintas", "Onur", ""], ["Cabric", "Danijela", ""]]}, {"id": "2010.05838", "submitter": "Royson Lee", "authors": "Royson Lee, Stylianos I. Venieris, Nicholas D. Lane", "title": "Neural Enhancement in Content Delivery Systems: The State-of-the-Art and\n  Future Directions", "comments": "Accepted at the 1st Workshop on Distributed Machine Learning at\n  CoNEXT 2020 (DistributedML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-enabled smartphones and ultra-wide displays are transforming a\nvariety of visual apps spanning from on-demand movies and 360-degree videos to\nvideo-conferencing and live streaming. However, robustly delivering visual\ncontent under fluctuating networking conditions on devices of diverse\ncapabilities remains an open problem. In recent years, advances in the field of\ndeep learning on tasks such as super-resolution and image enhancement have led\nto unprecedented performance in generating high-quality images from low-quality\nones, a process we refer to as neural enhancement. In this paper, we survey\nstate-of-the-art content delivery systems that employ neural enhancement as a\nkey component in achieving both fast response time and high visual quality. We\nfirst present the deployment challenges of neural enhancement models. We then\ncover systems targeting diverse use-cases and analyze their design decisions in\novercoming technical challenges. Moreover, we present promising directions\nbased on the latest insights from deep learning research to further boost the\nquality of experience of these systems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:41:29 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 12:42:00 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lee", "Royson", ""], ["Venieris", "Stylianos I.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2010.05872", "submitter": "Xin Liang", "authors": "Xin Liang, Ben Whitney, Jieyang Chen, Lipeng Wan, Qing Liu, Dingwen\n  Tao, James Kress, Dave Pugmire, Matthew Wolf, Norbert Podhorszki, and Scott\n  Klasky", "title": "MGARD+: Optimizing Multilevel Methods for Error-bounded Scientific Data\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data management is becoming increasingly important in dealing with the large\namounts of data produced by large-scale scientific simulations and instruments.\nExisting multilevel compression algorithms offer a promising way to manage\nscientific data at scale, but may suffer from relatively low performance and\nreduction quality. In this paper, we propose MGARD+, a multilevel data\nreduction and refactoring framework drawing on previous multilevel methods, to\nachieve high-performance data decomposition and high-quality error-bounded\nlossy compression. Our contributions are four-fold: 1) We propose a level-wise\ncoefficient quantization method, which uses different error tolerances to\nquantize the multilevel coefficients. 2) We propose an adaptive decomposition\nmethod which treats the multilevel decomposition as a preconditioner and\nterminates the decomposition process at an appropriate level. 3) We leverage a\nset of algorithmic optimization strategies to significantly improve the\nperformance of multilevel decomposition/recomposition. 4) We evaluate our\nproposed method using four real-world scientific datasets and compare with\nseveral state-of-the-art lossy compressors. Experiments demonstrate that our\noptimizations improve the decomposition/recomposition performance of the\nexisting multilevel method by up to 70X, and the proposed compression method\ncan improve compression ratio by up to 2X compared with other state-of-the-art\nerror-bounded lossy compressors under the same level of data distortion.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:20:28 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 00:16:10 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Liang", "Xin", ""], ["Whitney", "Ben", ""], ["Chen", "Jieyang", ""], ["Wan", "Lipeng", ""], ["Liu", "Qing", ""], ["Tao", "Dingwen", ""], ["Kress", "James", ""], ["Pugmire", "Dave", ""], ["Wolf", "Matthew", ""], ["Podhorszki", "Norbert", ""], ["Klasky", "Scott", ""]]}, {"id": "2010.05958", "submitter": "Zheng Chai", "authors": "Zheng Chai, Yujing Chen, Liang Zhao, Yue Cheng, Huzefa Rangwala", "title": "FedAT: A Communication-Efficient Federated Learning Method with\n  Asynchronous Tiers under Non-IID Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) involves training a model over massive distributed\ndevices, while keeping the training data localized. This form of collaborative\nlearning exposes new tradeoffs among model convergence speed, model accuracy,\nbalance across clients, and communication cost, with new challenges including:\n(1) straggler problem, where the clients lag due to data or (computing and\nnetwork) resource heterogeneity, and (2) communication bottleneck, where a\nlarge number of clients communicate their local updates to a central server and\nbottleneck the server. Many existing FL methods focus on optimizing along only\none dimension of the tradeoff space. Existing solutions use asynchronous model\nupdating or tiering-based synchronous mechanisms to tackle the straggler\nproblem. However, the asynchronous methods can easily create a network\ncommunication bottleneck, while tiering may introduce biases as tiering favors\nfaster tiers with shorter response latencies. To address these issues, we\npresent FedAT, a novel Federated learning method with Asynchronous Tiers under\nNon-i.i.d. data. FedAT synergistically combines synchronous intra-tier training\nand asynchronous cross-tier training. By bridging the synchronous and\nasynchronous training through tiering, FedAT minimizes the straggler effect\nwith improved convergence speed and test accuracy. FedAT uses a\nstraggler-aware, weighted aggregation heuristic to steer and balance the\ntraining for further accuracy improvement. FedAT compresses the uplink and\ndownlink communications using an efficient, polyline-encoding-based compression\nalgorithm, therefore minimizing the communication cost. Results show that FedAT\nimproves the prediction performance by up to 21.09%, and reduces the\ncommunication cost by up to 8.5x, compared to state-of-the-art FL methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:38:51 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chai", "Zheng", ""], ["Chen", "Yujing", ""], ["Zhao", "Liang", ""], ["Cheng", "Yue", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "2010.05969", "submitter": "Hang Zhu", "authors": "Hang Zhu, Kostis Kaffes, Zixu Chen, Zhenming Liu, Christos Kozyrakis,\n  Ion Stoica, Xin Jin", "title": "RackSched: A Microsecond-Scale Scheduler for Rack-Scale Computers\n  (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-latency online services have strict Service Level Objectives (SLOs) that\nrequire datacenter systems to support high throughput at microsecond-scale tail\nlatency. Dataplane operating systems have been designed to scale up multi-core\nservers with minimal overhead for such SLOs. However, as application demands\ncontinue to increase, scaling up is not enough, and serving larger demands\nrequires these systems to scale out to multiple servers in a rack. We present\nRackSched, the first rack-level microsecond-scale scheduler that provides the\nabstraction of a rack-scale computer (i.e., a huge server with hundreds to\nthousands of cores) to an external service with network-system co-design. The\ncore of RackSched is a two-layer scheduling framework that integrates\ninter-server scheduling in the top-of-rack (ToR) switch with intra-server\nscheduling in each server. We use a combination of analytical results and\nsimulations to show that it provides near-optimal performance as centralized\nscheduling policies, and is robust for both low-dispersion and high-dispersion\nworkloads. We design a custom switch data plane for the inter-server scheduler,\nwhich realizes power-of-k-choices, ensures request affinity, and tracks server\nloads accurately and efficiently. We implement a RackSched prototype on a\ncluster of commodity servers connected by a Barefoot Tofino switch. End-to-end\nexperiments on a twelve-server testbed show that RackSched improves the\nthroughput by up to 1.44x, and scales out the throughput near linearly, while\nmaintaining the same tail latency as one server until the system is saturated.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:59:51 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 04:07:18 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zhu", "Hang", ""], ["Kaffes", "Kostis", ""], ["Chen", "Zixu", ""], ["Liu", "Zhenming", ""], ["Kozyrakis", "Christos", ""], ["Stoica", "Ion", ""], ["Jin", "Xin", ""]]}, {"id": "2010.05975", "submitter": "Grzegorz Kwasniewski", "authors": "Grzegorz Kwasniewski (1), Tal Ben-Nun (1), Alexandros Nikolaos Ziogas\n  (1), Timo Schneider (1), Maciej Besta (1), Torsten Hoefler (1) ((1) ETH\n  Zurich)", "title": "On the Parallel I/O Optimality of Linear Algebra Kernels: Near-Optimal\n  LU Factorization", "comments": "13 pages without references, 12 figures, submitted to PPoPP 2021:\n  26th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel\n  Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense linear algebra kernels, such as linear solvers or tensor contractions,\nare fundamental components of many scientific computing applications. In this\nwork, we present a novel method of deriving parallel I/O lower bounds for this\nbroad family of programs. Based on the X-partitioning abstraction, our method\nexplicitly captures inter-statement dependencies. Applying our analysis to LU\nfactorization, we derive COnfLUX, an LU algorithm with the parallel I/O cost of\n$N^3 / (P \\sqrt{M})$ communicated elements per processor -- only $1/3\\times$\nover our established lower bound. We evaluate COnfLUX on various problem sizes,\ndemonstrating empirical results that match our theoretical analysis,\ncommunicating asymptotically less than Cray ScaLAPACK or SLATE, and\noutperforming the asymptotically-optimal CANDMC library. Running on $1$,$024$\nnodes of Piz Daint, COnfLUX communicates 1.6$\\times$ less than the second-best\nimplementation and is expected to communicate 2.1$\\times$ less on a full-scale\nrun on Summit.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:06:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kwasniewski", "Grzegorz", ""], ["Ben-Nun", "Tal", ""], ["Ziogas", "Alexandros Nikolaos", ""], ["Schneider", "Timo", ""], ["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.06003", "submitter": "Lam Duc Nguyen", "authors": "Lam Duc Nguyen, Israel Leyva-Mayorga, Amari N. Lewis, and Petar\n  Popovski", "title": "Modeling and Analysis of Data Trading on Blockchain-based Market in IoT\n  Networks", "comments": "10 pages, 8 figures, Accepted at IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile devices with embedded sensors for data collection and environmental\nsensing create a basis for a cost-effective approach for data trading. For\nexample, these data can be related to pollution and gas emissions, which can be\nused to check the compliance with national and international regulations. The\ncurrent approach for IoT data trading relies on a centralized third-party\nentity to negotiate between data consumers and data providers, which is\ninefficient and insecure on a large scale. In comparison, a decentralized\napproach based on distributed ledger technologies (DLT) enables data trading\nwhile ensuring trust, security, and privacy. However, due to the lack of\nunderstanding of the communication efficiency between sellers and buyers, there\nis still a significant gap in benchmarking the data trading protocols in IoT\nenvironments. Motivated by this knowledge gap, we introduce a model for\nDLT-based IoT data trading over the Narrowband Internet of Things (NB-IoT)\nsystem, intended to support massive environmental sensing. We characterize the\ncommunication efficiency of three basic DLT-based IoT data trading protocols\nvia NB-IoT connectivity in terms of latency and energy consumption. The model\nand analyses of these protocols provide a benchmark for IoT data trading\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 20:11:35 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 20:44:14 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Nguyen", "Lam Duc", ""], ["Leyva-Mayorga", "Israel", ""], ["Lewis", "Amari N.", ""], ["Popovski", "Petar", ""]]}, {"id": "2010.06081", "submitter": "Fan Lai", "authors": "Fan Lai, Xiangfeng Zhu, Harsha V. Madhyastha, Mosharaf Chowdhury", "title": "Oort: Efficient Federated Learning via Guided Participant Selection", "comments": null, "journal-ref": "USENIX OSDI (2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is an emerging direction in distributed machine\nlearning (ML) that enables in-situ model training and testing on edge data.\nDespite having the same end goals as traditional ML, FL executions differ\nsignificantly in scale, spanning thousands to millions of participating\ndevices. As a result, data characteristics and device capabilities vary widely\nacross clients. Yet, existing efforts randomly select FL participants, which\nleads to poor model and system efficiency.\n  In this paper, we propose Oort to improve the performance of federated\ntraining and testing with guided participant selection. With an aim to improve\ntime-to-accuracy performance in model training, Oort prioritizes the use of\nthose clients who have both data that offers the greatest utility in improving\nmodel accuracy and the capability to run training quickly. To enable FL\ndevelopers to interpret their results in model testing, Oort enforces their\nrequirements on the distribution of participant data while improving the\nduration of federated testing by cherry-picking clients. Our evaluation shows\nthat, compared to existing participant selection mechanisms, Oort improves\ntime-to-accuracy performance by 1.2x-14.1x and final model accuracy by\n1.3%-9.8%, while efficiently enforcing developer-specified model testing\ncriteria at the scale of millions of clients.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:48:30 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 21:46:17 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 00:31:41 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lai", "Fan", ""], ["Zhu", "Xiangfeng", ""], ["Madhyastha", "Harsha V.", ""], ["Chowdhury", "Mosharaf", ""]]}, {"id": "2010.06139", "submitter": "Abu Naser", "authors": "Abu Naser, Mehran Sadeghi Lahijani, Cong Wu, Mohsen Gavahi, Viet Tung\n  Hoang, Zhi Wang, and Xin Yuan", "title": "Performance Evaluation and Modeling of Cryptographic Libraries for MPI\n  Communications", "comments": "Under review - IEEE Transactions on Dependable and Secure Computing\n  (TDSC). 12 figures, 11 tables, and 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for High-Performance Computing (HPC) applications with data security\nrequirements to execute in the public cloud, the cloud infrastructure must\nensure the privacy and integrity of data. To meet this goal, we consider\nincorporating encryption in the Message Passing Interface (MPI) library. We\nempirically evaluate four contemporary cryptographic libraries, OpenSSL,\nBoringSSL, Libsodium, and CryptoPP using micro-benchmarks and NAS parallel\nbenchmarks on two different networking technologies, 10Gbps Ethernet and 40Gbps\nInfiniBand. We also develop accurate models that allow us to reason about the\nperformance of encrypted MPI communication in different situations and give\nguidance on how to improve encrypted MPI performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:28:20 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Naser", "Abu", ""], ["Lahijani", "Mehran Sadeghi", ""], ["Wu", "Cong", ""], ["Gavahi", "Mohsen", ""], ["Hoang", "Viet Tung", ""], ["Wang", "Zhi", ""], ["Yuan", "Xin", ""]]}, {"id": "2010.06186", "submitter": "Alessio Netti", "authors": "Alessio Netti, Daniele Tafani, Michael Ott and Martin Schulz", "title": "Correlation-wise Smoothing: Lightweight Knowledge Extraction for HPC\n  Monitoring Data", "comments": "Accepted for publication at the 35th IEEE International Parallel &\n  Distributed Processing Symposium (IPDPS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern High-Performance Computing (HPC) and data center operators rely more\nand more on data analytics techniques to improve the efficiency and reliability\nof their operations. They employ models that ingest time-series monitoring\nsensor data and transform it into actionable knowledge for system tuning: a\nprocess known as Operational Data Analytics (ODA). However, monitoring data has\na high dimensionality, is hardware-dependent and difficult to interpret. This,\ncoupled with the strict requirements of ODA, makes most traditional data mining\nmethods impractical and in turn renders this type of data cumbersome to\nprocess. Most current ODA solutions use ad-hoc processing methods that are not\ngeneric, are sensible to the sensors' features and are not fit for\nvisualization.\n  In this paper we propose a novel method, called Correlation-wise Smoothing\n(CS), to extract descriptive signatures from time-series monitoring data in a\ngeneric and lightweight way. Our CS method exploits correlations between data\ndimensions to form groups and produces image-like signatures that can be easily\nmanipulated, visualized and compared. We evaluate the CS method on HPC-ODA, a\ncollection of datasets that we release with this work, and show that it leads\nto the same performance as most state-of-the-art methods while producing\nsignatures that are up to ten times smaller and up to ten times faster, while\ngaining visualizability, portability across systems and clear scaling\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 05:22:47 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 07:08:19 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Netti", "Alessio", ""], ["Tafani", "Daniele", ""], ["Ott", "Michael", ""], ["Schulz", "Martin", ""]]}, {"id": "2010.06277", "submitter": "Stijn Eyerman", "authors": "Sriram Aananthakrishnan, Nesreen K. Ahmed, Vincent Cave, Marcelo\n  Cintra, Yigit Demir, Kristof Du Bois, Stijn Eyerman, Joshua B. Fryman, Ivan\n  Ganev, Wim Heirman, Hans-Christian Hoppe, Jason Howard, Ibrahim Hur,\n  MidhunChandra Kodiyath, Samkit Jain, Daniel S. Klowden, Marek M. Landowski,\n  Laurent Montigny, Ankit More, Przemyslaw Ossowski, Robert Pawlowski, Nick\n  Pepperling, Fabrizio Petrini, Mariusz Sikora, Balasubramanian Seshasayee,\n  Shaden Smith, Sebastian Szkoda, Sanjaya Tayal, Jesmin Jahan Tithi, Yves\n  Vandriessche, Izajasz P. Wrosz", "title": "PIUMA: Programmable Integrated Unified Memory Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance large scale graph analytics is essential to timely analyze\nrelationships in big data sets. Conventional processor architectures suffer\nfrom inefficient resource usage and bad scaling on graph workloads. To enable\nefficient and scalable graph analysis, Intel developed the Programmable\nIntegrated Unified Memory Architecture (PIUMA). PIUMA consists of many\nmulti-threaded cores, fine-grained memory and network accesses, a globally\nshared address space and powerful offload engines. This paper presents the\nPIUMA architecture, and provides initial performance estimations, projecting\nthat a PIUMA node will outperform a conventional compute node by one to two\norders of magnitude. Furthermore, PIUMA continues to scale across multiple\nnodes, which is a challenge in conventional multinode setups.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:35:52 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Aananthakrishnan", "Sriram", ""], ["Ahmed", "Nesreen K.", ""], ["Cave", "Vincent", ""], ["Cintra", "Marcelo", ""], ["Demir", "Yigit", ""], ["Bois", "Kristof Du", ""], ["Eyerman", "Stijn", ""], ["Fryman", "Joshua B.", ""], ["Ganev", "Ivan", ""], ["Heirman", "Wim", ""], ["Hoppe", "Hans-Christian", ""], ["Howard", "Jason", ""], ["Hur", "Ibrahim", ""], ["Kodiyath", "MidhunChandra", ""], ["Jain", "Samkit", ""], ["Klowden", "Daniel S.", ""], ["Landowski", "Marek M.", ""], ["Montigny", "Laurent", ""], ["More", "Ankit", ""], ["Ossowski", "Przemyslaw", ""], ["Pawlowski", "Robert", ""], ["Pepperling", "Nick", ""], ["Petrini", "Fabrizio", ""], ["Sikora", "Mariusz", ""], ["Seshasayee", "Balasubramanian", ""], ["Smith", "Shaden", ""], ["Szkoda", "Sebastian", ""], ["Tayal", "Sanjaya", ""], ["Tithi", "Jesmin Jahan", ""], ["Vandriessche", "Yves", ""], ["Wrosz", "Izajasz P.", ""]]}, {"id": "2010.06288", "submitter": "Igor Zablotchi", "authors": "Marcos K. Aguilera and Naama Ben-David and Rachid Guerraoui and\n  Virendra J. Marathe and Athanasios Xygkis and Igor Zablotchi", "title": "Microsecond Consensus for Microsecond Applications", "comments": "Full version of OSDI'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of making apps fault-tolerant through replication,\nwhen apps operate at the microsecond scale, as in finance, embedded computing,\nand microservices apps. These apps need a replication scheme that also operates\nat the microsecond scale, otherwise replication becomes a burden. We propose\nMu, a system that takes less than 1.3 microseconds to replicate a (small)\nrequest in memory, and less than a millisecond to fail-over the system - this\ncuts the replication and fail-over latencies of the prior systems by at least\n61% and 90%.\n  Mu implements bona fide state machine replication/consensus (SMR) with strong\nconsistency for a generic app, but it really shines on microsecond apps, where\neven the smallest overhead is significant. To provide this performance, Mu\nintroduces a new SMR protocol that carefully leverages RDMA. Roughly, in Mu a\nleader replicates a request by simply writing it directly to the log of other\nreplicas using RDMA, without any additional communication. Doing so, however,\nintroduces the challenge of handling concurrent leaders, changing leaders,\ngarbage collecting the logs, and more - challenges that we address in this\npaper through a judicious combination of RDMA permissions and distributed\nalgorithmic design.\n  We implemented Mu and used it to replicate several systems: a financial\nexchange app called Liquibook, Redis, Memcached, and HERD. Our evaluation shows\nthat Mu incurs a small replication latency, in some cases being the only viable\nreplication system that incurs an acceptable overhead.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:05:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Aguilera", "Marcos K.", ""], ["Ben-David", "Naama", ""], ["Guerraoui", "Rachid", ""], ["Marathe", "Virendra J.", ""], ["Xygkis", "Athanasios", ""], ["Zablotchi", "Igor", ""]]}, {"id": "2010.06312", "submitter": "Vibhatha Abeykoon", "authors": "Vibhatha Abeykoon, Niranda Perera, Chathura Widanage, Supun\n  Kamburugamuve, Thejaka Amila Kanewala, Hasara Maithree, Pulasthi\n  Wickramasinghe, Ahmet Uyar and Geoffrey Fox", "title": "Data Engineering for HPC with Python", "comments": "9 pages, 11 images, Accepted in 9th Workshop on Python for\n  High-Performance and Scientific Computing (In conjunction with Supercomputing\n  20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data engineering is becoming an increasingly important part of scientific\ndiscoveries with the adoption of deep learning and machine learning. Data\nengineering deals with a variety of data formats, storage, data extraction,\ntransformation, and data movements. One goal of data engineering is to\ntransform data from original data to vector/matrix/tensor formats accepted by\ndeep learning and machine learning applications. There are many structures such\nas tables, graphs, and trees to represent data in these data engineering\nphases. Among them, tables are a versatile and commonly used format to load and\nprocess data. In this paper, we present a distributed Python API based on table\nabstraction for representing and processing data. Unlike existing\nstate-of-the-art data engineering tools written purely in Python, our solution\nadopts high performance compute kernels in C++, with an in-memory table\nrepresentation with Cython-based Python bindings. In the core system, we use\nMPI for distributed memory computations with a data-parallel approach for\nprocessing large datasets in HPC clusters.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:53:11 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Abeykoon", "Vibhatha", ""], ["Perera", "Niranda", ""], ["Widanage", "Chathura", ""], ["Kamburugamuve", "Supun", ""], ["Kanewala", "Thejaka Amila", ""], ["Maithree", "Hasara", ""], ["Wickramasinghe", "Pulasthi", ""], ["Uyar", "Ahmet", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2010.06471", "submitter": "Abu Naser", "authors": "Abu Naser, Cong Wu, Mehran Sadeghi Lahijani, Mohsen Gavahi, Viet Tung\n  Hoang, Zhi Wang, and Xin Yuan", "title": "CryptMPI: A Fast Encrypted MPI Library", "comments": "Updated system description, format changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cloud infrastructure must provide security for High-Performance Computing\n(HPC) applications of sensitive data to execute in such an environment.\nHowever, supporting security in the communication infrastructure of today's\npublic cloud is challenging, because current networks for data centers are so\nfast that adding encryption can incur very significant overheads. In this work,\nwe introduce CryptMPI, a high performance encrypted MPI library that supports\ncommunication with both integrity and privacy. We present the techniques in\nCryptMPI and report our benchmarking results using micro-benchmarks and NAS\nparallel benchmarks. The evaluation results indicate that the aforementioned\ntechniques are effective in improving the performance of encrypted\ncommunication.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:23:20 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 23:04:58 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Naser", "Abu", ""], ["Wu", "Cong", ""], ["Lahijani", "Mehran Sadeghi", ""], ["Gavahi", "Mohsen", ""], ["Hoang", "Viet Tung", ""], ["Wang", "Zhi", ""], ["Yuan", "Xin", ""]]}, {"id": "2010.06474", "submitter": "D. S. Hollman", "authors": "D. S. Hollman and Bryce Adelstein Lelbach and H. Carter Edwards and\n  Mark Hoemmen and Daniel Sunderland and Christian R. Trott", "title": "mdspan in C++: A Case Study in the Integration of Performance Portable\n  Features into International Language Standards", "comments": null, "journal-ref": "2019 IEEE/ACM International Workshop on Performance, Portability\n  and Productivity in HPC (P3HPC), Denver, CO, USA, 2019, pp. 60-70", "doi": "10.1109/P3HPC49587.2019.00011", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional arrays are ubiquitous in high-performance computing (HPC),\nbut their absence from the C++ language standard is a long-standing and\nwell-known limitation of their use for HPC. This paper describes the design and\nimplementation of mdspan, a proposed C++ standard multidimensional array view\n(planned for inclusion in C++23). The proposal is largely inspired by work done\nin the Kokkos project---a C++ performance-portable programming model deployed\nby numerous HPC institutions to prepare their code base for exascale-class\nsupercomputing systems. This paper describes the final design of mdspan after a\nfive-year process to achieve consensus in the C++ community. In particular, we\nwill lay out how the design addresses some of the core challenges of\nperformance-portable programming, and how its customization points allow a\nseamless extension into areas not currently addressed by the C++ Standard but\nwhich are of critical importance in the heterogeneous computing world of\ntoday's systems. Finally, we have provided a production-quality implementation\nof the proposal in its current form. This work includes several benchmarks of\nthis implementation aimed at demonstrating the zero-overhead nature of the\nmodern design.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:29:20 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hollman", "D. S.", ""], ["Lelbach", "Bryce Adelstein", ""], ["Edwards", "H. Carter", ""], ["Hoemmen", "Mark", ""], ["Sunderland", "Daniel", ""], ["Trott", "Christian R.", ""]]}, {"id": "2010.06521", "submitter": "Michael Kruse", "authors": "Michael Kruse, Hal Finkel, Xingfu Wu", "title": "Autotuning Search Space for Loop Transformations", "comments": "LLVM-in-HPC 2020 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges for optimizing compilers is to predict whether applying\nan optimization will improve its execution speed. Programmers may override the\ncompiler's profitability heuristic using optimization directives such as\npragmas in the source code. Machine learning in the form of autotuning can\nassist users in finding the best optimizations for each platform.\n  In this paper we propose a loop transformation search space that takes the\nform of a tree, in contrast to previous approaches that usually use vector\nspaces to represent loop optimization configurations. We implemented a simple\nautotuner exploring the search space and applied it to a selected set of\nPolyBench kernels. While the autotuner is capable of representing every\npossible sequence of loop transformations and their relations, the results\nmotivate the use of better search strategies such as Monte Carlo tree search to\nfind sophisticated loop transformations such as multilevel tiling.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 16:26:57 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kruse", "Michael", ""], ["Finkel", "Hal", ""], ["Wu", "Xingfu", ""]]}, {"id": "2010.06574", "submitter": "Aymen Alsaadi", "authors": "Aymen Al Saadi, Dario Alfe, Yadu Babuji, Agastya Bhati, Ben Blaiszik,\n  Thomas Brettin, Kyle Chard, Ryan Chard, Peter Coveney, Anda Trifan, Alex\n  Brace, Austin Clyde, Ian Foster, Tom Gibbs, Shantenu Jha, Kristopher Keipert,\n  Thorsten Kurth, Dieter Kranzlm\\\"uller, Hyungro Lee, Zhuozhao Li, Heng Ma,\n  Andre Merzky, Gerald Mathias, Alexander Partin, Junqi Yin, Arvind Ramanathan,\n  Ashka Shah, Abraham Stern, Rick Stevens, Li Tan, Mikhail Titov, Aristeidis\n  Tsaris, Matteo Turilli, Huub Van Dam, Shunzhou Wan, David Wifling", "title": "IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing\n  Better LEads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The drug discovery process currently employed in the pharmaceutical industry\ntypically requires about 10 years and $2-3 billion to deliver one new drug.\nThis is both too expensive and too slow, especially in emergencies like the\nCOVID-19 pandemic. In silicomethodologies need to be improved to better select\nlead compounds that can proceed to later stages of the drug discovery protocol\naccelerating the entire process. No single methodological approach can achieve\nthe necessary accuracy with required efficiency. Here we describe multiple\nalgorithmic innovations to overcome this fundamental limitation, development\nand deployment of computational infrastructure at scale integrates multiple\nartificial intelligence and simulation-based approaches. Three measures of\nperformance are:(i) throughput, the number of ligands per unit time; (ii)\nscientific performance, the number of effective ligands sampled per unit time\nand (iii) peak performance, in flop/s. The capabilities outlined here have been\nused in production for several months as the workhorse of the computational\ninfrastructure to support the capabilities of the US-DOE National Virtual\nBiotechnology Laboratory in combination with resources from the EU Centre of\nExcellence in Computational Biomedicine.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:49:33 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Saadi", "Aymen Al", ""], ["Alfe", "Dario", ""], ["Babuji", "Yadu", ""], ["Bhati", "Agastya", ""], ["Blaiszik", "Ben", ""], ["Brettin", "Thomas", ""], ["Chard", "Kyle", ""], ["Chard", "Ryan", ""], ["Coveney", "Peter", ""], ["Trifan", "Anda", ""], ["Brace", "Alex", ""], ["Clyde", "Austin", ""], ["Foster", "Ian", ""], ["Gibbs", "Tom", ""], ["Jha", "Shantenu", ""], ["Keipert", "Kristopher", ""], ["Kurth", "Thorsten", ""], ["Kranzlm\u00fcller", "Dieter", ""], ["Lee", "Hyungro", ""], ["Li", "Zhuozhao", ""], ["Ma", "Heng", ""], ["Merzky", "Andre", ""], ["Mathias", "Gerald", ""], ["Partin", "Alexander", ""], ["Yin", "Junqi", ""], ["Ramanathan", "Arvind", ""], ["Shah", "Ashka", ""], ["Stern", "Abraham", ""], ["Stevens", "Rick", ""], ["Tan", "Li", ""], ["Titov", "Mikhail", ""], ["Tsaris", "Aristeidis", ""], ["Turilli", "Matteo", ""], ["Van Dam", "Huub", ""], ["Wan", "Shunzhou", ""], ["Wifling", "David", ""]]}, {"id": "2010.06706", "submitter": "Sebastian Angel", "authors": "Haoran Zhang, Adney Cardoza, Peter Baile Chen, Sebastian Angel,\n  Vincent Liu", "title": "Fault-tolerant and Transactional Stateful Serverless Workflows (extended\n  version)", "comments": "Paper appears at OSDI 2020", "journal-ref": "Proceedings of the USENIX Symposium on Operating Systems Design\n  and Implementation (OSDI). November, 2020", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Beldi, a library and runtime system for writing and\ncomposing fault-tolerant and transactional stateful serverless functions. Beldi\nruns on existing providers and lets developers write complex stateful\napplications that require fault tolerance and transactional semantics without\nthe need to deal with tasks such as load balancing or maintaining virtual\nmachines. Beldi's contributions include extending the log-based fault-tolerant\napproach in Olive (OSDI 2016) with new data structures, transaction protocols,\nfunction invocations, and garbage collection. They also include adapting the\nresulting framework to work over a federated environment where each serverless\nfunction has sovereignty over its own data. We implement three applications on\nBeldi, including a movie review service, a travel reservation system, and a\nsocial media site. Our evaluation on 1,000 AWS Lambdas shows that Beldi's\napproach is effective and affordable.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 21:34:59 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zhang", "Haoran", ""], ["Cardoza", "Adney", ""], ["Chen", "Peter Baile", ""], ["Angel", "Sebastian", ""], ["Liu", "Vincent", ""]]}, {"id": "2010.06834", "submitter": "Diksha Gupta", "authors": "Diksha Gupta, Jared Saia and Maxwell Young", "title": "Bankrupting Sybil Despite Churn", "comments": "41 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2006.02893, arXiv:1911.06462", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Sybil attack occurs when an adversary pretends to be multiple identities\n(IDs). Limiting the number of Sybil (bad) IDs to a minority permits the use of\nwell-established tools for tolerating malicious behavior, such as protocols for\nByzantine consensus and secure multiparty computation. A popular technique for\nenforcing this minority is resource burning; that is, the verifiable\nconsumption of a network resource, such as computational power, bandwidth, or\nmemory.\n  Unfortunately, prior defenses require non-Sybil (good) IDs to consume at\nleast as many resources as the adversary, unless the rate of churn for good IDs\nis sufficiently low. Since many systems exhibit high churn, this is a\nsignificant barrier to deployment.\n  We present two algorithms that offer useful guarantees against Sybil\nadversary under a broadly-applicable model of churn. The first is GoodJEst,\nwhich estimates the number of good IDs that join the system over any window of\ntime, despite the adversary injecting bad IDs. GoodJEst applies to a broad\nrange of system settings, and we demonstrate its use in our second algorithm, a\nnew Sybil defense called ERGO. Even under high churn, ERGO guarantee (1) there\nis always a minority of bad IDs in the system; and (2) when the system is under\nattack, the good IDs burn resources at a total rate that is sublinear in the\nadversary's consumption.\n  To evaluate the impact of our theoretical results, we investigate the\nperformance of ERGO alongside prior defenses that employ resource burning.\nBased on our experiments, we design heuristics that further improve the\nperformance of ERGO by up to four orders of magnitude over these previous Sybil\ndefenses.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:38:50 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 19:35:27 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 16:09:08 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gupta", "Diksha", ""], ["Saia", "Jared", ""], ["Young", "Maxwell", ""]]}, {"id": "2010.06870", "submitter": "Moming Duan", "authors": "Moming Duan, Duo Liu, Xinyuan Ji, Renping Liu, Liang Liang, Xianzhang\n  Chen, Yujuan Tan", "title": "FedGroup: Efficient Clustered Federated Learning via Decomposed\n  Data-Driven Measure", "comments": "This work will be presented at IEEE International Symposium on\n  Parallel and Distributed Processing with Applications (ISPA) 2021. NOTE: This\n  revision contains a crucial correction of the client cold start mechanism,\n  please discard all previous manuscripts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) enables the multiple participating devices to\ncollaboratively contribute to a global neural network model while keeping the\ntraining data locally. Unlike the centralized training setting, the non-IID and\nimbalanced (statistical heterogeneity) training data of FL is distributed in\nthe federated network, which will increase the divergences between the local\nmodels and global model, further degrading performance. In this paper, we\npropose a novel clustered federated learning (CFL) framework FedGroup, in which\nwe 1) group the training of clients based on the similarities between the\nclients' optimization directions for high training performance; 2) construct a\nnew data-driven distance measure to improve the efficiency of the client\nclustering procedure. 3) implement a newcomer device cold start mechanism based\non the auxiliary global model for framework scalability and practicality.\n  FedGroup can achieve improvements by dividing joint optimization into groups\nof sub-optimization and can be combined with FL optimizer FedProx. The\nconvergence and complexity are analyzed to demonstrate the efficiency of our\nproposed framework. We also evaluate FedGroup and FedGrouProx (combined with\nFedProx) on several open datasets and made comparisons with related CFL\nframeworks. The results show that FedGroup can significantly improve absolute\ntest accuracy by +14.1% on FEMNIST compared to FedAvg. +3.4% on Sentiment140\ncompared to FedProx, +6.9% on MNIST compared to FeSEM.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 08:15:34 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 12:15:18 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 03:01:59 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 12:53:36 GMT"}, {"version": "v5", "created": "Mon, 26 Jul 2021 08:40:07 GMT"}, {"version": "v6", "created": "Tue, 27 Jul 2021 06:53:49 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Duan", "Moming", ""], ["Liu", "Duo", ""], ["Ji", "Xinyuan", ""], ["Liu", "Renping", ""], ["Liang", "Liang", ""], ["Chen", "Xianzhang", ""], ["Tan", "Yujuan", ""]]}, {"id": "2010.07031", "submitter": "Josef Widder", "authors": "Sean Braithwaite and Ethan Buchman and Ismail Khoffi and Igor Konnov\n  and Zarko Milosevic and Romain Ruetschi and Josef Widder", "title": "A Tendermint Light Client", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Tendermint blockchains, the proof-of-stake mechanism and the underlying\nconsensus algorithm entail a dynamic fault model that implies that the active\nvalidators (nodes that sign blocks) may change over time, and a quorum of these\nvalidators is assumed to be correct only for a limited period of time (called\ntrusting period). The changes of the validator set are under control of the\nblockchain application, and are committed in every block. In order to check\nwhat is the state of the blockchain application at some height h, one needs to\nknow the validator set at that height so that one can verify the corresponding\ndigital signatures and hashes. A naive way of determining the validator set for\nheight h requires one to: (i) download all blocks before h, (ii) verify blocks\nby checking digital signatures and hashes and (iii) execute the corresponding\ntransactions so the changes in the validator sets are reproduced. This can\npotentially be very slow and computationally and data intensive.\n  In this paper we formalize the dynamic fault model imposed by Tendermint, and\ndescribe a light client protocol that allows to check the state of the\nblockchain application that, in realistic settings, reduces significantly the\namount of data needed to be downloaded, and the number of required\ncomputationally expensive signature verification operations. In addition to\nmathematical proofs, we have formalized the light client protocol in TLA+, and\nchecked safety and liveness with the APALACHE model checker.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:42:28 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 07:35:52 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Braithwaite", "Sean", ""], ["Buchman", "Ethan", ""], ["Khoffi", "Ismail", ""], ["Konnov", "Igor", ""], ["Milosevic", "Zarko", ""], ["Ruetschi", "Romain", ""], ["Widder", "Josef", ""]]}, {"id": "2010.07098", "submitter": "Weile Wei", "authors": "Weile Wei, Arghya Chatterjee, Kevin Huck, Oscar Hernandez, Hartmut\n  Kaiser", "title": "Performance Analysis of a Quantum Monte Carlo Application on Multiple\n  Hardware Architectures Using the HPX Runtime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how we successfully used the HPX programming model to\nport the DCA++ application on multiple architectures that include POWER9, x86,\nARM v8, and NVIDIA GPUs. We describe the lessons we can learn from this\nexperience as well as the benefits of enabling the HPX in the application to\nimprove the CPU threading part of the code, which led to an overall 21%\nimprovement across architectures. We also describe how we used HPX-APEX to\nraise the level of abstraction to understand performance issues and to identify\ntasking optimization opportunities in the code, and how these relate to CPU/GPU\nutilization counters, device memory allocation over time, and CPU kernel-level\ncontext switches on a given architecture.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:54:57 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 16:33:19 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 18:41:08 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Wei", "Weile", ""], ["Chatterjee", "Arghya", ""], ["Huck", "Kevin", ""], ["Hernandez", "Oscar", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "2010.07111", "submitter": "Pablo Ouro", "authors": "Pablo Ouro, Unai Lopez-Novoa and Martyn Guest", "title": "On the performance of a highly-scalable Computational Fluid Dynamics\n  code on AMD, ARM and Intel processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  No area of computing is hungrier for performance than High Performance\nComputing (HPC), the demands of which continue to be a major driver for\nprocessor performance and adoption of accelerators, and also advances in\nmemory, storage, and networking technologies. A key feature of the Intel\nprocessor domination of the past decade has been the extensive adoption of GPUs\nas coprocessors, whilst more recent developments have seen the increased\navailability of a number of CPU processors, including the novel ARM-based\nchips. This paper analyses the performance and scalability of a\nstate-of-the-art Computational Fluid Dynamics (CFD) code on three HPC cluster\nsystems equipped with AMD EPYC-Rome (EPYC, 4096 cores), ARM-based Marvell\nThunderX2 (TX2, 8192 cores) and Intel Skylake (SKL, 8000 cores) processors.\nThree benchmark cases are designed with increasing computation-to-communication\nratio and numerical complexity, namely lid-driven cavity flow, Taylor-Green\nvortex and a travelling solitary wave using the level-set method, adopted with\n$4^{th}$-order central-differences or a $5^{th}$-order WENO scheme. Our results\nshow that the EPYC cluster delivers the best code performance for all the\nsetups under consideration. In the first two benchmarks, the SKL cluster\ndemonstrates faster computing times than the TX2 system, whilst in the solitary\nwave simulations, the TX2 cluster achieves good scalability and similar\nperformance to the EPYC system, both improving on that obtained with the SKL\ncluster. These results suggest that while the Intel SKL cores deliver the best\nstrong scalability, the associated cluster performance is lower compared to the\nEPYC system. The TX2 cluster performance is promising considering its recent\naddition to the HPC portfolio.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:05:04 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ouro", "Pablo", ""], ["Lopez-Novoa", "Unai", ""], ["Guest", "Martyn", ""]]}, {"id": "2010.07115", "submitter": "Ju Long", "authors": "Ju Long, Hung-Ying Tai, Shen-Ta Hsieh, and Michael Juntao Yuan", "title": "A lightweight design for serverless Function-as-a-Service", "comments": "5 pages, 2 figures, 1 table, in IEEE Software, 2020", "journal-ref": null, "doi": "10.1109/MS.2020.3028991", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FaaS (Function as a Service) allows developers to upload and execute code in\nthe cloud without managing servers. FaaS offerings from leading public cloud\nproviders are based on system microVM or application container technologies\nsuch as Firecracker or Docker. In this paper, we demonstrate that lightweight\nhigh-level runtimes, such as WebAssembly, could offer performance and scaling\nadvantages over existing solutions, and could enable finely-grained\npay-as-you-use business models. We compared widely used performance benchmarks\nbetween Docker native and WebAssembly implementations of the same algorithms.\nWe also discuss the barriers for WebAssembly adoption in serverless computing,\nsuch as the lack of tooling support.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 05:47:46 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Long", "Ju", ""], ["Tai", "Hung-Ying", ""], ["Hsieh", "Shen-Ta", ""], ["Yuan", "Michael Juntao", ""]]}, {"id": "2010.07226", "submitter": "Aravind Sankaran", "authors": "Aravind Sankaran, Paolo Bientinesi", "title": "Robust Ranking of Equivalent Algorithms via Relative Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scientific computing, it is common that one target computation can be\ntranslated into many different sequences of library calls, each identifying an\nalgorithm. Although mathematically equivalent, those algorithms might exhibit\nsignificant differences in terms of performance. In practice, we observe that\nsubsets of algorithms show comparable performance characteristics. For this\nreason, we aim to identify and separate not one algorithm, but the subset of\nalgorithms, that are reliably faster than the rest. One of the motivations for\nthis setup is that it makes it then possible to select an algorithm based on\nadditional performance metrics such as those based on energy or scalability. To\nthis end, instead of using the usual methods of quantifying the performance of\nan algorithm in absolute terms, we present a measurement-based approach that\nassigns a relative score to the algorithms in comparison to one another. The\nrelative performance is encoded by sorting the algorithms based on pair-wise\ncomparisons and by ranking them into equivalence (or performance) classes, so\nthat multiple algorithms can obtain the same rank. We show that this approach,\nbased on relative performance, leads to robust identification of the fastest\nalgorithms, that is, reliable identification even under noisy system\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:40:23 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 10:41:03 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sankaran", "Aravind", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "2010.07244", "submitter": "Duncan Brown", "authors": "Duncan A. Brown, Karan Vahi, Michela Taufer, Von Welch, Ewa Deelman", "title": "Reproducing GW150914: the first observation of gravitational waves from\n  a binary black hole merger", "comments": "11 pages, 3 figures, 2 tables, 1 listing. Revised version accepted by\n  IEEE Computing in Science and Engineering. Code available from\n  https://doi.org/10.5281/zenodo.4085984 and data available from\n  http://dx.doi.org/10.21227/c634-qh33", "journal-ref": null, "doi": "10.1109/MCSE.2021.3059232", "report-no": null, "categories": "cs.DC astro-ph.IM gr-qc", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2016, LIGO and Virgo announced the first observation of gravitational\nwaves from a binary black hole merger, known as GW150914. To establish the\nconfidence of this detection, large-scale scientific workflows were used to\nmeasure the event's statistical significance. They used code written by the\nLIGO/Virgo and were executed on the LIGO Data Grid. The codes are publicly\navailable, but there has not yet been an attempt to directly reproduce the\nresults, although several analyses have replicated the analysis, confirming the\ndetection. We attempt to reproduce the result presented in the GW150914\ndiscovery paper using publicly available code on the Open Science Grid. We show\nthat we can reproduce the main result but we cannot exactly reproduce the LIGO\nanalysis as the original data set used is not public. We discuss the challenges\nwe encountered and make recommendations for scientists who wish to make their\nwork reproducible.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:06:16 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 16:19:04 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Brown", "Duncan A.", ""], ["Vahi", "Karan", ""], ["Taufer", "Michela", ""], ["Welch", "Von", ""], ["Deelman", "Ewa", ""]]}, {"id": "2010.07268", "submitter": "Benjamin Carver", "authors": "Benjamin Carver, Jingyuan Zhang, Ao Wang, Ali Anwar, Panruo Wu, Yue\n  Cheng", "title": "Wukong: A Scalable and Locality-Enhanced Framework for Serverless\n  Parallel Computing", "comments": "Appears at ACM Symposium on Cloud Computing (SoCC) 2020", "journal-ref": null, "doi": "10.1145/3419111.3421286", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is increasingly being used for parallel computing, which\nhave traditionally been implemented as stateful applications. Executing\ncomplex, burst-parallel, directed acyclic graph (DAG) jobs poses a major\nchallenge for serverless execution frameworks, which will need to rapidly scale\nand schedule tasks at high throughput, while minimizing data movement across\ntasks. We demonstrate that, for serverless parallel computations, decentralized\nscheduling enables scheduling to be distributed across Lambda executors that\ncan schedule tasks in parallel, and brings multiple benefits, including\nenhanced data locality, reduced network I/Os, automatic resource elasticity,\nand improved cost effectiveness. We describe the implementation and deployment\nof our new serverless parallel framework, called Wukong, on AWS Lambda. We show\nthat Wukong achieves near-ideal scalability, executes parallel computation jobs\nup to 68.17x faster, reduces network I/O by multiple orders of magnitude, and\nachieves 92.96% tenant-side cost savings compared to numpywren.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:47:56 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Carver", "Benjamin", ""], ["Zhang", "Jingyuan", ""], ["Wang", "Ao", ""], ["Anwar", "Ali", ""], ["Wu", "Panruo", ""], ["Cheng", "Yue", ""]]}, {"id": "2010.07277", "submitter": "Sambhav Satija", "authors": "Sambhav Satija, Apurv Mehra, Sudheesh Singanamalla, Karan Grover,\n  Muthian Sivathanu, Nishanth Chandran, Divya Gupta, Satya Lokam", "title": "Blockene: A High-throughput Blockchain Over Mobile Devices", "comments": "A version of this paper (without the appendix) will appear in OSDI\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Blockene, a blockchain that reduces resource usage at member\nnodes by orders of magnitude, requiring only a smartphone to participate in\nblock validation and consensus. Despite being lightweight, Blockene provides a\nhigh throughput of transactions and scales to a large number of participants.\nBlockene consumes negligible battery and data in smartphones, enabling millions\nof users to participate in the blockchain without incentives, to secure\ntransactions with their collective honesty. Blockene achieves these properties\nwith a novel split-trust design based on delegating storage and gossip to\nuntrusted nodes.\n  We show, with a prototype implementation, that Blockene provides throughput\nof 1045 transactions/sec, and runs with very low resource usage on smartphones,\npointing to a new paradigm for building secure, decentralized applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:53:18 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Satija", "Sambhav", ""], ["Mehra", "Apurv", ""], ["Singanamalla", "Sudheesh", ""], ["Grover", "Karan", ""], ["Sivathanu", "Muthian", ""], ["Chandran", "Nishanth", ""], ["Gupta", "Divya", ""], ["Lokam", "Satya", ""]]}, {"id": "2010.07352", "submitter": "Markus Nissl", "authors": "Markus Nissl, Emanuel Sallinger, Stefan Schulte and Michael Borkowski", "title": "Towards Cross-Blockchain Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, manifold blockchain protocols have been proposed by\nresearchers and industrial companies alike. This has led to a very\nheterogeneous blockchain landscape. Accordingly, it would be desirable if\nblockchains could interact with each other. However, current blockchain\ntechnologies offer only limited support for interoperability, thus preventing\ntokens or smart contracts from leaving the scope of a particular blockchain.\n  As a first step towards a solution for cross-chain smart contract\ninteractions, we introduce a framework which allows to invoke a smart contract\nfrom another blockchain. We offer support for continuing a smart contract after\nreceiving a result from a different blockchain, and for calling smart contracts\nrecursively across blockchains. We provide a reference implementation for\nEthereum-based blockchains using Solidity and evaluate the performance\nregarding time and cost overheads.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:39:52 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 14:27:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nissl", "Markus", ""], ["Sallinger", "Emanuel", ""], ["Schulte", "Stefan", ""], ["Borkowski", "Michael", ""]]}, {"id": "2010.07427", "submitter": "Mustafa Ozdayi", "authors": "Harsh Bimal Desai, Mustafa Safa Ozdayi, Murat Kantarcioglu", "title": "BlockFLA: Accountable Federated Learning via Hybrid Blockchain\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a distributed, and decentralized machine learning\nprotocol. By executing FL, a set of agents can jointly train a model without\nsharing their datasets with each other, or a third-party. This makes FL\nparticularly suitable for settings where data privacy is desired.\n  At the same time, concealing training data gives attackers an opportunity to\ninject backdoors into the trained model. It has been shown that an attacker can\ninject backdoors to the trained model during FL, and then can leverage the\nbackdoor to make the model misclassify later. Several works tried to alleviate\nthis threat by designing robust aggregation functions. However, given more\nsophisticated attacks are developed over time, which by-pass the existing\ndefenses, we approach this problem from a complementary angle in this work.\nParticularly, we aim to discourage backdoor attacks by detecting, and punishing\nthe attackers, possibly after the end of training phase.\n  To this end, we develop a hybrid blockchain-based FL framework that uses\nsmart contracts to automatically detect, and punish the attackers via monetary\npenalties. Our framework is general in the sense that, any aggregation\nfunction, and any attacker detection algorithm can be plugged into it. We\nconduct experiments to demonstrate that our framework preserves the\ncommunication-efficient nature of FL, and provide empirical results to\nillustrate that it can successfully penalize attackers by leveraging our novel\nattacker detection algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 22:43:39 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Desai", "Harsh Bimal", ""], ["Ozdayi", "Mustafa Safa", ""], ["Kantarcioglu", "Murat", ""]]}, {"id": "2010.07430", "submitter": "Akshit Kumar", "authors": "Akshit Kumar, Parikshit Hegde, Rahul Vaze, Amira Alloum, C\\'edric\n  Adjih", "title": "Breaking the Unit Throughput Barrier in Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-level random power transmit strategy that is used in conjunction with\na random access protocol (RAP) (e.g. ALOHA, IRSA) is proposed to fundamentally\nincrease the throughput in a distributed communication network. A SIR model is\nconsidered, where a packet is decodable as long as its SIR is above a certain\nthreshold. In a slot chosen for transmission by a RAP, a packet is transmitted\nwith power level chosen according to a distribution, such that multiple packets\nsent by different nodes can be decoded at the receiver in a single slot, by\nensuring that their SIRs are above the threshold with successive interference\ncancelation. Since the network is distributed this is a challenging task, and\nwe provide structural results that aid in finding the achievable throughputs,\ntogether with upper bounds on the maximum throughput possible. The achievable\nthroughput and the upper bounds are shown to be close with the help of\ncomprehensive simulations. The main takeaway is that the throughput of more\nthan 1 is possible in a distributed network, by using a judicious choice of\npower level distribution in conjuction with a RAP.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 22:52:53 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 06:11:02 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kumar", "Akshit", ""], ["Hegde", "Parikshit", ""], ["Vaze", "Rahul", ""], ["Alloum", "Amira", ""], ["Adjih", "C\u00e9dric", ""]]}, {"id": "2010.07541", "submitter": "Saurav Prakash", "authors": "Saurav Prakash, Hanieh Hashemi, Yongqin Wang, Murali Annavaram, Salman\n  Avestimehr", "title": "Byzantine-Resilient Federated Learning with Heterogeneous Data\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For mitigating Byzantine behaviors in federated learning (FL), most\nstate-of-the-art approaches, such as Bulyan, tend to leverage the similarity of\nupdates from the benign clients. However, in many practical FL scenarios, data\nis non-IID across clients, thus the updates received from even the benign\nclients are quite dissimilar. Hence, using similarity based methods result in\nwasted opportunities to train a model from interesting non-IID data, and also\nslower model convergence. We propose DiverseFL to overcome this challenge in\nheterogeneous data distribution settings. Rather than comparing each client's\nupdate with other client updates to detect Byzantine clients, DiverseFL\ncompares each client's update with a guiding update of that client. Any client\nwhose update diverges from its associated guiding update is then tagged as a\nByzantine node. The FL server in DiverseFL computes the guiding update in every\nround for each client over a small sample of the client's local data that is\nreceived only once before start of the training. However, sharing even a small\nsample of client's data with the FL server can compromise client's data privacy\nneeds. To tackle this challenge, DiverseFL creates a Trusted Execution\nEnvironment (TEE)-based enclave to receive each client's sample and to compute\nits guiding updates. TEE provides a hardware assisted verification and\nattestation to each client that its data is not leaked outside of TEE. Through\nexperiments involving neural networks, benchmark datasets and popular Byzantine\nattacks, we demonstrate that DiverseFL not only performs Byzantine mitigation\nquite effectively, it also almost matches the performance of OracleSGD, where\nthe server only aggregates the updates from the benign clients.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 06:21:42 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 06:06:43 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 05:12:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Prakash", "Saurav", ""], ["Hashemi", "Hanieh", ""], ["Wang", "Yongqin", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2010.07584", "submitter": "Mohan Baruwal Chhetri", "authors": "Mohan Baruwal Chhetri, Abdur Rahim Mohammad Forkan, Anton V. Uzunov,\n  Surya Nepal", "title": "Towards Self-Improving Hybrid Elasticity Control of Cloud-based Software\n  Systems", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elasticity is a form of self-adaptivity in cloud-based software systems that\nis typically restricted to the infrastructure layer and realized through\nauto-scaling. However, both reactive and proactive forms of infrastructure\nauto-scaling have limitations, when used separately as well as together. To\naddress these limitations, we propose an approach for self-improving hybrid\nelasticity control that combines (a) infrastructure and software elasticity,\nand (b) proactive, reactive and responsive decision-making. At the\ninfrastructure layer, resources are provisioned proactively based on\none-step-ahead workload forecasts, and reactively, based on observed workload\nvariations. At the software layer, features are activated or deactivated in\nresponse to transient, minor deviations from the predicted workload. The\nproposed approach can lead to better performance-aware and cost-effective\nresource management in cloud-based software systems. We validate our approach\nvia a partial realization and simulation with real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:08:11 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Chhetri", "Mohan Baruwal", ""], ["Forkan", "Abdur Rahim Mohammad", ""], ["Uzunov", "Anton V.", ""], ["Nepal", "Surya", ""]]}, {"id": "2010.07680", "submitter": "Pankesh Patel", "authors": "Bhavin Joshi and Tapan Pathak and Vatsal Patel and Sarth Kanani and\n  Pankesh Patel and Muhammad Intizar Ali and John Breslin", "title": "Demonstration of a Cloud-based Software Framework for Video Analytics\n  Application using Low-Cost IoT Devices", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.09065", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The design of products and services such as a Smart doorbell, demonstrating\nvideo analytics software/algorithm functionality, is expected to address a new\nkind of requirements such as designing a scalable solution while considering\nthe trade-off between cost and accuracy; a flexible architecture to deploy new\nAI-based models or update existing models, as user requirements evolve; as well\nas seamlessly integrating different kinds of user interfaces and devices. To\naddress these challenges, we propose a smart doorbell that orchestrates video\nanalytics across Edge and Cloud resources. The proposal uses AWS as a base\nplatform for implementation and leverages Commercially Available\nOff-The-Shelf(COTS) affordable devices such as Raspberry Pi in the form of an\nEdge device.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 06:05:32 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Joshi", "Bhavin", ""], ["Pathak", "Tapan", ""], ["Patel", "Vatsal", ""], ["Kanani", "Sarth", ""], ["Patel", "Pankesh", ""], ["Ali", "Muhammad Intizar", ""], ["Breslin", "John", ""]]}, {"id": "2010.07808", "submitter": "Raouf Kerkouche", "authors": "Raouf Kerkouche, Gergely \\'Acs and Claude Castelluccia", "title": "Federated Learning in Adversarial Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning enables entities to collaboratively learn a shared\nprediction model while keeping their training data locally. It prevents data\ncollection and aggregation and, therefore, mitigates the associated privacy\nrisks. However, it still remains vulnerable to various security attacks where\nmalicious participants aim at degrading the generated model, inserting\nbackdoors, or inferring other participants' training data. This paper presents\na new federated learning scheme that provides different trade-offs between\nrobustness, privacy, bandwidth efficiency, and model accuracy. Our scheme uses\nbiased quantization of model updates and hence is bandwidth efficient. It is\nalso robust against state-of-the-art backdoor as well as model degradation\nattacks even when a large proportion of the participant nodes are malicious. We\npropose a practical differentially private extension of this scheme which\nprotects the whole dataset of participating entities. We show that this\nextension performs as efficiently as the non-private but robust scheme, even\nwith stringent privacy requirements but are less robust against model\ndegradation and backdoor attacks. This suggests a possible fundamental\ntrade-off between Differential Privacy and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:57:02 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Kerkouche", "Raouf", ""], ["\u00c1cs", "Gergely", ""], ["Castelluccia", "Claude", ""]]}, {"id": "2010.07818", "submitter": "Kommy Weldemariam Dr", "authors": "Andrew Kinai, Fred Otieno, Nelson Bore, Komminist Weldemariam", "title": "Multi-factor authentication for users of non-internet based applications\n  of blockchain-based platforms", "comments": "7 papes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Attacks targeting several millions of non-internet based application users\nare on the rise. These applications such as SMS and USSD typically do not\nbenefit from existing multi-factor authentication methods due to the nature of\ntheir interaction interfaces and mode of operations. To address this problem,\nwe propose an approach that augments blockchain with multi-factor\nauthentication based on evidence from blockchain transactions combined with\nrisk analysis. A profile of how a user performs transactions is built overtime\nand is used to analyse the risk level of each new transaction. If a transaction\nis flagged as high risk, we generate n-factor layers of authentication using\npast endorsed blockchain transactions. A demonstration of how we used the\nproposed approach to authenticate critical financial transactions in a\nblockchain-based asset financing platform is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:19:36 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Kinai", "Andrew", ""], ["Otieno", "Fred", ""], ["Bore", "Nelson", ""], ["Weldemariam", "Komminist", ""]]}, {"id": "2010.07920", "submitter": "Stefan Schmid", "authors": "Janardhan Kulkarni, Stefan Schmid and Pawe{\\l} Schmidt", "title": "Scheduling Opportunistic Links in Two-Tiered Reconfigurable Datacenters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfigurable optical topologies are emerging as a promising technology to\nimprove the efficiency of datacenter networks. This paper considers the problem\nof scheduling opportunistic links in such reconfigurable datacenters. We study\nthe online setting and aim to minimize flow completion times. The problem is a\ntwo-tier generalization of classic switch scheduling problems. We present a\nstable-matching algorithm which is $2\\cdot (2/\\varepsilon+1)$-competitive\nagainst an optimal offline algorithm, in a resource augmentation model: the\nonline algorithm runs $2+\\varepsilon$ times faster. Our algorithm and result\nare fairly general and allow for different link delays and also apply to hybrid\ntopologies which combine fixed and reconfigurable links. Our analysis is based\non LP relaxation and dual fitting.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:52:10 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Kulkarni", "Janardhan", ""], ["Schmid", "Stefan", ""], ["Schmidt", "Pawe\u0142", ""]]}, {"id": "2010.08009", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Study of Automatic Offloading Method in Mixed Offloading Destination\n  Environment", "comments": "Descriptions of FPGA offloading results were insufficient in section\n  4", "journal-ref": null, "doi": null, "report-no": "IEICE Technical Report, IN2020-30, Oct. 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, utilization of heterogeneous hardware other than small core\nCPU such as GPU, FPGA or many core CPU is increasing. However, when using\nheterogeneous hardware, barriers of technical skills such as OpenMP, CUDA and\nOpenCL are high. Based on that, I have proposed environment-adaptive software\nthat enables automatic conversion, configuration, and high performance\noperation of once written code, according to the hardware to be placed.\nHowever, including existing technologies, there has been no research to\nproperly and automatically offload the mixed offloading destination environment\nsuch as GPU, FPGA and many core CPU. In this paper, as a new element of\nenvironment-adaptive software, I study a method for offloading applications\nproperly and automatically in the environment where the offloading destination\nis mixed with GPU, FPGA and many core CPU. I evaluate the effectiveness of the\nproposed method in multiple applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 20:30:48 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 23:10:09 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2010.08154", "submitter": "Soubhik Deb", "authors": "Soubhik Deb, Sreeram Kannan, David Tse", "title": "PoSAT: Proof-of-Work Availability and Unpredictability, without the Work", "comments": "arXiv admin note: text overlap with arXiv:2005.10484", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important feature of Proof-of-Work (PoW) blockchains is full dynamic\navailability, allowing miners to go online and offline while requiring only 50%\nof the online miners to be honest.\n  Existing Proof-of-stake (PoS), Proof-of-Space and related protocols are able\nto achieve this property only partially, either putting the additional\nassumption that adversary nodes to be online from the beginning and no new\nadversary nodes come online afterwards, or use additional trust assumptions for\nnewly joining nodes.We propose a new PoS protocol PoSAT which can provably\nachieve dynamic availability fully without any additional assumptions. The\nprotocol is based on the longest chain and uses a Verifiable Delay Function for\nthe block proposal lottery to provide an arrow of time. The security analysis\nof the protocol draws on the recently proposed technique of Nakamoto blocks as\nwell as the theory of branching random walks. An additional feature of PoSAT is\nthe complete unpredictability of who will get to propose a block next, even by\nthe winner itself. This unpredictability is at the same level of PoW protocols,\nand is stronger than that of existing PoS protocols using Verifiable Random\nFunctions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:55:50 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 03:41:51 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Deb", "Soubhik", ""], ["Kannan", "Sreeram", ""], ["Tse", "David", ""]]}, {"id": "2010.08176", "submitter": "Omid Ardakanian", "authors": "Leepakshi Bindra, Kalvin Eng, Omid Ardakanian, Eleni Stroulia", "title": "Flexible, Decentralized Access Control for Smart Buildings with Smart\n  Contracts", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large commercial buildings are complex cyber-physical systems containing\nexpensive and critical equipment that ensure the safety and comfort of their\nnumerous occupants. Yet occupant and visitor access to spaces and equipment\nwithin these buildings are still managed through unsystematic, inefficient, and\nhuman-intensive processes. As a standard practice, long-term building occupants\nare given access privileges to rooms and equipment based on their\norganizational roles, while visitors have to be escorted by their hosts. This\napproach is conservative and inflexible. In this paper, we describe a\nmethodology that can flexibly and securely manage building access privileges\nfor long-term occupants and short-term visitors alike, taking into account the\nrisk associated with accessing each space within the building. Our methodology\nrelies on blockchain smart contracts to describe, grant, audit, and revoke\nfine-grained permissions for building occupants and visitors, in a\ndecentralized fashion. The smart contracts are specified through a process that\nleverages the information compiled from Brick and BOT models of the building.\nWe illustrate the proposed method through a typical application scenario in the\ncontext of a real office building and argue that it can greatly reduce the\nadministration overhead, while, at the same time, providing fine-grained,\nauditable access control.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 05:35:29 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bindra", "Leepakshi", ""], ["Eng", "Kalvin", ""], ["Ardakanian", "Omid", ""], ["Stroulia", "Eleni", ""]]}, {"id": "2010.08222", "submitter": "Janne H. Korhonen", "authors": "Dan Alistarh and Janne H. Korhonen", "title": "Towards Tight Communication Lower Bounds for Distributed Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a standard distributed optimisation setting where $N$ machines,\neach holding a $d$-dimensional function $f_i$, aim to jointly minimise the sum\nof the functions $\\sum_{i = 1}^N f_i (x)$. This problem arises naturally in\nlarge-scale distributed optimisation, where a standard solution is to apply\nvariants of (stochastic) gradient descent. We focus on the communication\ncomplexity of this problem: our main result provides the first fully\nunconditional bounds on total number of bits which need to be sent and received\nby the $N$ machines to solve this problem under point-to-point communication,\nwithin a given error-tolerance. Specifically, we show that $\\Omega( Nd \\log d /\nN\\varepsilon)$ total bits need to be communicated between the machines to find\nan additive $\\epsilon$-approximation to the minimum of $\\sum_{i = 1}^N f_i\n(x)$. The result holds for both deterministic and randomised algorithms, and,\nimportantly, requires no assumptions on the algorithm structure. The lower\nbound is tight under certain restrictions on parameter values, and is matched\nwithin constant factors for quadratic objectives by a new variant of quantised\ngradient descent, which we describe and analyse. Our results bring over tools\nfrom communication complexity to distributed optimisation, which has potential\nfor further applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:10:02 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 12:13:53 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Alistarh", "Dan", ""], ["Korhonen", "Janne H.", ""]]}, {"id": "2010.08274", "submitter": "Alessandro Sorniotti PhD", "authors": "Elli Androulaki, Angelo De Caro, Kaoutar Elkhiyaoui, Christian\n  Gorenflo, Alessandro Sorniotti and Marko Vukolic", "title": "Multi-Shard Private Transactions for Permissioned Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, blockchain systems involve sharing transaction information\nacross all blockchain network participants. Clearly, this introduces barriers\nto the adoption of the technology by the enterprise world, where preserving the\nprivacy of the business data is a necessity. Previous efforts to bring privacy\nand blockchains together either still leak partial information, are restricted\nin their functionality or use costly mechanisms like zk-SNARKs. In this paper,\nwe propose the Multi-Shard Private Transaction (MSPT) protocol, a novel\nprivacy-preserving protocol for permissioned blockchains, which relies only on\nsimple cryptographic primitives and targeted dissemination of information to\nachieve atomicity and high performances.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:48:19 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Androulaki", "Elli", ""], ["De Caro", "Angelo", ""], ["Elkhiyaoui", "Kaoutar", ""], ["Gorenflo", "Christian", ""], ["Sorniotti", "Alessandro", ""], ["Vukolic", "Marko", ""]]}, {"id": "2010.08316", "submitter": "Matthias Grundmann", "authors": "Matthias Grundmann, Hannes Hartenstein", "title": "Fundamental Properties of the Layer Below a Payment Channel Network\n  (Extended Version)", "comments": "Extended version of short paper published at 4th International\n  Workshop on Cryptocurrencies and Blockchain Technology - CBT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment channel networks are a highly discussed approach for improving\nscalability of cryptocurrencies such as Bitcoin. As they allow processing\ntransactions off-chain, payment channel networks are referred to as second\nlayer technology, while the blockchain is the first layer. We uncouple payment\nchannel networks from blockchains and look at them as first-class citizens.\nThis brings up the question what model payment channel networks require as\nfirst layer. In response, we formalize a model (called RFL Model) for a first\nlayer below a payment channel network. While transactions are globally made\navailable by a blockchain, the RFL Model only provides the reduced property\nthat a transaction is delivered to the users being affected by a transaction.\nWe show that the reduced model's properties still suffice to implement payment\nchannels. By showing that the RFL Model can not only be instantiated by the\nBitcoin blockchain but also by trusted third parties like banks, we show that\nthe reduction widens the design space for the first layer. Further, we show\nthat the stronger property provided by blockchains allows for optimizations\nthat can be used to reduce the time for locking collateral during payments over\nmultiple hops in a payment channel network.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:13:30 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Grundmann", "Matthias", ""], ["Hartenstein", "Hannes", ""]]}, {"id": "2010.08405", "submitter": "Evangelos Mitsakis", "authors": "Areti Kotsi, Tom Lusco, Evangelos Mitsakis, Steve Sill", "title": "Harmonization and interoperability of C-ITS Architectures in Europe and\n  USA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative Intelligent Transportation Systems (C-ITS) constitute\ntechnologies which enable vehicles to communicate with each other and/ or with\nthe infrastructure. C-ITS include systems and services which use different\ncomponents, in order to share and exchange information via diverse\ncommunication interfaces. Since various C-ITS deployment initiatives have taken\nplace during the last years all over the world, the necessity to identify a\nframework, in order to ensure interoperability of C-ITS across borders is\ncurrently in sight constituting a challenging task. Such an approach should\nrely on the deployment of C-ITS services based on common architectures and\nstandards, pursuing harmonization and interoperability. The current paper aims\nto present the work conducted in the context of a United States (US) - European\nCommission (EC) collaboration on the development of harmonized and\ninteroperable C-ITS architectures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:03:49 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Kotsi", "Areti", ""], ["Lusco", "Tom", ""], ["Mitsakis", "Evangelos", ""], ["Sill", "Steve", ""]]}, {"id": "2010.08423", "submitter": "Suhas Thejaswi", "authors": "Suhas Thejaswi, Juho Lauri, Aristides Gionis", "title": "Restless reachability problems in temporal graphs", "comments": "The paper is updated with more illustrations for improving\n  readability and directed towards broader audience (non-expert group)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study a family of reachability problems under waiting-time restrictions in\ntemporal and vertex-colored temporal graphs. Given a temporal graph and a set\nof source vertices, we find the set of vertices that are reachable from a\nsource via a time-respecting path, where the difference in timestamps between\nconsecutive edges is at most a resting time. Given a vertex-colored temporal\ngraph and a multiset query of colors, we find the set of vertices reachable\nfrom a source via a time-respecting path such that the vertex colors of the\npath agree with the multiset query and the difference in timestamps between\nconsecutive edges is at most a resting time. These kind of problems have\nseveral applications in understanding the spread of a disease in a network,\ntracing contacts in epidemic outbreaks, finding signaling pathways in the brain\nnetwork, and recommending tours for tourists.\n  We present an algebraic algorithmic framework based on constrained\nmultilinear sieving for solving the restless reachability problems we propose.\nIn particular, parameterized by the length of a path $k$ sought, we show the\nproblems can be solved in $O(2^k k m \\Delta)$ time and $O(n \\tau)$ space, where\n$n$ is the number of vertices, $m$ the number of edges, $\\Delta$ the maximum\nresting time and $\\tau$ the maximum timestamp of an input temporal graph. In\naddition, we prove that the algorithms presented for the restless reachability\nproblems in vertex-colored temporal graphs are optimal under plausible\ncomplexity-theoretic assumptions. Finally, with an open-source implementation,\nwe demonstrate that our algorithm scales to large graphs with up to one billion\ntemporal edges, despite the problems being NP-hard. Specifically, we present\nextensive experiments to evaluate our scalability claims both on synthetic and\nreal-world graphs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 14:40:26 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 21:05:27 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 11:06:14 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Thejaswi", "Suhas", ""], ["Lauri", "Juho", ""], ["Gionis", "Aristides", ""]]}, {"id": "2010.08526", "submitter": "Ariful Azad", "authors": "Md Taufique Hussain, Oguz Selvitopi, Aydin Bulu\\c{c}, Ariful Azad", "title": "Communication-Avoiding and Memory-Constrained Sparse Matrix-Matrix\n  Multiplication at Extreme Scale", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-matrix multiplication (SpGEMM) is a widely used kernel in\nvarious graph, scientific computing and machine learning algorithms. In this\npaper, we consider SpGEMMs performed on hundreds of thousands of processors\ngenerating trillions of nonzeros in the output matrix. Distributed SpGEMM at\nthis extreme scale faces two key challenges: (1) high communication cost and\n(2) inadequate memory to generate the output. We address these challenges with\nan integrated communication-avoiding and memory-constrained SpGEMM algorithm\nthat scales to 262,144 cores (more than 1 million hardware threads) and can\nmultiply sparse matrices of any size as long as inputs and a fraction of output\nfit in the aggregated memory. As we go from 16,384 cores to 262,144 cores on a\nCray XC40 supercomputer, the new SpGEMM algorithm runs 10x faster when\nmultiplying large-scale protein-similarity matrices.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:37:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hussain", "Md Taufique", ""], ["Selvitopi", "Oguz", ""], ["Bulu\u00e7", "Aydin", ""], ["Azad", "Ariful", ""]]}, {"id": "2010.08556", "submitter": "Dylan Rankin", "authors": "Dylan Sheldon Rankin, Jeffrey Krupa, Philip Harris, Maria Acosta\n  Flechas, Burt Holzman, Thomas Klijnsma, Kevin Pedro, Nhan Tran, Scott Hauck,\n  Shih-Chieh Hsu, Matthew Trahms, Kelvin Lin, Yu Lou, Ta-Wei Ho, Javier Duarte,\n  Mia Liu", "title": "FPGAs-as-a-Service Toolkit (FaaST)", "comments": "10 pages, 7 figures, to appear in proceedings of the 2020 IEEE/ACM\n  International Workshop on Heterogeneous High-performance Reconfigurable\n  Computing", "journal-ref": null, "doi": null, "report-no": "FERMILAB-CONF-20-426-SCD", "categories": "physics.comp-ph cs.DC hep-ex physics.data-an physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing needs for high energy physics are already intensive and are\nexpected to increase drastically in the coming years. In this context,\nheterogeneous computing, specifically as-a-service computing, has the potential\nfor significant gains over traditional computing models. Although previous\nstudies and packages in the field of heterogeneous computing have focused on\nGPUs as accelerators, FPGAs are an extremely promising option as well. A series\nof workflows are developed to establish the performance capabilities of FPGAs\nas a service. Multiple different devices and a range of algorithms for use in\nhigh energy physics are studied. For a small, dense network, the throughput can\nbe improved by an order of magnitude with respect to GPUs as a service. For\nlarge convolutional networks, the throughput is found to be comparable to GPUs\nas a service. This work represents the first open-source FPGAs-as-a-service\ntoolkit.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:00:01 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Rankin", "Dylan Sheldon", ""], ["Krupa", "Jeffrey", ""], ["Harris", "Philip", ""], ["Flechas", "Maria Acosta", ""], ["Holzman", "Burt", ""], ["Klijnsma", "Thomas", ""], ["Pedro", "Kevin", ""], ["Tran", "Nhan", ""], ["Hauck", "Scott", ""], ["Hsu", "Shih-Chieh", ""], ["Trahms", "Matthew", ""], ["Lin", "Kelvin", ""], ["Lou", "Yu", ""], ["Ho", "Ta-Wei", ""], ["Duarte", "Javier", ""], ["Liu", "Mia", ""]]}, {"id": "2010.08695", "submitter": "Kartik Lakhotia", "authors": "Kartik Lakhotia, Rajgopal Kannan, Viktor Prasanna, Cesar A. F. De Rose", "title": "RECEIPT: REfine CoarsE-grained IndePendent Tasks for Parallel Tip\n  decomposition of Bipartite Graphs", "comments": "To appear in Proceedings of VLDB Vol. 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tip decomposition is a crucial kernel for mining dense subgraphs in bipartite\nnetworks, with applications in spam detection, analysis of affiliation networks\netc. It creates a hierarchy of vertex-induced subgraphs with varying densities\ndetermined by the participation of vertices in butterflies (2,2-bicliques). To\nbuild the hierarchy, existing algorithms iteratively follow a\ndelete-update(peeling) process: deleting vertices with the minimum number of\nbutterflies and correspondingly updating the butterfly count of their 2-hop\nneighbors. The need to explore 2-hop neighborhood renders tip-decomposition\ncomputationally very expensive. Furthermore, the inherent sequentiality in\npeeling only minimum butterfly vertices makes derived parallel algorithms prone\nto heavy synchronization.\n  In this paper, we propose a novel parallel tip-decomposition algorithm --\nREfine CoarsE-grained Independent Tasks (RECEIPT) that relaxes the peeling\norder restrictions by partitioning the vertices into multiple independent\nsubsets that can be concurrently peeled. This enables RECEIPT to simultaneously\nachieve a high degree of parallelism and dramatic reduction in\nsynchronizations. Further, RECEIPT employs a hybrid peeling strategy along with\nother optimizations that drastically reduce the amount of wedge exploration and\nexecution time.\n  We perform detailed experimental evaluation of RECEIPT on a shared-memory\nmulticore server. It can process some of the largest publicly available\nbipartite datasets orders of magnitude faster than the state-of-the-art\nalgorithms -- achieving up to 1100x and 64x reduction in the number of thread\nsynchronizations and traversed wedges, respectively. Using 36 threads, RECEIPT\ncan provide up to 17.1x self-relative speedup. Our implementation of RECEIPT is\navailable at https://github.com/kartiklakhotia/RECEIPT.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 01:57:49 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Lakhotia", "Kartik", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor", ""], ["De Rose", "Cesar A. F.", ""]]}, {"id": "2010.08730", "submitter": "Ziyao Liu", "authors": "Jiale Guo, Ziyao Liu, Kwok-Yan Lam, Jun Zhao, Yiqiang Chen, Chaoping\n  Xing", "title": "Secure Weighted Aggregation for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive adoption of Internet-connected digital services has led to a\ngrowing concern in the personal data privacy of their customers. On the other\nhand, machine learning (ML) techniques have been widely adopted by digital\nservice providers to improve operational productivity and customer\nsatisfaction. ML inevitably accesses and processes users' personal data, which\ncould potentially breach the relevant privacy protection regulations if not\nperformed carefully. The situation is exacerbated by the cloud-based\nimplementation of digital services when user data are captured and stored in\ndistributed locations, hence aggregation of the user data for ML could be a\nserious breach of privacy regulations. In this backdrop, Federated Learning\n(FL) is an emerging area that allows ML on distributed data without the data\nleaving their stored location. However, depending on the nature of the digital\nservices, data captured at different locations may carry different significance\nto the business operation, hence a weighted aggregation will be highly\ndesirable for enhancing the quality of the FL-learned model. Furthermore, to\nprevent leakage of user data from the aggregated gradients, cryptographic\nmechanisms are needed to allow secure aggregation of FL. In this paper, we\npropose a privacy-enhanced FL scheme for supporting secure weighted\naggregation. Besides, by devising a verification protocol based on\nZero-Knowledge Proof (ZKP), the proposed scheme is capable of guarding against\nfraudulent messages from FL participants. Experimental results show that our\nscheme is practical and secure. Compared to existing FL approaches, our scheme\nachieves secure weighted aggregation with an additional security guarantee\nagainst fraudulent messages with an affordable 1.2 times runtime overheads and\n1.3 times communication costs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 07:13:06 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 18:42:30 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Guo", "Jiale", ""], ["Liu", "Ziyao", ""], ["Lam", "Kwok-Yan", ""], ["Zhao", "Jun", ""], ["Chen", "Yiqiang", ""], ["Xing", "Chaoping", ""]]}, {"id": "2010.08774", "submitter": "Nick Brown", "authors": "Nick Brown, Rupert Nash, Gordon Gibb, Bianca Prodan, Max Kontak,\n  Vyacheslav Olshevsky, and Wei Der Chien", "title": "The role of interactive super-computing in using HPC for urgent decision\n  making", "comments": "Pre-print in International Conference on High Performance Computing\n  (pp. 528-540). Springer, Cham", "journal-ref": "In International Conference on High Performance Computing (pp.\n  528-540). 2019. Springer, Cham", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advances are creating exciting new opportunities that have the\npotential to move HPC well beyond traditional computational workloads. In this\npaper we focus on the potential for HPC to be instrumental in responding to\ndisasters such as wildfires, hurricanes, extreme flooding, earthquakes,\ntsunamis, winter weather conditions, and accidents. Driven by the VESTEC EU\nfunded H2020 project, our research looks to prove HPC as a tool not only\ncapable of simulating disasters once they have happened, but also one which is\nable to operate in a responsive mode, supporting disaster response teams making\nurgent decisions in real-time. Whilst this has the potential to revolutionise\ndisaster response, it requires the ability to drive HPC interactively, both\nfrom the user's perspective and also based upon the arrival of data. As such\ninteractivity is a critical component in enabling HPC to be exploited in the\nrole of supporting disaster response teams so that urgent decision makers can\nmake the correct decision first time, every time.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 12:15:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Brown", "Nick", ""], ["Nash", "Rupert", ""], ["Gibb", "Gordon", ""], ["Prodan", "Bianca", ""], ["Kontak", "Max", ""], ["Olshevsky", "Vyacheslav", ""], ["Der Chien", "Wei", ""]]}, {"id": "2010.08781", "submitter": "Nick Brown", "authors": "Ludovic A.R. Capelli, Zhenjiang Hu, Timothy A.K. Zakian, Nick Brown,\n  J. Mark Bull", "title": "iPregel: Vertex-centric programmability vs memory efficiency and\n  performance, why choose?", "comments": null, "journal-ref": "In Parallel Computing. 2019 Aug 1;86:45-56", "doi": "10.1016/j.parco.2019.04.005", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vertex-centric programming model, designed to improve the programmability\nin graph processing application writing, has attracted great attention over the\nyears. However, shared memory frameworks that implement the vertex-centric\ninterface all expose a common tradeoff: programmability against memory\nefficiency and performance.\n  Our approach, iPregel, preserves vertex-centric programmability, while\nimplementing optimisations for performance, and designing these so they are\ntransparent to a user's application code, hence not impacting programmability.\nIn this paper, we evaluate iPregel against FemtoGraph, whose characteristics\nare identical, an asynchronous counterpart GraphChi and the\nvertex-subset-centric framework Ligra. Our experiments include three of the\nmost popular vertex-centric benchmark applications over 4 real-world publicly\naccessible graphs, which cover orders of magnitude between a million to a\nbillion edges, measuring execution time and peak memory usage. Finally, we\nevaluate the programmability of each framework by comparing against Google's\noriginal Pregel framework.\n  Experiments demonstrate that iPregel, like FemtoGraph, does not sacrifice\nvertex-centric programmability for additional performance and memory efficiency\noptimisations, which contrasts against GraphChi and Ligra. Sacrificing\nvertex-centric programmability allowed the latter to benefit from substantial\nperformance and memory efficiency gains. We demonstrate that iPregel is up to\n2300 times faster than FemtoGraph, as well as generating a memory footprint up\nto 100 times smaller. Ligra and GraphChi are up to 17000 and 700 times faster\nthan FemtoGraph but, when comparing against iPregel, this maximum speed-up\ndrops to 10. Furthermore, with PageRank, iPregel is the fastest overall. For\nmemory efficiency, iPregel provides the same memory efficiency as Ligra and 3\nto 6 times lighter than GraphChi on average.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 12:40:52 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Capelli", "Ludovic A. R.", ""], ["Hu", "Zhenjiang", ""], ["Zakian", "Timothy A. K.", ""], ["Brown", "Nick", ""], ["Bull", "J. Mark", ""]]}, {"id": "2010.08899", "submitter": "Vipul Gupta", "authors": "Vipul Gupta, Dhruv Choudhary, Ping Tak Peter Tang, Xiaohan Wei, Xing\n  Wang, Yuzhen Huang, Arun Kejariwal, Kannan Ramchandran, Michael W. Mahoney", "title": "Training Recommender Systems at Scale: Communication-Efficient Model and\n  Data Parallelism", "comments": "27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n  (KDD 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider hybrid parallelism -- a paradigm that employs both\nData Parallelism (DP) and Model Parallelism (MP) -- to scale distributed\ntraining of large recommendation models. We propose a compression framework\ncalled Dynamic Communication Thresholding (DCT) for communication-efficient\nhybrid training. DCT filters the entities to be communicated across the network\nthrough a simple hard-thresholding function, allowing only the most relevant\ninformation to pass through. For communication efficient DP, DCT compresses the\nparameter gradients sent to the parameter server during model synchronization.\nThe threshold is updated only once every few thousand iterations to reduce the\ncomputational overhead of compression. For communication efficient MP, DCT\nincorporates a novel technique to compress the activations and gradients sent\nacross the network during the forward and backward propagation, respectively.\nThis is done by identifying and updating only the most relevant neurons of the\nneural network for each training sample in the data. We evaluate DCT on\npublicly available natural language processing and recommender models and\ndatasets, as well as recommendation systems used in production at Facebook. DCT\nreduces communication by at least $100\\times$ and $20\\times$ during DP and MP,\nrespectively. The algorithm has been deployed in production, and it improves\nend-to-end training time for a state-of-the-art industrial recommender model by\n37\\%, without any loss in performance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 01:44:42 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 08:23:19 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Gupta", "Vipul", ""], ["Choudhary", "Dhruv", ""], ["Tang", "Ping Tak Peter", ""], ["Wei", "Xiaohan", ""], ["Wang", "Xing", ""], ["Huang", "Yuzhen", ""], ["Kejariwal", "Arun", ""], ["Ramchandran", "Kannan", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2010.08916", "submitter": "Ruoshi Li", "authors": "Ruoshi Li, Hongjing Huang, Zeke Wang, Zhiyuan Shao, Xiaofei Liao, Hai\n  Jin", "title": "Optimizing Memory Performance of Xilinx FPGAs under Vitis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenty of research efforts have been devoted to FPGA-based acceleration, due\nto its low latency and high energy efficiency. However, using the original\nlow-level hardware description languages like Verilog to program FPGAs requires\ngenerally good knowledge of hardware design details and hand-on experiences.\nFortunately, the FPGA community intends to address this low programmability\nissues. For example, , with the intention that programming FPGAs is just as\neasy as programming GPUs. Even though Vitis is proven to increase\nprogrammability, we cannot directly obtain high performance without careful\ndesign regarding hardware pipeline and memory subsystem.In this paper, we focus\non the memory subsystem, comprehensively and systematically benchmarking the\neffect of optimization methods on memory performance. Upon benchmarking, we\nquantitatively analyze the typical memory access patterns for a broad range of\napplications, including AI, HPC, and database. Further, we also provide the\ncorresponding optimization direction for each memory access pattern so as to\nimprove overall performance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 04:14:03 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Ruoshi", ""], ["Huang", "Hongjing", ""], ["Wang", "Zeke", ""], ["Shao", "Zhiyuan", ""], ["Liao", "Xiaofei", ""], ["Jin", "Hai", ""]]}, {"id": "2010.08929", "submitter": "Yuichi Sudo", "authors": "Yuichi Sudo, Fukuhito Ooshita, Sayaka Kamei", "title": "Self-stabilizing Graph Exploration by a Single Agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give two self-stabilizing algorithms that solve graph\nexploration by a single (mobile) agent. The proposed algorithms are\nself-stabilizing: the agent running each of the algorithms visits all nodes\nstarting from any initial configuration where the state of the agent and the\nstates of all nodes are arbitrary and the agent is located at an arbitrary\nnode. We evaluate algorithms with two metrics, the cover time, that is the\nnumber of moves required to visit all nodes, and the amount of space to store\nthe state of the agent and the states of the nodes. The first algorithm is a\nrandomized one. The cover time of this algorithm is optimal (\\ie $O(m)$) in\nexpectation and it uses $O(\\log n)$ bits for both the agent and each node,\nwhere $n$ and $m$ are the number of agents and the number of edges in a given\ngraph, respectively. The second algorithm is deterministic. The cover time is\n$O(m + nD)$, where $D$ is the diameter of the graph. It uses $O(\\log n)$ bits\nfor the agent-memory and $O(\\delta + \\log n)$ bits for the memory of each node\nwith degree $\\delta$. We require the knowledge of an upper bound on $n$ (resp.\n$D$) for the first (resp.~the second) algorithm. However, this is a weak\nassumption from a practical point of view because the knowledge of any value\n$\\ge n$ (resp.~$\\ge D$) in $O(poly(n))$ is sufficient to obtain the above time\nand space complexity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 07:05:26 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sudo", "Yuichi", ""], ["Ooshita", "Fukuhito", ""], ["Kamei", "Sayaka", ""]]}, {"id": "2010.08933", "submitter": "Belal Sababha", "authors": "Mahmoud I. Banat, Belal H. Sababha, Sami Al-Hamdan", "title": "A CAD-Based tool for fault tolerant distributed embedded systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability and availability analysis are essential in dependable critical\nembedded systems. The classical implementation of dependability for an embedded\nsystem relies on merging both fundamental structures with the required\ndependability techniques to form one composite structure. The separation of the\nbasic system components from the dependability components, reduces complexity\nand improves the design. The goal of this work is to assist implementing\nreconfiguration-based fault tolerance in safety-critical embedded systems\napplications. The primary intention is to reduce the repair time in order to\nenhance fault tolerance and produce dependable embedded systems. The proposed\nsolution is a dedicated CAD-tool designed to generate a reference strategy for\nthe system manager of a distributed embedded system to control and\nautomatically reconfigure the processing elements of the system. The proposed\ntool auto-generates program codes to be executed by a system manager to govern\nthe DES. It also computes different reliability solutions with necessary\nsupporting calculated parameters and graphs sorted to support the fault\ntolerance design of the system. The proposed tool can be used to simulate\npossible configurations based on the desired degrees of faults and system\nreliability. The graphical interface of the tool is unique and hides the\ncomplexity of the systems underneath. A comparison with a similar tool is\npresented.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 07:45:50 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 15:42:41 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Banat", "Mahmoud I.", ""], ["Sababha", "Belal H.", ""], ["Al-Hamdan", "Sami", ""]]}, {"id": "2010.09007", "submitter": "Shuai Zhang", "authors": "Shuai Zhang, Zite Jiang, Xingzhong Hou, Zhen Guan, Mengting Yuan and\n  Haihang You", "title": "An Efficient and Balanced Graph Partition Algorithm for the\n  Subgraph-Centric Programming Model on Large-scale Power-law Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subgraph-centric programming model is a promising approach and has been\napplied in many state-of-the-art distributed graph computing frameworks.\nHowever, traditional graph partition algorithms have significant difficulties\nin processing large-scale power-law graphs. The major problem is the\ncommunication bottleneck found in many subgraph-centric frameworks. Detailed\nanalysis indicates that the communication bottleneck is caused by the huge\ncommunication volume or the extreme message imbalance among partitioned\nsubgraphs. The traditional partition algorithms do not consider both factors at\nthe same time, especially on power-law graphs.\n  In this paper, we propose a novel efficient and balanced vertex-cut graph\npartition algorithm (EBV) which grants appropriate weights to the overall\ncommunication cost and communication balance. We observe that the number of\nreplicated vertices and the balance of edge and vertex assignment have a great\ninfluence on communication patterns of distributed subgraph-centric frameworks,\nwhich further affect the overall performance. Based on this insight, We design\nan evaluation function that quantifies the proportion of replicated vertices\nand the balance of edges and vertices assignments as important parameters.\nBesides, we sort the order of edge processing by the sum of end-vertices'\ndegrees from small to large. Experiments show that EBV reduces replication\nfactor and communication by at least 21.8% and 23.7% respectively than other\nself-based partition algorithms. When deployed in the subgraph-centric\nframework, it reduces the running time on power-law graphs by an average of\n16.8% compared with the state-of-the-art partition algorithm. Our results\nindicate that EBV has a great potential in improving the performance of\nsubgraph-centric frameworks for the parallel large-scale power-law graph\nprocessing.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 15:35:30 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 02:54:44 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zhang", "Shuai", ""], ["Jiang", "Zite", ""], ["Hou", "Xingzhong", ""], ["Guan", "Zhen", ""], ["Yuan", "Mengting", ""], ["You", "Haihang", ""]]}, {"id": "2010.09012", "submitter": "Jiqing Chang", "authors": "Jiqing Chang, Jin Wang, Kejie Lu, Lingzhi Li, Fei Gu, Jianping Wang", "title": "Joint Storage Allocation and Computation Design for Private Edge\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, edge computing (EC) has attracted great attention for its\nhigh-speed computing and low-latency characteristics. However, there are many\nchallenges in the implementation of EC. Firstly, user's privacy has been raised\nas a major concern because the edge devices may be untrustworthy. In the case\nof Private Edge Computing (PEC), a user wants to compute a matrix\nmultiplication between its local matrix and one of the matrices in a library,\nwhich has been redundantly stored in edge devices. When utilizing resources of\nedge devices, the privacy requires that each edge device cannot know which\nmatrix stored on it is desired by the user for the multiplication. Secondly,\nedge devices usually have limited communication and storage resources, which\nmakes it impossible for them to store all matrices in the library. In this\npaper, we consider the limited resources of edge devices and propose an unified\nframework for PEC. Within the framework, we study two highly-coupled problems,\n(1) storage allocation, that determines which matrices are stored on each edge\ndevice, and (2) computation design, that determines which matrices (or linear\ncombinations of them) in each edge device are selected to participate in the\ncomputing process with the privacy consideration. Specifically, we give a\ngeneral storage allocation scheme and then design two feasible private\ncomputation schemes, i.e., General Private Computation (GPC) scheme and Private\nCoded Computation (PCC) scheme. In particular, GPC can be applied in general\ncase and PCC can only be applied in special cases, while PCC achieves less\ncommunication load. We theoretically analyze the proposed computing schemes and\ncompare them with other schemes. Finally, we conduct extensive simulations to\nshow the effectiveness of the proposed schemes.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 15:55:07 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chang", "Jiqing", ""], ["Wang", "Jin", ""], ["Lu", "Kejie", ""], ["Li", "Lingzhi", ""], ["Gu", "Fei", ""], ["Wang", "Jianping", ""]]}, {"id": "2010.09025", "submitter": "Maciej Besta", "authors": "Maciej Besta, Torsten Hoefler", "title": "Fault Tolerance for Remote Memory Access Programming Models", "comments": "Best Paper Finalist (3/130)", "journal-ref": "Proceedings of the 23rd ACM International Symposium on\n  High-Performance Parallel and Distributed Computing (HPDC'14), 2014", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote Memory Access (RMA) is an emerging mechanism for programming\nhigh-performance computers and datacenters. However, little work exists on\nresilience schemes for RMA-based applications and systems. In this paper we\nanalyze fault tolerance for RMA and show that it is fundamentally different\nfrom resilience mechanisms targeting the message passing (MP) model. We design\na model for reasoning about fault tolerance for RMA, addressing both flat and\nhierarchical hardware. We use this model to construct several highly-scalable\nmechanisms that provide efficient low-overhead in-memory checkpointing,\ntransparent logging of remote memory accesses, and a scheme for transparent\nrecovery of failed processes. Our protocols take into account diminishing\namounts of memory per core, one of major features of future exascale machines.\nThe implementation of our fault-tolerance scheme entails negligible additional\noverheads. Our reliability model shows that in-memory checkpointing and logging\nprovide high resilience. This study enables highly-scalable resilience\nmechanisms for RMA and fills a research gap between fault tolerance and\nemerging RMA programming models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 16:46:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.09055", "submitter": "Paritosh Ramanan", "authors": "Paritosh Ramanan, Murat Yildirim, Nagi Gebraeel, Edmond Chow", "title": "Large-Scale Maintenance and Unit Commitment: A Decentralized Subgradient\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unit Commitment (UC) is a fundamental problem in power system operations.\nWhen coupled with generation maintenance, the joint optimization problem poses\nsignificant computational challenges due to coupling constraints linking\nmaintenance and UC decisions. Obviously, these challenges grow with the size of\nthe network. With the introduction of sensors for monitoring generator health\nand condition-based maintenance(CBM), these challenges have been magnified.\nADMM-based decentralized methods have shown promise in solving large-scale UC\nproblems, especially in vertically integrated power systems. However, in their\ncurrent form, these methods fail to deliver similar computational performance\nand scalability when considering the joint UC and CBM problem.\n  This paper provides a novel decentralized optimization framework for solving\nlarge-scale, joint UC and CBM problems. Our approach relies on the novel use of\nthe subgradient method to temporally decouple various subproblems of the\nADMM-based formulation of the joint problem along the maintenance horizon. By\neffectively utilizing multithreading, our decentralized subgradient approach\ndelivers superior computational performance and eliminates the need to move\nsensor data thereby alleviating privacy and security concerns. Using\nexperiments on large scale test cases, we show that our framework can provide a\nspeedup of upto 50x as compared to various state of the art benchmarks without\ncompromising on solution quality.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 18:19:02 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 08:51:26 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 18:23:27 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ramanan", "Paritosh", ""], ["Yildirim", "Murat", ""], ["Gebraeel", "Nagi", ""], ["Chow", "Edmond", ""]]}, {"id": "2010.09086", "submitter": "Paritosh Ramanan", "authors": "Paritosh Ramanan, Dan Li, Nagi Gebraeel", "title": "Blockchain Based Decentralized Cyber Attack Detection for Large Scale\n  Power Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale power systems are comprised of regional utilities with IIoT\nenabled assets that stream sensor readings in real time. In order to detect\ncyberattacks, the globally acquired, real time sensor data needs to be analyzed\nin a centralized fashion. However, owing to operational constraints, such a\ncentralized sharing mechanism turns out to be a major obstacle. In this paper,\nwe propose a blockchain based decentralized framework for detecting coordinated\nreplay attacks with full privacy of sensor data. We develop a Bayesian\ninference mechanism employing locally reported attack probabilities that is\ntailor made for a blockchain framework. We compare our framework to a\ntraditional decentralized algorithm based on the broadcast gossip framework\nboth theoretically as well as empirically. With the help of experiments on a\nprivate Ethereum blockchain, we show that our approach achieves good detection\nquality and significantly outperforms gossip driven approaches in terms of\naccuracy, timeliness and scalability.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 19:51:09 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ramanan", "Paritosh", ""], ["Li", "Dan", ""], ["Gebraeel", "Nagi", ""]]}, {"id": "2010.09099", "submitter": "Paritosh Ramanan", "authors": "Paritosh Ramanan, Murat Yildirim, Nagi Gebraeel, Edmond Chow", "title": "Decentralized and Secure Generation Maintenance with Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized methods are gaining popularity for data-driven models in power\nsystems as they offer significant computational scalability while guaranteeing\nfull data ownership by utility stakeholders. However, decentralized methods\nstill require sharing information about network flow estimates over public\nfacing communication channels, which raises privacy concerns. In this paper we\npropose a differential privacy driven approach geared towards decentralized\nformulations of mixed integer operations and maintenance optimization problems\nthat protects network flow estimates. We prove strong privacy guarantees by\nleveraging the linear relationship between the phase angles and the flow. To\naddress the challenges associated with the mixed integer and dynamic nature of\nthe problem, we introduce an exponential moving average based consensus\nmechanism to enhance convergence, coupled with a control chart based\nconvergence criteria to improve stability. Our experimental results obtained on\nthe IEEE 118 bus case demonstrate that our privacy preserving approach yields\nsolution qualities on par with benchmark methods without differential privacy.\nTo demonstrate the computational robustness of our method, we conduct\nexperiments using a wide range of noise levels and operational scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 20:45:14 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ramanan", "Paritosh", ""], ["Yildirim", "Murat", ""], ["Gebraeel", "Nagi", ""], ["Chow", "Edmond", ""]]}, {"id": "2010.09112", "submitter": "Ivan Homoliak Ph.D.", "authors": "Sarad Venugopalan, Ivan Homoliak, Zengpeng Li, Pawel Szalachowski", "title": "BBB-Voting: 1-out-of-k Blockchain-Based Boardroom Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voting is a means to agree on a collective decision based on available\nchoices (e.g., candidates), where participants (voters) agree to abide by their\noutcome. To improve some features of e-voting, decentralized solutions based on\na blockchain can be employed, where the blockchain represents a public bulletin\nboard that in contrast to a centralized bulletin board provides $100\\%$\navailability and censorship resistance. A blockchain ensures that all entities\nin the voting system have the same view of the actions made by others due to\nits immutable and append-only log. The existing blockchain-based boardroom\nvoting solution called Open Voting Network (OVN) provides the privacy of votes\nand perfect ballot secrecy, but it supports only two candidates. We present\nBBB-Voting, an equivalent blockchain-based approach for decentralized voting\nthan OVN, but in contrast to it, BBB-Voting supports 1-out-of-$k$ choices and\nprovides a fault tolerance mechanism that enables recovery from stalling\nparticipants. We provide a cost-optimized implementation using Ethereum, which\nwe compare with OVN and show that our work decreases the costs for voters by\n$13.5\\%$ in terms of gas consumption. Next, we outline the extension of our\nimplementation scaling to magnitudes higher number of participants than in a\nboardroom voting, while preserving the costs paid by the authority and\nparticipants -- we made proof-of-concept experiments with up to 1000\nparticipants.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 21:34:58 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 20:51:18 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 14:21:12 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Venugopalan", "Sarad", ""], ["Homoliak", "Ivan", ""], ["Li", "Zengpeng", ""], ["Szalachowski", "Pawel", ""]]}, {"id": "2010.09135", "submitter": "Maciej Besta", "authors": "Maciej Besta, Torsten Hoefler", "title": "Accelerating Irregular Computations with Hardware Transactional Memory\n  and Active Messages", "comments": "Best Paper Award at ACM HPDC'15 (1/116)", "journal-ref": "Proceedings of the 24th ACM International Symposium on\n  High-Performance Parallel and Distributed Computing (HPDC'15), 2015", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Atomic Active Messages (AAM), a mechanism that accelerates\nirregular graph computations on both shared- and distributed-memory machines.\nThe key idea behind AAM is that hardware transactional memory (HTM) can be used\nfor simple and efficient processing of irregular structures in highly parallel\nenvironments. We illustrate techniques such as coarsening and coalescing that\nenable hardware transactions to considerably accelerate graph processing.We\nconduct a detailed performance analysis of AAM on Intel Haswell and IBM Blue\nGene/Q and we illustrate various performance tradeoffs between different HTM\nparameters that impact the efficiency of graph processing. AAM can be used to\nimplement abstractions offered by existing programming models and to improve\nthe performance of irregular graph processing codes such as Graph500 or Galois.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 23:10:27 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 22:40:03 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.09258", "submitter": "Sheng Shen", "authors": "Sheng Shen, Tianqing Zhu, Di Wu, Wei Wang, Wanlei Zhou", "title": "From Distributed Machine Learning To Federated Learning: In The View Of\n  Data Privacy And Security", "comments": "21 pages, 4 figures, 4 tables, 76 references", "journal-ref": "Concurrency and Computation Practice and Experience, 2020, 1-19", "doi": "10.1002/cpe.6002", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is an improved version of distributed machine learning\nthat further offloads operations which would usually be performed by a central\nserver. The server becomes more like an assistant coordinating clients to work\ntogether rather than micro-managing the workforce as in traditional DML. One of\nthe greatest advantages of federated learning is the additional privacy and\nsecurity guarantees it affords. Federated learning architecture relies on smart\ndevices, such as smartphones and IoT sensors, that collect and process their\nown data, so sensitive information never has to leave the client device.\nRather, clients train a sub-model locally and send an encrypted update to the\ncentral server for aggregation into the global model. These strong privacy\nguarantees make federated learning an attractive choice in a world where data\nbreaches and information theft are common and serious threats. This survey\noutlines the landscape and latest developments in data privacy and security for\nfederated learning. We identify the different mechanisms used to provide\nprivacy and security, such as differential privacy, secure multi-party\ncomputation and secure aggregation. We also survey the current attack models,\nidentifying the areas of vulnerability and the strategies adversaries use to\npenetrate federated systems. The survey concludes with a discussion on the open\nchallenges and potential directions of future work in this increasingly popular\nlearning paradigm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:00:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Shen", "Sheng", ""], ["Zhu", "Tianqing", ""], ["Wu", "Di", ""], ["Wang", "Wei", ""], ["Zhou", "Wanlei", ""]]}, {"id": "2010.09421", "submitter": "Dimitrios Amaxilatis", "authors": "Dimitrios Amaxilatis, Christos Tselios, Orestis Akrivopoulos and\n  Ioannis Chatzigiannakis", "title": "On the design of a Fog computing-based, driving behaviour monitoring\n  framework", "comments": null, "journal-ref": "CAMAD 2019", "doi": "10.1109/CAMAD.2019.8858432", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological improvements in vehicle manufacturing may greatly\nimprove safety however, the individuals' driving behaviour still remains a\nfactor of paramount importance with aggressiveness, lack of focus and\ncarelessness being the main cause of the majority of traffic incidents. The\nimminent deployment of 5G networking infrastructure, paired with the advent of\nFog computing and the establishment of the Internet of Things (IoT) as a\nreliable and cost-effective service delivery framework may provide the means\nfor the deployment of an accurate driving monitoring solution which could be\nutilized to further understand the underlying reasons of peculiar road\nbehaviour, as well as its correlation to the driver's physiological state, the\nvehicle condition and certain environmental parameters. This paper presents\nsome of the fundamental attributes of Fog computing along with the functional\nrequirements of a driving behaviour monitoring framework, followed by its high\nlevel architecture blueprint and the description of the prototype\nimplementation process.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:30:56 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Amaxilatis", "Dimitrios", ""], ["Tselios", "Christos", ""], ["Akrivopoulos", "Orestis", ""], ["Chatzigiannakis", "Ioannis", ""]]}, {"id": "2010.09422", "submitter": "Dimitrios Amaxilatis", "authors": "Christos Tselios, Stavros Nousias, Dimitris Bitzas, Dimitrios\n  Amaxilatis, Orestis Akrivopoulos, Aris S. Lalos, Konstantinos Moustakas, and\n  Ioannis Chatzigiannakis", "title": "Enhancing an eco-driving gamification platform through wearable and\n  vehicle sensor data integration", "comments": null, "journal-ref": "AmI 2019", "doi": "10.1007/978-3-030-34255-5_26", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As road transportation has been identified as a major contributor of\nenvironmental pollution, motivating individuals to adopt a more eco-friendly\ndriving style could have a substantial ecological as well as financial benefit.\nWith gamification being an effective tool towards guiding targeted behavioural\nchanges, the development of realistic frameworks delivering a high end user\nexperience, becomes a topic of active research. This paper presents a series of\nenhancements introduced to an eco-driving gamification platform by the\nintegration of additional wearable and vehicle-oriented sensing data sources,\nleading to a much more realistic evaluation of the context of a driving\nsession.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:31:17 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tselios", "Christos", ""], ["Nousias", "Stavros", ""], ["Bitzas", "Dimitris", ""], ["Amaxilatis", "Dimitrios", ""], ["Akrivopoulos", "Orestis", ""], ["Lalos", "Aris S.", ""], ["Moustakas", "Konstantinos", ""], ["Chatzigiannakis", "Ioannis", ""]]}, {"id": "2010.09454", "submitter": "Joshua Hoke Davis", "authors": "Joshua Hoke Davis, Christopher Daley, Swaroop Pophale, Thomas Huber,\n  Sunita Chandrasekaran, Nicholas J. Wright", "title": "Performance Assessment of OpenMP Compilers Targeting NVIDIA V100 GPUs", "comments": "20 pages, 7 figures, accepted in WACCPD 2020 at SC20 (under\n  publication with Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous systems are becoming increasingly prevalent. In order to\nexploit the rich compute resources of such systems, robust programming models\nare needed for application developers to seamlessly migrate legacy code from\ntoday's systems to tomorrow's. Over the past decade and more, directives have\nbeen established as one of the promising paths to tackle programmatic\nchallenges on emerging systems. This work focuses on applying and demonstrating\nOpenMP offloading directives on five proxy applications. We observe that the\nperformance varies widely from one compiler to the other; a crucial aspect of\nour work is reporting best practices to application developers who use OpenMP\noffloading compilers. While some issues can be worked around by the developer,\nthere are other issues that must be reported to the compiler vendors. By\nrestructuring OpenMP offloading directives, we gain an 18x speedup for the su3\nproxy application on NERSC's Cori system when using the Clang compiler, and a\n15.7x speedup by switching max reductions to add reductions in the laplace\nmini-app when using the Cray-llvm compiler on Cori.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:05:44 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 13:45:20 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 20:26:15 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Davis", "Joshua Hoke", ""], ["Daley", "Christopher", ""], ["Pophale", "Swaroop", ""], ["Huber", "Thomas", ""], ["Chandrasekaran", "Sunita", ""], ["Wright", "Nicholas J.", ""]]}, {"id": "2010.09667", "submitter": "Kaustav Bose", "authors": "Kaustav Bose, Archak Das, Buddhadeb Sau", "title": "Pattern Formation by Robots with Inaccurate Movements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textsc{Arbitrary Pattern Formation} is a fundamental problem in autonomous\nmobile robot systems. The problem asks to design a distributed algorithm that\nmoves a team of autonomous, anonymous and identical mobile robots to form any\narbitrary pattern $F$ given as input. In this paper, we study the problem for\nrobots whose movements can be inaccurate. Our movement model assumes errors in\nboth direction and extent of the intended movement. Forming the given pattern\nexactly is not possible in this setting. So we require that the robots must\nform a configuration which is close to the given pattern $F$. We call this the\n\\textsc{Approximate Arbitrary Pattern Formation} problem. We show that with no\nagreement in coordinate system, the problem is unsolvable, even by fully\nsynchronous robots, if the initial configuration 1) has rotational symmetry and\nthere is no robot at the center of rotation or 2) has reflectional symmetry and\nthere is no robot on the reflection axis. From all other initial\nconfigurations, we solve the problem by 1) oblivious, silent and\nsemi-synchronous robots and 2) oblivious, asynchronous robots that can\ncommunicate using externally visible lights.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:02:08 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 14:49:50 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 10:21:09 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Bose", "Kaustav", ""], ["Das", "Archak", ""], ["Sau", "Buddhadeb", ""]]}, {"id": "2010.09687", "submitter": "Pankesh Patel", "authors": "Vatsal Patel and Sarth Kanani and Tapan Pathak and Pankesh Patel and\n  Muhammad Intizar Ali and John Breslin", "title": "A Demonstration of Smart Doorbell Design Using Federated Deep Learning", "comments": "6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Smart doorbells have been playing an important role in protecting our modern\nhomes. Existing approaches of sending video streams to a centralized server (or\nCloud) for video analytics have been facing many challenges such as latency,\nbandwidth cost and more importantly users' privacy concerns. To address these\nchallenges, this paper showcases the ability of an intelligent smart doorbell\nbased on Federated Deep Learning, which can deploy and manage video analytics\napplications such as a smart doorbell across Edge and Cloud resources. This\nplatform can scale, work with multiple devices, seamlessly manage online\norchestration of the application components. The proposed framework is\nimplemented using state-of-the-art technology. We implement the Federated\nServer using the Flask framework, containerized using Nginx and Gunicorn, which\nis deployed on AWS EC2 and AWS Serverless architecture.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:22:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Patel", "Vatsal", ""], ["Kanani", "Sarth", ""], ["Pathak", "Tapan", ""], ["Patel", "Pankesh", ""], ["Ali", "Muhammad Intizar", ""], ["Breslin", "John", ""]]}, {"id": "2010.09746", "submitter": "Gian Giacomo Guerreschi", "authors": "Gian Giacomo Guerreschi", "title": "Fast simulation of quantum algorithms using circuit optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical simulators play a major role in the development and benchmark of\nquantum algorithms and practically any software framework for quantum\ncomputation provides the option of running the algorithms on simulators.\nHowever, the development of quantum simulators was substantially separated from\nthe rest of the software frameworks which, instead, focus on usability and\ncompilation. In practice, simulators are considered just one of many possible\nbackends. Here, we demonstrate the advantage of co-developing and integrating\nsimulators and compilers by proposing a specialized compiler pass to reduce the\nsimulation time for arbitrary circuits. While the concept is broadly\napplicable, we present a concrete implementation based on the Intel Quantum\nSimulator, a high-performance distributed simulator. As part of this work, we\nextend its implementation with additional functionalities related to the\nrepresentation of quantum states. The communication overhead is reduced by\nchanging the order in which state amplitudes are stored in the distributed\nmemory, a concept analogous to the distinction between local and global qubits\nfor distributed Schroedinger-type simulators. We then implement a compiler pass\nto exploit the novel functionalities by introducing special instructions\ngoverning data movement as part of the quantum circuit. Those instructions\ntarget unique capabilities of simulators and have no analogue in actual quantum\ndevices. To quantify the advantage, we compare the time required to simulate\nrandom circuits with and without our optimization. The simulation time is\ntypically halved.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 18:00:20 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 19:07:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Guerreschi", "Gian Giacomo", ""]]}, {"id": "2010.09852", "submitter": "Maciej Besta", "authors": "Hermann Schweizer, Maciej Besta, Torsten Hoefler", "title": "Evaluating the Cost of Atomic Operations on Modern Architectures", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Parallel\n  Architectures and Compilation (PACT'15), 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add\n(FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs\nbetween these operations and various characteristics of such systems, such as\nthe structure of caches, are unclear and have not been thoroughly analyzed. In\nthis paper we establish an evaluation methodology, develop a performance model,\nand present a set of detailed benchmarks for latency and bandwidth of different\natomics. We consider various state-of-the-art x86 architectures: Intel Haswell,\nXeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising\nperformance relationships between the considered atomics and architectural\nproperties such as the coherence state of the accessed cache lines. One key\nfinding is that all the tested atomics have comparable latency and bandwidth\neven if they are characterized by different consensus numbers. Another insight\nis that the hardware implementation of atomics prevents any instruction-level\nparallelism even if there are no dependencies between the issued operations.\nFinally, we discuss solutions to the discovered performance issues in the\nanalyzed architectures. Our analysis enables simpler and more effective\nparallel programming and accelerates data processing on various architectures\ndeployed in both off-the-shelf machines and large compute systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:43:06 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Schweizer", "Hermann", ""], ["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.09854", "submitter": "Maciej Besta", "authors": "Patrick Schmid, Maciej Besta, Torsten Hoefler", "title": "High-Performance Distributed RMA Locks", "comments": "Best Paper Award at ACM HPDC'16 (1/129)", "journal-ref": "Proceedings of the 25th ACM International Symposium on\n  High-Performance Parallel and Distributed Computing (ACM HPDC'16), 2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a topology-aware distributed Reader-Writer lock that accelerates\nirregular workloads for supercomputers and data centers. The core idea behind\nthe lock is a modular design that is an interplay of three distributed data\nstructures: a counter of readers/writers in the critical section, a set of\nqueues for ordering writers waiting for the lock, and a tree that binds all the\nqueues and synchronizes writers with readers. Each structure is associated with\na parameter for favoring either readers or writers, enabling adjustable\nperformance that can be viewed as a point in a three dimensional parameter\nspace. We also develop a distributed topology-aware MCS lock that is a building\nblock of the above design and improves state-of-the-art MPI implementations.\nBoth schemes use non-blocking Remote Memory Access (RMA) techniques for highest\nperformance and scalability. We evaluate our schemes on a Cray XC30 and\nillustrate that they outperform state-of-the-art MPI-3 RMA locking protocols by\n81% and 73%, respectively. Finally, we use them to accelerate a distributed\nhashtable that represents irregular workloads such as key-value stores or graph\nprocessing.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:47:44 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 11:07:13 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Schmid", "Patrick", ""], ["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.09913", "submitter": "Maciej Besta", "authors": "Maciej Besta, Florian Marending, Edgar Solomonik, Torsten Hoefler", "title": "SlimSell: A Vectorizable Graph Representation for Breadth-First Search", "comments": null, "journal-ref": "Proceedings of the 31st IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS'17), 2017", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectorization and GPUs will profoundly change graph processing. Traditional\ngraph algorithms tuned for 32- or 64-bit based memory accesses will be\ninefficient on architectures with 512-bit wide (or larger) instruction units\nthat are already present in the Intel Knights Landing (KNL) manycore CPU.\nAnticipating this shift, we propose SlimSell: a vectorizable graph\nrepresentation to accelerate Breadth-First Search (BFS) based on sparse-matrix\ndense-vector (SpMV) products. SlimSell extends and combines the\nstate-of-the-art SIMD-friendly Sell-C-sigma matrix storage format with\ntropical, real, boolean, and sel-max semiring operations. The resulting design\nreduces the necessary storage (by up to 50%) and thus pressure on the memory\nsubsystem. We augment SlimSell with the SlimWork and SlimChunk schemes that\nreduce the amount of work and improve load balance, further accelerating BFS.\nWe evaluate all the schemes on Intel Haswell multicore CPUs, the\nstate-of-the-art Intel Xeon Phi KNL manycore CPUs, and NVIDIA Tesla GPUs. Our\nexperiments indicate which semiring offers highest speedups for BFS and\nillustrate that SlimSell accelerates a tuned Graph500 BFS code by up to 33%.\nThis work shows that vectorization can secure high-performance in BFS based on\nSpMV products; the proposed principles and designs can be extended to other\ngraph algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:15:25 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 10:13:40 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Besta", "Maciej", ""], ["Marending", "Florian", ""], ["Solomonik", "Edgar", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.09993", "submitter": "Cesar A. Uribe", "authors": "Eduardo Mojica-Nava and David Yanguas-Rojas and C\\'esar A. Uribe", "title": "Robust Asynchronous and Network-Independent Cooperative Learning", "comments": "Submitted to ACC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the model of cooperative learning via distributed non-Bayesian\nlearning, where a network of agents tries to jointly agree on a hypothesis that\nbest described a sequence of locally available observations. Building upon\nrecently proposed weak communication network models, we propose a robust\ncooperative learning rule that allows asynchronous communications, message\ndelays, unpredictable message losses, and directed communication among nodes.\nWe show that our proposed learning dynamics guarantee that all agents in the\nnetwork will have an asymptotic exponential decay of their beliefs on the wrong\nhypothesis, indicating that the beliefs of all agents will concentrate on the\noptimal hypotheses. Numerical experiments provide evidence on a number of\nnetwork setups.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 03:54:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mojica-Nava", "Eduardo", ""], ["Yanguas-Rojas", "David", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "2010.10039", "submitter": "Dingwen Tao", "authors": "Jiannan Tian, Cody Rivera, Sheng Di, Jieyang Chen, Xin Liang, Dingwen\n  Tao, Franck Cappello", "title": "Revisiting Huffman Coding: Toward Extreme Performance on Modern GPU\n  Architectures", "comments": "11 pages, 3 figures, 6 tables, published by IEEE IPDPS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's high-performance computing (HPC) applications are producing vast\nvolumes of data, which are challenging to store and transfer efficiently during\nthe execution, such that data compression is becoming a critical technique to\nmitigate the storage burden and data movement cost. Huffman coding is arguably\nthe most efficient Entropy coding algorithm in information theory, such that it\ncould be found as a fundamental step in many modern compression algorithms such\nas DEFLATE. On the other hand, today's HPC applications are more and more\nrelying on the accelerators such as GPU on supercomputers, while Huffman\nencoding suffers from low throughput on GPUs, resulting in a significant\nbottleneck in the entire data processing. In this paper, we propose and\nimplement an efficient Huffman encoding approach based on modern GPU\narchitectures, which addresses two key challenges: (1) how to parallelize the\nentire Huffman encoding algorithm, including codebook construction, and (2) how\nto fully utilize the high memory-bandwidth feature of modern GPU architectures.\nThe detailed contribution is four-fold. (1) We develop an efficient parallel\ncodebook construction on GPUs that scales effectively with the number of input\nsymbols. (2) We propose a novel reduction based encoding scheme that can\nefficiently merge the codewords on GPUs. (3) We optimize the overall GPU\nperformance by leveraging the state-of-the-art CUDA APIs such as Cooperative\nGroups. (4) We evaluate our Huffman encoder thoroughly using six real-world\napplication datasets on two advanced GPUs and compare with our implemented\nmulti-threaded Huffman encoder. Experiments show that our solution can improve\nthe encoding throughput by up to 5.0X and 6.8X on NVIDIA RTX 5000 and V100,\nrespectively, over the state-of-the-art GPU Huffman encoder, and by up to 3.3X\nover the multi-thread encoder on two 28-core Xeon Platinum 8280 CPUs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:21:55 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 16:11:20 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tian", "Jiannan", ""], ["Rivera", "Cody", ""], ["Di", "Sheng", ""], ["Chen", "Jieyang", ""], ["Liang", "Xin", ""], ["Tao", "Dingwen", ""], ["Cappello", "Franck", ""]]}, {"id": "2010.10055", "submitter": "Giulia Guidi", "authors": "Giulia Guidi, Oguz Selvitopi, Marquita Ellis, Leonid Oliker, Katherine\n  Yelick, Aydin Buluc", "title": "Parallel String Graph Construction and Transitive Reduction for De Novo\n  Genome Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most computationally intensive tasks in computational biology is\nde novo genome assembly, the decoding of the sequence of an unknown genome from\nredundant and erroneous short sequences. A common assembly paradigm identifies\noverlapping sequences, simplifies their layout, and creates consensus. Despite\nmany algorithms developed in the literature, the efficient assembly of large\ngenomes is still an open problem. In this work, we introduce new\ndistributed-memory parallel algorithms for overlap detection and layout\nsimplification steps of de novo genome assembly, and implement them in the\ndiBELLA 2D pipeline. Our distributed memory algorithms for both overlap\ndetection and layout simplification are based on linear-algebra operations over\nsemirings using 2D distributed sparse matrices. Our layout step consists of\nperforming a transitive reduction from the overlap graph to a string graph. We\nprovide a detailed communication analysis of the main stages of our new\nalgorithms. diBELLA 2D achieves near linear scaling with over 80% parallel\nefficiency for the human genome, reducing the runtime for overlap detection by\n1.2-1.3x for the human genome and 1.5-1.9x for C. elegans compared to the\nstate-of-the-art. Our transitive reduction algorithm outperforms an existing\ndistributed-memory implementation by 10.5-13.3x for the human genome and 18-29x\nfor the C. elegans. Our work paves the way for efficient de novo assembly of\nlarge genomes using long reads in distributed memory.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 06:20:24 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Guidi", "Giulia", ""], ["Selvitopi", "Oguz", ""], ["Ellis", "Marquita", ""], ["Oliker", "Leonid", ""], ["Yelick", "Katherine", ""], ["Buluc", "Aydin", ""]]}, {"id": "2010.10083", "submitter": "Amir Ziashahabi", "authors": "Amir Ziashahabi, Mohammad Ali Maddah-Ali and Abbas Heydarnoori", "title": "Bias-Resistant Social News Aggregator Based on Blockchain", "comments": "23 page, 8 figures, Abstract abridged due to arXiv limits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world, social networks have become one of the primary sources for\ncreation and propagation of news. Social news aggregators are one of the actors\nin this area in which users post news items and use positive or negative votes\nto indicate their preference toward a news item. News items will be ordered and\ndisplayed according to their aggregated votes. This approach suffers from\nseveral problems raging from being prone to the dominance of the majority to\ndifficulty in discerning between correct and fake news, and lack of incentive\nfor honest behaviors. In this paper, we propose a graph-based news aggregator\nin which instead of voting on the news items, users submit their votes on the\nrelations between pairs of news items. More precisely, if a user believes two\nnews items support each other, he will submit a positive vote on the link\nbetween the two items, and if he believes that two news items undermine each\nother, he will submit a negative vote on the corresponding link. This approach\nhas mainly two desirable features: (1) mitigating the effect of personal\npreferences on voting, (2) connection of new items to endorsing and disputing\nevidence. This approach helps the newsreaders to understand different aspects\nof a news item better. We also introduce an incentive layer that uses\nblockchain as a distributed transparent manager to encourages users to behave\nhonestly and abstain from adversary behaviors. The incentive layer takes into\naccount that users can have different viewpoints toward news, enabling users\nfrom a wide range of viewpoints to contribute to the network and benefit from\nits rewards. In addition, we introduce a protocol that enables us to prove\nfraud in computations of the incentive layer model on the blockchain.\nUltimately, we will analyze the fraud proof protocol and examine our incentive\nlayer on a wide range of synthesized datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 07:21:11 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Ziashahabi", "Amir", ""], ["Maddah-Ali", "Mohammad Ali", ""], ["Heydarnoori", "Abbas", ""]]}, {"id": "2010.10131", "submitter": "Chao Yang", "authors": "Min Li and Chuanfu Xiao and Chao Yang", "title": "a-Tucker: Input-Adaptive and Matricization-Free Tucker Decomposition for\n  Dense Tensors on CPUs and GPUs", "comments": "10pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tucker decomposition is one of the most popular models for analyzing and\ncompressing large-scale tensorial data. Existing Tucker decomposition\nalgorithms usually rely on a single solver to compute the factor matrices and\ncore tensor, and are not flexible enough to adapt with the diversities of the\ninput data and the hardware. Moreover, to exploit highly efficient GEMM\nkernels, most Tucker decomposition implementations make use of explicit\nmatricizations, which could introduce extra costs in terms of data conversion\nand memory usage. In this paper, we present a-Tucker, a new framework for\ninput-adaptive and matricization-free Tucker decomposition of dense tensors. A\nmode-wise flexible Tucker decomposition algorithm is proposed to enable the\nswitch of different solvers for the factor matrices and core tensor, and a\nmachine-learning adaptive solver selector is applied to automatically cope with\nthe variations of both the input data and the hardware. To further improve the\nperformance and enhance the memory efficiency, we implement a-Tucker in a fully\nmatricization-free manner without any conversion between tensors and matrices.\nExperiments with a variety of synthetic and real-world tensors show that\na-Tucker can substantially outperform existing works on both CPUs and GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 08:52:14 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Li", "Min", ""], ["Xiao", "Chuanfu", ""], ["Yang", "Chao", ""]]}, {"id": "2010.10246", "submitter": "Zhaojing Luo", "authors": "Zhaojing Luo, Sai Ho Yeung, Meihui Zhang, Kaiping Zheng, Lei Zhu, Gang\n  Chen, Feiyi Fan, Qian Lin, Kee Yuan Ngiam, Beng Chin Ooi", "title": "MLCask: Efficient Management of Component Evolution in Collaborative\n  Data Analytics Pipelines", "comments": "13 pages; added new baselines, i.e., MLflow and ModelDB, in Section\n  VII-C; added experience on the system deployment in Section VIII; added Table\n  I to clarify the correctness of the prioritized pipeline search in Section\n  VII-E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing adoption of machine learning for data analytics,\nmaintaining a machine learning pipeline is becoming more complex as both the\ndatasets and trained models evolve with time. In a collaborative environment,\nthe changes and updates due to pipeline evolution often cause cumbersome\ncoordination and maintenance work, raising the costs and making it hard to use.\nExisting solutions, unfortunately, do not address the version evolution\nproblem, especially in a collaborative environment where non-linear version\ncontrol semantics are necessary to isolate operations made by different user\nroles. The lack of version control semantics also incurs unnecessary storage\nconsumption and lowers efficiency due to data duplication and repeated data\npre-processing, which are avoidable. In this paper, we identify two main\nchallenges that arise during the deployment of machine learning pipelines, and\naddress them with the design of versioning for an end-to-end analytics system\nMLCask. The system supports multiple user roles with the ability to perform\nGit-like branching and merging operations in the context of the machine\nlearning pipelines. We define and accelerate the metric-driven merge operation\nby pruning the pipeline search tree using reusable history records and pipeline\ncompatibility information. Further, we design and implement the prioritized\npipeline search, which gives preference to the pipelines that probably yield\nbetter performance. The effectiveness of MLCask is evaluated through an\nextensive study over several real-world deployment cases. The performance\nevaluation shows that the proposed merge operation is up to 7.8x faster and\nsaves up to 11.9x storage space than the baseline method that does not utilize\nhistory records.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 13:34:48 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 14:00:35 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 08:01:19 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 12:54:40 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Luo", "Zhaojing", ""], ["Yeung", "Sai Ho", ""], ["Zhang", "Meihui", ""], ["Zheng", "Kaiping", ""], ["Zhu", "Lei", ""], ["Chen", "Gang", ""], ["Fan", "Feiyi", ""], ["Lin", "Qian", ""], ["Ngiam", "Kee Yuan", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2010.10248", "submitter": "George Bisbas", "authors": "George Bisbas, Fabio Luporini, Mathias Louboutin, Rhodri Nelson,\n  Gerard Gorman, Paul H. J. Kelly", "title": "Temporal blocking of finite-difference stencil operators with sparse\n  \"off-the-grid\" sources", "comments": "Accepted for publication at 35th IEEE International Parallel &\n  Distributed Processing Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil kernels dominate a range of scientific applications, including\nseismic and medical imaging, image processing, and neural networks. Temporal\nblocking is a performance optimization that aims to reduce the required memory\nbandwidth of stencil computations by re-using data from the cache for multiple\ntime steps. It has already been shown to be beneficial for this class of\nalgorithms. However, applying temporal blocking to practical applications'\nstencils remains challenging. These computations often consist of sparsely\nlocated operators not aligned with the computational grid (\"off-the-grid\"). Our\nwork is motivated by modeling problems in which source injections result in\nwavefields that must then be measured at receivers by interpolation from the\ngrided wavefield. The resulting data dependencies make the adoption of temporal\nblocking much more challenging. We propose a methodology to inspect these data\ndependencies and reorder the computation, leading to performance gains in\nstencil codes where temporal blocking has not been applicable. We implement\nthis novel scheme in the Devito domain-specific compiler toolchain. Devito\nimplements a domain-specific language embedded in Python to generate optimized\npartial differential equation solvers using the finite-difference method from\nhigh-level symbolic problem definitions. We evaluate our scheme using isotropic\nacoustic, anisotropic acoustic, and isotropic elastic wave propagators of\nindustrial significance. After auto-tuning, performance evaluation shows that\nthis enables substantial performance improvement through temporal blocking over\nhighly-optimized vectorized spatially-blocked code of up to 1.6x.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:20:07 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 14:26:45 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Bisbas", "George", ""], ["Luporini", "Fabio", ""], ["Louboutin", "Mathias", ""], ["Nelson", "Rhodri", ""], ["Gorman", "Gerard", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "2010.10272", "submitter": "Lars Gottesb\\\"uren", "authors": "Lars Gottesb\\\"uren, Tobias Heuer, Peter Sanders, Sebastian Schlag", "title": "Scalable Shared-Memory Hypergraph Partitioning", "comments": "Accepted for publication at ALENEX21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph partitioning is an important preprocessing step for optimizing\ndata placement and minimizing communication volumes in high-performance\ncomputing applications. To cope with ever growing problem sizes, it has become\nincreasingly important to develop fast parallel partitioning algorithms whose\nsolution quality is competitive with existing sequential algorithms. To this\nend, we present Mt-KaHyPar, the first shared-memory multilevel hypergraph\npartitioner with parallel implementations of many techniques used by the\nsequential, high-quality partitioning systems: a parallel coarsening algorithm\nthat uses parallel community detection as guidance, initial partitioning via\nparallel recursive bipartitioning with work-stealing, a scalable label\npropagation refinement algorithm, and the first fully-parallel direct $k$-way\nformulation of the classical FM algorithm. Experiments performed on a large\nbenchmark set of instances from various application domains demonstrate the\nscalability and effectiveness of our approach. With 64 cores, we observe\nself-relative speedups of up to 51 and a harmonic mean speedup of 23.5. In\nterms of solution quality, we outperform the distributed hypergraph partitioner\nZoltan on 95% of the instances while also being a factor of 2.1 faster. With\njust four cores,Mt-KaHyPar is also slightly faster than the fastest sequential\nmultilevel partitioner PaToH while producing better solutions on 83% of all\ninstances. The sequential high-quality partitioner KaHyPar still finds better\nsolutions than our parallel approach, especially when using max-flow-based\nrefinement. This, however, comes at the cost of considerably longer running\ntimes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:45:12 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 14:51:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gottesb\u00fcren", "Lars", ""], ["Heuer", "Tobias", ""], ["Sanders", "Peter", ""], ["Schlag", "Sebastian", ""]]}, {"id": "2010.10350", "submitter": "Ying Mao", "authors": "Ying Mao, Yuqi Fu, Suwen Gu, Sudip Vhaduri, Long Cheng and Qingzhi Liu", "title": "Resource Management Schemes for Cloud-Native Platforms with Computing\n  Containers of Docker and Kubernetes", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Businesses have made increasing adoption and incorporation of cloud\ntechnology into internal processes in the last decade. The cloud-based\ndeployment provides on-demand availability without active management. More\nrecently, the concept of cloud-native application has been proposed and\nrepresents an invaluable step toward helping organizations develop software\nfaster and update it more frequently to achieve dramatic business outcomes.\nCloud-native is an approach to build and run applications that exploit the\ncloud computing delivery model's advantages. It is more about how applications\nare created and deployed than where. The container-based virtualization\ntechnology, such as Docker and Kubernetes, serves as the foundation for\ncloud-native applications. This paper investigates the performance of two\npopular computational-intensive applications, big data, and deep learning, in a\ncloud-native environment. We analyze the system overhead and resource usage for\nthese applications. Through extensive experiments, we show that the completion\ntime reduces by up to 79.4% by changing the default setting and increases by up\nto 96.7% due to different resource management schemes on two platforms.\nAdditionally, the resource release is delayed by up to 116.7% across different\nsystems. Our work can guide developers, administrators, and researchers to\nbetter design and deploy their applications by selecting and configuring a\nhosting platform.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:13:25 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mao", "Ying", ""], ["Fu", "Yuqi", ""], ["Gu", "Suwen", ""], ["Vhaduri", "Sudip", ""], ["Cheng", "Long", ""], ["Liu", "Qingzhi", ""]]}, {"id": "2010.10378", "submitter": "Amanda Bienz", "authors": "Amanda Bienz, Luke N. Olson, William D. Gropp, and Shelby Lockhart", "title": "Modeling Data Movement Performance on Heterogeneous Architectures", "comments": "7 pages, 6 Figures, Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of data movement on parallel systems varies greatly with machine\narchitecture, job partition, and nearby jobs. Performance models that\naccurately capture the cost of data movement provide a tool for analysis,\nallowing for communication bottlenecks to be pinpointed. Modern heterogeneous\narchitectures yield increased variance in data movement as there are a number\nof viable paths for inter-GPU communication. In this paper, we present\nperformance models for the various paths of inter-node communication on modern\nheterogeneous architectures, including the trade-off between GPUDirect\ncommunication and copying to CPUs. Furthermore, we present a novel optimization\nfor inter-node communication based on these models, utilizing all available CPU\ncores per node. Finally, we show associated performance improvements for MPI\ncollective operations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:38:27 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 02:05:59 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 23:00:09 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bienz", "Amanda", ""], ["Olson", "Luke N.", ""], ["Gropp", "William D.", ""], ["Lockhart", "Shelby", ""]]}, {"id": "2010.10447", "submitter": "Joachim Neu", "authors": "Joachim Neu, Ertem Nusret Tas, David Tse", "title": "Snap-and-Chat Protocols: System Aspects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability-finality dilemma says that blockchain protocols cannot be\nboth available under dynamic participation and safe under network partition.\nSnap-and-chat protocols have recently been proposed as a resolution to this\ndilemma. A snap-and-chat protocol produces an always available ledger\ncontaining a finalized prefix ledger which is always safe and catches up with\nthe available ledger whenever network conditions permit. In contrast to\nexisting handcrafted finality gadget based designs like Ethereum 2.0's\nconsensus protocol Gasper, snap-and-chat protocols are constructed as a\nblack-box composition of off-the-shelf BFT and longest chain protocols. In this\npaper, we consider system aspects of snap-and-chat protocols and show how they\ncan provide two important features: 1) accountability, 2) support of light\nclients. Through this investigation, a deeper understanding of the strengths\nand challenges of snap-and-chat protocols is gained.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:02:58 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Neu", "Joachim", ""], ["Tas", "Ertem Nusret", ""], ["Tse", "David", ""]]}, {"id": "2010.10458", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Xianhao Zhou, Shutao Song, Xingyao Wang, Zilin Zhu, Xue\n  Huang, Xinan Jiang, Feihu Zhou, Zhenyu Guo, Liqiang Xie, Rui Lan, Xianbin\n  Ouyang, Yan Zhang, Jieqian Wei, Jing Gong, Weiliang Lin, Ping Gao, Peng Meng,\n  Xiaomin Xu, Chenyang Guo, Bo Yang, Zhibo Chen, Yongjian Wu and Xiaowen Chu", "title": "Towards Scalable Distributed Training of Deep Learning on Public Cloud\n  Clusters", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training techniques have been widely deployed in large-scale deep\nneural networks (DNNs) training on dense-GPU clusters. However, on public cloud\nclusters, due to the moderate inter-connection bandwidth between instances,\ntraditional state-of-the-art distributed training systems cannot scale well in\ntraining large-scale models. In this paper, we propose a new computing and\ncommunication efficient top-k sparsification communication library for\ndistributed training. To further improve the system scalability, we optimize\nI/O by proposing a simple yet efficient multi-level data caching mechanism and\noptimize the update operation by introducing a novel parallel tensor operator.\nExperimental results on a 16-node Tencent Cloud cluster (each node with 8\nNvidia Tesla V100 GPUs) show that our system achieves 25%-40% faster than\nexisting state-of-the-art systems on CNNs and Transformer. We finally break the\nrecord on DAWNBench on training ResNet-50 to 93% top-5 accuracy on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:16:29 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Shi", "Shaohuai", ""], ["Zhou", "Xianhao", ""], ["Song", "Shutao", ""], ["Wang", "Xingyao", ""], ["Zhu", "Zilin", ""], ["Huang", "Xue", ""], ["Jiang", "Xinan", ""], ["Zhou", "Feihu", ""], ["Guo", "Zhenyu", ""], ["Xie", "Liqiang", ""], ["Lan", "Rui", ""], ["Ouyang", "Xianbin", ""], ["Zhang", "Yan", ""], ["Wei", "Jieqian", ""], ["Gong", "Jing", ""], ["Lin", "Weiliang", ""], ["Gao", "Ping", ""], ["Meng", "Peng", ""], ["Xu", "Xiaomin", ""], ["Guo", "Chenyang", ""], ["Yang", "Bo", ""], ["Chen", "Zhibo", ""], ["Wu", "Yongjian", ""], ["Chu", "Xiaowen", ""]]}, {"id": "2010.10509", "submitter": "Md. Redowan Mahmud", "authors": "Redowan Mahmud, Adel N. Toosi, Maria Alejandra Rodriguez, Sharat\n  Chandra Madanapalli, Vijay Sivaraman, Len Sciacca, Christos Sioutis, Rajkumar\n  Buyya", "title": "Software-Defined Multi-domain Tactical Networks: Foundations and Future\n  Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Networking (SDN) has emerged as a programmable approach for\nprovisioning and managing network resources by defining a clear separation\nbetween the control and data forwarding planes. Nowadays SDN has gained\nsignificant attention in the military domain. Its use in the battlefield\ncommunication facilitates the end-to-end interactions and assists the\nexploitation of edge computing resources for processing data in the proximity.\nHowever, there are still various challenges related to the security and\ninteroperability among several heterogeneous, dynamic, intermittent, and data\npacket technologies like multi-bearer network (MBN) that need to be addressed\nto leverage the benefits of SDN in tactical environments. In this chapter, we\nexplicitly analyse these challenges and review the current research initiatives\nin SDN-enabled tactical networks. We also present a taxonomy on SDN-based\ntactical network orchestration according to the identified challenges and map\nthe existing works to the taxonomy aiming at determining the research gaps and\nsuggesting future directions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 04:50:19 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Mahmud", "Redowan", ""], ["Toosi", "Adel N.", ""], ["Rodriguez", "Maria Alejandra", ""], ["Madanapalli", "Sharat Chandra", ""], ["Sivaraman", "Vijay", ""], ["Sciacca", "Len", ""], ["Sioutis", "Christos", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2010.10517", "submitter": "Shantenu Jha", "authors": "Hyungro Lee, Andre Merzky, Li Tan, Mikhail Titov, Matteo Turilli,\n  Dario Alfe, Agastya Bhati, Alex Brace, Austin Clyde, Peter Coveney, Heng Ma,\n  Arvind Ramanathan, Rick Stevens, Anda Trifan, Hubertus Van Dam, Shunzhou Wan,\n  Sean Wilkinson, Shantenu Jha", "title": "Scalable HPC and AI Infrastructure for COVID-19 Therapeutics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 has claimed more 1 million lives and resulted in over 40 million\ninfections. There is an urgent need to identify drugs that can inhibit\nSARS-CoV-2. In response, the DOE recently established the Medical Therapeutics\nproject as part of the National Virtual Biotechnology Laboratory, and tasked it\nwith creating the computational infrastructure and methods necessary to advance\ntherapeutics development. We discuss innovations in computational\ninfrastructure and methods that are accelerating and advancing drug design.\nSpecifically, we describe several methods that integrate artificial\nintelligence and simulation-based approaches, and the design of computational\ninfrastructure to support these methods at scale. We discuss their\nimplementation and characterize their performance, and highlight science\nadvances that these capabilities have enabled.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 14:13:52 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Lee", "Hyungro", ""], ["Merzky", "Andre", ""], ["Tan", "Li", ""], ["Titov", "Mikhail", ""], ["Turilli", "Matteo", ""], ["Alfe", "Dario", ""], ["Bhati", "Agastya", ""], ["Brace", "Alex", ""], ["Clyde", "Austin", ""], ["Coveney", "Peter", ""], ["Ma", "Heng", ""], ["Ramanathan", "Arvind", ""], ["Stevens", "Rick", ""], ["Trifan", "Anda", ""], ["Van Dam", "Hubertus", ""], ["Wan", "Shunzhou", ""], ["Wilkinson", "Sean", ""], ["Jha", "Shantenu", ""]]}, {"id": "2010.10572", "submitter": "Yupeng Jiang", "authors": "Yupeng Jiang, Yong Li, Yipeng Zhou and Xi Zheng", "title": "Mitigating Sybil Attacks on Differential Privacy based Federated\n  Learning", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated learning, machine learning and deep learning models are trained\nglobally on distributed devices. The state-of-the-art privacy-preserving\ntechnique in the context of federated learning is user-level differential\nprivacy. However, such a mechanism is vulnerable to some specific model\npoisoning attacks such as Sybil attacks. A malicious adversary could create\nmultiple fake clients or collude compromised devices in Sybil attacks to mount\ndirect model updates manipulation. Recent works on novel defense against model\npoisoning attacks are difficult to detect Sybil attacks when differential\nprivacy is utilized, as it masks clients' model updates with perturbation. In\nthis work, we implement the first Sybil attacks on differential privacy based\nfederated learning architectures and show their impacts on model convergence.\nWe randomly compromise some clients by manipulating different noise levels\nreflected by the local privacy budget epsilon of differential privacy on the\nlocal model updates of these Sybil clients such that the global model\nconvergence rates decrease or even leads to divergence. We apply our attacks to\ntwo recent aggregation defense mechanisms, called Krum and Trimmed Mean. Our\nevaluation results on the MNIST and CIFAR-10 datasets show that our attacks\neffectively slow down the convergence of the global models. We then propose a\nmethod to keep monitoring the average loss of all participants in each round\nfor convergence anomaly detection and defend our Sybil attacks based on the\nprediction cost reported from each client. Our empirical study demonstrates\nthat our defense approach effectively mitigates the impact of our Sybil attacks\non model convergence.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:17:25 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jiang", "Yupeng", ""], ["Li", "Yong", ""], ["Zhou", "Yipeng", ""], ["Zheng", "Xi", ""]]}, {"id": "2010.10638", "submitter": "Weiyun Jiang", "authors": "Weiyun Jiang, Kaiqi Zhang, Colin Yu Lin, Feng Xing, and Zheng Zhang", "title": "Sparse Tucker Tensor Decomposition on a Hybrid FPGA-CPU Platform", "comments": "10 pages, 7 figures, IEEE Transactions on Computer-Aided Design of\n  Integrated Circuits and Systems", "journal-ref": null, "doi": "10.1109/TCAD.2020.3032626", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems, social network analysis, medical imaging, and data\nmining often involve processing sparse high-dimensional data. Such\nhigh-dimensional data are naturally represented as tensors, and they cannot be\nefficiently processed by conventional matrix or vector computations. Sparse\nTucker decomposition is an important algorithm for compressing and analyzing\nthese sparse high-dimensional data sets. When energy efficiency and data\nprivacy are major concerns, hardware accelerators on resource-constraint\nplatforms become crucial for the deployment of tensor algorithms. In this work,\nwe propose a hybrid computing framework containing CPU and FPGA to accelerate\nsparse Tucker factorization. This algorithm has three main modules:\ntensor-times-matrix (TTM), Kronecker products, and QR decomposition with column\npivoting (QRP). In addition, we accelerate the former two modules on a Xilinx\nFPGA and the latter one on a CPU. Our hybrid platform achieves $23.6 \\times\n\\sim 1091\\times$ speedup and over $93.519\\% \\sim 99.514 \\%$ energy savings\ncompared with CPU on the synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 21:42:53 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jiang", "Weiyun", ""], ["Zhang", "Kaiqi", ""], ["Lin", "Colin Yu", ""], ["Xing", "Feng", ""], ["Zhang", "Zheng", ""]]}, {"id": "2010.10683", "submitter": "Maciej Besta", "authors": "Maciej Besta, Syed Minhaj Hassan, Sudhakar Yalamanchili, Rachata\n  Ausavarungnirun, Onur Mutlu, Torsten Hoefler", "title": "Slim NoC: A Low-Diameter On-Chip Network Topology for High Energy\n  Efficiency and Scalability", "comments": null, "journal-ref": "Proceedings of the 23rd ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems\n  (ASPLOS'18), 2018", "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging chips with hundreds and thousands of cores require networks with\nunprecedented energy/area efficiency and scalability. To address this, we\npropose Slim NoC (SN): a new on-chip network design that delivers significant\nimprovements in efficiency and scalability compared to the state-of-the-art.\nThe key idea is to use two concepts from graph and number theory,\ndegree-diameter graphs combined with non-prime finite fields, to enable the\nsmallest number of ports for a given core count. SN is inspired by\nstate-of-the-art off-chip topologies; it identifies and distills their\nadvantages for NoC settings while solving several key issues that lead to\nsignificant overheads on-chip. SN provides NoC-specific layouts, which further\nenhance area/energy efficiency. We show how to augment SN with state-of-the-art\nrouter microarchitecture schemes such as Elastic Links, to make the network\neven more scalable and efficient. Our extensive experimental evaluations show\nthat SN outperforms both traditional low-radix topologies (e.g., meshes and\ntori) and modern high-radix networks (e.g., various Flattened Butterflies) in\narea, latency, throughput, and static/dynamic power consumption for both\nsynthetic and real workloads. SN provides a promising direction in scalable and\nenergy-efficient NoC topologies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 00:30:20 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Besta", "Maciej", ""], ["Hassan", "Syed Minhaj", ""], ["Yalamanchili", "Sudhakar", ""], ["Ausavarungnirun", "Rachata", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.10858", "submitter": "Befekadu Gebraselase", "authors": "Befekadu G. Gebraselase, Bjarne E. Helvik, Yuming Jiang", "title": "Transaction Characteristics of Bitcoin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain has been considered as an important technique to enable secure\nmanagement of virtual network functions and network slices. To understand such\ncapabilities of a blockchain, e.g. transaction confirmation time, demands a\nthorough study on the transaction characteristics of the blockchain. This paper\npresents a comprehensive study on the transaction characteristics of Bitcoin --\nthe first blockchain application, focusing on the underlying fundamental\nprocesses. A set of results and findings are obtained, which provide new\ninsight into understanding the transaction and traffic characteristics of\nBitcoin. As a highlight, the validity of several hypotheses/assumptions used in\nthe literature is examined with measurement for the first time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:35:36 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gebraselase", "Befekadu G.", ""], ["Helvik", "Bjarne E.", ""], ["Jiang", "Yuming", ""]]}, {"id": "2010.10930", "submitter": "Nikunj Gupta", "authors": "Nikunj Gupta, Jackson R. Mayo, Adrian S. Lemoine, Hartmut Kaiser", "title": "Towards Distributed Software Resilience in Asynchronous Many-Task\n  Programming Models", "comments": "arXiv admin note: text overlap with arXiv:2004.07203", "journal-ref": null, "doi": null, "report-no": "SAND2020-11278 C", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Exceptions and errors occurring within mission critical applications due to\nhardware failures have a high cost. With the emerging Next Generation Platforms\n(NGPs), the rate of hardware failures will likely increase. Therefore,\ndesigning our applications to be resilient is a critical concern in order to\nretain the reliability of results while meeting the constraints on power\nbudgets. In this paper, we discuss software resilience in AMTs at both local\nand distributed scale. We choose HPX to prototype our resiliency designs. We\nimplement two resiliency APIs that we expose to the application developers,\nnamely task replication and task replay. Task replication repeats a task\nn-times and executes them asynchronously. Task replay reschedules a task up to\nn-times until a valid output is returned. Furthermore, we expose algorithm\nbased fault tolerance (ABFT) using user provided predicates (e.g., checksums)\nto validate the returned results. We benchmark the resiliency scheme for both\nsynthetic and real world applications at local and distributed scale and show\nthat most of the added execution time arises from the replay, replication or\ndata movement of the tasks and not the boilerplate code added to achieve\nresilience.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 21:54:25 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gupta", "Nikunj", ""], ["Mayo", "Jackson R.", ""], ["Lemoine", "Adrian S.", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "2010.10966", "submitter": "Andriy Miranskyy", "authors": "Mohammad Saiful Islam, William Pourmajidi, Lei Zhang, John\n  Steinbacher, Tony Erwin, Andriy Miranskyy", "title": "Anomaly Detection in a Large-scale Cloud Platform", "comments": "A condensed version to appear in Proceedings of the 43rd\n  International Conference on Software Engineering (ICSE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is ubiquitous: more and more companies are moving the\nworkloads into the Cloud. However, this rise in popularity challenges Cloud\nservice providers, as they need to monitor the quality of their ever-growing\nofferings effectively. To address the challenge, we designed and implemented an\nautomated monitoring system for the IBM Cloud Platform. This monitoring system\nutilizes deep learning neural networks to detect anomalies in near-real-time in\nmultiple Platform components simultaneously.\n  After running the system for a year, we observed that the proposed solution\nfrees the DevOps team's time and human resources from manually monitoring\nthousands of Cloud components. Moreover, it increases customer satisfaction by\nreducing the risk of Cloud outages.\n  In this paper, we share our solutions' architecture, implementation notes,\nand best practices that emerged while evolving the monitoring system. They can\nbe leveraged by other researchers and practitioners to build anomaly detectors\nfor complex systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:58:36 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 00:55:55 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Islam", "Mohammad Saiful", ""], ["Pourmajidi", "William", ""], ["Zhang", "Lei", ""], ["Steinbacher", "John", ""], ["Erwin", "Tony", ""], ["Miranskyy", "Andriy", ""]]}, {"id": "2010.10996", "submitter": "Yifan Hu", "authors": "Yifan Hu, Yuhang Zhou, Jun Xiao, Chao Wu", "title": "GFL: A Decentralized Federated Learning Framework Based On Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning(FL) is a rapidly growing field and many centralized and\ndecentralized FL frameworks have been proposed. However, it is of great\nchallenge for current FL frameworks to improve communication performance and\nmaintain the security and robustness under malicious node attacks. In this\npaper, we propose Galaxy Federated Learning Framework(GFL), a decentralized FL\nframework based on blockchain. GFL introduces the consistent hashing algorithm\nto improve communication performance and proposes a novel ring decentralized FL\nalgorithm(RDFL) to improve decentralized FL performance and bandwidth\nutilization. In addition, GFL introduces InterPlanetary File System(IPFS) and\nblockchain to further improve communication efficiency and FL security. Our\nexperiments show that GFL improves communication performance and decentralized\nFL performance under the data poisoning of malicious nodes and non-independent\nand identically distributed(Non-IID) datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:36:59 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 12:45:14 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 14:05:31 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hu", "Yifan", ""], ["Zhou", "Yuhang", ""], ["Xiao", "Jun", ""], ["Wu", "Chao", ""]]}, {"id": "2010.11105", "submitter": "Jakub Ber\\'anek", "authors": "Stanislav B\\\"ohm, Jakub Ber\\'anek", "title": "Runtime vs Scheduler: Analyzing Dask's Overheads", "comments": null, "journal-ref": null, "doi": "10.1109/WORKS51914.2020.00006", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dask is a distributed task framework which is commonly used by data\nscientists to parallelize Python code on computing clusters with little\nprogramming effort. It uses a sophisticated work-stealing scheduler which has\nbeen hand-tuned to execute task graphs as efficiently as possible. But is\nscheduler optimization a worthwhile effort for Dask? Our paper shows on many\nreal world task graphs that even a completely random scheduler is surprisingly\ncompetitive with its built-in scheduler and that the main bottleneck of Dask\nlies in its runtime overhead. We develop a drop-in replacement for the Dask\ncentral server written in Rust which is backwards compatible with existing Dask\nprograms. Thanks to its efficient runtime, our server implementation is able to\nscale up to larger clusters than Dask and consistently outperforms it on a\nvariety of task graphs, despite the fact that it uses a simpler scheduling\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:13:37 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["B\u00f6hm", "Stanislav", ""], ["Ber\u00e1nek", "Jakub", ""]]}, {"id": "2010.11146", "submitter": "Arles Rodr\\'iguez", "authors": "Arles Rodr\\'iguez, Jonatan G\\'omez and Ada Diaconescu", "title": "A Decentralised Self-Healing Approach for Network Topology Maintenance", "comments": null, "journal-ref": "Autonomous Agents and Multi-Agent Systems, 35(1), 6 (2020)", "doi": "10.1007/s10458-020-09486-3", "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many distributed systems, from cloud to sensor networks, different\nconfigurations impact system performance, while strongly depending on the\nnetwork topology. Hence, topological changes may entail costly reconfiguration\nand optimisation processes. This paper proposes a multi-agent solution for\nrecovering networks from node failures. To preserve the network topology, the\nproposed approach relies on local information about the network's structure,\nwhich is collected and disseminated at runtime. The paper studies two\nstrategies for distributing topological data: one based on Mobile Agents (our\nproposal) and the other based on Trickle (a reference gossiping protocol from\nthe literature). These two strategies were adapted for our self-healing\napproach to collect topological information for recovering the network; and\nwere evaluated in terms of resource overheads. Experimental results show that\nboth variants can recover the network topology, up to a certain node failure\nrate, which depends on the network topology. At the same time, Mobile Agents\ncollect less information, focusing on local dissemination, which suffices for\nnetwork recovery. This entails less bandwidth overheads than when Trickle is\nused. Still, Mobile Agents utilise more memory and exchange more messages,\nduring data-collection, than Trickle does. These results validate the viability\nof the proposed self-healing solution, offering two variant implementations\nwith diverse performance characteristics, which may suit different application\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:02:38 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Rodr\u00edguez", "Arles", ""], ["G\u00f3mez", "Jonatan", ""], ["Diaconescu", "Ada", ""]]}, {"id": "2010.11166", "submitter": "Aditya Balu", "authors": "Aditya Balu, Zhanhong Jiang, Sin Yong Tan, Chinmay Hedge, Young M Lee,\n  Soumik Sarkar", "title": "Decentralized Deep Learning using Momentum-Accelerated Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decentralized deep learning where multiple agents\ncollaborate to learn from a distributed dataset. While there exist several\ndecentralized deep learning approaches, the majority consider a central\nparameter-server topology for aggregating the model parameters from the agents.\nHowever, such a topology may be inapplicable in networked systems such as\nad-hoc mobile networks, field robotics, and power network systems where direct\ncommunication with the central parameter server may be inefficient. In this\ncontext, we propose and analyze a novel decentralized deep learning algorithm\nwhere the agents interact over a fixed communication topology (without a\ncentral server). Our algorithm is based on the heavy-ball acceleration method\nused in gradient-based optimization. We propose a novel consensus protocol\nwhere each agent shares with its neighbors its model parameters as well as\ngradient-momentum values during the optimization process. We consider both\nstrongly convex and non-convex objective functions and theoretically analyze\nour algorithm's performance. We present several empirical comparisons with\ncompeting decentralized learning methods to demonstrate the efficacy of our\napproach under different communication topologies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:39:52 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 17:06:17 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Balu", "Aditya", ""], ["Jiang", "Zhanhong", ""], ["Tan", "Sin Yong", ""], ["Hedge", "Chinmay", ""], ["Lee", "Young M", ""], ["Sarkar", "Soumik", ""]]}, {"id": "2010.11305", "submitter": "Jie Yang", "authors": "Jie Amy Yang, Jianyu Huang, Jongsoo Park, Ping Tak Peter Tang, Andrew\n  Tulloch", "title": "Mixed-Precision Embedding Using a Cache", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recommendation systems, practitioners observed that increase in the number\nof embedding tables and their sizes often leads to significant improvement in\nmodel performances. Given this and the business importance of these models to\nmajor internet companies, embedding tables for personalization tasks have grown\nto terabyte scale and continue to grow at a significant rate. Meanwhile, these\nlarge-scale models are often trained with GPUs where high-performance memory is\na scarce resource, thus motivating numerous work on embedding table compression\nduring training. We propose a novel change to embedding tables using a cache\nmemory architecture, where the majority of rows in an embedding is trained in\nlow precision, and the most frequently or recently accessed rows cached and\ntrained in full precision. The proposed architectural change works in\nconjunction with standard precision reduction and computer arithmetic\ntechniques such as quantization and stochastic rounding. For an open source\ndeep learning recommendation model (DLRM) running with Criteo-Kaggle dataset,\nwe achieve 3x memory reduction with INT8 precision embedding tables and\nfull-precision cache whose size are 5% of the embedding tables, while\nmaintaining accuracy. For an industrial scale model and dataset, we achieve\neven higher >7x memory reduction with INT4 precision and cache size 1% of\nembedding tables, while maintaining accuracy, and 16% end-to-end training\nspeedup by reducing GPU-to-host data transfers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:49:54 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 01:37:34 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yang", "Jie Amy", ""], ["Huang", "Jianyu", ""], ["Park", "Jongsoo", ""], ["Tang", "Ping Tak Peter", ""], ["Tulloch", "Andrew", ""]]}, {"id": "2010.11307", "submitter": "Ying Mao", "authors": "Ying Mao, Yuqi Fu, Wenjia Zheng, Long Cheng, Qingzhi Liu, and Dingwen\n  Tao", "title": "Speculative Container Scheduling for Deep Learning Applications in a\n  Kubernetes Cluster", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, we have witnessed a dramatically increasing volume of\ndata collected from varied sources. The explosion of data has transformed the\nworld as more information is available for collection and analysis than ever\nbefore. To maximize the utilization, various machine and deep learning models\nhave been developed, e.g. CNN [1] and RNN [2], to study data and extract\nvaluable information from different perspectives. While data-driven\napplications improve countless products, training models for hyperparameter\ntuning is still a time-consuming and resource-intensive process. Cloud\ncomputing provides infrastructure support for the training of deep learning\napplications. The cloud service providers, such as Amazon Web Services [3],\ncreate an isolated virtual environment (virtual machines and containers) for\nclients, who share physical resources, e.g., CPU and memory. On the cloud,\nresource management schemes are implemented to enable better sharing among\nusers and boost the system-wide performance. However, general scheduling\napproaches, such as spread priority and balanced resource schedulers, do not\nwork well with deep learning workloads. In this project, we propose SpeCon, a\nnovel container scheduler that is optimized for shortlived deep learning\napplications. Based on virtualized containers, such as Kubernetes [4] and\nDocker [5], SpeCon analyzes the common characteristics of training processes.\nWe design a suite of algorithms to monitor the progress of the training and\nspeculatively migrate the slow-growing models to release resources for\nfast-growing ones. Specifically, the extensive experiments demonstrate that\nSpeCon improves the completion time of an individual job by up to 41.5%, 14.8%\nsystem-wide and 24.7% in terms of makespan.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:56:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mao", "Ying", ""], ["Fu", "Yuqi", ""], ["Zheng", "Wenjia", ""], ["Cheng", "Long", ""], ["Liu", "Qingzhi", ""], ["Tao", "Dingwen", ""]]}, {"id": "2010.11320", "submitter": "Maciej Malawski", "authors": "Krzysztof Burkat, Maciej Pawlik, Bartosz Balis, Maciej Malawski, Karan\n  Vahi, Mats Rynge, Rafael Ferreira da Silva, Ewa Deelman", "title": "Serverless Containers -- rising viable approach to Scientific Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing popularity of the serverless computing approach has led to the\nemergence of new cloud infrastructures working in Container-as-a-Service (CaaS)\nmodel like AWS Fargate, Google Cloud Run, or Azure Container Instances. They\nintroduce an innovative approach to running cloud containers where developers\nare freed from managing underlying resources. In this paper, we focus on\nevaluating capabilities of elastic containers and their usefulness for\nscientific computing in the scientific workflow paradigm using AWS Fargate and\nGoogle Cloud Run infrastructures. For experimental evaluation of our approach,\nwe extended HyperFlow engine to support these CaaS platform, together with\nadapting four real-world scientific workflows composed of several dozen to over\na hundred of tasks organized into a dependency graph. We used these workflows\nto create cost-performance benchmarks and flow execution plots, measuring\ndelays, elasticity, and scalability. The experiments proved that serverless\ncontainers can be successfully applied for scientific workflows. Also, the\nresults allow us to gain insights on specific advantages and limits of such\nplatforms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 21:38:31 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Burkat", "Krzysztof", ""], ["Pawlik", "Maciej", ""], ["Balis", "Bartosz", ""], ["Malawski", "Maciej", ""], ["Vahi", "Karan", ""], ["Rynge", "Mats", ""], ["da Silva", "Rafael Ferreira", ""], ["Deelman", "Ewa", ""]]}, {"id": "2010.11454", "submitter": "Mohammad Jalalzai", "authors": "Mohammad M. Jalalzai, Jianyu Niu, Chen Feng and Fangyu Gai", "title": "Fast-HotStuff: A Fast and Resilient HotStuff Protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HotStuff protocol is a breakthrough in Byzantine Fault Tolerant (BFT)\nconsensus that enjoys both responsiveness and linear view change. It creatively\nadds an additional round to classic BFT protocols (like PBFT) using two rounds.\nThis brings us to an interesting question: Is this additional round really\nnecessary in practice? In this paper, we answer this question by designing a\nnew two-round BFT protocol called Fast-HotStuff, which enjoys responsiveness\nand efficient view change that is comparable to linear view change in terms of\nperformance. Compared to (three-round) HotStuff, Fast-HotStuff has lower\nlatency and is more robust against performance attacks that HotStuff is\nsusceptible to.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 05:30:39 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 00:00:29 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 16:34:46 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 20:56:33 GMT"}, {"version": "v5", "created": "Wed, 23 Dec 2020 22:34:28 GMT"}, {"version": "v6", "created": "Fri, 25 Dec 2020 06:25:17 GMT"}, {"version": "v7", "created": "Thu, 11 Feb 2021 16:36:53 GMT"}, {"version": "v8", "created": "Tue, 23 Mar 2021 06:55:39 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Jalalzai", "Mohammad M.", ""], ["Niu", "Jianyu", ""], ["Feng", "Chen", ""], ["Gai", "Fangyu", ""]]}, {"id": "2010.11497", "submitter": "Olivier Ruas", "authors": "George Giakkoupis (WIDE), Anne-Marie Kermarrec (EPFL), Olivier Ruas\n  (SPIRALS), Fran\\c{c}ois Ta\\\"iani (WIDE, IRISA)", "title": "Cluster-and-Conquer: When Randomness Meets Graph Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining\nand machine-learning applications. Some of the most efficient KNN graph\nalgorithms are incremental and local: they start from a random graph, which\nthey incrementally improve by traversing neighbors-of-neighbors links.\nParadoxically, this random start is also one of the key weaknesses of these\nalgorithms: nodes are initially connected to dissimilar neighbors, that lie far\naway according to the similarity metric. As a result, incremental algorithms\nmust first laboriously explore spurious potential neighbors before they can\nidentify similar nodes, and start converging. In this paper, we remove this\ndrawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts\nthe starting configuration of greedy algorithms thanks to a novel lightweight\nclustering mechanism, dubbed FastRandomHash. FastRandomHash leverages\nrandom-ness and recursion to pre-cluster similar nodes at a very low cost. Our\nextensive evaluation on real datasets shows that Cluster-and-Conquer\nsignificantly outperforms existing approaches, including LSH, yielding\nspeed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:31:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Giakkoupis", "George", "", "WIDE"], ["Kermarrec", "Anne-Marie", "", "EPFL"], ["Ruas", "Olivier", "", "SPIRALS"], ["Ta\u00efani", "Fran\u00e7ois", "", "WIDE, IRISA"]]}, {"id": "2010.11612", "submitter": "Jinliang Yuan", "authors": "Jinliang Yuan, Mengwei Xu, Xiao Ma, Ao Zhou, Xuanzhe Liu, Shangguang\n  Wang", "title": "Hierarchical Federated Learning through LAN-WAN Orchestration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) was designed to enable mobile phones to\ncollaboratively learn a global model without uploading their private data to a\ncloud server. However, exiting FL protocols has a critical communication\nbottleneck in a federated network coupled with privacy concerns, usually\npowered by a wide-area network (WAN). Such a WAN-driven FL design leads to\nsignificantly high cost and much slower model convergence. In this work, we\npropose an efficient FL protocol, which involves a hierarchical aggregation\nmechanism in the local-area network (LAN) due to its abundant bandwidth and\nalmost negligible monetary cost than WAN. Our proposed FL can accelerate the\nlearning process and reduce the monetary cost with frequent local aggregation\nin the same LAN and infrequent global aggregation on a cloud across WAN. We\nfurther design a concrete FL platform, namely LanFL, that incorporates several\nkey techniques to handle those challenges introduced by LAN: cloud-device\naggregation architecture, intra-LAN peer-to-peer (p2p) topology generation,\ninter-LAN bandwidth capacity heterogeneity. We evaluate LanFL on 2 typical\nNon-IID datasets, which reveals that LanFL can significantly accelerate FL\ntraining (1.5x-6.0x), save WAN traffic (18.3x-75.6x), and reduce monetary cost\n(3.8x-27.2x) while preserving the model accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:16:10 GMT"}], "update_date": "2020-10-24", "authors_parsed": [["Yuan", "Jinliang", ""], ["Xu", "Mengwei", ""], ["Ma", "Xiao", ""], ["Zhou", "Ao", ""], ["Liu", "Xuanzhe", ""], ["Wang", "Shangguang", ""]]}, {"id": "2010.11751", "submitter": "Christophe Coreixas", "authors": "Jonas Latt, Christophe Coreixas, and Jo\\\"el Beny", "title": "Cross-platform programming model for many-core lattice Boltzmann\n  simulations", "comments": "The STLBM library is available at https://gitlab.com/unigehpfs/stlbm\n  (see the tag \"benchmarks_plosone\" to reproduce data published in this paper)", "journal-ref": "PLoS One, Public Library of Science, 2021, 16, 1-29", "doi": "10.1371/journal.pone.0250306", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, hardware-agnostic implementation strategy for lattice\nBoltzmann (LB) simulations, which yields massive performance on homogeneous and\nheterogeneous many-core platforms. Based solely on C++17 Parallel Algorithms,\nour approach does not rely on any language extensions, external libraries,\nvendor-specific code annotations, or pre-compilation steps. Thanks in\nparticular to a recently proposed GPU back-end to C++17 Parallel Algorithms, it\nis shown that a single code can compile and reach state-of-the-art performance\non both many-core CPU and GPU environments for the solution of a given non\ntrivial fluid dynamics problem. The proposed strategy is tested with six\ndifferent, commonly used implementation schemes to test the performance impact\nof memory access patterns on different platforms. Nine different LB collision\nmodels are included in the tests and exhibit good performance, demonstrating\nthe versatility of our parallel approach. This work shows that it is less than\never necessary to draw a distinction between research and production software,\nas a concise and generic LB implementation yields performances comparable to\nthose achievable in a hardware specific programming language. The results also\nhighlight the gains of performance achieved by modern many-core CPUs and their\napparent capability to narrow the gap with the traditionally massively faster\nGPU platforms. All code is made available to the community in form of the\nopen-source project \"stlbm\", which serves both as a stand-alone simulation\nsoftware and as a collection of reusable patterns for the acceleration of\npre-existing LB codes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:15:17 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 20:11:08 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 13:16:28 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 14:44:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Latt", "Jonas", ""], ["Coreixas", "Christophe", ""], ["Beny", "Jo\u00ebl", ""]]}, {"id": "2010.11792", "submitter": "Danny Perez", "authors": "Andrew Garmon, Vinay Ramakrishnaiah, Danny Perez", "title": "Resource allocation for task-level speculative scientific applications:\n  a proof of concept using Parallel Trajectory Splicing", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-20-28295", "categories": "cs.DC cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constant increase in parallelism available on large-scale distributed\ncomputers poses major scalability challenges to many scientific applications. A\ncommon strategy to improve scalability is to express the algorithm in terms of\nindependent tasks that can be executed concurrently on a runtime system. In\nthis manuscript, we consider a generalization of this approach where task-level\nspeculation is allowed. In this context, a probability is attached to each task\nwhich corresponds to the likelihood that the product of the task will be\nconsumed as part of the calculation. We consider the problem of optimal\nresource allocation to each of the possible tasks so as too maximize the\nexpected overall computational throughput. The power of this approach is\ndemonstrated by analyzing its application to Parallel Trajectory Splicing, a\nmassively-parallel long-time-dynamics method for atomistic simulations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 15:14:07 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Garmon", "Andrew", ""], ["Ramakrishnaiah", "Vinay", ""], ["Perez", "Danny", ""]]}, {"id": "2010.11893", "submitter": "Kapil Ahuja", "authors": "Rohit Agrawal, Kapil Ahuja, Dhaarna Maheshwari, and Akash Kumar", "title": "ParaLarH: Parallel FPGA Router based upon Lagrange Heuristics", "comments": "10 pages, 3 Figures, and 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routing of the nets in Field Programmable Gate Array (FPGA) design flow is\none of the most time consuming steps. Although Versatile Place and Route (VPR),\nwhich is a commonly used algorithm for this purpose, routes effectively, it is\nslow in execution. One way to accelerate this design flow is to use\nparallelization. Since VPR is intrinsically sequential, a set of parallel\nalgorithms have been recently proposed for this purpose (ParaLaR and\nParaLarPD).\n  These algorithms formulate the routing process as a Linear Program (LP) and\nsolve it using the Lagrange relaxation, the sub-gradient method, and the\nSteiner tree algorithm. Out of the many metrics available to check the\neffectiveness of routing, ParaLarPD, which is an improved version of ParaLaR,\nsuffers from large violations in the constraints of the LP problem (which is\nrelated to the minimum channel width metric) as well as an easily measurable\ncritical path delay metric that can be improved further.\n  In this paper, we introduce a set of novel Lagrange heuristics that improve\nthe Lagrange relaxation process. When tested on the MCNC benchmark circuits, on\nan average, this leads to halving of the constraints violation, up to 10%\nimprovement in the minimum channel width, and up to 8% reduction in the\ncritical path delay as obtained from ParaLarPD. We term our new algorithm as\nParaLarH. Due to the increased work in the Lagrange relaxation process, as\ncompared to ParaLarPD, ParaLarH does slightly deteriorate the speedup obtained\nbecause of parallelization, however, this aspect is easily compensated by using\nmore number of threads.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:30:19 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Agrawal", "Rohit", ""], ["Ahuja", "Kapil", ""], ["Maheshwari", "Dhaarna", ""], ["Kumar", "Akash", ""]]}, {"id": "2010.11935", "submitter": "Prasad Krishnan Dr", "authors": "K V Sushena Sree, Prasad Krishnan", "title": "Coded Data Rebalancing for Decentralized Distributed Databases", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of replication-based distributed databases is affected due to\nnon-uniform storage across storage nodes (also called \\textit{data skew}) and\nreduction in the replication factor during operation, particularly due to node\nadditions or removals. Data rebalancing refers to the communication involved\nbetween the nodes in correcting this data skew, while maintaining the\nreplication factor. For carefully designed distributed databases, transmitting\ncoded symbols during the rebalancing phase has been recently shown to reduce\nthe communication load of rebalancing. In this work, we look at balanced\ndistributed databases with \\textit{random placement}, in which each data\nsegment is stored in a random subset of $r$ nodes in the system, where $r$\nrefers to the replication factor of the distributed database. We call these as\ndecentralized databases. For a natural class of such decentralized databases,\nwe propose rebalancing schemes for correcting data skew and the reduction in\nthe replication factor arising due to a single node addition or removal. We\ngive converse arguments which show that our proposed rebalancing schemes are\noptimal asymptotically in the size of the file.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:58:35 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 06:58:17 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Sree", "K V Sushena", ""], ["Krishnan", "Prasad", ""]]}, {"id": "2010.12020", "submitter": "Olasupo Ajayi", "authors": "Olasupo O. Ajayi, Antoine B. Bagula, Hloniphani M. Maluleke", "title": "Africa 3: A Continental Network Model to Enable the African Fourth\n  Industrial Revolution", "comments": "30 pages, 8 tables, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely recognised that collaboration can help fast-track the\ndevelopment of countries in Africa. Leveraging on the fourth industrial\nrevolution, Africa can achieve accelerated development in health care services,\neducational systems and socio-economic infrastructures. While a number of\nconceptual frameworks have been proposed for the African continent, many have\ndiscounted the Cloud infrastructure used for data storage and processing, as\nwell as the underlying network infrastructure upon which such frameworks would\nbe built. This work therefore presents a continental network model for\ninterconnecting nations in Africa through its data centres. The proposed model\nis based on a multilayer network engineering approach, which first groups\nAfrican countries into clusters of data centres using a hybrid combination of\nclustering techniques; then utilizes Ant Colony Optimization with Stench\nPheromone, that is modified to support variable evaporation rates, to find the\nideal network path(s) across the clusters and the continent as a whole. The\npropsoed model takes into consideration the geo-spatial location, population\nsizes, data centre counts and intercontinental submarine cable landings of each\nAfrican country, when clustering and routing. For bench-marking purposes, the\npath selection algorithm was tested on both the obtained clusters and African\nUnion's regional clusters.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:19:25 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ajayi", "Olasupo O.", ""], ["Bagula", "Antoine B.", ""], ["Maluleke", "Hloniphani M.", ""]]}, {"id": "2010.12056", "submitter": "Linjian Ma", "authors": "Linjian Ma and Edgar Solomonik", "title": "Efficient parallel CP decomposition with pairwise perturbation and\n  multi-sweep dimension tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CP tensor decomposition with alternating least squares (ALS) is dominated in\ncost by the matricized-tensor times Khatri-Rao product (MTTKRP) kernel that is\nnecessary to set up the quadratic optimization subproblems. State-of-art\nparallel ALS implementations use dimension trees to avoid redundant\ncomputations across MTTKRPs within each ALS sweep. In this paper, we propose\ntwo new parallel algorithms to accelerate CP-ALS. We introduce the multi-sweep\ndimension tree (MSDT) algorithm, which requires the contraction between an\norder N input tensor and the first-contracted input matrix once every (N-1)/N\nsweeps. This algorithm reduces the leading order computational cost by a factor\nof 2(N-1)/N relative to the best previously known approach. In addition, we\nintroduce a more communication-efficient approach to parallelizing an\napproximate CP-ALS algorithm, pairwise perturbation. This technique uses\nperturbative corrections to the subproblems rather than recomputing the\ncontractions, and asymptotically accelerates ALS. Our benchmark results show\nthat the per-sweep time achieves 1.25X speed-up for MSDT and 1.94X speed-up for\npairwise perturbation compared to the state-of-art dimension trees running on\n1024 processors on the Stampede2 supercomputer.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:56:03 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ma", "Linjian", ""], ["Solomonik", "Edgar", ""]]}, {"id": "2010.12064", "submitter": "Yassine Himeur", "authors": "Abdullah Alsalemi and Ayman Al-Kababji and Yassine Himeur and Faycal\n  Bensaali and Abbes Amira", "title": "Cloud Energy Micro-Moment Data Classification: A Platform Study", "comments": "This paper has been accepted in IEEE RTDPCC 2020: International\n  Symposium on Real-time Data Processing for Cloud Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency is a crucial factor in the well-being of our planet. In\nparallel, Machine Learning (ML) plays an instrumental role in automating our\nlives and creating convenient workflows for enhancing behavior. So, analyzing\nenergy behavior can help understand weak points and lay the path towards better\ninterventions. Moving towards higher performance, cloud platforms can assist\nresearchers in conducting classification trials that need high computational\npower. Under the larger umbrella of the Consumer Engagement Towards Energy\nSaving Behavior by means of Exploiting Micro Moments and Mobile Recommendation\nSystems (EM)3 framework, we aim to influence consumers behavioral change via\nimproving their power consumption consciousness. In this paper, common cloud\nartificial intelligence platforms are benchmarked and compared for micro-moment\nclassification. The Amazon Web Services, Google Cloud Platform, Google Colab,\nand Microsoft Azure Machine Learning are employed on simulated and real energy\nconsumption datasets. The KNN, DNN, and SVM classifiers have been employed.\nSuperb performance has been observed in the selected cloud platforms, showing\nrelatively close performance. Yet, the nature of some algorithms limits the\ntraining performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:01:31 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 11:09:34 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Alsalemi", "Abdullah", ""], ["Al-Kababji", "Ayman", ""], ["Himeur", "Yassine", ""], ["Bensaali", "Faycal", ""], ["Amira", "Abbes", ""]]}, {"id": "2010.12117", "submitter": "Jianjun Wei", "authors": "Jianjun Wei and Liangyu Chen", "title": "Optimized Multivariate Polynomial Determinant on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimized algorithm calculating determinant for multivariate\npolynomial matrix on GPU. The novel algorithm provides precise determinant for\ninput multivariate polynomial matrix in controllable time. Our approach is\nbased on modular methods and split into Fast Fourier Transformation,\nCondensation method and Chinese Remainder Theorem where each algorithm is\nparalleled on GPU. The experiment results show that our parallel method owns\nsubstantial speedups compared to Maple, allowing memory overhead and time\nexpedition in steady increment. We are also able to deal with complex matrix\nwhich is over the threshold on Maple and constrained on CPU. In addition,\ncalculation during the process could be recovered without losing accuracy at\nany point regardless of disruptions. Furthermore, we propose a time prediction\nfor calculation of polynomial determinant according to some basic matrix\nattributes and we solve an open problem relating to harmonic elimination\nequations on the basis of our GPU implementation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 00:48:32 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wei", "Jianjun", ""], ["Chen", "Liangyu", ""]]}, {"id": "2010.12195", "submitter": "Nikunj Gupta", "authors": "Nikunj Gupta, Rohit Ashiwal, Bine Brank, Sateesh K. Peddoju, Dirk\n  Pleiter", "title": "Performance Evaluation of ParalleX Execution model on Arm-based\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The HPC community shows a keen interest in creating diversity in the CPU\necosystem. The advent of Arm-based processors provides an alternative to the\nexisting HPC ecosystem, which is primarily dominated by x86 processors. In this\npaper, we port an Asynchronous Many-Task runtime system based on the ParalleX\nmodel, i.e., High Performance ParalleX (HPX), and evaluate it on the Arm\necosystem with a suite of benchmarks. We wrote these benchmarks with an\nemphasis on vectorization and distributed scaling. We present the performance\nresults on a variety of Arm processors and compare it with their x86 brethren\nfrom Intel. We show that the results obtained are equally good or better than\ntheir x86 brethren. Finally, we also discuss a few drawbacks of the present Arm\necosystem.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 07:05:32 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gupta", "Nikunj", ""], ["Ashiwal", "Rohit", ""], ["Brank", "Bine", ""], ["Peddoju", "Sateesh K.", ""], ["Pleiter", "Dirk", ""]]}, {"id": "2010.12229", "submitter": "Othmane Marfoq", "authors": "Othmane Marfoq, Chuan Xu, Giovanni Neglia, Richard Vidal", "title": "Throughput-Optimal Topology Design for Cross-Silo Federated Learning", "comments": "41 pages, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning usually employs a client-server architecture where an\norchestrator iteratively aggregates model updates from remote clients and\npushes them back a refined model. This approach may be inefficient in\ncross-silo settings, as close-by data silos with high-speed access links may\nexchange information faster than with the orchestrator, and the orchestrator\nmay become a communication bottleneck. In this paper we define the problem of\ntopology design for cross-silo federated learning using the theory of max-plus\nlinear systems to compute the system throughput---number of communication\nrounds per time unit. We also propose practical algorithms that, under the\nknowledge of measurable network characteristics, find a topology with the\nlargest throughput or with provable throughput guarantees. In realistic\nInternet networks with 10 Gbps access links for silos, our algorithms speed up\ntraining by a factor 9 and 1.5 in comparison to the master-slave architecture\nand to state-of-the-art MATCHA, respectively. Speedups are even larger with\nslower access links.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:28:29 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 19:04:14 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Marfoq", "Othmane", ""], ["Xu", "Chuan", ""], ["Neglia", "Giovanni", ""], ["Vidal", "Richard", ""]]}, {"id": "2010.12288", "submitter": "Stefan Vlaski", "authors": "Stefan Vlaski, Ali H. Sayed", "title": "Graph-Homomorphic Perturbations for Private Decentralized Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized algorithms for stochastic optimization and learning rely on the\ndiffusion of information as a result of repeated local exchanges of\nintermediate estimates. Such structures are particularly appealing in\nsituations where agents may be hesitant to share raw data due to privacy\nconcerns. Nevertheless, in the absence of additional privacy-preserving\nmechanisms, the exchange of local estimates, which are generated based on\nprivate data can allow for the inference of the data itself. The most common\nmechanism for guaranteeing privacy is the addition of perturbations to local\nestimates before broadcasting. These perturbations are generally chosen\nindependently at every agent, resulting in a significant performance loss. We\npropose an alternative scheme, which constructs perturbations according to a\nparticular nullspace condition, allowing them to be invisible (to first order\nin the step-size) to the network centroid, while preserving privacy guarantees.\nThe analysis allows for general nonconvex loss functions, and is hence\napplicable to a large number of machine learning and signal processing\nproblems, including deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 10:35:35 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Vlaski", "Stefan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2010.12328", "submitter": "Gordon Gibb", "authors": "Gordon P. S. Gibb, Nick Brown, Rupert W, Nash, Miguel Mendes, Santiago\n  Monedero, Humberto D\\'iaz Fidalgo, Joaqu\\'in Ram\\'irez Cisneros, Adri\\'an\n  Cardil, Max Kontak", "title": "A Bespoke Workflow Management System for Data-Driven Urgent HPC", "comments": "Preprint of paper in 2020 IEEE/ACM HPC for Urgent Decision Making\n  (UrgentHPC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a workflow management system which permits the kinds\nof data-driven workflows required by urgent computing, namely where new data is\nintegrated into the workflow as a disaster progresses in order refine the\npredictions as time goes on. This allows the workflow to adapt to new data at\nruntime, a capability that most workflow management systems do not possess. The\nworkflow management system was developed for the EU-funded VESTEC project,\nwhich aims to fuse HPC with real-time data for supporting urgent decision\nmaking. We first describe an example workflow from the VESTEC project, and show\nwhy existing workflow technologies do not meet the needs of the project. We\nthen go on to present the design of our Workflow Management System, describe\nhow it is implemented into the VESTEC system, and provide an example of the\nworkflow system in use for a test case.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:11:55 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gibb", "Gordon P. S.", ""], ["Brown", "Nick", ""], ["W", "Rupert", ""], ["Nash", "", ""], ["Mendes", "Miguel", ""], ["Monedero", "Santiago", ""], ["Fidalgo", "Humberto D\u00edaz", ""], ["Cisneros", "Joaqu\u00edn Ram\u00edrez", ""], ["Cardil", "Adri\u00e1n", ""], ["Kontak", "Max", ""]]}, {"id": "2010.12438", "submitter": "Yanqi Zhou", "authors": "Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma,\n  Qiumin Xu, Hanxiao Liu, Phitchaya Mangpo Phothilimthana, Shen Wang, Anna\n  Goldie, Azalia Mirhoseini, and James Laudon", "title": "Transferable Graph Optimizers for ML Compilers", "comments": "arXiv admin note: text overlap with arXiv:1910.01578", "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most compilers for machine learning (ML) frameworks need to solve many\ncorrelated optimization problems to generate efficient machine code. Current ML\ncompilers rely on heuristics based algorithms to solve these optimization\nproblems one at a time. However, this approach is not only hard to maintain but\noften leads to sub-optimal solutions especially for newer model architectures.\nExisting learning based approaches in the literature are sample inefficient,\ntackle a single optimization problem, and do not generalize to unseen graphs\nmaking them infeasible to be deployed in practice. To address these\nlimitations, we propose an end-to-end, transferable deep reinforcement learning\nmethod for computational graph optimization (GO), based on a scalable\nsequential attention mechanism over an inductive graph neural network. GO\ngenerates decisions on the entire graph rather than on each individual node\nautoregressively, drastically speeding up the search compared to prior methods.\nMoreover, we propose recurrent attention layers to jointly optimize dependent\ngraph optimization tasks and demonstrate 33%-60% speedup on three graph\noptimization tasks compared to TensorFlow default optimization. On a diverse\nset of representative graphs consisting of up to 80,000 nodes, including\nInception-v3, Transformer-XL, and WaveNet, GO achieves on average 21%\nimprovement over human experts and 18% improvement over the prior state of the\nart with 15x faster convergence, on a device placement task evaluated in real\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:28:33 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 00:35:53 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhou", "Yanqi", ""], ["Roy", "Sudip", ""], ["Abdolrashidi", "Amirali", ""], ["Wong", "Daniel", ""], ["Ma", "Peter", ""], ["Xu", "Qiumin", ""], ["Liu", "Hanxiao", ""], ["Phothilimthana", "Phitchaya Mangpo", ""], ["Wang", "Shen", ""], ["Goldie", "Anna", ""], ["Mirhoseini", "Azalia", ""], ["Laudon", "James", ""]]}, {"id": "2010.12463", "submitter": "Alfredo Navarra", "authors": "Serafino Cicerone, Gabriele Di Stefano, Alfredo Navarra", "title": "A methodology to design distributed algorithms for mobile entities: the\n  pattern formation problem as case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the wide investigation in distributed computing issues by mobile\nentities of the last two decades, we consider the need of a structured\nmethodology to tackle the arisen problems. The aim is to simplify both the\ndesign of the resolution algorithms and the writing of the required correctness\nproofs. We would encourage the usage of a common framework in order to help\nboth algorithm designer and reviewers in the intricate work of analyzing the\nproposed resolution strategies. In order to better understand the potentials of\nour methodology, we consider the Pattern Formation (PF) problem approached in\n[Fujinaga et al. SIAM J. Comput., 2015] as case study. Since the proposed\nresolution algorithm has turned out to be inaccurate and also of difficult\nfixing, we design a new algorithm guided by the proposed methodology, hence\nfully characterizing the problem.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:00:05 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 09:01:16 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cicerone", "Serafino", ""], ["Di Stefano", "Gabriele", ""], ["Navarra", "Alfredo", ""]]}, {"id": "2010.12478", "submitter": "Marcin Copik", "authors": "Marcin Copik, Tobias Grosser, Torsten Hoefler, Paolo Bientinesi,\n  Benjamin Berkels", "title": "Work-stealing prefix scan: Addressing load imbalance in large-scale\n  image registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallelism patterns (e.g., map or reduce) have proven to be effective tools\nfor parallelizing high-performance applications. In this paper, we study the\nrecursive registration of a series of electron microscopy images - a time\nconsuming and imbalanced computation necessary for nano-scale microscopy\nanalysis. We show that by translating the image registration into a specific\ninstance of the prefix scan, we can convert this seemingly sequential problem\ninto a parallel computation that scales to over thousand of cores. We analyze a\nvariety of scan algorithms that behave similarly for common low-compute\noperators and propose a novel work-stealing procedure for a hierarchical prefix\nscan. Our evaluation shows that by identifying a suitable and well-optimized\nprefix scan algorithm, we reduce time-to-solution on a series of 4,096 images\nspanning ten seconds of microscopy acquisition from over 10 hours to less than\n3 minutes (using 1024 Intel Haswell cores), enabling derivation of material\nproperties at nanoscale for long microscopy image series.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:20:55 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Copik", "Marcin", ""], ["Grosser", "Tobias", ""], ["Hoefler", "Torsten", ""], ["Bientinesi", "Paolo", ""], ["Berkels", "Benjamin", ""]]}, {"id": "2010.12607", "submitter": "Ra\\'ul Nozal", "authors": "Ra\\'ul Nozal, Jose Luis Bosque and Ramon Beivide", "title": "Towards Co-execution on Commodity Heterogeneous Systems: Optimizations\n  for Time-Constrained Scenarios", "comments": "8 pages, 6 figures, conference", "journal-ref": "2019 International Conference on High Performance Computing &\n  Simulation (HPCS), pp. 628-635", "doi": "10.1109/HPCS48598.2019.9188188", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous systems are present from powerful supercomputers, to mobile\ndevices, including desktop computers, thanks to their excellent performance and\nenergy consumption. The ubiquity of these architectures in both desktop systems\nand medium-sized service servers allow enough variability to exploit a wide\nrange of problems, such as multimedia workloads, video encoding, image\nfiltering and inference in machine learning. Due to the heterogeneity, some\nefforts have been done to reduce the programming effort and preserve\nperformance portability, but these systems include a set of challenges. The\ncontext in which applications offload the workload along with the management\noverheads introduced when doing co-execution, penalize the performance gains\nunder time-constrained scenarios. Therefore, this paper proposes optimizations\nfor the EngineCL runtime to reduce the penalization when co-executing in\ncommodity systems, as well as algorithmic improvements when load balancing. An\nexhaustive experimental evaluation is performed, showing optimization\nimprovements of 7.5\\% and 17.4\\% for binary and ROI-based offloading modes,\nrespectively. Thanks to all the optimizations, the new load balancing algorithm\nis always the most efficient scheduling configuration, achieving an average\nefficiency of 0.84 under a pessimistic scenario.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 18:32:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Nozal", "Ra\u00fal", ""], ["Bosque", "Jose Luis", ""], ["Beivide", "Ramon", ""]]}, {"id": "2010.12728", "submitter": "Ying Mao", "authors": "Ying Mao, Weifeng Yan, Yun Song, Yue Zeng, Ming Chen, Long Cheng, and\n  Qingzhi Liu", "title": "Differentiate Quality of Experience Scheduling for Deep Learning\n  Applications with Docker Containers in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of big-data-driven applications, such as face recognition\non smartphones and tailored recommendations from Google Ads, we are on the road\nto a lifestyle with significantly more intelligence than ever before. For\nexample, Aipoly Vision [1] is an object and color recognizer that helps the\nblind, visually impaired, and color blind understand their surroundings. At the\nback end side of their intelligence, various neural networks powered models are\nrunning to enable quick responses to users. Supporting those models requires\nlots of cloud-based computational resources, e.g. CPUs and GPUs. The cloud\nproviders charge their clients by the amount of resources that they occupied.\nFrom clients' perspective, they have to balance the budget and quality of\nexperiences (e.g. response time). The budget leans on individual business\nowners and the required Quality of Experience (QoE) depends on usage scenarios\nof different applications, for instance, an autonomous vehicle requires\nrealtime response, but, unlocking your smartphone can tolerate delays. However,\ncloud providers fail to offer a QoE based option to their clients. In this\npaper, we propose DQoES, a differentiate quality of experience scheduler for\ndeep learning applications. DQoES accepts client's specification on targeted\nQoEs, and dynamically adjust resources to approach their targets. Through\nextensive, cloud-based experiments, DQoES demonstrates that it can schedule\nmultiple concurrent jobs with respect to various QoEs and achieve up to 8x\ntimes more satisfied models compared to the existing system.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 00:57:31 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mao", "Ying", ""], ["Yan", "Weifeng", ""], ["Song", "Yun", ""], ["Zeng", "Yue", ""], ["Chen", "Ming", ""], ["Cheng", "Long", ""], ["Liu", "Qingzhi", ""]]}, {"id": "2010.12746", "submitter": "Dingwen Tao", "authors": "Baodi Shan, Aabid Shamji, Jiannan Tian, Guanpeng Li, Dingwen Tao", "title": "LCFI: A Fault Injection Tool for Studying Lossy Compression Error\n  Propagation in HPC Programs", "comments": "8 pages, 6 figures, 2 tables, 8 listings, published by IWBDR workshop\n  at IEEE BigData' 20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Error-bounded lossy compression is becoming more and more important to\ntoday's extreme-scale HPC applications because of the ever-increasing volume of\ndata generated because it has been widely used in in-situ visualization, data\nstream intensity reduction, storage reduction, I/O performance improvement,\ncheckpoint/restart acceleration, memory footprint reduction, etc. Although many\nworks have optimized ratio, quality, and performance for different\nerror-bounded lossy compressors, there is none of the existing works attempting\nto systematically understand the impact of lossy compression errors on HPC\napplication due to error propagation.\n  In this paper, we propose and develop a lossy compression fault injection\ntool, called LCFI. To the best of our knowledge, this is the first fault\ninjection tool that helps both lossy compressor developers and users to\nsystematically and comprehensively understand the impact of lossy compression\nerrors on HPC programs. The contributions of this work are threefold: (1) We\npropose an efficient approach to inject lossy compression errors according to a\nstatistical analysis of compression errors for different state-of-the-art\ncompressors. (2) We build a fault injector which is highly applicable,\ncustomizable, easy-to-use in generating top-down comprehensive results, and\ndemonstrate the use of LCFI. (3) We evaluate LCFI on four representative HPC\nbenchmarks with different abstracted fault models and make several observations\nabout error propagation and their impacts on program outputs.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 02:14:54 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 03:23:04 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 01:10:45 GMT"}, {"version": "v4", "created": "Sun, 22 Nov 2020 23:43:36 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shan", "Baodi", ""], ["Shamji", "Aabid", ""], ["Tian", "Jiannan", ""], ["Li", "Guanpeng", ""], ["Tao", "Dingwen", ""]]}, {"id": "2010.12933", "submitter": "Dmitry Ignatov", "authors": "Dmitry Egurnov, Dmitry I. Ignatov, and Dmitry Tochilkin", "title": "Triclustering in Big Data Setting", "comments": "The paper contains an extended version of the prior work presented at\n  the workshop on FCA in the Big Data Era held on June 25, 2019 at Frankfurt\n  University of Applied Sciences, Frankfurt, Germany", "journal-ref": "LNCS (2020)", "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe versions of triclustering algorithms adapted for\nefficient calculations in distributed environments with MapReduce model or\nparallelisation mechanism provided by modern programming languages. OAC-family\nof triclustering algorithms shows good parallelisation capabilities due to the\nindependent processing of triples of a triadic formal context. We provide the\ntime and space complexity of the algorithms and justify their relevance. We\nalso compare performance gain from using a distributed system and scalability.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 16:55:55 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Egurnov", "Dmitry", ""], ["Ignatov", "Dmitry I.", ""], ["Tochilkin", "Dmitry", ""]]}, {"id": "2010.12998", "submitter": "Mingyue Ji", "authors": "Jiayi Wang, Shiqiang Wang, Rong-Rong Chen, Mingyue Ji", "title": "Local Averaging Helps: Hierarchical Federated Learning and Convergence\n  Analysis", "comments": "42 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is an effective approach to realize collaborative learning\namong edge devices without exchanging raw data. In practice, these devices may\nconnect to local hubs instead of connecting to the global server (aggregator)\ndirectly. Due to the (possibly limited) computation capability of these local\nhubs, it is reasonable to assume that they can perform simple averaging\noperations. A natural question is whether such local averaging is beneficial\nunder different system parameters and how much gain can be obtained compared to\nthe case without such averaging. In this paper, we study hierarchical federated\nlearning with stochastic gradient descent (HF-SGD) and conduct a thorough\ntheoretical analysis to analyze its convergence behavior. In particular, we\nfirst consider the two-level HF-SGD (one level of local averaging) and then\nextend this result to arbitrary number of levels (multiple levels of local\naveraging). The analysis demonstrates the impact of local averaging precisely\nas a function of system parameters. Due to the higher communication cost of\nglobal averaging, a strategy of decreasing the global averaging frequency and\nincreasing the local averaging frequency is proposed. Experiments validate the\nproposed theoretical analysis and the advantages of HF-SGD.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 22:05:41 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 21:04:44 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Jiayi", ""], ["Wang", "Shiqiang", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "2010.13058", "submitter": "Lu Wang", "authors": "Wen Sun, Shiyu Lei, Lu Wang, Zhiqiang Liu, and Yan Zhang", "title": "Adaptive Federated Learning and Digital Twin for Industrial Internet of\n  Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial Internet of Things (IoT) enables distributed intelligent services\nvarying with the dynamic and realtime industrial devices to achieve Industry\n4.0 benefits. In this paper, we consider a new architecture of digital twin\nempowered Industrial IoT where digital twins capture the characteristics of\nindustrial devices to assist federated learning. Noticing that digital twins\nmay bring estimation deviations from the actual value of device state, a\ntrusted based aggregation is proposed in federated learning to alleviate the\neffects of such deviation. We adaptively adjust the aggregation frequency of\nfederated learning based on Lyapunov dynamic deficit queue and deep\nreinforcement learning, to improve the learning performance under the resource\nconstraints. To further adapt to the heterogeneity of Industrial IoT, a\nclustering-based asynchronous federated learning framework is proposed.\nNumerical results show that the proposed framework is superior to the benchmark\nin terms of learning accuracy, convergence, and energy saving.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 07:59:17 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 02:53:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sun", "Wen", ""], ["Lei", "Shiyu", ""], ["Wang", "Lu", ""], ["Liu", "Zhiqiang", ""], ["Zhang", "Yan", ""]]}, {"id": "2010.13100", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Yunjae Lee, Minsoo Rhu", "title": "Tensor Casting: Co-Designing Algorithm-Architecture for Personalized\n  Recommendation Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations are one of the most widely deployed machine\nlearning (ML) workload serviced from cloud datacenters. As such, architectural\nsolutions for high-performance recommendation inference have recently been the\ntarget of several prior literatures. Unfortunately, little have been explored\nand understood regarding the training side of this emerging ML workload. In\nthis paper, we first perform a detailed workload characterization study on\ntraining recommendations, root-causing sparse embedding layer training as one\nof the most significant performance bottlenecks. We then propose our\nalgorithm-architecture co-design called Tensor Casting, which enables the\ndevelopment of a generic accelerator architecture for tensor gather-scatter\nthat encompasses all the key primitives of training embedding layers. When\nprototyped on a real CPU-GPU system, Tensor Casting provides 1.9-21x\nimprovements in training throughput compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:04:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kwon", "Youngeun", ""], ["Lee", "Yunjae", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2010.13103", "submitter": "Minsoo Rhu", "authors": "Yujeong Choi, Yunseong Kim, Minsoo Rhu", "title": "LazyBatching: An SLA-aware Batching System for Cloud Machine Learning\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud ML inference systems, batching is an essential technique to increase\nthroughput which helps optimize total-cost-of-ownership. Prior graph batching\ncombines the individual DNN graphs into a single one, allowing multiple inputs\nto be concurrently executed in parallel. We observe that the coarse-grained\ngraph batching becomes suboptimal in effectively handling the dynamic inference\nrequest traffic, leaving significant performance left on the table. This paper\nproposes LazyBatching, an SLA-aware batching system that considers both\nscheduling and batching in the granularity of individual graph nodes, rather\nthan the entire graph for flexible batching. We show that LazyBatching can\nintelligently determine the set of nodes that can be efficiently batched\ntogether, achieving an average 15x, 1.5x, and 5.5x improvement than graph\nbatching in terms of average response time, throughput, and SLA satisfaction,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:13:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Choi", "Yujeong", ""], ["Kim", "Yunseong", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2010.13104", "submitter": "Y. Efe Erginbas", "authors": "Y. Efe Erginbas, Stefan Vlaski, Ali H. Sayed", "title": "Gramian-Based Adaptive Combination Policies for Diffusion Learning over\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an adaptive combination strategy for distributed learning\nover diffusion networks. Since learning relies on the collaborative processing\nof the stochastic information at the dispersed agents, the overall performance\ncan be improved by designing combination policies that adjust the weights\naccording to the quality of the data. Such policies are important because they\nwould add a new degree of freedom and endow multi-agent systems with the\nability to control the flow of information over their edges for enhanced\nperformance. Most adaptive and static policies available in the literature\noptimize certain performance metrics related to steady-state behavior, to the\ndetriment of transient behavior. In contrast, we develop an adaptive\ncombination rule that aims at optimizing the transient learning performance,\nwhile maintaining the enhanced steady-state performance obtained using policies\npreviously developed in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:26:26 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Erginbas", "Y. Efe", ""], ["Vlaski", "Stefan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2010.13112", "submitter": "Aleksandr Beznosikov", "authors": "Aleksandr Beznosikov, Valentin Samokhin, Alexander Gasnikov", "title": "Distributed Saddle-Point Problems: Lower Bounds, Optimal and Robust\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the distributed optimization of smooth stochastic\nsaddle-point problems. The first part of the paper is devoted to lower bounds\nfor the cenralized and decentralized distributed methods for smooth\n(strongly-)convex-(strongly-)concave saddle-point problems as well as the\noptimal algorithms by which these bounds are achieved. Next, we present a new\nfederated algorithm for saddle-point problems - Extra Step Local SGD.\nTheoretical analysis of the new method is carried out for\n(strongly-)convex-(strongly-)concave and non-convex-non-concave problems. In\nthe experimental part of the paper, we show the effectiveness of our method in\npractice. In particular, we train GANs in a distributed manner.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 13:13:44 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 13:39:20 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 08:58:53 GMT"}, {"version": "v4", "created": "Thu, 18 Feb 2021 06:24:22 GMT"}, {"version": "v5", "created": "Sat, 27 Feb 2021 14:29:59 GMT"}, {"version": "v6", "created": "Mon, 14 Jun 2021 13:50:08 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Beznosikov", "Aleksandr", ""], ["Samokhin", "Valentin", ""], ["Gasnikov", "Alexander", ""]]}, {"id": "2010.13216", "submitter": "Ayaz Akram", "authors": "Ayaz Akram, Anna Giannakou, Venkatesh Akella, Jason Lowe-Power, Sean\n  Peisert", "title": "Performance Analysis of Scientific Computing Workloads on Trusted\n  Execution Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computing sometimes involves computation on sensitive data.\nDepending on the data and the execution environment, the HPC (high-performance\ncomputing) user or data provider may require confidentiality and/or integrity\nguarantees. To study the applicability of hardware-based trusted execution\nenvironments (TEEs) to enable secure scientific computing, we deeply analyze\nthe performance impact of AMD SEV and Intel SGX for diverse HPC benchmarks\nincluding traditional scientific computing, machine learning, graph analytics,\nand emerging scientific computing workloads. We observe three main findings: 1)\nSEV requires careful memory placement on large scale NUMA machines\n(1$\\times$$-$3.4$\\times$ slowdown without and 1$\\times$$-$1.15$\\times$ slowdown\nwith NUMA aware placement), 2) virtualization$-$a prerequisite for\nSEV$-$results in performance degradation for workloads with irregular memory\naccesses and large working sets (1$\\times$$-$4$\\times$ slowdown compared to\nnative execution for graph applications) and 3) SGX is inappropriate for HPC\ngiven its limited secure memory size and inflexible programming model\n(1.2$\\times$$-$126$\\times$ slowdown over unsecure execution). Finally, we\ndiscuss forthcoming new TEE designs and their potential impact on scientific\ncomputing.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 20:46:25 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Akram", "Ayaz", ""], ["Giannakou", "Anna", ""], ["Akella", "Venkatesh", ""], ["Lowe-Power", "Jason", ""], ["Peisert", "Sean", ""]]}, {"id": "2010.13234", "submitter": "Emna Baccour", "authors": "Emna Baccour, Aiman Erbad, Amr Mohamed, Mounir Hamdi and Mohsen\n  Guizani", "title": "DistPrivacy: Privacy-Aware Distributed Deep Neural Networks in IoT\n  surveillance systems", "comments": "Accepted in Globecom conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of smart cities, Internet of Things (IoT) devices as well\nas deep learning technologies have witnessed an increasing adoption. To support\nthe requirements of such paradigm in terms of memory and computation, joint and\nreal-time deep co-inference framework with IoT synergy was introduced. However,\nthe distribution of Deep Neural Networks (DNN) has drawn attention to the\nprivacy protection of sensitive data. In this context, various threats have\nbeen presented, including black-box attacks, where a malicious participant can\naccurately recover an arbitrary input fed into his device. In this paper, we\nintroduce a methodology aiming to secure the sensitive data through re-thinking\nthe distribution strategy, without adding any computation overhead. First, we\nexamine the characteristics of the model structure that make it susceptible to\nprivacy threats. We found that the more we divide the model feature maps into a\nhigh number of devices, the better we hide proprieties of the original image.\nWe formulate such a methodology, namely DistPrivacy, as an optimization\nproblem, where we establish a trade-off between the latency of co-inference,\nthe privacy level of the data, and the limited-resources of IoT participants.\nDue to the NP-hardness of the problem, we introduce an online heuristic that\nsupports heterogeneous IoT devices as well as multiple DNNs and datasets,\nmaking the pervasive system a general-purpose platform for privacy-aware and\nlow decision-latency applications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 22:05:22 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Baccour", "Emna", ""], ["Erbad", "Aiman", ""], ["Mohamed", "Amr", ""], ["Hamdi", "Mounir", ""], ["Guizani", "Mohsen", ""]]}, {"id": "2010.13265", "submitter": "Hao Wang", "authors": "Qing Yang, Hao Wang", "title": "Cooperative Energy Management of HVAC via Transactive Energy", "comments": "The 16th International Conference on Control & Automation (IEEE\n  ICCA), 2020", "journal-ref": null, "doi": "10.1109/ICCA51439.2020.9264462", "report-no": null, "categories": "eess.SY cs.DC cs.SY math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heating, Ventilation, and Air Conditioning (HVAC) energy consumption accounts\nfor a significant part of the total energy consumption of buildings and\nhouseholds. The ubiquitous adoption of distributed renewable energy and smart\nmeters helps to decarbonize the HVAC energy consumption and improve energy\nefficiency. However, how to scale up HVAC energy management for a group of\nusers while persevering users' privacy remains a big challenge. In this work,\nwe utilize the concept of transactive energy to build a cooperative energy\nmanagement system for independent HVAC units in a distributed manner.\nSpecifically, we develop a distributed energy trading algorithm that consists\nof two layers based on the alternating direction method of multipliers method.\nThe distributed energy trading algorithm achieves optimal trading performance\nand also preserves users' privacy. Furthermore, we evaluate the performance of\nthe distributed trading algorithm by extensive simulations with real-world\ndata. Simulation results show that the energy trading algorithm converges fast\nand the cooperative energy platform reduces the user's individual cost by up to\n50% and lowers the overall cost of all users by 23%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 00:46:25 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 13:02:07 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Qing", ""], ["Wang", "Hao", ""]]}, {"id": "2010.13296", "submitter": "Ying Mao", "authors": "Ying Mao and Peizhao Hu", "title": "Enhancing Cloud Storage with Shareable Instances for Social Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud storage plays an important role in social computing. This paper aims to\ndevelop a cloud storage management system for mobile devices to support an\nextended set of file operations. Because of the limit of storage, bandwidth,\npower consumption, and other resource restrictions, most existing cloud storage\napps for smartphones do not keep local copies of files. This efficient design,\nhowever, limits the application capacities. In this paper, we attempt to extend\nthe available file operations for cloud storage service to better serve\nsmartphone users. We develop an efficient and secure file management system,\nSkyfiles, to support more advanced file operations. The basic idea of our\ndesign is to utilize cloud instances to assist file operations. Particularly,\nSkyfiles supports downloading, compressing, encrypting, and converting\noperations, as well as file transfer between two smartphone users' cloud\nstorage spaces. In addition, we propose a protocol for users to share their\nidle instances. All file operations supported by Skyfiles can be efficiently\nand securely accomplished with either a self-created instance or shared\ninstance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 02:52:43 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mao", "Ying", ""], ["Hu", "Peizhao", ""]]}, {"id": "2010.13342", "submitter": "Linda Stals Assoc. Prof.", "authors": "Emmanuel Agullo, Mirco Altenbernd, Hartwig Anzt, Leonardo\n  Bautista-Gomez, Tommaso Benacchio, Luca Bonaventura, Hans-Joachim Bungartz,\n  Sanjay Chatterjee, Florina M. Ciorba, Nathan DeBardeleben, Daniel Drzisga,\n  Sebastian Eibl, Christian Engelmann, Wilfried N. Gansterer, Luc Giraud,\n  Dominik Goeddeke, Marco Heisig, Fabienne Jezequel, Nils Kohl, Xiaoye Sherry\n  Li, Romain Lion, Miriam Mehl, Paul Mycek, Michael Obersteiner, Enrique S.\n  Quintana-Orti, Francesco Rizzi, Ulrich Ruede, Martin Schulz, Fred Fung,\n  Robert Speck, Linda Stals, Keita Teranishi, Samuel Thibault, Dominik\n  Thoennes, Andreas Wagner and Barbara Wohlmuth", "title": "Resiliency in Numerical Algorithm Design for Extreme Scale Simulations", "comments": "45 pages, 3 figures, submitted to The International Journal of High\n  Performance Computing Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is based on the seminar titled ``Resiliency in Numerical Algorithm\nDesign for Extreme Scale Simulations'' held March 1-6, 2020 at Schloss\nDagstuhl, that was attended by all the authors.\n  Naive versions of conventional resilience techniques will not scale to the\nexascale regime: with a main memory footprint of tens of Petabytes,\nsynchronously writing checkpoint data all the way to background storage at\nfrequent intervals will create intolerable overheads in runtime and energy\nconsumption. Forecasts show that the mean time between failures could be lower\nthan the time to recover from such a checkpoint, so that large calculations at\nscale might not make any progress if robust alternatives are not investigated.\n  More advanced resilience techniques must be devised. The key may lie in\nexploiting both advanced system features as well as specific application\nknowledge. Research will face two essential questions: (1) what are the\nreliability requirements for a particular computation and (2) how do we best\ndesign the algorithms and software to meet these requirements? One avenue would\nbe to refine and improve on system- or application-level checkpointing and\nrollback strategies in the case an error is detected. Developers might use\nfault notification interfaces and flexible runtime systems to respond to node\nfailures in an application-dependent fashion. Novel numerical algorithms or\nmore stochastic computational approaches may be required to meet accuracy\nrequirements in the face of undetectable soft errors.\n  The goal of this Dagstuhl Seminar was to bring together a diverse group of\nscientists with expertise in exascale computing to discuss novel ways to make\napplications resilient against detected and undetected faults. In particular,\nparticipants explored the role that algorithms and applications play in the\nholistic approach needed to tackle this challenge.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 05:05:14 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Agullo", "Emmanuel", ""], ["Altenbernd", "Mirco", ""], ["Anzt", "Hartwig", ""], ["Bautista-Gomez", "Leonardo", ""], ["Benacchio", "Tommaso", ""], ["Bonaventura", "Luca", ""], ["Bungartz", "Hans-Joachim", ""], ["Chatterjee", "Sanjay", ""], ["Ciorba", "Florina M.", ""], ["DeBardeleben", "Nathan", ""], ["Drzisga", "Daniel", ""], ["Eibl", "Sebastian", ""], ["Engelmann", "Christian", ""], ["Gansterer", "Wilfried N.", ""], ["Giraud", "Luc", ""], ["Goeddeke", "Dominik", ""], ["Heisig", "Marco", ""], ["Jezequel", "Fabienne", ""], ["Kohl", "Nils", ""], ["Li", "Xiaoye Sherry", ""], ["Lion", "Romain", ""], ["Mehl", "Miriam", ""], ["Mycek", "Paul", ""], ["Obersteiner", "Michael", ""], ["Quintana-Orti", "Enrique S.", ""], ["Rizzi", "Francesco", ""], ["Ruede", "Ulrich", ""], ["Schulz", "Martin", ""], ["Fung", "Fred", ""], ["Speck", "Robert", ""], ["Stals", "Linda", ""], ["Teranishi", "Keita", ""], ["Thibault", "Samuel", ""], ["Thoennes", "Dominik", ""], ["Wagner", "Andreas", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "2010.13359", "submitter": "Xiaojun Chen Dr.", "authors": "Xiaojun Chen and Shu Yang and Li Shen and Xuanrong Pang", "title": "A Distributed Training Algorithm of Generative Adversarial Networks with\n  Quantized Gradients", "comments": "20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative adversarial networks (GAN) in a distributed fashion is a\npromising technology since it is contributed to training GAN on a massive of\ndata efficiently in real-world applications. However, GAN is known to be\ndifficult to train by SGD-type methods (may fail to converge) and the\ndistributed SGD-type methods may also suffer from massive amount of\ncommunication cost. In this paper, we propose a {distributed GANs training\nalgorithm with quantized gradient, dubbed DQGAN,} which is the first\ndistributed training method with quantized gradient for GANs. The new method\ntrains GANs based on a specific single machine algorithm called Optimistic\nMirror Descent (OMD) algorithm, and is applicable to any gradient compression\nmethod that satisfies a general $\\delta$-approximate compressor. The\nerror-feedback operation we designed is used to compensate for the bias caused\nby the compression, and moreover, ensure the convergence of the new method.\nTheoretically, we establish the non-asymptotic convergence of {DQGAN} algorithm\nto first-order stationary point, which shows that the proposed algorithm can\nachieve a linear speedup in the parameter server model. Empirically, our\nexperiments show that our {DQGAN} algorithm can reduce the communication cost\nand save the training time with slight performance degradation on both\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 06:06:43 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Xiaojun", ""], ["Yang", "Shu", ""], ["Shen", "Li", ""], ["Pang", "Xuanrong", ""]]}, {"id": "2010.13392", "submitter": "Lam Duc Nguyen", "authors": "Dick Carrillo, Lam Duc Nguyen, Pedro H. J. Nardelli, Evangelos\n  Pournaras, Plinio Morita, Dem\\'ostenes Z. Rodr\\'iguez, Merim Dzaferagic,\n  Harun Siljak, Alexander Jung, Laurent H\\'ebert-Dufresne, Irene Macaluso,\n  Mehar Ullah, Gustavo Fraidenraich, Petar Popovski", "title": "Containing Future Epidemics with Trustworthy Federated Systems for\n  Ubiquitous Warning and Response", "comments": "9 pages, 3 figures, Accepted for Publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a global digital platform to avoid and combat\nepidemics by providing relevant real-time information to support selective\nlockdowns. It leverages the pervasiveness of wireless connectivity while being\ntrustworthy and secure. The proposed system is conceptualized to be\ndecentralized yet federated, based on ubiquitous public systems and active\ncitizen participation. Its foundations lie on the principle of informational\nself-determination. We argue that only in this way it can become a trustworthy\nand legitimate public good infrastructure for citizens by balancing the\nasymmetry of the different hierarchical levels within the federated\norganization while providing highly effective detection and guiding mitigation\nmeasures towards graceful lockdown of the society. To exemplify the proposed\nsystem, we choose the remote patient monitoring as use case. In which, the\nintegration of distributed ledger technologies with narrowband IoT technology\nis evaluated considering different number of endorsed peers. An experimental\nproof of concept setup is used to evaluate the performance of this integration,\nin which the end-to-end latency is slightly increased when a new endorsed\nelement is added. However, the system reliability, privacy, and\ninteroperability are guaranteed. In this sense, we expect active participation\nof empowered citizens to supplement the more usual top-down management of\nepidemics.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:42:09 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 22:30:32 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Carrillo", "Dick", ""], ["Nguyen", "Lam Duc", ""], ["Nardelli", "Pedro H. J.", ""], ["Pournaras", "Evangelos", ""], ["Morita", "Plinio", ""], ["Rodr\u00edguez", "Dem\u00f3stenes Z.", ""], ["Dzaferagic", "Merim", ""], ["Siljak", "Harun", ""], ["Jung", "Alexander", ""], ["H\u00e9bert-Dufresne", "Laurent", ""], ["Macaluso", "Irene", ""], ["Ullah", "Mehar", ""], ["Fraidenraich", "Gustavo", ""], ["Popovski", "Petar", ""]]}, {"id": "2010.13432", "submitter": "Nick Brown", "authors": "Nick Brown, Oliver Thomson Brown, J. Mark Bull", "title": "Driving asynchronous distributed tasks with events", "comments": "Preprint of paper in the 4th Workshop on Open Source Supercomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-source matters, not just to the current cohort of HPC users but also to\npotential new HPC communities, such as machine learning, themselves often\nrooted in open-source. Many of these potential new workloads are, by their very\nnature, far more asynchronous and unpredictable than traditional HPC codes and\nopen-source solutions must be found to enable new communities of developers to\neasily take advantage of large scale parallel machines. Task-based models have\nthe potential to help here, but many of these either entirely abstract the user\nfrom the distributed nature of their code, placing emphasis on the runtime to\nmake important decisions concerning scheduling and locality, or require the\nprogrammer to explicitly combine their task-based code with a distributed\nmemory technology such as MPI, which adds considerable complexity. In this\npaper we describe a new approach where the programmer still splits their code\nup into distinct tasks, but is explicitly aware of the distributed nature of\nthe machine and drives interactions between tasks via events. This provides the\nbest of both worlds; the programmer is able to direct important aspects of\nparallelism whilst still being abstracted from the low level mechanism of how\nthis parallelism is achieved. We demonstrate our approach via two use-cases,\nthe Graph500 BFS benchmark and in-situ data analytics of MONC, an atmospheric\nmodel. For both applications we demonstrate considerably improved performance\nat large core counts and the result of this work is an approach and open-source\nlibrary which is readily applicable to a wide range of codes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:59:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Brown", "Nick", ""], ["Brown", "Oliver Thomson", ""], ["Bull", "J. Mark", ""]]}, {"id": "2010.13437", "submitter": "Nick Brown", "authors": "Nick Brown, Michael Bareford, Mich\\`ele Weiland", "title": "Leveraging MPI RMA to optimise halo-swapping communications in MONC on\n  Cray machines", "comments": "This is the pre-peer reviewed version of the following article:\n  Leveraging MPI RMA to optimise halo-swapping communications in MONC on Cray\n  machines, which has been published in final form at\n  https://doi.org/10.1002/cpe.5008", "journal-ref": "In Concurrency and Computation: Practice and Experience 31, no. 16\n  (2019): e5008", "doi": "10.1002/cpe.5008", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote Memory Access (RMA), also known as single sided communications,\nprovides a way of accessing the memory of other processes without having to\nissue explicit message passing style communication calls. Previous studies have\nconcluded that MPI RMA can provide increased performance over traditional MPI\nPoint to Point (P2P) but these are based on synthetic benchmarks. In this work,\nwe replace the existing non-blocking P2P communication calls in the MONC\natmospheric model with MPI RMA. We describe our approach in detail and discuss\noptions taken for correctness and performance. Experiments on illustrate that\nby using RMA we can obtain between a 5\\% and 10\\% reduction in communication\ntime at each timestep on up to 32768 cores, which over the entirety of a run\n(of many timesteps) results in a significant improvement in performance\ncompared to P2P. However, RMA is not a silver bullet and there are challenges\nwhen integrating RMA into existing codes: important optimisations are necessary\nto achieve good performance and library support is not universally mature. In\nthis paper we discuss, in the context of a real world code, the lessons learned\nconverting P2P to RMA, explore performance and scaling challenges, and contrast\nalternative RMA synchronisation approaches in detail.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:17:28 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Brown", "Nick", ""], ["Bareford", "Michael", ""], ["Weiland", "Mich\u00e8le", ""]]}, {"id": "2010.13463", "submitter": "Martin Karp", "authors": "Martin Karp, Artur Podobas, Niclas Jansson, Tobias Kenter, Christian\n  Plessl, Philipp Schlatter, and Stefano Markidis", "title": "High-Performance Spectral Element Methods on Field-Programmable Gate\n  Arrays", "comments": "10 pages, IEEE International Parallel and Distributed Processing\n  Symposium 2021 (IPDPS'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvements in computer systems have historically relied on two well-known\nobservations: Moore's law and Dennard's scaling. Today, both these observations\nare ending, forcing computer users, researchers, and practitioners to abandon\nthe general-purpose architectures' comforts in favor of emerging post-Moore\nsystems. Among the most salient of these post-Moore systems is the\nField-Programmable Gate Array (FPGA), which strikes a convenient balance\nbetween complexity and performance. In this paper, we study modern FPGAs'\napplicability in accelerating the Spectral Element Method (SEM) core to many\ncomputational fluid dynamics (CFD) applications. We design a custom SEM\nhardware accelerator operating in double-precision that we empirically evaluate\non the latest Stratix 10 GX-series FPGAs and position its performance (and\npower-efficiency) against state-of-the-art systems such as ARM ThunderX2,\nNVIDIA Pascal/Volta/Ampere Tesla-series cards, and general-purpose manycore\nCPUs. Finally, we develop a performance model for our SEM-accelerator, which we\nuse to project future FPGAs' performance and role to accelerate CFD\napplications, ultimately answering the question: what characteristics would a\nperfect FPGA for CFD applications have?\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 10:03:21 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 09:14:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Karp", "Martin", ""], ["Podobas", "Artur", ""], ["Jansson", "Niclas", ""], ["Kenter", "Tobias", ""], ["Plessl", "Christian", ""], ["Schlatter", "Philipp", ""], ["Markidis", "Stefano", ""]]}, {"id": "2010.13575", "submitter": "Rooji Jinan", "authors": "Rooji Jinan, Ajay Badita, Tejas Bodas, Parimal Parag", "title": "Load balancing policies with server-side cancellation of replicas", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular dispatching policies such as the join shortest queue (JSQ), join\nsmallest work (JSW) and their power of two variants are used in load balancing\nsystems where the instantaneous queue length or workload information at all\nqueues or a subset of them can be queried. In situations where the dispatcher\nhas an associated memory, one can minimize this query overhead by maintaining a\nlist of idle servers to which jobs can be dispatched. Recent alternative\napproaches that do not require querying such information include the cancel on\nstart and cancel on complete based replication policies. The downside of such\npolicies however is that the servers must communicate the start or completion\nof each service to the dispatcher and must allow cancellation of redundant\ncopies. In this work, we consider a load balancing environment where the\ndispatcher cannot query load information, does not have a memory, and cannot\ncancel any replica that it may have created. In such a rigid environment, we\nallow the dispatcher to possibly append a server side cancellation criteria to\neach job or its replica. A job or a replica is served only if it satisfies the\npredefined criteria at the time of service. We focus on a criteria that is\nbased on the waiting time experienced by a job or its replica and analyze\nseveral variants of this policy based on the assumption of asymptotic\nindependence of queues. The proposed policies are novel and perform remarkably\nwell in spite of the rigid operating constraints.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 13:24:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Jinan", "Rooji", ""], ["Badita", "Ajay", ""], ["Bodas", "Tejas", ""], ["Parag", "Parimal", ""]]}, {"id": "2010.13593", "submitter": "Artem Khyzha", "authors": "Artem Khyzha and Ori Lahav", "title": "Taming x86-TSO Persistency (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the formal semantics of non-volatile memory in the x86-TSO\narchitecture. We show that while the explicit persist operations in the recent\nmodel of Raad et al. from POPL'20 only enforce order between writes to the\nnon-volatile memory, it is equivalent, in terms of reachable states, to a model\nwhose explicit persist operations mandate that prior writes are actually\nwritten to the non-volatile memory. The latter provides a novel model that is\nmuch closer to common developers' understanding of persistency semantics. We\nfurther introduce a simpler and stronger sequentially consistent persistency\nmodel, develop a sound mapping from this model to x86, and establish a\ndata-race-freedom guarantee providing programmers with a safe programming\ndiscipline. Our operational models are accompanied with equivalent declarative\nformulations, which facilitate our formal arguments, and may prove useful for\nprogram verification under x86 persistency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:00:52 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 06:53:48 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Khyzha", "Artem", ""], ["Lahav", "Ori", ""]]}, {"id": "2010.13600", "submitter": "Elsa Rizk", "authors": "Elsa Rizk, Stefan Vlaski, Ali H. Sayed", "title": "Optimal Importance Sampling for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning involves a mixture of centralized and decentralized\nprocessing tasks, where a server regularly selects a sample of the agents and\nthese in turn sample their local data to compute stochastic gradients for their\nlearning updates. This process runs continually. The sampling of both agents\nand data is generally uniform; however, in this work we consider non-uniform\nsampling. We derive optimal importance sampling strategies for both agent and\ndata selection and show that non-uniform sampling without replacement improves\nthe performance of the original FedAvg algorithm. We run experiments on a\nregression and classification problem to illustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:15:33 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Rizk", "Elsa", ""], ["Vlaski", "Stefan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2010.13681", "submitter": "Jonathan Mace", "authors": "Vaastav Anand, Matheus Stolet, Thomas Davidson, Ivan Beschastnikh,\n  Tamara Munzner, and Jonathan Mace", "title": "Aggregate-Driven Trace Visualizations for Performance Debugging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance issues in cloud systems are hard to debug. Distributed tracing is\na widely adopted approach that gives engineers visibility into cloud systems.\nExisting trace analysis approaches focus on debugging single request\ncorrectness issues but not debugging single request performance issues.\nDiagnosing a performance issue in a given request requires comparing the\nperformance of the offending request with the aggregate performance of typical\nrequests. Effective and efficient debugging of such issues faces three\nchallenges: (i) identifying the correct aggregate data for diagnosis; (ii)\nvisualizing the aggregated data; and (iii) efficiently collecting, storing, and\nprocessing trace data.\n  We present TraVista, a tool designed for debugging performance issues in a\nsingle trace that addresses these challenges. TraVista extends the popular\nsingle trace Gantt chart visualization with three types of aggregate data -\nmetric, temporal, and structure data, to contextualize the performance of the\noffending trace across all traces.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:59:02 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Anand", "Vaastav", ""], ["Stolet", "Matheus", ""], ["Davidson", "Thomas", ""], ["Beschastnikh", "Ivan", ""], ["Munzner", "Tamara", ""], ["Mace", "Jonathan", ""]]}, {"id": "2010.13723", "submitter": "Samuel Horv\\'ath", "authors": "Wenlin Chen, Samuel Horvath, Peter Richtarik", "title": "Optimal Client Sampling for Federated Learning", "comments": "11 pages, 10 pages of Appendix, 6 Figures, 3 algorithms, code\n  available: https://github.com/SamuelHorvath/FL-optimal-client-sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well understood that client-master communication can be a primary\nbottleneck in Federated Learning. In this work, we address this issue with a\nnovel client subsampling scheme, where we restrict the number of clients\nallowed to communicate their updates back to the master node. In each\ncommunication round, all participated clients compute their updates, but only\nthe ones with \"important\" updates communicate back to the master. We show that\nimportance can be measured using only the norm of the update and we give a\nformula for optimal client participation. This formula minimizes the distance\nbetween the full update, where all clients participate, and our limited update,\nwhere the number of participating clients is restricted. In addition, we\nprovide a simple algorithm that approximates the optimal formula for client\nparticipation which only requires secure aggregation and thus does not\ncompromise client privacy. We show both theoretically and empirically that our\napproach leads to superior performance for Distributed SGD (DSGD) and Federated\nAveraging (FedAvg) compared to the baseline where participating clients are\nsampled uniformly. Finally, our approach is orthogonal to and compatible with\nexisting methods for reducing communication overhead, such as local methods and\ncommunication compression methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:05:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Wenlin", ""], ["Horvath", "Samuel", ""], ["Richtarik", "Peter", ""]]}, {"id": "2010.13737", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti, Roch Gu\\'erin, Chenyang Lu, Jiangnan Liu", "title": "Real-Time Edge Classification: Optimal Offloading under Token Bucket\n  Constraints", "comments": "Code available at https://github.com/ayanc/edgeml.mdp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:25:29 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 22:48:21 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Chakrabarti", "Ayan", ""], ["Gu\u00e9rin", "Roch", ""], ["Lu", "Chenyang", ""], ["Liu", "Jiangnan", ""]]}, {"id": "2010.13831", "submitter": "Volodymyr Polosukhin", "authors": "Keren Censor-Hillel, Dean Leitersdorf, Volodymyr Polosukhin", "title": "Distance Computations in the Hybrid Network Model via Oracle Simulations", "comments": "To appear in STACS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hybrid network model was introduced in [Augustine et al., SODA '20] for\nlaying down a theoretical foundation for networks which combine two possible\nmodes of communication: One mode allows high-bandwidth communication with\nneighboring nodes, and the other allows low-bandwidth communication over few\nlong-range connections at a time. This fundamentally abstracts networks such as\nhybrid data centers, and class-based software-defined networks.\n  Our technical contribution is a \\emph{density-aware} approach that allows us\nto simulate a set of \\emph{oracles} for an overlay skeleton graph over a Hybrid\nnetwork.\n  As applications of our oracle simulations, with additional machinery that we\nprovide, we derive fast algorithms for fundamental distance-related tasks. One\nof our core contributions is an algorithm in the Hybrid model for computing\n\\emph{exact} weighted shortest paths from $\\tilde O(n^{1/3})$ sources which\ncompletes in $\\tilde O(n^{1/3})$ rounds w.h.p. This improves, in both the\nruntime and the number of sources, upon the algorithm of [Kuhn and Schneider,\nPODC '20], which computes shortest paths from a single source in $\\tilde\nO(n^{2/5})$ rounds w.h.p.\n  We additionally show a 2-approximation for weighted diameter and a\n$(1+\\epsilon)$-approximation for unweighted diameter, both in $\\tilde\nO(n^{1/3})$ rounds w.h.p., which is comparable to the $\\tilde \\Omega(n^{1/3})$\nlower bound of [Kuhn and Schneider, PODC '20] for a\n$(2-\\epsilon)$-approximation for weighted diameter and an exact unweighted\ndiameter. We also provide fast distance \\emph{approximations} from multiple\nsources and fast approximations for eccentricities.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:32:35 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 19:50:19 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Leitersdorf", "Dean", ""], ["Polosukhin", "Volodymyr", ""]]}, {"id": "2010.13972", "submitter": "Rory Mitchell", "authors": "Rory Mitchell, Eibe Frank, Geoffrey Holmes", "title": "GPUTreeShap: Massively Parallel Exact Calculation of SHAP Scores for\n  Tree Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SHAP (SHapley Additive exPlanation) values provide a game theoretic\ninterpretation of the predictions of machine learning models based on Shapley\nvalues. While exact calculation of SHAP values is computationally intractable\nin general, a recursive polynomial-time algorithm called TreeShap is available\nfor decision tree models. However, despite its polynomial time complexity,\nTreeShap can become a significant bottleneck in practical machine learning\npipelines when applied to large decision tree ensembles. We present\nGPUTreeShap, a modified TreeShap algorithm suitable for massively parallel\ncomputation on graphics processing units. Our approach first preprocesses each\ndecision tree to isolate variable sized sub-problems from the original\nrecursive algorithm, then solves a bin packing problem, and finally maps\nsub-problems to single-instruction, multiple-thread (SIMT) tasks for parallel\nexecution with specialised hardware instructions. With a single NVIDIA Tesla\nV100-32 GPU, we achieve speedups of up to 19x for SHAP values, and speedups of\nup to 340x for SHAP interaction values, over a state-of-the-art multi-core CPU\nimplementation executed on two 20-core Xeon E5-2698 v4 2.2 GHz CPUs. We also\nexperiment with multi-GPU computing using eight V100 GPUs, demonstrating\nthroughput of 1.2M rows per second -- equivalent CPU-based performance is\nestimated to require 6850 CPU cores.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 00:55:07 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 22:54:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Mitchell", "Rory", ""], ["Frank", "Eibe", ""], ["Holmes", "Geoffrey", ""]]}, {"id": "2010.14027", "submitter": "Runyu Jin", "authors": "Qirui Yang, Runyu Jin, Nabil Gandhi, Xiongzi Ge, Hoda Aghaei Khouzani,\n  and Ming Zhao", "title": "EdgeBench: A Workflow-based Benchmark for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Edge computing has been developed to utilize multiple tiers of resources for\nprivacy, cost and Quality of Service (QoS) reasons. Edge workloads have the\ncharacteristics of data-driven and latency-sensitive. Because of this, edge\nsystems have developed to be both heterogeneous and distributed. The unique\ncharacteristics of edge workloads and edge systems have motivated EdgeBench, a\nworkflow-based benchmark aims to provide the ability to explore the full design\nspace of edge workloads and edge systems. EdgeBench is both customizable and\nrepresentative. It allows users to customize the workflow logic of edge\nworkloads, the data storage backends, and the distribution of the individual\nworkflow stages to different computing tiers. To illustrate the usability of\nEdgeBench, we also implements two representative edge workflows, a video\nanalytics workflow and an IoT hub workflow that represents two distinct but\ncommon edge workloads. Both workflows are evaluated using the workflow-level\nand function-level metrics reported by EdgeBench to illustrate both the\nperformance bottlenecks of the edge systems and the edge workloads.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 03:11:41 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yang", "Qirui", ""], ["Jin", "Runyu", ""], ["Gandhi", "Nabil", ""], ["Ge", "Xiongzi", ""], ["Khouzani", "Hoda Aghaei", ""], ["Zhao", "Ming", ""]]}, {"id": "2010.14127", "submitter": "Nick Brown", "authors": "Nick Brown, Mich\\`ele Weiland, Adrian Hill, Ben Shipway", "title": "In-situ data analytics for highly scalable cloud modelling on Cray\n  machines", "comments": "This is the pre-peer reviewed version of the following article:\n  Leveraging MPI RMA to optimise halo-swapping communications in MONC on Cray\n  machines, which has been published in final form at\n  https://doi.org/10.1002/cpe.4331", "journal-ref": "In Concurrency and Computation: Practice and Experience 30, no. 1\n  (2018): e4331", "doi": "10.1002/cpe.4331", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MONC is a highly scalable modelling tool for the investigation of atmospheric\nflows, turbulence and cloud microphysics. Typical simulations produce very\nlarge amounts of raw data which must then be analysed for scientific\ninvestigation. For performance and scalability reasons this analysis and\nsubsequent writing to disk should be performed in-situ on the data as it is\ngenerated however one does not wish to pause the computation whilst analysis is\ncarried out.\n  In this paper we present the analytics approach of MONC, where cores of a\nnode are shared between computation and data analytics. By asynchronously\nsending their data to an analytics core, the computational cores can run\ncontinuously without having to pause for data writing or analysis. We describe\nour IO server framework and analytics workflow, which is highly asynchronous,\nalong with solutions to challenges that this approach raises and the\nperformance implications of some common configuration choices. The result of\nthis work is a highly scalable analytics approach and we illustrate on up to\n32768 computational cores of a Cray XC30 that there is minimal performance\nimpact on the runtime when enabling data analytics in MONC and also investigate\nthe performance and suitability of our approach on the KNL.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:35:28 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Brown", "Nick", ""], ["Weiland", "Mich\u00e8le", ""], ["Hill", "Adrian", ""], ["Shipway", "Ben", ""]]}, {"id": "2010.14132", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "A comparison of techniques for solving the Poisson equation in CFD", "comments": "Pre-print of paper in the Journal of Civil Aircraft Design and\n  Research 2017-03", "journal-ref": "Journal of Civil Aircraft Design and Research pages 85-94 2017-03", "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CFD is a ubiquitous technique central to much of computational simulation\nsuch as that required by aircraft design. Solving of the Poisson equation\noccurs frequently in CFD and there are a number of possible approaches one may\nleverage. The dynamical core of the MONC atmospheric model is one example of\nCFD which requires the solving of the Poisson equation to determine pressure\nterms. Traditionally this aspect of the model has been very time consuming\nand-so it is important to consider how we might reduce the runtime cost.\n  In this paper we survey the different approaches implemented in MONC to\nperform the pressure solve. Designed to take advantage of large scale, modern,\nHPC machines, we are concerned with the computation and communication behaviour\nof the available techniques and in this text we focus on direct FFT and\nindirect iterative methods. In addition to describing the implementation of\nthese techniques we illustrate on up to 32768 processor cores of a Cray XC30\nboth the performance and scalability of our approaches. Raw runtime is not the\nonly measure so we also make some comments around the stability and accuracy of\nsolution. The result of this work are a number of techniques, optimised for\nlarge scale HPC systems, and an understanding of which is most appropriate in\ndifferent situations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:43:53 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2010.14152", "submitter": "Alfredo Navarra", "authors": "Serafino Cicerone, Alessia Di Fonso, Gabriele Di Stefano, Alfredo\n  Navarra", "title": "Arbitrary Pattern Formation on Infinite Regular Tessellation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set R of robots, each one located at different vertices of an\ninfinite regular tessellation graph, we aim to explore the Arbitrary Pattern\nFormation (APF) problem. Given a multiset F of grid vertices such that |R|=|F|,\nAPF asks for a distributed algorithm that moves robots so as to reach a\nconfiguration similar to F. Similarity means that robots must be disposed as F\nregardless of translations, rotations, reflections. So far, as possible graph\ndiscretizing the Euclidean plane only the standard square grid has been\nconsidered in the context of the classical Look-Compute-Move model. However, it\nis natural to consider also the other regular tessellation graphs, that are\ntriangular and hexagonal grids. We provide a resolution algorithm for APF when\nthe initial configuration is asymmetric and the considered topology is any\nregular tessellation graph.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 09:30:39 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Cicerone", "Serafino", ""], ["Di Fonso", "Alessia", ""], ["Di Stefano", "Gabriele", ""], ["Navarra", "Alfredo", ""]]}, {"id": "2010.14189", "submitter": "Roy Friedman", "authors": "Dolev Adas and Roy Friedman", "title": "Jiffy: A Fast, Memory Efficient, Wait-Free Multi-Producers\n  Single-Consumer Queue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as sharded data processing systems, sharded in-memory\nkey-value stores, data flow programming and load sharing applications, multiple\nconcurrent data producers are feeding requests into the same data consumer.\nThis can be naturally realized through concurrent queues, where each consumer\npulls its tasks from its dedicated queue. For scalability, wait-free queues are\noften preferred over lock based structures.\n  The vast majority of wait-free queue implementations, and even lock-free\nones, support the multi-producer multi-consumer model. Yet, this comes at a\npremium, since implementing wait-free multi-producer multi-consumer queues\nrequires utilizing complex helper data structures. The latter increases the\nmemory consumption of such queues and limits their performance and scalability.\nAdditionally, many such designs employ (hardware) cache unfriendly memory\naccess patterns.\n  In this work we study the implementation of wait-free multi-producer\nsingle-consumer queues. Specifically, we propose Jiffy, an efficient memory\nfrugal novel wait-free multi-producer single-consumer queue and formally prove\nits correctness. We then compare the performance and memory requirements of\nJiffy with other state of the art lock-free and wait-free queues. We show that\nindeed Jiffy can maintain good performance with up to 128 threads, delivers up\nto 50% better throughput than the next best construction we compared against,\nand consumes ~90% less memory.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 10:51:05 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:42:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Adas", "Dolev", ""], ["Friedman", "Roy", ""]]}, {"id": "2010.14373", "submitter": "Mohamed Wahib", "authors": "Jens Domke, Emil Vatai, Aleksandr Drozd, Peng Chen, Yosuke Oyama,\n  Lingqi Zhang, Shweta Salaria, Daichi Mukunoki, Artur Podobas, Mohamed Wahib,\n  Satoshi Matsuoka", "title": "Matrix Engines for High Performance Computing:A Paragon of Performance\n  or Grasping at Straws?", "comments": "IEEE International Parallel and Distributed Processing Symposium 2021\n  (IPDPS'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix engines or units, in different forms and affinities, are becoming a\nreality in modern processors; CPUs and otherwise. The current and dominant\nalgorithmic approach to Deep Learning merits the commercial investments in\nthese units, and deduced from the No.1 benchmark in supercomputing, namely High\nPerformance Linpack, one would expect an awakened enthusiasm by the HPC\ncommunity, too.\n  Hence, our goal is to identify the practical added benefits for HPC and\nmachine learning applications by having access to matrix engines. For this\npurpose, we perform an in-depth survey of software stacks, proxy applications\nand benchmarks, and historical batch job records. We provide a cost-benefit\nanalysis of matrix engines, both asymptotically and in conjunction with\nstate-of-the-art processors. While our empirical data will temper the\nenthusiasm, we also outline opportunities to misuse these dense\nmatrix-multiplication engines if they come for free.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 15:30:01 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 16:37:21 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Domke", "Jens", ""], ["Vatai", "Emil", ""], ["Drozd", "Aleksandr", ""], ["Chen", "Peng", ""], ["Oyama", "Yosuke", ""], ["Zhang", "Lingqi", ""], ["Salaria", "Shweta", ""], ["Mukunoki", "Daichi", ""], ["Podobas", "Artur", ""], ["Wahib", "Mohamed", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "2010.14596", "submitter": "Niranda Perera", "authors": "Niranda Perera, Vibhatha Abeykoon, Chathura Widanage, Supun\n  Kamburugamuve, Thejaka Amila Kanewala, Pulasthi Wickramasinghe, Ahmet Uyar,\n  Hasara Maithree, Damitha Lenadora, and Geoffrey Fox", "title": "A Fast, Scalable, Universal Approach For Distributed Data Aggregations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current era of Big Data, data engineering has transformed into an\nessential field of study across many branches of science. Advancements in\nArtificial Intelligence (AI) have broadened the scope of data engineering and\nopened up new applications in both enterprise and research communities.\nAggregations (also termed reduce in functional programming) are an integral\nfunctionality in these applications. They are traditionally aimed at generating\nmeaningful information on large data-sets, and today, they are being used for\nengineering more effective features for complex AI models. Aggregations are\nusually carried out on top of data abstractions such as tables/ arrays and are\ncombined with other operations such as grouping of values. There are frameworks\nthat excel in the said domains individually. But, we believe that there is an\nessential requirement for a data analytics tool that can universally integrate\nwith existing frameworks, and thereby increase the productivity and efficiency\nof the entire data analytics pipeline. Cylon endeavors to fulfill this void. In\nthis paper, we present Cylon's fast and scalable aggregation operations\nimplemented on top of a distributed in-memory table structure that universally\nintegrates with existing frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 20:39:21 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 02:10:12 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Perera", "Niranda", ""], ["Abeykoon", "Vibhatha", ""], ["Widanage", "Chathura", ""], ["Kamburugamuve", "Supun", ""], ["Kanewala", "Thejaka Amila", ""], ["Wickramasinghe", "Pulasthi", ""], ["Uyar", "Ahmet", ""], ["Maithree", "Hasara", ""], ["Lenadora", "Damitha", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2010.14684", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marc Fischer, Tal Ben-Nun, Dimitri Stanojevic, Johannes\n  De Fine Licht, Torsten Hoefler", "title": "Substream-Centric Maximum Matchings on FPGA", "comments": "Best Paper finalist at ACM FPGA'19, invited to special issue of ACM\n  TRETS'20", "journal-ref": "Proceedings of the ACM Transactions on Reconfigurable Technology\n  and Systems (TRETS), 2020. Proceedings of the 27th ACM/SIGDA International\n  Symposium on Field-Programmable Gate Arrays (FPGA), 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing high-performance and energy-efficient algorithms for maximum\nmatchings is becoming increasingly important in social network analysis,\ncomputational sciences, scheduling, and others. In this work, we propose the\nfirst maximum matching algorithm designed for FPGAs; it is energy-efficient and\nhas provable guarantees on accuracy, performance, and storage utilization. To\nachieve this, we forego popular graph processing paradigms, such as\nvertex-centric programming, that often entail large communication costs.\nInstead, we propose a substream-centric approach, in which the input stream of\ndata is divided into substreams processed independently to enable more\nparallelism while lowering communication costs. We base our work on the theory\nof streaming graph algorithms and analyze 14 models and 28 algorithms. We use\nthis analysis to provide theoretical underpinning that matches the physical\nconstraints of FPGA platforms. Our algorithm delivers high performance (more\nthan 4x speedup over tuned parallel CPU variants), low memory, high accuracy,\nand effective usage of FPGA resources. The substream-centric approach could\neasily be extended to other algorithms to offer low-power and high-performance\ngraph processing on FPGAs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 00:31:27 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Besta", "Maciej", ""], ["Fischer", "Marc", ""], ["Ben-Nun", "Tal", ""], ["Stanojevic", "Dimitri", ""], ["Licht", "Johannes De Fine", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.14756", "submitter": "Aymen Al Saadi", "authors": "Aymen Al-Saadi, Ioannis Paraskevakos, Bento Collares Gon\\c{c}alves,\n  Heather J. Lynch, Shantenu Jha, Matteo Turilli", "title": "Comparing Workflow Application Designs for High Resolution Satellite\n  Image Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.09766", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very High Resolution satellite and aerial imagery are used to monitor and\nconduct large scale surveys of ecological systems. Convolutional Neural\nNetworks have successfully been employed to analyze such imagery to detect\nlarge animals and salient features. As the datasets increase in volume and\nnumber of images, utilizing High Performance Computing resources becomes\nnecessary. In this paper, we investigate three task-parallel, data-driven\nworkflow designs to support imagery analysis pipelines with heterogeneous tasks\non HPC. We analyze the capabilities of each design when processing datasets\nfrom two use cases for a total of 4,672 satellite and aerial images, and 8.35\nTB of data. We experimentally model the execution time of the tasks of the\nimage processing pipelines. We perform experiments to characterize the resource\nutilization, total time to completion, and overheads of each design. Based on\nthe model, overhead and utilization analysis, we show which design is best\nsuited to scientific pipelines with similar characteristics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 15:11:41 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Al-Saadi", "Aymen", ""], ["Paraskevakos", "Ioannis", ""], ["Gon\u00e7alves", "Bento Collares", ""], ["Lynch", "Heather J.", ""], ["Jha", "Shantenu", ""], ["Turilli", "Matteo", ""]]}, {"id": "2010.14823", "submitter": "Nick Brown", "authors": "Nick Brown, Alexandr Nigay, Mich\\`ele Weiland, Adrian Hill, Ben\n  Shipway", "title": "Porting the microphysics model CASIM to GPU and KNL Cray machines", "comments": "Presented at CUG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CASIM is a microphysics scheme which calculates the interaction between\nmoisture droplets in the atmosphere and forms a critical part of weather and\nclimate modelling codes. However the calculations involved are computationally\nintensive and so investigating whether CASIM can take advantage of novel\nhardware architectures and the likely increase in performance this might afford\nmakes sense.\n  In this paper we present work done in porting CASIM to GPUs via the directive\ndriven OpenACC and also modifying CASIM to take advantage of the Knights\nLanding (KNL) processor using OpenMP. Due to the design, models extracting out\nspecific computational kernels for offload to the GPU proved suboptimal and\ninstead the entire scheme was ported over to the GPU. We consider the\nsuitability and maturity of OpenACC for this approach as well as important\noptimisations that were identified. Enabling CASIM to take advantage of the KNL\nwas significantly easier, but still required careful experimentation to\nunderstand the best design and configuration. The performance of both versions\nof CASIM, in comparison to the latest generation of CPUs is discussed, before\nidentifying lessons learnt about the suitability of CASIM and other similar\nmodels for these architectures. The result of this work are versions of CASIM\nwhich show promising performance benefits when utilising both GPUs and KNLs and\nenable the communities to take advantage of these technologies, in addition to\ngeneral techniques that can be applied to other similar weather and climate\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:54:42 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Brown", "Nick", ""], ["Nigay", "Alexandr", ""], ["Weiland", "Mich\u00e8le", ""], ["Hill", "Adrian", ""], ["Shipway", "Ben", ""]]}, {"id": "2010.14827", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "ePython: An implementation of Python for the many-core Epiphany\n  coprocessor", "comments": "Preprint of article in the 6th Workshop on Python for\n  High-Performance and Scientific Computing", "journal-ref": "In 2016 6th Workshop on Python for High-Performance and Scientific\n  Computing (PyHPC) (pp. 59-66). IEEE", "doi": "10.1109/PyHPC.2016.012", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Epiphany is a many-core, low power, low on-chip memory architecture and\none can very cheaply gain access to a number of parallel cores which is\nbeneficial for HPC education and prototyping. The very low power nature of\nthese architectures also means that there is potential for their use in future\nHPC machines, however there is a high barrier to entry in programming them due\nto the associated complexities and immaturity of supporting tools.\n  In this paper we present our work on ePython, a subset of Python for the\nEpiphany and similar many-core co-processors. Due to the limited on-chip memory\nper core we have developed a new Python interpreter and this, combined with\nadditional support for parallelism, has meant that novices can take advantage\nof Python to very quickly write parallel codes on the Epiphany and explore\nconcepts of HPC using a smaller scale parallel machine. The high level nature\nof Python opens up new possibilities on the Epiphany, we examine a\ncomputationally intensive Gauss-Seidel code from the programmability and\nperformance perspective, discuss running Python hybrid on both the host CPU and\nEpiphany, and interoperability between a full Python interpreter on the CPU and\nePython on the Epiphany. The result of this work is support for developing\nPython on the Epiphany, which can be applied to other similar architectures,\nthat the community have already started to adopt and use to explore concepts of\nparallelism and HPC.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:01:27 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2010.14845", "submitter": "Ayoub Ben-Ameur", "authors": "Ayoub Ben-Ameur, Andrea Araldo and Francesco Bronzino", "title": "On the Deployability of Augmented Reality Using Embedded Edge Devices", "comments": "6 pages, 12 figures, Accepted in IEEE CCNC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge Computing exploits computational capabilities deployed at the very edge\nof the network to support applications with low latency requirements. Such\ncapabilities can reside in small embedded devices that integrate dedicated\nhardware -- e.g., a GPU -- in a low cost package. But these devices have\nlimited computing capabilities compared to standard server grade equipment.\nWhen deploying an Edge Computing based application, understanding whether the\navailable hardware can meet target requirements is key in meeting the expected\nperformance. In this paper, we study the feasibility of deploying Augmented\nReality applications using Embedded Edge Devices (EEDs). We compare such\ndeployment approach to one exploiting a standard dedicated server grade\nmachine. Starting from an empirical evaluation of the capabilities of these\ndevices, we propose a simple theoretical model to compare the performance of\nthe two approaches. We then validate such model with NS-3 simulations and study\ntheir feasibility. Our results show that there is no one-fits-all solution. If\nwe need to deploy high responsiveness applications, we need a centralized\nserver grade architecture and we can in any case only support very few users.\nThe centralized architecture fails to serve a larger number of users, even when\nlow to mid responsiveness is required. In this case, we need to resort instead\nto a distributed deployment based on EEDs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:32:53 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 21:19:26 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ben-Ameur", "Ayoub", ""], ["Araldo", "Andrea", ""], ["Bronzino", "Francesco", ""]]}, {"id": "2010.14931", "submitter": "Hebatalla Eldakiky", "authors": "Hebatalla Eldakiky, David Hung-Chang Du, Eman Ramadan (Department of\n  Computer Science and Engineering, University of Minnesota - Twin Cities, USA)", "title": "TurboKV: Scaling Up The Performance of Distributed Key-Value Stores With\n  In-Switch Coordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power and flexibility of software-defined networks lead to a programmable\nnetwork infrastructure in which in-network computation can help accelerating\nthe performance of applications. This can be achieved by offloading some\ncomputational tasks to the network. However, what kind of computational tasks\nshould be delegated to the network to accelerate applications performance? In\nthis paper, we propose a way to exploit the usage of programmable switches to\nscale up the performance of distributed key-value stores. Moreover, as a\nproof-of-concept, we propose TurboKV, an efficient distributed key-value store\narchitecture that utilizes programmable switches as: 1) partition management\nnodes to store the key-value store partitions and replicas information; and 2)\nmonitoring stations to measure the load of storage nodes, this monitoring\ninformation is used to balance the load among storage nodes. We also propose a\nkey-based routing protocol to route the search queries of clients based on the\nrequested keys to targeted storage nodes. Our experimental results of an\ninitial prototype show that our proposed architecture improves the throughput\nand reduces the latency of distributed key-value stores when compared to the\nexisting architectures.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:17:30 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Eldakiky", "Hebatalla", "", "Department of\n  Computer Science and Engineering, University of Minnesota - Twin Cities, USA"], ["Du", "David Hung-Chang", "", "Department of\n  Computer Science and Engineering, University of Minnesota - Twin Cities, USA"], ["Ramadan", "Eman", "", "Department of\n  Computer Science and Engineering, University of Minnesota - Twin Cities, USA"]]}, {"id": "2010.14934", "submitter": "Hugo Hadjur", "authors": "Hugo Hadjur (AVALON), Doreid Ammar, Laurent Lef\\`evre (AVALON)", "title": "Analysis of Energy Consumption in a Precision Beekeeping System", "comments": "IoT '20: 10th International Conference on the Internet of Things, Oct\n  2020, Malm{\\\"o}, Sweden", "journal-ref": null, "doi": "10.1145/3410992.3411010", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Honey bees have been domesticated by humans for several thousand years and\nmainly provide honey and pollination, which is fundamental for plant\nreproduction. Nowadays, the work of beekeepers is constrained by external\nfactors that stress their production (parasites and pesticides among others).\nTaking care of large numbers of beehives is time-consuming, so integrating\nsensors to track their status can drastically simplify the work of beekeepers.\nPrecision bee-keeping complements beekeepers' work thanks to the In-ternet of\nThings (IoT) technology. If used correctly, data can help to make the right\ndiagnosis for honey bees colony, increase honey production and decrease bee\nmortality. Providing enough energy for on-hive and in-hive sensors is a\nchallenge. Some solutions rely on energy harvesting, others target usage of\nlarge batteries. Either way, it is mandatory to analyze the energy usage of\nembedded equipment in order to design an energy efficient and autonomous bee\nmonitoring system. This paper relies on a fully autonomous IoT framework that\ncollects environmental and image data of a beehive. It consists of a data\ncollecting node (environmental data sensors, camera, Raspberry Pi and Arduino)\nand a solar energy supplying node. Supported services are analyzed task by task\nfrom an energy profiling and efficiency standpoint , in order to identify the\nhighly pressured areas of the framework. This first step will guide our goal of\ndesigning a sustainable precision beekeeping system, both technically and\nenergy-wise.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 12:44:09 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Hadjur", "Hugo", "", "AVALON"], ["Ammar", "Doreid", "", "AVALON"], ["Lef\u00e8vre", "Laurent", "", "AVALON"]]}, {"id": "2010.14993", "submitter": "Radoslava Hristova", "authors": "I. Hristov, R. Hristova, S. Dimova, P. Armyanov, N. Shegunov, I.\n  Puzynin, T. Puzynina, Z. Sharipov, Z. Tukhliev", "title": "Parallelizing multiple precision Taylor series method for integrating\n  the Lorenz system", "comments": "arXiv admin note: text overlap with arXiv:1908.09301", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid MPI+OpenMP strategy for parallelizing multiple precision Taylor\nseries method is proposed, realized and tested. To parallelize the algorithm we\ncombine MPI and OpenMP parallel technologies together with GMP library (GNU\nmiltiple precision libary) and the tiny MPIGMP library. The details of the\nparallelization are explained on the paradigmatic model of the Lorenz system.\nWe succeed to obtain a correct reference solution in the rather long time\ninterval - [0,7000]. The solution is verified by comparing the results for\n2700-th order Taylor series method and precision of ~ 3374 decimal digits, and\nthose with 2800-th order and precision of ~ 3510 decimal digits. With 192 CPU\ncores in Nestum cluster, Sofia, Bulgaria, the 2800-th order computation was ~\n145 hours with speedup ~ 105.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:28:08 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 17:41:26 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 17:42:38 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hristov", "I.", ""], ["Hristova", "R.", ""], ["Dimova", "S.", ""], ["Armyanov", "P.", ""], ["Shegunov", "N.", ""], ["Puzynin", "I.", ""], ["Puzynina", "T.", ""], ["Sharipov", "Z.", ""], ["Tukhliev", "Z.", ""]]}, {"id": "2010.15032", "submitter": "Daniel Barcelona-Pons", "authors": "Daniel Barcelona-Pons and Pedro Garc\\'ia-L\\'opez", "title": "Benchmarking Parallelism in FaaS Platforms", "comments": "19 pages, 15 figures, submitted to FGCS, revised", "journal-ref": null, "doi": "10.1016/j.future.2021.06.005", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has seen a myriad of work exploring its potential. Some\nsystems tackle Function-as-a-Service (FaaS) properties on automatic elasticity\nand scale to run highly-parallel computing jobs. However, they focus on\nspecific platforms and convey that their ideas can be extrapolated to any FaaS\nruntime.\n  An important question arises: do all FaaS platforms fit parallel\ncomputations? In this paper, we argue that not all of them provide the\nnecessary means to host highly-parallel applications. To validate our\nhypothesis, we create a comparative framework and categorize the architectures\nof four cloud FaaS offerings, emphasizing parallel performance. We attest and\nextend this description with an empirical experiment that consists in plotting\nin deep detail the evolution of a parallel computing job on each service.\n  The analysis of our results evinces that FaaS is not inherently good for\nparallel computations and architectural differences across platforms are\ndecisive to categorize their performance. A key insight is the importance of\nvirtualization technologies and the scheduling approach of FaaS platforms.\nParallelism improves with lighter virtualization and proactive scheduling due\nto finer resource allocation and faster elasticity. This causes some platforms\nlike AWS and IBM to perform well for highly-parallel computations, while others\nsuch as Azure present difficulties to achieve the required parallelism degree.\nConsequently, the information in this paper becomes of special interest to help\nusers choose the most adequate infrastructure for their parallel applications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 15:09:55 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 13:21:59 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 13:45:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Barcelona-Pons", "Daniel", ""], ["Garc\u00eda-L\u00f3pez", "Pedro", ""]]}, {"id": "2010.15162", "submitter": "Simon Eismann", "authors": "Simon Eismann, Long Bui, Johannes Grohmann, Cristina L. Abad, Nikolas\n  Herbst, Samuel Kounev", "title": "Sizeless: Predicting the optimal size of serverless functions", "comments": "11 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Serverless functions are a cloud computing paradigm where the provider takes\ncare of resource management tasks such as resource provisioning, deployment,\nand auto-scaling. The only resource management task that developers are still\nin charge of is selecting how much resources are allocated to each worker\ninstance. However, selecting the optimal size of serverless functions is quite\nchallenging, so developers often neglect it despite its significant cost and\nperformance benefits. Existing approaches aiming to automate serverless\nfunctions resource sizing require dedicated performance tests, which are\ntime-consuming to implement and maintain. In this paper, we introduce an\napproach to predict the optimal resource size of a serverless function using\nmonitoring data from a single resource size. As our approach does not require\ndedicated performance tests, it enables cloud providers to implement resource\nsizing on a platform level and automate the last resource management task\nassociated with serverless functions. We evaluate our approach on three\ndifferent serverless applications, where it selects the optimal memory size for\n71.7% of the serverless functions and the second-best memory size for 22.3% of\nthe serverless functions, which results in an average speedup of 43.6% while\nsimultaneously decreasing average costs by 10.2%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 18:22:00 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 10:49:58 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 08:34:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Eismann", "Simon", ""], ["Bui", "Long", ""], ["Grohmann", "Johannes", ""], ["Abad", "Cristina L.", ""], ["Herbst", "Nikolas", ""], ["Kounev", "Samuel", ""]]}, {"id": "2010.15206", "submitter": "Qiong Wu", "authors": "Qiong Wu, Zhenming Liu", "title": "Rosella: A Self-Driving Distributed Scheduler for Heterogeneous Clusters", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale interactive web services and advanced AI applications make\nsophisticated decisions in real-time, based on executing a massive amount of\ncomputation tasks on thousands of servers. Task schedulers, which often operate\nin heterogeneous and volatile environments, require high throughput, i.e.,\nscheduling millions of tasks per second, and low latency, i.e., incurring\nminimal scheduling delays for millisecond-level tasks. Scheduling is further\ncomplicated by other users' workloads in a shared system, other background\nactivities, and the diverse hardware configurations inside datacenters.\n  We present Rosella, a new self-driving, distributed approach for task\nscheduling in heterogeneous clusters. Our system automatically learns the\ncompute environment and adjust its scheduling policy in real-time. The solution\nprovides high throughput and low latency simultaneously, because it runs in\nparallel on multiple machines with minimum coordination and only performs\nsimple operations for each scheduling decision. Our learning module monitors\ntotal system load, and uses the information to dynamically determine optimal\nestimation strategy for the backends' compute-power. Our scheduling policy\ngeneralizes power-of-two-choice algorithms to handle heterogeneous workers,\nreducing the max queue length of $O(\\log n)$ obtained by prior algorithms to\n$O(\\log \\log n)$. We implement a Rosella prototype and evaluate it with a\nvariety of workloads. Experimental results show that Rosella significantly\nreduces task response times, and adapts to environment changes quickly.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:12:29 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 00:43:50 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wu", "Qiong", ""], ["Liu", "Zhenming", ""]]}, {"id": "2010.15210", "submitter": "Xing Hu", "authors": "Vassos Hadzilacos, Xing Hu, Sam Toueg", "title": "On Linearizability and the Termination of Randomized Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of whether the \"termination with probability 1\"\nproperty of a randomized algorithm is preserved when one replaces the atomic\nregisters that the algorithm uses with linearizable (implementations of)\nregisters. We show that in general this is not so: roughly speaking, every\nrandomized algorithm A has a corresponding algorithm A' that solves the same\nproblem if the registers that it uses are atomic or strongly-linearizable, but\ndoes not terminate if these registers are replaced with \"merely\" linearizable\nones. Together with a previous result shown in [15], this implies that one\ncannot use the well-known ABD implementation of registers in message-passing\nsystems to automatically transform any randomized algorithm that works in\nshared-memory systems into a randomized algorithm that works in message-passing\nsystems: with a strong adversary the resulting algorithm may not terminate.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:21:51 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hadzilacos", "Vassos", ""], ["Hu", "Xing", ""], ["Toueg", "Sam", ""]]}, {"id": "2010.15218", "submitter": "Johannes de Fine Licht", "authors": "Johannes de Fine Licht, Andreas Kuster, Tiziano De Matteis, Tal\n  Ben-Nun, Dominic Hofer, Torsten Hoefler", "title": "StencilFlow: Mapping Large Stencil Programs to Distributed Spatial\n  Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing devices have been shown to significantly accelerate stencil\ncomputations, but have so far relied on unrolling the iterative dimension of a\nsingle stencil operation to increase temporal locality. This work considers the\ngeneral case of mapping directed acyclic graphs of heterogeneous stencil\ncomputations to spatial computing systems, assuming large input programs\nwithout an iterative component. StencilFlow maximizes temporal locality and\nensures deadlock freedom in this setting, providing end-to-end analysis and\nmapping from a high-level program description to distributed hardware. We\nevaluate our generated architectures on a Stratix 10 FPGA testbed, yielding\n1.31 TOp/s and 4.18 TOp/s on single-device and multi-device, respectively,\ndemonstrating the highest performance recorded for stencil programs on FPGAs to\ndate. We then leverage the framework to study a complex stencil program from a\nproduction weather simulation application. Our work enables productively\ntargeting distributed spatial computing systems with large stencil programs,\nand offers insight into architecture characteristics required for their\nefficient execution in practice.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:32:19 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 16:57:07 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Licht", "Johannes de Fine", ""], ["Kuster", "Andreas", ""], ["De Matteis", "Tiziano", ""], ["Ben-Nun", "Tal", ""], ["Hofer", "Dominic", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.15388", "submitter": "Pulkit Misra", "authors": "Alok Kumbhare, Reza Azimi, Ioannis Manousakis, Anand Bonde, Felipe\n  Frujeri, Nithish Mahalingam, Pulkit Misra, Seyyed Ahmad Javadi, Bianca\n  Schroeder, Marcus Fontoura, Ricardo Bianchini", "title": "Prediction-Based Power Oversubscription in Cloud Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datacenter designers rely on conservative estimates of IT equipment power\ndraw to provision resources. This leaves resources underutilized and requires\nmore datacenters to be built. Prior work has used power capping to shave the\nrare power peaks and add more servers to the datacenter, thereby\noversubscribing its resources and lowering capital costs. This works well when\nthe workloads and their server placements are known. Unfortunately, these\nfactors are unknown in public clouds, forcing providers to limit the\noversubscription so that performance is never impacted.\n  In this paper, we argue that providers can use predictions of workload\nperformance criticality and virtual machine (VM) resource utilization to\nincrease oversubscription. This poses many challenges, such as identifying the\nperformance-critical workloads from black-box VMs, creating support for\ncriticality-aware power management, and increasing oversubscription while\nlimiting the impact of capping. We address these challenges for the hardware\nand software infrastructures of Microsoft Azure. The results show that we\nenable a 2x increase in oversubscription with minimum impact to critical\nworkloads.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 06:55:41 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Kumbhare", "Alok", ""], ["Azimi", "Reza", ""], ["Manousakis", "Ioannis", ""], ["Bonde", "Anand", ""], ["Frujeri", "Felipe", ""], ["Mahalingam", "Nithish", ""], ["Misra", "Pulkit", ""], ["Javadi", "Seyyed Ahmad", ""], ["Schroeder", "Bianca", ""], ["Fontoura", "Marcus", ""], ["Bianchini", "Ricardo", ""]]}, {"id": "2010.15444", "submitter": "Andreas Gocht", "authors": "Andreas Gocht and Robert Sch\\\"one and Jan Frenzel", "title": "Advanced Python Performance Monitoring with Score-P", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the last years, Python became more prominent in the scientific\ncommunity and is now used for simulations, machine learning, and data analysis.\nAll these tasks profit from additional compute power offered by parallelism and\noffloading. In the domain of High Performance Computing (HPC), we can look back\nto decades of experience exploiting different levels of parallelism on the\ncore, node or inter-node level, as well as utilising accelerators. By using\nperformance analysis tools to investigate all these levels of parallelism, we\ncan tune applications for unprecedented performance. Unfortunately, standard\nPython performance analysis tools cannot cope with highly parallel programs.\nSince the development of such software is complex and error-prone, we\ndemonstrate an easy-to-use solution based on an existing tool infrastructure\nfor performance analysis. In this paper, we describe how to apply the\nestablished instrumentation framework \\scorep to trace Python applications. We\nfinish with a study of the overhead that users can expect for instrumenting\ntheir applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:35:50 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Gocht", "Andreas", ""], ["Sch\u00f6ne", "Robert", ""], ["Frenzel", "Jan", ""]]}, {"id": "2010.15534", "submitter": "Sebastian Frischbier", "authors": "Manuel Coenen, Christoph Wagner, Alexander Echler, Sebastian\n  Frischbier", "title": "Poster: Benchmarking Financial Data Feed Systems", "comments": "Authors' version of the accepted submission; final version published\n  by ACM as part of the proceedings of DEBS '19: The 13th ACM International\n  Conference on Distributed and Event-based Systems (DEBS '19); 2 pages, 2\n  figures", "journal-ref": null, "doi": "10.1145/3328905.3332506", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven solutions for the investment industry require event-based backend\nsystems to process high-volume financial data feeds with low latency, high\nthroughput, and guaranteed delivery modes.\n  At vwd we process an average of 18 billion incoming event notifications from\n500+ data sources for 30 million symbols per day and peak rates of 1+ million\nnotifications per second using custom-built platforms that keep audit logs of\nevery event.\n  We currently assess modern open source event-processing platforms such as\nKafka, NATS, Redis, Flink or Storm for the use in our ticker plant to reduce\nthe maintenance effort for cross-cutting concerns and leverage hybrid\ndeployment models. For comparability and repeatability we benchmark candidates\nwith a standardized workload we derived from our real data feeds.\n  We have enhanced an existing light-weight open source benchmarking tool in\nits processing, logging, and reporting capabilities to cope with our workloads.\nThe resulting tool wrench can simulate workloads or replay snapshots in volume\nand dynamics like those we process in our ticker plant. We provide the tool as\nopen source.\n  As part of ongoing work we contribute details on (a) our workload and\nrequirements for benchmarking candidate platforms for financial feed\nprocessing; (b) the current state of the tool wrench.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:59:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Coenen", "Manuel", ""], ["Wagner", "Christoph", ""], ["Echler", "Alexander", ""], ["Frischbier", "Sebastian", ""]]}, {"id": "2010.15559", "submitter": "Rajkumar Buyya", "authors": "Sukhpal Singh Gill, Adarsh Kumar, Harvinder Singh, Manmeet Singh,\n  Kamalpreet Kaur, Muhammad Usman, Rajkumar Buyya", "title": "Quantum Computing: A Taxonomy, Systematic Review and Future Directions", "comments": "37 pages, 7 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": "Technical Report CLOUDS-TR-2020-1, Cloud Computing and Distributed\n  Systems Laboratory, The University of Melbourne, Australia, September 28,\n  2020", "categories": "cs.ET cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computing is an emerging paradigm with the potential to offer\nsignificant computational advantage over conventional classical computing by\nexploiting quantum-mechanical principles such as entanglement and\nsuperposition. It is anticipated that this computational advantage of quantum\ncomputing will help to solve many complex and computationally intractable\nproblems in several areas such as drug design, data science, clean energy,\nfinance, industrial chemical development, secure communications, and quantum\nchemistry. In recent years, tremendous progress in both quantum hardware\ndevelopment and quantum software/algorithm have brought quantum computing much\ncloser to reality. Indeed, the demonstration of quantum supremacy marks a\nsignificant milestone in the Noisy Intermediate Scale Quantum (NISQ) era - the\nnext logical step being the quantum advantage whereby quantum computers solve a\nreal-world problem much more efficiently than classical computing. As the\nquantum devices are expected to steadily scale up in the next few years,\nquantum decoherence and qubit interconnectivity are two of the major challenges\nto achieve quantum advantage in the NISQ era. Quantum computing is a highly\ntopical and fast-moving field of research with significant ongoing progress in\nall facets. This article presents a comprehensive review of quantum computing\nliterature, and taxonomy of quantum computing. Further, the proposed taxonomy\nis used to map various related studies to identify the research gaps. A\ndetailed overview of quantum software tools and technologies, post-quantum\ncryptography and quantum computer hardware development to document the current\nstate-of-the-art in the respective areas. We finish the article by highlighting\nvarious open challenges and promising future directions for research.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 09:15:22 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 06:16:44 GMT"}, {"version": "v3", "created": "Sun, 18 Apr 2021 11:15:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gill", "Sukhpal Singh", ""], ["Kumar", "Adarsh", ""], ["Singh", "Harvinder", ""], ["Singh", "Manmeet", ""], ["Kaur", "Kamalpreet", ""], ["Usman", "Muhammad", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2010.15582", "submitter": "Mustafa Ozdayi", "authors": "Mustafa Safa Ozdayi, Murat Kantarcioglu, Rishabh Iyer", "title": "Improving Accuracy of Federated Learning in Non-IID Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a decentralized machine learning protocol that\nallows a set of participating agents to collaboratively train a model without\nsharing their data. This makes FL particularly suitable for settings where data\nprivacy is desired. However, it has been observed that the performance of FL is\nclosely tied with the local data distributions of agents. Particularly, in\nsettings where local data distributions vastly differ among agents, FL performs\nrather poorly with respect to the centralized training. To address this\nproblem, we hypothesize the reasons behind the performance degradation, and\ndevelop some techniques to address these reasons accordingly. In this work, we\nidentify four simple techniques that can improve the performance of trained\nmodels without incurring any additional communication overhead to FL, but\nrather, some light computation overhead either on the client, or the\nserver-side. In our experimental analysis, combination of our techniques\nimproved the validation accuracy of a model trained via FL by more than 12%\nwith respect to our baseline. This is about 5% less than the accuracy of the\nmodel trained on centralized data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 21:02:14 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ozdayi", "Mustafa Safa", ""], ["Kantarcioglu", "Murat", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2010.15718", "submitter": "Jia Qian", "authors": "Jia Qian, Hiba Nassar, Lars Kai Hansen", "title": "Minimal conditions analysis of gradient-based reconstruction in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input data from a neural network may be reconstructed using knowledge of\nthe gradients of that network, as demonstrated by \\cite{zhu2019deep}. By\nimposing prior information and utilising a uniform initialization we\ndemonstrate faster and more accurate image reconstruction. Exploring the\ntheoretical limits of reconstruction, we show that a single input may be\nreconstructed, regardless of network depth using a fully-connected neural\nnetwork with one hidden node. Then we generalize this result to a gradient\naveraged over mini-batches of size $B$. In this case, the full mini-batch can\nbe reconstructed if the number of hidden units exceeds $B$, with an\northogonality regularizer to improve the precision. For a Convolutional Neural\nNetwork, the required number of filters in the first convolutional layer is\ndecided by multiple factors (e.g., padding, kernel and stride size). Therefore,\nwe require the number of filters, $h\\geq (\\frac{d}{d^{'}})^2C$, where $d$ is\ninput width, $d^{'}$ is output width after convolution kernel, and $C$ is\nchannel number of input. Finally, we validate our theoretical analysis and\nimprovements using bio-medical (fMRI) and benchmark data (MNIST,\nKuzushiji-MNIST, CIFAR100, ImageNet and face images).\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 16:05:45 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 15:50:14 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 10:03:16 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Qian", "Jia", ""], ["Nassar", "Hiba", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "2010.15755", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff, Manuel P\\\"oter", "title": "A more Pragmatic Implementation of the Lock-free, Ordered, Linked List", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lock-free, ordered, linked list is an important, standard example of a\nconcurrent data structure. An obvious, practical drawback of textbook\nimplementations is that failed compare-and-swap (CAS) operations lead to\nretraversal of the entire list (retries), which is particularly harmful for\nsuch a linear-time data structure. We alleviate this drawback by first\nobserving that failed CAS operations under some conditions do not require a\nfull retry, and second by maintaining approximate backwards pointers that are\nused to find a closer starting position in the list for operation retry.\nExperiments with both a worst-case deterministic benchmark, and a standard,\nrandomized, mixed-operation throughput benchmark on three shared-memory systems\n(Intel Xeon, AMD EPYC, SPARC-T5) show practical improvements ranging from\nsignificant, to dramatic, several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 16:59:49 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 09:57:37 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""], ["P\u00f6ter", "Manuel", ""]]}, {"id": "2010.15862", "submitter": "Carlos Pedroso", "authors": "Carlos Pedroso, Aldri Santos, Michele Nogueira", "title": "Detecting FDI Attack on Dense IoT Network with Distributed Filtering\n  Collaboration and Consensus", "comments": "This work has been accept to the IEEE LATINCOM2020. Copyright\n  978-1-7281-8903-1/20/$31.00 \\c{opyright}2020 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of IoT has made possible the development of %increasingly\npersonalized services, like industrial services that often deal with massive\namounts of data. However, as IoT grows, its threats are even greater. The false\ndata injection (FDI) attack stands out as being one of the most harmful to data\nnetworks like IoT. The majority of current systems to handle this attack do not\ntake into account the data validation, especially on the data clustering\nservice. This work introduces CONFINIT, an intrusion detection system against\nFDI attacks on the data dissemination service into dense IoT. It combines\nwatchdog surveillance and collaborative consensus among IoT devices for getting\nthe swift detection of attackers. CONFINIT was evaluated in the NS-3 simulator\ninto a dense industrial IoT and it has gotten detection rates of 99%, 3.2% of\nfalse negative and 3.6% of false positive rates, adding up to 35% in clustering\nwithout FDI attackers.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:08:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Pedroso", "Carlos", ""], ["Santos", "Aldri", ""], ["Nogueira", "Michele", ""]]}, {"id": "2010.15879", "submitter": "Maciej Besta", "authors": "Maciej Besta, Dimitri Stanojevic, Tijana Zivic, Jagpreet Singh,\n  Maurice Hoerold, Torsten Hoefler", "title": "Log(Graph): A Near-Optimal High-Performance Graph Representation", "comments": null, "journal-ref": "Proceedings of the 27th International Conference on Parallel\n  Architectures and Compilation (PACT'18), 2018", "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's graphs used in domains such as machine learning or social network\nanalysis may contain hundreds of billions of edges. Yet, they are not\nnecessarily stored efficiently, and standard graph representations such as\nadjacency lists waste a significant number of bits while graph compression\nschemes such as WebGraph often require time-consuming decompression. To address\nthis, we propose Log(Graph): a graph representation that combines high\ncompression ratios with very low-overhead decompression to enable cheaper and\nfaster graph processing. The key idea is to encode a graph so that the parts of\nthe representation approach or match the respective storage lower bounds. We\ncall our approach \"graph logarithmization\" because these bounds are usually\nlogarithmic. Our high-performance Log(Graph) implementation based on modern\nbitwise operations and state-of-the-art succinct data structures achieves high\ncompression ratios as well as performance. For example, compared to the tuned\nGraph Algorithm Processing Benchmark Suite (GAPBS), it reduces graph sizes by\n20-35% while matching GAPBS' performance or even delivering speedups due to\nreducing amounts of transferred data. It approaches the compression ratio of\nthe established WebGraph compression library while enabling speedups of up to\nmore than 2x. Log(Graph) can improve the design of various graph processing\nengines or libraries on single NUMA nodes as well as distributed-memory\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:41:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Besta", "Maciej", ""], ["Stanojevic", "Dimitri", ""], ["Zivic", "Tijana", ""], ["Singh", "Jagpreet", ""], ["Hoerold", "Maurice", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.15965", "submitter": "Dhruv Guliani", "authors": "Dhruv Guliani, Francoise Beaufays, Giovanni Motta", "title": "Training Speech Recognition Models with Federated Learning: A\n  Quality/Cost Framework", "comments": "Paper published at ICASSP 2021", "journal-ref": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), 2021, pp. 3080-3084", "doi": "10.1109/ICASSP39728.2021.9413397", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using federated learning, a decentralized on-device learning\nparadigm, to train speech recognition models. By performing epochs of training\non a per-user basis, federated learning must incur the cost of dealing with\nnon-IID data distributions, which are expected to negatively affect the quality\nof the trained model. We propose a framework by which the degree of\nnon-IID-ness can be varied, consequently illustrating a trade-off between model\nquality and the computational cost of federated training, which we capture\nthrough a novel metric. Finally, we demonstrate that hyper-parameter\noptimization and appropriate use of variational noise are sufficient to\ncompensate for the quality impact of non-IID distributions, while decreasing\nthe cost.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:01:37 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 18:49:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Guliani", "Dhruv", ""], ["Beaufays", "Francoise", ""], ["Motta", "Giovanni", ""]]}, {"id": "2010.15973", "submitter": "Alexandra Bates", "authors": "Alexandra Bates and Joseph Bates", "title": "Toward Lattice QCD On Billion Core Approximate Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present evidence of the feasibility of using billion core approximate\ncomputers to run simple U(1) sigma models, and discuss how the approach might\nbe extended to Lattice Quantum Chromodynamics (LQCD) models. This work is\nmotivated by the extreme time, power, and cost needed to run LQCD on current\ncomputing hardware. We show that, using massively parallel approximate\nhardware, at least some models can run with great speed and power efficiency\nwithout sacrificing accuracy. As a test of accuracy, a 32 x 32 x 32 U(1) sigma\nmodel yielded similar results using floating point and approximate\nrepresentations for the spins. A 20 million point 3D model, run on a\n34,000-core single-board prototype approximate computer, showed encouraging\naccuracy with a ~750 times improvement in speed and ~2500 times improvement in\nspeed/watt compared to a traditional CPU. These results suggest there is value\nin future research to determine whether similar speed-ups and accuracies are\npossible running full LQCD on the compact billion-core approximate computing\nsystems that are now practical.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:29:57 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Bates", "Alexandra", ""], ["Bates", "Joseph", ""]]}, {"id": "2010.16012", "submitter": "Maciej Besta", "authors": "Maciej Besta, Michal Podstawski, Linus Groner, Edgar Solomonik,\n  Torsten Hoefler", "title": "To Push or To Pull: On Reducing Communication and Synchronization in\n  Graph Computations", "comments": null, "journal-ref": "Proceedings of the 26th ACM International Symposium on\n  High-Performance Parallel and Distributed Computing (HPDC'17), 2017", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce the cost of communication and synchronization in graph processing\nby analyzing the fastest way to process graphs: pushing the updates to a shared\nstate or pulling the updates to a private state.We investigate the\napplicability of this push-pull dichotomy to various algorithms and its impact\non complexity, performance, and the amount of used locks, atomics, and\nreads/writes. We consider 11 graph algorithms, 3 programming models, 2 graph\nabstractions, and various families of graphs. The conducted analysis\nillustrates surprising differences between push and pull variants of different\nalgorithms in performance, speed of convergence, and code complexity; the\ninsights are backed up by performance data from hardware counters.We use these\nfindings to illustrate which variant is faster for each algorithm and to\ndevelop generic strategies that enable even higher speedups. Our insights can\nbe used to accelerate graph processing engines or libraries on both\nmassively-parallel shared-memory machines as well as distributed-memory\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 01:04:57 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Besta", "Maciej", ""], ["Podstawski", "Michal", ""], ["Groner", "Linus", ""], ["Solomonik", "Edgar", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.16034", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, Ting Yang, Yu Wang", "title": "State sharding model on the blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is an incrementally updated ledger maintained by distributed nodes\nrather than centralized organizations. The current blockchain technology faces\nscalability issues, which include two aspects: low transaction throughput and\nhigh storage capacity costs. This paper studies the blockchain structure based\non state sharding technology, and mainly solves the problem of non-scalability\nof block chain storage. This paper designs and implements the blockchain state\nsharding scheme, proposes a specific state sharding data structure and\nalgorithm implementation, and realizes a complete blockchain structure so that\nthe blockchain has the advantages of high throughput, processing a large number\nof transactions and saving storage costs. Experimental results show that a\nblockchain network with more than 100,000 nodes can be divided into 1024\nshards. A blockchain network with this structure can process 500,000\ntransactions in about 5 seconds. If the consensus time of the blockchain is\nabout 10 seconds, and the block generation time of the blockchain system of the\nsharding mechanism is 15 seconds, the transaction throughput can reach 33,000\ntx/sec. Experimental results show that the throughput of the proposed protocol\nincreases with the increase of the network node size. This confirms the\nscalability of the blockchain structure based on sharding technology.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 02:55:19 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wang", "Xiangyu", ""], ["Yang", "Ting", ""], ["Wang", "Yu", ""]]}, {"id": "2010.16058", "submitter": "Anton Eremeev", "authors": "Anton V. Eremeev, Anton A. Malakhov, Maxim A. Sakhno, and Maria Y.\n  Sosnovskaya", "title": "Multi-Core Processor Scheduling with Respect to Data Bus Bandwidth", "comments": "Accepted for the second volume of OPTIMA'2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of scheduling software modules on a\nmulti-core processor, taking into account the limited bandwidth of the data bus\nand the precedence constraints. Two problem formulations with different levels\nof problem-specific detail are suggested and both shown to be NP-hard. A mixed\ninteger linear programming (MILP) model is proposed for the first problem\nformulation, and a greedy algorithm is developed for the second one. An\nexperimental comparison of the results of the greedy algorithm and the MILP\nsolutions found by CPLEX solver is carried out.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 04:16:06 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Eremeev", "Anton V.", ""], ["Malakhov", "Anton A.", ""], ["Sakhno", "Maxim A.", ""], ["Sosnovskaya", "Maria Y.", ""]]}]