[{"id": "1812.00111", "submitter": "Swapneel Mehta", "authors": "Swapneel Mehta, Prasanth Kothuri, Daniel Lanza Garcia", "title": "A Big Data Architecture for Log Data Storage and Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/978-981-10-8797-4_22", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an architecture for analysing database connection logs across\ndifferent instances of databases within an intranet comprising over 10,000\nusers and associated devices. Our system uses Flume agents to send\nnotifications to a Hadoop Distributed File System for long-term storage and\nElasticSearch and Kibana for short-term visualisation, effectively creating a\ndata lake for the extraction of log data. We adopt machine learning models with\nan ensemble of approaches to filter and process the indicators within the data\nand aim to predict anomalies or outliers using feature vectors built from this\nlog data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 00:50:45 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Mehta", "Swapneel", ""], ["Kothuri", "Prasanth", ""], ["Garcia", "Daniel Lanza", ""]]}, {"id": "1812.00182", "submitter": "Fabian Schuiki", "authors": "Fabian Schuiki, Michael Schaffner, Luca Benini", "title": "NTX: An Energy-efficient Streaming Accelerator for Floating-point\n  Generalized Reduction Workloads in 22nm FD-SOI", "comments": "6 pages, invited paper at DATE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized coprocessors for Multiply-Accumulate (MAC) intensive workloads\nsuch as Deep Learning are becoming widespread in SoC platforms, from GPUs to\nmobile SoCs. In this paper we revisit NTX (an efficient accelerator developed\nfor training Deep Neural Networks at scale) as a generalized MAC and reduction\nstreaming engine. The architecture consists of a set of 32 bit floating-point\nstreaming co-processors that are loosely coupled to a RISC-V core in charge of\norchestrating data movement and computation. Post-layout results of a recent\nsilicon implementation in 22 nm FD-SOI technology show the accelerator's\ncapability to deliver up to 20 Gflop/s at 1.25 GHz and 168 mW. Based on these\nresults we show that a version of NTX scaled down to 14 nm can achieve a 3x\nenergy efficiency improvement over contemporary GPUs at 10.4x less silicon\narea, and a compute performance of 1.4 Tflop/s for training large\nstate-of-the-art networks with full floating-point precision. An extended\nevaluation of MAC-intensive kernels shows that NTX can consistently achieve up\nto 87% of its peak performance across general reduction workloads beyond\nmachine learning. Its modular architecture enables deployment at different\nscales ranging from high-performance GPU-class to low-power embedded scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 09:58:51 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Schuiki", "Fabian", ""], ["Schaffner", "Michael", ""], ["Benini", "Luca", ""]]}, {"id": "1812.00300", "submitter": "Rajkumar Buyya", "authors": "Maria A. Rodriguez and Rajkumar Buyya", "title": "Containers Orchestration with Cost-Efficient Autoscaling in Cloud\n  Computing Environments", "comments": "22 pages, 4 figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containers are standalone, self-contained units that package software and its\ndependencies together. They offer lightweight performance isolation, fast and\nflexible deployment, and fine-grained resource sharing. They have gained\npopularity in better application management and deployment in recent years and\nare being widely used by organizations to deploy their increasingly diverse\nworkloads such as web services, big data, and IoT in either proprietary\nclusters or cloud data centres. This has led to the emergence of container\norchestration platforms, which are designed to manage the deployment of\ncontainerized applications in large-scale clusters. The majority of these\nplatforms are tailored to optimize the scheduling of containers on a\nfixed-sized private cluster but are not enabled to autoscale the size of the\ncluster nor to consider features specific to public cloud environments. In this\nwork, we propose a comprehensive container resource management approach that\nhas three different objectives. The first one is to optimize the initial\nplacement of containers by efficiently scheduling them on existing resources.\nThe second one is to autoscale the number of resources at runtime based on the\ncurrent cluster's workload. The third one is a rescheduling mechanism to\nfurther support the efficient use of resources by consolidating applications\ninto fewer VMs when possible. Our algorithms are implemented as a\nplugin-scheduler for Kubernetes platform. We evaluated our framework and the\neffectiveness of the proposed algorithms on an Australian national cloud\ninfrastructure. Our experiments demonstrate that considerable cost savings can\nbe achieved by dynamically managing the cluster size and placement of\napplications. We find that our proposed approaches are capable of reducing the\ncost by 58% when compared to the default Kubernetes scheduler.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 01:58:23 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Rodriguez", "Maria A.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1812.00302", "submitter": "Rajkumar Buyya", "authors": "Rajinder Sandhu, Adel Nadjaran Toosi, and Rajkumar Buyya", "title": "An API for Development of User-Defined Scheduling Algorithms in Aneka\n  PaaS Cloud Software: User Defined Schedulers in Aneka PaaS Cloud Software", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has been developed as one of the prominent paradigm for\nproviding on demand resources to the end user based on signed service level\nagreement and pay as use model. Cloud computing provides resources using\nmultitenant architecture where infrastructure is generated from multiple or\nsingle geographical distributed cloud datacenters. Scheduling of cloud\napplication requests to cloud infrastructure is one of the main research area\nin cloud computing. Researchers have developed many scheduling applications for\nwhich they have used different simulators available in the market such as\nCloudSim. Performance of any scheduling algorithm will be different when\napplied to real time cloud environment as compared to simulation software.\nAneka is one of the prominent PaaS software which allows users to develop cloud\napplication using various programming models and underline infrastructure. In\nthis chapter, a scheduling API is developed over the Aneka software platform\nwhich can be easily integrated with the Aneka software. Users can develop their\nown scheduling algorithms using this API and integrate it with Aneka software\nso that they can test their scheduling algorithm in real cloud environment. The\nproposed API provides all the required functionalities to integrate and\nschedule private, public or hybrid cloud with the Aneka software.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 02:07:50 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 10:01:03 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sandhu", "Rajinder", ""], ["Toosi", "Adel Nadjaran", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1812.00476", "submitter": "Abdulkadir Celik", "authors": "Abdulkadir Celik, Ming-Cheng Tsai, Redha M. Radaydeh, Fawaz S.\n  Al-Qahtani, Mohamed-Slim Alouini", "title": "Distributed Cluster Formation and Power-Bandwidth Allocation for\n  Imperfect NOMA in DL-HetNets", "comments": "To appear in IEEE TCOM", "journal-ref": null, "doi": "10.1109/TCOMM.2018.2879508", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an non-ideal successive interference cancellation\n(SIC) receiver based imperfect non-orthogonal multiple access (NOMA) schemes\nwhose performance is limited by three factors: 1) Power disparity \\&\nsensitivity constraints (PDSCs), 2) Intra-cluster interference (ICRI), and 3)\nIntercell-interference (ICI). By quantifying the residual interference with a\nfractional error factor (FEF), we show that NOMA cannot always perform better\nthan orthogonal multiple access (OMA) especially under certain receiver\nsensitivity and FEF levels. Assuming the existence of an offline/online ICI\nmanagement scheme, the proposed solution accounts for the ICI which is shown to\ndeteriorate the NOMA performance particularly when it becomes significant\ncompared to the ICRI. Then, a distributed cluster formation (CF) and\npower-bandwidth allocation (PBA) approach are proposed for downlink (DL)\nheterogeneous networks (HetNets) operating on the imperfect NOMA. We develop a\nhierarchically distributed solution methodology where BSs independently form\nclusters and distributively determine the power-bandwidth allowance of each\ncluster. A generic CF scheme is obtained by creating a multi-partite graph\n(MPG) via partitioning user equipments (UEs) with respect to their channel\ngains since NOMA performance is primarily determined by the channel gain\ndisparity of cluster members. A sequential weighted bi-partite matching method\nis proposed for solving the resulted weighted multi-partite matching problem.\nThereafter, we present a hierarchically distributed PBA approach which consists\nof the primary master, secondary masters, and slave problems...\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 22:03:10 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Celik", "Abdulkadir", ""], ["Tsai", "Ming-Cheng", ""], ["Radaydeh", "Redha M.", ""], ["Al-Qahtani", "Fawaz S.", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1812.00563", "submitter": "Rajkumar Buyya", "authors": "Muhammed Tawfiqul Islam and Rajkumar Buyya", "title": "Resource Management and Scheduling for Big Data Applications in Cloud\n  Computing Environments", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter presents software architectures of the big data processing\nplatforms. It will provide an in-depth knowledge on resource management\ntechniques involved while deploying big data processing systems on cloud\nenvironment. It starts from the very basics and gradually introduce the core\ncomponents of resource management which we have divided in multiple layers. It\ncovers the state-of-art practices and researches done in SLA-based resource\nmanagement with a specific focus on the job scheduling mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 05:38:29 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 03:59:53 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Islam", "Muhammed Tawfiqul", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1812.00591", "submitter": "Rajkumar Buyya", "authors": "Chii Chang, Satish Narayana Srirama and Rajkumar Buyya", "title": "Internet of Things (IoT) and New Computing Paradigms", "comments": "23 pages, 3 figures", "journal-ref": "Fog and Edge Computing: Principles and Paradigms, ISBN:\n  978-1-119-52498-4, Wiley Press, New York, USA, 2018", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chapter presents foundations of computing paradigms for realizing\nemerging IoT applications, especially fog and edge computing, their background,\ncharacteristics, architectures and open challenges.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 08:07:43 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Chang", "Chii", ""], ["Srirama", "Satish Narayana", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1812.00593", "submitter": "Rajkumar Buyya", "authors": "Adel Nadjaran Toosi, Redowan Mahmud, Qinghua Chi and Rajkumar Buyya", "title": "Management and Orchestration of Network Slices in 5G, Fog, Edge and\n  Clouds", "comments": "31 pages, 4 figures, Fog and Edge Computing: Principles and\n  Paradigms, Wiley Press, New York, USA, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network slicing allows network operators to build multiple isolated virtual\nnetworks on a shared physical network to accommodate a wide variety of services\nand applications. With network slicing, service providers can provide a\ncost-efficient solution towards meeting diverse performance requirements of\ndeployed applications and services. Despite slicing benefits, End-to-End\norchestration and management of network slices is a challenging and complicated\ntask. In this chapter, we intend to survey all the relevant aspects of network\nslicing, with the focus on networking technologies such as Software-defined\nnetworking (SDN) and Network Function Virtualization (NFV) in 5G, Fog/Edge and\nCloud Computing platforms. To build the required background, this chapter\nbegins with a brief overview of 5G, Fog/Edge and Cloud computing, and their\ninterplay. Then we cover the 5G vision for network slicing and extend it to the\nFog and Cloud computing through surveying the state-of-the-art slicing\napproaches in these platforms. We conclude the chapter by discussing future\ndirections, analyzing gaps and trends towards the network slicing realization.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 08:11:01 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Toosi", "Adel Nadjaran", ""], ["Mahmud", "Redowan", ""], ["Chi", "Qinghua", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1812.00854", "submitter": "Juho Hirvonen", "authors": "Klaus-Tycho Foerster, Juho Hirvonen, Stefan Schmid, Jukka Suomela", "title": "On the Power of Preprocessing in Decentralized Network Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As communication networks are growing at a fast pace, the need for more\nscalable approaches to operate such networks is pressing. Decentralization and\nlocality are key concepts to provide scalability. Existing models for which\nlocal algorithms are designed fail to model an important aspect of many modern\ncommunication networks such as software-defined networks: the possibility to\nprecompute distributed network state. We take this as an opportunity to study\nthe fundamental question of how and to what extent local algorithms can benefit\nfrom preprocessing. In particular, we show that preprocessing allows for\nsignificant speedups of various networking problems. A main benefit is the\nprecomputation of structural primitives, where purely distributed algorithms\nhave to start from scratch. Maybe surprisingly, we also show that there are\nstrict limitations on how much preprocessing can help in different scenarios.\nTo this end, we provide approximation bounds for the maximum independent set\nproblem---which however show that our obtained speedups are asymptotically\noptimal. Even though we show that physical link failures in general hinder the\npower of preprocessing, we can still facilitate the precomputation of symmetry\nbreaking processes to bypass various runtime barriers. We believe that our\nmodel and results are of interest beyond the scope of this paper and apply to\nother dynamic networks as well.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 15:54:33 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Foerster", "Klaus-Tycho", ""], ["Hirvonen", "Juho", ""], ["Schmid", "Stefan", ""], ["Suomela", "Jukka", ""]]}, {"id": "1812.00904", "submitter": "Gyorgy Matyasfalvi", "authors": "Jonathan Eckstein and Gyorgy Matyasfalvi", "title": "Efficient Distributed-Memory Parallel Matrix-Vector Multiplication with\n  Wide or Tall Unstructured Sparse Matrices", "comments": "8 pages, IEEE format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient technique for matrix-vector and\nvector-transpose-matrix multiplication in distributed-memory parallel computing\nenvironments, where the matrices are unstructured, sparse, and have a\nsubstantially larger number of columns than rows or vice versa. Our method\nallows for parallel I/O, does not require extensive preprocessing, and has the\nsame communication complexity as matrix-vector multiplies with column or row\npartitioning. Our implementation of the method uses MPI. We partition the\nmatrix by individual nonzero elements, rather than by row or column, and use an\n\"overlapped\" vector representation that is matched to the matrix. The transpose\nmultiplies use matrix-specific MPI communicators and reductions that we show\ncan be set up in an efficient manner. The proposed technique achieves a good\nwork per processor balance even if some of the columns are dense, while keeping\ncommunication costs relatively low.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 17:03:15 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Eckstein", "Jonathan", ""], ["Matyasfalvi", "Gyorgy", ""]]}, {"id": "1812.00994", "submitter": "Md. Redowan Mahmud", "authors": "Redowan Mahmud, Rajkumar Buyya", "title": "Modelling and Simulation of Fog and Edge Computing Environments using\n  iFogSim Toolkit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource management in Fog computing is very complicated as it engages\nsignificant number of diverse and resource constraint Fog nodes to meet\ncomputational demand of IoT-enabled systems in distributed manner. Its\nintegration with Cloud triggers further difficulties in combined resource\nmanagement. Different sensing frequency of IoT devices, distributed application\nstructure and their coordination also influence resource management in Fog\ncomputing environment. For advancement of Fog and its resource management, the\nnecessity of extensive research in beyond question. To develop and evaluate\ndifferent ideas and resource management policies, empirical analysis on Fog\nenvironment is the key. Since Fog computing environment incorporates IoT\ndevices, Fog nodes and Cloud datacenters along with huge amount of IoT-data and\ndistributed applications, real-world implementation of Fog environment for\nresearch will be very costly. Moreover, modification of any entity in\nreal-world Fog environment will be tedious. In this circumstance, simulation of\nFog computing environment can be very helpful. Simulation toolkits not only\nprovide frameworks to design customized experiment environment but also assist\nin repeatable evaluation. In this chapter we focus on delivering a tutorial on\niFogSim toolkit for modelling and simulating Fog computing environments. We\nbelieve this chapter will offer researchers a simplified way to apply iFogSim\nin their research works.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 00:43:32 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Mahmud", "Redowan", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1812.01141", "submitter": "Vibhuti Gupta", "authors": "Vibhuti Gupta and Rattikorn Hewett", "title": "Unleashing the Power of Hashtags in Tweet Analytics with Distributed\n  Framework on Apache Storm", "comments": "IEEE International Conference on Big Data 2018", "journal-ref": "2018, pp. 4554-4558", "doi": "10.1109/BigData.2018.8622302", "report-no": null, "categories": "cs.DC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is a popular social network platform where users can interact and\npost texts of up to 280 characters called tweets. Hashtags, hyperlinked words\nin tweets, have increasingly become crucial for tweet retrieval and search.\nUsing hashtags for tweet topic classification is a challenging problem because\nof context dependent among words, slangs, abbreviation and emoticons in a short\ntweet along with evolving use of hashtags. Since Twitter generates millions of\ntweets daily, tweet analytics is a fundamental problem of Big data stream that\noften requires a real-time Distributed processing. This paper proposes a\ndistributed online approach to tweet topic classification with hashtags. Being\nimplemented on Apache Storm, a distributed real time framework, our approach\nincrementally identifies and updates a set of strong predictors in the Na\\\"ive\nBayes model for classifying each incoming tweet instance. Preliminary\nexperiments show promising results with up to 97% accuracy and 37% increase in\nthroughput on eight processors.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 00:35:22 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gupta", "Vibhuti", ""], ["Hewett", "Rattikorn", ""]]}, {"id": "1812.01324", "submitter": "Alexey Shigarov", "authors": "I. Bychkov, A. Demichev, J. Dubenskaya, O. Fedorov, A. Hmelnov, Y.\n  Kazarina, E. Korosteleva, D. Kostunin, A. Kryukov, A. Mikhailov, M.D. Nguyen,\n  S. Polyakov, E. Postnikov, A. Shigarov, D. Shipilov, D. Zhurov", "title": "Using Binary File Format Description Languages for Documenting, Parsing,\n  and Verifying Raw Data in TAIGA Experiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is devoted to the issues of raw binary data documenting, parsing\nand verifying in astroparticle data lifecycle. The long-term preservation of\nraw data of astroparticle experiments as originally generated is essential for\nre-running analyses and reproducing research results. The selected high-quality\nraw data should have detailed documentation and accompanied by open software\ntools for access to them. We consider applicability of binary file format\ndescription languages to specify, parse and verify raw data of the Tunka\nAdvanced Instrument for cosmic rays and Gamma Astronomy (TAIGA) experiment. The\nformal specifications are implemented for five data formats of the experiment\nand provide automatic generation of source code for data reading libraries in\ntarget programming languages (e.g. C++, Java, and Python). These libraries were\ntested on TAIGA data. They showed a good performance and help us to locate the\nparts with corrupted data. The format specifications can be used as metadata\nfor exchanging of astroparticle raw data. They can also simplify software\ndevelopment for data aggregation from various sources for the multi-messenger\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 10:51:53 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Bychkov", "I.", ""], ["Demichev", "A.", ""], ["Dubenskaya", "J.", ""], ["Fedorov", "O.", ""], ["Hmelnov", "A.", ""], ["Kazarina", "Y.", ""], ["Korosteleva", "E.", ""], ["Kostunin", "D.", ""], ["Kryukov", "A.", ""], ["Mikhailov", "A.", ""], ["Nguyen", "M. D.", ""], ["Polyakov", "S.", ""], ["Postnikov", "E.", ""], ["Shigarov", "A.", ""], ["Shipilov", "D.", ""], ["Zhurov", "D.", ""]]}, {"id": "1812.01344", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Massimo Villari and Omer Rana and Philip James\n  and Tejal Shal and Maria Fazio and Rajiv Ranjan", "title": "Realizing Edge Marketplaces: Challenges and Opportunities", "comments": "Published in IEEE Cloud Computing, Volume 5, Issue 6, 2018, pp. 9-20", "journal-ref": "B. Varghese et al., \"Realizing Edge Marketplaces: Challenges and\n  Opportunities,\" in IEEE Cloud Computing, vol. 5, no. 6, pp. 9-20, Nov./Dec.\n  2018", "doi": "10.1109/MCC.2018.064181115", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edge of the network has the potential to host services for supporting a\nvariety of user applications, ranging in complexity from data preprocessing,\nimage and video rendering, and interactive gaming, to embedded systems in\nautonomous cars and built environments. However, the computational and data\nresources over which such services are hosted, and the actors that interact\nwith these services, have an intermittent availability and access profile,\nintroducing significant risk for user applications that must rely on them. This\narticle investigates the development of an edge marketplace, which is able to\nsupport multiple providers for offering services at the network edge, and to\nenable demand supply for influencing the operation of such a marketplace.\nResilience, cost, and quality of service and experience will subsequently\nenable such a marketplace to adapt its services over time. This article also\ndescribes how distributed-ledger technologies (such as blockchains) provide a\npromising approach to support the operation of such a marketplace and regulate\nits behavior (such as the GDPR in Europe) and operation. Two application\nscenarios provide context for the discussion of how such a marketplace would\nfunction and be utilized in practice.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 11:28:42 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Varghese", "Blesson", ""], ["Villari", "Massimo", ""], ["Rana", "Omer", ""], ["James", "Philip", ""], ["Shal", "Tejal", ""], ["Fazio", "Maria", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1812.01371", "submitter": "Moritz Hoffmann", "authors": "Moritz Hoffmann, Andrea Lattuada, Frank McSherry, Vasiliki Kalavri,\n  John Liagouris, and Timothy Roscoe", "title": "Megaphone: Latency-conscious state migration for distributed streaming\n  dataflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and implement Megaphone, a data migration mechanism for stateful\ndistributed dataflow engines with latency objectives. When compared to existing\nmigration mechanisms, Megaphone has the following differentiating\ncharacteristics: (i) migrations can be subdivided to a configurable granularity\nto avoid latency spikes, and (ii) migrations can be prepared ahead of time to\navoid runtime coordination. Megaphone is implemented as a library on an\nunmodified timely dataflow implementation, and provides an operator interface\ncompatible with its existing APIs. We evaluate Megaphone on established\nbenchmarks with varying amounts of state and observe that compared to na\\\"ive\napproaches Megaphone reduces service latencies during reconfiguration by orders\nof magnitude without significantly increasing steady-state overhead.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 12:30:04 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 08:42:46 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 06:04:05 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Hoffmann", "Moritz", ""], ["Lattuada", "Andrea", ""], ["McSherry", "Frank", ""], ["Kalavri", "Vasiliki", ""], ["Liagouris", "John", ""], ["Roscoe", "Timothy", ""]]}, {"id": "1812.01450", "submitter": "Massimo Cafaro", "authors": "Marco Pulimeno, Italo Epicoco, Massimo Cafaro", "title": "Distributed mining of time--faded heavy hitters", "comments": "arXiv admin note: text overlap with arXiv:1806.06580", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\textsc{P2PTFHH} (Peer--to--Peer Time--Faded Heavy Hitters) which,\nto the best of our knowledge, is the first distributed algorithm for mining\ntime--faded heavy hitters on unstructured P2P networks. \\textsc{P2PTFHH} is\nbased on the \\textsc{FDCMSS} (Forward Decay Count--Min Space-Saving) sequential\nalgorithm, and efficiently exploits an averaging gossip protocol, by merging in\neach interaction the involved peers' underlying data structures. We formally\nprove the convergence and correctness properties of our distributed algorithm\nand show that it is fast and simple to implement. Extensive experimental\nresults confirm that \\textsc{P2PTFHH} retains the extreme accuracy and error\nbound provided by \\textsc{FDCMSS} whilst showing excellent scalability. Our\ncontributions are three-fold: (i) we prove that the averaging gossip protocol\ncan be used jointly with our augmented sketch data structure for mining\ntime--faded heavy hitters; (ii) we prove the error bounds on frequency\nestimation; (iii) we experimentally prove that \\textsc{P2PTFHH} is extremely\naccurate and fast, allowing near real time processing of large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 20:53:16 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Pulimeno", "Marco", ""], ["Epicoco", "Italo", ""], ["Cafaro", "Massimo", ""]]}, {"id": "1812.01477", "submitter": "Mauricio Araya", "authors": "Mauricio Araya, Maximiliano Osorio, Mat\\'ias D\\'iaz, Carlos Ponce,\n  Mart\\'in Villanueva, Camilo Valenzuela, Mauricio Solar", "title": "JOVIAL: Notebook-based Astronomical Data Analysis in the Cloud", "comments": "8 pages, 10 figures, special issue of ADASS 2017", "journal-ref": "Astronomy and Computing, Volume 25 (2018)", "doi": "10.1016/j.ascom.2018.09.001", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing astronomical data analysis using only personal computers is\nbecoming impractical for the very large data sets produced nowadays. As\nanalysis is not a task that can be automatized to its full extent, the idea of\nmoving processing where the data is located means also moving the whole\nscientific process towards the archives and data centers. Using Jupyter\nNotebooks as a remote service is a recent trend in data analysis that aims to\ndeal with this problem, but harnessing the infrastructure to serve the\nastronomer without increasing the complexity of the service is a challenge. In\nthis paper we present the architecture and features of JOVIAL, a Cloud service\nwhere astronomers can safely use Jupyter notebooks over a personal space\ndesigned for high-performance processing under the high-availability principle.\nWe show that features existing only in specific packages can be adapted to run\nin the notebooks, and that algorithms can be adapted to run across the data\ncenter without necessarily redesigning them.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 15:13:40 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Araya", "Mauricio", ""], ["Osorio", "Maximiliano", ""], ["D\u00edaz", "Mat\u00edas", ""], ["Ponce", "Carlos", ""], ["Villanueva", "Mart\u00edn", ""], ["Valenzuela", "Camilo", ""], ["Solar", "Mauricio", ""]]}, {"id": "1812.01551", "submitter": "Evgeny Postnikov", "authors": "E. B. Postnikov (1), I. V. Bychkov (2 and 3), J. Y. Dubenskaya (1), O.\n  L. Fedorov (4), Y. A. Kazarina (4), E. E. Korosteleva (1), A. P. Kryukov (1),\n  A. A. Mikhailov (2), M. D. Nguyen (1), S. P. Polyakov (1), A. O. Shigarov (2\n  and 3), D. A. Shipilov (4), D.P. Zhurov (4) ((1) Lomonosov Moscow State\n  University Skobeltsyn Institute of Nuclear Physics (MSU SINP), Moscow,\n  Russia, (2) Matrosov Institute for System Dynamics and Control Theory,\n  Siberian Branch of Russian Academy of Sciences, Irkutsk, Russia, (3) Irkutsk\n  State University (ISU), Irkutsk, Russia, (4) Applied Physics Institute of\n  Irkutsk State University (API ISU), Irkutsk, Russia)", "title": "Particle identification in ground-based gamma-ray astronomy using\n  convolutional neural networks", "comments": "5 pages, 2 figures. Submitted to CEUR Workshop Proceedings, 8th\n  International Conference \"Distributed Computing and Grid-technologies in\n  Science and Education\" GRID 2018, 10 - 14 September 2018, Dubna, Russia", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern detectors of cosmic gamma-rays are a special type of imaging\ntelescopes (air Cherenkov telescopes) supplied with cameras with a relatively\nlarge number of photomultiplier-based pixels. For example, the camera of the\nTAIGA-IACT telescope has 560 pixels of hexagonal structure. Images in such\ncameras can be analysed by deep learning techniques to extract numerous\nphysical and geometrical parameters and/or for incoming particle\nidentification. The most powerful deep learning technique for image analysis,\nthe so-called convolutional neural network (CNN), was implemented in this\nstudy. Two open source libraries for machine learning, PyTorch and TensorFlow,\nwere tested as possible software platforms for particle identification in\nimaging air Cherenkov telescopes. Monte Carlo simulation was performed to\nanalyse images of gamma-rays and background particles (protons) as well as\nestimate identification accuracy. Further steps of implementation and\nimprovement of this technique are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 17:43:21 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Postnikov", "E. B.", "", "2 and 3"], ["Bychkov", "I. V.", "", "2 and 3"], ["Dubenskaya", "J. Y.", "", "2\n  and 3"], ["Fedorov", "O. L.", "", "2\n  and 3"], ["Kazarina", "Y. A.", "", "2\n  and 3"], ["Korosteleva", "E. E.", "", "2\n  and 3"], ["Kryukov", "A. P.", "", "2\n  and 3"], ["Mikhailov", "A. A.", "", "2\n  and 3"], ["Nguyen", "M. D.", "", "2\n  and 3"], ["Polyakov", "S. P.", "", "2\n  and 3"], ["Shigarov", "A. O.", "", "2\n  and 3"], ["Shipilov", "D. A.", ""], ["Zhurov", "D. P.", ""]]}, {"id": "1812.01665", "submitter": "Niranjan Hasabnis", "authors": "Niranjan Hasabnis", "title": "Auto-tuning TensorFlow Threading Model for CPU Backend", "comments": "Paper presented at Machine Learning in HPC Environments workshop held\n  along with SuperComputing 2018, Dallas, Texas", "journal-ref": null, "doi": "10.1109/MLHPC.2018.000-7", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow is a popular deep learning framework used by data scientists to\nsolve a wide-range of machine learning and deep learning problems such as image\nclassification and speech recognition. It also operates at a large scale and in\nheterogeneous environments --- it allows users to train neural network models\nor deploy them for inference using GPUs, CPUs and deep learning specific\ncustom-designed hardware such as TPUs. Even though TensorFlow supports a\nvariety of optimized backends, realizing the best performance using a backend\nmay require additional efforts. For instance, getting the best performance from\na CPU backend requires careful tuning of its threading model. Unfortunately,\nthe best tuning approach used today is manual, tedious, time-consuming, and,\nmore importantly, may not guarantee the best performance.\n  In this paper, we develop an automatic approach, called TensorTuner, to\nsearch for optimal parameter settings of TensorFlow's threading model for CPU\nbackends. We evaluate TensorTuner on both Eigen and Intel's MKL CPU backends\nusing a set of neural networks from TensorFlow's benchmarking suite. Our\nevaluation results demonstrate that the parameter settings found by TensorTuner\nproduce 2% to 123% performance improvement for the Eigen CPU backend and 1.5%\nto 28% performance improvement for the MKL CPU backend over the performance\nobtained using their best-known parameter settings. This highlights the fact\nthat the default parameter settings in Eigen CPU backend are not the ideal\nsettings; and even for a carefully hand-tuned MKL backend, the settings may be\nsub-optimal. Our evaluations also revealed that TensorTuner is efficient at\nfinding the optimal settings --- it is able to converge to the optimal settings\nquickly by pruning more than 90% of the parameter search space.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 20:20:29 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Hasabnis", "Niranjan", ""]]}, {"id": "1812.01762", "submitter": "Zachariah Carmichael", "authors": "Zachariah Carmichael, Hamed F. Langroudi, Char Khazanov, Jeffrey\n  Lillie, John L. Gustafson, Dhireesha Kudithipudi", "title": "Deep Positron: A Deep Neural Network Using the Posit Number System", "comments": "6 pages, Design, Automation and Test in Europe 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent surge of interest in Deep Neural Networks (DNNs) has led to\nincreasingly complex networks that tax computational and memory resources. Many\nDNNs presently use 16-bit or 32-bit floating point operations. Significant\nperformance and power gains can be obtained when DNN accelerators support\nlow-precision numerical formats. Despite considerable research, there is still\na knowledge gap on how low-precision operations can be realized for both DNN\ntraining and inference. In this work, we propose a DNN architecture, Deep\nPositron, with posit numerical format operating successfully at $\\leq$8 bits\nfor inference. We propose a precision-adaptable FPGA soft core for exact\nmultiply-and-accumulate for uniform comparison across three numerical formats,\nfixed, floating-point and posit. Preliminary results demonstrate that 8-bit\nposit has better accuracy than 8-bit fixed or floating-point for three\ndifferent low-dimensional datasets. Moreover, the accuracy is comparable to\n32-bit floating-point on a Xilinx Virtex-7 FPGA device. The trade-offs between\nDNN performance and hardware resources, i.e. latency, power, and resource\nutilization, show that posit outperforms in accuracy and latency at 8-bit and\nbelow.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 00:36:53 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2019 00:03:16 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Carmichael", "Zachariah", ""], ["Langroudi", "Hamed F.", ""], ["Khazanov", "Char", ""], ["Lillie", "Jeffrey", ""], ["Gustafson", "John L.", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1812.01776", "submitter": "Daniel Crankshaw", "authors": "Daniel Crankshaw, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E.\n  Gonzalez, Ion Stoica, Alexey Tumanov", "title": "InferLine: ML Prediction Pipeline Provisioning and Management for Tight\n  Latency Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serving ML prediction pipelines spanning multiple models and hardware\naccelerators is a key challenge in production machine learning. Optimally\nconfiguring these pipelines to meet tight end-to-end latency goals is\ncomplicated by the interaction between model batch size, the choice of hardware\naccelerator, and variation in the query arrival process.\n  In this paper we introduce InferLine, a system which provisions and manages\nthe individual stages of prediction pipelines to meet end-to-end tail latency\nconstraints while minimizing cost. InferLine consists of a low-frequency\ncombinatorial planner and a high-frequency auto-scaling tuner. The\nlow-frequency planner leverages stage-wise profiling, discrete event\nsimulation, and constrained combinatorial search to automatically select\nhardware type, replication, and batching parameters for each stage in the\npipeline. The high-frequency tuner uses network calculus to auto-scale each\nstage to meet tail latency goals in response to changes in the query arrival\nprocess. We demonstrate that InferLine outperforms existing approaches by up to\n7.6x in cost while achieving up to 34.5x lower latency SLO miss rate on\nrealistic workloads and generalizes across state-of-the-art model serving\nframeworks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 01:50:51 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 16:09:19 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Crankshaw", "Daniel", ""], ["Sela", "Gur-Eyal", ""], ["Zumar", "Corey", ""], ["Mo", "Xiangxi", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""], ["Tumanov", "Alexey", ""]]}, {"id": "1812.01823", "submitter": "Guangyan Hu", "authors": "Guangyan Hu, Desheng Zhang, Sandro Rigo, Thu D. Nguyen", "title": "Approximation with Error Bounds in Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a sampling framework to support approximate computing with\nestimated error bounds in Spark. Our framework allows sampling to be performed\nat the beginning of a sequence of multiple transformations ending in an\naggregation operation. The framework constructs a data provenance tree as the\ncomputation proceeds, then combines the tree with multi-stage sampling and\npopulation estimation theories to compute error bounds for the aggregation.\nWhen information about output keys are available early, the framework can also\nuse adaptive stratified reservoir sampling to avoid (or reduce) key losses in\nthe final output and to achieve more consistent error bounds across popular and\nrare keys. Finally, the framework includes an algorithm to dynamically choose\nsampling rates to meet user specified constraints on the CDF of error bounds in\nthe outputs. We have implemented a prototype of our framework called\nApproxSpark, and used it to implement five approximate applications from\ndifferent domains. Evaluation results show that ApproxSpark can (a)\nsignificantly reduce execution time if users can tolerate small amounts of\nuncertainties and, in many cases, loss of rare keys, and (b) automatically find\nsampling rates to meet user specified constraints on error bounds. We also\nexplore and discuss extensively trade-offs between sampling rates, execution\ntime, accuracy and key loss.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 05:40:28 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 03:51:18 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 15:01:52 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Hu", "Guangyan", ""], ["Zhang", "Desheng", ""], ["Rigo", "Sandro", ""], ["Nguyen", "Thu D.", ""]]}, {"id": "1812.01837", "submitter": "Ignacio Cano", "authors": "Ignacio Cano, Lequn Chen, Pedro Fonseca, Tianqi Chen, Chern Cheah,\n  Karan Gupta, Ramesh Chandra, Arvind Krishnamurthy", "title": "ADARES: Adaptive Resource Management for Virtual Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual execution environments allow for consolidation of multiple\napplications onto the same physical server, thereby enabling more efficient use\nof server resources. However, users often statically configure the resources of\nvirtual machines through guesswork, resulting in either insufficient resource\nallocations that hinder VM performance, or excessive allocations that waste\nprecious data center resources. In this paper, we first characterize real-world\nresource allocation and utilization of VMs through the analysis of an extensive\ndataset, consisting of more than 250k VMs from over 3.6k private enterprise\nclusters. Our large-scale analysis confirms that VMs are often misconfigured,\neither overprovisioned or underprovisioned, and that this problem is pervasive\nacross a wide range of private clusters. We then propose ADARES, an adaptive\nsystem that dynamically adjusts VM resources using machine learning techniques.\nIn particular, ADARES leverages the contextual bandits framework to effectively\nmanage the adaptations. Our system exploits easily collectible data, at the\ncluster, node, and VM levels, to make more sensible allocation decisions, and\nuses transfer learning to safely explore the configurations space and speed up\ntraining. Our empirical evaluation shows that ADARES can significantly improve\nsystem utilization without sacrificing performance. For instance, when compared\nto threshold and prediction-based baselines, it achieves more predictable\nVM-level performance and also reduces the amount of virtual CPUs and memory\nprovisioned by up to 35% and 60% respectively for synthetic workloads on real\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 07:20:06 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 03:05:47 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Cano", "Ignacio", ""], ["Chen", "Lequn", ""], ["Fonseca", "Pedro", ""], ["Chen", "Tianqi", ""], ["Cheah", "Chern", ""], ["Gupta", "Karan", ""], ["Chandra", "Ramesh", ""], ["Krishnamurthy", "Arvind", ""]]}, {"id": "1812.01906", "submitter": "Minh Duc Nguyen", "authors": "Minh-Duc Nguyen (1), Alexander Kryukov (1), Julia Dubenskaya (1),\n  Elena Korosteleva (1), Stanislav Polyakov (1), Evgeny Postnikov (1), Igor\n  Bychkov (2), Andrey Mikhailov (2), Alexey Shigarov (2), Oleg Fedorov (3),\n  Yulia Kazarina (3), Dmitry Shipilov (3), Dmitry Zhurov (3) ((1) Lomonosov\n  Moscow State University, Skobeltsyn Institute of Nuclear Physics, (2)\n  Matrosov Institute for System Dynamics and Control Theory, Siberian Branch of\n  Russian Academy of Sciences, (3) Applied Physics Institute, Irkutsk State\n  University)", "title": "A distributed data warehouse system for astroparticle physics", "comments": "5 pages, 3 figures, The 8th International Conference \"Distributed\n  Computing and Grid-technologies in Science and Education\" (GRID 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A distributed data warehouse system is one of the actual issues in the field\nof astroparticle physics. Famous experiments, such as TAIGA, KASCADE-Grande,\nproduce tens of terabytes of data measured by their instruments. It is critical\nto have a smart data warehouse system on-site to store the collected data for\nfurther distribution effectively. It is also vital to provide scientists with a\nhandy and user-friendly interface to access the collected data with proper\npermissions not only on-site but also online. The latter case is handy when\nscientists need to combine data from different experiments for analysis. In\nthis work, we describe an approach to implementing a distributed data warehouse\nsystem that allows scientists to acquire just the necessary data from different\nexperiments via the Internet on demand. The implementation is based on\nCernVM-FS with additional components developed by us to search through the\nwhole available data sets and deliver their subsets to users' computers.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 10:42:23 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Nguyen", "Minh-Duc", ""], ["Kryukov", "Alexander", ""], ["Dubenskaya", "Julia", ""], ["Korosteleva", "Elena", ""], ["Polyakov", "Stanislav", ""], ["Postnikov", "Evgeny", ""], ["Bychkov", "Igor", ""], ["Mikhailov", "Andrey", ""], ["Shigarov", "Alexey", ""], ["Fedorov", "Oleg", ""], ["Kazarina", "Yulia", ""], ["Shipilov", "Dmitry", ""], ["Zhurov", "Dmitry", ""]]}, {"id": "1812.01941", "submitter": "Swapneel Mehta", "authors": "Swapneel Mehta, Prasanth Kothuri, Daniel Lanza Garcia", "title": "Anomaly Detection for Network Connection Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage a streaming architecture based on ELK, Spark and Hadoop in order\nto collect, store, and analyse database connection logs in near real-time. The\nproposed system investigates outliers using unsupervised learning; widely\nadopted clustering and classification algorithms for log data, highlighting the\nsubtle variances in each model by visualisation of outliers. Arriving at a\nnovel solution to evaluate untagged, unfiltered connection logs, we propose an\napproach that can be extrapolated to a generalised system of analysing\nconnection logs across a large infrastructure comprising thousands of\nindividual nodes and generating hundreds of lines in logs per second.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 00:50:51 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Mehta", "Swapneel", ""], ["Kothuri", "Prasanth", ""], ["Garcia", "Daniel Lanza", ""]]}, {"id": "1812.01963", "submitter": "Stefan Nothaas", "authors": "Stefan Nothaas, Kevin Beineke and Michael Schoettner", "title": "Ibdxnet: Leveraging InfiniBand in Highly Concurrent Java Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we describe the design and implementation of Ibdxnet, a\nlow-latency and high-throughput transport providing the benefits of InfiniBand\nnetworks to Java applications. Ibdxnet is part of the Java-based DXNet library,\na highly concurrent and simple to use messaging stack with transparent\nserialization of messaging objects and focus on very small messages (< 64\nbytes). Ibdxnet implements the transport interface of DXNet in Java and a\ncustom C++ library in native space using JNI. Several optimizations in both\nspaces minimize context switching overhead between Java and C++ and are not\nburdening message latency or throughput. Communication is implemented using the\nmessaging verbs of the ibverbs library complemented by an automatic connection\nmanagement in the native library. We compared DXNet with the Ibdxnet transport\nto the MPI implementations FastMPJ and MVAPICH2. For small messages up to 64\nbytes using multiple threads, DXNet with the Ibdxnet transport achieves a\nbi-directional message rate of 10 million messages per second and surpasses\nFastMPJ by a factor of 4 and MVAPICH by a factor of 2. Furthermore, DXNet\nscales well on a high load all-to-all communication with up to 8 nodes\nachieving a total aggregated message rate of 43.4 million messages per second\nfor small messages and a throughput saturation of 33.6 GB/s with only 2 kb\nmessage size.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 12:46:44 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Nothaas", "Stefan", ""], ["Beineke", "Kevin", ""], ["Schoettner", "Michael", ""]]}, {"id": "1812.02407", "submitter": "Siddharth Pramod", "authors": "Siddharth Pramod", "title": "Elastic Gossip: Distributing Neural Network Training Using Gossip-like\n  Protocols", "comments": "M.S. Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributing Neural Network training is of particular interest for several\nreasons including scaling using computing clusters, training at data sources\nsuch as IOT devices and edge servers, utilizing underutilized resources across\nheterogeneous environments, and so on. Most contemporary approaches primarily\naddress scaling using computing clusters and require high network bandwidth and\nfrequent communication. This thesis presents an overview of standard approaches\nto distribute training and proposes a novel technique involving\npairwise-communication using Gossip-like protocols, called Elastic Gossip. This\napproach builds upon an existing technique known as Elastic Averaging SGD\n(EASGD), and is similar to another technique called Gossiping SGD which also\nuses Gossip-like protocols. Elastic Gossip is empirically evaluated against\nGossiping SGD using the MNIST digit recognition and CIFAR-10 classification\ntasks, using commonly used Neural Network architectures spanning Multi-Layer\nPerceptrons (MLPs) and Convolutional Neural Networks (CNNs). It is found that\nElastic Gossip, Gossiping SGD, and All-reduce SGD perform quite comparably,\neven though the latter entails a substantially higher communication cost. While\nElastic Gossip performs better than Gossiping SGD in these experiments, it is\npossible that a more thorough search over hyper-parameter space, specific to a\ngiven application, may yield configurations of Gossiping SGD that work better\nthan Elastic Gossip.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 09:00:34 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Pramod", "Siddharth", ""]]}, {"id": "1812.02565", "submitter": "Longfei Wang", "authors": "Xinhang Zhang, Haoyuan Hu, Longfei Wang, Zhijun Sun, Ying Zhang,\n  Kunpeng Han, Yinghui Xu", "title": "A Novel Bin Design Problem and High Performance Algorithm for E-commerce\n  Logistics System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing cost accounts for a large part of the e-commerce logistics cost.\nMining the patterns of customer orders and designing suitable packing bins help\nto reduce operating cost. In the classical bin packing problem, a given set of\ncuboid-shaped items should be packed into bins with given and fixed-sizes\n(length, width and height) to minimize the number of bins that are used.\nHowever, a novel bin design problem is proposed in this paper. The decision\nvariables are the geometric sizes of bins, and the objective is to minimize the\ntotal surface area. To solve the problem, a low computational-complexity,\nhigh-performance heuristic algorithm based on dynamic programming and\ndepth-first tree search, named DPTS, is developed. Based on real historical\ndata that are collected from logistics scenario, numerical experiments show\nthat the DPTS out-performed 5.8% than the greedy local search (GLS) algorithm\nin the total cost. What's more, DPTS algorithm requires only about 1/50 times\nof the computational resources compared to the GLS algorithm. This demonstrates\nthat DPTS algorithm is very efficient in bin design problem and can help\nlogistics companies to make appropriate design.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 05:26:23 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Zhang", "Xinhang", ""], ["Hu", "Haoyuan", ""], ["Wang", "Longfei", ""], ["Sun", "Zhijun", ""], ["Zhang", "Ying", ""], ["Han", "Kunpeng", ""], ["Xu", "Yinghui", ""]]}, {"id": "1812.02639", "submitter": "Andrea Lattuada", "authors": "Frank McSherry and Andrea Lattuada and Malte Schwarzkopf and Timothy\n  Roscoe", "title": "Shared Arrangements: practical inter-query sharing for streaming\n  dataflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current systems for data-parallel, incremental processing and view\nmaintenance over high-rate streams isolate the execution of independent\nqueries. This creates unwanted redundancy and overhead in the presence of\nconcurrent incrementally maintained queries: each query must independently\nmaintain the same indexed state over the same input streams, and new queries\nmust build this state from scratch before they can begin to emit their first\nresults. This paper introduces shared arrangements: indexed views of maintained\nstate that allow concurrent queries to reuse the same in-memory state without\ncompromising data-parallel performance and scaling. We implement shared\narrangements in a modern stream processor and show order-of-magnitude\nimprovements in query response time and resource consumption for interactive\nqueries against high-throughput streams, while also significantly improving\nperformance in other domains including business analytics, graph processing,\nand program analysis.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:17:37 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 17:47:09 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 20:20:08 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["McSherry", "Frank", ""], ["Lattuada", "Andrea", ""], ["Schwarzkopf", "Malte", ""], ["Roscoe", "Timothy", ""]]}, {"id": "1812.02641", "submitter": "Matthew Reyes", "authors": "Matthew G. Reyes", "title": "Local Conditioning: Exact Message Passing for Cyclic Undirected\n  Distributed Networks", "comments": "This work was presented at the Future Technologies Conference (FTC),\n  Vancouver, Canada, November 2018", "journal-ref": "Proceedings of the Future Technologies Conference (FTC) 2018, Eds.\n  Kohei Arai, Rahul Bhatia, and Supriya Kapoor, Vol. 2, Springer", "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper addresses practical implementation of summing out, expanding, and\nreordering of messages in Local Conditioning (LC) for undirected networks. In\nparticular, incoming messages conditioned on potentially different subsets of\nthe receiving node's relevant set must be expanded to be conditioned on this\nrelevant set, then reordered so that corresponding columns of the conditioned\nmatrices can be fused through element-wise multiplication. An outgoing message\nis then reduced by summing out loop cutset nodes that are upstream of the\noutgoing edge. The emphasis on implementation is the primary contribution over\nthe theoretical justification of LC given in Fay et al. Nevertheless, the\ncomplexity of Local Conditioning in grid networks is still no better than that\nof Clustering.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:19:53 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Reyes", "Matthew G.", ""]]}, {"id": "1812.02944", "submitter": "Luanzheng Guo", "authors": "Luanzheng Guo, Dong Li, Ignacio Laguna", "title": "PARIS: Predicting Application Resilience Using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme-scale scientific applications can be more vulnerable to soft errors\n(transient faults) as high-performance computing systems increase in scale. The\ncommon practice to evaluate the resilience to faults of an application is\nrandom fault injection, a method that can be highly time consuming. While\nresilience prediction modeling has been recently proposed to predict\napplication resilience in a faster way than fault injection, it can only\npredict a single class of fault manifestation (SDC) and there is no evidence\ndemonstrating that it can work on previously unseen programs, which greatly\nlimits its re-usability. We present PARIS, a resilience prediction method that\naddresses the problems of existing prediction methods using machine learning.\nUsing carefully-selected features and a machine learning model, our method is\nable to make resilience predictions of three classes of fault manifestations\n(success, SDC, and interruption) as opposed to one class like in current\nresilience prediction modeling. The generality of our approach allows us to\nmake prediction on new applications, i.e., previously unseen applications,\nproviding large applicability to our model. Our evaluation on 125 programs\nshows that PARIS provides high prediction accuracy, 82% and 77% on average for\npredicting the rate of success and interruption, respectively, while the\nstate-of-the-art resilience prediction model cannot predict them. When\npredicting the rate of SDC, PARIS provides much better accuracy than the\nstate-of-the-art (38% vs. -273%). PARIS is much faster (up to 450x speedup)\nthan the traditional method (random fault injection).\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 08:42:18 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Guo", "Luanzheng", ""], ["Li", "Dong", ""], ["Laguna", "Ignacio", ""]]}, {"id": "1812.03237", "submitter": "Md Mehedi Hassan Onik", "authors": "Md Mehedi Hassan Onik, Mahdi H. Miraz, Chul-Soo Kim", "title": "A Recruitment and Human Resource Management Technique Using Blockchain\n  Technology for Industry 4.0", "comments": "Onik, M. M. H., Miraz, M. H., & Kim, C. S. (2018, April). A\n  recruitment and human resource management technique using Blockchain\n  technology for Industry 4.0. In Proceedings of the Smart Cities Symposium\n  (SCS-2018), Manama, Bahrain (pp. 11-16). IET", "journal-ref": null, "doi": "10.1049/cp.2018.1371", "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of Information Technology (IT) in the domain of Human Resource\nManagement (HRM) systems is a sine qua non for any organization for\nsuccessfully adopting and implementing Fourth Industrial Revolution (Industry\n4.0). However, these systems are required to ensure non-biased, efficient,\ntransparent and secure environment. Blockchain, a technology based on\ndistributed digital ledgers, can help facilitate the process of successfully\neffectuating these specifications. A detailed literature review has been\nconducted to identify the current status of usage of Information Technology in\nthe domain of Human Resource Management and how Blockchain can help achieve a\nsmart, cost-effective, efficient, transparent and secure factory management\nsystem. A Blockchain based Recruitment Management System (BcRMS) as well as\nBlockchain based Human Resource Management System (BcHRMS) algorithm have been\nproposed. From the analysis of the results obtained through the case study, it\nis evident that the proposed system holds definite advantages compared to the\nexisting recruitment systems. Future research directions have also been\nidentified and advocated.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 23:09:06 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Onik", "Md Mehedi Hassan", ""], ["Miraz", "Mahdi H.", ""], ["Kim", "Chul-Soo", ""]]}, {"id": "1812.03288", "submitter": "Praneeth Vepakomma", "authors": "Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta,\n  Abhimanyu Dubey", "title": "No Peek: A Survey of private distributed deep learning", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey distributed deep learning models for training or inference without\naccessing raw data from clients. These methods aim to protect confidential\npatterns in data while still allowing servers to train models. The distributed\ndeep learning methods of federated learning, split learning and large batch\nstochastic gradient descent are compared in addition to private and secure\napproaches of differential privacy, homomorphic encryption, oblivious transfer\nand garbled circuits in the context of neural networks. We study their\nbenefits, limitations and trade-offs with regards to computational resources,\ndata leakage and communication efficiency and also share our anticipated future\ntrends.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 08:54:37 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Swedish", "Tristan", ""], ["Raskar", "Ramesh", ""], ["Gupta", "Otkrist", ""], ["Dubey", "Abhimanyu", ""]]}, {"id": "1812.03296", "submitter": "Yuri G. Gordienko", "authors": "Oleksandr Kovalchuk, Yuri Gordienko, Sergii Stirenko", "title": "Impact of MQTT Based Sensor Network Architecture on Delivery Delay Time", "comments": "10 pages, 5 figures, 3 tables", "journal-ref": "In Proc. 2019 IEEE 39th International Conference on Electronics\n  and Nanotechnology (ELNANO), 838 - 842 (2019)", "doi": "10.1109/ELNANO.2019.8783323", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to present two new architectures of the\ncomplexes of embedded systems in the context of the Internet of Things. The\nproposed new schemes use the MQTT protocol, but provide both an autonomous\noperation and an online mode. This is achieved by adding one additional layer,\nwhich consists of its own MQTT-broker and critical messages handler, so that in\ncase of the Internet access disappearance, the system could continue to respond\nto changes. The research of message delivery delays has been carried out for\nthe proposed models. The comparative analysis was conducted to identify the\nadvantages and disadvantages of both solutions.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 10:06:09 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Kovalchuk", "Oleksandr", ""], ["Gordienko", "Yuri", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1812.03651", "submitter": "Joseph Hellerstein", "authors": "Joseph M. Hellerstein, Jose Faleiro, Joseph E. Gonzalez, Johann\n  Schleier-Smith, Vikram Sreekanti, Alexey Tumanov and Chenggang Wu", "title": "Serverless Computing: One Step Forward, Two Steps Back", "comments": "8 pages, draft for CIDR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing offers the potential to program the cloud in an\nautoscaling, pay-as-you go manner. In this paper we address critical gaps in\nfirst-generation serverless computing, which place its autoscaling potential at\nodds with dominant trends in modern computing: notably data-centric and\ndistributed computing, but also open source and custom hardware. Put together,\nthese gaps make current serverless offerings a bad fit for cloud innovation and\nparticularly bad for data systems innovation. In addition to pinpointing some\nof the main shortfalls of current serverless architectures, we raise a set of\nchallenges we believe must be met to unlock the radical potential that the\ncloud---with its exabytes of storage and millions of cores---should offer to\ninnovative developers.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:10:57 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hellerstein", "Joseph M.", ""], ["Faleiro", "Jose", ""], ["Gonzalez", "Joseph E.", ""], ["Schleier-Smith", "Johann", ""], ["Sreekanti", "Vikram", ""], ["Tumanov", "Alexey", ""], ["Wu", "Chenggang", ""]]}, {"id": "1812.03770", "submitter": "Pierre Vandenhove", "authors": "Pierre Vandenhove", "title": "Functional Design of Computation Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the control flow of a computer program as a computation graph\ncan bring many benefits in a broad variety of domains where performance is\ncritical. This technique is a core component of most major numerical libraries\n(TensorFlow, PyTorch, Theano, MXNet,...) and is successfully used to speed up\nand optimise many computationally-intensive tasks. However, different design\nchoices in each of these libraries lead to noticeable differences in efficiency\nand in the way an end user writes efficient code. In this report, we detail the\nimplementation and features of the computation graph support in OCaml's\nnumerical library Owl, a recent entry in the world of scientific computing.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 12:52:03 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Vandenhove", "Pierre", ""]]}, {"id": "1812.03825", "submitter": "Zijian Zhang", "authors": "Avishek Anand, Megha Khosla, Jaspreet Singh, Jan-Hendrik Zab and\n  Zijian Zhang", "title": "Asynchronous Training of Word Embeddings for Large Text Corpora", "comments": "This paper contains 9 pages and has been accepted in the WSDM2019", "journal-ref": null, "doi": "10.1145/3289600.3291011", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Word embeddings are a powerful approach for analyzing language and have been\nwidely popular in numerous tasks in information retrieval and text mining.\nTraining embeddings over huge corpora is computationally expensive because the\ninput is typically sequentially processed and parameters are synchronously\nupdated. Distributed architectures for asynchronous training that have been\nproposed either focus on scaling vocabulary sizes and dimensionality or suffer\nfrom expensive synchronization latencies.\n  In this paper, we propose a scalable approach to train word embeddings by\npartitioning the input space instead in order to scale to massive text corpora\nwhile not sacrificing the performance of the embeddings. Our training procedure\ndoes not involve any parameter synchronization except a final sub-model merge\nphase that typically executes in a few minutes. Our distributed training scales\nseamlessly to large corpus sizes and we get comparable and sometimes even up to\n45% performance improvement in a variety of NLP benchmarks using models trained\nby our distributed procedure which requires $1/10$ of the time taken by the\nbaseline approach. Finally we also show that we are robust to missing words in\nsub-models and are able to effectively reconstruct word representations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 11:34:33 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Anand", "Avishek", ""], ["Khosla", "Megha", ""], ["Singh", "Jaspreet", ""], ["Zab", "Jan-Hendrik", ""], ["Zhang", "Zijian", ""]]}, {"id": "1812.03871", "submitter": "Franck Iutzeler", "authors": "Dmitry Grishchenko, Franck Iutzeler, J\\'er\\^ome Malick, Massih-Reza\n  Amini", "title": "Distributed Learning with Sparse Communications by Identification", "comments": "v2 is a significant improvement over v1 (titled \"Asynchronous\n  Distributed Learning with Sparse Communications and Identification\") with new\n  algorithms, results, and discussions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed optimization for large-scale learning, a major performance\nlimitation comes from the communications between the different entities. When\ncomputations are performed by workers on local data while a coordinator machine\ncoordinates their updates to minimize a global loss, we present an asynchronous\noptimization algorithm that efficiently reduces the communications between the\ncoordinator and workers. This reduction comes from a random sparsification of\nthe local updates. We show that this algorithm converges linearly in the\nstrongly convex case and also identifies optimal strongly sparse solutions. We\nfurther exploit this identification to propose an automatic dimension\nreduction, aptly sparsifying all exchanges between coordinator and workers.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 15:25:20 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 15:31:03 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Grishchenko", "Dmitry", ""], ["Iutzeler", "Franck", ""], ["Malick", "J\u00e9r\u00f4me", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1812.03917", "submitter": "Anik Islam", "authors": "Anik Islam, Md. Fazlul Kader, Soo Young Shin", "title": "BSSSQS: A Blockchain Based Smart and Secured Scheme for Question Sharing\n  in the Smart Education System", "comments": "14 pages, 7 figures, Preprint submitted to Journal of Parallel and\n  Distributed Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing education systems are facing a threat of question paper leaking\n(QPL) in the exam which jeopardizes the quality of education. Therefore, it is\nhigh time to think about a more secure and flexible question sharing system\nwhich can prevent QPL issue in the future education system. Blockchain enables\na way of creating and storing transactions, contracts or anything that requires\nprotection against tampering, accessing etc. This paper presents a new scheme\nfor smart education, by utilizing the concept of blockchain, for question\nsharing. A two-phase encryption technique for encrypting question paper (QSP)\nis proposed. In the first phase, QSPs are encrypted using timestamp and in the\nsecond phase, previous encrypted QSPs are encrypted again using a timestamp,\nsalt hash and hashes from previous QSPs. These encrypted QSPs are stored in the\nblockchain along with a smart contract which helps the user to unlock the\nselected QSP. An algorithm is also proposed for selecting a QSP for the exam\nwhich picks a QSP randomly. Moreover, a timestamp based lock is imposed on the\nscheme so that no one can decrypt the QSP before the allowed time. Finally,\nsecurity is analyzed by proving different propositions and the superiority of\nthe proposed scheme over existing schemes is proven through a comparative study\nbased on the different features.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 00:28:30 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Islam", "Anik", ""], ["Kader", "Md. Fazlul", ""], ["Shin", "Soo Young", ""]]}, {"id": "1812.04048", "submitter": "Xin Zhang", "authors": "Xin Zhang, Jia Liu, Zhengyuan Zhu, Elizabeth S. Bentley", "title": "Compressed Distributed Gradient Descent: Communication-Efficient\n  Consensus over Networks", "comments": "11 pages, 11 figures, IEEE INFOCOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network consensus optimization has received increasing attention in recent\nyears and has found important applications in many scientific and engineering\nfields. To solve network consensus optimization problems, one of the most\nwell-known approaches is the distributed gradient descent method (DGD).\nHowever, in networks with slow communication rates, DGD's performance is\nunsatisfactory for solving high-dimensional network consensus problems due to\nthe communication bottleneck. This motivates us to design a\ncommunication-efficient DGD-type algorithm based on compressed information\nexchanges. Our contributions in this paper are three-fold: i) We develop a\ncommunication-efficient algorithm called amplified-differential compression DGD\n(ADC-DGD) and show that it converges under {\\em any} unbiased compression\noperator; ii) We rigorously prove the convergence performances of ADC-DGD and\nshow that they match with those of DGD without compression; iii) We reveal an\ninteresting phase transition phenomenon in the convergence speed of ADC-DGD.\nCollectively, our findings advance the state-of-the-art of network consensus\noptimization theory.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 19:37:26 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 01:48:51 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 22:11:08 GMT"}, {"version": "v4", "created": "Sun, 21 Jul 2019 19:48:29 GMT"}, {"version": "v5", "created": "Mon, 9 Sep 2019 00:47:44 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zhang", "Xin", ""], ["Liu", "Jia", ""], ["Zhu", "Zhengyuan", ""], ["Bentley", "Elizabeth S.", ""]]}, {"id": "1812.04070", "submitter": "Hang Liu", "authors": "Hang Liu, H. Howie Huang", "title": "SIMD-X: Programming and Processing of Graph Algorithms on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With high computation power and memory bandwidth, graphics processing units\n(GPUs) lend themselves to accelerate data-intensive analytics, especially when\nsuch applications fit the single instruction multiple data (SIMD) model.\nHowever, graph algorithms such as breadth-first search and k-core, often fail\nto take full advantage of GPUs, due to irregularity in memory access and\ncontrol flow. To address this challenge, we have developed SIMD-X, for\nprogramming and processing of single instruction multiple, complex, data on\nGPUs. Specifically, the new Active-Compute-Combine (ACC) model not only\nprovides ease of programming to programmers, but more importantly creates\nopportunities for system-level optimizations. To this end, SIMD-X utilizes\njust-in-time task management which filters out inactive vertices at runtime and\nintelligently maps various tasks to different amount of GPU cores in pursuit of\nworkload balancing. In addition, SIMD-X leverages push-pull based kernel fusion\nthat, with the help of a new deadlock-free global barrier, reduces a large\nnumber of computation kernels to very few. Using SIMD-X, a user can program a\ngraph algorithm in tens of lines of code, while achieving 3?, 6?, 24?, 3?\nspeedup over Gunrock, Galois, CuSha, and Ligra, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:29:58 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Liu", "Hang", ""], ["Huang", "H. Howie", ""]]}, {"id": "1812.04197", "submitter": "Haruna Isah", "authors": "Haruna Isah, Farhana Zulkernine", "title": "A Scalable and Robust Framework for Data Stream Ingestion", "comments": "6 pages", "journal-ref": null, "doi": "10.1109/BigData.2018.8622360", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An essential part of building a data-driven organization is the ability to\nhandle and process continuous streams of data to discover actionable insights.\nThe explosive growth of interconnected devices and the social Web has led to a\nlarge volume of data being generated on a continuous basis. Streaming data\nsources such as stock quotes, credit card transactions, trending news, traffic\nconditions, time-sensitive patients data are not only very common but can\nrapidly depreciate if not processed quickly. The ever-increasing volume and\nhighly irregular nature of data rates pose new challenges to data stream\nprocessing systems. One such challenging but important task is how to\naccurately ingest and integrate data streams from various sources and locations\ninto an analytics platform. These challenges demand new strategies and systems\nthat can offer the desired degree of scalability and robustness in handling\nfailures. This paper investigates the fundamental requirements and the state of\nthe art of existing data stream ingestion systems, propose a scalable and\nfault-tolerant data stream ingestion and integration framework that can serve\nas a reusable component across many feeds of structured and unstructured input\ndata in a given platform, and demonstrate the utility of the framework in a\nreal-world data stream processing case study that integrates Apache NiFi and\nKafka for processing high velocity news articles from across the globe. The\nstudy also identifies best practices and gaps for future research in developing\nlarge-scale data stream processing infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 02:55:22 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""]]}, {"id": "1812.04362", "submitter": "Martin Grambow", "authors": "Martin Grambow, Jonathan Hasenburg, Tobias Pfandzelter, David Bermbach", "title": "Dockerization Impacts in Database Performance Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": "MCC.2018.1", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Docker seems to be an attractive solution for cloud database benchmarking as\nit simplifies the setup process through pre-built images that are portable and\nsimple to maintain. However, the usage of Docker for benchmarking is only valid\nif there is no effect on measurement results. Existing work has so far only\nfocused on the performance overheads that Docker directly induces for specific\napplications. In this paper, we have studied indirect effects of dockerization\non the results of database benchmarking. Among others, our results clearly show\nthat containerization has a measurable and non-constant influence on\nmeasurement results and should, hence, only be used after careful analysis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:42:08 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Grambow", "Martin", ""], ["Hasenburg", "Jonathan", ""], ["Pfandzelter", "Tobias", ""], ["Bermbach", "David", ""]]}, {"id": "1812.04380", "submitter": "Xiaole Wen", "authors": "Xiaole Wen, Shuai Zhang, Haihang You", "title": "DRONE: a Distributed Subgraph-Centric Framework for Processing Large\n  Scale Power-law Graphs", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, in the big data era, social networks, graph databases, knowledge\ngraphs, electronic commerce etc. demand efficient and scalable capability to\nprocess an ever increasing volume of graph-structured data. To meet the\nchallenge, two mainstream distributed programming models, vertex-centric (VC)\nand subgraph-centric (SC) were proposed. Compared to the VC model, the SC model\nconverges faster with less communication overhead on well-partitioned graphs,\nand is easy to program due to the \"think like a graph\" philosophy. The edge-cut\nmethod is considered as a natural choice of subgraph-centric model for graph\npartitioning, and has been adopted by Giraph++, Blogel and GRAPE. However, the\nedge-cut method causes significant performance bottleneck for processing large\nscale power-law graphs. Thus, the SC model is less competitive in practice. In\nthis paper, we present an innovative distributed graph computing framework,\nDRONE (Distributed gRaph cOmputiNg Engine). It combines the subgraph-centric\nmodel and the vertex-cut graph partitioning strategy. Experiments show that\nDRONE outperforms the state-of-art distributed graph computing engines on\nreal-world graphs and synthetic power-law graphs. DRONE is capable of scaling\nup to process one-trillion-edge synthetic power-law graphs, which is orders of\nmagnitude larger than previously reported by existing SC-based frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 13:17:55 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 13:22:47 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Wen", "Xiaole", ""], ["Zhang", "Shuai", ""], ["You", "Haihang", ""]]}, {"id": "1812.04431", "submitter": "Apostolos Rikos", "authors": "Apostolos I. Rikos", "title": "Distributed Weight Balancing in Directed Topologies", "comments": "doctoral thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This doctoral thesis concerns novel distributed algorithms for weight\nbalancing over directed (communication) topologies. A directed topology\n(digraph) with nonnegative (or positive) weights assigned on each edge is\nweight-balanced if, for each node, the sum of the weights of in-coming edges\nequals the sum of the weights of out-going edges. The novel algorithms\nintroduced in this thesis can facilitate the development of strategies for\ngenerating weight balanced digraphs, in a distributed manner, and find numerous\napplications in coordination and control of multi-component systems. In the\nfirst part of this thesis, we introduce a novel distributed algorithm that\noperates over a static topology and solves the weight balancing problem when\nthe weights are restricted to be nonnegative integers. In the second part of\nthe thesis, we present a novel distributed algorithm which solves the integer\nweight balancing problem in the presence of arbitrary (time-varying and\ninhomogeneous) delays that might affect the transmission at a particular link\nat a particular time. In the third part of this thesis, we present a novel\ndistributed algorithm for obtaining admissible and balanced integer weights for\nthe case when there are lower and upper weight constraints on the communication\nlinks. In the fourth part of this thesis we present a novel distributed\nalgorithm which solves the integer weight balancing problem under lower and\nupper weight constraints over the communication links for the case where\narbitrary (time-varying and inhomogeneous) time delays and possible packet\ndrops affect the transmission at a particular link at a particular time.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 00:06:54 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Rikos", "Apostolos I.", ""]]}, {"id": "1812.04492", "submitter": "Eylon Yogev", "authors": "Merav Parter and Eylon Yogev", "title": "Low Congestion Cycle Covers and their Applications", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.01139", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cycle cover of a bridgeless graph $G$ is a collection of simple cycles in\n$G$ such that each edge $e$ appears on at least one cycle. The common objective\nin cycle cover computation is to minimize the total lengths of all cycles.\nMotivated by applications to distributed computation, we introduce the notion\nof \\emph{low-congestion} cycle covers, in which all cycles in the cycle\ncollection are both \\emph{short} and nearly \\emph{edge-disjoint}. Formally, a\n$(d,c)$-cycle cover of a graph $G$ is a collection of cycles in $G$ in which\neach cycle is of length at most $d$ and each edge participates in at least one\ncycle and at most $c$ cycles. A-priori, it is not clear that cycle covers that\nenjoy both a small overlap and a short cycle length even exist, nor if it is\npossible to efficiently find them. Perhaps quite surprisingly, we prove the\nfollowing: Every bridgeless graph of diameter $D$ admits a $(d,c)$-cycle cover\nwhere $d = \\tilde{O}(D)$ and $c=\\tilde{O}(1)$. These parameters are\nexistentially tight up to polylogarithmic terms. Furthermore, we show how to\nextend our result to achieve universally optimal cycle covers. Let $C_e$ is the\nlength of the shortest cycle that covers $e$, and let $OPT(G)= \\max_{e \\in G}\nC_e$. We show that every bridgeless graph admits a $(d,c)$-cycle cover where $d\n= \\tilde{O}(OPT(G))$ and $c=\\tilde{O}(1)$. We demonstrate the usefulness of low\ncongestion cycle covers in different settings of resilient computation. For\ninstance, we consider a Byzantine fault model where in each round, the\nadversary chooses a single message and corrupt in an arbitrarily manner. We\nprovide a compiler that turns any $r$-round distributed algorithm for a graph\n$G$ with diameter $D$, into an equivalent fault tolerant algorithm with $r\\cdot\npoly(D)$ rounds.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 13:04:45 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 07:02:31 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Parter", "Merav", ""], ["Yogev", "Eylon", ""]]}, {"id": "1812.04575", "submitter": "Lixing Chen", "authors": "Lixing Chen and Jie Xu", "title": "Task Offloading and Replication for Vehicular Cloud Computing: A\n  Multi-Armed Bandit Approach", "comments": "To appear in INFOCOM2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicular Cloud Computing (VCC) is a new technological shift which exploits\nthe computation and storage resources on vehicles for computational service\nprovisioning. Spare on-board resources are pooled by a VCC operator, e.g. a\nroadside unit, to complete task requests using the vehicle-as-a-resource\nframework. In this paper, we investigate timely service provisioning for\ndeadline-constrained tasks in VCC systems by leveraging the task replication\ntechnique (i.e., allowing one task to be executed by several server vehicles).\nA learning-based algorithm, called DATE-V (Deadline-Aware Task rEplication for\nVehicular Cloud), is proposed to address the special issues in VCC systems\nincluding uncertainty of vehicle movements, volatile vehicle members, and large\nvehicle population. The proposed algorithm is developed based on a novel\nContextual-Combinatorial Multi-Armed Bandit (CC-MAB) learning framework. DATE-V\nis `contextual' because it utilizes side information (context) of vehicles and\ntasks to infer the completion probability of a task replication under random\nvehicle movements. DATE-V is `combinatorial' because it aims to replicate the\nreceived task and send the task replications to multiple server vehicles to\nguarantee the service timeliness. We rigorously prove that our learning\nalgorithm achieves a sublinear regret bound compared to an oracle algorithm\nthat knows the exact completion probability of any task replications.\nSimulations are carried out based on real-world vehicle movement traces and the\nresults show that DATE-V significantly outperforms benchmark solutions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 17:57:16 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Chen", "Lixing", ""], ["Xu", "Jie", ""]]}, {"id": "1812.04971", "submitter": "Yijie Mei", "authors": "Yijie Mei, Yanyan Shen, Yanmin Zhu, Linpeng Huang", "title": "STEP : A Distributed Multi-threading Framework Towards Efficient Data\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Various general-purpose distributed systems have been proposed to cope with\nhigh-diversity applications in the pipeline of Big Data analytics. Most of them\nprovide simple yet effective primitives to simplify distributed programming.\nWhile the rigid primitives offer great ease of use to savvy programmers, they\nprobably compromise efficiency in performance and flexibility in data\nrepresentation and programming specifications, which are critical properties in\nreal systems. In this paper, we discuss the limitations of coarse-grained\nprimitives and aim to provide an alternative for users to have flexible control\nover distributed programs and operate globally shared data more efficiently. We\ndevelop STEP, a novel distributed framework based on in-memory key-value store.\nThe key idea of STEP is to adapt multi-threading in a single machine to a\ndistributed environment. STEP enables users to take fine-grained control over\ndistributed threads and apply task-specific optimizations in a flexible manner.\nThe underlying key-value store serves as distributed shared memory to keep\nglobally shared data. To ensure ease-of-use, STEP offers plentiful effective\ninterfaces in terms of distributed shared data manipulation, cluster\nmanagement, distributed thread management and synchronization. We conduct\nextensive experimental studies to evaluate the performance of STEP using real\ndata sets. The results show that STEP outperforms the state-of-the-art\ngeneral-purpose distributed systems as well as a specialized ML platform in\nmany real applications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 14:32:42 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Mei", "Yijie", ""], ["Shen", "Yanyan", ""], ["Zhu", "Yanmin", ""], ["Huang", "Linpeng", ""]]}, {"id": "1812.04974", "submitter": "Elena Pastorelli", "authors": "Francesco Simula, Elena Pastorelli, Pier Stanislao Paolucci, Michele\n  Martinelli, Alessandro Lonardo, Andrea Biagioni, Cristiano Capone, Fabrizio\n  Capuani, Paolo Cretaro, Giulia De Bonis, Francesca Lo Cicero, Luca Pontisso,\n  Piero Vicini, Roberto Ammendola", "title": "Real-time cortical simulations: energy and interconnect scaling on\n  distributed systems", "comments": "8 pages, 8 figures, 4 tables, submitted after final publication on\n  PDP2019 proceedings, corrected final DOI. arXiv admin note: text overlap with\n  arXiv:1812.04974, arXiv:1804.03441", "journal-ref": "27th Euromicro International Conference on Parallel, Distributed\n  and Network-based Processing (PDP), Pavia, Italy, February 13-15, 2019, pp.\n  283-290", "doi": "10.1109/EMPDP.2019.8671627", "report-no": null, "categories": "cs.DC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We profile the impact of computation and inter-processor communication on the\nenergy consumption and on the scaling of cortical simulations approaching the\nreal-time regime on distributed computing platforms. Also, the speed and energy\nconsumption of processor architectures typical of standard HPC and embedded\nplatforms are compared. We demonstrate the importance of the design of\nlow-latency interconnect for speed and energy consumption. The cost of cortical\nsimulations is quantified using the Joule per synaptic event metric on both\narchitectures. Reaching efficient real-time on large scale cortical simulations\nis of increasing relevance for both future bio-inspired artificial intelligence\napplications and for understanding the cognitive functions of the brain, a\nscientific quest that will require to embed large scale simulations into highly\ncomplex virtual or real worlds. This work stands at the crossroads between the\nWaveScalES experiment in the Human Brain Project (HBP), which includes the\nobjective of large scale thalamo-cortical simulations of brain states and their\ntransitions, and the ExaNeSt and EuroExa projects, that investigate the design\nof an ARM-based, low-power High Performance Computing (HPC) architecture with a\ndedicated interconnect scalable to million of cores; simulation of deep sleep\nSlow Wave Activity (SWA) and Asynchronous aWake (AW) regimes expressed by\nthalamo-cortical models are among their benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 14:42:10 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 13:16:02 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 15:33:39 GMT"}, {"version": "v4", "created": "Tue, 26 Nov 2019 11:18:26 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Simula", "Francesco", ""], ["Pastorelli", "Elena", ""], ["Paolucci", "Pier Stanislao", ""], ["Martinelli", "Michele", ""], ["Lonardo", "Alessandro", ""], ["Biagioni", "Andrea", ""], ["Capone", "Cristiano", ""], ["Capuani", "Fabrizio", ""], ["Cretaro", "Paolo", ""], ["De Bonis", "Giulia", ""], ["Cicero", "Francesca Lo", ""], ["Pontisso", "Luca", ""], ["Vicini", "Piero", ""], ["Ammendola", "Roberto", ""]]}, {"id": "1812.05032", "submitter": "Ke Liang", "authors": "Ke Liang", "title": "Fission: A Provably Fast, Scalable, and Secure Permissionless Blockchain", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Fission, a new permissionless blockchain that achieves scalability\nin both terms of system throughput and transaction confirmation time, while at\nthe same time, retaining blockchain's core values of equality and\ndecentralization. Fission overcomes the system throughput bottleneck by\nemploying a novel Eager-Lazy pipeling model that achieves very high system\nthroughputs via block pipelining, an adaptive partitioning mechanism that\nauto-scales to transaction volumes, and a provably secure energy-efficient\nconsensus protocol to ensure security and robustness. Fission applies a hybrid\nnetwork which consists of a relay network, and a peer-to-peer network. The goal\nof the relay network is to minimize the transaction confirmation time by\nminimizing the information propagation latency. To optimize the performance on\nthe relay network in the presence of churn, dynamic network topologies, and\nnetwork heterogeneity, we propose an ultra-fast game-theoretic relay selection\nalgorithm that achieves near-optimal performance in a fully distributed manner.\nFission's peer-to-peer network complements the relay network and provides a\nvery high data availability via enabling users to contribute their storage and\nbandwidth for information dissemination (with incentive). We propose a\ndistributed online data retrieval strategy that optimally offloads the relay\nnetwork without degrading the system performance. By re-innovating all the core\nelements of the blockchain technology - computation, networking, and storage -\nin a holistic manner, Fission aims to achieve the best balance among\nscalability, security and decentralization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 17:08:37 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 06:29:06 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Liang", "Ke", ""]]}, {"id": "1812.05257", "submitter": "Timur Bazhirov", "authors": "Mohammad Mohammadi and Timur Bazhirov", "title": "Continuous evaluation of the performance of cloud infrastructure for\n  scientific applications", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing recently developed into a viable alternative to on-premises\nsystems for executing high-performance computing (HPC) applications. With the\nemergence of new vendors and hardware options, there is now a growing need to\ncontinuously evaluate the performance of the infrastructure with respect to the\nmost commonly-used simulation workflows. We present an online ecosystem and the\ncorresponding tools aimed at providing a collaborative and repeatable way to\nassess the performance of the underlying hardware for multiple real-world\napplication-specific benchmark cases. The ecosystem allows for the benchmark\nresults to be stored and shared online in a centrally accessible database in\norder to facilitate their comparison, traceability, and curation. We include\nthe current up-to-date example results for multiple cloud vendors and explain\nhow to contribute new results and benchmark cases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 04:26:29 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Mohammadi", "Mohammad", ""], ["Bazhirov", "Timur", ""]]}, {"id": "1812.05352", "submitter": "Anisur Molla Rahaman", "authors": "Ajay D. Kshemkalyani and Anisur Rahaman Molla and Gokarna Sharma", "title": "Efficient Dispersion of Mobile Robots on Arbitrary Graphs and Grids", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mobile robot dispersion problem on graphs asks $k\\leq n$ robots placed\ninitially arbitrarily on the nodes of an $n$-node anonymous graph to reposition\nautonomously to reach a configuration in which each robot is on a distinct node\nof the graph. This problem is of significant interest due to its relationship\nto other fundamental robot coordination problems, such as exploration,\nscattering, load balancing, and relocation of self-driven electric cars\n(robots) to recharge stations (nodes). In this paper, we provide two novel\ndeterministic algorithms for dispersion, one for arbitrary graphs and another\nfor grid graphs, in a synchronous setting where all robots perform their\nactions in every time step. Our algorithm for arbitrary graphs has\n$O(\\min(m,k\\Delta) \\cdot \\log k)$ steps runtime using $O(\\log n)$ bits of\nmemory at each robot, where $m$ is the number of edges and $\\Delta$ is the\nmaximum degree of the graph. This is an exponential improvement over the\n$O(mk)$ steps best previously known algorithm. In particular, the runtime of\nour algorithm is optimal (up to a $O(\\log k)$ factor) in constant-degree\narbitrary graphs. Our algorithm for grid graphs has $O(\\min(k,\\sqrt{n}))$ steps\nruntime using $\\Theta(\\log k)$ bits at each robot. This is the first algorithm\nfor dispersion in grid graphs. Moreover, this algorithm is optimal for both\nmemory and time when $k=\\Omega(n)$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 10:23:19 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 12:48:48 GMT"}, {"version": "v3", "created": "Sat, 27 Apr 2019 12:42:17 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kshemkalyani", "Ajay D.", ""], ["Molla", "Anisur Rahaman", ""], ["Sharma", "Gokarna", ""]]}, {"id": "1812.05445", "submitter": "Amar  Prasad Misra", "authors": "A. Roy, A. P. Misra and S. Banerjee", "title": "Discrete model for cloud computing: Analysis of data security and data\n  loss", "comments": "12 pages, 3 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is recognized as one of the most promising solutions to\ninformation technology, e.g., for storing and sharing data in the web service\nwhich is sustained by a company or third party instead of storing data in a\nhard drive or other devices. It is essentially a physical storage system which\nprovides large storage of data and faster computing to users over the Internet.\nIn this cloud system, the third party allows to preserve data of clients or\nusers only for business purpose and also for a limited period of time. The\nusers are used to share data confidentially among themselves and to store data\nvirtually to save the cost of physical devices as well as the time. In this\npaper, we propose a discrete dynamical system for cloud computing and data\nmanagement of the storage service between a third party and users. A framework,\ncomprised of different techniques and procedures for distribution of storage\nand their implementation with users and the third party is given. For\nillustration purpose, the model is considered for two users and a third party,\nand its dynamical properties are briefly analyzed and discussed. It is shown\nthat the discrete system exhibits periodic, quasiperiodic and chaotic states.\nThe latter discerns that the cloud computing system with distribution of data\nand storage between users and the third party may be secured. Some issues of\ndata security are discussed and a random replication scheme is proposed to\nensure that the data loss can be highly reduced compared to the existing\nschemes in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 13:06:16 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Roy", "A.", ""], ["Misra", "A. P.", ""], ["Banerjee", "S.", ""]]}, {"id": "1812.05727", "submitter": "Roberto Palmieri", "authors": "Mohamed M. Saad and Masoomeh Javidi Kishi and Shihao Jing and Sandeep\n  Hans and Roberto Palmieri", "title": "Processing Transactions in a Predefined Order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a high performance solution to the problem of\ncommitting transactions while enforcing a predefined order. We provide the\ndesign and implementation of three algorithms, which deploy a specialized\ncooperative transaction execution model. This model permits the propagation of\nwritten values along the chain of ordered transactions. We show that, even in\nthe presence of data conflicts, the proposed algorithms are able to outperform\nsingle-threaded execution, and other baseline and specialized state-of-the-art\ncompetitors (e.g., STMLite). The maximum speedup achieved in micro benchmarks,\nSTAMP, PARSEC and SPEC200 applications is in the range of 4.3x -- 16.5x.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 23:17:37 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Saad", "Mohamed M.", ""], ["Kishi", "Masoomeh Javidi", ""], ["Jing", "Shihao", ""], ["Hans", "Sandeep", ""], ["Palmieri", "Roberto", ""]]}, {"id": "1812.05955", "submitter": "Thomas Rolinger", "authors": "Thomas B. Rolinger, Christopher D. Krieger", "title": "Impact of Traditional Sparse Optimizations on a Migratory Thread\n  Architecture", "comments": "8th Workshop on Irregular Applications: Architectures and Algorithms\n  (IA^3) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving high performance for sparse applications is challenging due to\nirregular access patterns and weak locality. These properties preclude many\nstatic optimizations and degrade cache performance on traditional systems. To\naddress these challenges, novel systems such as the Emu architecture have been\nproposed. The Emu design uses light-weight migratory threads, narrow memory,\nand near-memory processing capabilities to address weak locality and reduce the\ntotal load on the memory system. Because the Emu architecture is fundamentally\ndifferent than cache based hierarchical memory systems, it is crucial to\nunderstand the cost-benefit tradeoffs of standard sparse algorithm\noptimizations on Emu hardware. In this work, we explore sparse matrix-vector\nmultiplication (SpMV) on the Emu architecture. We investigate the effects of\ndifferent sparse optimizations such as dense vector data layouts, work\ndistributions, and matrix reorderings. Our study finds that initially\ndistributing work evenly across the system is inadequate to maintain load\nbalancing over time due to the migratory nature of Emu threads. In severe\ncases, matrix sparsity patterns produce hot-spots as many migratory threads\nconverge on a single resource. We demonstrate that known matrix reordering\ntechniques can improve SpMV performance on the Emu architecture by as much as\n70% by encouraging more consistent load balancing. This can be compared with a\nperformance gain of no more than 16% on a cache-memory based system.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:27:38 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Rolinger", "Thomas B.", ""], ["Krieger", "Christopher D.", ""]]}, {"id": "1812.05961", "submitter": "Thomas Rolinger", "authors": "Thomas B. Rolinger, Tyler A. Simon, Christopher D. Krieger", "title": "Parallel Sparse Tensor Decomposition in Chapel", "comments": "2018 IEEE International Parallel and Distributed Processing Symposium\n  Workshops (IPDPSW), 5th Annual Chapel Implementers and Users Workshop (CHIUW\n  2018)", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00143", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big-data analytics, using tensor decomposition to extract patterns from\nlarge, sparse multivariate data is a popular technique. Many challenges exist\nfor designing parallel, high performance tensor decomposition algorithms due to\nirregular data accesses and the growing size of tensors that are processed.\nThere have been many efforts at implementing shared-memory algorithms for\ntensor decomposition, most of which have focused on the traditional C/C++ with\nOpenMP framework. However, Chapel is becoming an increasingly popular\nprograming language due to its expressiveness and simplicity for writing\nscalable parallel programs. In this work, we port a state of the art C/OpenMP\nparallel sparse tensor decomposition tool, SPLATT, to Chapel. We present a\nperformance study that investigates bottlenecks in our Chapel code and\ndiscusses approaches for improving its performance. Also, we discuss features\nin Chapel that would have been beneficial to our porting effort. We demonstrate\nthat our Chapel code is competitive with the C/OpenMP code for both runtime and\nscalability, achieving 83%-96% performance of the original code and near linear\nscalability up to 32 cores.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:39:26 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Rolinger", "Thomas B.", ""], ["Simon", "Tyler A.", ""], ["Krieger", "Christopher D.", ""]]}, {"id": "1812.05964", "submitter": "Thomas Rolinger", "authors": "Thomas B. Rolinger, Tyler A. Simon, Christopher D. Krieger", "title": "An Empirical Evaluation of Allgatherv on Multi-GPU Systems", "comments": "2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid\n  Computing (CCGRID)", "journal-ref": null, "doi": "10.1109/CCGRID.2018.00027", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications for deep learning and big data analytics have compute and memory\nrequirements that exceed the limits of a single GPU. However, effectively\nscaling out an application to multiple GPUs is challenging due to the\ncomplexities of communication between the GPUs, particularly for collective\ncommunication with irregular message sizes. In this work, we provide a\nperformance evaluation of the Allgatherv routine on multi-GPU systems, focusing\non GPU network topology and the communication library used. We present results\nfrom the OSU-micro benchmark as well as conduct a case study for sparse tensor\nfactorization, one application that uses Allgatherv with highly irregular\nmessage sizes. We extend our existing tensor factorization tool to run on\nsystems with different node counts and varying number of GPUs per node. We then\nevaluate the communication performance of our tool when using traditional MPI,\nCUDA-aware MVAPICH and NCCL across a suite of real-world data sets on three\ndifferent systems: a 16-node cluster with one GPU per node, NVIDIA's DGX-1 with\n8 GPUs and Cray's CS-Storm with 16 GPUs. Our results show that irregularity in\nthe tensor data sets produce trends that contradict those in the OSU\nmicro-benchmark, as well as trends that are absent from the benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:46:25 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Rolinger", "Thomas B.", ""], ["Simon", "Tyler A.", ""], ["Krieger", "Christopher D.", ""]]}, {"id": "1812.05974", "submitter": "Andrea Testa", "authors": "Andrea Testa, Ivano Notarnicola, Giuseppe Notarstefano", "title": "Distributed Submodular Minimization over Networks: a Greedy Column\n  Generation Approach", "comments": "12 pages, 4 figures, 57th IEEE Conference on Decision and Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular optimization is a special class of combinatorial optimization\narising in several machine learning problems, but also in cooperative control\nof complex systems. In this paper, we consider agents in an asynchronous,\nunreliable and time-varying directed network that aim at cooperatively solving\nsubmodular minimization problems in a fully distributed way. The challenge is\nthat the (submodular) objective set-function is only partially known by agents,\nthat is, each one is able to evaluate the function only for subsets including\nitself. We propose a distributed algorithm based on a proper linear programming\nreformulation of the combinatorial problem. Our algorithm builds on a column\ngeneration approach in which each agent maintains a local candidate basis and\nlocally generates columns with a suitable greedy inner routine. A key\ninteresting feature of the proposed algorithm is that the pricing problem,\nwhich involves an exponential number of constraints, is solved by the agents\nthrough a polynomial time greedy algorithm. We prove that the proposed\ndistributed algorithm converges in finite time to an optimal solution of the\nsubmodular minimization problem and we corroborate the theoretical results by\nperforming numerical computations on instances of the $s$--$t$ minimum graph\ncut problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 15:11:23 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Testa", "Andrea", ""], ["Notarnicola", "Ivano", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1812.06011", "submitter": "Sergio Rajsbaum", "authors": "Sergio Rajsbaum and Michel Raynal", "title": "Mastering Concurrent Computing Through Sequential Thinking: A\n  Half-century Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrency, the art of doing many things at the same time is slowly becoming\na science. It is very difficult to master, yet it arises all over modern\ncomputing systems, both when the communication medium is shared memory and when\nit is by message passing. Concurrent programming is hard because it requires to\ncope with many possible, unpredictable behaviors of communicating processes\ninteracting with each other. Right from the start in the 1960s, the main way of\ndealing with concurrency has been by reduction to sequential reasoning. We\ntrace this history, and illustrate it through several examples, from early\nideas based on mutual exclusion, passing through consensus and concurrent\nobjects, until today ledgers and blockchains. We conclude with a discussion on\nthe limits that this approach encounters, related to fault-tolerance,\nperformance, and inherently concurrent problems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 16:30:00 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Rajsbaum", "Sergio", ""], ["Raynal", "Michel", ""]]}, {"id": "1812.06156", "submitter": "Alvaro Garcia Recuero", "authors": "Alvaro Garcia-Recuero, Aneta Morawin, Gareth Tyson", "title": "Trollslayer: Crowdsourcing and Characterization of Abusive Birds in\n  Twitter", "comments": "SNAMS 2018", "journal-ref": null, "doi": "10.1109/SNAMS.2018.8554898", "report-no": null, "categories": "cs.CY cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of today, abuse is a pressing issue to participants and administrators of\nOnline Social Networks (OSN). Abuse in Twitter can spawn from arguments\ngenerated for influencing outcomes of a political election, the use of bots to\nautomatically spread misinformation, and generally speaking, activities that\ndeny, disrupt, degrade or deceive other participants and, or the network. Given\nthe difficulty in finding and accessing a large enough sample of abuse ground\ntruth from the Twitter platform, we built and deployed a custom crawler that we\nuse to judiciously collect a new dataset from the Twitter platform with the aim\nof characterizing the nature of abusive users, a.k.a abusive birds, in the\nwild. We provide a comprehensive set of features based on users' attributes, as\nwell as social-graph metadata. The former includes metadata about the account\nitself, while the latter is computed from the social graph among the sender and\nthe receiver of each message. Attribute-based features are useful to\ncharacterize user's accounts in OSN, while graph-based features can reveal the\ndynamics of information dissemination across the network. In particular, we\nderive the Jaccard index as a key feature to reveal the benign or malicious\nnature of directed messages in Twitter. To the best of our knowledge, we are\nthe first to propose such a similarity metric to characterize abuse in Twitter.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 20:38:25 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Garcia-Recuero", "Alvaro", ""], ["Morawin", "Aneta", ""], ["Tyson", "Gareth", ""]]}, {"id": "1812.06160", "submitter": "Joshua Booth", "authors": "Joshua Dennis Booth, Gregory Bolet", "title": "Javelin: A Scalable Implementation for Sparse Incomplete LU\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new scalable incomplete LU factorization framework\ncalled Javelin to be used as a preconditioner for solving sparse linear systems\nwith iterative methods. Javelin allows for improved parallel factorization on\nshared-memory many-core systems by packaging the coefficient matrix into a\nformat that allows for high performance sparse matrix-vector multiplication and\nsparse triangular solves with minimal overheads. The framework achieves these\ngoals by using a collection of traditional permutations, point-to-point thread\nsynchronizations, tasking, and segmented prefix scans in a conventional\ncompressed sparse row format. Moreover, this framework stresses the importance\nof co-designing dependent tasks, such as sparse factorization and triangular\nsolves, on highly-threaded architectures. Using these changes, traditional\nfill-in and drop tolerance methods can be used, while still being able to have\nobserved speedups of up to ~42x on 68 Intel Knights Landing cores and ~12x on\n14 Intel Haswell cores.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 15:20:41 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 13:52:26 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 22:34:10 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Booth", "Joshua Dennis", ""], ["Bolet", "Gregory", ""]]}, {"id": "1812.06205", "submitter": "Yizheng Liao", "authors": "Yizheng Liao and Ram Rajagopal", "title": "Sequential Multiple Structural Damage Detection and Localization: A\n  Distributed Approach", "comments": "38 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As essential components of the modern urban system, the health conditions of\ncivil structures are the foundation of urban system sustainability and need to\nbe continuously monitored. In Structural Health Monitoring (SHM), many existing\nworks will have limited performance in the sequential damage diagnosis process\nbecause 1) the damage events needs to be reported with short delay, 2) multiple\ndamage locations have to be identified simultaneously, and 3) the computational\ncomplexity is intractable in large-scale wireless sensor networks (WSNs). To\naddress these drawbacks, we propose a new damage identification approach that\nutilizes the time-series of damage sensitive features extracted from multiple\nsensors' measurements and the optimal change point detection theory to find\ndamage occurrence time and identify the number of damage locations. As the\nexisting change point detection methods require to centralize the sensor data,\nwhich is impracticable in many applications, we use the probabilistic graphical\nmodel to formulate WSNs and the targeting structure and propose a distributed\nalgorithm for structural damage identification. Validation results show highly\naccurate damage identification in a shake table experiment and American Society\nof Civil Engineers benchmark structure. Also, we demonstrate that the detection\ndelay is reduced significantly by utilizing multiple sensors' data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 00:13:09 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Liao", "Yizheng", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1812.06255", "submitter": "Ranesh Kumar Naha", "authors": "Nasrin Akhter, Mohamed Othman and Ranesh Kumar Naha", "title": "Evaluation of Energy-efficient VM Consolidation for Cloud Based Data\n  Center - Revisited", "comments": "15 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a re-evaluation undertaken for dynamic VM consolidation\nproblem and optimal online deterministic algorithms for the single VM migration\nin an experimental environment. We proceeded to focus on energy and performance\ntrade-off by planet lab workload traces, which consists of a thousand Planetlab\nVMs with widespread simulation environments. All experiments are done in a\nsimulated cloud environment by the CloudSim simulation tool. A new paradigm of\nutility-oriented IT services is cloud computing, which offers a pay-as-you-go\nmodel. In recent years, there has been increasing interest among many users\nfrom business, scientific, engineering and educational territories in cloud\ncomputing. There is increasing concern that high energy consumption issues are\na disadvantage for various institutions. However, so far too little attention\nhas been given to the various methods to reduce energy consumption in cloud\nenvironments while ensuring performance. Besides the evaluation of\nenergy-efficient data center management algorithms in the cloud, we proposed a\nfurther research directed toward the development of energy efficient\nalgorithms. By the experimental evaluation of the current proposal for the\ncompetitive analysis of dynamic VM consolidation and optimal online\ndeterministic algorithms for the single VM migration, we found different\nresults for different algorithm combinations. Cloud-based data centers` consume\nmassive energy, which has a negative effect on the environment and operational\ncost, this work contributes to the energy consumption reduction in the cloud\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 08:43:36 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Akhter", "Nasrin", ""], ["Othman", "Mohamed", ""], ["Naha", "Ranesh Kumar", ""]]}, {"id": "1812.06415", "submitter": "Yaochen Hu", "authors": "Yaochen Hu, Di Niu, Jianming Yang and Shengping Zhou", "title": "Stochastic Distributed Optimization for Machine Learning from\n  Decentralized Features", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning has been widely studied in the literature to\nscale up machine learning model training in the presence of an ever-increasing\namount of data. We study distributed machine learning from another perspective,\nwhere the information about the training same samples are inherently\ndecentralized and located on different parities. We propose an asynchronous\nstochastic gradient descent (SGD) algorithm for such a feature distributed\nmachine learning (FDML) problem, to jointly learn from decentralized features,\nwith theoretical convergence guarantees under bounded asynchrony. Our algorithm\ndoes not require sharing the original feature data or even local model\nparameters between parties, thus preserving a high level of data\nconfidentiality. We implement our algorithm for FDML in a parameter server\narchitecture. We compare our system with fully centralized training (which\nviolates data locality requirements) and training only based on local features,\nthrough extensive experiments performed on a large amount of data from a\nreal-world application, involving 5 million samples and $8700$ features in\ntotal. Experimental results have demonstrated the effectiveness and efficiency\nof the proposed FDML system.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 08:00:36 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 23:15:07 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Hu", "Yaochen", ""], ["Niu", "Di", ""], ["Yang", "Jianming", ""], ["Zhou", "Shengping", ""]]}, {"id": "1812.06426", "submitter": "Guangli Li", "authors": "Guangli Li, Lei Liu, Xueying Wang, Xiao Dong, Peng Zhao, Xiaobing Feng", "title": "Auto-tuning Neural Network Quantization Framework for Collaborative\n  Inference Between the Cloud and Edge", "comments": "Published at ICANN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks (DNNs) have been widely applied in mobile\nintelligent applications. The inference for the DNNs is usually performed in\nthe cloud. However, it leads to a large overhead of transmitting data via\nwireless network. In this paper, we demonstrate the advantages of the\ncloud-edge collaborative inference with quantization. By analyzing the\ncharacteristics of layers in DNNs, an auto-tuning neural network quantization\nframework for collaborative inference is proposed. We study the effectiveness\nof mixed-precision collaborative inference of state-of-the-art DNNs by using\nImageNet dataset. The experimental results show that our framework can generate\nreasonable network partitions and reduce the storage on mobile devices with\ntrivial loss of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 09:05:44 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Li", "Guangli", ""], ["Liu", "Lei", ""], ["Wang", "Xueying", ""], ["Dong", "Xiao", ""], ["Zhao", "Peng", ""], ["Feng", "Xiaobing", ""]]}, {"id": "1812.06492", "submitter": "Val\\'erie Hayot-Sasson", "authors": "Val\\'erie Hayot-Sasson, Shawn T Brown and Tristan Glatard", "title": "Performance Evaluation of Big Data Processing Strategies for\n  Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuroimaging datasets are rapidly growing in size as a result of advancements\nin image acquisition methods, open-science and data sharing. However, the\nadoption of Big Data processing strategies by neuroimaging processing engines\nremains limited. Here, we evaluate three Big Data processing strategies\n(in-memory computing, data locality and lazy evaluation) on typical\nneuroimaging use cases, represented by the BigBrain dataset. We contrast these\nvarious strategies using Apache Spark and Nipype as our representative Big Data\nand neuroimaging processing engines, on Dell EMC's Top-500 cluster. Big Data\nthresholds were modelled by comparing the data-write rate of the application to\nthe filesystem bandwidth and number of concurrent processes. This model\nacknowledges the fact that page caching provided by the Linux kernel is\ncritical to the performance of Big Data applications. Results show that\nin-memory computing alone speeds-up executions by a factor of up to 1.6,\nwhereas when combined with data locality, this factor reaches 5.3. Lazy\nevaluation strategies were found to increase the likelihood of cache hits,\nfurther improving processing time. Such important speed-up values are likely to\nbe observed on typical image processing operations performed on images of size\nlarger than 75GB. A ballpark speculation from our model showed that in-memory\ncomputing alone will not speed-up current functional MRI analyses unless\ncoupled with data locality and processing around 280 subjects concurrently.\nFurthermore, we observe that emulating in-memory computing using in-memory file\nsystems (tmpfs) does not reach the performance of an in-memory engine,\npresumably due to swapping to disk and the lack of data cleanup. We conclude\nthat Big Data processing strategies are worth developing for neuroimaging\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 15:55:08 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 20:30:03 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hayot-Sasson", "Val\u00e9rie", ""], ["Brown", "Shawn T", ""], ["Glatard", "Tristan", ""]]}, {"id": "1812.06545", "submitter": "Roohollah Amiri", "authors": "Roohollah Amiri, Hani Mehrpouyan", "title": "Multi-Stream LDPC Decoder on GPU of Mobile Devices", "comments": "6 pages, 7 figures, 2019 IEEE 8th Annual Computing and Communication\n  Workshop and Conference (CCWC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-density parity check (LDPC) codes have been extensively applied in mobile\ncommunication systems due to their excellent error correcting capabilities.\nHowever, their broad adoption has been hindered by the high complexity of the\nLDPC decoder. Although to date, dedicated hardware has been used to implement\nlow latency LDPC decoders, recent advancements in the architecture of mobile\nprocessors have made it possible to develop software solutions. In this paper,\nwe propose a multi-stream LDPC decoder designed for a mobile device. The\nproposed decoder uses graphics processing unit (GPU) of a mobile device to\nachieve efficient real-time decoding. The proposed solution is implemented on\nan NVIDIA Tegra board as a system on a chip (SoC), where our results indicate\nthat we can control the load on the central processing units through the\nmulti-stream structure.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 22:22:03 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Amiri", "Roohollah", ""], ["Mehrpouyan", "Hani", ""]]}, {"id": "1812.07123", "submitter": "Mohammad Roohitavaf", "authors": "Mohammad Roohitavaf, Murat Demirbas, Sandeep Kulkarni", "title": "CausalSpartanX: Causal Consistency and Non-Blocking Read-Only\n  Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal consistency is an intermediate consistency model that can be achieved\ntogether with high availability and performance requirements even in presence\nof network partitions. In the context of partitioned data stores, it has been\nshown that implicit dependency tracking using timestamps is more efficient than\nexplicit dependency tracking. Existing time-based solutions depend on monotonic\npsychical clocks that are closely synchronized. These requirements make current\nprotocols vulnerable to clock anomalies. In this paper, we propose a new\ntime-based algorithm, CausalSpartanX, that instead of physical clocks, utilizes\nHybrid Logical Clocks (HLCs). We show that using HLCs, without any overhead, we\nmake the system robust on physical clock anomalies. This improvement is more\nsignificant in the context of query amplification, where a single query results\nin multiple GET/PUT operations. We also show that CausalSpartanX decreases the\nvisibility latency for a given data item compared with existing time-based\napproaches. In turn, this reduces the completion time of collaborative\napplications where two clients accessing two different replicas edit same items\nof the data store. CausalSpartanX also provides causally consistent distributed\nread-only transactions. CausalSpartanX read-only transactions are non-blocking\nand require only one round of communication between the client and the servers.\nAlso, the slowdowns of partitions that are unrelated to a transaction do not\naffect the performance of the transaction. Like previous protocols,\nCausalSpartanX assumes that a given client does not access more than one\nreplica. We show that in presence of network partitions, this assumption (made\nin several other works) is essential if one were to provide causal consistency\nas well as immediate availability to local updates.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 01:10:27 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Roohitavaf", "Mohammad", ""], ["Demirbas", "Murat", ""], ["Kulkarni", "Sandeep", ""]]}, {"id": "1812.07152", "submitter": "Bangtian Liu", "authors": "Bangtian Liu, Kazem Cheshmi, Saeed Soori, Michelle Mills Strout and\n  Maryam Mehri Dehnavi", "title": "MatRox: Modular approach for improving data locality in Hierarchical\n  (Mat)rix App(Rox)imation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical matrix approximations have gained significant traction in the\nmachine learning and scientific community as they exploit available low-rank\nstructures in kernel methods to compress the kernel matrix. The resulting\ncompressed matrix, HMatrix, is used to reduce the computational complexity of\noperations such as HMatrix-matrix multiplications with tuneable accuracy in an\nevaluation phase. Existing implementations of HMatrix evaluations do not\npreserve locality and often lead to unbalanced parallel execution with high\nsynchronization. Also, current solutions require the compression phase to\nre-execute if the kernel method or the required accuracy change. In this work,\nwe describe MatRox, a framework that uses novel structure analysis strategies,\nblocking and coarsen, with code specialization and a storage format to improve\nlocality and create load-balanced parallel tasks for HMatrix-matrix\nmultiplications. Modularization of the matrix compression phase enables the\nreuse of computations when there are changes to the input accuracy and the\nkernel function. The MatRox-generated code for matrix-matrix multiplication is\n2.98x, 1.60x, and 5.98x faster than library implementations available in GOFMM,\nSMASH, and STRUMPACK respectively. Additionally, the ability to reuse portions\nof the compression computation for changes to the accuracy leads to up to 2.64x\nimprovement with MatRox over five changes to accuracy using GOFMM.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 03:43:23 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 21:21:16 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 21:12:00 GMT"}, {"version": "v4", "created": "Thu, 1 Aug 2019 18:11:20 GMT"}, {"version": "v5", "created": "Wed, 20 Nov 2019 17:00:57 GMT"}, {"version": "v6", "created": "Thu, 21 Nov 2019 17:16:45 GMT"}, {"version": "v7", "created": "Sat, 30 Nov 2019 20:32:58 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Liu", "Bangtian", ""], ["Cheshmi", "Kazem", ""], ["Soori", "Saeed", ""], ["Strout", "Michelle Mills", ""], ["Dehnavi", "Maryam Mehri", ""]]}, {"id": "1812.07183", "submitter": "Junwei Zhang", "authors": "Junwei Zhang, Yang Liu, Li Shi, Thomas G. Robertazzi", "title": "Optimizing Data Intensive Flows for Networks on Chips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Data flow analysis and optimization is considered for homogeneous rectangular\nmesh networks. We propose a flow matrix equation which allows a closed-form\ncharacterization of the nature of the minimal time solution, speedup and a\nsimple method to determine when and how much load to distribute to processors.\nWe also propose a rigorous mathematical proof about the flow matrix optimal\nsolution existence and that the solution is unique. The methodology introduced\nhere is applicable to many interconnection networks and switching protocols (as\nan example we examine toroidal networks and hypercube networks in this paper).\nAn important application is improving chip area and chip scalability for\nnetworks on chips processing divisible style loads.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 06:04:39 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 03:41:21 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhang", "Junwei", ""], ["Liu", "Yang", ""], ["Shi", "Li", ""], ["Robertazzi", "Thomas G.", ""]]}, {"id": "1812.07210", "submitter": "Sebastian Caldas", "authors": "Sebastian Caldas, Jakub Kone\\v{c}n\\`y, H. Brendan McMahan and Ameet\n  Talwalkar", "title": "Expanding the Reach of Federated Learning by Reducing Client Resource\n  Requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication on heterogeneous edge networks is a fundamental bottleneck in\nFederated Learning (FL), restricting both model capacity and user\nparticipation. To address this issue, we introduce two novel strategies to\nreduce communication costs: (1) the use of lossy compression on the global\nmodel sent server-to-client; and (2) Federated Dropout, which allows users to\nefficiently train locally on smaller subsets of the global model and also\nprovides a reduction in both client-to-server communication and local\ncomputation. We empirically show that these strategies, combined with existing\ncompression approaches for client-to-server communication, collectively provide\nup to a $14\\times$ reduction in server-to-client communication, a $1.7\\times$\nreduction in local computation, and a $28\\times$ reduction in upload\ncommunication, all without degrading the quality of the final model. We thus\ncomprehensively reduce FL's impact on client device resources, allowing higher\ncapacity models to be trained, and a more diverse set of users to be reached.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:31:18 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 16:01:46 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Caldas", "Sebastian", ""], ["Kone\u010dny", "Jakub", ""], ["McMahan", "H. Brendan", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1812.07264", "submitter": "Niklas Carlsson", "authors": "Niklas Carlsson and Derek Eager", "title": "Worst-case Bounds and Optimized Cache on $M^{th}$ Request Cache\n  Insertion Policies under Elastic Conditions", "comments": "To appear in IFIP Performance, Dec. 2018, Toulouse, France. The final\n  version will appear in Performance Evaluation, volumes 127-128, Nov. 2018,\n  pp. 70-92", "journal-ref": null, "doi": "10.1016/j.peva.2018.09.006", "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud services and other shared third-party infrastructures allow individual\ncontent providers to easily scale their services based on current resource\ndemands. In this paper, we consider an individual content provider that wants\nto minimize its delivery costs under the assumptions that the storage and\nbandwidth resources it requires are elastic, the content provider only pays for\nthe resources that it consumes, and costs are proportional to the resource\nusage. Within this context, we (i) derive worst-case bounds for the optimal\ncost and competitive cost ratios of different classes of \"cache on $M^{th}$\nrequest\" cache insertion policies, (ii) derive explicit average cost\nexpressions and bounds under arbitrary inter-request distributions, (iii)\nderive explicit average cost expressions and bounds for short-tailed\n(deterministic, Erlang, and exponential) and heavy-tailed (Pareto)\ninter-request distributions, and (iv) present numeric and trace-based\nevaluations that reveal insights into the relative cost performance of the\npolicies. Our results show that a window-based \"cache on $2^{nd}$ request\"\npolicy using a single threshold optimized to minimize worst-case costs provides\ngood average performance across the different distributions and the full\nparameter ranges of each considered distribution, making it an attractive\nchoice for a wide range of practical conditions where request rates of\nindividual file objects typically are not known and can change quickly.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 09:48:47 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Carlsson", "Niklas", ""], ["Eager", "Derek", ""]]}, {"id": "1812.07277", "submitter": "Niklas Carlsson", "authors": "Mathias Almquist, Viktor Almquist, Vengatanathan Krishnamoorthi,\n  Niklas Carlsson, and Derek Eager", "title": "The Prefetch Aggressiveness Tradeoff in 360$^{\\circ}$ Video Streaming", "comments": "This paper is an extended version of our original ACM MMSys 2018\n  paper. Please cite our original paper (with the same title) published in ACM\n  Multimedia Systems (MMSys), Amsterdam, Netherlands, June 2018, pp. 258-269", "journal-ref": null, "doi": "10.1145/3204949.3204970", "report-no": null, "categories": "cs.MM cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With 360$^{\\circ}$ video, only a limited fraction of the full view is\ndisplayed at each point in time. This has prompted the design of streaming\ndelivery techniques that allow alternative playback qualities to be delivered\nfor each candidate viewing direction. However, while prefetching based on the\nuser's expected viewing direction is best done close to playback deadlines,\nlarge buffers are needed to protect against shortfalls in future available\nbandwidth. This results in conflicting goals and an important prefetch\naggressiveness tradeoff problem regarding how far ahead in time from the\ncurrent playpoint prefetching should be done. This paper presents the first\ncharacterization of this tradeoff. The main contributions include an empirical\ncharacterization of head movement behavior based on data from viewing sessions\nof four different categories of 360$^{\\circ}$ video, an optimization-based\ncomparison of the prefetch aggressiveness tradeoffs seen for these video\ncategories, and a data-driven discussion of further optimizations, which\ninclude a novel system design that allows both tradeoff objectives to be\ntargeted simultaneously. By qualitatively and quantitatively analyzing the\nabove tradeoffs, we provide insights into how to best design tomorrow's\ndelivery systems for 360$^{\\circ}$ videos, allowing content providers to reduce\nbandwidth costs and improve users' playback experiences.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 10:28:35 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Almquist", "Mathias", ""], ["Almquist", "Viktor", ""], ["Krishnamoorthi", "Vengatanathan", ""], ["Carlsson", "Niklas", ""], ["Eager", "Derek", ""]]}, {"id": "1812.07511", "submitter": "Gabor Elek", "authors": "G\\'abor Elek", "title": "Qualitative graph limit theory. Cantor Dynamical Systems and\n  Constant-Time Distributed Algorithms", "comments": "Typos are corrected, references and the converse of the statement of\n  Prop. 5.2 are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the paper is to lay the foundation for the qualitative analogue\nof the classical, quantitative sparse graph limit theory. In the first part of\nthe paper we introduce the qualitative analogues of the Benjamini-Schramm and\nlocal-global graph limit theories for sparse graphs. The natural limit objects\nare continuous actions of finitely generated groups on totally disconnected\ncompact metric spaces. We prove that the space of weak equivalent classes of\nfree Cantor actions is compact and contains a smallest element, as in the\nmeasurable case. We will introduce and study various notions of almost\nfiniteness, the qualitative analogue of hyperfiniteness, for classes of bounded\ndegree graphs. We prove the almost finiteness of a new class of \\'etale\ngroupoids associated to Cantor actions and construct an example of a\nnonamenable, almost finite totally disconnected \\'etale groupoid, answering a\nquery of Suzuki. Motivated by the notions and results on qualitative graph\nlimits, in the second part of our paper we give a precise definition of\nconstant-time distributed algorithms on sparse graphs. We construct such\nconstant-time algorithms for various approximation problems for hyperfinite and\nalmost finite graph classes. We also prove the Hausdorff convergence of the\nspectra of convergent graph sequences in the strongly almost finite category.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:29:57 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 17:10:42 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Elek", "G\u00e1bor", ""]]}, {"id": "1812.07636", "submitter": "Syed Eqbal Alam", "authors": "Syed Eqbal Alam, Robert Shorten, Fabian Wirth and Jia Yuan Yu", "title": "Distributed Algorithms for Internet-of-Things-enabled Prosumer Markets:\n  A Control Theoretic Perspective", "comments": "To appear as a chapter in \"Analytics for the Sharing Economy:\n  Mathematics, Engineering and Business Perspectives\", Editors: E. Crisostomi\n  et al., Springer, 2019 (forthcoming book)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.MA math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet-of-Things (IoT) enables the development of sharing economy\napplications. In many sharing economy scenarios, agents both produce as well as\nconsume a resource; we call them prosumers. A community of prosumers agrees to\nsell excess resource to another community in a prosumer market. In this\nchapter, we propose a control theoretic approach to regulate the number of\nprosumers in a prosumer community, where each prosumer has a cost function that\nis coupled through its time-averaged production and consumption of the\nresource. Furthermore, each prosumer runs its distributed algorithm and takes\nonly binary decisions in a probabilistic way, whether to produce one unit of\nthe resource or not and to consume one unit of the resource or not. In the\nproposed approach, prosumers do not explicitly exchange information with each\nother due to privacy reasons, but little exchange of information is required\nfor feedback signals, broadcast by a central agency. In the proposed approach,\nprosumers achieve the optimal values asymptotically. Furthermore, the proposed\napproach is suitable to implement in an IoT context with minimal demands on\ninfrastructure. We describe two use cases; community-based car sharing and\ncollaborative energy storage for prosumer markets. We also present simulation\nresults to check the efficacy of the algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 20:46:37 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 20:12:33 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Alam", "Syed Eqbal", ""], ["Shorten", "Robert", ""], ["Wirth", "Fabian", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1812.07723", "submitter": "Amirhossein Esmaili", "authors": "Amirhossein Esmaili, Mahdi Nazemi, Massoud Pedram", "title": "Modeling Processor Idle Times in MPSoC Platforms to Enable Integrated\n  DPM, DVFS, and Task Scheduling Subject to a Hard Deadline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency is one of the most critical design criteria for modern\nembedded systems such as multiprocessor system-on-chips (MPSoCs). Dynamic\nvoltage and frequency scaling (DVFS) and dynamic power management (DPM) are two\nmajor techniques for reducing energy consumption in such embedded systems.\nFurthermore, MPSoCs are becoming more popular for many real-time applications.\nOne of the challenges of integrating DPM with DVFS and task scheduling of\nreal-time applications on MPSoCs is the modeling of idle intervals on these\nplatforms. In this paper, we present a novel approach for modeling idle\nintervals in MPSoC platforms which leads to a mixed integer linear programming\n(MILP) formulation integrating DPM, DVFS, and task scheduling of periodic task\ngraphs subject to a hard deadline. We also present a heuristic approach for\nsolving the MILP and compare its results with those obtained from solving the\nMILP.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 01:18:05 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Esmaili", "Amirhossein", ""], ["Nazemi", "Mahdi", ""], ["Pedram", "Massoud", ""]]}, {"id": "1812.07751", "submitter": "Michael McCourt", "authors": "Alexandra Johnson and Michael McCourt", "title": "Orchestrate: Infrastructure for Enabling Parallelism during\n  Hyperparameter Optimization", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two key factors dominate the development of effective production grade\nmachine learning models. First, it requires a local software implementation and\niteration process. Second, it requires distributed infrastructure to\nefficiently conduct training and hyperparameter optimization. While modern\nmachine learning frameworks are very effective at the former, practitioners are\noften left building ad hoc frameworks for the latter. We present SigOpt\nOrchestrate, a library for such simultaneous training in a cloud environment.\nWe describe the motivating factors and resulting design of this library,\nfeedback from initial testing, and future goals.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 04:37:01 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Johnson", "Alexandra", ""], ["McCourt", "Michael", ""]]}, {"id": "1812.07771", "submitter": "Poorva Kulkarni", "authors": "Poorva Kulkarni, Varsha Deshpande, Latika Sarna, Sumedha Shenolikar\n  and Supriya Kelkar", "title": "Fault Diagnosis for Distributed Systems using Accuracy Technique", "comments": "13 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Systems involve two or more computer systems which may be\nsituated at geographically distinct locations and are connected by a\ncommunication network. Due to failures in the communication link, faults arise\nwhich may make the entire system dysfunctional. To enable seamless operation of\nthe distributed system, these faults need to be detected and located\naccurately. This paper examines various techniques of handling faults in\ndistributed systems and proposes and innovative technique which uses percent\naccuracy for detecting faulty nodes in the system. Every node in the system\nacts as an initiator and votes for certifying faulty nodes in the system. This\ncertification is done on the basis of percent accuracy value of each faulty\nnode which should exceed a predefined threshold value to qualify node as\nfaulty. As the threshold increases, the number of faulty nodes detected in the\nsystem reduces. This is a decentralized approach with no dependency on a single\nnode to act as a leader for diagnosis. This technique is also applicable to\nad-hoc networks, which are static in nature.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 06:16:46 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Kulkarni", "Poorva", ""], ["Deshpande", "Varsha", ""], ["Sarna", "Latika", ""], ["Shenolikar", "Sumedha", ""], ["Kelkar", "Supriya", ""]]}, {"id": "1812.07782", "submitter": "Poorva Kulkarni", "authors": "Latika Sarna, Sumedha Shenolikar, Poorva Kulkarni, Varsha Deshpande\n  and Supriya Kelkar", "title": "Decentralized Periodic Approach for Adaptive Fault Diagnosis in\n  Distributed Systems", "comments": "19 pages, 13 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, Decentralized Periodic Approach for Adaptive Fault Diagnosis\n(DP-AFD) algorithm is proposed for fault diagnosis in distributed systems with\narbitrary topology. Faulty nodes may be either unresponsive, may have either\nsoftware or hardware faults. The proposed algorithm detects the faulty nodes\nsituated in geographically distributed locations. This algorithm does not\ndepend on a single node or leader to detect the faults in the system. However,\nit empowers more than one node to detect the fault-free and faulty nodes in the\nsystem. Thus, at the end of each test cycle, every fault-free node acts as a\nleader to diagnose faults in the system. This feature of the algorithm makes it\napplicable to any arbitrary network. After every test cycle of the algorithm,\nall the nodes have knowledge about faulty nodes and each node is tested only\nonce. With this knowledge, there can be redistribution of load, which was\nearlier assigned to the faulty nodes. Also, the algorithm permits repaired node\nre-entry and new node entry. In a system of n nodes, the maximum number of\nfaulty nodes can be (n-1) which is detected by DP-AFD algorithm. DP-AFD is\nperiodic in nature which executes test cycles after regular intervals to detect\nthe faulty nodes in the given distributed system.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 07:12:42 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Sarna", "Latika", ""], ["Shenolikar", "Sumedha", ""], ["Kulkarni", "Poorva", ""], ["Deshpande", "Varsha", ""], ["Kelkar", "Supriya", ""]]}, {"id": "1812.07977", "submitter": "Reuben D. Budiardja", "authors": "Reuben D. Budiardja and Christian Y. Cardall", "title": "Targeting GPUs with OpenMP Directives on Summit: A Simple and Effective\n  Fortran Experience", "comments": "Version accepted by Parallel Computing: Systems and Applications", "journal-ref": "Parallel Computing, Volume 88, 2019, 102544", "doi": "10.1016/j.parco.2019.102544", "report-no": null, "categories": "physics.comp-ph astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use OpenMP to target hardware accelerators (GPUs) on Summit, a newly\ndeployed supercomputer at the Oak Ridge Leadership Computing Facility (OLCF),\ndemonstrating simplified access to GPU devices for users of our astrophysics\ncode GenASiS and useful speedup on a sample fluid dynamics problem. We modify\nour workhorse class for data storage to include members and methods that\nsignificantly streamline the persistent allocation of and association to GPU\nmemory. Users offload computational kernels with OpenMP target directives that\nare rather similar to constructs already familiar from multi-core\nparallelization. In this initial example we ask, \"With a given number of Summit\nnodes, how fast can we compute with and without GPUs?\", and find total wall\ntime speedups of $\\sim 12\\mathrm{X}$. We also find reasonable weak scaling up\nto 8000 GPUs (1334 Summit nodes). We make available the source code from this\nwork at https://github.com/GenASiS/GenASiS_Basics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 14:46:00 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 15:36:00 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Budiardja", "Reuben D.", ""], ["Cardall", "Christian Y.", ""]]}, {"id": "1812.08017", "submitter": "Peng Liu", "authors": "Lanny Z.N. Yuan, Huaibing Jian, Peng Liu, Pengxin Zhu, ShanYang Fu", "title": "AME Blockchain: An Architecture Design for Closed-Loop Fluid Economy\n  Token System", "comments": "arXiv admin note: text overlap with arXiv:1805.02707,\n  arXiv:1802.09651, arXiv:1412.7584, arXiv:1809.00554, arXiv:1405.4951 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this white paper, we propose a blockchain-based system, named AME, which\nis a decentralized infrastructure and application platform with enhanced\nsecurity and self-management properties. The AME blockchain technology aims to\nincrease the transaction throughput by adopting various optimizations in\nnetwork transport and storage layers, and to enhance smart contracts with AI\nalgorithm support. We introduce all major technologies adopted in our system,\nincluding blockchain, distributed storage, P2P network, service application\nframework, and data encryption. To properly provide a cohesive, concise, yet\ncomprehensive introduction to the AME system, we mainly focus on describing the\nunique definitions and features that guide the system implementation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 14:52:56 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Yuan", "Lanny Z. N.", ""], ["Jian", "Huaibing", ""], ["Liu", "Peng", ""], ["Zhu", "Pengxin", ""], ["Fu", "ShanYang", ""]]}, {"id": "1812.08073", "submitter": "Jovonni Pharr", "authors": "Jovonni L. Pharr", "title": "Exposing A Customizable, Decentralized Cryptoeconomy as a Data Type", "comments": "working/active paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.GT cs.MA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purposely modular, this protocol enables customization of several protocol\nproperties, including the consensus properties implemented, blockchain type,\nthe roots used, and virtual machine opcodes, among others. These modules enable\nimplementing parties to control the behavior of their economy, with a minimal\namount of effort, and no sacrifice in participant cryptoeconomic quality. This\nwork also demonstrates the simplification of the developer experience by\nabstracting away all technological details, except basic CRUD-based operations,\nusing various programming languages. We demonstrate the mechanism design\napproach taken, and formalize a process for deploying populations of blockchain\neconomies at scale. The framework shown includes adequate tooling for\nsimulation, development, deployment, maintenance, and analytic-based decision\nmaking. Lastly, we introduce an expressive programming language for the purpose\nof creating, and interacting with the cryptoeconomy designed by the\nimplementing developer.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 16:49:26 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Pharr", "Jovonni L.", ""]]}, {"id": "1812.08375", "submitter": "Ranesh Kumar Naha", "authors": "Nasrin Akhter, Mohamed Othman and Ranesh Kumar Naha", "title": "Energy-aware virtual machine selection method for cloud data center\n  resource allocation", "comments": "16 Pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saving energy is an important issue for cloud providers to reduce energy cost\nin a data center. With the increasing popularity of cloud computing, it is time\nto examine various energy reduction methods for which energy consumption could\nbe reduced and lead us to green cloud computing. In this paper, our aim is to\npropose a virtual machine selection algorithm to improve the energy efficiency\nof a cloud data center. We are also presenting experimental results of the\nproposed algorithm in a cloud computing based simulation environment. The\nproposed algorithm dynamically took the virtual machines' allocation,\ndeallocation, and reallocation action to the physical server. However, it\ndepends on the load and heuristics based on the analysis placement of a virtual\nmachine which is decided over time. From the results obtained from the\nsimulation, we have found that our proposed virtual machine selection algorithm\nreduces the total energy consumption by 19% compared to the existing one.\nTherefore, the energy consumption cost of a cloud data center reduces and also\nlowers the carbon footprint. Simulation-based experimental results show that\nthe proposed heuristics which are based on resource provisioning algorithms\nreduce the energy consumption of the cloud data center and decrease the virtual\nmachine's migration rate.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 06:29:12 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Akhter", "Nasrin", ""], ["Othman", "Mohamed", ""], ["Naha", "Ranesh Kumar", ""]]}, {"id": "1812.08446", "submitter": "Chryssis Georgiou", "authors": "Antonio Fernandez Anta, Chryssis Georgiou, Nicolas Nicolaou", "title": "Atomic Appends: Selling Cars and Coordinating Armies with Multiple\n  Distributed Ledgers", "comments": "14 pages, 3 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The various applications using Distributed Ledger Technologies (DLT) or\nblockchains, have led to the introduction of a new `marketplace' where multiple\ntypes of digital assets may be exchanged. As each blockchain is designed to\nsupport specific types of assets and transactions, and no blockchain will\nprevail, the need to perform interblockchain transactions is already pressing.\n  In this work we examine the fundamental problem of interoperable and\ninterconnected blockchains. In particular, we begin by introducing the\nMulti-Distributed Ledger Objects (MDLO), which is the result of aggregating\nmultiple Distributed Ledger Objects -- DLO (a DLO is a formalization of the\nblockchain) and that supports append and get operations of records (e.g.,\ntransactions) in them from multiple clients concurrently. Next, we define the\nAtomicAppends problem, which emerges when the exchange of digital assets\nbetween multiple clients may involve appending records in more than one DLO.\nSpecifically, AtomicAppend requires that either all records will be appended on\nthe involved DLOs or none. We examine the solvability of this problem assuming\nrational and risk-averse clients that may fail by crashing, and under different\nclient utility and append models, timing models, and client failure scenarios.\nWe show that for some cases the existence of an intermediary is necessary for\nthe problem solution. We propose the implementation of such intermediary over a\nspecialized blockchain, we term Smart DLO (SDLO), and we show how this can be\nused to solve the AtomicAppends problem even in an asynchronous, client\ncompetitive environment, where all the clients may crash.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 09:47:43 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Anta", "Antonio Fernandez", ""], ["Georgiou", "Chryssis", ""], ["Nicolaou", "Nicolas", ""]]}, {"id": "1812.08491", "submitter": "Matin Hashemi", "authors": "Behrooz Zarebavani, Foad Jafarinejad, Matin Hashemi, Saber\n  Salehkaleybar", "title": "cuPC: CUDA-based Parallel PC Algorithm for Causal Structure Learning on\n  GPU", "comments": "Published in IEEE Transactions on Parallel and Distributed Systems\n  (TPDS), 2019", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems (TPDS), Vol.\n  31, No. 3, March 2020", "doi": "10.1109/TPDS.2019.2939126", "report-no": null, "categories": "cs.DC cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal in many fields in the empirical sciences is to discover causal\nrelationships among a set of variables from observational data. PC algorithm is\none of the promising solutions to learn underlying causal structure by\nperforming a number of conditional independence tests. In this paper, we\npropose a novel GPU-based parallel algorithm, called cuPC, to execute an\norder-independent version of PC. The proposed solution has two variants, cuPC-E\nand cuPC-S, which parallelize PC in two different ways for multivariate normal\ndistribution. Experimental results show the scalability of the proposed\nalgorithms with respect to the number of variables, the number of samples, and\ndifferent graph densities. For instance, in one of the most challenging\ndatasets, the runtime is reduced from more than 11 hours to about 4 seconds. On\naverage, cuPC-E and cuPC-S achieve 500 X and 1300 X speedup, respectively,\ncompared to serial implementation on CPU. The source code of cuPC is available\nonline [1].\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 11:30:09 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 20:13:37 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 08:20:37 GMT"}, {"version": "v4", "created": "Mon, 23 Sep 2019 06:50:13 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zarebavani", "Behrooz", ""], ["Jafarinejad", "Foad", ""], ["Hashemi", "Matin", ""], ["Salehkaleybar", "Saber", ""]]}, {"id": "1812.08655", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Danial Azam, Arpit Kapoor, R. Dietmar M\\\"uller", "title": "Surrogate-assisted Bayesian inversion for landscape and basin evolution\n  models", "comments": "Geoscientific Model Development", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NE physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The complex and computationally expensive nature of landscape evolution\nmodels pose significant challenges in the inference and optimisation of unknown\nparameters. Bayesian inference provides a methodology for estimation and\nuncertainty quantification of unknown model parameters. In our previous work,\nwe developed parallel tempering Bayeslands as a framework for parameter\nestimation and uncertainty quantification for the Badlands landscape evolution\nmodel. Parallel tempering Bayeslands features high-performance computing with\ndozens of processing cores running in parallel to enhance computational\nefficiency. Although we use parallel computing, the procedure remains\ncomputationally challenging since thousands of samples need to be drawn and\nevaluated. \\textcolor{black}{In large-scale landscape and basin evolution\nproblems, a single model evaluation can take from several minutes to hours, and\nin some instances, even days. Surrogate-assisted optimisation has been used for\nseveral computationally expensive engineering problems which motivate its use\nin optimisation and inference of complex geoscientific models.} The use of\nsurrogate models can speed up parallel tempering Bayeslands by developing\ncomputationally inexpensive models to mimic expensive ones. In this paper, we\napply surrogate-assisted parallel tempering where that surrogate mimics a\nlandscape evolution model by estimating the likelihood function from the model.\n\\textcolor{black}{We employ a neural network-based surrogate model that learns\nfrom the history of samples generated. } The entire framework is developed in a\nparallel computing infrastructure to take advantage of parallelism. The results\nshow that the proposed methodology is effective in lowering the overall\ncomputational cost significantly while retaining the quality of solutions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 03:56:20 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 09:51:22 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 13:05:43 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Chandra", "Rohitash", ""], ["Azam", "Danial", ""], ["Kapoor", "Arpit", ""], ["M\u00fcller", "R. Dietmar", ""]]}, {"id": "1812.08806", "submitter": "Martin Garriga", "authors": "Martin Garriga, Maximiliano Arias, Alan De Renzis", "title": "Blockchain and Cryptocurrency: A comparative framework of the main\n  Architectural Drivers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is a decentralized transaction and data management solution, the\ntechnological weapon-of-choice behind the success of Bitcoin and other\ncryptocurrencies. As the number and variety of existing blockchain\nimplementations continues to increase, adopters should focus on selecting the\nbest one to support their decentralized applications (dApps), rather than\ndeveloping new ones from scratch. In this paper we present a framework to aid\nsoftware architects, developers, tool selectors and decision makers to adopt\nthe right blockchain technology for their problem at hand. The framework\nexposes the correlation between technological decisions and architectural\nfeatures, capturing the knowledge from existing industrial products, technical\nforums/blogs, experts' feedback and academic literature; plus our own\nexperience using and developing blockchain-based applications. We validate our\nframework by applying it to dissect the most outstanding blockchain platforms,\ni.e., the ones behind the top 10 cryptocurrencies apart from Bitcoin. Then, we\nshow how we applied it to a real-world case study in the insurtech domain.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:35:22 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Garriga", "Martin", ""], ["Arias", "Maximiliano", ""], ["De Renzis", "Alan", ""]]}, {"id": "1812.09141", "submitter": "Anastasios Gounaris", "authors": "Christos Bellas and Anastasios Gounaris", "title": "Speeding-up the Verification Phase of Set Similarity Joins in the GPGPU\n  paradigm", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of exact set similarity joins using a co-process\nCPU-GPU scheme. The state-of-the-art CPU solutions split the wok in two main\nphases. First, filtering and index building takes place to reduce the candidate\nsets to be compared as much as possible; then the pairs are compared to verify\nwhether they should become part of the result. We investigate in-depth\nsolutions for transferring the second, so-called verification phase, to the GPU\naddressing several challenges regarding the data serialization and layout, the\nthread management and the techniques to compare sets of tokens. Using real\ndatasets, we provide concrete experimental proofs that our solutions have\nreached their maximum potential, since they totally overlap verification with\nCPU tasks, and manage to yield significant speed-ups, up to 2.6X in our cases.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:24:56 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Bellas", "Christos", ""], ["Gounaris", "Anastasios", ""]]}, {"id": "1812.09233", "submitter": "Shantanu Sharma", "authors": "Sharad Mehrotra, Shantanu Sharma, Jeffrey D. Ullman, and Anurag Mishra", "title": "Partitioned Data Security on Outsourced Sensitive and Non-sensitive Data", "comments": "Accepted in IEEE International Conference on Data Engineering (ICDE),\n  2019. arXiv admin note: text overlap with arXiv:1812.01741", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite extensive research on cryptography, secure and efficient query\nprocessing over outsourced data remains an open challenge. This paper continues\nalong the emerging trend in secure data processing that recognizes that the\nentire dataset may not be sensitive, and hence, non-sensitivity of data can be\nexploited to overcome limitations of existing encryption-based approaches. We\npropose a new secure approach, entitled query binning (QB) that allows\nnon-sensitive parts of the data to be outsourced in clear-text while\nguaranteeing that no information is leaked by the joint processing of\nnon-sensitive data (in clear-text) and sensitive data (in encrypted form). QB\nmaps a query to a set of queries over the sensitive and non-sensitive data in a\nway that no leakage will occur due to the joint processing over sensitive and\nnon-sensitive data. Interestingly, in addition to improve performance, we show\nthat QB actually strengthens the security of the underlying cryptographic\ntechnique by preventing size, frequency-count, and workload-skew attacks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 03:06:17 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""], ["Mishra", "Anurag", ""]]}, {"id": "1812.09247", "submitter": "Mengshuo Jia", "authors": "Mengshuo Jia, Shaowei Huang, Zhiwen Wang, Chen Shen", "title": "Privacy-Preserving Distributed Parameter Estimation for Probability\n  Distribution of Wind Power Forecast Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building the conditional probability distribution of wind power forecast\nerrors benefits both wind farms (WFs) and independent system operators (ISOs).\nEstablishing the joint probability distribution of wind power and the\ncorresponding forecast data of spatially correlated WFs is the foundation for\nderiving the conditional probability distribution. Traditional parameter\nestimation methods for probability distributions require the collection of\nhistorical data of all WFs. However, in the context of multi-regional\ninterconnected grids, neither regional ISOs nor WFs can collect the raw data of\nWFs in other regions due to privacy or competition considerations. Therefore,\nbased on the Gaussian mixture model, this paper first proposes a\nprivacy-preserving distributed expectation-maximization algorithm to estimate\nthe parameters of the joint probability distribution. This algorithm consists\nof two original methods: (1) a privacy-preserving distributed summation\nalgorithm and (2) a privacy-preserving distributed inner product algorithm.\nThen, we derive each WF's conditional probability distribution of forecast\nerror from the joint one. By the proposed algorithms, WFs only need local\ncalculations and privacy-preserving neighboring communications to achieve the\nwhole parameter estimation. These algorithms are verified using the wind\nintegration data set published by the NREL.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 08:44:12 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 13:22:08 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 16:02:41 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 18:19:50 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Jia", "Mengshuo", ""], ["Huang", "Shaowei", ""], ["Wang", "Zhiwen", ""], ["Shen", "Chen", ""]]}, {"id": "1812.09404", "submitter": "Syed Eqbal Alam", "authors": "Syed Eqbal Alam and Robert Shorten and Fabian Wirth and Jia Yuan Yu", "title": "Derandomized Distributed Multi-resource Allocation with Little\n  Communication Overhead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of distributed optimization problems for multiple shared\nresource allocation in Internet-connected devices. We propose a derandomized\nversion of an existing stochastic additive-increase and multiplicative-decrease\n(AIMD) algorithm. The proposed solution uses one bit feedback signal for each\nresource between the system and the Internet-connected devices and does not\nrequire inter-device communication. Additionally, the Internet-connected\ndevices do not compromise their privacy and the solution does not dependent on\nthe number of participating devices. In the system, each Internet-connected\ndevice has private cost functions which are strictly convex, twice continuously\ndifferentiable and increasing. We show empirically that the long-term average\nallocations of multiple shared resources converge to optimal allocations and\nthe system achieves minimum social cost. Furthermore, we show that the proposed\nderandomized AIMD algorithm converges faster than the stochastic AIMD algorithm\nand both the approaches provide approximately same solutions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 22:59:01 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Alam", "Syed Eqbal", ""], ["Shorten", "Robert", ""], ["Wirth", "Fabian", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1812.09442", "submitter": "Eyal Cidon", "authors": "Manu Bansal, Eyal Cidon, Arjun Balasingam, Aditya Gudipati, Christos\n  Kozyrakis, Sachin Katti", "title": "Trevor: Automatic configuration and scaling of stream processing\n  pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operating a distributed data stream processing workload efficiently at scale\nis hard. The operator of the workload must parallelize and lay out tasks of the\nworkload with resources that match the requirement of target data rate. The\nchallenge is that neither the operator nor the programmer is typically aware of\nthe scaling behavior of the workload as a function of resources. An operator\nmanually searches for a safe operating point that can handle predicted peak\nload and deploys with ample headroom for absorbing unpredictable spikes. Such\nempirical, static over-provisioning is wasteful of both compute and human\nresources. We show that precise performance models can be automatically learned\nfor distributed stream processing systems that can predict the execution\nperformance of a job even before deployment. Further, those models can be used\nto optimally schedule logically specified jobs onto available physical\nhardware. Finally, those models and the derived execution schedules can be\nrefined online to dynamically adapt to unpredictable changes in the runtime\nenvironment or auto-scale with variations in job load.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 03:15:41 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Bansal", "Manu", ""], ["Cidon", "Eyal", ""], ["Balasingam", "Arjun", ""], ["Gudipati", "Aditya", ""], ["Kozyrakis", "Christos", ""], ["Katti", "Sachin", ""]]}, {"id": "1812.09459", "submitter": "Yong Deng", "authors": "Yong Deng and Min Dong", "title": "Optimal Cache Placement for Modified Coded Caching with Arbitrary Cache\n  Size", "comments": "In IEEE SPAWC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider content caching between a service provider and multiple\ncache-enabled users, using the recently proposed modified coded caching scheme\n(MCCS) that provides an improved delivery strategy for random user requests. We\ndevelop the optimal cache placement solution for the MCCS with arbitrary cache\nsize by formulating the cache placement as an optimization problem to minimize\nthe average rate during the delivery phase under random user requests. Through\nreformulation, we show that the problem is a linear programming problem. By\nexploring the properties in the caching constraints, we obtain the optimal\ncache placement solution in closed-form. We verify that the existing cache\nplacement scheme obtained at specific cache sizes is a special case of our\nsolution. Numerical studies show how the caching gain changes as the user\npopulation increases, as a result of different cache placement patterns.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 06:12:42 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 16:36:38 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 15:15:43 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Deng", "Yong", ""], ["Dong", "Min", ""]]}, {"id": "1812.09537", "submitter": "Christopher Harrison", "authors": "Christopher Harrison and Christine R. Kirkpatrick and In\\^es Dutra", "title": "Bioinformatics Computational Cluster Batch Task Profiling with Machine\n  Learning for Failure Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Traditional computational cluster schedulers are based on user\ninputs and run time needs request for memory and CPU, not IO. Heavily IO bound\ntask run times, like ones seen in many big data and bioinformatics problems,\nare dependent on the IO subsystems scheduling and are problematic for cluster\nresource scheduling. The problematic rescheduling of IO intensive and errant\ntasks is a lost resource. Understanding the conditions in both successful and\nfailed tasks and differentiating them could provide knowledge to enhancing\ncluster scheduling and intelligent resource optimization.\n  Results: We analyze a production computational cluster contributing 6.7\nthousand CPU hours to research over two years. Through this analysis we develop\na machine learning task profiling agent for clusters that attempts to predict\nfailures between identically provision requested tasks.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 14:53:15 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Harrison", "Christopher", ""], ["Kirkpatrick", "Christine R.", ""], ["Dutra", "In\u00eas", ""]]}, {"id": "1812.09727", "submitter": "Marco Guazzone", "authors": "Cosimo Anglano and Massimo Canonico and Marco Guazzone", "title": "The ShareGrid Portal: an easy way to submit jobs on computational Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-INF-2008-10-08-UNIPMN", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid computing is a distributed computing paradigm which aims to aggregate\nseveral heterogeneous and distributed resources, belonging to different and\nindependent organizations, in a dynamic, transparent and coordinated way. Since\nits introduction, Grid computing has been successfully applied to solve several\nscientific challenging applications. Despite of the consolidation of many of\nits aspects, there are some issues that are still open. One of them is the\ntransparency: in many real Grid systems, users still need to be aware of Grid\ncomputing, either for adapting their applications to this paradigm or for\nwrapping them in a suitable software framework. In this paper we present the\nShareGrid Portal, a Web portal and a portal framework, built on top of the\nShareGrid project infrastructure. Its intent is both to ease the execution of\nuser applications in a Grid system and to allow developers to flexibly add new\nportal functionalities. In this work, we compare it with other well-known Grid\nportals and we show its user interface and its architecture. Finally we discuss\nuser experiences and future extensions.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 15:14:49 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Anglano", "Cosimo", ""], ["Canonico", "Massimo", ""], ["Guazzone", "Marco", ""]]}, {"id": "1812.09730", "submitter": "Marco Guazzone", "authors": "Cosimo Anglano and Massimo Canonico and Marco Guazzone and Matteo Zola", "title": "The TAAROA Project Specification", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-INF-2009-02-02-UNIPMN", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction, the Grid computing paradigm has been widely adopted\nboth in scientific and also in industrial areas. The main advantage of the Grid\ncomputing paradigm is the ability to enable, in a transparent way, the sharing\nand the coordination of several heterogeneous and large-scale distributed\nresources belonging to different institutional domains. One of its limitation\nis the lack of facilities for executing services. In fact, Grid computing has\nbeen traditionally used and improved for running computational-intensive or\ndata-intensive applications. A service differs from this kind of applications\nin that it usually waits for requests from clients and replies with useful\ninformation; moreover, a service is typically subjected to some predefined\nconstraints, called Service Level Agreement (SLA), including both temporal and\nperformance restrictions. In this paper we present the TAAROA middleware, a\nsoftware system that tries to extend the traditional target of the Grid\ncomputing paradigm to include the service concept. It attempts to accomplish\nits goal by using the virtualization technology. By abstracting the hardware\nand software resources of a computer, virtualization brings to TAAROA two\nimportant benefits: (1) the encapsulation of the service runtime environment,\nand (2) the possibility, through the migration facility, to move a service from\nthe computer where it is running to another one that hopefully reduces the risk\nof violating some of the SLA constraints. In the current version of TAAROA\nmiddleware there is no explicit mechanism for achieving the level of a service\nas defined by the related SLA; this means that actually TAAROA is only able to\nprovide a best-effort service.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 15:36:22 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Anglano", "Cosimo", ""], ["Canonico", "Massimo", ""], ["Guazzone", "Marco", ""], ["Zola", "Matteo", ""]]}, {"id": "1812.09849", "submitter": "Sabber Ahamed", "authors": "Sabber Ahamed and Eunseo Choi", "title": "Incorporating Deformation Energetics in Long-Term Tectonic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The deformation-related energy budget is usually considered in the simplest\nform or even completely omitted from the energy balance equation. We derive a\nfull energy balance equation that accounts not only for heat energy but also\nfor mechanical (elastic, plastic and viscous) work. The derived equation is\nimplemented in DES3D, an unstructured finite element solver for long-term\ntectonic deformation. We verify the implementation by comparing numerical\nsolutions to the corresponding semi-analytic solutions in three benchmarks\nextended from the classical oedometer test. Two of the benchmarks are designed\nto evaluate the temperature change in a Mohr-Coulomb elasto-plastic square\ngoverned by a simplified equation involving plastic power only and by the full\ntemperature evolution equation, respectively. The third benchmark differs in\nthat it computes thermal stresses associated with a prescribed uniform\ntemperature increase. All the solutions from DES3D show relative error less\nthan 0.1%. We also investigate the long-term effects of deformation energetics\non the evolution of large offset normal faults. We find that the models\nconsidering the full energy balance equation tend to produce more secondary\nfaults and an elongated core complex. Our results for the normal fault system\nconfirm that persistent inelastic deformation has a significant impact on the\nlong-term evolution of faults, motivating further exploration of the role of\nthe full energy balance equation in other geodynamic systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 07:25:00 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Ahamed", "Sabber", ""], ["Choi", "Eunseo", ""]]}, {"id": "1812.09940", "submitter": "Federico Spini", "authors": "Marco Conoscenti, Antonio Vetr\\`o, Juan Carlos De Martin, Federico\n  Spini, Fabio Castaldo, Sebastiano Scr\\`ofina", "title": "CLoTH: a Simulator for HTLC Payment Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Lightning Network (LN) is one of the most promising off-chain scaling\nsolutions for Bitcoin, as it enables off-chain payments which are not subject\nto the well-known blockchain scalability limit. In this work, we introduce\nCLoTH, a simulator for HTLC payment networks, of which LN is the best working\nexample. It simulates input-defined payments on an input-defined HTLC network\nand produces performance measures in terms of payment-related statistics, such\nas time to complete payments and probability of payment failure. CLoTH helps to\npredict issues that might arise in the development of an HTLC payment network,\nand to estimate the effects of an optimisation before deploying it. In upcoming\nworks we'll publish the results of CLoTH simulations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 15:37:21 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Conoscenti", "Marco", ""], ["Vetr\u00f2", "Antonio", ""], ["De Martin", "Juan Carlos", ""], ["Spini", "Federico", ""], ["Castaldo", "Fabio", ""], ["Scr\u00f2fina", "Sebastiano", ""]]}, {"id": "1812.10027", "submitter": "Hongshan Li", "authors": "Hongshan Li, Chenghao Hu, Jingyan Jiang, Zhi Wang, Yonggang Wen, Wenwu\n  Zhu", "title": "JALAD: Joint Accuracy- and Latency-Aware Deep Structure Decoupling for\n  Edge-Cloud Execution", "comments": "conference, copyright transfered to IEEE", "journal-ref": null, "doi": "10.1109/PADSW.2018.8645013", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a rapid growth of deep-network based services and\napplications. A practical and critical problem thus has emerged: how to\neffectively deploy the deep neural network models such that they can be\nexecuted efficiently. Conventional cloud-based approaches usually run the deep\nmodels in data center servers, causing large latency because a significant\namount of data has to be transferred from the edge of network to the data\ncenter. In this paper, we propose JALAD, a joint accuracy- and latency-aware\nexecution framework, which decouples a deep neural network so that a part of it\nwill run at edge devices and the other part inside the conventional cloud,\nwhile only a minimum amount of data has to be transferred between them. Though\nthe idea seems straightforward, we are facing challenges including i) how to\nfind the best partition of a deep structure; ii) how to deploy the component at\nan edge device that only has limited computation power; and iii) how to\nminimize the overall execution latency. Our answers to these questions are a\nset of strategies in JALAD, including 1) A normalization based in-layer data\ncompression strategy by jointly considering compression rate and model\naccuracy; 2) A latency-aware deep decoupling strategy to minimize the overall\nexecution latency; and 3) An edge-cloud structure adaptation strategy that\ndynamically changes the decoupling for different network conditions.\nExperiments demonstrate that our solution can significantly reduce the\nexecution latency: it speeds up the overall inference execution with a\nguaranteed model accuracy loss.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 04:46:15 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Li", "Hongshan", ""], ["Hu", "Chenghao", ""], ["Jiang", "Jingyan", ""], ["Wang", "Zhi", ""], ["Wen", "Yonggang", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1812.10169", "submitter": "Jared Saia", "authors": "Valerie King and Jared Saia", "title": "Correction to Byzantine Agreement in Expected Polynomial Time, JACM 2016", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a correction by the authors to \"Byzantine Agreement in Expected\nPolynomial Time\" which appeared in the Journal of the ACM in 2016. It corrects\na failure in the paper to consider the adversary's ability to decide the number\nof fair coinflips in an iteration, where this number ranges between n(n-t) and\nn(n-2t).\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 21:25:46 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 19:58:42 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["King", "Valerie", ""], ["Saia", "Jared", ""]]}, {"id": "1812.10302", "submitter": "Mikhail Zymbler", "authors": "Yana Kraeva, Mikhail Zymbler", "title": "The Use of MPI and OpenMP Technologies for Subsequence Similarity Search\n  in Very Large Time Series on Computer Cluster System with Nodes Based on the\n  Intel Xeon Phi Knights Landing Many-core Processor", "comments": "Accepted for publication in the \"Numerical Methods and Programming\"\n  journal (http://num-meth.srcc.msu.ru/english/, in Russian \"Vychislitelnye\n  Metody i Programmirovanie\"), in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, subsequence similarity search is required in a wide range of time\nseries mining applications: climate modeling, financial forecasts, medical\nresearch, etc. In most of these applications, the Dynamic TimeWarping (DTW)\nsimilarity measure is used since DTW is empirically confirmed as one of the\nbest similarity measure for most subject domains. Since the DTW measure has a\nquadratic computational complexity w.r.t. the length of query subsequence, a\nnumber of parallel algorithms for various many-core architectures have been\ndeveloped, namely FPGA, GPU, and Intel MIC. In this article, we propose a new\nparallel algorithm for subsequence similarity search in very large time series\non computer cluster systems with nodes based on Intel Xeon Phi Knights Landing\n(KNL) many-core processors. Computations are parallelized on two levels as\nfollows: through MPI at the level of all cluster nodes, and through OpenMP\nwithin one cluster node. The algorithm involves additional data structures and\nredundant computations, which make it possible to effectively use the\ncapabilities of vector computations on Phi KNL. Experimental evaluation of the\nalgorithm on real-world and synthetic datasets shows that it is highly\nscalable.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 13:12:19 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Kraeva", "Yana", ""], ["Zymbler", "Mikhail", ""]]}, {"id": "1812.10437", "submitter": "Mostafa Tavassolipour", "authors": "Mostafa Tavassolipour, Armin Karamzade, Reza Mirzaeifard, Seyed\n  Abolfazl Motahari, and Mohammad-Taghi Manzuri Shalmani", "title": "Structure Learning of Sparse GGMs over Multiple Access Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central machine is interested in estimating the underlying structure of a\nsparse Gaussian Graphical Model (GGM) from datasets distributed across multiple\nlocal machines. The local machines can communicate with the central machine\nthrough a wireless multiple access channel. In this paper, we are interested in\ndesigning effective strategies where reliable learning is feasible under power\nand bandwidth limitations. Two approaches are proposed: Signs and Uncoded\nmethods. In Signs method, the local machines quantize their data into binary\nvectors and an optimal channel coding scheme is used to reliably send the\nvectors to the central machine where the structure is learned from the received\ndata. In Uncoded method, data symbols are scaled and transmitted through the\nchannel. The central machine uses the received noisy symbols to recover the\nstructure. Theoretical results show that both methods can recover the structure\nwith high probability for large enough sample size. Experimental results\nindicate the superiority of Signs method over Uncoded method under several\ncircumstances.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 18:10:40 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Tavassolipour", "Mostafa", ""], ["Karamzade", "Armin", ""], ["Mirzaeifard", "Reza", ""], ["Motahari", "Seyed Abolfazl", ""], ["Shalmani", "Mohammad-Taghi Manzuri", ""]]}, {"id": "1812.10460", "submitter": "Tayyebeh Jahani-Nezhad", "authors": "Tayyebeh Jahani-Nezhad, Mohammad Ali Maddah-Ali", "title": "CodedSketch: A Coding Scheme for Distributed Computation of Approximated\n  Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose CodedSketch, as a distributed straggler-resistant\nscheme to compute an approximation of the multiplication of two massive\nmatrices. The objective is to reduce the recovery threshold, defined as the\ntotal number of worker nodes that we need to wait for to be able to recover the\nfinal result. To exploit the fact that only an approximated result is required,\nin reducing the recovery threshold, some sorts of pre-compression are required.\nHowever, compression inherently involves some randomness that would lose the\nstructure of the matrices. On the other hand, considering the structure of the\nmatrices is crucial to reduce the recovery threshold. In CodedSketch, we use\ncount--sketch, as a hash-based compression scheme, on the rows of the first and\ncolumns of the second matrix, and a structured polynomial code on the columns\nof the first and rows of the second matrix. This arrangement allows us to\nexploit the gain of both in reducing the recovery threshold. To increase the\naccuracy of computation, multiple independent count--sketches are needed. This\nindependency allows us to theoretically characterize the accuracy of the result\nand establish the recovery threshold achieved by the proposed scheme. To\nguarantee the independency of resulting count--sketches in the output, while\nkeeping its cost on the recovery threshold minimum, we use another layer of\nstructured codes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 18:52:41 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 14:51:56 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 07:02:11 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Jahani-Nezhad", "Tayyebeh", ""], ["Maddah-Ali", "Mohammad Ali", ""]]}, {"id": "1812.10485", "submitter": "Maged Eljazzar", "authors": "Ahmed S. Almasoud, Maged M. Eljazzar, Farookh Hussain", "title": "Toward a self-learned Smart Contracts", "comments": "5", "journal-ref": "2018 IEEE 15th International Conference on e-Business Engineering\n  (ICEBE)", "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Blockchain technology has been highly valued and disruptive.\nSeveral researches have presented a merge between blockchain and current\napplication i.e. medical, supply chain, and e-commerce. Although Blockchain\narchitecture does not have a standard yet, IBM, MS, AWS offer BaaS (Blockchain\nas a Service). In addition to the current public chains i.e. Ethereum, NEO, and\nCardeno; there are some differences between several public ledgers in terms of\ndevelopment and architecture. This paper introduces the main factors that\naffect integration of Artificial Intelligence with Blockchain. As well as, how\nit could be integrated for forecasting and automating; building self-regulated\nchain.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 20:34:17 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Almasoud", "Ahmed S.", ""], ["Eljazzar", "Maged M.", ""], ["Hussain", "Farookh", ""]]}, {"id": "1812.10499", "submitter": "Vijay Garg", "authors": "Vijay K. Garg", "title": "Removing Sequential Bottleneck of Dijkstra's Algorithm for the Shortest\n  Path Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  All traditional methods of computing shortest paths depend upon\nedge-relaxation where the cost of reaching a vertex from a source vertex is\npossibly decreased if that edge is used. We introduce a method which maintains\nlower bounds as well as upper bounds for reaching a vertex. This method enables\none to find the optimal cost for multiple vertices in one iteration and thereby\nreduces the sequential bottleneck in Dijkstra's algorithm.\n  We present four algorithms in this paper --- $SP_1$, $SP_2$, $SP_3$ and\n$SP_4$. $SP_1$ and $SP_2$ reduce the number of heap operations in Dijkstra's\nalgorithm. For directed acyclic graphs, or directed unweighted graphs they have\nthe optimal complexity of $O(e)$ where $e$ is the number of edges in the graph\nwhich is better than that of Dijkstra's algorithm. For general graphs, their\nworst case complexity matches that of Dijkstra's algorithm for a sequential\nimplementation but allows for greater parallelism. Algorithms $SP_3$ and $SP_4$\nallow for even more parallelism but with higher work complexity. Algorithm\n$SP_3$ requires $O(n + e(\\max(\\log n, \\Delta)))$ work where $n$ is the number\nof vertices and $\\Delta$ is the maximum in-degree of a node. Algorithm $SP_4$\nhas the most parallelism. It requires $O(ne)$ work. These algorithms generalize\nthe work by Crauser, Mehlhorn, Meyer, and Sanders on parallelizing Dijkstra's\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 19:00:15 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Garg", "Vijay K.", ""]]}, {"id": "1812.10624", "submitter": "Xiaorui Wu", "authors": "Xiaorui Wu, Hong Xu, Bo Li, Yongqiang Xiong", "title": "Stanza: Layer Separation for Distributed Training in Deep Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameter server architecture is prevalently used for distributed deep\nlearning. Each worker machine in a parameter server system trains the complete\nmodel, which leads to a hefty amount of network data transfer between workers\nand servers. We empirically observe that the data transfer has a non-negligible\nimpact on training time.\n  To tackle the problem, we design a new distributed training system called\nStanza. Stanza exploits the fact that in many models such as convolution neural\nnetworks, most data exchange is attributed to the fully connected layers, while\nmost computation is carried out in convolutional layers. Thus, we propose layer\nseparation in distributed training: the majority of the nodes just train the\nconvolutional layers, and the rest train the fully connected layers only.\nGradients and parameters of the fully connected layers no longer need to be\nexchanged across the cluster, thereby substantially reducing the data transfer\nvolume. We implement Stanza on PyTorch and evaluate its performance on Azure\nand EC2. Results show that Stanza accelerates training significantly over\ncurrent parameter server systems: on EC2 instances with Tesla V100 GPU and 10Gb\nbandwidth for example, Stanza is 1.34x--13.9x faster for common deep learning\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 05:01:19 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 07:16:22 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Wu", "Xiaorui", ""], ["Xu", "Hong", ""], ["Li", "Bo", ""], ["Xiong", "Yongqiang", ""]]}, {"id": "1812.10668", "submitter": "\\'Alvaro L\\'opez Garc\\'ia", "authors": "\\'Alvaro L\\'opez Garc\\'ia, Enol Fern\\'andez-del-Castillo, Isabel\n  Campos Plasencia", "title": "An efficient cloud scheduler design supporting preemptible instances", "comments": null, "journal-ref": "Future Generation Computer Systems (2019)", "doi": "10.1016/j.future.2018.12.057", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Maximizing resource utilization by performing an efficient resource\nprovisioning is a key factor for any cloud provider: commercial actors can\nmaximize their revenues, whereas scientific and non-commercial providers can\nmaximize their infrastructure utilization. Traditionally, batch systems have\nallowed data centers to fill their resources as much as possible by using\nbackfilling and similar techniques. However, in an IaaS cloud, where virtual\nmachines are supposed to live indefinitely, or at least as long as the user is\nable to pay for them, these policies are not easily implementable. In this work\nwe present a new scheduling algorithm for IaaS providers that is able to\nsupport preemptible instances, that can be stopped by higher priority requests\nwithout introducing large modifications in the current cloud schedulers. This\nscheduler enables the implementation of new cloud usage and payment models that\nallow more efficient usage of the resources and potential new revenue sources\nfor commercial providers. We also study the correctness and the performace\noverhead of the proposed scheduler agains existing solutions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 09:14:37 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 10:10:37 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 08:42:41 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Garc\u00eda", "\u00c1lvaro L\u00f3pez", ""], ["Fern\u00e1ndez-del-Castillo", "Enol", ""], ["Plasencia", "Isabel Campos", ""]]}, {"id": "1812.10844", "submitter": "Dragos-Adrian Seredinschi M.Sc.", "authors": "Rachid Guerraoui and Petr Kuznetsov and Matteo Monti and Matej\n  Pavlovic and Dragos-Adrian Seredinschi", "title": "AT2: Asynchronous Trustworthy Transfers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many blockchain-based protocols, such as Bitcoin, implement a decentralized\nasset transfer (or exchange) system. As clearly stated in the original paper by\nNakamoto, the crux of this problem lies in prohibiting any participant from\nengaging in double-spending. There seems to be a common belief that consensus\nis necessary for solving the double-spending problem. Indeed, whether it is for\na permissionless or a permissioned environment, the typical solution uses\nconsensus to build a totally ordered ledger of submitted transfers. In this\npaper we show that this common belief is false: consensus is not needed to\nimplement of a decentralized asset transfer system. We do so by introducing AT2\n(Asynchronous Trustworthy Transfers), a class of consensusless algorithms. To\nshow formally that consensus is unnecessary for asset transfers, we consider\nthis problem first in the shared-memory context. We introduce AT2$_{SM}$, a\nwait-free algorithm that asynchronously implements asset transfer in the\nread-write shared-memory model. In other words, we show that the consensus\nnumber of an asset-transfer object is one. In the message passing model with\nByzantine faults, we introduce a generic asynchronous algorithm called\nAT2$_{MP}$ and discuss two instantiations of this solution. First, AT2$_{D}$\nensures deterministic guarantees and consequently targets a small scale\ndeployment (tens to hundreds of nodes), typically for a permissioned\nenvironment. Second, AT2$_{P}$ provides probabilistic guarantees and scales\nwell to a very large system size (tens of thousands of nodes), ensuring\nlogarithmic latency and communication complexity. Instead of consensus, we\nconstruct AT2$_{D}$ and AT2$_{P}$ on top of a broadcast primitive with causal\nordering guarantees offering deterministic and probabilistic properties,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 23:24:46 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 16:29:54 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 14:38:26 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Kuznetsov", "Petr", ""], ["Monti", "Matteo", ""], ["Pavlovic", "Matej", ""], ["Seredinschi", "Dragos-Adrian", ""]]}, {"id": "1812.10917", "submitter": "Eylon Yogev", "authors": "Moni Naor and Merav Parter and Eylon Yogev", "title": "The Power of Distributed Verifiers in Interactive Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the power of interactive proofs with a distributed verifier. In\nthis setting, the verifier consists of $n$ nodes and a graph $G$ that defines\ntheir communication pattern. The prover is a single entity that communicates\nwith all nodes by short messages. The goal is to verify that the graph $G$\nbelongs to some language in a small number of rounds, and with small\ncommunication bound, i.e., the proof size.\n  This interactive model was introduced by Kol, Oshman and Saxena (PODC 2018)\nas a generalization of non-interactive distributed proofs. They demonstrated\nthe power of interaction in this setting by constructing protocols for problems\nas Graph Symmetry and Graph Non-Isomorphism -- both of which require proofs of\n$\\Omega(n^2)$-bits without interaction.\n  In this work, we provide a new general framework for distributed interactive\nproofs that allows one to translate standard interactive protocols to ones\nwhere the verifier is distributed with short proof size. We show the following:\n* Every (centralized) computation that can be performed in time $O(n)$ can be\ntranslated into three-round distributed interactive protocol with $O(\\log n)$\nproof size. This implies that many graph problems for sparse graphs have\nsuccinct proofs.\n  * Every (centralized) computation implemented by either a small space or by\nuniform NC circuit can be translated into a distributed protocol with $O(1)$\nrounds and $O(\\log n)$ bits proof size for the low space case and $polylog(n)$\nmany rounds and proof size for NC.\n  * We show that for Graph Non-Isomorphism, there is a 4-round protocol with\n$O(\\log n)$ proof size, improving upon the $O(n \\log n)$ proof size of Kol et\nal.\n  * For many problems we show how to reduce proof size below the naturally\nseeming barrier of $\\log n$. We get a 5-round protocols with proof size $O(\\log\n\\log n)$ for a family of problems.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 07:52:24 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Naor", "Moni", ""], ["Parter", "Merav", ""], ["Yogev", "Eylon", ""]]}, {"id": "1812.10959", "submitter": "Mikhail Zymbler L.", "authors": "Mikhail Zymbler", "title": "Parallel Algorithm for Frequent Itemset Mining on Intel Many-core\n  Systems", "comments": "Accepted for publication in Journal of Computing and Information\n  Technology (http://cit.fer.hr)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent itemset mining leads to the discovery of associations and\ncorrelations among items in large transactional databases. Apriori is a\nclassical frequent itemset mining algorithm, which employs iterative passes\nover database combining with generation of candidate itemsets based on frequent\nitemsets found at the previous iteration, and pruning of clearly infrequent\nitemsets. The Dynamic Itemset Counting (DIC) algorithm is a variation of\nApriori, which tries to reduce the number of passes made over a transactional\ndatabase while keeping the number of itemsets counted in a pass relatively low.\nIn this paper, we address the problem of accelerating DIC on the Intel Xeon Phi\nmany-core system for the case when the transactional database fits in main\nmemory. Intel Xeon Phi provides a large number of small compute cores with\nvector processing units. The paper presents a parallel implementation of DIC\nbased on OpenMP technology and thread-level parallelism. We exploit the\nbit-based internal layout for transactions and itemsets. This technique reduces\nthe memory space for storing the transactional database, simplifies the support\ncount via logical bitwise operation, and allows for vectorization of such a\nstep. Experimental evaluation on the platforms of the Intel Xeon CPU and the\nIntel Xeon Phi coprocessor with large synthetic and real databases showed good\nperformance and scalability of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 11:49:26 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Zymbler", "Mikhail", ""]]}, {"id": "1812.11255", "submitter": "Tevfik Kosar", "authors": "Zulkar Nine and Tevfik Kosar", "title": "A Two-Phase Dynamic Throughput Optimization Model for Big Data Transfers", "comments": "arXiv admin note: substantial text overlap with arXiv:1707.09455", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data moved over dedicated and non-dedicated network links\nincreases much faster than the increase in the network capacity, but the\ncurrent solutions fail to guarantee even the promised achievable transfer\nthroughputs. In this paper, we propose a novel dynamic throughput optimization\nmodel based on mathematical modeling with offline knowledge discovery/analysis\nand adaptive online decision making. In offline analysis, we mine historical\ntransfer logs to perform knowledge discovery about the transfer\ncharacteristics. Online phase uses the discovered knowledge from the offline\nanalysis along with real-time investigation of the network condition to\noptimize the protocol parameters. As real-time investigation is expensive and\nprovides partial knowledge about the current network status, our model uses\nhistorical knowledge about the network and data to reduce the real-time\ninvestigation overhead while ensuring near optimal throughput for each\ntransfer. Our novel approach is tested over different networks with different\ndatasets and outperformed its closest competitor by 1.7x and the default case\nby 5x. It also achieved up to 93% accuracy compared with the optimal achievable\nthroughput possible on those networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 00:50:51 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Nine", "Zulkar", ""], ["Kosar", "Tevfik", ""]]}, {"id": "1812.11309", "submitter": "Yuichi Sudo", "authors": "Yuichi Sudo, Fukuhito Ooshita, Taisuke Izumi, Hirotsugu Kakugawa,\n  Toshimitsu Masuzawa", "title": "Logarithmic Expected-Time Leader Election in Population Protocol Model", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the leader election problem in the population protocol model\nis considered. A leader election protocol with logarithmic stabilization time\nis given. Given a rough knowledge m of the population size n such that m >=\n\\log_2 n and m=O(log n), the proposed protocol guarantees that exactly one\nleader is elected from n agents within O(log n) parallel time in expectation\nand the unique leader is kept forever thereafter. The number of states per\nagent of the protocol is O(log n).\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 08:02:41 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 03:00:13 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 12:40:56 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Sudo", "Yuichi", ""], ["Ooshita", "Fukuhito", ""], ["Izumi", "Taisuke", ""], ["Kakugawa", "Hirotsugu", ""], ["Masuzawa", "Toshimitsu", ""]]}, {"id": "1812.11747", "submitter": "Vincent Gramoli", "authors": "Tyler Crain, Christopher Natoli and Vincent Gramoli", "title": "Evaluating the Red Belly Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present the most extensive evaluation of blockchain system\nto date. To achieve scalability across servers in more than 10 countries\nlocated on 4 different continents, we drastically revisited Byzantine fault\ntolerant blockchains and verification of signatures. The resulting blockchain,\ncalled the Red Belly Blockchain (RBBC), commits more than a hundred thousand\ntransactions issued by permissionless nodes. These transactions are grouped\ninto blocks within few seconds through a partially synchronous consensus run by\npermissioned nodes. It prevents double spending by guaranteeing that a unique\nblock is decided at any given index of the chain in a deterministic way by all\nparticipants. We compared the performance of RBBC against traditional Byzantine\nfault tolerant alternatives and more recent randomized solutions. In the same\ngeo-distributed environment with low-end machines, we noticed two interesting\ncomparisons: (i) the RBBC throughput scales to hundreds of machines whereas the\nclassic 3-step leader-based BFT state machine used by consortium blockchains\ncannot scale to 40 identically configured nodes; (ii) RBBC guarantees\ntransaction finality in 3 seconds and experiences a third of the latency that\nrandomized-based solutions like HoneyBadgerBFT can offer. This empirical\nevaluation demonstrates that blockchain scalability can be achieved without\nsacrificing security.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 10:19:49 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Crain", "Tyler", ""], ["Natoli", "Christopher", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1812.11793", "submitter": "Dennis Schunselaar", "authors": "D. M. M. Schunselaar, H. M. W. Verbeek", "title": "Task Elimination may Actually Increase Throughput Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known Task Elimination redesign principle suggests to remove\nunnecessary tasks from a process to improve on time and cost. Although there\nseems to be a general consensus that removing work can only improve the\nthroughput time of the process, this paper shows that this is not necessarily\nthe case by providing an example that uses plain M/M/c activities. This paper\nalso shows that the Task Automation and Parallelism redesign principles may\nalso lead to longer throughput times. Finally, apart from these negative\nresults, the paper also show under which assumption these redesign principles\nindeed can only improve the throughput time.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 14:36:48 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Schunselaar", "D. M. M.", ""], ["Verbeek", "H. M. W.", ""]]}, {"id": "1812.11903", "submitter": "Dariusz Kowalski R", "authors": "Dariusz R. Kowalski and Christopher Thraves Caro", "title": "Randomized Rumor Spreading in Ad Hoc Networks with Buffers", "comments": "Manuscript submitted to DISC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized rumor spreading problem generates a big interest in the area\nof distributed algorithms due to its simplicity, robustness and wide range of\napplications. The two most popular communication paradigms used for spreading\nthe rumor are Push and Pull algorithms. The former protocol allows nodes to\nsend the rumor to a randomly selected neighbor at each step, while the latter\nis based on sending a request and downloading the rumor from a randomly\nselected neighbor, provided the neighbor has it. Previous analysis of these\nprotocols assumed that every node could process all such push/pull operations\nwithin a single step, which could be unrealistic in practical situations.\nTherefore we propose a new framework for analysis rumor spreading accommodating\nbuffers, in which a node can process only one push/pull message or push request\nat a time. We develop upper and lower bounds for randomized rumor spreading\ntime in the new framework, and compare the results with analogous in the old\nframework without buffers.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 17:12:30 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Kowalski", "Dariusz R.", ""], ["Caro", "Christopher Thraves", ""]]}]